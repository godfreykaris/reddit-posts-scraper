[
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Python 3.12 docs include built-in support for themes, including a dark theme!", "Author": "u/bleeddonor", "Content": "Python gives you wings, yes, but you used to have to wear aviator glasses to get through the docs on a bright display. No more. :)"},
{"Title": "Linting Python Monorepo with Bazel and Ruff", "Author": "u/Spindelkryp", "Content": "Heya, I have recently integrated Ruff in the Bazel monorepo of my company. The results were quite impressive, it takes around ~100ms to analyze and apply format / lint results to 1.1k python files. Integration with Bazel, however, was not exactly painless so I wrote a small guide for it as well as an example project. . Hope it helps someone! What My Project Does Guide on how to setup Ruff linting for Bazel based Python projects Target Audience Maintainers of large Python repos Source code How-to guide Source code"},
{"Title": "My library VidGear `v0.3.3` - brings libcamera API Support to python.", "Author": "u/abhi_uno", "Content": "Hello Python developers! I'm excited to announce the release of VidGear v0.3.3 , which brings official support for the libcamera backend in its PiGear API ! This update enhances the capabilities of Raspberry Pi Camera Modules and provides limited USB camera support. More about PiGear: PiGear is a specialized API optimized for Raspberry Pi üçá Boards, offering comprehensive support for camera modules (e.g., OmniVision OV5647 Camera Module, Sony IMX219 Camera Module) and limited compatibility for USB cameras. PiGear implements a seamless and robust wrapper around the Picamera2 Python library, simplifying integration with minimal code changes and ensuring a smooth transition for developers already familiar with the Picamera2 API. PiGear leverages the libcamera API under the hood with multi-threading, providing high-performance üî•, enhanced control, and functionality for Raspberry Pi camera modules. PiGear handles common configuration parameters and non-standard settings for various camera types, simplifying the integration process. PiGear currently supports PiCamera2 API parameters such as sensor, controls, transform, and format, with internal type and sanity checks for robust performance. While primarily focused on Raspberry Pi camera modules, PiGear also provides basic functionality for USB webcams (only with Picamera2 API), along with the ability to accurately differentiate between USB and Raspberry Pi cameras using metadata. PiGear seamlessly switches to the legacy picamera library if the Picamera2 library is unavailable, ensuring seamless backward compatibility. PiGear also provides a flexible multi-threaded framework around the complete picamera API, allowing developers to effortlessly exploit a wide range of parameters, such as brightness, saturation, sensor_mode, iso, exposure, and more. Furthermore, PiGear supports the use of multiple camera modules, including those found on Raspberry Pi Compute Module IO boards and USB cameras (only with Picamera2 API). We're eager to see the innovative projects you'll create with PiGear! For more details and to get started, check out our GitHub repository . Happy coding! Feel free to ask any questions or share your feedback below. Let's discuss and innovate together! üöÄ"},
{"Title": "How to add chart in django tinymce text field", "Author": "u/Funny-Crab-9495", "Content": "i want to add chart in django blog so I'm using tinymce for blog content can add line graph ,bar graph inside blog. Im trying to add plugin in static file  but it doesn't work."},
{"Title": "How to detect (modified|headless) Chrome instrumented with Selenium (2024 edition)", "Author": "u/antvas", "Content": "https://deviceandbrowserinfo.com/learning_zone/articles/detecting-headless-chrome-selenium-2024 TL;DR The 4 techniques are the following: Using the user agent HTTP headers or with navigator.userAgent in JS to detect user agents linked to Headless Chrome: is Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/126.0.6478.114 Safari/537.36 Similarly, by detecting the presence of the HeadlessChrome substring in the sec-ch-ua header By detecting if navigator.webdriver = true in JavaScript By detecting the side effects of CDP (Chrome DevTools Protocol) (detailed in the article)"},
{"Title": "I made a simple web analytics app using Fast API", "Author": "u/sagunsh", "Content": "What My Project Does It s a simple web analytics app that tracks visitors in your website. I am using ip-api.com for geolocation and uap-python library for parsing user agent. At the moment, it simply logs these information to a sqlite database. Target Audience Anyone who is learning Python and more specifically Fast API and want to try building some projects. The code is available on github so feel free to explore, add features. Comparision Having used Django and Flask in the past, I made this project as a way to learn and to explore about Fast API. For a more sophisticated/paid alternative, check out Google Analytics, Plausible, etc. Source Code https://github.com/sagunsh/webanalytics"},
{"Title": "discover-plugins - track which plugins are installed into your python environment", "Author": "u/fesch2", "Content": "What my project does discover-plugins is a simple CLI tool that lets you list and filter plugins entrypoints installed into a python environment. I recently had to track down a bug that I ultimately resulted from a plugin I had installed into my environment.  To my surprise, there was no easy way to list which kind of plugins were installed, so I decided to build my own tool. Target Audience discover-plugins is intended as a debugging tool for those times when you are not quite sure which plugins are currently part of your python environment. Installation pipx install discover-plugins Usage Find all installed plugins related to pytest (the relevant group name is pytest11 ): discover-plugins --group pytest11 The tool defaults to use the first python interpreter on your path, you can optionally specify which interpreter to use with the --interpreter flag. The output will list all entrypoints belonging to the pytest11 group. For example, if you had installed a single pytest plugin ( pytest-aws-apigateway ) the output would look like this: { \"pytest11\": [ { \"name\": \"pytest_httpx\", \"group\": \"pytest11\", \"value\": \"pytest_httpx\" }, { \"name\": \"anyio\", \"group\": \"pytest11\", \"value\": \"anyio.pytest_plugin\" }, { \"name\": \"pytest-aws-apigateway\", \"group\": \"pytest11\", \"value\": \"pytest_aws_apigateway.plugin\" } ] } Links Link to GitHub: https://github.com/felixscherz/discover-plugins PyPI: https://pypi.org/project/discover-plugins/ Let me know if the tool helped you out! Cheers!"},
{"Title": "Is anyone here looking for a developer to contribute to your personal projects?", "Author": "u/homelander_30", "Content": "I guess the title explains it all, I'm looking for some personal projects to work or contribute on and would be really helpful if anyone is looking for a dev. I did look upon some open-source projects but they were too advanced and out of scope for me so I just wanna start small and learn."},
{"Title": "I made a little Python quiz for interns and new Python developers at my company", "Author": "u/Penny-loafers", "Content": "I put this quiz together to help create conversation for interns and new python developers at my company. Its based on the content from one of my favourite books ( Fluent Python ). I hope you enjoy it! Quiz"},
{"Title": "Running Python in Web Browsers", "Author": "u/pdfisk", "Content": "Python is one of the world's most popular programming languages and the web is the most ubiquitous application platform. There are several projects which aim to enable Python to run in web browsers. Brython is an implementation of Python 3 written in JavaScript. Skulpt is an implementation of Python 2/3 written in JavaScript. PyScript is an implementation of Python 3 written in WebAssembly. Transcrypt is a Python to JavaScript compiler - unfortunately, the project seems to have been abandoned. Batavia is a Python virtual machine written in JavaScript - unfortunately, the project seems to have been abandoned. Finally, I have created VistaPython which is also intended to run Python 3 in web browsers but by using a bytecode interpreter written in JavaScript. Each design has strengths and weaknesses: Both Brython and Skulpt use hand-written Python parsers which are difficult to maintain. VistaPython uses a parser generator, Antlr , to automatically generate the JavaScript code for the parser. The parser can be updated to match the latest Python version by simply running a script. Also, both Brython and Skulpt generate JavaScript code which is then evaluated. In VistaPython, the compiler produces a \"code object\" which is then executed using the bytecode interpreter. The first approach will result in faster code whereas the second approach can be more flexible for code stepping, etc. PyScript is based on Pyodide which is a port of CPython to WebAssembly. PyScript can be upgraded the latest Python release by recompiling the latest CPython sources. Its main disadvantage is that it is very heavy to load and seems to run poorly on mobile devices. In VistaPython, the load profiles are: vm.js (Python virtual machine) 761kb Python parser 368 kb Mobile client GUI 2.4 Mb Desktop client GUI 2.9 Mb Compiled applications can be run using only the Python virtual machine (761kb). The design goal of VistaPython is to be able to load compiled applications from a database and run them quickly on any web device."},
{"Title": "cool tool made with python", "Author": "u/SuccessfulLiving757", "Content": "I made a cool tool with python named Cookie Monster (not the one from sesmene street). It fetches cookies from a website and it is kinda broken... you can install it from https://sojoyork.github.io ! And yes it is one of my best python projects! I hope you like it.! And the GitHub repository is https://github.com/sojoyork/sojoyork.github.io ! (also am new to reddit)"},
{"Title": "A simple website scraper script", "Author": "u/SAV_NC", "Content": "Web Scraper Script What My Project Does This project scrapes websites to extract and display titles and links of articles. It processes multiple websites in parallel, fetching and parsing content to provide a consolidated list of articles with their full URLs. Target Audience Home users, researchers, and web enthusiasts who need to gather information from multiple websites quickly and efficiently. Features Parallel Processing : Uses ThreadPoolExecutor to fetch multiple websites concurrently, speeding up the scraping process. Error Handling and Logging : Provides detailed logging for debugging and retry mechanisms for robustness. Full URL Extraction : Ensures that all extracted links are complete URLs, enhancing usability. Customizable Headers : Allows customization of HTTP headers to mimic different browsers. Script Overview The script consists of several key components: Fetching URLs The script fetches content from the given URLs using the requests library. It includes retry logic with exponential backoff to handle transient errors. Parsing Content The script uses BeautifulSoup to parse the fetched HTML content and extract article titles and links. It ensures that the links are converted to full URLs using urljoin . Concurrent Execution The script employs ThreadPoolExecutor to fetch and parse content from multiple websites in parallel, improving efficiency. Access the Script You can access the script on GitHub here: Web Scraper Script on GitHub How to Use Install Dependencies : Ensure you have requests and beautifulsoup4 installed: pip install requests beautifulsoup4 Run the Script : Provide the URLs of the websites you want to scrape as arguments: python3 web-scraper.py https://yahoo.com https://sports.yahoo.com Conclusion This web scraper script is designed to be robust, efficient, and easy to use. It handles multiple websites in parallel, provides detailed logging, and ensures full URL extraction for all links. Ideal for users who need to quickly gather and consolidate information from various sources."},
{"Title": "Open source Python projects with good software design that is worth studying", "Author": "u/bolt_runner", "Content": "What are some software projects written in python that are well-structured and use good code design practices that are worth spending time to study?"},
{"Title": "Log Monitoring with Kafka ETL using Python via Docker and Pathway", "Author": "u/muditjps", "Content": "Hi r/Python , This project is for a Streaming ETL problem statement for Fraud-detection/Log Monitoring use-case. Here's a link to the blog explainer: https://pathway.com/developers/templates/kafka-etl GitHub Repo link: https://github.com/pathwaycom/pathway/tree/main/examples/projects/kafka-ETL What the Project Does Let's say we're monitoring logs from servers in New York and Paris. The logs have different time zones so you need to unify these different time zones into a single format to maintain data integrity. Now, Kafka is a popular ETL tool but it's usable only in Java/Scala. Target Audience This is mostly for Python developers/data scientists/ML engineers and people who work on Fraud Detection or ETL. Comparison This project leverages Pathway, a Python ETL framework powered by an underlying Rust engine that surpasses Flink/Kafka in benchmarks. With this Pythonic framework we: Extract data streams from Kafka using built-in Kafka input connectors. Convert times with varying time zones into unified timestamps the datetime module. Load the final data stream back into Kafka. The entire script is available as an app template on the repo, which can be run via Docker in minutes. Open to your feedback/questions!"},
{"Title": "localslackirc - bridge slack and IRC", "Author": "u/sonobanana33", "Content": "I made a minor bugfix release of localslackirc https://codeberg.org/ltworf/localslackirc It can be installed via apt or ran from sources. No pypi package, sorry. What My Project Does After configuring it with a token from slack, it creates a local IRC server that bridges with slack. It supports threads, sending files. It doesn't support reactions. It supports muting @here notifications from certain users or certain channels. It allows to silently leave a channel, but rejoins it if the user is personally mentioned there. Target Audience Mostly people who have to use slack for work and would prefer IRC. Comparison I am not aware of a project doing the same thing. I know weechat has a slack plugin, but that's slightly different. I don't use weechat and I wanted to keep using my IRC client. out of date link to avoid the post from being removed: https://github.com/ltworf/localslackirc"},
{"Title": "Python community in Amsterdam, The Netherlands", "Author": "u/FuturesBrightDavid", "Content": "Hi, I'm trying to find a Python community in Amsterdam in The Netherlands.  There used to be an active MeetUp group and Slack , but there has been little to no activity on either in a long, long time. Pythonistas in my city, what social / networking events or activities are there around here? Additionally, would anyone be interested in reviving the Python MeetUps in Amsterdam?"},
{"Title": "Eventum: Flexible event generator", "Author": "u/rnv812", "Content": "Hi, recently I created event generator in Python called Eventum . Here is a link to website: https://eventum-generatives.github.io/Website/ And the main repo: https://github.com/Eventum-Generatives/EventumCore What My Project Does It can be used in task like: Generation of datasets Simulation of processes in real time Filling different systems with demo data Testing on arbitrary data and stress testing Target Audience This generating tool is mostly for developers and people who work with data. It is also very near to ELK stack, OpenSearch and SIEM systems like Splunk . But you can use it as you want :) Comparison There is a project Eventgen developed by Splunk, but Eventum has next advantages over it: More rich events scheduling Extended functionality in event templates More parametrizable configurations Has content developing tools (UI for visualization time distributions and rendering templates)"},
{"Title": "Robogram - Minimal Wrapper for Telegram Bot API in Python", "Author": "u/RitvikTheGod", "Content": "Guys, I recently released my first (in a while) open-source project wrapper on Telegram Bot API . I call it robogram and when I was developing in Python, I had a use case to send notifications from Raspberry Pi to my iPhone via Telegram . After searching online, I found no minimalist wrapper in Python 3+ to send messages via Bot API. So, I decided to create one :-) What My Project Does Minimal Wrapper around the Telegram Bot API. It's only dependency is requests in Python which is ubiquitous. It allows to retrieve info on Bot, or to send messages to users via personal chat, channel, or group. Target Audience Toy project I just came up with, after realizing no solution out there was best fit for me. But I have deployed this on production for personal project, and it's for sure production-ready. Target audience here would be other developers who are on Telegram and looking to leverage Bot API to facilitate the sending of messages or notifications to an audience on Telegram. Comparison Some packages out there are only async support, or only work on Python 2 (actually I found one with some popularity that doesn't work in Python 3+ at all), or are dependency- or code- heavy and can introduce code bloat, especially to small, personal projects. As someone working on a personal project myself, I wanted a lightweight solution that only used minimal dependencies such as requests for making API requests. So, since I could not find one out there in wild, I decided to create my own! -- Interested to get your thoughts, if anyone likes it I will be glad to feedback. https://github.com/rnag/robogram"},
{"Title": "Introducing Lambda Forge: Simplifying AWS Lambda Deployment and Development!", "Author": "u/No_Coffee_9879", "Content": "Hey everyone, I just wanted to share a project I've been working on called Lambda Forge . It's a tool designed to simplify the deployment and management of AWS Lambda functions. If you're like me and spend a lot of time working with serverless architecture, you might find it pretty useful. What My Project Does Lambda Forge helps you deploy and manage AWS Lambda functions with ease. One of its standout features is the WebSocket connection for hot reloading of local code. It uses MQTT over Websockets to proxy requests to a local server, making development seamless. No more redeploying code just to see if your changes work! Target Audience This project is meant for developers who work with AWS Lambda in both production and development environments. Whether you're a seasoned backend engineer or just getting started with serverless, Lambda Forge can help streamline your workflow. Comparison Compared to other deployment tools, Lambda Forge focuses on enhancing the development experience with hot reloading capabilities. Many existing tools require a full redeployment for changes to take effect, which can be time-consuming. Lambda Forge's WebSocket integration saves time by allowing you to see changes in real-time without redeployment. If you're interested, you can check out the documentation here: Lambda Forge Docs And if you want to dive into the code or contribute, here's the GitHub repo: Lambda Forge on GitHub I‚Äôd love to hear your thoughts and feedback."},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Mirascope-Python's Alternative To Langchain", "Author": "u/Active-Fuel-49", "Content": "Mirascope is a Python library that lets you access a range of Large Language Models, but in a more straightforward and Pythonic way. https://www.i-programmer.info/news/90-tools/17275-mirascope-pythons-alternative-to-langchain.html"},
{"Title": "Python on Cloud GPUs", "Author": "u/jrbourbeau", "Content": "Hi All, I wanted to try to speed up some Python code with a GPU recently and was pretty shocked at how difficult it is to properly set up and configure things. I have lots of experience in the PyData space, but am definitely not a cloud devops expert. So some colleagues and I wrote a decorator that automatically sets up a cloud VM, runs the decorated function, and returns the function‚Äôs result back locally to my laptop. Here‚Äôs an example that trains a PyTorch model on an NVIDIA A10 GPU on AWS. The Coiled Function API is nice because I didn‚Äôt have to build any Docker images, muck around in the AWS console, or do anything special to set up a cloud GPU. That said, there are definitely tradeoffs here. We optimized for privacy and no standing costs, so it takes ~1-2 minutes to fully spin up VMs (no warm pool of VMs waiting). We also only run on AWS/GCP/Azure so this doesn‚Äôt help with on-prem workloads. I‚Äôm definitely biased as I work at Coiled, but think this is a simple way to run Python on cloud hardware (especially for folks without a lot of cloud experience). I‚Äôm curious to hear from folks here. Do you have a favorite way to run Python code on the cloud? What do you like about your current setup? For example, ergonomics, performance, something else?"},
{"Title": "Web scraper for protein prices", "Author": "u/Knockoutpie1", "Content": "Hey everyone, looking for some input. For work I‚Äôve worked on web scraping for prices to see if my components are adequately priced on the internet compared to competitors. I can use this for protein prices as a personal project. I have experience now with Beautiful Soup, Selenium, and Excel Power BI. What route should I go? Should I only pull pricing from Amazon? Or should I do Amazon and the manufacturer site to see which is better pricing? Ideas would be great. Should be a fun project. If I go with beautiful soup, there‚Äôs no UI and I can print all to terminal If I use selenium, I can use UC to pass anti-bot measures and also print to terminal, but it will open a browser window for each price scrape. If I use excel power BI, I‚Äôll just load data to a worksheet and pricing will update at the price of a button."},
{"Title": "Textual Serve - Serve TUIs in the browser", "Author": "u/willm", "Content": "Textual Serve ( https://github.com/Textualize/textual-serve ) is a project which serves TUIs (built with Textual) in the browser. This is self-hosted, so you don't need to rely on any external service."},
{"Title": "Python Project Management Primer", "Author": "u/Martynoas", "Content": "This article explores how to manage Python project environments and dependencies, as well as how to structure projects effectively."},
{"Title": "Python on ARM laptops", "Author": "u/Fun-Asparagus-837", "Content": "Hi there ! I'm thinking about buying an ARM windows laptop with the new Qualcomm chips. They will replace the x86 so I was wondering : Will There be a massive risk of non-compatibility of Python packages ? I guess they are made for x86 but I don't know if it's possible to work with them with an ARM based CPU. Edit : Had a great deal on the ideapad pro 5 gen 9 so I went for it. Glad to have these incredible specs and decided to rely on x86 chip for the moment, because I wanted to avoid all the early-adoption problems"},
{"Title": "Video Quality Ranker", "Author": "u/SAV_NC", "Content": "What my project does Ranks videos based on overall quality. Takes into account multiple metrics to determine what quality is the best. Target audience Home users / Video enthusiasts Comparison This project uses the following metrics to rank videos: Resolution : Higher resolution videos are preferred. Frame Rate : Videos with higher frame rates are ranked higher. Bitrate : Higher bitrate often indicates better quality. Codec : Some codecs provide better quality than others at the same bitrate. The script extracts these metrics using ffprobe from the FFmpeg suite and sorts the videos accordingly. Here's how the metrics are used: Resolution : The script first compares the resolution (width x height) of the videos. Higher resolutions are ranked higher. Frame Rate : If two videos have the same resolution, the one with the higher frame rate is ranked higher. Bitrate : For videos with the same resolution and frame rate, the bitrate is used to determine the quality. Codec : In case of a tie in all other metrics, the codec is considered to break the tie. Access the Script You can access the script on GitHub here"},
{"Title": "A JIT compiler for CPython", "Author": "u/lutipri", "Content": "Brandt Bucher talks on JIT compiler for Python at CPython Core Developer Sprint. Brandt is  a member of the Faster CPython project , which is working on making the reference implementation of the language faster via a variety of techniques. https://www.youtube.com/watch?v=HxSHIpEQRjs"},
{"Title": "Conway's game of life. can you find an optimization?", "Author": "u/Significant_Water_28", "Content": "little challenge for you, how fast can this be pushed in python? This function takes a numpy.ndarray / 2d numpy array, and returns the updated array. iv updated this function several times, this i the fastiest so far. numba jit dosn't like the double roll, and its faster than for loops in jit. def conways_game_of_life(board:numpy.ndarray): n_neighbour = sum(numpy.roll(numpy.roll(board, i, 0), j, 1) for i in (-1, 0, 1) for j in (-1, 0, 1) if (i != 0 or j != 0)) board[(n_neighbour<2) | (n_neighbour>3)] = 0 board[(n_neighbour==3)] = 1 return board"},
{"Title": "json3pdf : Batch OCR for high quality document archiving.", "Author": "u/DrumcanSmith", "Content": "What my project does Performs OCR on scanned Books using Microsoft Azure Document Intelligence read Target audience People who are unsatisfied with traditional OCR People who want to add clear text to the original PDF and not just extract the text. People who want to archive documents at best quality. Comparasion In my use case traditional OCR was near to useless. Tesseract was meh, Google API didn't process large files. Document Intelligence takes up to 500MB (although in practice a little less), and is possible to OCR 400-600 pages over books in batch by dividing and merging the source and results locally by only a few chunks. It doesn't provide the text in PDF form so that was my reason to start this project. Still in alpha and in separate modules and a lot of rigid coding, but it is working fine for my original task so thought maybe I'd showcase it. https://github.com/DesertDoggy/json3pdf"},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Experimental Python Wheels for Windows on ARM64", "Author": "u/Balance-", "Content": "For anyone on a (new) Windows on Arm system, I found this great repo with Arm64 Windows wheels: https://github.com/cgohlke/win_arm64-wheels Highlights 256 packages for Python 3.12 Built with numpy 2 if possible Scipy stack: numpy with OpenBLAS, scipy, matplotlib, Pandas, scikit-learn, scikit-image, numba, etc. GIS stack: GDAL, netCDF4, pyproj, Shapely, rasterio, basemap, Fiona, etc. Image IO: Imagecodecs, Pillow, OpenImageIO, OpenEXR, pylibCZIrw, etc. Noteworthy: Pytorch, Kivy, opencv_python_headless, pymol-open-source, pywin32"},
{"Title": "Parsing Python ASTs 20x faster with Rust", "Author": "u/the1024", "Content": "https://www.gauge.sh/blog/parsing-python-asts-20x-faster-with-rust"},
{"Title": "Trying to find this package", "Author": "u/CompositePrime", "Content": "I should have saved the post but maybe 4-6 months ago I was reading a post (I am pretty sure it was in r/Python ) where someone created a package that creates a visual for data contained within a list. For example, let‚Äôs say I have a data frame where one of the columns is named ‚Äúcolors‚Äù and each record contains a list of colors. One record might be [black,blue,yellow] another record might have [blue,yellow,black]. The visual had two parts where the top was a column chart to show the frequency of the list combinations and below the column chart was more of a table that showed each ‚Äúcolor‚Äù as one column and then across the row for each color and under the columns from the chart above was an indicator of sorts that would be greyed out of the color for that row was not in the corresponding columns list and highlighted another color of it was. Anyways this is probably a long shot but either the package or the name of this visual would be super helpful. Thanks python community!"},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "I am Nominating Myself for PSF Board of Directors", "Author": "u/tavallaie", "Content": "I am nominating myself for the PSF Board of Directors! üåü Check out my latest blog post to learn more about my journey, my commitment to the Python community, and my application for an OFAC license to expand educational activities in restricted areas. Read more at my blog"},
{"Title": "Scrapegraph AI Tutorial; Scrape Websites Easily With LLaMA AI", "Author": "u/INSERT_KEYWORD", "Content": "I'm going to show you how to get Scrapegraph AI up and running, how to set up a language model, how to process JSON, scrape websites, use different AI models, and even turning your data into audio. Sounds like a lot, but it's easier than you think, and I'll walk you through it step by step. https://www.scrapingbee.com/blog/scrapegraph-ai-tutorial-scrape-websites-easily-with-llama-ai/"},
{"Title": "What are the hardware requirements in a laptop to run Python + Future AI based projects?", "Author": "u/Madlynik", "Content": "I will buy a laptop for coding purposes but just started learning and practising Python using Pyecharm. What are the software requirements that lead to hardware specs a general Python coder must look into? Please suggest the hardware setup within a pocket friendly budget."},
{"Title": "Looking for a good WYZIWIG/visual editor to go with with Jinja + Weasyprint", "Author": "u/Benoss", "Content": "End goal is to produce PDF using external data and a template. Needs to support Jinja tags, conditionals and loops. Using https://github.com/Kozea/WeasyPrint and https://github.com/pallets/jinja as base stack (Open to other suggestions) I was thinking of building some base HTML templates but would be awesome if I could find a visual HTML editor that could produce code 100% compatible with Weasyprint so that end users can build templates by themselves or modify existing ones. Could be Wysiwyg based using https://editorjs.io or https://github.com/slab/quill or more advanced web builders like https://github.com/GrapesJS/grapesjs Anybody built something similar?"},
{"Title": "NumPy 2.0.0 is the first major release since 2006.", "Author": "u/commandlineluser", "Content": "NumPy 2.0.0 is the first major release since 2006. https://github.com/numpy/numpy/releases/tag/v2.0.0 https://numpy.org/devdocs/release/2.0.0-notes.html https://numpy.org/devdocs/numpy_2_0_migration_guide.html"},
{"Title": "Load Tests Python Task Queues", "Author": "u/tuple32", "Content": "What My Project Does While looking for task queues, I found that there are many options available in the Python ecosystem, making it really hard to choose the right one. To get a sense of how each library performs and to help make an informed decision, I conducted a load test on some of the most popular ones: Python-RQ, ARQ, Celery, Huey, and Dramatiq. Target Audience I hope my findings can help those who are also looking for a task queue solution in Python. Comparison Most articles out there seem to focus on comparing the features of these libraries but rarely discuss performance. While there could be a lot of improvements on my tests, I think it still provide some different insights into how each library handles heavy loads and concurrency. Links: You can read  my findings on my blog Check out the source code: on Github Thanks"},
{"Title": "Aurora: An extensible Python static site generator", "Author": "u/zerojames_", "Content": "What My Project Does Aurora is a fast, extensible Python static site generator. With Aurora, I can generate my personal website (~1,700 files, with multiple layers of jinja2 templates for each page) in < 4 seconds. Aurora generated 292,884 pages from a Hacker News post dataset in 2m:20s. Aurora supports incremental static regeneration, where pages can be regenerated in under 400ms, with hot reloading. I documented how this works on my blog . Target Audience I'm building Aurora to help me run my website, but it is built to be general so you can use it for your own projects. I would love feedback! I want this to be a tool for running static sites in production, at scale. Comparison Aurora is inspired by the folder structure of Jekyll, but is written in Python. It has a hooks API that lets you define custom Python functions that manipulate the state of a page. This allows you to implement custom behaviours in isolation of the engine itself. I use this to open link previews from a cache that I plan to use on my website, among other things."},
{"Title": "Suggestion: make ray.io a part of Python's std lib", "Author": "u/jmakov", "Content": "Imagine having the option to write code once and run on multiple cores or on the cluster as part of the std lib. I know there's a company (currently) behind it - Anyscale, also not sure what the license is but other than that, what's holding the Py community back?"},
{"Title": "Advise on choosing UI technology with Python", "Author": "u/green9cactus", "Content": "I am new to python and currently working on simple 3 layer web application - frontend - ? backend API to fetch data from DB - python DB - cloud This application has main intention to fetch data from DB, display graphs , table format data etc.  also perform some combination analysis of data and show on UI. Which less complex and stable technology I should prefer for frontend ? python flask, Bulma, Mesop by google or any other ? Thank you."},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Ruff: A Modern Python Linter for Error-Free and Maintainable Code", "Author": "u/ajpinedam", "Content": "Linting is essential to writing clean and readable code to share with others. A linter, like Ruff, is a tool that analyzes your code and looks for errors, stylistic issues, and suspicious constructs. Linting allows you to address issues and improve your code quality before you commit your code and share it with others. Ruff is a modern linter that‚Äôs extremely fast and has a simple interface, making it straightforward to use. It also aims to be a drop-in replacement for many other linting and formatting tools, such as Flake8, isort, and Black. It‚Äôs quickly becoming one of the most popular Python linters. Installing Ruff Now that you know why linting your code is important and how Ruff is a powerful tool for the job, it‚Äôs time to install it. Thankfully, Ruff works out of the box, so no complicated installation instructions or configurations are needed to start using it. Assuming your project is already set up with a virtual environment, you can install Ruff in the following ways: ```bash $ python -m pip install ruff ``` You can check that Ruff installed correctly by using the ruff version command: ```bash $ ruff version ruff 0.4.7 ``` Linting Your Python Code While linting helps keep your code consistent and error-free, it doesn‚Äôt guarantee that your code will be bug-free. Finding the bugs in your code is best handled with a debugger and adequate testing, which won‚Äôt be covered in this tutorial. Coming up in the next sections, you‚Äôll learn how to use Ruff to check for errors and speed up your workflow. Checking for Errors ```bash $ ruff check one_ring.py:1:8: F401 [*] `os` imported but unused one_ring.py:10:12: F821 Undefined name `name` Found 2 errors. [*] 1 fixable with the `--fix` option. ``` Success! Ruff found two errors. Not only does it show the file and line numbers of the errors, but it also gives you error codes and messages. In addition, it lets you know that one of the two errors is fixable. Great! You can tell Ruff to fix errors by applying the --fix flag. Here‚Äôs what happens when you follow its suggestion: ```bash $ ruff check --fix one_ring.py:9:12: F821 Undefined name `name` Found 2 errors (1 fixed, 1 remaining). ``` You can find the rest of this Free tutorial here"},
{"Title": "Advice for creating 3D modelling program", "Author": "u/Latter-History-8053", "Content": "I am creating a Python program which models 3D shapes so that they can be saved and or interacted with (i.e. rotated). The process currently takes a while to render shapes consisting of multiple materials. The libraries being implemented are currently matplotlib and numpy. What would you advise for improving the rendering process (library choice etc)?"},
{"Title": "pieshell: python for shell scripting and as an interactive shell", "Author": "u/Severe_Inflation5326", "Content": "Pieshell is a Python shell environment that combines the expressiveness of shell pipelines with the power of python iterators. It can be used in two major ways: As an interactive shell replacing e.g. bash As an ordinary python module replacing e.g. subprocess.Popen Obligatory example: 140:/home/oven/pieshell >>> for x in ls(-a) | tr(\"s\", \"S\"): ...   if x.endswith('.py'): ...      print x ... Setup.py Source code: https://github.com/redhog/pieshell What the project does It's a replacement for the subprocess module, and for bash as an interactive shell, and makes interacting with shell pipelines easier. Target Audience System administrators, system software developers, data scientists Comparison While os.system is very limited but easy to use, subprocess.Popen offers a lot of flexibility, but the interface is very low level. Any actual pipelining of multiple programs is pretty much required to be done by e.g. a bash process, constructing the pipeline as a shell script string. Further, interacting with standard in and standard out requires careful IO handling. Pieshell on the other hand lets you construct pipelines as python objects. Standard io from a pipeline can be handled using iterators or async iterators. Pieshell has full asyncio integration."},
{"Title": "How does Python earn money? What would have been their business model?", "Author": "u/Civil-Captain5676", "Content": "I was wondering recently about any startup and any coding language that how does they make money. So I was curious to know about Python which is widely used"},
{"Title": "I created a script to automatically patch revanced", "Author": "u/ltlbwu", "Content": "What My Project Does AutoReVanced is a Python script that automates downloading and patching APKs using ReVanced patches from ApkPure. It's perfect for anyone wanting to patch their revanced app. Target Audience Suitable for a fun side project or hobbyists, AutoReVanced is designed for anyone wanting to customize Android apps with ReVanced patches. Comparison Unlike alternatives, AutoReVanced is automatic. GitHub: autorevanced"},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Python 3.12 docs include built-in support for themes, including a dark theme!", "Author": "u/bleeddonor", "Content": "Python gives you wings, yes, but you used to have to wear aviator glasses to get through the docs on a bright display. No more. :)"},
{"Title": "Linting Python Monorepo with Bazel and Ruff", "Author": "u/Spindelkryp", "Content": "Heya, I have recently integrated Ruff in the Bazel monorepo of my company. The results were quite impressive, it takes around ~100ms to analyze and apply format / lint results to 1.1k python files. Integration with Bazel, however, was not exactly painless so I wrote a small guide for it as well as an example project. . Hope it helps someone! What My Project Does Guide on how to setup Ruff linting for Bazel based Python projects Target Audience Maintainers of large Python repos Source code How-to guide Source code"},
{"Title": "My library VidGear `v0.3.3` - brings libcamera API Support to python.", "Author": "u/abhi_uno", "Content": "Hello Python developers! I'm excited to announce the release of VidGear v0.3.3 , which brings official support for the libcamera backend in its PiGear API ! This update enhances the capabilities of Raspberry Pi Camera Modules and provides limited USB camera support. More about PiGear: PiGear is a specialized API optimized for Raspberry Pi üçá Boards, offering comprehensive support for camera modules (e.g., OmniVision OV5647 Camera Module, Sony IMX219 Camera Module) and limited compatibility for USB cameras. PiGear implements a seamless and robust wrapper around the Picamera2 Python library, simplifying integration with minimal code changes and ensuring a smooth transition for developers already familiar with the Picamera2 API. PiGear leverages the libcamera API under the hood with multi-threading, providing high-performance üî•, enhanced control, and functionality for Raspberry Pi camera modules. PiGear handles common configuration parameters and non-standard settings for various camera types, simplifying the integration process. PiGear currently supports PiCamera2 API parameters such as sensor, controls, transform, and format, with internal type and sanity checks for robust performance. While primarily focused on Raspberry Pi camera modules, PiGear also provides basic functionality for USB webcams (only with Picamera2 API), along with the ability to accurately differentiate between USB and Raspberry Pi cameras using metadata. PiGear seamlessly switches to the legacy picamera library if the Picamera2 library is unavailable, ensuring seamless backward compatibility. PiGear also provides a flexible multi-threaded framework around the complete picamera API, allowing developers to effortlessly exploit a wide range of parameters, such as brightness, saturation, sensor_mode, iso, exposure, and more. Furthermore, PiGear supports the use of multiple camera modules, including those found on Raspberry Pi Compute Module IO boards and USB cameras (only with Picamera2 API). We're eager to see the innovative projects you'll create with PiGear! For more details and to get started, check out our GitHub repository . Happy coding! Feel free to ask any questions or share your feedback below. Let's discuss and innovate together! üöÄ"},
{"Title": "How to add chart in django tinymce text field", "Author": "u/Funny-Crab-9495", "Content": "i want to add chart in django blog so I'm using tinymce for blog content can add line graph ,bar graph inside blog. Im trying to add plugin in static file  but it doesn't work."},
{"Title": "How to detect (modified|headless) Chrome instrumented with Selenium (2024 edition)", "Author": "u/antvas", "Content": "https://deviceandbrowserinfo.com/learning_zone/articles/detecting-headless-chrome-selenium-2024 TL;DR The 4 techniques are the following: Using the user agent HTTP headers or with navigator.userAgent in JS to detect user agents linked to Headless Chrome: is Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/126.0.6478.114 Safari/537.36 Similarly, by detecting the presence of the HeadlessChrome substring in the sec-ch-ua header By detecting if navigator.webdriver = true in JavaScript By detecting the side effects of CDP (Chrome DevTools Protocol) (detailed in the article)"},
{"Title": "I made a simple web analytics app using Fast API", "Author": "u/sagunsh", "Content": "What My Project Does It s a simple web analytics app that tracks visitors in your website. I am using ip-api.com for geolocation and uap-python library for parsing user agent. At the moment, it simply logs these information to a sqlite database. Target Audience Anyone who is learning Python and more specifically Fast API and want to try building some projects. The code is available on github so feel free to explore, add features. Comparision Having used Django and Flask in the past, I made this project as a way to learn and to explore about Fast API. For a more sophisticated/paid alternative, check out Google Analytics, Plausible, etc. Source Code https://github.com/sagunsh/webanalytics"},
{"Title": "discover-plugins - track which plugins are installed into your python environment", "Author": "u/fesch2", "Content": "What my project does discover-plugins is a simple CLI tool that lets you list and filter plugins entrypoints installed into a python environment. I recently had to track down a bug that I ultimately resulted from a plugin I had installed into my environment.  To my surprise, there was no easy way to list which kind of plugins were installed, so I decided to build my own tool. Target Audience discover-plugins is intended as a debugging tool for those times when you are not quite sure which plugins are currently part of your python environment. Installation pipx install discover-plugins Usage Find all installed plugins related to pytest (the relevant group name is pytest11 ): discover-plugins --group pytest11 The tool defaults to use the first python interpreter on your path, you can optionally specify which interpreter to use with the --interpreter flag. The output will list all entrypoints belonging to the pytest11 group. For example, if you had installed a single pytest plugin ( pytest-aws-apigateway ) the output would look like this: { \"pytest11\": [ { \"name\": \"pytest_httpx\", \"group\": \"pytest11\", \"value\": \"pytest_httpx\" }, { \"name\": \"anyio\", \"group\": \"pytest11\", \"value\": \"anyio.pytest_plugin\" }, { \"name\": \"pytest-aws-apigateway\", \"group\": \"pytest11\", \"value\": \"pytest_aws_apigateway.plugin\" } ] } Links Link to GitHub: https://github.com/felixscherz/discover-plugins PyPI: https://pypi.org/project/discover-plugins/ Let me know if the tool helped you out! Cheers!"},
{"Title": "Is anyone here looking for a developer to contribute to your personal projects?", "Author": "u/homelander_30", "Content": "I guess the title explains it all, I'm looking for some personal projects to work or contribute on and would be really helpful if anyone is looking for a dev. I did look upon some open-source projects but they were too advanced and out of scope for me so I just wanna start small and learn."},
{"Title": "I made a little Python quiz for interns and new Python developers at my company", "Author": "u/Penny-loafers", "Content": "I put this quiz together to help create conversation for interns and new python developers at my company. Its based on the content from one of my favourite books ( Fluent Python ). I hope you enjoy it! Quiz"},
{"Title": "Running Python in Web Browsers", "Author": "u/pdfisk", "Content": "Python is one of the world's most popular programming languages and the web is the most ubiquitous application platform. There are several projects which aim to enable Python to run in web browsers. Brython is an implementation of Python 3 written in JavaScript. Skulpt is an implementation of Python 2/3 written in JavaScript. PyScript is an implementation of Python 3 written in WebAssembly. Transcrypt is a Python to JavaScript compiler - unfortunately, the project seems to have been abandoned. Batavia is a Python virtual machine written in JavaScript - unfortunately, the project seems to have been abandoned. Finally, I have created VistaPython which is also intended to run Python 3 in web browsers but by using a bytecode interpreter written in JavaScript. Each design has strengths and weaknesses: Both Brython and Skulpt use hand-written Python parsers which are difficult to maintain. VistaPython uses a parser generator, Antlr , to automatically generate the JavaScript code for the parser. The parser can be updated to match the latest Python version by simply running a script. Also, both Brython and Skulpt generate JavaScript code which is then evaluated. In VistaPython, the compiler produces a \"code object\" which is then executed using the bytecode interpreter. The first approach will result in faster code whereas the second approach can be more flexible for code stepping, etc. PyScript is based on Pyodide which is a port of CPython to WebAssembly. PyScript can be upgraded the latest Python release by recompiling the latest CPython sources. Its main disadvantage is that it is very heavy to load and seems to run poorly on mobile devices. In VistaPython, the load profiles are: vm.js (Python virtual machine) 761kb Python parser 368 kb Mobile client GUI 2.4 Mb Desktop client GUI 2.9 Mb Compiled applications can be run using only the Python virtual machine (761kb). The design goal of VistaPython is to be able to load compiled applications from a database and run them quickly on any web device."},
{"Title": "cool tool made with python", "Author": "u/SuccessfulLiving757", "Content": "I made a cool tool with python named Cookie Monster (not the one from sesmene street). It fetches cookies from a website and it is kinda broken... you can install it from https://sojoyork.github.io ! And yes it is one of my best python projects! I hope you like it.! And the GitHub repository is https://github.com/sojoyork/sojoyork.github.io ! (also am new to reddit)"},
{"Title": "A simple website scraper script", "Author": "u/SAV_NC", "Content": "Web Scraper Script What My Project Does This project scrapes websites to extract and display titles and links of articles. It processes multiple websites in parallel, fetching and parsing content to provide a consolidated list of articles with their full URLs. Target Audience Home users, researchers, and web enthusiasts who need to gather information from multiple websites quickly and efficiently. Features Parallel Processing : Uses ThreadPoolExecutor to fetch multiple websites concurrently, speeding up the scraping process. Error Handling and Logging : Provides detailed logging for debugging and retry mechanisms for robustness. Full URL Extraction : Ensures that all extracted links are complete URLs, enhancing usability. Customizable Headers : Allows customization of HTTP headers to mimic different browsers. Script Overview The script consists of several key components: Fetching URLs The script fetches content from the given URLs using the requests library. It includes retry logic with exponential backoff to handle transient errors. Parsing Content The script uses BeautifulSoup to parse the fetched HTML content and extract article titles and links. It ensures that the links are converted to full URLs using urljoin . Concurrent Execution The script employs ThreadPoolExecutor to fetch and parse content from multiple websites in parallel, improving efficiency. Access the Script You can access the script on GitHub here: Web Scraper Script on GitHub How to Use Install Dependencies : Ensure you have requests and beautifulsoup4 installed: pip install requests beautifulsoup4 Run the Script : Provide the URLs of the websites you want to scrape as arguments: python3 web-scraper.py https://yahoo.com https://sports.yahoo.com Conclusion This web scraper script is designed to be robust, efficient, and easy to use. It handles multiple websites in parallel, provides detailed logging, and ensures full URL extraction for all links. Ideal for users who need to quickly gather and consolidate information from various sources."},
{"Title": "Open source Python projects with good software design that is worth studying", "Author": "u/bolt_runner", "Content": "What are some software projects written in python that are well-structured and use good code design practices that are worth spending time to study?"},
{"Title": "Log Monitoring with Kafka ETL using Python via Docker and Pathway", "Author": "u/muditjps", "Content": "Hi r/Python , This project is for a Streaming ETL problem statement for Fraud-detection/Log Monitoring use-case. Here's a link to the blog explainer: https://pathway.com/developers/templates/kafka-etl GitHub Repo link: https://github.com/pathwaycom/pathway/tree/main/examples/projects/kafka-ETL What the Project Does Let's say we're monitoring logs from servers in New York and Paris. The logs have different time zones so you need to unify these different time zones into a single format to maintain data integrity. Now, Kafka is a popular ETL tool but it's usable only in Java/Scala. Target Audience This is mostly for Python developers/data scientists/ML engineers and people who work on Fraud Detection or ETL. Comparison This project leverages Pathway, a Python ETL framework powered by an underlying Rust engine that surpasses Flink/Kafka in benchmarks. With this Pythonic framework we: Extract data streams from Kafka using built-in Kafka input connectors. Convert times with varying time zones into unified timestamps the datetime module. Load the final data stream back into Kafka. The entire script is available as an app template on the repo, which can be run via Docker in minutes. Open to your feedback/questions!"},
{"Title": "localslackirc - bridge slack and IRC", "Author": "u/sonobanana33", "Content": "I made a minor bugfix release of localslackirc https://codeberg.org/ltworf/localslackirc It can be installed via apt or ran from sources. No pypi package, sorry. What My Project Does After configuring it with a token from slack, it creates a local IRC server that bridges with slack. It supports threads, sending files. It doesn't support reactions. It supports muting @here notifications from certain users or certain channels. It allows to silently leave a channel, but rejoins it if the user is personally mentioned there. Target Audience Mostly people who have to use slack for work and would prefer IRC. Comparison I am not aware of a project doing the same thing. I know weechat has a slack plugin, but that's slightly different. I don't use weechat and I wanted to keep using my IRC client. out of date link to avoid the post from being removed: https://github.com/ltworf/localslackirc"},
{"Title": "Python community in Amsterdam, The Netherlands", "Author": "u/FuturesBrightDavid", "Content": "Hi, I'm trying to find a Python community in Amsterdam in The Netherlands.  There used to be an active MeetUp group and Slack , but there has been little to no activity on either in a long, long time. Pythonistas in my city, what social / networking events or activities are there around here? Additionally, would anyone be interested in reviving the Python MeetUps in Amsterdam?"},
{"Title": "Eventum: Flexible event generator", "Author": "u/rnv812", "Content": "Hi, recently I created event generator in Python called Eventum . Here is a link to website: https://eventum-generatives.github.io/Website/ And the main repo: https://github.com/Eventum-Generatives/EventumCore What My Project Does It can be used in task like: Generation of datasets Simulation of processes in real time Filling different systems with demo data Testing on arbitrary data and stress testing Target Audience This generating tool is mostly for developers and people who work with data. It is also very near to ELK stack, OpenSearch and SIEM systems like Splunk . But you can use it as you want :) Comparison There is a project Eventgen developed by Splunk, but Eventum has next advantages over it: More rich events scheduling Extended functionality in event templates More parametrizable configurations Has content developing tools (UI for visualization time distributions and rendering templates)"},
{"Title": "Robogram - Minimal Wrapper for Telegram Bot API in Python", "Author": "u/RitvikTheGod", "Content": "Guys, I recently released my first (in a while) open-source project wrapper on Telegram Bot API . I call it robogram and when I was developing in Python, I had a use case to send notifications from Raspberry Pi to my iPhone via Telegram . After searching online, I found no minimalist wrapper in Python 3+ to send messages via Bot API. So, I decided to create one :-) What My Project Does Minimal Wrapper around the Telegram Bot API. It's only dependency is requests in Python which is ubiquitous. It allows to retrieve info on Bot, or to send messages to users via personal chat, channel, or group. Target Audience Toy project I just came up with, after realizing no solution out there was best fit for me. But I have deployed this on production for personal project, and it's for sure production-ready. Target audience here would be other developers who are on Telegram and looking to leverage Bot API to facilitate the sending of messages or notifications to an audience on Telegram. Comparison Some packages out there are only async support, or only work on Python 2 (actually I found one with some popularity that doesn't work in Python 3+ at all), or are dependency- or code- heavy and can introduce code bloat, especially to small, personal projects. As someone working on a personal project myself, I wanted a lightweight solution that only used minimal dependencies such as requests for making API requests. So, since I could not find one out there in wild, I decided to create my own! -- Interested to get your thoughts, if anyone likes it I will be glad to feedback. https://github.com/rnag/robogram"},
{"Title": "Introducing Lambda Forge: Simplifying AWS Lambda Deployment and Development!", "Author": "u/No_Coffee_9879", "Content": "Hey everyone, I just wanted to share a project I've been working on called Lambda Forge . It's a tool designed to simplify the deployment and management of AWS Lambda functions. If you're like me and spend a lot of time working with serverless architecture, you might find it pretty useful. What My Project Does Lambda Forge helps you deploy and manage AWS Lambda functions with ease. One of its standout features is the WebSocket connection for hot reloading of local code. It uses MQTT over Websockets to proxy requests to a local server, making development seamless. No more redeploying code just to see if your changes work! Target Audience This project is meant for developers who work with AWS Lambda in both production and development environments. Whether you're a seasoned backend engineer or just getting started with serverless, Lambda Forge can help streamline your workflow. Comparison Compared to other deployment tools, Lambda Forge focuses on enhancing the development experience with hot reloading capabilities. Many existing tools require a full redeployment for changes to take effect, which can be time-consuming. Lambda Forge's WebSocket integration saves time by allowing you to see changes in real-time without redeployment. If you're interested, you can check out the documentation here: Lambda Forge Docs And if you want to dive into the code or contribute, here's the GitHub repo: Lambda Forge on GitHub I‚Äôd love to hear your thoughts and feedback."},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Mirascope-Python's Alternative To Langchain", "Author": "u/Active-Fuel-49", "Content": "Mirascope is a Python library that lets you access a range of Large Language Models, but in a more straightforward and Pythonic way. https://www.i-programmer.info/news/90-tools/17275-mirascope-pythons-alternative-to-langchain.html"},
{"Title": "Python on Cloud GPUs", "Author": "u/jrbourbeau", "Content": "Hi All, I wanted to try to speed up some Python code with a GPU recently and was pretty shocked at how difficult it is to properly set up and configure things. I have lots of experience in the PyData space, but am definitely not a cloud devops expert. So some colleagues and I wrote a decorator that automatically sets up a cloud VM, runs the decorated function, and returns the function‚Äôs result back locally to my laptop. Here‚Äôs an example that trains a PyTorch model on an NVIDIA A10 GPU on AWS. The Coiled Function API is nice because I didn‚Äôt have to build any Docker images, muck around in the AWS console, or do anything special to set up a cloud GPU. That said, there are definitely tradeoffs here. We optimized for privacy and no standing costs, so it takes ~1-2 minutes to fully spin up VMs (no warm pool of VMs waiting). We also only run on AWS/GCP/Azure so this doesn‚Äôt help with on-prem workloads. I‚Äôm definitely biased as I work at Coiled, but think this is a simple way to run Python on cloud hardware (especially for folks without a lot of cloud experience). I‚Äôm curious to hear from folks here. Do you have a favorite way to run Python code on the cloud? What do you like about your current setup? For example, ergonomics, performance, something else?"},
{"Title": "Web scraper for protein prices", "Author": "u/Knockoutpie1", "Content": "Hey everyone, looking for some input. For work I‚Äôve worked on web scraping for prices to see if my components are adequately priced on the internet compared to competitors. I can use this for protein prices as a personal project. I have experience now with Beautiful Soup, Selenium, and Excel Power BI. What route should I go? Should I only pull pricing from Amazon? Or should I do Amazon and the manufacturer site to see which is better pricing? Ideas would be great. Should be a fun project. If I go with beautiful soup, there‚Äôs no UI and I can print all to terminal If I use selenium, I can use UC to pass anti-bot measures and also print to terminal, but it will open a browser window for each price scrape. If I use excel power BI, I‚Äôll just load data to a worksheet and pricing will update at the price of a button."},
{"Title": "Textual Serve - Serve TUIs in the browser", "Author": "u/willm", "Content": "Textual Serve ( https://github.com/Textualize/textual-serve ) is a project which serves TUIs (built with Textual) in the browser. This is self-hosted, so you don't need to rely on any external service."},
{"Title": "Python Project Management Primer", "Author": "u/Martynoas", "Content": "This article explores how to manage Python project environments and dependencies, as well as how to structure projects effectively."},
{"Title": "Python on ARM laptops", "Author": "u/Fun-Asparagus-837", "Content": "Hi there ! I'm thinking about buying an ARM windows laptop with the new Qualcomm chips. They will replace the x86 so I was wondering : Will There be a massive risk of non-compatibility of Python packages ? I guess they are made for x86 but I don't know if it's possible to work with them with an ARM based CPU. Edit : Had a great deal on the ideapad pro 5 gen 9 so I went for it. Glad to have these incredible specs and decided to rely on x86 chip for the moment, because I wanted to avoid all the early-adoption problems"},
{"Title": "Video Quality Ranker", "Author": "u/SAV_NC", "Content": "What my project does Ranks videos based on overall quality. Takes into account multiple metrics to determine what quality is the best. Target audience Home users / Video enthusiasts Comparison This project uses the following metrics to rank videos: Resolution : Higher resolution videos are preferred. Frame Rate : Videos with higher frame rates are ranked higher. Bitrate : Higher bitrate often indicates better quality. Codec : Some codecs provide better quality than others at the same bitrate. The script extracts these metrics using ffprobe from the FFmpeg suite and sorts the videos accordingly. Here's how the metrics are used: Resolution : The script first compares the resolution (width x height) of the videos. Higher resolutions are ranked higher. Frame Rate : If two videos have the same resolution, the one with the higher frame rate is ranked higher. Bitrate : For videos with the same resolution and frame rate, the bitrate is used to determine the quality. Codec : In case of a tie in all other metrics, the codec is considered to break the tie. Access the Script You can access the script on GitHub here"},
{"Title": "A JIT compiler for CPython", "Author": "u/lutipri", "Content": "Brandt Bucher talks on JIT compiler for Python at CPython Core Developer Sprint. Brandt is  a member of the Faster CPython project , which is working on making the reference implementation of the language faster via a variety of techniques. https://www.youtube.com/watch?v=HxSHIpEQRjs"},
{"Title": "Conway's game of life. can you find an optimization?", "Author": "u/Significant_Water_28", "Content": "little challenge for you, how fast can this be pushed in python? This function takes a numpy.ndarray / 2d numpy array, and returns the updated array. iv updated this function several times, this i the fastiest so far. numba jit dosn't like the double roll, and its faster than for loops in jit. def conways_game_of_life(board:numpy.ndarray): n_neighbour = sum(numpy.roll(numpy.roll(board, i, 0), j, 1) for i in (-1, 0, 1) for j in (-1, 0, 1) if (i != 0 or j != 0)) board[(n_neighbour<2) | (n_neighbour>3)] = 0 board[(n_neighbour==3)] = 1 return board"},
{"Title": "json3pdf : Batch OCR for high quality document archiving.", "Author": "u/DrumcanSmith", "Content": "What my project does Performs OCR on scanned Books using Microsoft Azure Document Intelligence read Target audience People who are unsatisfied with traditional OCR People who want to add clear text to the original PDF and not just extract the text. People who want to archive documents at best quality. Comparasion In my use case traditional OCR was near to useless. Tesseract was meh, Google API didn't process large files. Document Intelligence takes up to 500MB (although in practice a little less), and is possible to OCR 400-600 pages over books in batch by dividing and merging the source and results locally by only a few chunks. It doesn't provide the text in PDF form so that was my reason to start this project. Still in alpha and in separate modules and a lot of rigid coding, but it is working fine for my original task so thought maybe I'd showcase it. https://github.com/DesertDoggy/json3pdf"},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Experimental Python Wheels for Windows on ARM64", "Author": "u/Balance-", "Content": "For anyone on a (new) Windows on Arm system, I found this great repo with Arm64 Windows wheels: https://github.com/cgohlke/win_arm64-wheels Highlights 256 packages for Python 3.12 Built with numpy 2 if possible Scipy stack: numpy with OpenBLAS, scipy, matplotlib, Pandas, scikit-learn, scikit-image, numba, etc. GIS stack: GDAL, netCDF4, pyproj, Shapely, rasterio, basemap, Fiona, etc. Image IO: Imagecodecs, Pillow, OpenImageIO, OpenEXR, pylibCZIrw, etc. Noteworthy: Pytorch, Kivy, opencv_python_headless, pymol-open-source, pywin32"},
{"Title": "Parsing Python ASTs 20x faster with Rust", "Author": "u/the1024", "Content": "https://www.gauge.sh/blog/parsing-python-asts-20x-faster-with-rust"},
{"Title": "Trying to find this package", "Author": "u/CompositePrime", "Content": "I should have saved the post but maybe 4-6 months ago I was reading a post (I am pretty sure it was in r/Python ) where someone created a package that creates a visual for data contained within a list. For example, let‚Äôs say I have a data frame where one of the columns is named ‚Äúcolors‚Äù and each record contains a list of colors. One record might be [black,blue,yellow] another record might have [blue,yellow,black]. The visual had two parts where the top was a column chart to show the frequency of the list combinations and below the column chart was more of a table that showed each ‚Äúcolor‚Äù as one column and then across the row for each color and under the columns from the chart above was an indicator of sorts that would be greyed out of the color for that row was not in the corresponding columns list and highlighted another color of it was. Anyways this is probably a long shot but either the package or the name of this visual would be super helpful. Thanks python community!"},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "I am Nominating Myself for PSF Board of Directors", "Author": "u/tavallaie", "Content": "I am nominating myself for the PSF Board of Directors! üåü Check out my latest blog post to learn more about my journey, my commitment to the Python community, and my application for an OFAC license to expand educational activities in restricted areas. Read more at my blog"},
{"Title": "Scrapegraph AI Tutorial; Scrape Websites Easily With LLaMA AI", "Author": "u/INSERT_KEYWORD", "Content": "I'm going to show you how to get Scrapegraph AI up and running, how to set up a language model, how to process JSON, scrape websites, use different AI models, and even turning your data into audio. Sounds like a lot, but it's easier than you think, and I'll walk you through it step by step. https://www.scrapingbee.com/blog/scrapegraph-ai-tutorial-scrape-websites-easily-with-llama-ai/"},
{"Title": "What are the hardware requirements in a laptop to run Python + Future AI based projects?", "Author": "u/Madlynik", "Content": "I will buy a laptop for coding purposes but just started learning and practising Python using Pyecharm. What are the software requirements that lead to hardware specs a general Python coder must look into? Please suggest the hardware setup within a pocket friendly budget."},
{"Title": "Looking for a good WYZIWIG/visual editor to go with with Jinja + Weasyprint", "Author": "u/Benoss", "Content": "End goal is to produce PDF using external data and a template. Needs to support Jinja tags, conditionals and loops. Using https://github.com/Kozea/WeasyPrint and https://github.com/pallets/jinja as base stack (Open to other suggestions) I was thinking of building some base HTML templates but would be awesome if I could find a visual HTML editor that could produce code 100% compatible with Weasyprint so that end users can build templates by themselves or modify existing ones. Could be Wysiwyg based using https://editorjs.io or https://github.com/slab/quill or more advanced web builders like https://github.com/GrapesJS/grapesjs Anybody built something similar?"},
{"Title": "NumPy 2.0.0 is the first major release since 2006.", "Author": "u/commandlineluser", "Content": "NumPy 2.0.0 is the first major release since 2006. https://github.com/numpy/numpy/releases/tag/v2.0.0 https://numpy.org/devdocs/release/2.0.0-notes.html https://numpy.org/devdocs/numpy_2_0_migration_guide.html"},
{"Title": "Load Tests Python Task Queues", "Author": "u/tuple32", "Content": "What My Project Does While looking for task queues, I found that there are many options available in the Python ecosystem, making it really hard to choose the right one. To get a sense of how each library performs and to help make an informed decision, I conducted a load test on some of the most popular ones: Python-RQ, ARQ, Celery, Huey, and Dramatiq. Target Audience I hope my findings can help those who are also looking for a task queue solution in Python. Comparison Most articles out there seem to focus on comparing the features of these libraries but rarely discuss performance. While there could be a lot of improvements on my tests, I think it still provide some different insights into how each library handles heavy loads and concurrency. Links: You can read  my findings on my blog Check out the source code: on Github Thanks"},
{"Title": "Aurora: An extensible Python static site generator", "Author": "u/zerojames_", "Content": "What My Project Does Aurora is a fast, extensible Python static site generator. With Aurora, I can generate my personal website (~1,700 files, with multiple layers of jinja2 templates for each page) in < 4 seconds. Aurora generated 292,884 pages from a Hacker News post dataset in 2m:20s. Aurora supports incremental static regeneration, where pages can be regenerated in under 400ms, with hot reloading. I documented how this works on my blog . Target Audience I'm building Aurora to help me run my website, but it is built to be general so you can use it for your own projects. I would love feedback! I want this to be a tool for running static sites in production, at scale. Comparison Aurora is inspired by the folder structure of Jekyll, but is written in Python. It has a hooks API that lets you define custom Python functions that manipulate the state of a page. This allows you to implement custom behaviours in isolation of the engine itself. I use this to open link previews from a cache that I plan to use on my website, among other things."},
{"Title": "Suggestion: make ray.io a part of Python's std lib", "Author": "u/jmakov", "Content": "Imagine having the option to write code once and run on multiple cores or on the cluster as part of the std lib. I know there's a company (currently) behind it - Anyscale, also not sure what the license is but other than that, what's holding the Py community back?"},
{"Title": "Advise on choosing UI technology with Python", "Author": "u/green9cactus", "Content": "I am new to python and currently working on simple 3 layer web application - frontend - ? backend API to fetch data from DB - python DB - cloud This application has main intention to fetch data from DB, display graphs , table format data etc.  also perform some combination analysis of data and show on UI. Which less complex and stable technology I should prefer for frontend ? python flask, Bulma, Mesop by google or any other ? Thank you."},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Ruff: A Modern Python Linter for Error-Free and Maintainable Code", "Author": "u/ajpinedam", "Content": "Linting is essential to writing clean and readable code to share with others. A linter, like Ruff, is a tool that analyzes your code and looks for errors, stylistic issues, and suspicious constructs. Linting allows you to address issues and improve your code quality before you commit your code and share it with others. Ruff is a modern linter that‚Äôs extremely fast and has a simple interface, making it straightforward to use. It also aims to be a drop-in replacement for many other linting and formatting tools, such as Flake8, isort, and Black. It‚Äôs quickly becoming one of the most popular Python linters. Installing Ruff Now that you know why linting your code is important and how Ruff is a powerful tool for the job, it‚Äôs time to install it. Thankfully, Ruff works out of the box, so no complicated installation instructions or configurations are needed to start using it. Assuming your project is already set up with a virtual environment, you can install Ruff in the following ways: ```bash $ python -m pip install ruff ``` You can check that Ruff installed correctly by using the ruff version command: ```bash $ ruff version ruff 0.4.7 ``` Linting Your Python Code While linting helps keep your code consistent and error-free, it doesn‚Äôt guarantee that your code will be bug-free. Finding the bugs in your code is best handled with a debugger and adequate testing, which won‚Äôt be covered in this tutorial. Coming up in the next sections, you‚Äôll learn how to use Ruff to check for errors and speed up your workflow. Checking for Errors ```bash $ ruff check one_ring.py:1:8: F401 [*] `os` imported but unused one_ring.py:10:12: F821 Undefined name `name` Found 2 errors. [*] 1 fixable with the `--fix` option. ``` Success! Ruff found two errors. Not only does it show the file and line numbers of the errors, but it also gives you error codes and messages. In addition, it lets you know that one of the two errors is fixable. Great! You can tell Ruff to fix errors by applying the --fix flag. Here‚Äôs what happens when you follow its suggestion: ```bash $ ruff check --fix one_ring.py:9:12: F821 Undefined name `name` Found 2 errors (1 fixed, 1 remaining). ``` You can find the rest of this Free tutorial here"},
{"Title": "Advice for creating 3D modelling program", "Author": "u/Latter-History-8053", "Content": "I am creating a Python program which models 3D shapes so that they can be saved and or interacted with (i.e. rotated). The process currently takes a while to render shapes consisting of multiple materials. The libraries being implemented are currently matplotlib and numpy. What would you advise for improving the rendering process (library choice etc)?"},
{"Title": "pieshell: python for shell scripting and as an interactive shell", "Author": "u/Severe_Inflation5326", "Content": "Pieshell is a Python shell environment that combines the expressiveness of shell pipelines with the power of python iterators. It can be used in two major ways: As an interactive shell replacing e.g. bash As an ordinary python module replacing e.g. subprocess.Popen Obligatory example: 140:/home/oven/pieshell >>> for x in ls(-a) | tr(\"s\", \"S\"): ...   if x.endswith('.py'): ...      print x ... Setup.py Source code: https://github.com/redhog/pieshell What the project does It's a replacement for the subprocess module, and for bash as an interactive shell, and makes interacting with shell pipelines easier. Target Audience System administrators, system software developers, data scientists Comparison While os.system is very limited but easy to use, subprocess.Popen offers a lot of flexibility, but the interface is very low level. Any actual pipelining of multiple programs is pretty much required to be done by e.g. a bash process, constructing the pipeline as a shell script string. Further, interacting with standard in and standard out requires careful IO handling. Pieshell on the other hand lets you construct pipelines as python objects. Standard io from a pipeline can be handled using iterators or async iterators. Pieshell has full asyncio integration."},
{"Title": "How does Python earn money? What would have been their business model?", "Author": "u/Civil-Captain5676", "Content": "I was wondering recently about any startup and any coding language that how does they make money. So I was curious to know about Python which is widely used"},
{"Title": "I created a script to automatically patch revanced", "Author": "u/ltlbwu", "Content": "What My Project Does AutoReVanced is a Python script that automates downloading and patching APKs using ReVanced patches from ApkPure. It's perfect for anyone wanting to patch their revanced app. Target Audience Suitable for a fun side project or hobbyists, AutoReVanced is designed for anyone wanting to customize Android apps with ReVanced patches. Comparison Unlike alternatives, AutoReVanced is automatic. GitHub: autorevanced"},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "abstract-factories - a simple framework for content creation pipelines", "Author": "u/HistoricalCrow", "Content": "Hey all, my project abstract_factories is up to gauge interest and primarily feedback. The design goal is to make it easier to iterate on typical Content Creation pipeline tools (tool dev, rigging, validation, asset management etc) with a flexible framework to provide convenience, open and simple design and no dependencies (currently). It's an approach I've used a lot over the years and found it pretty versatile in production across numerous projects. Key features Auto-registration of matching items (types or instances) from any given path or python module. Simple or conditional item identifiers. Versioning. Recursive path searching (recursive module search in review). Dynamic resolving and importing modules in packaged (supports relative importing). Usage Examples There are a couple of simple examples given along with tests to cover all of the current features. What the project does It's a convenience package for creating scalable tools and frameworks using Abstract Factory design pattern. Target Audience Due to the solutions it's built for, it's aimed primarily at Technical Artists, Technical Animators, Pipeline and Tool Developers, but I'm interested in hearing about other possible applications. Comparison Compared to other Factory and Abstract Factory convenience packages, mine is based on the work from this GDC talk . The direct abstract-factories currently comes with a few more conveniences I've found useful during production. The idea stems from boiling down Pyblish to something that became a little more reusable when writing frameworks as opposed to being the framework. Suggestions, questions, comments etc welcome."},
{"Title": "Showcase: pdf-to-podcast.com -- Convert PDF's to podcast episodes. Free and open-source :)", "Author": "u/knowsuchagency", "Content": "What My Project Does Upload any PDF and have it converted into a podcast episode with two or more speakers discussing its contents. https://github.com/knowsuchagency/pdf-to-podcast Target Audience Anyone, but other developers in-particular. The code is open-source on GitHub and there's a link to the source on https://pdf-to-podcast.com . I want the project to serve as an illustrative example of how to build useful things on top of LLMs with relatively little code. Comparison I just made this for fun. It's possible there are other similar projects"},
{"Title": "Have anyone tried google/mesop", "Author": "u/codes_astro", "Content": "Google Open sourced Mesop. Mesop is a Python-based UI framework that allows you to rapidly build web apps. Used at Google for rapid internal app development similar to Streamlit. find more here"},
{"Title": "I created Yu-Gi-Oh! Power of Chaos save handler", "Author": "u/wildpantz", "Content": "Quick backstory: Upper floor of my house is sort of a man-cave until we decorate it, so during this time I have two PCs which I use to play games with a friend when we have extra time to waste. The other day I remembered the game mentioned in the title and we had lots of fun playing it (there's 3 different games in this series). I decided I'd transfer the save file to my main PC so I can play when he's not visiting and I quickly learned it's an extremely annoying process to transfer save files across different PCs. Long story short, you need to find a proper registry key (which isn't always located at same spot for some reason) and you need to locate a system.dat file also located in a folder that isn't always in the same place. This process gets tedious pretty quick, so I decided to use the power of Python to make my life easier. What the project does: It's essentially a CLI save handler for the game mentioned in the title. It has 5 slots where you can backup your current save or load the backup to the computer. It can also fix minor registry issues if needed. Target audience: Given that I'm about 20 years too late... I'd say mostly people with very slow PCs or people who like to inhale nostalgia. I learned a lot about using winreg and msvcrt and getch, so while I will likely get bored of the game in the coming weeks, I'm happy I learned something new in the meantime, plus maybe someone finds it useful! Source code: markomavrinac/yugioh_poc_save_handler: Yu-Gi-Oh! Power of Chaos save handler - A script to manage your save games across multiple computers (github.com)"},
{"Title": "Tutorial: A Timely Python Multi-page Streamlit Application on Olympic Medal Winning Countries", "Author": "u/jgloewen", "Content": "Streamlit is an open-source app framework that allows data scientists and analysts to create interactive web applications with ease. Using just a few lines of Python, you can turn data scripts into shareable web apps. And combined with a data visualization library like Plotly, you can create beautiful charts and maps with only a few lines of code. In this article, let me step you through how to use Streamlit to create a multi-page interactive application that visualizes Olympic medal data. The application will have three pages: an overview of medal counts, a country-specific analysis, and a choropleth map displaying global medal distributions. Let‚Äôs get to it! Link to free article HERE Github repo HERE"},
{"Title": "Learning Python coming from a JVM background", "Author": "u/Human_Dependent6814", "Content": "I have 4 years worth JVM languages (Java, Kotlin) and have a need to learn some Python.  What's a good resource to get up to speed quickly with idiomatic Python?"},
{"Title": "I made a cool calendar app with PyQt6", "Author": "u/Specialist-Arachnid6", "Content": "Tempus is a calendar with horoscopes, reminders, etc made with PyQt6 What my Project does? Tempus is a desktop-based calendar management application built with PyQt6, allowing users to manage their todos, reminders, and special dates efficiently. It offers features like adding, editing, and deleting tasks and reminders, as well as marking dates as special. Tempus ensures users stay organized and never miss important events. Plus, it shows you how many days are remaining until a special day in the dashboard. Target Audience Well, anyone who uses a desktop calendar app I guess? Comparison I did some research and couldn't find good calendar apps made with PyQt6.  If you guys knows any, please mention it below and I'm sorry in advance. GitHub https://github.com/rohankishore/Tempus"},
{"Title": "Better-OrderedMultiDict - a fast pure-pyton implementation of an ordered multi-valued dictionary.", "Author": "u/JoachimCoenen", "Content": "What my project does It provides a fast pure-python implementation of an ordered, multi-valued dictionary. Target audience Python developers that need this kind of specialized functionality. This can be used in production. It has no dependencies. The code is unit-tested (almost fully, I'm working on it) It requires Python 3.12+ Comparison Comparison to dict and OrderedDict dict and OederedDict are already ordered, but they only allow one value per key. You could use a defaultdict of lists, but then you have these disadvantages: you can end up with empty lists within the dict if you aren't careful you lose the order of individual items within the dict: items = [(1, '1'), (2, '2'), (2, '22'), (1, '11')] normal_dict = defaultdict(list) for key, value in items: normal_dict [key].append(value) om_dict = OrderedMultiDict(items) print(list(normal_dict .items)) # prints [(1, ['1', '11']), (2, ['2', '22'])] print(list(om\\_dict.items))     # prints [(1, '1'), (2, '2'), (2, '22'), (1, '11')] iterating over all key/value pairs can be cumbersome as you need nested loops Comparison to omdict . OederedDict provides a (in my opinion) nicer interface with less surprising behavior or pitfalls. My implementation is also faster. e.g iterating over all items is ~5x faster. More info This started as a toy project, that later became useful to me, so I decided to cleanup the code, add tests, and publish it. from better_orderedmultidict import OrderedMultiDict omd: OrderedMultiDict[int, int] = OrderedMultiDict([(1,1), (2,2), (1,11), (2,22)]) for key in reversed(omd.unique_keys()): print(f\"{key}: {omd.getall(key)}\") # prints: # 2: [2, 22] # 1: [1, 11] print(omd.popfirstitem())  # prints: (1, 1) print(omd.poplast(2))  # prints: 22 for key in reversed(omd.unique_keys()): print(f\"{key}: {omd.getall(key)}\") # prints: # 2: [2] # 1: [11] Installation You can install Better-OrderedMultiDict using pip: pip install better-orderedmultidict Contributing If you have any suggestions or improvements for Better-OrderedMultiDict, feel free to submit a pull request or open an issue on the GitHub repository . I appreciate any feedback or contributions! Links Here's the link to the GitHub repository: https://github.com/JoachimCoenen/Better-OrderedMultiDict Here's the link to PyPi: https://pypi.org/project/better-orderedmultidict"},
{"Title": "Cant decide between flask, django ninja or fastAPI for sideproject", "Author": "u/Eggesalt", "Content": "As the title says, I cant decide what to use for rest api for mye summer project. I am uni student, so this project will only be very small scale project. I have made simpel rest apis in sll of them, but still cant decide which one to actuslly use for my project. Do anyone have any tips for which might be right one? A thing to consider for me answel is how easy it is to host."},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Built a RAG ( Retrieval-Augmented Generation ) model using Gemini Api.", "Author": "u/inobody_somebody", "Content": "what my project does This project is built to solve the issue of LLM unable to produce relevant answers for information in a particular context. uses the information  to train the model and stored it in a database and uses  this database to get relevant answers from the Model. Target audiance This project is for people who want to train a LLM on a particular piece of information. comparison This model only gives answers for information regarding the data you provided in the file. It will not answer any other questions including formal greetings. GitHub link : https://github.com/dharmateja2810/RAG-Retrieval-Augmented-Generation-Model"},
{"Title": "Python automation ideas", "Author": "u/LeastPrice8673", "Content": "Hi I‚Äôm looking for inspiration for some stupid python automation projects. If you have done something funny or stupid using python automation I would love to hear it."},
{"Title": "Lua-style code blocks for Python", "Author": "u/guangrei", "Content": "Python is a great programming language, but sometimes the indentation can be terrible for some people (especially people with visual impairments). So i created Lython . What the project does: Lython replacing the Python indentation to lua-style code blocks. this is example lython code def test(num) for i in range(num) do if i == 0 then print(\"zero\") elif i % 2 == 1 then print(\"odd\") else print(\"even\") end # if else end # for end # def test(10) for more info, please visit lython repo. Target audience: Peoples with visual impairments (especially) and Programmers who want to write python code with new experience (generally) Repo & Source code: guangrei/lython"},
{"Title": "I made an MMORPG with Python & Telegram in 4 weeks", "Author": "u/LordOmbro", "Content": "well, kind of. I made Pilgram, an infinite idle RPG where your character goes on adventures and notifies you when stuff happens. What my project does The bot provides a text interface with wich you can \"play\" an MMO RPG, it's basically an online idle adventure game Target audience It's a toy project that i made out of boredom, also it sounded cool Comparison I never heard of anything like this except for some really old browser games. Maybe i'm just not informed. More info How is it infinite? The secret is AI . Every quest and event in the game is generated by AI depending on the demand of the players, so in theory you can go on an infinite amount of quests. Why did i call it an MMO? Because you can kind of play with your friends by creating & joining guilds and by sending gifts to eachother. There even is a guild leaderboard to see who gets the most points :) The interface is exclusively text based, but the command interpreter i wrote is pretty easy to integrate in other places, even in GUIs if anyone wants to try. I tried out a lot of new things for this project, like using ORMs, writing unit tests (don't look at those, i kinda got bored after a short while), using AI & writing generic enough code that it can be swapped with any other implementation. I think most of the code i wrote is pretty ok, but you can tell me what to change & what to improve if you want. Links here's the link to the code: https://github.com/SudoOmbro/pilgram if you wanna try out the version i'm running on my server start a conversation with pilgram_bot on Telegram, don't expect a balanced experience at first since that was kind of the last of my problems lol"},
{"Title": "Introducing Temporal Adjusters: Simplify Time Series Adjustments in Python!", "Author": "u/MDTv_Teka", "Content": "Hey guys! I'm excited to introduce Temporal Adjusters, a new Python package designed to make time series adjustments easier and more efficient. If you work with time series data, you'll find this tool incredibly useful for various temporal adjustments. What my project does Adjusters are a key tool for modifying temporal objects. They exist to externalize the process of adjustment, permitting different approaches, as per the strategy design pattern. Temporal Adjuster provides tools that help pinpoint very specific moments in time, without having to manually count days, weeks, or months. In essence, a Temporal Adjuster is a function that encapsulates a specific date/time manipulation rule. It operates on a temporal object (representing a date, time, or datetime) to produce a new temporal object adjusted according to the rule. Examples might be an adjuster that sets the date avoiding weekends, or one that sets the date to the last day of the month. Installation You can install Temporal Adjuster using pip: pip install temporal-adjuster Usage This package provides a set of predefined temporal adjusters that can be used to adjust a temporal object in various ways. For example: >>> from datetime import date, datetime >>> from temporal_adjuster import TemporalAdjuster >>> from temporal_adjuster.common.enums import Weekday >>> TemporalAdjuster.first_day_of_next_week(date(2021, 1, 1)) datetime.date(2021, 1, 4) >>> TemporalAdjuster.last_day_of_last_month(datetime(2021, 1, 1)) datetime.datetime(2020, 12, 31) >>> TemporalAdjuster.first_of_year(Weekday.SATURDAY, date(2021, 1, 1)) datetime.date(2021, 1, 2) >>> TemporalAdjuster.nth_of_month(Weekday.SUNDAY, datetime(2021, 5, 1), 2) datetime.datetime(2021, 5, 9) >>> TemporalAdjuster.next(Weekday.MONDAY, datetime(2021, 2, 11), 2) datetime.datetime(2021, 2, 15) Contributing If you have any suggestions or improvements for pynimbar, feel free to submit a pull request or open an issue on the GitHub repository as per the CONTRIBUTING document. We appreciate any feedback or contributions! Target audience This can be used in production. It has only one depedency, dateutils, which if you're manipulating temporal objects you probably already have. All the code is 100% unit-tested, as well as build tested for all supported Python versions. Comparison This is based on Java's native TemporalAdjuster interfaces, but I found no similar library/functionality for Python."},
{"Title": "Created an Api for APKpure", "Author": "u/ltlbwu", "Content": "Like the title said. I created an API fro apkpure.com . I was creating a script to automate YouTube Revanced, but i couldn't find anyway to download the apk. You can try out the app here: https://github.com/anishomsy/apkpure What My Project Does It allows you to download apk from apkpure . Users can easily fetch specific versions of Android apps programmatically. Target Audience it is a hobby project, anyone can use it Comparison I did not find any existing alternatives. So I created my own. The only other way was to download it manually which is very tedious. Please lmk how i can improve. Thank you"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "My first Python package, D1py: A very simple library to interact with Cloudflare D1 Database API", "Author": "u/ogMasterPloKoon", "Content": "What My Project Does Cloudflare offers a free SQLite based database D1. I needed it for some personal project so I thought of creating a very simple wrapper for it. D1py let's you connect to D1 database in your cloudflare account and run SQL queries(CRUD operations). Target audience For those who need a simple wrapper for Cloudflare D1 API for their projects. Comparison Right now there are no Python wrappers or libraries for D1 yet.... that's why I thought of creating one. It's not perfect but it is my first attempt at writing a small library/package for doing a task. Source Repository: https://github.com/Suleman-Elahi/D1py Feel free to drop any suggestions. Thanks."},
{"Title": "Perpetual - a self-generalizing, hyperparameter-free gradient boosting machine", "Author": "u/mutlu_simsek", "Content": "https://github.com/perpetual-ml/perpetual What My Project Does PerpetualBooster is a gradient boosting machine (GBM) algorithm which doesn't have hyperparameters to be tuned so that you can use it without needing hyperparameter optimization packages unlike other GBM algorithms. Similar to AutoML libraries, it has a budget parameter which ranges between (0, 1) . Increasing the budget parameter increases predictive power of the algorithm and gives better results on unseen data. Start with a small budget and increase it once you are confident with your features. If you don't see any improvement with further increasing budget , it means that you are already extracting the most predictive power out of your data. Target Audience The project is meant for production. You can replace hyperparameter packages plus other gradient boosting algorithms with PerpetualBooster. Comparison Other gradient boosting algorithms (XGBoost, LightGBM, Catboost) and most of the machine learning algorithms need hyperparameter optimization for the best performance on unseen data. But PerpetualBooster doesn't have hyperparameters so it doesn't need hyperparameter tuning. It has a built-in generalization algorithm and provides the best performance. The following table summarizes the results for the California Housing dataset: Perpetual budget LightGBM n_estimators Perpetual mse LightGBM mse Perpetual cpu time LightGBM cpu time Speed-up 0.33 100 0.192 0.192 10.1 990 98x 0.35 200 0.190 0.191 11.0 2030 186x 0.45 300 0.187 0.188 18.7 3272 179x"},
{"Title": "Problem details for FastAPI applications (RFC9457)", "Author": "u/BluesFiend", "Content": "Just released v0.8.0 of fastapi_problem to provide problem details for FastAPI applications. Hoping it can provide value to some other peoples projects. Code: https://github.com/NRWLDev/fastapi-problem Docs: https://nrwldev.github.io/fastapi-problem/ Pypi: https://pypi.org/project/fastapi-problem/ What My Project Does Provides a simple exception handler and an underlying exception class heirarchy to remove the need to think about error management in your FastAPI project, just raise errors as appropriate and let the handler deal with responses. Target Audience Web developers Comparison There was a previous project that supported RFC7807 but that is no longer maintained, and is also made obsolete by RFC9457. RFC9457 For anyone who does not make use of FastAPI, the underlying exception library has also been released, and can be used to implement handlers for any web framework you might be into. https://github.com/NRWLDev/rfc9457 https://pypi.org/project/rfc9457/"},
{"Title": "I tried to explain python imports", "Author": "u/MaKaNuReddit", "Content": "When I was a beginner (or maybe still I am) I struggled a lot with pythons import function. Over the years I went over different approaches, how to handle imports and ended up using mostly exclusive poetry. I've met a lot of people struggling the same way, bit always could just explain very shortly my experience. I've now decided to write it down as a scenario, where I can show and explain my pitfalls: https://github.com/MaKaNu/pyimport-explained"},
{"Title": "a new version of ultimateultimateguitar", "Author": "u/sonobanana33", "Content": "What My Project Does It is a CLI to get songs from ultimateguitar. How it looks like: https://youtu.be/Spm1IIaYo8Q I've only tried it on linux. Available in debian and pypi. Target audience For musicians who also use the terminal and who don't especially like the ultimateguitar website. Comparison I'm not aware of other projects doing the same thing. Compared to the website, it can transpose and it is much faster. Source Project website: https://codeberg.org/ltworf/ultimateultimateguitar Out of date website (just here to avoid the post to be auto-removed): https://github.com/ltworf/ultimateultimateguitar"},
{"Title": "how about one-line try-except statement ?", "Author": "u/17thCurlyBrace", "Content": "is there a proposal for a shorter exception handling syntax for those very frequent cases where a library function doesn't return \"error value\" like str.index ? something like instead of : try: i = my_str.index(\"sub\", st, en) except ValueError: # if \"sub\" has not been found pass else: # do stuff with i (note that i usually want independent error handling here) something like this : i = my_str.index(\"sub\", st, en) except ValueError -1 # or maybe even return here if i == -1: # also can return right away if i want to avoid an indent next # do stuff with i ... i suspect there might be something \"un-pythonic\" here in what i am imagining , but please forgive me if that's the case . i am a fan of Python for many years , but haven't really invested any time in learning the philosophy so i am interested in what the community thinks about this , how ok would such syntax be from the point of the \"Python way\" , and if there is such a proposal i would like to know if i can consider maybe voting on it somehow"},
{"Title": "uv added experimental commands for `uv add/remove`", "Author": "u/BaggiPonte", "Content": "uv is the \"pip but blazingly fast‚Ñ¢Ô∏è because it's written in rust\" and is developed by the same folks that did ruff. In 0.2.11 they released an experimental/preview command of `uv add/remove` that adds a library to pyproject.toml. It's the first step to become a fully-fledged package manager! I noticed you can also manage python installations with uv using `uv toolchain` command (i.e. be like pyenv) and run tools (like a smaller version of pipx) with `uv run`. I'm genuinely excited about this, Python packaging is going to become such a smooth experience üòé Commands are in preview so expect missing stuff. (I bear no affiliation with astral) https://github.com/astral-sh/uv"},
{"Title": "Pathway - Build Mission Critical ETL and RAG in Python (used by NATO, F1)", "Author": "u/dxtros", "Content": "Hi Python data folks, I am excited to share Pathway, a Python data processing framework we built for ETL and RAG pipelines. https://github.com/pathwaycom/pathway What My Project Does We started Pathway to solve event processing for IoT and geospatial indexing. Think freight train operations in unmapped depots bringing key merchandise from China to Europe. This was not something we could use Flink or Elastic for. Then we added more connectors for streaming ETL (Kafka, Postgres CDC‚Ä¶), data indexing (yay vectors!), and LLM wrappers for RAG. Today Pathway provides a data indexing layer for live data updates, stateless and stateful data transformations over streams, and retrieval of structured and unstructured data. Pathway ships with a Python API and a Rust runtime based on Differential Dataflow to perform incremental computation. All the pipeline is kept in memory and can be easily deployed with Docker and Kubernetes (pipelines-as-code). We built Pathway to support enterprises like F1 teams and processors of highly sensitive information to build mission-critical data pipelines. We do this by putting security and performance first. For example, you can build and deploy self-hosted RAG pipelines with local LLM models and Pathway‚Äôs in-memory vector index, so no data ever leaves your infrastructure. Pathway connectors and transformations work with live data by default, so you can avoid expensive reprocessing and rely on fresh data. You can install Pathway with pip and Docker, and get started with templates and notebooks: https://pathway.com/developers/showcases We also host demo RAG pipelines implemented 100% in Pathway, feel free to interact with their API endpoints: https://pathway.com/solutions/rag-pipelines#try-it-out We'd love to hear what you think of Pathway!"},
{"Title": "abstract-factories - a simple framework for content creation pipelines", "Author": "u/HistoricalCrow", "Content": "Hey all, my project abstract_factories is up to gauge interest and primarily feedback. The design goal is to make it easier to iterate on typical Content Creation pipeline tools (tool dev, rigging, validation, asset management etc) with a flexible framework to provide convenience, open and simple design and no dependencies (currently). It's an approach I've used a lot over the years and found it pretty versatile in production across numerous projects. Key features Auto-registration of matching items (types or instances) from any given path or python module. Simple or conditional item identifiers. Versioning. Recursive path searching (recursive module search in review). Dynamic resolving and importing modules in packaged (supports relative importing). Usage Examples There are a couple of simple examples given along with tests to cover all of the current features. What the project does It's a convenience package for creating scalable tools and frameworks using Abstract Factory design pattern. Target Audience Due to the solutions it's built for, it's aimed primarily at Technical Artists, Technical Animators, Pipeline and Tool Developers, but I'm interested in hearing about other possible applications. Comparison Compared to other Factory and Abstract Factory convenience packages, mine is based on the work from this GDC talk . The direct abstract-factories currently comes with a few more conveniences I've found useful during production. The idea stems from boiling down Pyblish to something that became a little more reusable when writing frameworks as opposed to being the framework. Suggestions, questions, comments etc welcome."},
{"Title": "Showcase: pdf-to-podcast.com -- Convert PDF's to podcast episodes. Free and open-source :)", "Author": "u/knowsuchagency", "Content": "What My Project Does Upload any PDF and have it converted into a podcast episode with two or more speakers discussing its contents. https://github.com/knowsuchagency/pdf-to-podcast Target Audience Anyone, but other developers in-particular. The code is open-source on GitHub and there's a link to the source on https://pdf-to-podcast.com . I want the project to serve as an illustrative example of how to build useful things on top of LLMs with relatively little code. Comparison I just made this for fun. It's possible there are other similar projects"},
{"Title": "Have anyone tried google/mesop", "Author": "u/codes_astro", "Content": "Google Open sourced Mesop. Mesop is a Python-based UI framework that allows you to rapidly build web apps. Used at Google for rapid internal app development similar to Streamlit. find more here"},
{"Title": "I created Yu-Gi-Oh! Power of Chaos save handler", "Author": "u/wildpantz", "Content": "Quick backstory: Upper floor of my house is sort of a man-cave until we decorate it, so during this time I have two PCs which I use to play games with a friend when we have extra time to waste. The other day I remembered the game mentioned in the title and we had lots of fun playing it (there's 3 different games in this series). I decided I'd transfer the save file to my main PC so I can play when he's not visiting and I quickly learned it's an extremely annoying process to transfer save files across different PCs. Long story short, you need to find a proper registry key (which isn't always located at same spot for some reason) and you need to locate a system.dat file also located in a folder that isn't always in the same place. This process gets tedious pretty quick, so I decided to use the power of Python to make my life easier. What the project does: It's essentially a CLI save handler for the game mentioned in the title. It has 5 slots where you can backup your current save or load the backup to the computer. It can also fix minor registry issues if needed. Target audience: Given that I'm about 20 years too late... I'd say mostly people with very slow PCs or people who like to inhale nostalgia. I learned a lot about using winreg and msvcrt and getch, so while I will likely get bored of the game in the coming weeks, I'm happy I learned something new in the meantime, plus maybe someone finds it useful! Source code: markomavrinac/yugioh_poc_save_handler: Yu-Gi-Oh! Power of Chaos save handler - A script to manage your save games across multiple computers (github.com)"},
{"Title": "Tutorial: A Timely Python Multi-page Streamlit Application on Olympic Medal Winning Countries", "Author": "u/jgloewen", "Content": "Streamlit is an open-source app framework that allows data scientists and analysts to create interactive web applications with ease. Using just a few lines of Python, you can turn data scripts into shareable web apps. And combined with a data visualization library like Plotly, you can create beautiful charts and maps with only a few lines of code. In this article, let me step you through how to use Streamlit to create a multi-page interactive application that visualizes Olympic medal data. The application will have three pages: an overview of medal counts, a country-specific analysis, and a choropleth map displaying global medal distributions. Let‚Äôs get to it! Link to free article HERE Github repo HERE"},
{"Title": "Learning Python coming from a JVM background", "Author": "u/Human_Dependent6814", "Content": "I have 4 years worth JVM languages (Java, Kotlin) and have a need to learn some Python.  What's a good resource to get up to speed quickly with idiomatic Python?"},
{"Title": "I made a cool calendar app with PyQt6", "Author": "u/Specialist-Arachnid6", "Content": "Tempus is a calendar with horoscopes, reminders, etc made with PyQt6 What my Project does? Tempus is a desktop-based calendar management application built with PyQt6, allowing users to manage their todos, reminders, and special dates efficiently. It offers features like adding, editing, and deleting tasks and reminders, as well as marking dates as special. Tempus ensures users stay organized and never miss important events. Plus, it shows you how many days are remaining until a special day in the dashboard. Target Audience Well, anyone who uses a desktop calendar app I guess? Comparison I did some research and couldn't find good calendar apps made with PyQt6.  If you guys knows any, please mention it below and I'm sorry in advance. GitHub https://github.com/rohankishore/Tempus"},
{"Title": "Better-OrderedMultiDict - a fast pure-pyton implementation of an ordered multi-valued dictionary.", "Author": "u/JoachimCoenen", "Content": "What my project does It provides a fast pure-python implementation of an ordered, multi-valued dictionary. Target audience Python developers that need this kind of specialized functionality. This can be used in production. It has no dependencies. The code is unit-tested (almost fully, I'm working on it) It requires Python 3.12+ Comparison Comparison to dict and OrderedDict dict and OederedDict are already ordered, but they only allow one value per key. You could use a defaultdict of lists, but then you have these disadvantages: you can end up with empty lists within the dict if you aren't careful you lose the order of individual items within the dict: items = [(1, '1'), (2, '2'), (2, '22'), (1, '11')] normal_dict = defaultdict(list) for key, value in items: normal_dict [key].append(value) om_dict = OrderedMultiDict(items) print(list(normal_dict .items)) # prints [(1, ['1', '11']), (2, ['2', '22'])] print(list(om\\_dict.items))     # prints [(1, '1'), (2, '2'), (2, '22'), (1, '11')] iterating over all key/value pairs can be cumbersome as you need nested loops Comparison to omdict . OederedDict provides a (in my opinion) nicer interface with less surprising behavior or pitfalls. My implementation is also faster. e.g iterating over all items is ~5x faster. More info This started as a toy project, that later became useful to me, so I decided to cleanup the code, add tests, and publish it. from better_orderedmultidict import OrderedMultiDict omd: OrderedMultiDict[int, int] = OrderedMultiDict([(1,1), (2,2), (1,11), (2,22)]) for key in reversed(omd.unique_keys()): print(f\"{key}: {omd.getall(key)}\") # prints: # 2: [2, 22] # 1: [1, 11] print(omd.popfirstitem())  # prints: (1, 1) print(omd.poplast(2))  # prints: 22 for key in reversed(omd.unique_keys()): print(f\"{key}: {omd.getall(key)}\") # prints: # 2: [2] # 1: [11] Installation You can install Better-OrderedMultiDict using pip: pip install better-orderedmultidict Contributing If you have any suggestions or improvements for Better-OrderedMultiDict, feel free to submit a pull request or open an issue on the GitHub repository . I appreciate any feedback or contributions! Links Here's the link to the GitHub repository: https://github.com/JoachimCoenen/Better-OrderedMultiDict Here's the link to PyPi: https://pypi.org/project/better-orderedmultidict"},
{"Title": "Cant decide between flask, django ninja or fastAPI for sideproject", "Author": "u/Eggesalt", "Content": "As the title says, I cant decide what to use for rest api for mye summer project. I am uni student, so this project will only be very small scale project. I have made simpel rest apis in sll of them, but still cant decide which one to actuslly use for my project. Do anyone have any tips for which might be right one? A thing to consider for me answel is how easy it is to host."},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Built a RAG ( Retrieval-Augmented Generation ) model using Gemini Api.", "Author": "u/inobody_somebody", "Content": "what my project does This project is built to solve the issue of LLM unable to produce relevant answers for information in a particular context. uses the information  to train the model and stored it in a database and uses  this database to get relevant answers from the Model. Target audiance This project is for people who want to train a LLM on a particular piece of information. comparison This model only gives answers for information regarding the data you provided in the file. It will not answer any other questions including formal greetings. GitHub link : https://github.com/dharmateja2810/RAG-Retrieval-Augmented-Generation-Model"},
{"Title": "Python automation ideas", "Author": "u/LeastPrice8673", "Content": "Hi I‚Äôm looking for inspiration for some stupid python automation projects. If you have done something funny or stupid using python automation I would love to hear it."},
{"Title": "Lua-style code blocks for Python", "Author": "u/guangrei", "Content": "Python is a great programming language, but sometimes the indentation can be terrible for some people (especially people with visual impairments). So i created Lython . What the project does: Lython replacing the Python indentation to lua-style code blocks. this is example lython code def test(num) for i in range(num) do if i == 0 then print(\"zero\") elif i % 2 == 1 then print(\"odd\") else print(\"even\") end # if else end # for end # def test(10) for more info, please visit lython repo. Target audience: Peoples with visual impairments (especially) and Programmers who want to write python code with new experience (generally) Repo & Source code: guangrei/lython"},
{"Title": "I made an MMORPG with Python & Telegram in 4 weeks", "Author": "u/LordOmbro", "Content": "well, kind of. I made Pilgram, an infinite idle RPG where your character goes on adventures and notifies you when stuff happens. What my project does The bot provides a text interface with wich you can \"play\" an MMO RPG, it's basically an online idle adventure game Target audience It's a toy project that i made out of boredom, also it sounded cool Comparison I never heard of anything like this except for some really old browser games. Maybe i'm just not informed. More info How is it infinite? The secret is AI . Every quest and event in the game is generated by AI depending on the demand of the players, so in theory you can go on an infinite amount of quests. Why did i call it an MMO? Because you can kind of play with your friends by creating & joining guilds and by sending gifts to eachother. There even is a guild leaderboard to see who gets the most points :) The interface is exclusively text based, but the command interpreter i wrote is pretty easy to integrate in other places, even in GUIs if anyone wants to try. I tried out a lot of new things for this project, like using ORMs, writing unit tests (don't look at those, i kinda got bored after a short while), using AI & writing generic enough code that it can be swapped with any other implementation. I think most of the code i wrote is pretty ok, but you can tell me what to change & what to improve if you want. Links here's the link to the code: https://github.com/SudoOmbro/pilgram if you wanna try out the version i'm running on my server start a conversation with pilgram_bot on Telegram, don't expect a balanced experience at first since that was kind of the last of my problems lol"},
{"Title": "Introducing Temporal Adjusters: Simplify Time Series Adjustments in Python!", "Author": "u/MDTv_Teka", "Content": "Hey guys! I'm excited to introduce Temporal Adjusters, a new Python package designed to make time series adjustments easier and more efficient. If you work with time series data, you'll find this tool incredibly useful for various temporal adjustments. What my project does Adjusters are a key tool for modifying temporal objects. They exist to externalize the process of adjustment, permitting different approaches, as per the strategy design pattern. Temporal Adjuster provides tools that help pinpoint very specific moments in time, without having to manually count days, weeks, or months. In essence, a Temporal Adjuster is a function that encapsulates a specific date/time manipulation rule. It operates on a temporal object (representing a date, time, or datetime) to produce a new temporal object adjusted according to the rule. Examples might be an adjuster that sets the date avoiding weekends, or one that sets the date to the last day of the month. Installation You can install Temporal Adjuster using pip: pip install temporal-adjuster Usage This package provides a set of predefined temporal adjusters that can be used to adjust a temporal object in various ways. For example: >>> from datetime import date, datetime >>> from temporal_adjuster import TemporalAdjuster >>> from temporal_adjuster.common.enums import Weekday >>> TemporalAdjuster.first_day_of_next_week(date(2021, 1, 1)) datetime.date(2021, 1, 4) >>> TemporalAdjuster.last_day_of_last_month(datetime(2021, 1, 1)) datetime.datetime(2020, 12, 31) >>> TemporalAdjuster.first_of_year(Weekday.SATURDAY, date(2021, 1, 1)) datetime.date(2021, 1, 2) >>> TemporalAdjuster.nth_of_month(Weekday.SUNDAY, datetime(2021, 5, 1), 2) datetime.datetime(2021, 5, 9) >>> TemporalAdjuster.next(Weekday.MONDAY, datetime(2021, 2, 11), 2) datetime.datetime(2021, 2, 15) Contributing If you have any suggestions or improvements for pynimbar, feel free to submit a pull request or open an issue on the GitHub repository as per the CONTRIBUTING document. We appreciate any feedback or contributions! Target audience This can be used in production. It has only one depedency, dateutils, which if you're manipulating temporal objects you probably already have. All the code is 100% unit-tested, as well as build tested for all supported Python versions. Comparison This is based on Java's native TemporalAdjuster interfaces, but I found no similar library/functionality for Python."},
{"Title": "Created an Api for APKpure", "Author": "u/ltlbwu", "Content": "Like the title said. I created an API fro apkpure.com . I was creating a script to automate YouTube Revanced, but i couldn't find anyway to download the apk. You can try out the app here: https://github.com/anishomsy/apkpure What My Project Does It allows you to download apk from apkpure . Users can easily fetch specific versions of Android apps programmatically. Target Audience it is a hobby project, anyone can use it Comparison I did not find any existing alternatives. So I created my own. The only other way was to download it manually which is very tedious. Please lmk how i can improve. Thank you"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "My first Python package, D1py: A very simple library to interact with Cloudflare D1 Database API", "Author": "u/ogMasterPloKoon", "Content": "What My Project Does Cloudflare offers a free SQLite based database D1. I needed it for some personal project so I thought of creating a very simple wrapper for it. D1py let's you connect to D1 database in your cloudflare account and run SQL queries(CRUD operations). Target audience For those who need a simple wrapper for Cloudflare D1 API for their projects. Comparison Right now there are no Python wrappers or libraries for D1 yet.... that's why I thought of creating one. It's not perfect but it is my first attempt at writing a small library/package for doing a task. Source Repository: https://github.com/Suleman-Elahi/D1py Feel free to drop any suggestions. Thanks."},
{"Title": "Perpetual - a self-generalizing, hyperparameter-free gradient boosting machine", "Author": "u/mutlu_simsek", "Content": "https://github.com/perpetual-ml/perpetual What My Project Does PerpetualBooster is a gradient boosting machine (GBM) algorithm which doesn't have hyperparameters to be tuned so that you can use it without needing hyperparameter optimization packages unlike other GBM algorithms. Similar to AutoML libraries, it has a budget parameter which ranges between (0, 1) . Increasing the budget parameter increases predictive power of the algorithm and gives better results on unseen data. Start with a small budget and increase it once you are confident with your features. If you don't see any improvement with further increasing budget , it means that you are already extracting the most predictive power out of your data. Target Audience The project is meant for production. You can replace hyperparameter packages plus other gradient boosting algorithms with PerpetualBooster. Comparison Other gradient boosting algorithms (XGBoost, LightGBM, Catboost) and most of the machine learning algorithms need hyperparameter optimization for the best performance on unseen data. But PerpetualBooster doesn't have hyperparameters so it doesn't need hyperparameter tuning. It has a built-in generalization algorithm and provides the best performance. The following table summarizes the results for the California Housing dataset: Perpetual budget LightGBM n_estimators Perpetual mse LightGBM mse Perpetual cpu time LightGBM cpu time Speed-up 0.33 100 0.192 0.192 10.1 990 98x 0.35 200 0.190 0.191 11.0 2030 186x 0.45 300 0.187 0.188 18.7 3272 179x"},
{"Title": "Problem details for FastAPI applications (RFC9457)", "Author": "u/BluesFiend", "Content": "Just released v0.8.0 of fastapi_problem to provide problem details for FastAPI applications. Hoping it can provide value to some other peoples projects. Code: https://github.com/NRWLDev/fastapi-problem Docs: https://nrwldev.github.io/fastapi-problem/ Pypi: https://pypi.org/project/fastapi-problem/ What My Project Does Provides a simple exception handler and an underlying exception class heirarchy to remove the need to think about error management in your FastAPI project, just raise errors as appropriate and let the handler deal with responses. Target Audience Web developers Comparison There was a previous project that supported RFC7807 but that is no longer maintained, and is also made obsolete by RFC9457. RFC9457 For anyone who does not make use of FastAPI, the underlying exception library has also been released, and can be used to implement handlers for any web framework you might be into. https://github.com/NRWLDev/rfc9457 https://pypi.org/project/rfc9457/"},
{"Title": "I tried to explain python imports", "Author": "u/MaKaNuReddit", "Content": "When I was a beginner (or maybe still I am) I struggled a lot with pythons import function. Over the years I went over different approaches, how to handle imports and ended up using mostly exclusive poetry. I've met a lot of people struggling the same way, bit always could just explain very shortly my experience. I've now decided to write it down as a scenario, where I can show and explain my pitfalls: https://github.com/MaKaNu/pyimport-explained"},
{"Title": "a new version of ultimateultimateguitar", "Author": "u/sonobanana33", "Content": "What My Project Does It is a CLI to get songs from ultimateguitar. How it looks like: https://youtu.be/Spm1IIaYo8Q I've only tried it on linux. Available in debian and pypi. Target audience For musicians who also use the terminal and who don't especially like the ultimateguitar website. Comparison I'm not aware of other projects doing the same thing. Compared to the website, it can transpose and it is much faster. Source Project website: https://codeberg.org/ltworf/ultimateultimateguitar Out of date website (just here to avoid the post to be auto-removed): https://github.com/ltworf/ultimateultimateguitar"},
{"Title": "how about one-line try-except statement ?", "Author": "u/17thCurlyBrace", "Content": "is there a proposal for a shorter exception handling syntax for those very frequent cases where a library function doesn't return \"error value\" like str.index ? something like instead of : try: i = my_str.index(\"sub\", st, en) except ValueError: # if \"sub\" has not been found pass else: # do stuff with i (note that i usually want independent error handling here) something like this : i = my_str.index(\"sub\", st, en) except ValueError -1 # or maybe even return here if i == -1: # also can return right away if i want to avoid an indent next # do stuff with i ... i suspect there might be something \"un-pythonic\" here in what i am imagining , but please forgive me if that's the case . i am a fan of Python for many years , but haven't really invested any time in learning the philosophy so i am interested in what the community thinks about this , how ok would such syntax be from the point of the \"Python way\" , and if there is such a proposal i would like to know if i can consider maybe voting on it somehow"},
{"Title": "uv added experimental commands for `uv add/remove`", "Author": "u/BaggiPonte", "Content": "uv is the \"pip but blazingly fast‚Ñ¢Ô∏è because it's written in rust\" and is developed by the same folks that did ruff. In 0.2.11 they released an experimental/preview command of `uv add/remove` that adds a library to pyproject.toml. It's the first step to become a fully-fledged package manager! I noticed you can also manage python installations with uv using `uv toolchain` command (i.e. be like pyenv) and run tools (like a smaller version of pipx) with `uv run`. I'm genuinely excited about this, Python packaging is going to become such a smooth experience üòé Commands are in preview so expect missing stuff. (I bear no affiliation with astral) https://github.com/astral-sh/uv"},
{"Title": "Pathway - Build Mission Critical ETL and RAG in Python (used by NATO, F1)", "Author": "u/dxtros", "Content": "Hi Python data folks, I am excited to share Pathway, a Python data processing framework we built for ETL and RAG pipelines. https://github.com/pathwaycom/pathway What My Project Does We started Pathway to solve event processing for IoT and geospatial indexing. Think freight train operations in unmapped depots bringing key merchandise from China to Europe. This was not something we could use Flink or Elastic for. Then we added more connectors for streaming ETL (Kafka, Postgres CDC‚Ä¶), data indexing (yay vectors!), and LLM wrappers for RAG. Today Pathway provides a data indexing layer for live data updates, stateless and stateful data transformations over streams, and retrieval of structured and unstructured data. Pathway ships with a Python API and a Rust runtime based on Differential Dataflow to perform incremental computation. All the pipeline is kept in memory and can be easily deployed with Docker and Kubernetes (pipelines-as-code). We built Pathway to support enterprises like F1 teams and processors of highly sensitive information to build mission-critical data pipelines. We do this by putting security and performance first. For example, you can build and deploy self-hosted RAG pipelines with local LLM models and Pathway‚Äôs in-memory vector index, so no data ever leaves your infrastructure. Pathway connectors and transformations work with live data by default, so you can avoid expensive reprocessing and rely on fresh data. You can install Pathway with pip and Docker, and get started with templates and notebooks: https://pathway.com/developers/showcases We also host demo RAG pipelines implemented 100% in Pathway, feel free to interact with their API endpoints: https://pathway.com/solutions/rag-pipelines#try-it-out We'd love to hear what you think of Pathway!"},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Polars 1.0 will be out in a few weeks, but you can already install the pre-release!", "Author": "u/marcogorelli", "Content": "In a few weeks, Polars 1.0 will be out. How exciting! You can already try out the pre-release by running: ``` pip install -U --pre polars ``` If you encounter any bugs, you can report them to https://github.com/pola-rs/polars/issues , so they can be fixed before 1.0 comes out. Release notes: https://github.com/pola-rs/polars/releases/tag/py-1.0.0-alpha.1"},
{"Title": "Try PyCharm (30% off!) and they donate 100% to the Django Software Foundation", "Author": "u/Affectionate_Sky9709", "Content": "There's a promotion right now to try PyCharm, get a 30% discount, and 100% of what you pay goes directly to the Django Software Foundation, which maintains Django and keeps it free for everyone. https://jb.gg/2atgzm I hope this kind of post is allowed."},
{"Title": "I ported Rust's Regex Library To Python, but the time taken by the compile parameter was high.", "Author": "u/RevolutionaryPen4661", "Content": "(.venv) PS D:\\flpc> python .\\seed\\test.py Operation  | flpc (ms)  | re (ms) ---------------------------------- Compile    | 1496.18077 | 0.00000 Search     | 19.67597   | 1721.07339 Find Match | 15.62524   | 16.72506 Full Match | 15.62500   | 0.00000 Split      | 0.00000    | 1722.88108 Find All   | 3.02815    | 1660.32910 Find Iter  | 5.96547    | 1672.50776 Sub        | 0.00000    | 1548.61116 Subn       | 6.70719    | 1676.84698 Escape     | 4.87757    | 0.00000 (.venv) PS D:\\flpc> flpc is the name of the library. I named it (spelt as flacpuc). The strange thing is that why the compile time is high of flpc (rust) than of re module (implemented in Pure-Python) (it does the same thing what re.compile does in Python). The benchmark is done on: PATTERN = r'(\\w+)\\s+(\\d+)' TEXT = ''.join(choices(ascii_letters + digits, k=1000)) # choices function from random module ITERATIONS = 100 The problem is that, the python should be slow in the parameter (Regex Compile). However, the rest of parameters looks great! VERY FAST!"},
{"Title": "Vedo or PyVista?", "Author": "u/AlexTheRandomizer", "Content": "Hi guys! What are your experiences with Vedo or PyVista? Which one do you prefer? Did you have any specific issues which either of these libraries? I'm mostly interested in meshes and point clouds rendering."},
{"Title": "Sold my Python open source project to a San Francisco AI company. Now I work for them. AMA.", "Author": "u/romerio86", "Content": "About a year ago, I posted on this sub. I was terrified. I was launching a new framework. Another framework? Yes, I was crazy enough to think we needed yet another framework. Thankfully, the response was great. Many were excited to try it. Others were understandably skeptical, and respectfully asking good questions. This time, I'm posting for completely different reasons. I want to share a story. A story of which this sub, and hundreds of you, are part. It all started 2 years ago, when I was laid off from my analytics consulting job. I had a well-paying, comfortable job in the UK. Then I moved from the UK to Poland, where I live now, and continued working remotely. I was living the dream; earning a London salary while living in a place with a lower cost of living. Until it ended with a layoff. I thought, this is it. My career is dead. I didn't speak Polish properly, limiting my options. And finding another fully remote job working for the UK sounded overly optimistic at the time. Being in my mid 30s and with a family to support, I didn't want to start over again. I knew Python and data analytics quite well, and also had frontend skills I had gained throughout the years. So I thought... I need to show what I can do. I didn't have a portfolio at all; my GitHub was empty. After trying Streamlit, I thought the concept was great, but the execution wasn't. So I wrote an article on Medium, discussing how a better, faster alternative was possible. I also created a POC and shared it on GitHub. Thankfully, due to contacts at my previous job, I was able to find another remote job, working for the UK w. With even better pay. So naturally, I forgot about my portfolio-building efforts. But after a few months, an investor (VC) from Germany reached out to me. He had seen the Medium article and asked me whether I'd like to do this full time. I hesitated, but eventually decided to explore this further. I didn't need any investment though; my idea was quite simple. And to be honest, not too different from other frameworks, just faster. I had to think bigger. One day, at London Stansted Airport, while waiting to board a plane home, I decided to go for it and came up with the idea of no-code in the front, Python in the back. In other words, building the frontend using a visual editor, while allowing for full freedom in the backend using Python, and abstracting all the connectivity between. The VC liked the idea, but wasn't fully convinced about my ability to execute. He decided not to invest. But since I liked the idea and thought it could go somewhere, I decided to try building it myself, at night, after work. For 9 months, that was my reality. Nights, weekends. If my baby son would wake up, early mornings too. In May 2023, I managed to get the framework to a state I was happy with, and launched it. The response was very good. I eventually got to 1000 stars on GitHub, a milestone for any open source project. To a great extent, thanks to the support of communities such as r/python and r/opensource . Also, thanks to sites like Medium and Product Hunt. A few months later, in November 2023, the CTO of a multibillion AI company reached out to me. They wanted to acquire my framework, hire me, and build a team for me to continue developing it. I was ecstatic. He told me he'd go on a Thanksgiving break for a few days and that he'd reach out to me after. He never got back to me. Accepting that this wasn't going to happen was tough. Two weeks later, the CTO of another AI company called me, together with the CEO. They also wanted to acquire me and make me a part of their team. A smaller company, much more interesting and already quite established, with clients such as Accenture and Salesforce. But with grit and determination to win in the space of enterprise generative AI. This time, it did work out and my framework was finally acquired. Now I work for them and I lead a team focused on maintaining this open source project. Happy to answer any questions. And THANK YOU for your support r/python !!! For those curious: https://github.com/writer/writer-framework"},
{"Title": "Building AI Text-to-Video Model From Scratch", "Author": "u/FareedKhan557", "Content": "What My Project Does This project aims to create a small-scale text-to-video model that can generate videos based on text prompts. Target audience This project is designed for individuals who want to learn how to create their own text-to-video model from scratch but don't know where to start. It will provide a basic guide from beginning to end, covering everything from generating the training data to training a model and using that trained model to generate AI videos. Comparison Currently available text-to-video models require high computational power, and their complex code makes it difficult for Rookie developers to understand the practical implementation, beyond just the theory. To address this, I have created a small-scale GAN architecture, similar to text-to-video models, which can be trained on a CPU or a single T4 GPU. GitHub Code, documentation, and example can all be found on GitHub: https://github.com/FareedKhan-dev/AI-text-to-video-model-from-scratch"},
{"Title": "My Thoughts on Python in Excel", "Author": "u/fzumstein", "Content": "Hi all, it's been almost 1 year since the preview of Python in Excel has been revealed. So I wrote up a blog post pointing out what works well and what should be improved: https://www.xlwings.org/blog/my-thoughts-on-python-in-excel Here‚Äôs the TL;DR: We wanted an alternative to VBA, but got an alternative to the Excel formula language Integrating the Jupyter notebook cells inside the Excel grid was a mistake Python in Excel isn‚Äôt suitable for Python beginners nor for interactive data analysis Right now, there are too many restrictions (can‚Äôt use your own packages and can‚Äôt connect to web APIs) Here are the current use cases I see for Python in Excel: Computationally intensive things like Monte Carlo simulations AI stuff via the included packages (scikit-learn, nltk, statsmodels, imbalanced-learn, gensim) Advanced visualizations via Matplotlib/Seaborn Time-series analysis (this is one of Excel‚Äôs blind spots) Not sure about data cleaning/data analysis: since you almost certainly need Power Query, it may actually be simpler and faster to just stick to Power Query (instead of using Power Query and Python in Excel together)"},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Kwargs appreciation thread", "Author": "u/TheRealFrostMana", "Content": "Edit: Thanks a lot to those who pointed it out: The name of the concept in question is actually keyword-only arguments . **kwargs is lovely as well, though! I learned Python as my first language and it's the one I'm most proficient in. However, I've since written JavaScript, TypeScript, C#, and a little bit of Go. Even though each language has its own way of doing things, I find that I often miss being able to use kwargs for the sake of readability. This is what I mean: some_function(semantic_parameter_name=value1, explanatory_parameter_name=value2) Often times, the usage of kwargs is sufficiently explanatory of what the function does. Whether it's someone else's code or code that you've written a while back, not only does it save you having to peek at the function's signature/code, it also helps piece together what a block of code intends to do at first glance. At this point, for codebases that I maintain, I almost exclusively define my functions to force the usage of kwargs: kwargs_are_mandatory(*, parameter1: int, parameter2: str) -> None: return When I read code in a language that doesn't support some form of kwargs, I find it more difficult and time consuming to wrap my head around what's happening. What are your thoughts on kwargs?"},
{"Title": "Open-source AI shorts generator in python", "Author": "u/ANil1729", "Content": "I have open-sourced a Text-To-Video-AI generated which generates video from a topic by collecting relevant stock videos and stitching them together similar to popular video tools like Invideo, Pictory etc. Link to code :- https://github.com/SamurAIGPT/Text-To-Video-AI"},
{"Title": "Made a Minimalistic Router for Uvicorn", "Author": "u/achaayb", "Content": "Hey everyone, I've been working on a simple router for Uvicorn called ASGIRouter . If you like how Flask handles routing but want to stick with ASGI, you might find this useful. What My Project Does ASGIRouter provides a minimalistic routing solution for ASGI applications. It offers a straightforward way to define routes, similar to Flask, but is built to work any asgi compatible webservers mainly uvicorn. This project is aimed at developers who prefer a minimalistic approach to routing in their ASGI applications. It's suitable for both toy projects and production use, depending on your needs. Compared to existing ASGI routers, ASGIRouter stands out for its simplicity and ease of use. While other routers might offer more features or complexity, ASGIRouter focuses on providing a minimalistic, Flask-like experience for those who want to keep things straightforward. Check it out and let me know what you think."},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "A super easy-to-use API monitoring & analytics tool", "Author": "u/itssimon86", "Content": "Hey Python community! I‚Äôd like to introduce you to my indie product Apitally , a simple API monitoring & analytics tool for Python projects. What My Project Does Apitally provides insights into API traffic, errors, and performance, for the whole API, each endpoint and individual API consumers. It also monitors API uptime, alerting users when their API is down. Apitally directly integrates with various Python web frameworks (FastAPI, Django, Flask, Litestar) through middleware, which captures request & response metadata (never anything sensitive!) and asynchronously ships it to Apitally‚Äôs servers in regular intervals. The client library is open-source with the source code available on GitHub . Below is a code example, demonstrating how easy it is to set up Apitally for a FastAPI app (see complete setup guide here ): from fastapi import FastAPI from apitally.fastapi import ApitallyMiddleware app = FastAPI() app.add_middleware( ApitallyMiddleware, client_id=\"your-client-id\", env=\"dev\",  # or \"prod\" etc. ) Target Audience Engineering teams, individual developers and product owners who build, ship and maintain REST APIs in Python. Comparison The big monitoring platforms (Datadog etc.) can be a bit overwhelming & expensive, particularly for simpler use cases. So Apitally‚Äôs key differentiators are simplicity & affordability, with the goal to make it as easy as possible for users to understand usage of their APIs. I hope people here find this useful. Please let me know what you think!"},
{"Title": "TIL that selenium has opt out telemetry. what other common packages do this / similar experiences?", "Author": "u/aman6944", "Content": "While monitoring my network while doing some browser automation with selenium, I found strange traffic. After some digging I found https://github.com/SeleniumHQ/selenium/pull/13173 . Searching for SE_AVOID_STATS on google to disable this has only 7 results, and practially impossible to find. I didn't expect to see this kind of dark patterns telemetry in python packages - so yeah. Has anyone else seen this? Is this some sort of recent trend?"},
{"Title": "Textchat: TUI Single Server IRC Client", "Author": "u/rmpython", "Content": "Hello all! I have made an irc client with textual`. Source is available here: https://github.com/rmblau/textchat/ I would love any and all feedback on code quality and how it can be improved as well as people to test out the client. On first launch it will open a settings screen where you can input your user information once you hit the save button. Right now there's a bug that I'm working on resolving where that you'll have to quit the application once you enter your information and relaunch it to get it to connect. Feel free to file issues and contribute; I hope you all will find this fun and interesting! What My Project Does Only confirmed working on Linux right now.. Right now it does not support SASL, SSL, or znc. It's in alpha and can be installed from pypi. Once installed it can be ran from the cmd line with `textchat` Target Audience This is aimed at people who love irc as much as I do. Comparison There didn't seem to be any application like this so I decided to make it."},
{"Title": "Python Open-CV Tool-Chip Contact Length Calculation", "Author": "u/0akman", "Content": "Just posted a video on a case study of a Python OpenCV algo that calculates the contact length between the tool and the chip in a metalworking machining process. The images have been captured with a high-speed camera. The Python code and documentation on my GitHub: https://github.com/FrunzaDan/Tool-Chip_Contact_Length The video: https://youtu.be/bndai6SlF6E Enjoy! What My Project Does The Python algo uses Hough lines to locate the edges of the tool and the chip and calculate the distance between them. Target Audience Python OpenCV enthusiasts and people in metalworking research. Comparison I haven't seen any application like this in metalworking machining."},
{"Title": "ChatGPT hallucinated a plugin called pytest-edit. So I created it.", "Author": "u/Character-Maybe-4400", "Content": "I have several codebases with around 500+ different tests in each. If one of these tests fails, I need to spend ~20 seconds to find the right file, open it in neovim, and find the right test function. 20 seconds might not sound like much, but trying not to fat-finger paths in the terminal for this amount of time makes my blood boil. I wanted Pytest to do this for me, thought there would be a plugin for it. Google brought up no results, so I asked ChatGPT. It said there's a pytest-edit plugin that adds an --edit option to Pytest. There isn't. So I created just that. Enjoy. https://github.com/MrMino/pytest-edit Now, my issue is that I don't know if it works on Windows/Mac with VS Code / PyCharm, etc. - so if anyone would like to spend some time on betatesting a small pytest plugin - issue reports & PRs very much welcome. What My Project Does It adds an --edit option to Pytest, that opens failing test code in the user's editor of choice. Target Audience Pytest users. Comparison AFAIK nothing like this on the market, but I hope I'm wrong. Think %edit magic from IPython but for failed pytest executions."},
{"Title": "Why would anyone use pyqt if pyside exists", "Author": "u/AccordingBeyond2985", "Content": "Like the only different is in pyqt you must share the code or buy a license and in pyside you can share it whether you want to or not. Yet i still see so many videos on pyqt and not pyside"},
{"Title": "Building an HTTP Server in Python", "Author": "u/mraza007", "Content": "I have always been curious on how http servers works. Therefore, I decided to write a post on how they work and implementing a simple server in Python. Link to blog post"},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "open source CLI tool for finding out how programs work", "Author": "u/PossiblePandaYT", "Content": "What my project does Allow users to find out what frameworks, tools, engines a program / game was made in. Looks through a directory and searches for common folder structures, and file names. You can add the -d flag to do a \"deep dive\" and it will look through for strings inside of the binaries. Image Example Target Audience Anyone! Developers looking to learn how other programs were made, people who are just interested. Comparison Not sure if there are any alternatives, but another way of finding out how a program runs is looking through the names of files & folders. GitHub: https://github.com/PossiblePanda/hdiw Contributors are appreciated :), adding new frameworks, or improving the core of hdiw."},
{"Title": "Kivy School - Crowdfunding Update", "Author": "u/filipemarch", "Content": "We're excited to share that our Kivy School crowdfunding project on Kickstarter is over 50% funded, but we only have 2 days left to reach our goal! We want to show our appreciation to everyone who has supported us. Even if we don't reach 100% funding, everyone who trusted us will still receive free access to all free resources at kivyschool.com and our course on Udemy. Kivy School is an organization made by volunteers to teach others how to create Python apps using the Kivy framework and deploy them on all platforms: Android, iOS, Windows, macOS, Linux, Raspberry and on your toaster! So if you are still interested on helping Kivy School or on having free access to our Udemy course, you can risk free pledge on the crowdfunding link above before it expires. Keep an eye at Kivy School, soon we will publishing about: Hot Reload on Android Supabase integration Sentry integration Using SQLAlchemy / SQLModel / Pydantic with Kivy GPS, Bluetooth, Wi-Fi, Android Services & much more! It is Python. It is open source. And it is free. Join us at Kivy School, and let's code together!"},
{"Title": "Async Python Clarifications", "Author": "u/Cool-Focus6556", "Content": "Ok, so just so I have this straight: Asyncio runs in a single thread and uses cooperative multitasking to context switch between tasks The threading library creates threads and uses preemptive multitasking to context switch between threads Asyncio is more efficient than threading for the reasons above Both share the same CPU core/resources Multiprocessing is using additional cores to speed up CPU bound tasks So to summarize: a process can create threads and threads can create tasks Is it just me or do people confuse processes as threads or also confuses tasks as threads? This makes getting it all straight pretty confusing and so any help here to confirm what I‚Äôve learned above would be appreciated üôè"},
{"Title": "The Problems with Celery", "Author": "u/hatchet-dev", "Content": "Hey everyone - I wrote up a blog post on the problems that we've encountered using Celery: https://docs.hatchet.run/blog/problems-with-celery Our issues with the Celery project were part of the reason why we started Hatchet . Would love to hear comments or feedback!"},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Polars 1.0 will be out in a few weeks, but you can already install the pre-release!", "Author": "u/marcogorelli", "Content": "In a few weeks, Polars 1.0 will be out. How exciting! You can already try out the pre-release by running: ``` pip install -U --pre polars ``` If you encounter any bugs, you can report them to https://github.com/pola-rs/polars/issues , so they can be fixed before 1.0 comes out. Release notes: https://github.com/pola-rs/polars/releases/tag/py-1.0.0-alpha.1"},
{"Title": "Try PyCharm (30% off!) and they donate 100% to the Django Software Foundation", "Author": "u/Affectionate_Sky9709", "Content": "There's a promotion right now to try PyCharm, get a 30% discount, and 100% of what you pay goes directly to the Django Software Foundation, which maintains Django and keeps it free for everyone. https://jb.gg/2atgzm I hope this kind of post is allowed."},
{"Title": "I ported Rust's Regex Library To Python, but the time taken by the compile parameter was high.", "Author": "u/RevolutionaryPen4661", "Content": "(.venv) PS D:\\flpc> python .\\seed\\test.py Operation  | flpc (ms)  | re (ms) ---------------------------------- Compile    | 1496.18077 | 0.00000 Search     | 19.67597   | 1721.07339 Find Match | 15.62524   | 16.72506 Full Match | 15.62500   | 0.00000 Split      | 0.00000    | 1722.88108 Find All   | 3.02815    | 1660.32910 Find Iter  | 5.96547    | 1672.50776 Sub        | 0.00000    | 1548.61116 Subn       | 6.70719    | 1676.84698 Escape     | 4.87757    | 0.00000 (.venv) PS D:\\flpc> flpc is the name of the library. I named it (spelt as flacpuc). The strange thing is that why the compile time is high of flpc (rust) than of re module (implemented in Pure-Python) (it does the same thing what re.compile does in Python). The benchmark is done on: PATTERN = r'(\\w+)\\s+(\\d+)' TEXT = ''.join(choices(ascii_letters + digits, k=1000)) # choices function from random module ITERATIONS = 100 The problem is that, the python should be slow in the parameter (Regex Compile). However, the rest of parameters looks great! VERY FAST!"},
{"Title": "Vedo or PyVista?", "Author": "u/AlexTheRandomizer", "Content": "Hi guys! What are your experiences with Vedo or PyVista? Which one do you prefer? Did you have any specific issues which either of these libraries? I'm mostly interested in meshes and point clouds rendering."},
{"Title": "Sold my Python open source project to a San Francisco AI company. Now I work for them. AMA.", "Author": "u/romerio86", "Content": "About a year ago, I posted on this sub. I was terrified. I was launching a new framework. Another framework? Yes, I was crazy enough to think we needed yet another framework. Thankfully, the response was great. Many were excited to try it. Others were understandably skeptical, and respectfully asking good questions. This time, I'm posting for completely different reasons. I want to share a story. A story of which this sub, and hundreds of you, are part. It all started 2 years ago, when I was laid off from my analytics consulting job. I had a well-paying, comfortable job in the UK. Then I moved from the UK to Poland, where I live now, and continued working remotely. I was living the dream; earning a London salary while living in a place with a lower cost of living. Until it ended with a layoff. I thought, this is it. My career is dead. I didn't speak Polish properly, limiting my options. And finding another fully remote job working for the UK sounded overly optimistic at the time. Being in my mid 30s and with a family to support, I didn't want to start over again. I knew Python and data analytics quite well, and also had frontend skills I had gained throughout the years. So I thought... I need to show what I can do. I didn't have a portfolio at all; my GitHub was empty. After trying Streamlit, I thought the concept was great, but the execution wasn't. So I wrote an article on Medium, discussing how a better, faster alternative was possible. I also created a POC and shared it on GitHub. Thankfully, due to contacts at my previous job, I was able to find another remote job, working for the UK w. With even better pay. So naturally, I forgot about my portfolio-building efforts. But after a few months, an investor (VC) from Germany reached out to me. He had seen the Medium article and asked me whether I'd like to do this full time. I hesitated, but eventually decided to explore this further. I didn't need any investment though; my idea was quite simple. And to be honest, not too different from other frameworks, just faster. I had to think bigger. One day, at London Stansted Airport, while waiting to board a plane home, I decided to go for it and came up with the idea of no-code in the front, Python in the back. In other words, building the frontend using a visual editor, while allowing for full freedom in the backend using Python, and abstracting all the connectivity between. The VC liked the idea, but wasn't fully convinced about my ability to execute. He decided not to invest. But since I liked the idea and thought it could go somewhere, I decided to try building it myself, at night, after work. For 9 months, that was my reality. Nights, weekends. If my baby son would wake up, early mornings too. In May 2023, I managed to get the framework to a state I was happy with, and launched it. The response was very good. I eventually got to 1000 stars on GitHub, a milestone for any open source project. To a great extent, thanks to the support of communities such as r/python and r/opensource . Also, thanks to sites like Medium and Product Hunt. A few months later, in November 2023, the CTO of a multibillion AI company reached out to me. They wanted to acquire my framework, hire me, and build a team for me to continue developing it. I was ecstatic. He told me he'd go on a Thanksgiving break for a few days and that he'd reach out to me after. He never got back to me. Accepting that this wasn't going to happen was tough. Two weeks later, the CTO of another AI company called me, together with the CEO. They also wanted to acquire me and make me a part of their team. A smaller company, much more interesting and already quite established, with clients such as Accenture and Salesforce. But with grit and determination to win in the space of enterprise generative AI. This time, it did work out and my framework was finally acquired. Now I work for them and I lead a team focused on maintaining this open source project. Happy to answer any questions. And THANK YOU for your support r/python !!! For those curious: https://github.com/writer/writer-framework"},
{"Title": "Building AI Text-to-Video Model From Scratch", "Author": "u/FareedKhan557", "Content": "What My Project Does This project aims to create a small-scale text-to-video model that can generate videos based on text prompts. Target audience This project is designed for individuals who want to learn how to create their own text-to-video model from scratch but don't know where to start. It will provide a basic guide from beginning to end, covering everything from generating the training data to training a model and using that trained model to generate AI videos. Comparison Currently available text-to-video models require high computational power, and their complex code makes it difficult for Rookie developers to understand the practical implementation, beyond just the theory. To address this, I have created a small-scale GAN architecture, similar to text-to-video models, which can be trained on a CPU or a single T4 GPU. GitHub Code, documentation, and example can all be found on GitHub: https://github.com/FareedKhan-dev/AI-text-to-video-model-from-scratch"},
{"Title": "My Thoughts on Python in Excel", "Author": "u/fzumstein", "Content": "Hi all, it's been almost 1 year since the preview of Python in Excel has been revealed. So I wrote up a blog post pointing out what works well and what should be improved: https://www.xlwings.org/blog/my-thoughts-on-python-in-excel Here‚Äôs the TL;DR: We wanted an alternative to VBA, but got an alternative to the Excel formula language Integrating the Jupyter notebook cells inside the Excel grid was a mistake Python in Excel isn‚Äôt suitable for Python beginners nor for interactive data analysis Right now, there are too many restrictions (can‚Äôt use your own packages and can‚Äôt connect to web APIs) Here are the current use cases I see for Python in Excel: Computationally intensive things like Monte Carlo simulations AI stuff via the included packages (scikit-learn, nltk, statsmodels, imbalanced-learn, gensim) Advanced visualizations via Matplotlib/Seaborn Time-series analysis (this is one of Excel‚Äôs blind spots) Not sure about data cleaning/data analysis: since you almost certainly need Power Query, it may actually be simpler and faster to just stick to Power Query (instead of using Power Query and Python in Excel together)"},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Kwargs appreciation thread", "Author": "u/TheRealFrostMana", "Content": "Edit: Thanks a lot to those who pointed it out: The name of the concept in question is actually keyword-only arguments . **kwargs is lovely as well, though! I learned Python as my first language and it's the one I'm most proficient in. However, I've since written JavaScript, TypeScript, C#, and a little bit of Go. Even though each language has its own way of doing things, I find that I often miss being able to use kwargs for the sake of readability. This is what I mean: some_function(semantic_parameter_name=value1, explanatory_parameter_name=value2) Often times, the usage of kwargs is sufficiently explanatory of what the function does. Whether it's someone else's code or code that you've written a while back, not only does it save you having to peek at the function's signature/code, it also helps piece together what a block of code intends to do at first glance. At this point, for codebases that I maintain, I almost exclusively define my functions to force the usage of kwargs: kwargs_are_mandatory(*, parameter1: int, parameter2: str) -> None: return When I read code in a language that doesn't support some form of kwargs, I find it more difficult and time consuming to wrap my head around what's happening. What are your thoughts on kwargs?"},
{"Title": "Open-source AI shorts generator in python", "Author": "u/ANil1729", "Content": "I have open-sourced a Text-To-Video-AI generated which generates video from a topic by collecting relevant stock videos and stitching them together similar to popular video tools like Invideo, Pictory etc. Link to code :- https://github.com/SamurAIGPT/Text-To-Video-AI"},
{"Title": "Made a Minimalistic Router for Uvicorn", "Author": "u/achaayb", "Content": "Hey everyone, I've been working on a simple router for Uvicorn called ASGIRouter . If you like how Flask handles routing but want to stick with ASGI, you might find this useful. What My Project Does ASGIRouter provides a minimalistic routing solution for ASGI applications. It offers a straightforward way to define routes, similar to Flask, but is built to work any asgi compatible webservers mainly uvicorn. This project is aimed at developers who prefer a minimalistic approach to routing in their ASGI applications. It's suitable for both toy projects and production use, depending on your needs. Compared to existing ASGI routers, ASGIRouter stands out for its simplicity and ease of use. While other routers might offer more features or complexity, ASGIRouter focuses on providing a minimalistic, Flask-like experience for those who want to keep things straightforward. Check it out and let me know what you think."},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "A super easy-to-use API monitoring & analytics tool", "Author": "u/itssimon86", "Content": "Hey Python community! I‚Äôd like to introduce you to my indie product Apitally , a simple API monitoring & analytics tool for Python projects. What My Project Does Apitally provides insights into API traffic, errors, and performance, for the whole API, each endpoint and individual API consumers. It also monitors API uptime, alerting users when their API is down. Apitally directly integrates with various Python web frameworks (FastAPI, Django, Flask, Litestar) through middleware, which captures request & response metadata (never anything sensitive!) and asynchronously ships it to Apitally‚Äôs servers in regular intervals. The client library is open-source with the source code available on GitHub . Below is a code example, demonstrating how easy it is to set up Apitally for a FastAPI app (see complete setup guide here ): from fastapi import FastAPI from apitally.fastapi import ApitallyMiddleware app = FastAPI() app.add_middleware( ApitallyMiddleware, client_id=\"your-client-id\", env=\"dev\",  # or \"prod\" etc. ) Target Audience Engineering teams, individual developers and product owners who build, ship and maintain REST APIs in Python. Comparison The big monitoring platforms (Datadog etc.) can be a bit overwhelming & expensive, particularly for simpler use cases. So Apitally‚Äôs key differentiators are simplicity & affordability, with the goal to make it as easy as possible for users to understand usage of their APIs. I hope people here find this useful. Please let me know what you think!"},
{"Title": "TIL that selenium has opt out telemetry. what other common packages do this / similar experiences?", "Author": "u/aman6944", "Content": "While monitoring my network while doing some browser automation with selenium, I found strange traffic. After some digging I found https://github.com/SeleniumHQ/selenium/pull/13173 . Searching for SE_AVOID_STATS on google to disable this has only 7 results, and practially impossible to find. I didn't expect to see this kind of dark patterns telemetry in python packages - so yeah. Has anyone else seen this? Is this some sort of recent trend?"},
{"Title": "Textchat: TUI Single Server IRC Client", "Author": "u/rmpython", "Content": "Hello all! I have made an irc client with textual`. Source is available here: https://github.com/rmblau/textchat/ I would love any and all feedback on code quality and how it can be improved as well as people to test out the client. On first launch it will open a settings screen where you can input your user information once you hit the save button. Right now there's a bug that I'm working on resolving where that you'll have to quit the application once you enter your information and relaunch it to get it to connect. Feel free to file issues and contribute; I hope you all will find this fun and interesting! What My Project Does Only confirmed working on Linux right now.. Right now it does not support SASL, SSL, or znc. It's in alpha and can be installed from pypi. Once installed it can be ran from the cmd line with `textchat` Target Audience This is aimed at people who love irc as much as I do. Comparison There didn't seem to be any application like this so I decided to make it."},
{"Title": "Python Open-CV Tool-Chip Contact Length Calculation", "Author": "u/0akman", "Content": "Just posted a video on a case study of a Python OpenCV algo that calculates the contact length between the tool and the chip in a metalworking machining process. The images have been captured with a high-speed camera. The Python code and documentation on my GitHub: https://github.com/FrunzaDan/Tool-Chip_Contact_Length The video: https://youtu.be/bndai6SlF6E Enjoy! What My Project Does The Python algo uses Hough lines to locate the edges of the tool and the chip and calculate the distance between them. Target Audience Python OpenCV enthusiasts and people in metalworking research. Comparison I haven't seen any application like this in metalworking machining."},
{"Title": "ChatGPT hallucinated a plugin called pytest-edit. So I created it.", "Author": "u/Character-Maybe-4400", "Content": "I have several codebases with around 500+ different tests in each. If one of these tests fails, I need to spend ~20 seconds to find the right file, open it in neovim, and find the right test function. 20 seconds might not sound like much, but trying not to fat-finger paths in the terminal for this amount of time makes my blood boil. I wanted Pytest to do this for me, thought there would be a plugin for it. Google brought up no results, so I asked ChatGPT. It said there's a pytest-edit plugin that adds an --edit option to Pytest. There isn't. So I created just that. Enjoy. https://github.com/MrMino/pytest-edit Now, my issue is that I don't know if it works on Windows/Mac with VS Code / PyCharm, etc. - so if anyone would like to spend some time on betatesting a small pytest plugin - issue reports & PRs very much welcome. What My Project Does It adds an --edit option to Pytest, that opens failing test code in the user's editor of choice. Target Audience Pytest users. Comparison AFAIK nothing like this on the market, but I hope I'm wrong. Think %edit magic from IPython but for failed pytest executions."},
{"Title": "Why would anyone use pyqt if pyside exists", "Author": "u/AccordingBeyond2985", "Content": "Like the only different is in pyqt you must share the code or buy a license and in pyside you can share it whether you want to or not. Yet i still see so many videos on pyqt and not pyside"},
{"Title": "Building an HTTP Server in Python", "Author": "u/mraza007", "Content": "I have always been curious on how http servers works. Therefore, I decided to write a post on how they work and implementing a simple server in Python. Link to blog post"},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "open source CLI tool for finding out how programs work", "Author": "u/PossiblePandaYT", "Content": "What my project does Allow users to find out what frameworks, tools, engines a program / game was made in. Looks through a directory and searches for common folder structures, and file names. You can add the -d flag to do a \"deep dive\" and it will look through for strings inside of the binaries. Image Example Target Audience Anyone! Developers looking to learn how other programs were made, people who are just interested. Comparison Not sure if there are any alternatives, but another way of finding out how a program runs is looking through the names of files & folders. GitHub: https://github.com/PossiblePanda/hdiw Contributors are appreciated :), adding new frameworks, or improving the core of hdiw."},
{"Title": "Kivy School - Crowdfunding Update", "Author": "u/filipemarch", "Content": "We're excited to share that our Kivy School crowdfunding project on Kickstarter is over 50% funded, but we only have 2 days left to reach our goal! We want to show our appreciation to everyone who has supported us. Even if we don't reach 100% funding, everyone who trusted us will still receive free access to all free resources at kivyschool.com and our course on Udemy. Kivy School is an organization made by volunteers to teach others how to create Python apps using the Kivy framework and deploy them on all platforms: Android, iOS, Windows, macOS, Linux, Raspberry and on your toaster! So if you are still interested on helping Kivy School or on having free access to our Udemy course, you can risk free pledge on the crowdfunding link above before it expires. Keep an eye at Kivy School, soon we will publishing about: Hot Reload on Android Supabase integration Sentry integration Using SQLAlchemy / SQLModel / Pydantic with Kivy GPS, Bluetooth, Wi-Fi, Android Services & much more! It is Python. It is open source. And it is free. Join us at Kivy School, and let's code together!"},
{"Title": "Async Python Clarifications", "Author": "u/Cool-Focus6556", "Content": "Ok, so just so I have this straight: Asyncio runs in a single thread and uses cooperative multitasking to context switch between tasks The threading library creates threads and uses preemptive multitasking to context switch between threads Asyncio is more efficient than threading for the reasons above Both share the same CPU core/resources Multiprocessing is using additional cores to speed up CPU bound tasks So to summarize: a process can create threads and threads can create tasks Is it just me or do people confuse processes as threads or also confuses tasks as threads? This makes getting it all straight pretty confusing and so any help here to confirm what I‚Äôve learned above would be appreciated üôè"},
{"Title": "The Problems with Celery", "Author": "u/hatchet-dev", "Content": "Hey everyone - I wrote up a blog post on the problems that we've encountered using Celery: https://docs.hatchet.run/blog/problems-with-celery Our issues with the Celery project were part of the reason why we started Hatchet . Would love to hear comments or feedback!"},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "EFT - A file extension for implementing user created themes", "Author": "u/PossiblePandaYT", "Content": "What my project does Allow developers to implement custom themes into their programs, while having a file format that is human readable Example: my_theme.eft - My Theme background_color : 255,255,255 : Color title : \"Hi\" : String number : 5 : Int enabled : true : Bool Target Audience Developers & users who make themes Comparison CSS Themes - May be difficult to implement, difficult to understand for people who aren't programmers JSON Themes - Viable option, may not be readable in some cases GitHub: https://github.com/PossiblePanda/EFT-py Contributors are greatly appreciated :) If you have questions feel free to ask"},
{"Title": "Flappy Berd in PyQt", "Author": "u/GhnQuix", "Content": "Hello there What my project does: I‚Äôm excited to share my Flappy Bird clone, written in PyQt! This project captures all the fun of the original game with key features like pressing the spacebar to make the bird jump. Yes, I know, getting that key feature was challenging! üòÉ As Richard Watterson once said: \"10/10 game, would play again.\" Target Audience This game is for anyone who‚Äôs bored and looking for a quick, fun way to pass the time. Whether you're a casual gamer or just curious, this Flappy Bird clone is a not so good way to relive the original experience. Comparison Think of it as a faithful recreation of Flappy Bird with a PyQt twist. Update I had some time, so I made an update. The pipes now start from the middle. Code You can check out the code here . Please note that the code is definitely not the best, but hey it works!"},
{"Title": "I have made an open source library for logging errors / messages :)", "Author": "u/PossiblePandaYT", "Content": "What my project does Allow developers to easily log errors, messages, and warnings to the console, and an optional log file. Have you ever released a project, and then somebody runs your project and encounter an error, and you need to see their console? No worries, pandalog can store logs in a file wherever you choose. You can also have errors automatically be sent in the console & log through pandalog. Target Audience Developers Comparison using pandalog has many benefits over just using print , such as storing logs in a log file, colored output in console, extremely configurable You can download it on pypi by running pip install pandalog GitHub: https://github.com/PossiblePanda/pandalog Contributors are appreciated :)"},
{"Title": "Just released my first Python package: Melodica Notes üé∂", "Author": "u/bettercallsenna", "Content": "Hey everyone! I‚Äôm excited to share my first Python package: Melodica Notes . It's a CLI tool aimed at helping melodica players with musical scales, chords, and harmonics. What My Project Does: Melodica Notes helps melodica players by providing easy access to musical scales, chords, and harmonic information directly from the command line. It's designed to be a simple yet powerful tool for both beginners and advanced players. Target Audience: This project is meant for anyone who plays the melodica (or piano), from casual hobbyists to serious musicians. It's also a project for developers interested in music-related applications. While it‚Äôs fully functional, I consider it an evolving tool and welcome contributions to enhance its features. Comparison: There are other musical tools out there, but Melodica Notes is specifically tailored for melodica players. Unlike general-purpose music theory tools, this CLI focuses on the needs and nuances of melodica playing, making it a unique addition to the musician's toolkit. I‚Äôd love to hear your thoughts and suggestions! Whether it's feedback, feature ideas, or pull requests, I welcome all contributions. Your insights can help make this tool even better. Check it out on PyPI and feel free to dive into the code on GitHub . Thanks for your support, and happy coding and playing üéµ"},
{"Title": "steer - An interactive CLI tool to write json and yaml file from JSON schemas", "Author": "u/jcoelho93", "Content": "What my project does: It's an interactive tool to help you write json or yaml based on a JSON schema. I built this because I thought it would be helpful to write values.yaml files for Helm charts. But it can be used for a lot of other things like CICD configuration, OpenAPI specifications, etc. Target Audience Developers mostly, I guess Comparison I haven't seen anything similar to this. Except maybe spotlight for writing OpenAPI specs, except steer is from the command line. Code: Here's the GitHub repo https://github.com/jcoelho93/steer"},
{"Title": "Async Python adoption?", "Author": "u/mcharytoniuk", "Content": "Are there any studies, large-scale polls, or anything about async coding adoption in Python? I wonder how widely the community accepts it, how widespread its usage is, and what the general sentiment is towards it."},
{"Title": "Understanding Python Decorators", "Author": "u/ashok_tankala", "Content": "Without using decorators I think mostly we can‚Äôt build a decent application. They are everywhere. I wrote an article to get an understanding of Decorators. https://newsletter.piptrends.com/p/understanding-python-decorators I hope this will give you a good understanding of Decorators if you don't know about them."},
{"Title": "Archand: Control your mouse entirely using hand gestures.", "Author": "u/prateekvellala", "Content": "Link: https://github.com/prateekvellala/Archand What My Project Does Archand allows you to control your mouse entirely using hand gestures which are performed in the air and captured via a webcam. Archand also has a speech-to-text feature which is activated by a specific gesture, transforming your spoken words into written text on your computer. With this, you can perform any task you would normally do with a keyboard as well, such as visiting websites, writing emails, texting people, etc. Archand has the following features, each controlled by a unique hand gesture: Move pointer Single left click Single right click Double left click Hold left click and move pointer (for dragging, etc) Scroll up Scroll down Enable your microphone, and then whatever you say will be converted to text and typed where your cursor is blinking (automating keyboard functionality) Target Audience Everyone Comparison There is no comparison with any other projects, as I have not seen any that incorporate all the features I have implemented, which work accurately with both low-resolution integrated laptop webcams and high-end webcams. All the projects I've encountered with a similar concept mainly fall into three categories: They¬†don't work at¬†all, failing¬†even to¬†move¬†the¬†cursor¬†smoothly. The cursor moves pretty well and smoothly, but they do not fully automate the mouse, as they always lack some other feature like double-clicking, right-clicking, or scrolling, etc. They have many features that work well, but require high-end webcams, such as the Logitech Brio."},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "CopySave - And easy to use clipboard manager", "Author": "u/legend3008", "Content": "What my project does CopySave is an app that saves everything you copy in your clipboard locally, so it can be used later, thus saving time. Target Audience Everyone who works at a pc, with data. Programmers, especially. Comparison I couldn't find any similar applications. Of course there are some better ones out there. https://github.com/mpiele/CopySave"},
{"Title": "Seeking Feedback: Should Robyn(Web Framework) Support ASGI?", "Author": "u/stealthanthrax", "Content": "Hey Everyone¬†üëã The author of Robyn here. For those unaware, Robyn is one of the fastest Python web frameworks with a Rust runtime.Robyn offers a variety of features designed to enhance your web development experience. However, one topic that has sparked mixed feelings within the community is Robyn's choice of not supporting ASGI. I'd love to hear your thoughts on this. Specifically, what specific features of ASGI do you miss in Robyn? You can find Robyn's documentation here . We're aiming for a v1.0 release soon, and your feedback will be invaluable in determining whether introducing ASGI support should be a priority. Please avoid generic responses like \"ASGI is a standard and should be supported.\" Instead, share detailed insights and evidence-based arguments to help me understand the tangible benefits ASGI could bring to Robyn or the lack of a specific ASGI feature that will hinder you from using Robyn. Looking forward to your feedback! Thanks again. Repo - https://github.com/sparckles/Robyn/ Docs - https://robyn.tech/documentation"},
{"Title": "YouTube playlist with 100 most-watched Python 2023 conference talks", "Author": "u/TechTalksWeekly", "Content": "tldr; https://www.youtube.com/playlist?list=PLsaeJ8d49kCnv20dizZqF_EjAoAByNfMj long: Hello r/python ! As a part of Tech Talks Weekly newsletter , I've put together a list of the most watched Python conference talks from 2023 as a youtube playlist. The list is ordered by the view count for your convenience. The talks come from conferences like PyCon (all locations), PyData (all locations), EuroPython, Conf42 , and many more to give you a complete overview of the landscape. I've built the playlist as a part of my newsletter called Tech Talks Weekly where once a week I send out all the recently uploaded tech conference talks across engineering conferences ( see a recent issue and subscribe if this sounds useful). Let me know what do you think!"},
{"Title": "Introducing Zenaura, python framework for building scalable, maintainable component based SPAs.", "Author": "u/Beneficial-Ad-9243", "Content": "What My Project Does Zenaura is a cutting-edge Python library, leveraging Pyodide and PyScript, designed to empower developers to create lightweight, performant, stateful, component-based Single Page Applications (SPAs) with ease. By utilizing a virtual DOM implementation, Zenaura enhances performance, reactivity, responsiveness, and interactivity, allowing developers to build dynamic web applications using familiar Python concepts and syntax. key features Exceptional Developer Experience:¬†Intuitive and efficient development workflow. Smooth Learning Curve:¬†Easy to learn and get started. Modular Code Structure:¬†Write clean, readable, and maintainable code. Component-Based Architecture:¬†Build reusable and scalable components. Page Management:¬†Simplify page creation and navigation. Built-in Router:¬†Seamless client-side routing. State and Props Management:¬†Efficiently handle component states and properties. Dependency Injection:¬†Manage dependencies effortlessly. Global States and Components:¬†Share states and components across the application. Optimized Virtual DOM:¬†Enhance application performance with a highly efficient virtual DOM. Component Lifecycle Methods:¬†Control component behavior at different stages. Form Support:¬†Easily manage form inputs and validation. API Integration:¬†Integrate external APIs using the requests module. target Audience Python developers who want to build stateful, component based SPA using pure python. Comparison with existing SPA building libraries, frameworks: Python Integration: Leverages PyScript and Pyodide: Zenaura allows your Python code to be compiled and transpiled into WebAssembly (WASM), enabling the execution of Python in the browser. This is a significant departure from traditional JavaScript-based frameworks like React, Angular, and Vue, which rely solely on JavaScript for client-side development. Developer Ecosystem: Pythonic Development: Zenaura enables Python developers to build modern web applications without needing to switch to JavaScript, providing a seamless experience for those who are more comfortable with Python. Unified Language: By using Python for both front-end and back-end development, Zenaura reduces the context-switching overhead and allows for a more cohesive development experience. Performance and Efficiency: Virtual DOM Implementation: Similar to React and Vue, Zenaura utilizes a virtual DOM to optimize rendering performance. However, Zenaura's implementation play more well with the virtual DOM as it update the real DOM in non-blocking asyn way. Also thanks to pydide the python interpreter is ported to WASM, which means less JS footprint , very light library sizes on every library developed around zenaura. Component-Based Architecture: Stateful Components: Zenaura's component-based architecture allows for building reusable, stateful components, akin to React and Vue. This promotes code reusability and modularity. Ease of Learning and Use: Smooth Learning Curve: Zenaura offers an intuitive and straightforward learning path, especially for developers already familiar with Python. This makes it accessible and easy to adopt compared to the steeper learning curves of frameworks like Angular. Ecosystem and Community: Growing Python Ecosystem: By integrating with the Python ecosystem, Zenaura can leverage existing Python libraries and tools, providing a rich set of functionalities and a vibrant community for support and collaboration. Resources: GitHub Repository: https://github.com/ARAldhafeeri/Zenaura Landing Page: https://araldhafeeri.github.io/zenaura-landing-page/ Documentation: https://araldhafeeri.github.io/Zenaura/"},
{"Title": "[OS] Burr -- Build AI Applications/Agents as State Machines", "Author": "u/benizzy1", "Content": "Hey folks! I wanted to share Burr , an open-source project we've been working on that I'm really excited about. Target Audience Developers looking to integrate AI into their web services, or who are curious about state machines. The problem Most AI-application frameworks are overly opinionated about how to craft prompts, interact with LLMs, and store memory in a specific format. See this comment for a nice summary. The problem is they often overlook more production-critical aspects such as managing and persisting state, integrating telemetry, bringing apps to production, and seamlessly switching between human input and AI decisions. What My Project Does Our solution is to represent applications explicitly as state machines, which offers several advantages: Mentally model your system as a flowchart and directly translate it to code Execute custom hooks before/after step execution Decouple state persistence from application logic Rewind back in time/test counterfactuals (load up, fork, and debug) Query the exact (reproducible) application state at any point in time This is why we built Burr -- to make these capabilities easy and accessible. The design starts simple: define your actions as functions (or classes) and wire them together in an application. Each action reads from and writes to state, and the application orchestrates, deciding which action to delegate to next. An OS tracking UI lets you inspect the current state/get at *why* your application made a certain decision. While most people use it for LLM-based applications (where state is often complex and critical), we see potential for broader applications such as running time-series simulations, ML training, managing parallel jobs, and more. Burr is entirely dependency-free (using only the standard library), though it offers plugins that you can opt into. We've gotten some great initial traction, and would love more users and feedback. The repository has code examples + links to get started. Feel free to DM if you have any questions!"},
{"Title": "Request to journalists: no snakes", "Author": "u/qabr", "Content": "Could you please stop using photos of snakes on your articles about Python? Not only is it unimaginative, stale, and clich√©, but many of us also find it genuinely off-putting. Our passion certainly lies in coding, not necessarily in reptiles. P.S. Imagine 9 out of 10 articles on Windows featuring photos of pretty youknowwhat"},
{"Title": "Instant Python pipeline from OpenAPI spec", "Author": "u/Thinker_Assignment", "Content": "Hey folks, I work on dlt, the open source python library for turning messy jsons into clean relational tables or typed, clean parquet datasets. We recently created 2 new tools: A python-dict based REST API extractor where you can just declare how to extract, and a tool that can init the above source fully configured by reading an OpenAPI spec. The generation of the pipes is algorithmic and deterministic, not LLM based. What My Project Does dlt-init-openapi, and the REST API toolkit are tool designed to simplify the creation of data pipelines by automating the integration with APIs defined by OpenAPI specifications. The pipelines generated are customizable Python pipelines that use the REST API source template that dlt offers (a declarative python-dict first way of writing pipelines). Target Audience dlt-init-openapi is designed for data engineers, and other developers who frequently work with API data and require an efficient method to ingest and manage this data within their applications or services. It is particularly useful for those working in environments that support Python and is compatible with various operating systems, making it a versatile tool for both development and production environments. dlt's loader features automatic typing and schema evolution and processes data in microbatches to handle memory, reducing maintenance to almost nothing. Comparison Both the generation and the python declarative REST API source are new to our industry so it's hard to compare. dlt is open source and you will own your pipelines to run as you please in your existing orchestrators, as dlt is just a lightweight library that can run anywhere Python runs, including lightweight things like serverless functions. dlt is like requests + df.to_sql() on steroids, while the generator is similar to generators that create python clients for apis - which is what we basically do with extra info relevant to data engineering work (like incremental loading etc) Someone from community created a blog post comparing it to Airbyte's low code connector: https://untitleddata.company/blog/How-to-create-a-dlt-source-with-a-custom-authentication-method-rest-api-vs-airbyte-low-code More Info For more detailed information on how dlt-init-openapi works and how you can integrate it into your projects, check out the links below: GitHub Repository for the tool OpenAPI specs repository you can use Video Walkthrough Colab Demo Documentation and Quick Start Guide blog: REST API toolkit which helps understand how to edit the generated pipeline"},
{"Title": "What are the best Python projects you've worked on?", "Author": "u/NINTSKARI", "Content": "Off with the hate, what have been the best Python projects you have worked on? What did the code look like? What were the standards? Why was it the best?"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Python script to automate Bing searches for reward generation", "Author": "u/adityashrivastav", "Content": "What My Project Does (Link) Check this out : aditya-shrivastavv/ranwcopy Python program which generates random words and sentences and copy them to clipboardüóíÔ∏è. I created a script to automate Bing searches for reward generation üëç Excellent command line experience. üôÇ User friendly. üîä Produces sound so you don't have to start at it. üîÅ Auto copy to clipboardüóíÔ∏è üí° Intuitive help menu Target Audience Anyone who wants to quickly get points from bing searches under there daily limit Comparison This is no comparison, this is a very unique approch to the problem. You will find many browser extensions which claim to do the same thing, but they don't work like the search engine expects Commands Help menu ranwcopy -h #OR ranwcopy --help Start generating words (10 default with 8 seconds gap) ranwcopy Generate 20 words with 9 seconds gap ranwcopy -i 20 -g 9 # or ranwcopy --iterations 20 --timegap 9 This is a semi automatic script"},
{"Title": "I created a video on why you should be careful when using Python dictionaries as function parameter", "Author": "u/justinezila", "Content": "This talks about mutability as Changes inside a function affect the original dictionary which could lead to unexpected behaviors and hard to debug issues. Here is a link to the video https://www.youtube.com/watch?v=zTTDQePffxU"},
{"Title": "py4cli (A python library for developing scalable cli utility tools using declarative programming)", "Author": "u/Palani-SN", "Content": "py4cli (Scalable Argument Parser) Target Audience * Developers who want to develop scalable cli utility tools in python using declarative programming Comparison * Even Though Python have great libraries for passing command line arguments, those libraries aren't scalable for complex use case. So, I have developed a scalable argument parser, which not only helps in passing cli arguments, but also can alter the execution flow of the code based on arguments. * The Library have two variants minimal and moderate argument parsers, minimal can be used for creating simple cli tool, while moderate is vertically scaled version of minimal argument parser & helps in controlling execution flow of the tool in addition to routing the arguments to the respective methods. What My Project Does * The library works fine with windows & Linux supporting basic data types like int, float, str, list, dict, bool . Further developments for making the solution even more scalable is in progress. Kindly check out the project and documentation below, GitHub Link : https://github.com/Palani-SN/py4cli , * Kindly rate the project in GitHub with stars if you like PYPI Link : https://pypi.org/project/py4cli/ * Feel free to try this out with installation and usage. I am still actively developing it, so any feedback/comments would be appreciated! EDIT : How is it different than already existing tools : argparse - argparse is good in supporting different data types, but might not be able to control the flow of the code, or the arguments passed in can not be hierarchical always, which is what I term as scalability. In py4cli, the motive is to have better scalability in terms of hierarchical argument parsing. click, typer & cyclopts - Even though they support hierarchical cli arguments parsing, I feel, they rely much on decorators and its arguments more than necessary, In py4cli, the motive is to have, no extra decorators, or annotations as code, all that needs to be done is define a derived class from one of the base class provided in the lib, as per need and you can directly pass arguments to different methods of the class like how you will pass args and kwargs to a function natively. Py4Cli will be fulfilling the very basic aspects of cli interface to parse arguments, while ignoring on cli sophistication to concentrate on the scalability of the arguments passed, and in future to pass nested configuration files as inputs, with an emphasis on loosely coupled architecture. Additional Resources : docs : https://github.com/Palani-SN/py4cli/blob/main/README.md examples : https://github.com/Palani-SN/py4cli/tree/main/EXAMPLES"},
{"Title": "Lightning-Fast Text Classification with LLM Embeddings on CPU", "Author": "u/brunneis", "Content": "I'm happy to introduce fastc , a humble Python library designed to make text classification efficient and straightforward, especially in CPU environments. Whether you‚Äôre working on sentiment analysis, spam detection, or other text classification tasks, fastc is oriented for small models and avoids fine-tuning, making it perfect for resource-constrained settings. Despite its simple approach, the performance is quite good. Key Features Focused on CPU execution: Use efficient models like deepset/tinyroberta-6l-768d for embedding generation. Cosine Similarity Classification: Instead of fine-tuning, classify texts using cosine similarity between class embedding centroids and text embeddings. Efficient Multi-Classifier Execution: Run multiple classifiers without extra overhead when using the same model for embeddings. Easy Export and Loading with HuggingFace: Models can be easily exported to and loaded from HuggingFace. Unlike with fine-tuning, only one model for embeddings needs to be loaded in memory to serve any number of classifiers. https://github.com/EveripediaNetwork/fastc"},
{"Title": "Password protect Pdf using python", "Author": "u/Trinity_software", "Content": "https://youtu.be/sSPWHRpDZXo?si=b-HJ4Cu1sN-tFls1 This video explains how files ( all types) are encrypted and decrypted with PyAesCrypt module of python. Also using pypdf module , pdf files are password protected. Decryption of password protected pdf can also be done"},
{"Title": "RESTful API Hosting", "Author": "u/Loose_Read_9400", "Content": "Good morrow all, I have a simple rest api I have initially developed using Flask. This is a super low utilization app, that may receive 10-12 requests per week. Currently, I have it running a local network using my main machine as the server. This has been great for testing and development, but I need to transition to a more permanent hosting situation. I have been looking at Azure Functions and this seems like the way to go, and would fall under the free tier from what I can tell. Is this the way to go? OR Should i look at other options? This is something for work, not a personal project."},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "EFT - A file extension for implementing user created themes", "Author": "u/PossiblePandaYT", "Content": "What my project does Allow developers to implement custom themes into their programs, while having a file format that is human readable Example: my_theme.eft - My Theme background_color : 255,255,255 : Color title : \"Hi\" : String number : 5 : Int enabled : true : Bool Target Audience Developers & users who make themes Comparison CSS Themes - May be difficult to implement, difficult to understand for people who aren't programmers JSON Themes - Viable option, may not be readable in some cases GitHub: https://github.com/PossiblePanda/EFT-py Contributors are greatly appreciated :) If you have questions feel free to ask"},
{"Title": "Flappy Berd in PyQt", "Author": "u/GhnQuix", "Content": "Hello there What my project does: I‚Äôm excited to share my Flappy Bird clone, written in PyQt! This project captures all the fun of the original game with key features like pressing the spacebar to make the bird jump. Yes, I know, getting that key feature was challenging! üòÉ As Richard Watterson once said: \"10/10 game, would play again.\" Target Audience This game is for anyone who‚Äôs bored and looking for a quick, fun way to pass the time. Whether you're a casual gamer or just curious, this Flappy Bird clone is a not so good way to relive the original experience. Comparison Think of it as a faithful recreation of Flappy Bird with a PyQt twist. Update I had some time, so I made an update. The pipes now start from the middle. Code You can check out the code here . Please note that the code is definitely not the best, but hey it works!"},
{"Title": "I have made an open source library for logging errors / messages :)", "Author": "u/PossiblePandaYT", "Content": "What my project does Allow developers to easily log errors, messages, and warnings to the console, and an optional log file. Have you ever released a project, and then somebody runs your project and encounter an error, and you need to see their console? No worries, pandalog can store logs in a file wherever you choose. You can also have errors automatically be sent in the console & log through pandalog. Target Audience Developers Comparison using pandalog has many benefits over just using print , such as storing logs in a log file, colored output in console, extremely configurable You can download it on pypi by running pip install pandalog GitHub: https://github.com/PossiblePanda/pandalog Contributors are appreciated :)"},
{"Title": "Just released my first Python package: Melodica Notes üé∂", "Author": "u/bettercallsenna", "Content": "Hey everyone! I‚Äôm excited to share my first Python package: Melodica Notes . It's a CLI tool aimed at helping melodica players with musical scales, chords, and harmonics. What My Project Does: Melodica Notes helps melodica players by providing easy access to musical scales, chords, and harmonic information directly from the command line. It's designed to be a simple yet powerful tool for both beginners and advanced players. Target Audience: This project is meant for anyone who plays the melodica (or piano), from casual hobbyists to serious musicians. It's also a project for developers interested in music-related applications. While it‚Äôs fully functional, I consider it an evolving tool and welcome contributions to enhance its features. Comparison: There are other musical tools out there, but Melodica Notes is specifically tailored for melodica players. Unlike general-purpose music theory tools, this CLI focuses on the needs and nuances of melodica playing, making it a unique addition to the musician's toolkit. I‚Äôd love to hear your thoughts and suggestions! Whether it's feedback, feature ideas, or pull requests, I welcome all contributions. Your insights can help make this tool even better. Check it out on PyPI and feel free to dive into the code on GitHub . Thanks for your support, and happy coding and playing üéµ"},
{"Title": "steer - An interactive CLI tool to write json and yaml file from JSON schemas", "Author": "u/jcoelho93", "Content": "What my project does: It's an interactive tool to help you write json or yaml based on a JSON schema. I built this because I thought it would be helpful to write values.yaml files for Helm charts. But it can be used for a lot of other things like CICD configuration, OpenAPI specifications, etc. Target Audience Developers mostly, I guess Comparison I haven't seen anything similar to this. Except maybe spotlight for writing OpenAPI specs, except steer is from the command line. Code: Here's the GitHub repo https://github.com/jcoelho93/steer"},
{"Title": "Async Python adoption?", "Author": "u/mcharytoniuk", "Content": "Are there any studies, large-scale polls, or anything about async coding adoption in Python? I wonder how widely the community accepts it, how widespread its usage is, and what the general sentiment is towards it."},
{"Title": "Understanding Python Decorators", "Author": "u/ashok_tankala", "Content": "Without using decorators I think mostly we can‚Äôt build a decent application. They are everywhere. I wrote an article to get an understanding of Decorators. https://newsletter.piptrends.com/p/understanding-python-decorators I hope this will give you a good understanding of Decorators if you don't know about them."},
{"Title": "Archand: Control your mouse entirely using hand gestures.", "Author": "u/prateekvellala", "Content": "Link: https://github.com/prateekvellala/Archand What My Project Does Archand allows you to control your mouse entirely using hand gestures which are performed in the air and captured via a webcam. Archand also has a speech-to-text feature which is activated by a specific gesture, transforming your spoken words into written text on your computer. With this, you can perform any task you would normally do with a keyboard as well, such as visiting websites, writing emails, texting people, etc. Archand has the following features, each controlled by a unique hand gesture: Move pointer Single left click Single right click Double left click Hold left click and move pointer (for dragging, etc) Scroll up Scroll down Enable your microphone, and then whatever you say will be converted to text and typed where your cursor is blinking (automating keyboard functionality) Target Audience Everyone Comparison There is no comparison with any other projects, as I have not seen any that incorporate all the features I have implemented, which work accurately with both low-resolution integrated laptop webcams and high-end webcams. All the projects I've encountered with a similar concept mainly fall into three categories: They¬†don't work at¬†all, failing¬†even to¬†move¬†the¬†cursor¬†smoothly. The cursor moves pretty well and smoothly, but they do not fully automate the mouse, as they always lack some other feature like double-clicking, right-clicking, or scrolling, etc. They have many features that work well, but require high-end webcams, such as the Logitech Brio."},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "CopySave - And easy to use clipboard manager", "Author": "u/legend3008", "Content": "What my project does CopySave is an app that saves everything you copy in your clipboard locally, so it can be used later, thus saving time. Target Audience Everyone who works at a pc, with data. Programmers, especially. Comparison I couldn't find any similar applications. Of course there are some better ones out there. https://github.com/mpiele/CopySave"},
{"Title": "Seeking Feedback: Should Robyn(Web Framework) Support ASGI?", "Author": "u/stealthanthrax", "Content": "Hey Everyone¬†üëã The author of Robyn here. For those unaware, Robyn is one of the fastest Python web frameworks with a Rust runtime.Robyn offers a variety of features designed to enhance your web development experience. However, one topic that has sparked mixed feelings within the community is Robyn's choice of not supporting ASGI. I'd love to hear your thoughts on this. Specifically, what specific features of ASGI do you miss in Robyn? You can find Robyn's documentation here . We're aiming for a v1.0 release soon, and your feedback will be invaluable in determining whether introducing ASGI support should be a priority. Please avoid generic responses like \"ASGI is a standard and should be supported.\" Instead, share detailed insights and evidence-based arguments to help me understand the tangible benefits ASGI could bring to Robyn or the lack of a specific ASGI feature that will hinder you from using Robyn. Looking forward to your feedback! Thanks again. Repo - https://github.com/sparckles/Robyn/ Docs - https://robyn.tech/documentation"},
{"Title": "YouTube playlist with 100 most-watched Python 2023 conference talks", "Author": "u/TechTalksWeekly", "Content": "tldr; https://www.youtube.com/playlist?list=PLsaeJ8d49kCnv20dizZqF_EjAoAByNfMj long: Hello r/python ! As a part of Tech Talks Weekly newsletter , I've put together a list of the most watched Python conference talks from 2023 as a youtube playlist. The list is ordered by the view count for your convenience. The talks come from conferences like PyCon (all locations), PyData (all locations), EuroPython, Conf42 , and many more to give you a complete overview of the landscape. I've built the playlist as a part of my newsletter called Tech Talks Weekly where once a week I send out all the recently uploaded tech conference talks across engineering conferences ( see a recent issue and subscribe if this sounds useful). Let me know what do you think!"},
{"Title": "Introducing Zenaura, python framework for building scalable, maintainable component based SPAs.", "Author": "u/Beneficial-Ad-9243", "Content": "What My Project Does Zenaura is a cutting-edge Python library, leveraging Pyodide and PyScript, designed to empower developers to create lightweight, performant, stateful, component-based Single Page Applications (SPAs) with ease. By utilizing a virtual DOM implementation, Zenaura enhances performance, reactivity, responsiveness, and interactivity, allowing developers to build dynamic web applications using familiar Python concepts and syntax. key features Exceptional Developer Experience:¬†Intuitive and efficient development workflow. Smooth Learning Curve:¬†Easy to learn and get started. Modular Code Structure:¬†Write clean, readable, and maintainable code. Component-Based Architecture:¬†Build reusable and scalable components. Page Management:¬†Simplify page creation and navigation. Built-in Router:¬†Seamless client-side routing. State and Props Management:¬†Efficiently handle component states and properties. Dependency Injection:¬†Manage dependencies effortlessly. Global States and Components:¬†Share states and components across the application. Optimized Virtual DOM:¬†Enhance application performance with a highly efficient virtual DOM. Component Lifecycle Methods:¬†Control component behavior at different stages. Form Support:¬†Easily manage form inputs and validation. API Integration:¬†Integrate external APIs using the requests module. target Audience Python developers who want to build stateful, component based SPA using pure python. Comparison with existing SPA building libraries, frameworks: Python Integration: Leverages PyScript and Pyodide: Zenaura allows your Python code to be compiled and transpiled into WebAssembly (WASM), enabling the execution of Python in the browser. This is a significant departure from traditional JavaScript-based frameworks like React, Angular, and Vue, which rely solely on JavaScript for client-side development. Developer Ecosystem: Pythonic Development: Zenaura enables Python developers to build modern web applications without needing to switch to JavaScript, providing a seamless experience for those who are more comfortable with Python. Unified Language: By using Python for both front-end and back-end development, Zenaura reduces the context-switching overhead and allows for a more cohesive development experience. Performance and Efficiency: Virtual DOM Implementation: Similar to React and Vue, Zenaura utilizes a virtual DOM to optimize rendering performance. However, Zenaura's implementation play more well with the virtual DOM as it update the real DOM in non-blocking asyn way. Also thanks to pydide the python interpreter is ported to WASM, which means less JS footprint , very light library sizes on every library developed around zenaura. Component-Based Architecture: Stateful Components: Zenaura's component-based architecture allows for building reusable, stateful components, akin to React and Vue. This promotes code reusability and modularity. Ease of Learning and Use: Smooth Learning Curve: Zenaura offers an intuitive and straightforward learning path, especially for developers already familiar with Python. This makes it accessible and easy to adopt compared to the steeper learning curves of frameworks like Angular. Ecosystem and Community: Growing Python Ecosystem: By integrating with the Python ecosystem, Zenaura can leverage existing Python libraries and tools, providing a rich set of functionalities and a vibrant community for support and collaboration. Resources: GitHub Repository: https://github.com/ARAldhafeeri/Zenaura Landing Page: https://araldhafeeri.github.io/zenaura-landing-page/ Documentation: https://araldhafeeri.github.io/Zenaura/"},
{"Title": "[OS] Burr -- Build AI Applications/Agents as State Machines", "Author": "u/benizzy1", "Content": "Hey folks! I wanted to share Burr , an open-source project we've been working on that I'm really excited about. Target Audience Developers looking to integrate AI into their web services, or who are curious about state machines. The problem Most AI-application frameworks are overly opinionated about how to craft prompts, interact with LLMs, and store memory in a specific format. See this comment for a nice summary. The problem is they often overlook more production-critical aspects such as managing and persisting state, integrating telemetry, bringing apps to production, and seamlessly switching between human input and AI decisions. What My Project Does Our solution is to represent applications explicitly as state machines, which offers several advantages: Mentally model your system as a flowchart and directly translate it to code Execute custom hooks before/after step execution Decouple state persistence from application logic Rewind back in time/test counterfactuals (load up, fork, and debug) Query the exact (reproducible) application state at any point in time This is why we built Burr -- to make these capabilities easy and accessible. The design starts simple: define your actions as functions (or classes) and wire them together in an application. Each action reads from and writes to state, and the application orchestrates, deciding which action to delegate to next. An OS tracking UI lets you inspect the current state/get at *why* your application made a certain decision. While most people use it for LLM-based applications (where state is often complex and critical), we see potential for broader applications such as running time-series simulations, ML training, managing parallel jobs, and more. Burr is entirely dependency-free (using only the standard library), though it offers plugins that you can opt into. We've gotten some great initial traction, and would love more users and feedback. The repository has code examples + links to get started. Feel free to DM if you have any questions!"},
{"Title": "Request to journalists: no snakes", "Author": "u/qabr", "Content": "Could you please stop using photos of snakes on your articles about Python? Not only is it unimaginative, stale, and clich√©, but many of us also find it genuinely off-putting. Our passion certainly lies in coding, not necessarily in reptiles. P.S. Imagine 9 out of 10 articles on Windows featuring photos of pretty youknowwhat"},
{"Title": "Instant Python pipeline from OpenAPI spec", "Author": "u/Thinker_Assignment", "Content": "Hey folks, I work on dlt, the open source python library for turning messy jsons into clean relational tables or typed, clean parquet datasets. We recently created 2 new tools: A python-dict based REST API extractor where you can just declare how to extract, and a tool that can init the above source fully configured by reading an OpenAPI spec. The generation of the pipes is algorithmic and deterministic, not LLM based. What My Project Does dlt-init-openapi, and the REST API toolkit are tool designed to simplify the creation of data pipelines by automating the integration with APIs defined by OpenAPI specifications. The pipelines generated are customizable Python pipelines that use the REST API source template that dlt offers (a declarative python-dict first way of writing pipelines). Target Audience dlt-init-openapi is designed for data engineers, and other developers who frequently work with API data and require an efficient method to ingest and manage this data within their applications or services. It is particularly useful for those working in environments that support Python and is compatible with various operating systems, making it a versatile tool for both development and production environments. dlt's loader features automatic typing and schema evolution and processes data in microbatches to handle memory, reducing maintenance to almost nothing. Comparison Both the generation and the python declarative REST API source are new to our industry so it's hard to compare. dlt is open source and you will own your pipelines to run as you please in your existing orchestrators, as dlt is just a lightweight library that can run anywhere Python runs, including lightweight things like serverless functions. dlt is like requests + df.to_sql() on steroids, while the generator is similar to generators that create python clients for apis - which is what we basically do with extra info relevant to data engineering work (like incremental loading etc) Someone from community created a blog post comparing it to Airbyte's low code connector: https://untitleddata.company/blog/How-to-create-a-dlt-source-with-a-custom-authentication-method-rest-api-vs-airbyte-low-code More Info For more detailed information on how dlt-init-openapi works and how you can integrate it into your projects, check out the links below: GitHub Repository for the tool OpenAPI specs repository you can use Video Walkthrough Colab Demo Documentation and Quick Start Guide blog: REST API toolkit which helps understand how to edit the generated pipeline"},
{"Title": "What are the best Python projects you've worked on?", "Author": "u/NINTSKARI", "Content": "Off with the hate, what have been the best Python projects you have worked on? What did the code look like? What were the standards? Why was it the best?"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Python script to automate Bing searches for reward generation", "Author": "u/adityashrivastav", "Content": "What My Project Does (Link) Check this out : aditya-shrivastavv/ranwcopy Python program which generates random words and sentences and copy them to clipboardüóíÔ∏è. I created a script to automate Bing searches for reward generation üëç Excellent command line experience. üôÇ User friendly. üîä Produces sound so you don't have to start at it. üîÅ Auto copy to clipboardüóíÔ∏è üí° Intuitive help menu Target Audience Anyone who wants to quickly get points from bing searches under there daily limit Comparison This is no comparison, this is a very unique approch to the problem. You will find many browser extensions which claim to do the same thing, but they don't work like the search engine expects Commands Help menu ranwcopy -h #OR ranwcopy --help Start generating words (10 default with 8 seconds gap) ranwcopy Generate 20 words with 9 seconds gap ranwcopy -i 20 -g 9 # or ranwcopy --iterations 20 --timegap 9 This is a semi automatic script"},
{"Title": "I created a video on why you should be careful when using Python dictionaries as function parameter", "Author": "u/justinezila", "Content": "This talks about mutability as Changes inside a function affect the original dictionary which could lead to unexpected behaviors and hard to debug issues. Here is a link to the video https://www.youtube.com/watch?v=zTTDQePffxU"},
{"Title": "py4cli (A python library for developing scalable cli utility tools using declarative programming)", "Author": "u/Palani-SN", "Content": "py4cli (Scalable Argument Parser) Target Audience * Developers who want to develop scalable cli utility tools in python using declarative programming Comparison * Even Though Python have great libraries for passing command line arguments, those libraries aren't scalable for complex use case. So, I have developed a scalable argument parser, which not only helps in passing cli arguments, but also can alter the execution flow of the code based on arguments. * The Library have two variants minimal and moderate argument parsers, minimal can be used for creating simple cli tool, while moderate is vertically scaled version of minimal argument parser & helps in controlling execution flow of the tool in addition to routing the arguments to the respective methods. What My Project Does * The library works fine with windows & Linux supporting basic data types like int, float, str, list, dict, bool . Further developments for making the solution even more scalable is in progress. Kindly check out the project and documentation below, GitHub Link : https://github.com/Palani-SN/py4cli , * Kindly rate the project in GitHub with stars if you like PYPI Link : https://pypi.org/project/py4cli/ * Feel free to try this out with installation and usage. I am still actively developing it, so any feedback/comments would be appreciated! EDIT : How is it different than already existing tools : argparse - argparse is good in supporting different data types, but might not be able to control the flow of the code, or the arguments passed in can not be hierarchical always, which is what I term as scalability. In py4cli, the motive is to have better scalability in terms of hierarchical argument parsing. click, typer & cyclopts - Even though they support hierarchical cli arguments parsing, I feel, they rely much on decorators and its arguments more than necessary, In py4cli, the motive is to have, no extra decorators, or annotations as code, all that needs to be done is define a derived class from one of the base class provided in the lib, as per need and you can directly pass arguments to different methods of the class like how you will pass args and kwargs to a function natively. Py4Cli will be fulfilling the very basic aspects of cli interface to parse arguments, while ignoring on cli sophistication to concentrate on the scalability of the arguments passed, and in future to pass nested configuration files as inputs, with an emphasis on loosely coupled architecture. Additional Resources : docs : https://github.com/Palani-SN/py4cli/blob/main/README.md examples : https://github.com/Palani-SN/py4cli/tree/main/EXAMPLES"},
{"Title": "Lightning-Fast Text Classification with LLM Embeddings on CPU", "Author": "u/brunneis", "Content": "I'm happy to introduce fastc , a humble Python library designed to make text classification efficient and straightforward, especially in CPU environments. Whether you‚Äôre working on sentiment analysis, spam detection, or other text classification tasks, fastc is oriented for small models and avoids fine-tuning, making it perfect for resource-constrained settings. Despite its simple approach, the performance is quite good. Key Features Focused on CPU execution: Use efficient models like deepset/tinyroberta-6l-768d for embedding generation. Cosine Similarity Classification: Instead of fine-tuning, classify texts using cosine similarity between class embedding centroids and text embeddings. Efficient Multi-Classifier Execution: Run multiple classifiers without extra overhead when using the same model for embeddings. Easy Export and Loading with HuggingFace: Models can be easily exported to and loaded from HuggingFace. Unlike with fine-tuning, only one model for embeddings needs to be loaded in memory to serve any number of classifiers. https://github.com/EveripediaNetwork/fastc"},
{"Title": "Password protect Pdf using python", "Author": "u/Trinity_software", "Content": "https://youtu.be/sSPWHRpDZXo?si=b-HJ4Cu1sN-tFls1 This video explains how files ( all types) are encrypted and decrypted with PyAesCrypt module of python. Also using pypdf module , pdf files are password protected. Decryption of password protected pdf can also be done"},
{"Title": "RESTful API Hosting", "Author": "u/Loose_Read_9400", "Content": "Good morrow all, I have a simple rest api I have initially developed using Flask. This is a super low utilization app, that may receive 10-12 requests per week. Currently, I have it running a local network using my main machine as the server. This has been great for testing and development, but I need to transition to a more permanent hosting situation. I have been looking at Azure Functions and this seems like the way to go, and would fall under the free tier from what I can tell. Is this the way to go? OR Should i look at other options? This is something for work, not a personal project."},
{"Title": "The Python on Microcontrollers (and Raspberry Pi) Newsletter, a weekly news and project resource", "Author": "u/HP7933", "Content": "The Python on Microcontrollers (and Raspberry Pi) Newsletter: subscribe for free With the Python on Microcontrollers newsletter, you get all the latest information on Python running on hardware in one place! MicroPython, CircuitPython and Python on single Board Computers like Raspberry Pi & many more. The Python on Microcontrollers newsletter is the place for the latest news. It arrives Monday morning with all the week‚Äôs happenings. No advertising, no spam, easy to unsubscribe. 11,023 subscribers - the largest Python on hardware newsletter out there. Catch all the weekly news on Python for Microcontrollers with adafruitdaily.com. This ad-free, spam-free weekly email is filled with CircuitPython, MicroPython, and Python information that you may have missed, all in one place! Ensure you catch the weekly Python on Hardware roundup‚Äì you can cancel anytime ‚Äì try our spam-free newsletter today! https://www.adafruitdaily.com/"},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Fields and class properties should be sorted alphabetically?", "Author": "u/Myterro", "Content": "Hello, I'm having code-review suggestion doubts about sorting alphabetically fields in classes, e.g. Pydantic models. For example, there's a model: class Example(BaseModel): id: int name: str surname: str age: int operation: str One of developers suggests that fields should be sorted alphabetically: class Example(BaseModel): age: int id: int name: str operation: str surname: str I think there shouldn't be any specific order but only developer' subjective look at importance and connection between fields, like \"name\" and \"surname\" should be next to each other because they are in some way connected. What is your opinion? Maybe there are some PEP8 rules about that?"},
{"Title": "Tuples Are Underrated! List vs Tuple üêç", "Author": "u/JosephLovesPython", "Content": "Do you feel like you're underutilizing tuples in you code? Maybe cause you think lists are always the correct choice, and tuples don't have a place to exist. In this video we will walk through the differences between lists and tuples, especially focusing on a difference very rarely discussed, albeit it being the most crucial one: the semantic. Following that we will elaborate how and when it is better to utilize either lists or tuples! Any feedback on the content would be highly appreciated ‚ò∫Ô∏è https://youtu.be/-sO4FG6W4ho"},
{"Title": "CMake configs for Python modules (Pytest, Sphinx, ‚Ä¶)", "Author": "u/buddly27", "Content": "As Python is one of the most popular languages, many C++ projects end up using Python bindings of some sort. Pytest and Sphinx are very popular frameworks , so many CMake modules have been written, and most projects end up including a copy of these modules or using some hardcoded paths . I wrote two Python packages to manage the installation and update of CMake configs for Pytest and Sphinx. https://github.com/python-cmake/pytest-cmake https://github.com/python-cmake/sphinx-cmake It uses the pip package management, providing a module for each package and automatically generating a configuration based on the package version found. > pip install pytest-cmake > pip install sphinx-cmake I hope this method can standardize module integration for common Python tools. Let me know what you think!"},
{"Title": "Polars news: Faster CSV writer, dead expr elimination optimization, hiring engineers.", "Author": "u/commandlineluser", "Content": "Details about added features in the releases of Polars 0.20.17 to Polars 0.20.31 https://pola.rs/posts/polars-in-aggregate-jun24/"},
{"Title": "Code review for my simple project", "Author": "u/Mews75", "Content": "I've made this simple little package to stretch out audios https://github.com/Mews/simpleaudiostretch However I'm still new to uploading packages to pypi and doing documentation and the sorts, so I'd appreciate it if someone could review my project and see if what I'm doing are the best practices. Thank you in advance if anyone is willing to help"},
{"Title": "Wave Equation Solver in Python", "Author": "u/salastrodaemon", "Content": "Hello r/Python , What My Project Does I wanted to share a Python project I've been working on called WavePDE. WavePDE is a simulation and animation tool for studying wave equations in one or two dimensions. It's a handy tool for anyone interested in wave phenomena, also it's customizable and interactive. You can adjust domain size, grid resolution, wave speed, time step, boundary conditions (Dirichlet or Neumann), initial conditions, and more. Additionally, it is possible save your simulations as video files for further analysis or presentations. Target Audience I mainly created this tool while working on my research project. It is not yet complete since it deadens heavily on some parts I still didn't finish. It is about numeric computations of the wave equation on arbitrary boundaries. So I still need to apply some mask on these results and extend the Neumann conditions beyond the current implementation. Comparison This tool is way more customizable (at least imho) than other Python tools I found online. The code is more structured allowing for future extensibility. I also tried to make it as user-friendly as possible. I hope you find it useful and I would appreciate any feedback you might have. I still didn't implement tests, so if you find any bugs please let me know. Also, the documentation is lacking, but I'm working on it. You can find the code on GitHub: https://github.com/salastro/wavepde"},
{"Title": "Tutorial: How To Create Professional Python Shiny Dashboards In A Jiffy", "Author": "u/jgloewen", "Content": "The Python Shiny library is a framework for building interactive web applications in Python. Developed by RStudio, the same team behind the Shiny library for R, this library is particularly useful for data scientists and analysts who want to build interactive dashboards and applications without having extensive front-end development skills. All that is needed is knowledge of the Shiny user interface Application Programming Interface (API). Python Shiny can be used to develop applications that allow users to interact with data in real time. Data scientists can quickly prototype data applications and share them with anyone. How easy is it to use? Let‚Äôs use a simple data set and a basic interactive data visualization to take it for a test drive. Free article HERE ."},
{"Title": "PSA: PySimpleGUI has deleted [almost] all old LGPL versions from PyPI; update your dependencies", "Author": "u/ManyInterests", "Content": "Months ago, PySimpleGUI relicensed from LGPL3 to a proprietary license/subscription model with the release of version 5 and nuked the source code and history from GitHub. Up until recently, the old versions of PySimpleGUI remained on PyPI. However, all but two of these have been deleted and those that remain are yanked . The important effect this has had is anyone who may have defined their requirements as something like PySimpleGUI<5 or PySimpleGUI==4.x.x for a now-deleted version, your installations will fail with a message like: ERROR: No matching distribution found for pysimplegui<5 If you have no specific version requested for PySimpleGUI you will end up installing the version with a proprietary license and nagware. There are three options to deal with this without compeltely changing your code: Specify the latest yanked, but now unsupported version of PySimpleGUI PySimpleGUI==4.60.5 and hope they don't delete that some time in the future Use the supported LGPL fork, FreeSimpleGUI (full disclosure, I maintain this fork) Pay up for a PySimpleGUI 5 license."},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Tach - enforce module boundaries + deps, now in Rust ü¶Ä", "Author": "u/the1024", "Content": "https://github.com/gauge-sh/tach Hey everyone! Wanted to share some pretty significant updates to the tool I've been working on. Tach lets you define module boundaries and enforce rules across your modules, including isolation, dependencies, and strict interfaces. Some updates - Re-wrote the core in Rust, leading to a ~19x speed up on large repos Re-worked the interface, and added a TUI to let you interactively declare modules We built Tach to solve the ‚Äúball of mud‚Äù problem that we‚Äôve ran into throughout all of my previous work experiences. Over time, the codebase would become tightly coupled together, making even simple changes/refactors painful. By setting up module boundaries and enforcing them early on, you can avoid all of this! Tach is the best way to grow a modular monolith without creating a ball of mud. If anyone has any questions or feedback, I‚Äôd love chat! https://github.com/gauge-sh/tach What My Project Does Tach enables you to interactively declare module boundaries, dependencies between modules, and strict interfaces for those modules. You can then enforce those declarations through a static code check. Target Audience Teams maintaining python monorepos. Comparison Import linter is probably the most similar tool - for a github discussion on the differences, check out this link - https://github.com/gauge-sh/tach/discussions/72"},
{"Title": "New Lands RPG (Play testers welcome)", "Author": "u/Littlebudddy_321", "Content": "Good \"time of day\" my fellow peeps What my project does: I wanted to share my Python game I've been slowly working on over the past... I'd say 1.5 years. It is a simple texted based resource collection game where you travel to different areas, collect resources, sell them in town but be careful there are bandits about, so don't go too far without having some cooked fish on you... Target Audience: I'd say its mainly for well... everyone, anyone who enjoys text based games and anyone who wants to chill out on a rainy day when all the other games in their steam library are looking boring and they just want to relax... Comparison: I'd say Colossal cave adventure but that is a much bigger... better... game I would call it, but this is just a simple \"learning python\" project I started a while ago and just recently got back into it so I said what the heck why not finish the game. but now I'm stuck as to what to do next, so I thought I'd ask for play testers to come and tell me how bad my coding and game was so I could try and make it more playable... because lord knows I made it so I know how to play it but what about other people. you can find the code on GitHub: https://github.com/littlebudddy321/New-Lands-RPG"},
{"Title": "Bayesian bandits item pricing in a simplified Moonlighter shop simulation using Python and SQLite", "Author": "u/JaggedParadigm", "Content": "What My Project Does: Moonlighter is a game that includes a mechanic where you place items on shelves in your store and set the price. Customer's reactions give you hints about what prices would be ideal. These reactions take the form of four moods: ecstatic: price too low so they are extra happy content: price is what they were expecting, sad: price is too high to them but they buy anyway and this lowers the price everyone will pay for a certain period angry: price is too high so they don't buy I built a simplified version where a sad reaction doesn't lower the prices customers will accept for that item using Python and SQLite. The Bayesian bandits algorithm is an algorithm to optimize rewards when choosing among different options. The probability of different rewards (e.g. revenue) is kept track of and updated as rewards for options are collected. When a new option is to be selected a competition occurs where the rewards are sampled from these probability distributions and the option with the highest reward is chosen. For this simulation, the reward distributions are the probability that a price is the ideal price for that item. This scenario is so simple that the probability of any particular ideal price is flat or the same for all prices between an upper and lower bound and zero outside. This makes item/price selection simply randomly selecting a price from the lower to upper bounds for every item and selecting the item with the highest price. Customer reaction moods update the item upper/lower price bounds in these ways: ecstatic or content: lower bound is set to price plus 1 gold sad: lower bound is set to price if upper and lower bounds don't match angry: upper bound set to price minus 1 gold if the upper and lower bounds don't match The SQLite database keeps track of items in your inventory, items on shelves, customer reactions, item price bounds, and Thompson competitions (i.e. prices randomly chosen between price bounds for each item). The algorithm ended up identifying groups of items with the same ideal prices and selling them off from highest to lowest. For the full write up and a lot of pretty graphs check out the article in the link below. I've also included the Github link for those that want to see the full implementation and/or a Jupyter notebook where I generate the plots. Full write-up: https://cmshymansky.com/MoonlighterBayesianBanditsPricing/?source=rPython Github: https://github.com/JaggedParadigm/moonlighter_bayesian_bandit_pricing Target Audience: This a toy, though the Thompson sampling code could be hacked into something useful. Comparison: To my knowledge, I am the first to apply the Bayesian bandits algorithm to a Moonlighter shop simulation. However, pricing via Bayesian bandits is a classic application and there are many blogs and scientific papers on the topic."},
{"Title": "Mesop, open-source Python UI framework used at Google to quickly build delightful web apps", "Author": "u/wwwillchen", "Content": "What my project does: I‚Äôm excited to share about Mesop - a new, open-source Python UI framework that enables Python developers to quickly build delightful web apps in a scalable way. A small team of us at Google have been developing Mesop as an unofficial 20% project for the past few months. A wide range of research and product teams at Google have been using it to rapidly build internal apps and we‚Äôve gotten a lot of positive feedback internally so now we‚Äôre looking to get feedback from the open-source community. Target audience: Python developers looking to build AI demos & internal apps. Comparison: We think that Mesop provides a unique approach to building web UIs in Python compared to existing alternatives like Streamlit and Gradio - making it both easy to get started and also flexible enough to build customized UIs for a wide range of use cases. You can learn more about why we built Mesop here . To look at some example Mesop apps, check out our demo gallery . Also, the demo gallery itself is built with Mesop which demonstrates the type of flexibility you have in building apps with Mesop. GitHub repo: https://github.com/google/mesop"},
{"Title": "Dask DataFrame is Fast Now!", "Author": "u/phofl93", "Content": "My colleagues and I have been working on making Dask fast. It‚Äôs been fun. Dask DataFrame is now 20x faster and ~50% faster than Spark (but it depends a lot on the workload). I wrote a blog post on what we did: https://docs.coiled.io/blog/dask-dataframe-is-fast.html Really, this came down not to doing one thing really well, but doing lots of small things ‚Äúpretty good‚Äù. Some of the most prominent changes include: Apache Arrow support in pandas Better shuffling algorithm for faster joins Automatic query optimization There are a bunch of other improvements too like copy-on-write for pandas 2.0 which ensures copies are only triggered when necessary, GIL fixes in pandas, better serialization, a new parquet reader, etc. We were able to get a 20x speedup on traditional DataFrame benchmarks. I‚Äôd love it if people tried things out or suggested improvements we might have overlooked. Blog post: https://docs.coiled.io/blog/dask-dataframe-is-fast.html"},
{"Title": "Automate your WordPress new-page creation with Python", "Author": "u/EngineeringFit5761", "Content": "Hi everyone! I'm sharing with you a Python tool I've built and been using, intended to haste new-page creation in WordPress (with Elementor plugin). It's a simple app, but has great expansion potential and it's really easy to use. To start, you will previously need a WordPress site with Elementor installed and activated, and the content you want to introduce into the new page. Run the program, add sections, choose your desired structure, and select the right widgets for your content. Once you've loaded the content, add your credentials and click Confirm & Run (or just press Enter). The system will do the rest :) You can download and see the project at: https://github.com/MauBorre/WordPress-new-page-auto Hope you find it useful! üòÅ"},
{"Title": "Granian 1.4 is out", "Author": "u/gi0baro", "Content": "Granian ‚Äì the Rust HTTP server for Python applications ‚Äì 1.4 was released! Blog post: https://polar.sh/emmett-framework/posts/granian-1-4 Release details: https://github.com/emmett-framework/granian/releases/tag/v1.4.0 Repo: https://github.com/emmett-framework/granian"},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "My little ChatGPT-Multimodal Server Starter", "Author": "u/TwistedMinda", "Content": "Please check out my Desktoppy Server . What My Project Does It allows you to run your own personal AI on your computer, say bye-bye rate-limits and paywalls from mainstream AI's. It uses ollama internally so you can use all the open-source Models but by default it's using: LLama3 for text-generation LLava for image recognition Stable Diffusion 2 for image generation Target Audience Perfect for new-comers... I wish I had this when I started tackling AI dev. I think it can be a good base to create your awesome AI-powered products! Please let me know what you think about it! Comparison It differentiates from the other zillion starters by being very basic, allowing for full customization, and joining the 3 models together into 1 for a multi-modal feeling. Easiest possible setup, even for those who don't know the tools yet, all you need is Python3 installed on your PC. Basically a tutorial-starter-multimodal. Much love Link: https://github.com/TwistedMinda/desktoppy-server The very basic Web UI that goes along with it: https://github.com/TwistedMinda/desktoppy-web"},
{"Title": "Ludic Update: Web Apps in pure Python with HTMX, Themes, Component Catalog, new Documentation", "Author": "u/pdcz", "Content": "Hi everyone, I'd like to share couple of news regarding my personal project: New documentation written in Ludic showcasing it's capabilities: https://getludic.dev/docs/ New section regrading Layouts inspired from the Every Layout Book: https://getludic.dev/catalog/layouts Cookiecutter template to get quickly started: https://github.com/paveldedik/ludic-template I have a lot of plans with this project and I'd appreciate any feedback. About The Project Ludic allows web development in pure Python with components. It uses HTMX to add UI interactivity and has a catalog of components. Target Audience Web developers People who want to build HTML pages in Python with typing People without knowledge of JavaScript who want to build interactive UIs People who want to use HTMX in their projects Comparison With Similar Tools Feature Ludic FastUI Reflex HTML rendering Server Side Client Side Client Side Uses Template Engine No No No UI interactivity </> htmx React React Backend framework Starlette FastAPI FastAPI Client-Server Communication HTML + REST JSON + REST WebSockets Any feedback is highly appreciated."},
{"Title": "Python's many command-line utilities", "Author": "u/treyhunner", "Content": "Python 3.12 comes bundled with 50 command-line tools. For example, python -m webbrowser http://example.com opens a web browser, python -m sqlite3 launches a sqlite prompt, and python -m ast my_file.py shows the abstract syntax tree for a given Python file. I've dug into each of them and categorized them based on their purpose and how useful they are. Python's many command-line tools"},
{"Title": "Notion2Pandas: A new python package to import Notion Database into Pandas framework and viceversa", "Author": "u/Jaeger1987", "Content": "What My Project Does Hello everyone! I've just released a new Python package, notion2pandas, which allows you to import a Notion database into a pandas dataframe with just one line of code, and to update a Notion database from a pandas dataframe also with just one line of code. Target Audience Whether you're a data scientist, a data engineer, a Python enthusiast, or just curious, 'pip install notion2pandas' from the terminal, follow the tutorial in the README, and happy coding! üîó GitLab repo: https://gitlab.com/Jaeger87/notion2pandas Key Features Easy to use . import in a single line of code, export with another single line of code No more boring parsing . You can import any Notion Database in a pandas framework Flexibility . If you don't like the default parsing mode of a data provided by notion2pandas, you can use your own parse function for a specific kind of data. Maintainability . If Notion broke something with an update, the possibility to provide a different parsing function allows you to use Notion2Pandas even if it's not updated with latest notion update. Quick Start In the ReadMe you can find everything you need to start. Comparison When I started this project, I couldn't find anything capable of transforming a Notion database into a pandas DataFrame without specifying how to parse the data. If you got any kind of feedback I'm really curious to read it!"},
{"Title": "Rate Limiting + Multiprocessing = Nightmare? But I think I've found one nice way to do it ü§û", "Author": "u/VoyZan", "Content": "If you're interested in Python multiprocessing, I'd appreciate if you read this and share your thoughts: tl;dr: I've implemented a cross-process request rate limiter, allowing for N requests per T seconds. See it in this Gist . Problem Request rate limiting (or throttling) requires a place in memory to track the the amount of calls already made - some kind of counter . Multiprocessing is not great at having a single shared variable. I have a use case for a multiprocessing system in which each process can make a number of requests to a REST API server. That server imposes a 1000 requests per minute limit. Hence I needed a way to implement a rate limiter that would work across processes and threads. I've spent the past 2 days digging through a ton of SO posts and articles suggesting how to do it, and I came at a few bad solutions. I finally came up with one that I think works quite well. It uses a multiprocessing.Manager , and its Value , Lock and Condition proxies. Solution I've created a CrossProcessThrottle class which stores that counter . The way that the information about the counter is shared with all the processes and threads is through a ThrottleBarrier class instance. Its wait method will do the following: def wait(self): with self._condition: self._condition.wait() with self._lock: self._counter.value += 1 Wait for the shared Condition - this will stop all the processes and their threads and keep them dormant. If the CrossProcessThrottle calculates that we have available requests (ie. the counter is below max_requests , so we don't need to limit the requests), it uses Condition.notify(n) ( docs ) in order to let n amount of threads through and carry out the request. Once approved, each process/thread will bump the shared Value , indicating that a new request was made. That Value is then used by the CrossProcessThrottle to figure out how many requests have been made since the last check, and adjust its counter . If counter is equal or greater than max_requests , the Condition will be used to stop all processes and threads, until enough time passes. The following is the example code using this system. You can find it in this Gist if you prefer. import datetime from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor from ratelimiter import ThrottleBarrier, CrossProcessesThrottle def log(*args, **kwargs): print(datetime.datetime.now().strftime('[%H:%M:%S]'), *args, **kwargs) def task(i, j, throttle_barrier: ThrottleBarrier): # This will block until there is a free slot to make a request throttle_barrier.wait() log(f'request: {i:2d}, {j:2d}  (process, thread)') # make the request here... def worker(i, throttle_barrier: ThrottleBarrier): # example process worker, starting a bunch of threads with ThreadPoolExecutor(max_workers=5) as executor: for j in range(5): executor.submit(task, i, j, throttle_barrier) if __name__ == '__main__': cross_process_throttle = CrossProcessesThrottle(max_requests=3, per_seconds=10) throttle_barrier = cross_process_throttle.get_barrier() log('start') futures = [] # schedule 9 jobs, which should exceed our limit of 3 requests per 10 seconds with ProcessPoolExecutor(max_workers=10) as executor: for i in range(3): futures.append(executor.submit(worker, i, throttle_barrier)) while len(futures): # calling this method carries out the rate limit calculation cross_process_throttle.cycle() for future in futures: if future.done(): futures.remove(future) log('finish') I've uploaded the source code for CrossProcessThrottle and ThrottleBarrier as a Gist too . Calculating the counter is a bit more code, so I refrain from sharing it here, but in a nutshell: Store the last amount of requests made as last_counter , initialised as 0 Every time the cycle() is called, compare the difference between the current counter and the last_counter The difference is how many requests have been made since the last check, hence we increment the counter by that many. We calculate how many calls remaining are allowed: remaining_calls = max_requests - counter And notify that many threads to go ahead and proceed: condition.notify(remaining_calls) The actual process is a little more involved, as at the step 3 we need to store not only the amount of calls made, but also the times they've been made at - so that we can be checking against these later and decrease the counter . You can see it in detail in the Gist . If you've read through the code - what are your thoughts? Am I missing something here? In my tests it works out pretty nicely, producing: [14:57:26] start [14:57:26] Calls in the last 10 seconds: current=0 :: remaining=3 :: total=0 :: next slot in=0s [14:57:27] request:  0,  1  (process, thread) [14:57:27] request:  0,  0  (process, thread) [14:57:27] request:  0,  2  (process, thread) [14:57:31] Calls in the last 10 seconds: current=3 :: remaining=0 :: total=3 :: next slot in=7s [14:57:36] Calls in the last 10 seconds: current=3 :: remaining=0 :: total=3 :: next slot in=2s [14:57:38] request:  0,  4  (process, thread) [14:57:38] request:  0,  3  (process, thread) [14:57:38] request:  1,  0  (process, thread) [14:57:41] Calls in the last 10 seconds: current=3 :: remaining=0 :: total=6 :: next slot in=7s [14:57:46] Calls in the last 10 seconds: current=3 :: remaining=0 :: total=6 :: next slot in=2s [14:57:48] request:  2,  0  (process, thread) [14:57:48] request:  1,  1  (process, thread) [14:57:48] request:  1,  2  (process, thread) [14:57:51] Calls in the last 10 seconds: current=3 :: remaining=0 :: total=9 :: next slot in=8s [14:57:56] Calls in the last 10 seconds: current=3 :: remaining=0 :: total=9 :: next slot in=3s [14:57:59] request:  2,  4  (process, thread) [14:57:59] request:  2,  2  (process, thread) [14:57:59] request:  2,  1  (process, thread) [14:58:01] Calls in the last 10 seconds: current=3 :: remaining=0 :: total=12 :: next slot in=8s [14:58:06] Calls in the last 10 seconds: current=3 :: remaining=0 :: total=12 :: next slot in=3s [14:58:09] request:  1,  3  (process, thread) [14:58:09] request:  1,  4  (process, thread) [14:58:09] request:  2,  3  (process, thread) [14:58:10] finish I've also tested it with 1000s scheduled jobs to 60 processes, each spawning several threads, each of which simulates a request. The requests are limited as expected, up to N per T seconds. I really like that I can construct a single ThrottleBarrier instance that can be passed to all processes and simply call the wait method to get permission for a request. It feels like an elegant solution. Research There are a bunch of libraries for rate limiting, some claiming to support multiprocess, however I couldn't get them to do so: https://pypi.org/project/ratelimit/ https://pypi.org/project/ratelimiter/ https://pypi.org/project/ratemate/ https://github.com/JWCook/requests-ratelimiter There's a few SO threads and posts discussing the process too, however they either don't consider multiprocessing, or when they do they don't allow using ProcessPoolExecutor : https://stackoverflow.com/questions/69306420/rate-limit-api-multi-process https://stackoverflow.com/questions/40748687/python-api-rate-limiting-how-to-limit-api-calls-globally https://gist.github.com/justinvanwinkle/d9f04950083c4554835c1a35f9d22dad https://stackoverflow.com/questions/6920858/interprocess-communication-in-python The issue with ProcessPoolExecutor comes up when you try to use shared resources as it raises an error along the lines of: Synchronized objects should only be shared between processes through inheritance And to be fair the Googling didn't really help me figuring out how to get around it, just finding more people struggling with the issue: https://stackoverflow.com/questions/69907453/lock-objects-should-only-be-shared-between-processes-through-inheritance https://github.com/python/cpython/issues/79967#issuecomment-1455216546 The solution would be to not use the ProcessPoolExecutor but that was a bummer. This comment helped me to find the way I've ended up using: https://stackoverflow.com/a/65377770/3508719 I'm glad that using the SyncManager and its proxies I managed to come up with a solution that allows me to use the executor. Note I use multiprocessing instead of multithreading as there is some post-processing done to the data returned from the REST API. I imagine that for better efficiency I could split the system into a single process that does a lot of multithreading for REST API interaction, and then pass the returned data to several processes for post-processing. I didn't have time to do it at the moment, but I'm aware of this as a potential alternative. I've built an earlier version of the rate limiter using multiprocessing Listener and Client - and carried out the communication through sockets/pipes. While this is useful to know about for inter-process communication, it turned out to be too slow and not support 100s of concurrent requests. If one of the existing libraries (eg. one of the ones I've listed) supports cross-process rate limiting with ProcessPoolExecutor , I'd love to see how to do it, please share an example! Multiprocessing can be a pain üò≠ Any feedback on my implementation welcome!"},
{"Title": "Using python for static typing benefits", "Author": "u/britishbanana", "Content": "I'm interested in using cython specifically for introducing static typing to parts of a code base. For anyone who has used cython, could you give any details about your experience with introducing it gradually, how it changed the deployment and execution processes, how well it played with code that is calling lots of 3rd party frameworks. Also curious to hear about any headaches or issues it introduced. I'm less interested in the performance benefits, more interested in static type checks. I do use mypy already but I'm left quite lacking with it compared to real compilation checks. I'm curious more generally about the possibility of having a code base that mixes static and dynamic typing, and if I could stay in Python while doing that instead of going to Rust that would really simplify things. Thanks!"},
{"Title": "The Python on Microcontrollers (and Raspberry Pi) Newsletter, a weekly news and project resource", "Author": "u/HP7933", "Content": "The Python on Microcontrollers (and Raspberry Pi) Newsletter: subscribe for free With the Python on Microcontrollers newsletter, you get all the latest information on Python running on hardware in one place! MicroPython, CircuitPython and Python on single Board Computers like Raspberry Pi & many more. The Python on Microcontrollers newsletter is the place for the latest news. It arrives Monday morning with all the week‚Äôs happenings. No advertising, no spam, easy to unsubscribe. 11,023 subscribers - the largest Python on hardware newsletter out there. Catch all the weekly news on Python for Microcontrollers with adafruitdaily.com. This ad-free, spam-free weekly email is filled with CircuitPython, MicroPython, and Python information that you may have missed, all in one place! Ensure you catch the weekly Python on Hardware roundup‚Äì you can cancel anytime ‚Äì try our spam-free newsletter today! https://www.adafruitdaily.com/"},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Fields and class properties should be sorted alphabetically?", "Author": "u/Myterro", "Content": "Hello, I'm having code-review suggestion doubts about sorting alphabetically fields in classes, e.g. Pydantic models. For example, there's a model: class Example(BaseModel): id: int name: str surname: str age: int operation: str One of developers suggests that fields should be sorted alphabetically: class Example(BaseModel): age: int id: int name: str operation: str surname: str I think there shouldn't be any specific order but only developer' subjective look at importance and connection between fields, like \"name\" and \"surname\" should be next to each other because they are in some way connected. What is your opinion? Maybe there are some PEP8 rules about that?"},
{"Title": "Tuples Are Underrated! List vs Tuple üêç", "Author": "u/JosephLovesPython", "Content": "Do you feel like you're underutilizing tuples in you code? Maybe cause you think lists are always the correct choice, and tuples don't have a place to exist. In this video we will walk through the differences between lists and tuples, especially focusing on a difference very rarely discussed, albeit it being the most crucial one: the semantic. Following that we will elaborate how and when it is better to utilize either lists or tuples! Any feedback on the content would be highly appreciated ‚ò∫Ô∏è https://youtu.be/-sO4FG6W4ho"},
{"Title": "CMake configs for Python modules (Pytest, Sphinx, ‚Ä¶)", "Author": "u/buddly27", "Content": "As Python is one of the most popular languages, many C++ projects end up using Python bindings of some sort. Pytest and Sphinx are very popular frameworks , so many CMake modules have been written, and most projects end up including a copy of these modules or using some hardcoded paths . I wrote two Python packages to manage the installation and update of CMake configs for Pytest and Sphinx. https://github.com/python-cmake/pytest-cmake https://github.com/python-cmake/sphinx-cmake It uses the pip package management, providing a module for each package and automatically generating a configuration based on the package version found. > pip install pytest-cmake > pip install sphinx-cmake I hope this method can standardize module integration for common Python tools. Let me know what you think!"},
{"Title": "Polars news: Faster CSV writer, dead expr elimination optimization, hiring engineers.", "Author": "u/commandlineluser", "Content": "Details about added features in the releases of Polars 0.20.17 to Polars 0.20.31 https://pola.rs/posts/polars-in-aggregate-jun24/"},
{"Title": "Code review for my simple project", "Author": "u/Mews75", "Content": "I've made this simple little package to stretch out audios https://github.com/Mews/simpleaudiostretch However I'm still new to uploading packages to pypi and doing documentation and the sorts, so I'd appreciate it if someone could review my project and see if what I'm doing are the best practices. Thank you in advance if anyone is willing to help"},
{"Title": "Wave Equation Solver in Python", "Author": "u/salastrodaemon", "Content": "Hello r/Python , What My Project Does I wanted to share a Python project I've been working on called WavePDE. WavePDE is a simulation and animation tool for studying wave equations in one or two dimensions. It's a handy tool for anyone interested in wave phenomena, also it's customizable and interactive. You can adjust domain size, grid resolution, wave speed, time step, boundary conditions (Dirichlet or Neumann), initial conditions, and more. Additionally, it is possible save your simulations as video files for further analysis or presentations. Target Audience I mainly created this tool while working on my research project. It is not yet complete since it deadens heavily on some parts I still didn't finish. It is about numeric computations of the wave equation on arbitrary boundaries. So I still need to apply some mask on these results and extend the Neumann conditions beyond the current implementation. Comparison This tool is way more customizable (at least imho) than other Python tools I found online. The code is more structured allowing for future extensibility. I also tried to make it as user-friendly as possible. I hope you find it useful and I would appreciate any feedback you might have. I still didn't implement tests, so if you find any bugs please let me know. Also, the documentation is lacking, but I'm working on it. You can find the code on GitHub: https://github.com/salastro/wavepde"},
{"Title": "Tutorial: How To Create Professional Python Shiny Dashboards In A Jiffy", "Author": "u/jgloewen", "Content": "The Python Shiny library is a framework for building interactive web applications in Python. Developed by RStudio, the same team behind the Shiny library for R, this library is particularly useful for data scientists and analysts who want to build interactive dashboards and applications without having extensive front-end development skills. All that is needed is knowledge of the Shiny user interface Application Programming Interface (API). Python Shiny can be used to develop applications that allow users to interact with data in real time. Data scientists can quickly prototype data applications and share them with anyone. How easy is it to use? Let‚Äôs use a simple data set and a basic interactive data visualization to take it for a test drive. Free article HERE ."},
{"Title": "PSA: PySimpleGUI has deleted [almost] all old LGPL versions from PyPI; update your dependencies", "Author": "u/ManyInterests", "Content": "Months ago, PySimpleGUI relicensed from LGPL3 to a proprietary license/subscription model with the release of version 5 and nuked the source code and history from GitHub. Up until recently, the old versions of PySimpleGUI remained on PyPI. However, all but two of these have been deleted and those that remain are yanked . The important effect this has had is anyone who may have defined their requirements as something like PySimpleGUI<5 or PySimpleGUI==4.x.x for a now-deleted version, your installations will fail with a message like: ERROR: No matching distribution found for pysimplegui<5 If you have no specific version requested for PySimpleGUI you will end up installing the version with a proprietary license and nagware. There are three options to deal with this without compeltely changing your code: Specify the latest yanked, but now unsupported version of PySimpleGUI PySimpleGUI==4.60.5 and hope they don't delete that some time in the future Use the supported LGPL fork, FreeSimpleGUI (full disclosure, I maintain this fork) Pay up for a PySimpleGUI 5 license."},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Tach - enforce module boundaries + deps, now in Rust ü¶Ä", "Author": "u/the1024", "Content": "https://github.com/gauge-sh/tach Hey everyone! Wanted to share some pretty significant updates to the tool I've been working on. Tach lets you define module boundaries and enforce rules across your modules, including isolation, dependencies, and strict interfaces. Some updates - Re-wrote the core in Rust, leading to a ~19x speed up on large repos Re-worked the interface, and added a TUI to let you interactively declare modules We built Tach to solve the ‚Äúball of mud‚Äù problem that we‚Äôve ran into throughout all of my previous work experiences. Over time, the codebase would become tightly coupled together, making even simple changes/refactors painful. By setting up module boundaries and enforcing them early on, you can avoid all of this! Tach is the best way to grow a modular monolith without creating a ball of mud. If anyone has any questions or feedback, I‚Äôd love chat! https://github.com/gauge-sh/tach What My Project Does Tach enables you to interactively declare module boundaries, dependencies between modules, and strict interfaces for those modules. You can then enforce those declarations through a static code check. Target Audience Teams maintaining python monorepos. Comparison Import linter is probably the most similar tool - for a github discussion on the differences, check out this link - https://github.com/gauge-sh/tach/discussions/72"},
{"Title": "New Lands RPG (Play testers welcome)", "Author": "u/Littlebudddy_321", "Content": "Good \"time of day\" my fellow peeps What my project does: I wanted to share my Python game I've been slowly working on over the past... I'd say 1.5 years. It is a simple texted based resource collection game where you travel to different areas, collect resources, sell them in town but be careful there are bandits about, so don't go too far without having some cooked fish on you... Target Audience: I'd say its mainly for well... everyone, anyone who enjoys text based games and anyone who wants to chill out on a rainy day when all the other games in their steam library are looking boring and they just want to relax... Comparison: I'd say Colossal cave adventure but that is a much bigger... better... game I would call it, but this is just a simple \"learning python\" project I started a while ago and just recently got back into it so I said what the heck why not finish the game. but now I'm stuck as to what to do next, so I thought I'd ask for play testers to come and tell me how bad my coding and game was so I could try and make it more playable... because lord knows I made it so I know how to play it but what about other people. you can find the code on GitHub: https://github.com/littlebudddy321/New-Lands-RPG"},
{"Title": "Bayesian bandits item pricing in a simplified Moonlighter shop simulation using Python and SQLite", "Author": "u/JaggedParadigm", "Content": "What My Project Does: Moonlighter is a game that includes a mechanic where you place items on shelves in your store and set the price. Customer's reactions give you hints about what prices would be ideal. These reactions take the form of four moods: ecstatic: price too low so they are extra happy content: price is what they were expecting, sad: price is too high to them but they buy anyway and this lowers the price everyone will pay for a certain period angry: price is too high so they don't buy I built a simplified version where a sad reaction doesn't lower the prices customers will accept for that item using Python and SQLite. The Bayesian bandits algorithm is an algorithm to optimize rewards when choosing among different options. The probability of different rewards (e.g. revenue) is kept track of and updated as rewards for options are collected. When a new option is to be selected a competition occurs where the rewards are sampled from these probability distributions and the option with the highest reward is chosen. For this simulation, the reward distributions are the probability that a price is the ideal price for that item. This scenario is so simple that the probability of any particular ideal price is flat or the same for all prices between an upper and lower bound and zero outside. This makes item/price selection simply randomly selecting a price from the lower to upper bounds for every item and selecting the item with the highest price. Customer reaction moods update the item upper/lower price bounds in these ways: ecstatic or content: lower bound is set to price plus 1 gold sad: lower bound is set to price if upper and lower bounds don't match angry: upper bound set to price minus 1 gold if the upper and lower bounds don't match The SQLite database keeps track of items in your inventory, items on shelves, customer reactions, item price bounds, and Thompson competitions (i.e. prices randomly chosen between price bounds for each item). The algorithm ended up identifying groups of items with the same ideal prices and selling them off from highest to lowest. For the full write up and a lot of pretty graphs check out the article in the link below. I've also included the Github link for those that want to see the full implementation and/or a Jupyter notebook where I generate the plots. Full write-up: https://cmshymansky.com/MoonlighterBayesianBanditsPricing/?source=rPython Github: https://github.com/JaggedParadigm/moonlighter_bayesian_bandit_pricing Target Audience: This a toy, though the Thompson sampling code could be hacked into something useful. Comparison: To my knowledge, I am the first to apply the Bayesian bandits algorithm to a Moonlighter shop simulation. However, pricing via Bayesian bandits is a classic application and there are many blogs and scientific papers on the topic."},
{"Title": "Mesop, open-source Python UI framework used at Google to quickly build delightful web apps", "Author": "u/wwwillchen", "Content": "What my project does: I‚Äôm excited to share about Mesop - a new, open-source Python UI framework that enables Python developers to quickly build delightful web apps in a scalable way. A small team of us at Google have been developing Mesop as an unofficial 20% project for the past few months. A wide range of research and product teams at Google have been using it to rapidly build internal apps and we‚Äôve gotten a lot of positive feedback internally so now we‚Äôre looking to get feedback from the open-source community. Target audience: Python developers looking to build AI demos & internal apps. Comparison: We think that Mesop provides a unique approach to building web UIs in Python compared to existing alternatives like Streamlit and Gradio - making it both easy to get started and also flexible enough to build customized UIs for a wide range of use cases. You can learn more about why we built Mesop here . To look at some example Mesop apps, check out our demo gallery . Also, the demo gallery itself is built with Mesop which demonstrates the type of flexibility you have in building apps with Mesop. GitHub repo: https://github.com/google/mesop"},
{"Title": "Dask DataFrame is Fast Now!", "Author": "u/phofl93", "Content": "My colleagues and I have been working on making Dask fast. It‚Äôs been fun. Dask DataFrame is now 20x faster and ~50% faster than Spark (but it depends a lot on the workload). I wrote a blog post on what we did: https://docs.coiled.io/blog/dask-dataframe-is-fast.html Really, this came down not to doing one thing really well, but doing lots of small things ‚Äúpretty good‚Äù. Some of the most prominent changes include: Apache Arrow support in pandas Better shuffling algorithm for faster joins Automatic query optimization There are a bunch of other improvements too like copy-on-write for pandas 2.0 which ensures copies are only triggered when necessary, GIL fixes in pandas, better serialization, a new parquet reader, etc. We were able to get a 20x speedup on traditional DataFrame benchmarks. I‚Äôd love it if people tried things out or suggested improvements we might have overlooked. Blog post: https://docs.coiled.io/blog/dask-dataframe-is-fast.html"},
{"Title": "Automate your WordPress new-page creation with Python", "Author": "u/EngineeringFit5761", "Content": "Hi everyone! I'm sharing with you a Python tool I've built and been using, intended to haste new-page creation in WordPress (with Elementor plugin). It's a simple app, but has great expansion potential and it's really easy to use. To start, you will previously need a WordPress site with Elementor installed and activated, and the content you want to introduce into the new page. Run the program, add sections, choose your desired structure, and select the right widgets for your content. Once you've loaded the content, add your credentials and click Confirm & Run (or just press Enter). The system will do the rest :) You can download and see the project at: https://github.com/MauBorre/WordPress-new-page-auto Hope you find it useful! üòÅ"},
{"Title": "Granian 1.4 is out", "Author": "u/gi0baro", "Content": "Granian ‚Äì the Rust HTTP server for Python applications ‚Äì 1.4 was released! Blog post: https://polar.sh/emmett-framework/posts/granian-1-4 Release details: https://github.com/emmett-framework/granian/releases/tag/v1.4.0 Repo: https://github.com/emmett-framework/granian"},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "My little ChatGPT-Multimodal Server Starter", "Author": "u/TwistedMinda", "Content": "Please check out my Desktoppy Server . What My Project Does It allows you to run your own personal AI on your computer, say bye-bye rate-limits and paywalls from mainstream AI's. It uses ollama internally so you can use all the open-source Models but by default it's using: LLama3 for text-generation LLava for image recognition Stable Diffusion 2 for image generation Target Audience Perfect for new-comers... I wish I had this when I started tackling AI dev. I think it can be a good base to create your awesome AI-powered products! Please let me know what you think about it! Comparison It differentiates from the other zillion starters by being very basic, allowing for full customization, and joining the 3 models together into 1 for a multi-modal feeling. Easiest possible setup, even for those who don't know the tools yet, all you need is Python3 installed on your PC. Basically a tutorial-starter-multimodal. Much love Link: https://github.com/TwistedMinda/desktoppy-server The very basic Web UI that goes along with it: https://github.com/TwistedMinda/desktoppy-web"},
{"Title": "Ludic Update: Web Apps in pure Python with HTMX, Themes, Component Catalog, new Documentation", "Author": "u/pdcz", "Content": "Hi everyone, I'd like to share couple of news regarding my personal project: New documentation written in Ludic showcasing it's capabilities: https://getludic.dev/docs/ New section regrading Layouts inspired from the Every Layout Book: https://getludic.dev/catalog/layouts Cookiecutter template to get quickly started: https://github.com/paveldedik/ludic-template I have a lot of plans with this project and I'd appreciate any feedback. About The Project Ludic allows web development in pure Python with components. It uses HTMX to add UI interactivity and has a catalog of components. Target Audience Web developers People who want to build HTML pages in Python with typing People without knowledge of JavaScript who want to build interactive UIs People who want to use HTMX in their projects Comparison With Similar Tools Feature Ludic FastUI Reflex HTML rendering Server Side Client Side Client Side Uses Template Engine No No No UI interactivity </> htmx React React Backend framework Starlette FastAPI FastAPI Client-Server Communication HTML + REST JSON + REST WebSockets Any feedback is highly appreciated."},
{"Title": "Python's many command-line utilities", "Author": "u/treyhunner", "Content": "Python 3.12 comes bundled with 50 command-line tools. For example, python -m webbrowser http://example.com opens a web browser, python -m sqlite3 launches a sqlite prompt, and python -m ast my_file.py shows the abstract syntax tree for a given Python file. I've dug into each of them and categorized them based on their purpose and how useful they are. Python's many command-line tools"},
{"Title": "Notion2Pandas: A new python package to import Notion Database into Pandas framework and viceversa", "Author": "u/Jaeger1987", "Content": "What My Project Does Hello everyone! I've just released a new Python package, notion2pandas, which allows you to import a Notion database into a pandas dataframe with just one line of code, and to update a Notion database from a pandas dataframe also with just one line of code. Target Audience Whether you're a data scientist, a data engineer, a Python enthusiast, or just curious, 'pip install notion2pandas' from the terminal, follow the tutorial in the README, and happy coding! üîó GitLab repo: https://gitlab.com/Jaeger87/notion2pandas Key Features Easy to use . import in a single line of code, export with another single line of code No more boring parsing . You can import any Notion Database in a pandas framework Flexibility . If you don't like the default parsing mode of a data provided by notion2pandas, you can use your own parse function for a specific kind of data. Maintainability . If Notion broke something with an update, the possibility to provide a different parsing function allows you to use Notion2Pandas even if it's not updated with latest notion update. Quick Start In the ReadMe you can find everything you need to start. Comparison When I started this project, I couldn't find anything capable of transforming a Notion database into a pandas DataFrame without specifying how to parse the data. If you got any kind of feedback I'm really curious to read it!"},
{"Title": "Rate Limiting + Multiprocessing = Nightmare? But I think I've found one nice way to do it ü§û", "Author": "u/VoyZan", "Content": "If you're interested in Python multiprocessing, I'd appreciate if you read this and share your thoughts: tl;dr: I've implemented a cross-process request rate limiter, allowing for N requests per T seconds. See it in this Gist . Problem Request rate limiting (or throttling) requires a place in memory to track the the amount of calls already made - some kind of counter . Multiprocessing is not great at having a single shared variable. I have a use case for a multiprocessing system in which each process can make a number of requests to a REST API server. That server imposes a 1000 requests per minute limit. Hence I needed a way to implement a rate limiter that would work across processes and threads. I've spent the past 2 days digging through a ton of SO posts and articles suggesting how to do it, and I came at a few bad solutions. I finally came up with one that I think works quite well. It uses a multiprocessing.Manager , and its Value , Lock and Condition proxies. Solution I've created a CrossProcessThrottle class which stores that counter . The way that the information about the counter is shared with all the processes and threads is through a ThrottleBarrier class instance. Its wait method will do the following: def wait(self): with self._condition: self._condition.wait() with self._lock: self._counter.value += 1 Wait for the shared Condition - this will stop all the processes and their threads and keep them dormant. If the CrossProcessThrottle calculates that we have available requests (ie. the counter is below max_requests , so we don't need to limit the requests), it uses Condition.notify(n) ( docs ) in order to let n amount of threads through and carry out the request. Once approved, each process/thread will bump the shared Value , indicating that a new request was made. That Value is then used by the CrossProcessThrottle to figure out how many requests have been made since the last check, and adjust its counter . If counter is equal or greater than max_requests , the Condition will be used to stop all processes and threads, until enough time passes. The following is the example code using this system. You can find it in this Gist if you prefer. import datetime from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor from ratelimiter import ThrottleBarrier, CrossProcessesThrottle def log(*args, **kwargs): print(datetime.datetime.now().strftime('[%H:%M:%S]'), *args, **kwargs) def task(i, j, throttle_barrier: ThrottleBarrier): # This will block until there is a free slot to make a request throttle_barrier.wait() log(f'request: {i:2d}, {j:2d}  (process, thread)') # make the request here... def worker(i, throttle_barrier: ThrottleBarrier): # example process worker, starting a bunch of threads with ThreadPoolExecutor(max_workers=5) as executor: for j in range(5): executor.submit(task, i, j, throttle_barrier) if __name__ == '__main__': cross_process_throttle = CrossProcessesThrottle(max_requests=3, per_seconds=10) throttle_barrier = cross_process_throttle.get_barrier() log('start') futures = [] # schedule 9 jobs, which should exceed our limit of 3 requests per 10 seconds with ProcessPoolExecutor(max_workers=10) as executor: for i in range(3): futures.append(executor.submit(worker, i, throttle_barrier)) while len(futures): # calling this method carries out the rate limit calculation cross_process_throttle.cycle() for future in futures: if future.done(): futures.remove(future) log('finish') I've uploaded the source code for CrossProcessThrottle and ThrottleBarrier as a Gist too . Calculating the counter is a bit more code, so I refrain from sharing it here, but in a nutshell: Store the last amount of requests made as last_counter , initialised as 0 Every time the cycle() is called, compare the difference between the current counter and the last_counter The difference is how many requests have been made since the last check, hence we increment the counter by that many. We calculate how many calls remaining are allowed: remaining_calls = max_requests - counter And notify that many threads to go ahead and proceed: condition.notify(remaining_calls) The actual process is a little more involved, as at the step 3 we need to store not only the amount of calls made, but also the times they've been made at - so that we can be checking against these later and decrease the counter . You can see it in detail in the Gist . If you've read through the code - what are your thoughts? Am I missing something here? In my tests it works out pretty nicely, producing: [14:57:26] start [14:57:26] Calls in the last 10 seconds: current=0 :: remaining=3 :: total=0 :: next slot in=0s [14:57:27] request:  0,  1  (process, thread) [14:57:27] request:  0,  0  (process, thread) [14:57:27] request:  0,  2  (process, thread) [14:57:31] Calls in the last 10 seconds: current=3 :: remaining=0 :: total=3 :: next slot in=7s [14:57:36] Calls in the last 10 seconds: current=3 :: remaining=0 :: total=3 :: next slot in=2s [14:57:38] request:  0,  4  (process, thread) [14:57:38] request:  0,  3  (process, thread) [14:57:38] request:  1,  0  (process, thread) [14:57:41] Calls in the last 10 seconds: current=3 :: remaining=0 :: total=6 :: next slot in=7s [14:57:46] Calls in the last 10 seconds: current=3 :: remaining=0 :: total=6 :: next slot in=2s [14:57:48] request:  2,  0  (process, thread) [14:57:48] request:  1,  1  (process, thread) [14:57:48] request:  1,  2  (process, thread) [14:57:51] Calls in the last 10 seconds: current=3 :: remaining=0 :: total=9 :: next slot in=8s [14:57:56] Calls in the last 10 seconds: current=3 :: remaining=0 :: total=9 :: next slot in=3s [14:57:59] request:  2,  4  (process, thread) [14:57:59] request:  2,  2  (process, thread) [14:57:59] request:  2,  1  (process, thread) [14:58:01] Calls in the last 10 seconds: current=3 :: remaining=0 :: total=12 :: next slot in=8s [14:58:06] Calls in the last 10 seconds: current=3 :: remaining=0 :: total=12 :: next slot in=3s [14:58:09] request:  1,  3  (process, thread) [14:58:09] request:  1,  4  (process, thread) [14:58:09] request:  2,  3  (process, thread) [14:58:10] finish I've also tested it with 1000s scheduled jobs to 60 processes, each spawning several threads, each of which simulates a request. The requests are limited as expected, up to N per T seconds. I really like that I can construct a single ThrottleBarrier instance that can be passed to all processes and simply call the wait method to get permission for a request. It feels like an elegant solution. Research There are a bunch of libraries for rate limiting, some claiming to support multiprocess, however I couldn't get them to do so: https://pypi.org/project/ratelimit/ https://pypi.org/project/ratelimiter/ https://pypi.org/project/ratemate/ https://github.com/JWCook/requests-ratelimiter There's a few SO threads and posts discussing the process too, however they either don't consider multiprocessing, or when they do they don't allow using ProcessPoolExecutor : https://stackoverflow.com/questions/69306420/rate-limit-api-multi-process https://stackoverflow.com/questions/40748687/python-api-rate-limiting-how-to-limit-api-calls-globally https://gist.github.com/justinvanwinkle/d9f04950083c4554835c1a35f9d22dad https://stackoverflow.com/questions/6920858/interprocess-communication-in-python The issue with ProcessPoolExecutor comes up when you try to use shared resources as it raises an error along the lines of: Synchronized objects should only be shared between processes through inheritance And to be fair the Googling didn't really help me figuring out how to get around it, just finding more people struggling with the issue: https://stackoverflow.com/questions/69907453/lock-objects-should-only-be-shared-between-processes-through-inheritance https://github.com/python/cpython/issues/79967#issuecomment-1455216546 The solution would be to not use the ProcessPoolExecutor but that was a bummer. This comment helped me to find the way I've ended up using: https://stackoverflow.com/a/65377770/3508719 I'm glad that using the SyncManager and its proxies I managed to come up with a solution that allows me to use the executor. Note I use multiprocessing instead of multithreading as there is some post-processing done to the data returned from the REST API. I imagine that for better efficiency I could split the system into a single process that does a lot of multithreading for REST API interaction, and then pass the returned data to several processes for post-processing. I didn't have time to do it at the moment, but I'm aware of this as a potential alternative. I've built an earlier version of the rate limiter using multiprocessing Listener and Client - and carried out the communication through sockets/pipes. While this is useful to know about for inter-process communication, it turned out to be too slow and not support 100s of concurrent requests. If one of the existing libraries (eg. one of the ones I've listed) supports cross-process rate limiting with ProcessPoolExecutor , I'd love to see how to do it, please share an example! Multiprocessing can be a pain üò≠ Any feedback on my implementation welcome!"},
{"Title": "Using python for static typing benefits", "Author": "u/britishbanana", "Content": "I'm interested in using cython specifically for introducing static typing to parts of a code base. For anyone who has used cython, could you give any details about your experience with introducing it gradually, how it changed the deployment and execution processes, how well it played with code that is calling lots of 3rd party frameworks. Also curious to hear about any headaches or issues it introduced. I'm less interested in the performance benefits, more interested in static type checks. I do use mypy already but I'm left quite lacking with it compared to real compilation checks. I'm curious more generally about the possibility of having a code base that mixes static and dynamic typing, and if I could stay in Python while doing that instead of going to Rust that would really simplify things. Thanks!"},
{"Title": "PyODMongo an ODM for MongoDB", "Author": "u/maurinhoandre", "Content": "What My Project Does: PyODMongo is a modern Python library that serves as a robust Object-Document Mapper (ODM) and seamlessly bridges the gap between Python and MongoDB. It offers an intuitive and efficient way to interact with documents. Built on top of Pydantic V2, PyODMongo ensures that documents in the database rigorously represent the structure of Python objects. This means that documents are saved and retrieved from the database exactly as a Python object is structured, regardless of how nested the objects are and whether they are stored persistently or by reference. PyODMongo can automatically populate these documents. Target Audience: Backend developers who want a simple and efficient way to work with MongoDB Comparison: ODMantic ODM GitHub repository PyPi"},
{"Title": "Tutorial on Surprisingly Simple Python Streamlit Dashboards", "Author": "u/jgloewen", "Content": "Streamlit¬†is becoming an increasingly a popular framework for data visualization prototyping with Python. The¬†Streamlit¬†framework saves time, effort, and reduces the complexity traditionally associated with crafting maps and charts.Particularly if we approach application development with a modular approach. Starting simple, let‚Äôs put together 4 specific examples that leverage¬†Streamlit¬†for interactive data visualization: A global choropleth map for a dataset for a specific year. An animated global choropleth map for a dataset across a number of years An animated choropleth map for a specific region A line chart to provide an alternative representation of the data Link to tutorial HERE"},
{"Title": "Self updating spreadsheet with popular questions from Ask Reddit and summarized answers using OpenAI", "Author": "u/DouweOsinga", "Content": "What My Project Does This is a small Python script that runs inside a Google Sheet by way of the Python add-on . It uses the reddit api to fetch posts from Ask Reddit twice daily. For posts with enough upvotes, it uses the OpenAI API to summarize an answer to the question based on the comments. I then inserts any new questions and their answers into the spreadsheet and uses the Twitter API to also post the answer to Twitter I mean X. Should be interesting to anybody looking to connect (a subset) of those APIs. Target Audience Anybody who is looking to mash-up different APIs (Python is great at this and I feel like it is getting a little harder to do this every year). Comparison I'm not aware of any Python code that does this. Even finding a good example of the V2 twitter API is harder than it seems. To accomplish some of this, you could try to ask ChatGPT directly to summarize the answers for a url but when I tried it said it couldn't access Reddit. Resource The spreadsheet where this happens The twitter bot in action The source code (or make a copy of the spreadsheet to see)"},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "1BRC solution using CPython", "Author": "u/pappuks", "Content": "What My Project Does: I finally got some time to attempt the 1 Billion Row Challenge (1BRC) ( https://www.morling.dev/blog/one-billion-row-challenge/ ) where we are supposed to process a file with 1 billion records of temperature values for cities and print a sorted list with min, max and mean temperature per city. I am a sucker for optimization. So when I heard about 1BRC I got intrigued and in the last few days started experimenting with python implementations. I achieved my goal and implemented the fastest implementation running on CPython, without any external libraries. My motivation for CPython was so that I can apply any of the learning's in my day to day work, as I don't see us moving to PyPy any time sooner. You can check out the performance numbers and implementation at : https://github.com/pappuks/1brc Few learning's: Python Multiprocessing is very powerful in enabling multi core processing and overcoming GIL bottleneck for multi-threading. Using `Pool.starmap` is the easiest way to spawn child processes and collect response. AI code generation can help you jump start your implementation, but it will most likely be sub-optimal and you need to spend time in optimizing the code by understanding the core logic. PyPy gives good boost over CPython but compatibility of PyPy with external libraries is a limiting factor. Mypyc compilation was not any faster than default CPython implementation. Always measure after making the change. Optimizing for PyPy does not make the implementation any faster in CPython, but optimizing for CPython does make the implementation faster in PyPy. Target Audience: This is a hobby project, but most of the findings and learning can be applied to production projects as well. And given that all optimizations are done on CPython its applicability to production is easy. Comparison: This is the fastest CPython implementation for solving the 1BRC problem. The detailed comparison is provided in the above github repository. Interpreter File Time (sec) Python3 py_1brc_final.py 24.882 Python3 py_1brc_mypyc.py (process_chunk.py precompiled using mypyc) 24.441 Python3 calculateAverage.py (from https://github.com/ifnesi/1brc ) 36.303 Python3 calculateAveragePyPy.py (from https://github.com/ifnesi/1brc ) 60.60 Python3 doug_booty4.py (from https://github.com/dougmercer-yt/1brc ) 62.91"},
{"Title": "Community Insights on PgQueuer", "Author": "u/GabelSnabel", "Content": "Hey r/Python ! A while ago , I introduced you to PgQueuer , a Python library designed for handling job queues using Postgres native functionalities. If you've started using PgQueuer, I‚Äôm keen to initiate a discussion on your experiences with it. How and where have you integrated PgQueuer into your projects? Any difficulties or shortcomings you‚Äôve experienced while using PgQueuer? Thoughts on the library‚Äôs efficiency and features?"},
{"Title": "Keep system awake (prevent sleep) using python: wakepy", "Author": "u/fohrloop", "Content": "Hi all, I had previously a problem that I wanted to run some long running python scripts without being interrupted by the automatic suspend. I did not find a package that would solve the problem, so I decided to create my own. In the design, I have selected non-disruptive methods which do not rely on mouse movement or pressing a button like F15 or alter system settings. Instead, I've chosen methods that use the APIs and executables meant specifically for the purpose. I've just released wakepy 0.9.0 which supports Windows, macOS, Gnome, KDE and freedesktop.org compliant DEs. GitHub: https://github.com/fohrloop/wakepy Comparison to other alternatives: typical other solutions rely on moving the mouse using some library or pressing F15. These might cause problems as your mouse will not be as accurate if it moves randomly, and pressing F15 or other key might have side effects on some systems. Other solutions might also prevent screen lock (e.g. wiggling mouse or pressing a button), but wakepy has a mode for just preventing the automatic sleep, which is better for security and advisable if the display is not required. Hope you like it, and I would be happy to hear your thoughts and answer to any questions!"},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "PDF Reports for SonarQube Analysis ( Community Edition )", "Author": "u/Content_Ad_4153", "Content": "Problem Statement I recently explored SonarQube for static code analysis. While it‚Äôs a great tool, the free edition lacks the ability to generate PDF reports, making it hard to share issues. There was no maintained plugin available, so I decided to solve this problem myself. Target Audience This started as a hobby/side project, but I wanted to share it in case others find it useful. I'm open to suggestions and feedback! Comparison with Similar Tools There was only one similar tool in the Sonar Marketplace, but it‚Äôs no longer maintained. Project Details I've developed and published a Python library called RedCoffee, which generates PDF reports from SonarQube analysis. You can find it on PyPi and GitHub. Links: PyPi: RedCoffee GitHub: RedCoffee Repository Feel free to check it out and let me know your thoughts!"},
{"Title": "ReqFlow - Simplifying API Testing with Python", "Author": "u/Eastern_Reporter_834", "Content": "Hello everyone! What My Project Does I'm excited to share ReqFlow - a Python library designed to make API testing straightforward and efficient. It offers a fluent interface for building and validating HTTP requests, making it a handy tool for small-sized testing frameworks or utilities. While it's still in development and might have some bugs, I would love your feedback and contributions to improve it! Target Audience It would be suitable for beginners due to its reduced entry barrier and also supports advanced use cases with a RestAssured-like approach. Comparison While standard approaches for API testing with Python (e.g., requests ) definitely makes sense, ReqFlow provides a more fluent and expressive syntax, making it easier to write and understand tests. Check it out on GitHub and the docs at reqflow.org . All feedback and contributions are welcome! üôÇ"},
{"Title": "NiimPrintX: A desktop app for NiimBot Label Printers developed in Python", "Author": "u/good-guy-coder", "Content": "I'm super excited to share NiimPrintX, a desktop app I've been working on for NiimBot label printers. This is my first release, and I am actively working on adding new functionalities. What My Project Does: NiimPrintX offers both a command line and graphical user interface app to connect with your NiimBot printer. It connects via Bluetooth and makes label printing a breeze. The app is developed completely using Python 3.12 and the Tkinter library for the GUI. GitHub Repository: NiimPrintX Target Audience: This project is aimed at hobbyists who use NiimBot label printers. It's a proof of concept project for me to learn GUI app development in Python. Comparison: Currently, there is no desktop app support for NiimBot thermal label printers. Only the official Android/iOS app is available, and it has limited functionality without a paid subscription. NiimPrintX aims to fill this gap by providing a free, more versatile desktop solution. Supported Printer Models: D11/B21/B1 D110 B18 Cool Features: Bluetooth Auto Discovery: Automatically finds your printer using its model name. Easy Label Design: Create labels with a simple and intuitive GUI. Predefined Icons: Spice up your labels with built-in icons. Cross-Platform: Works on Mac, Windows, and Linux. Advanced Print Options: Includes calibration features for perfect prints. Coming Soon: Barcode Creation: Make your own barcodes right in the app. QR Code Printing: Generate and print QR codes. Better Object Alignment: More shapes and borders for your designs. I'm constantly working on adding new features, so keep an eye out for updates! Check out the GitHub repo for more info and installation instructions: NiimPrintX I'd love to hear what you think! Drop a comment or open an issue on GitHub with any feedback or suggestions."},
{"Title": "zeroize: Securely clear secrets from memory", "Author": "u/radumarias", "Content": "https://github.com/radumarias/zeroize-python What My Project Does: Clear secrets from memory. Built on stable Rust primitives which guarantee memory is zeroed using an operation will not be 'optimized away' by the compiler. Target Audience it can be used in production, it's just a simple wrapper over zeroize crate from Rust Comparison Personally I didn't found an easy and safe solution in Python to do this, hence I created this lib"},
{"Title": "A blend of Rust and Python: a faster encryption for Python", "Author": "u/radumarias", "Content": "https://github.com/radumarias/rencrypt-python What My Project Does: A Python encryption library implemented in Rust. It supports AEAD with AES-GCM and ChaCha20Poly1305 . It uses ring to handle encryption. If offers slightly higher speed compared to other Python libs, especially for small chunks of data. Target Audience This lib hasn't been audited, but it mostly wraps ring crate which is a well known library, so in principle it should offer as similar level of security. This is still under development. Please do not use it with sensitive data just yet. Comparison If offers slightly higher speed compared to other Python libs, especially for small chunks of data. I compared it to PyFLocker , cryptography , NaCl ( l ibsodium ) , PyCryptodome . The API also tries to be easy to use but it's more optimized for speed than usability."},
{"Title": "AI Voice Assistant using on-device LLM, STT, TTS and Wake Word tech", "Author": "u/eonlav", "Content": "What My Project Does Allows you to have a voice-to-voice interaction with an LLM, similar to the ChatGPT app, except with all inference running locally. You can choose from a few different open-weight models. Video running Phi-2 model on a MacBook Air with 8GB RAM, all CPU Target Audience Devs looking to experiment with integrating on-device AI into their software. Comparison JARVIS - an all API-based solution using DeepGram, OpenAI and ElevenLabs Local Talking LLM - a higher-latency, more resource intensive local approach using Whisper, Llama and Bark, but with no wake word. Source code: https://github.com/Picovoice/pico-cookbook/tree/main/recipes/llm-voice-assistant/python"},
{"Title": "Calculator without eval()", "Author": "u/Zorgon-589", "Content": "WHAT MY PROJECT DOES: Solves basic arithmetic problems in an interactive way in python. TARGET AUDIENCE: Anyone, it's just a program to get practice using loops, lists, and functions. COMPARISON: This program functions as a calculator without the use of the eval() function to make everything superfluously easy. It's not perfect and my next version is gonna try and address queries with parenthesis and multiple operators! See the below link for github: https://github.com/Zorgon589/Calculator/tree/main"},
{"Title": "A blend of Rust and Python: speeding up Python encryption", "Author": "u/radumarias", "Content": "REncrypt What My Project Does A Python encryption library implemented in Rust. It supports AEAD with AES-GCM and ChaCha20Poly1305. It uses ring to handle encryption. If offers slightly higher speed compared to other Python libs, especially for small chunks of data. The API also tries to be easy to use but it's more optimized for speed than usability. So if you are open to experiment and want to achieve the highest possible encryption speed, consider giving it a try. Target Audience This is just a toy project as a learning experience Comparison This is slightly faster than PyFLocker which from my benchmarks is the faster among other Python libs like cryptography, NaCl (libsodium), PyCryptodome"},
{"Title": "RAGFlow: Deep document understanding RAG engine", "Author": "u/neozhaoliang", "Content": "What My Project Does An open-source RAG (Retrieval-Augmented Generation) engine based on deep document understanding. It offers layout recognition, OCR-based chunking templates for data cleasing and provides hallucination-free answers with traceable citations. Compatible with mainstream LLMs. Target Audience RAG applications developers. Comparison It offers various chunking templates for various fils categories, such as resume, legal documents, table, and print copies. Enables human intervention in chunking, making the data cleansing process no longer a black box. It not only presents answers but also offers quick views of references and links to the citations when answering to queries. Link: https://github.com/infiniflow/ragflow"},
{"Title": "Circler imports in Observer design pattern in Python", "Author": "u/ALior1", "Content": "So I'm trying to do a small refresher in design patterns and I reached the Observer pattern. And I'm encounter a circular import error that I'm not sure how to solve. At first I had two files: `observers.py` and `subjects.py`, which each contained the abstract class and some concrete ones. But because each  had to know about the other, I got a circler import error. I tried to put them in the same file, but than the first cant use the second. Also tried to put the Observer in the \"subjects.py\" file, textualy before \"Subject\", that worked, but not clear to me why. I know that in compiled languages, they just use an interface, but we dont have it in Python. Tried to solved it in a various ways, but want to hear others, how you think this can be solved and opinons on this. The base classes are: class Subject(ABC): @abstractmethod def attach(self, observer: Observer) -> None: # The Observer is in the method parameters, so we need to import it pass class Observer(ABC): @abstractmethod def update(self, subject: Subject) -> None: # The Subjectis in the method parameters, so we need to import it pass"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "New project: A blend of Rust and Python: speeding up Python encryption", "Author": "u/radumarias", "Content": "[ https://github.com/radumarias/rencrypt-python](https://github.com/radumarias/rencrypt-python) * **What My Project Does** A Python encryption library implemented in Rust. It supports AEAD with AES-GCM and ChaCha20Poly1305. It uses ring to handle encryption.If offers slightly higher speed compared to other Python libs, especially for small chunks of data. The API also tries to be easy to use but it's more optimized for speed than usability. So if you are open to experiment and want to achieve the highest possible encryption speed, consider giving it a try. * **Target Audience** This is just a toy project as a learning experience * **Comparison** This is slightly faster than PyFLocker which from my benchmarks is the faster among other Python libs like cryptography, NaCl (libsodium), PyCryptodome"},
{"Title": "AndroidWorld ‚Äî Build and test AI agents on Android", "Author": "u/crawles89", "Content": "What it does: It is for building AI agents that perform tasks for you on Android using LLMs. Agents read the screen and perform actions like clicking, typing, and dragging. It includes a test suite of 116 tasks across 20 real-world apps to evaluate agent performance. Think of each task like a unit test, with a setup, evaluation, and tear down procedure. Every task is written in Python. The most powerful agents should be able to pass all of them. Target Audience: Anyone looking to experiment with LLM for controlling Android UIs. You can download any app you‚Äôd like and test out the default agent, M3A, on it. Just give it a task like ‚ÄúShow my most recent purchases on Amazon.‚Äù You can also build your own agent. Comparison For desktop OSes, there is OSWorld , although it requires costly commercial software (VMWare) to run. AndroidWorld only requires free Android emulator. While this is OSS and for research, the closest commercial product would be the Rabbit R1 . They should test their agent on AndroidWorld to improve accuracy before shipping again :P Link to repo: https://github.com/google-research/android_world"},
{"Title": "Rio: WebApps in pure Python ‚Äì Thanks and Feedback wanted!", "Author": "u/Sn3llius", "Content": "Hey everyone, I'm a Rio developer, and I just wanted to say thanks for all the feedback we've received so far! Since our launch, we've implemented a lot of the features you asked for! As requested, we are currently working on an in-depth technical description of Rio, explaining how it works under the hood. So stay tuned! We are looking forward to your feedback, so let us hear from you! :) GitHub"},
{"Title": "2024 StackOverflow Survey", "Author": "u/monorepo", "Content": "This years SO survey is out now. It includes questions for Python tooling and frameworks. Contribute when you can, it closes soon. It takes ~10 minutes to finish. Link to the survey: https://stackoverflow.az1.qualtrics.com/jfe/form/SV_6rJVT6XXsfTo1JI"},
{"Title": "cachebox: The fastest caching library written in Rust", "Author": "u/SpareRevolution1487", "Content": "What my library does You can easily and powerfully perform caching and memoizing operations in your Python projects using my library. This library is written in Rust, which makes its performance very fast and efficient. By using this library, you can use 7 different caching algorithms that allow you to choose the best algorithm based on your needs. One prominent feature of this library is its simplicity to work with. You just need to import the library into your project and then behave with it like a dictionary. Therefore, if you are looking for a powerful, fast, and simple library for caching and memoizing in Python, my library will be responsive to your needs. By using this library, you can improve the performance of your program and significantly reduce the execution time of your Python code. Target Audience For anyone who needs caching and values speed Comparison When compared to other caching libraries: It's very faster than others (about 5-20x) It's very simple and easy to use It's completely thread-safe (uses RwLock) It uses lower memory than others You can see benchmark here: https://github.com/awolverp/cachebox-benchmark More Info My project github: https://github.com/awolverp/cachebox"},
{"Title": "PyODMongo an ODM for MongoDB", "Author": "u/maurinhoandre", "Content": "What My Project Does: PyODMongo is a modern Python library that serves as a robust Object-Document Mapper (ODM) and seamlessly bridges the gap between Python and MongoDB. It offers an intuitive and efficient way to interact with documents. Built on top of Pydantic V2, PyODMongo ensures that documents in the database rigorously represent the structure of Python objects. This means that documents are saved and retrieved from the database exactly as a Python object is structured, regardless of how nested the objects are and whether they are stored persistently or by reference. PyODMongo can automatically populate these documents. Target Audience: Backend developers who want a simple and efficient way to work with MongoDB Comparison: ODMantic ODM GitHub repository PyPi"},
{"Title": "Tutorial on Surprisingly Simple Python Streamlit Dashboards", "Author": "u/jgloewen", "Content": "Streamlit¬†is becoming an increasingly a popular framework for data visualization prototyping with Python. The¬†Streamlit¬†framework saves time, effort, and reduces the complexity traditionally associated with crafting maps and charts.Particularly if we approach application development with a modular approach. Starting simple, let‚Äôs put together 4 specific examples that leverage¬†Streamlit¬†for interactive data visualization: A global choropleth map for a dataset for a specific year. An animated global choropleth map for a dataset across a number of years An animated choropleth map for a specific region A line chart to provide an alternative representation of the data Link to tutorial HERE"},
{"Title": "Self updating spreadsheet with popular questions from Ask Reddit and summarized answers using OpenAI", "Author": "u/DouweOsinga", "Content": "What My Project Does This is a small Python script that runs inside a Google Sheet by way of the Python add-on . It uses the reddit api to fetch posts from Ask Reddit twice daily. For posts with enough upvotes, it uses the OpenAI API to summarize an answer to the question based on the comments. I then inserts any new questions and their answers into the spreadsheet and uses the Twitter API to also post the answer to Twitter I mean X. Should be interesting to anybody looking to connect (a subset) of those APIs. Target Audience Anybody who is looking to mash-up different APIs (Python is great at this and I feel like it is getting a little harder to do this every year). Comparison I'm not aware of any Python code that does this. Even finding a good example of the V2 twitter API is harder than it seems. To accomplish some of this, you could try to ask ChatGPT directly to summarize the answers for a url but when I tried it said it couldn't access Reddit. Resource The spreadsheet where this happens The twitter bot in action The source code (or make a copy of the spreadsheet to see)"},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "1BRC solution using CPython", "Author": "u/pappuks", "Content": "What My Project Does: I finally got some time to attempt the 1 Billion Row Challenge (1BRC) ( https://www.morling.dev/blog/one-billion-row-challenge/ ) where we are supposed to process a file with 1 billion records of temperature values for cities and print a sorted list with min, max and mean temperature per city. I am a sucker for optimization. So when I heard about 1BRC I got intrigued and in the last few days started experimenting with python implementations. I achieved my goal and implemented the fastest implementation running on CPython, without any external libraries. My motivation for CPython was so that I can apply any of the learning's in my day to day work, as I don't see us moving to PyPy any time sooner. You can check out the performance numbers and implementation at : https://github.com/pappuks/1brc Few learning's: Python Multiprocessing is very powerful in enabling multi core processing and overcoming GIL bottleneck for multi-threading. Using `Pool.starmap` is the easiest way to spawn child processes and collect response. AI code generation can help you jump start your implementation, but it will most likely be sub-optimal and you need to spend time in optimizing the code by understanding the core logic. PyPy gives good boost over CPython but compatibility of PyPy with external libraries is a limiting factor. Mypyc compilation was not any faster than default CPython implementation. Always measure after making the change. Optimizing for PyPy does not make the implementation any faster in CPython, but optimizing for CPython does make the implementation faster in PyPy. Target Audience: This is a hobby project, but most of the findings and learning can be applied to production projects as well. And given that all optimizations are done on CPython its applicability to production is easy. Comparison: This is the fastest CPython implementation for solving the 1BRC problem. The detailed comparison is provided in the above github repository. Interpreter File Time (sec) Python3 py_1brc_final.py 24.882 Python3 py_1brc_mypyc.py (process_chunk.py precompiled using mypyc) 24.441 Python3 calculateAverage.py (from https://github.com/ifnesi/1brc ) 36.303 Python3 calculateAveragePyPy.py (from https://github.com/ifnesi/1brc ) 60.60 Python3 doug_booty4.py (from https://github.com/dougmercer-yt/1brc ) 62.91"},
{"Title": "Community Insights on PgQueuer", "Author": "u/GabelSnabel", "Content": "Hey r/Python ! A while ago , I introduced you to PgQueuer , a Python library designed for handling job queues using Postgres native functionalities. If you've started using PgQueuer, I‚Äôm keen to initiate a discussion on your experiences with it. How and where have you integrated PgQueuer into your projects? Any difficulties or shortcomings you‚Äôve experienced while using PgQueuer? Thoughts on the library‚Äôs efficiency and features?"},
{"Title": "Keep system awake (prevent sleep) using python: wakepy", "Author": "u/fohrloop", "Content": "Hi all, I had previously a problem that I wanted to run some long running python scripts without being interrupted by the automatic suspend. I did not find a package that would solve the problem, so I decided to create my own. In the design, I have selected non-disruptive methods which do not rely on mouse movement or pressing a button like F15 or alter system settings. Instead, I've chosen methods that use the APIs and executables meant specifically for the purpose. I've just released wakepy 0.9.0 which supports Windows, macOS, Gnome, KDE and freedesktop.org compliant DEs. GitHub: https://github.com/fohrloop/wakepy Comparison to other alternatives: typical other solutions rely on moving the mouse using some library or pressing F15. These might cause problems as your mouse will not be as accurate if it moves randomly, and pressing F15 or other key might have side effects on some systems. Other solutions might also prevent screen lock (e.g. wiggling mouse or pressing a button), but wakepy has a mode for just preventing the automatic sleep, which is better for security and advisable if the display is not required. Hope you like it, and I would be happy to hear your thoughts and answer to any questions!"},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "PDF Reports for SonarQube Analysis ( Community Edition )", "Author": "u/Content_Ad_4153", "Content": "Problem Statement I recently explored SonarQube for static code analysis. While it‚Äôs a great tool, the free edition lacks the ability to generate PDF reports, making it hard to share issues. There was no maintained plugin available, so I decided to solve this problem myself. Target Audience This started as a hobby/side project, but I wanted to share it in case others find it useful. I'm open to suggestions and feedback! Comparison with Similar Tools There was only one similar tool in the Sonar Marketplace, but it‚Äôs no longer maintained. Project Details I've developed and published a Python library called RedCoffee, which generates PDF reports from SonarQube analysis. You can find it on PyPi and GitHub. Links: PyPi: RedCoffee GitHub: RedCoffee Repository Feel free to check it out and let me know your thoughts!"},
{"Title": "ReqFlow - Simplifying API Testing with Python", "Author": "u/Eastern_Reporter_834", "Content": "Hello everyone! What My Project Does I'm excited to share ReqFlow - a Python library designed to make API testing straightforward and efficient. It offers a fluent interface for building and validating HTTP requests, making it a handy tool for small-sized testing frameworks or utilities. While it's still in development and might have some bugs, I would love your feedback and contributions to improve it! Target Audience It would be suitable for beginners due to its reduced entry barrier and also supports advanced use cases with a RestAssured-like approach. Comparison While standard approaches for API testing with Python (e.g., requests ) definitely makes sense, ReqFlow provides a more fluent and expressive syntax, making it easier to write and understand tests. Check it out on GitHub and the docs at reqflow.org . All feedback and contributions are welcome! üôÇ"},
{"Title": "NiimPrintX: A desktop app for NiimBot Label Printers developed in Python", "Author": "u/good-guy-coder", "Content": "I'm super excited to share NiimPrintX, a desktop app I've been working on for NiimBot label printers. This is my first release, and I am actively working on adding new functionalities. What My Project Does: NiimPrintX offers both a command line and graphical user interface app to connect with your NiimBot printer. It connects via Bluetooth and makes label printing a breeze. The app is developed completely using Python 3.12 and the Tkinter library for the GUI. GitHub Repository: NiimPrintX Target Audience: This project is aimed at hobbyists who use NiimBot label printers. It's a proof of concept project for me to learn GUI app development in Python. Comparison: Currently, there is no desktop app support for NiimBot thermal label printers. Only the official Android/iOS app is available, and it has limited functionality without a paid subscription. NiimPrintX aims to fill this gap by providing a free, more versatile desktop solution. Supported Printer Models: D11/B21/B1 D110 B18 Cool Features: Bluetooth Auto Discovery: Automatically finds your printer using its model name. Easy Label Design: Create labels with a simple and intuitive GUI. Predefined Icons: Spice up your labels with built-in icons. Cross-Platform: Works on Mac, Windows, and Linux. Advanced Print Options: Includes calibration features for perfect prints. Coming Soon: Barcode Creation: Make your own barcodes right in the app. QR Code Printing: Generate and print QR codes. Better Object Alignment: More shapes and borders for your designs. I'm constantly working on adding new features, so keep an eye out for updates! Check out the GitHub repo for more info and installation instructions: NiimPrintX I'd love to hear what you think! Drop a comment or open an issue on GitHub with any feedback or suggestions."},
{"Title": "zeroize: Securely clear secrets from memory", "Author": "u/radumarias", "Content": "https://github.com/radumarias/zeroize-python What My Project Does: Clear secrets from memory. Built on stable Rust primitives which guarantee memory is zeroed using an operation will not be 'optimized away' by the compiler. Target Audience it can be used in production, it's just a simple wrapper over zeroize crate from Rust Comparison Personally I didn't found an easy and safe solution in Python to do this, hence I created this lib"},
{"Title": "A blend of Rust and Python: a faster encryption for Python", "Author": "u/radumarias", "Content": "https://github.com/radumarias/rencrypt-python What My Project Does: A Python encryption library implemented in Rust. It supports AEAD with AES-GCM and ChaCha20Poly1305 . It uses ring to handle encryption. If offers slightly higher speed compared to other Python libs, especially for small chunks of data. Target Audience This lib hasn't been audited, but it mostly wraps ring crate which is a well known library, so in principle it should offer as similar level of security. This is still under development. Please do not use it with sensitive data just yet. Comparison If offers slightly higher speed compared to other Python libs, especially for small chunks of data. I compared it to PyFLocker , cryptography , NaCl ( l ibsodium ) , PyCryptodome . The API also tries to be easy to use but it's more optimized for speed than usability."},
{"Title": "AI Voice Assistant using on-device LLM, STT, TTS and Wake Word tech", "Author": "u/eonlav", "Content": "What My Project Does Allows you to have a voice-to-voice interaction with an LLM, similar to the ChatGPT app, except with all inference running locally. You can choose from a few different open-weight models. Video running Phi-2 model on a MacBook Air with 8GB RAM, all CPU Target Audience Devs looking to experiment with integrating on-device AI into their software. Comparison JARVIS - an all API-based solution using DeepGram, OpenAI and ElevenLabs Local Talking LLM - a higher-latency, more resource intensive local approach using Whisper, Llama and Bark, but with no wake word. Source code: https://github.com/Picovoice/pico-cookbook/tree/main/recipes/llm-voice-assistant/python"},
{"Title": "Calculator without eval()", "Author": "u/Zorgon-589", "Content": "WHAT MY PROJECT DOES: Solves basic arithmetic problems in an interactive way in python. TARGET AUDIENCE: Anyone, it's just a program to get practice using loops, lists, and functions. COMPARISON: This program functions as a calculator without the use of the eval() function to make everything superfluously easy. It's not perfect and my next version is gonna try and address queries with parenthesis and multiple operators! See the below link for github: https://github.com/Zorgon589/Calculator/tree/main"},
{"Title": "A blend of Rust and Python: speeding up Python encryption", "Author": "u/radumarias", "Content": "REncrypt What My Project Does A Python encryption library implemented in Rust. It supports AEAD with AES-GCM and ChaCha20Poly1305. It uses ring to handle encryption. If offers slightly higher speed compared to other Python libs, especially for small chunks of data. The API also tries to be easy to use but it's more optimized for speed than usability. So if you are open to experiment and want to achieve the highest possible encryption speed, consider giving it a try. Target Audience This is just a toy project as a learning experience Comparison This is slightly faster than PyFLocker which from my benchmarks is the faster among other Python libs like cryptography, NaCl (libsodium), PyCryptodome"},
{"Title": "RAGFlow: Deep document understanding RAG engine", "Author": "u/neozhaoliang", "Content": "What My Project Does An open-source RAG (Retrieval-Augmented Generation) engine based on deep document understanding. It offers layout recognition, OCR-based chunking templates for data cleasing and provides hallucination-free answers with traceable citations. Compatible with mainstream LLMs. Target Audience RAG applications developers. Comparison It offers various chunking templates for various fils categories, such as resume, legal documents, table, and print copies. Enables human intervention in chunking, making the data cleansing process no longer a black box. It not only presents answers but also offers quick views of references and links to the citations when answering to queries. Link: https://github.com/infiniflow/ragflow"},
{"Title": "Circler imports in Observer design pattern in Python", "Author": "u/ALior1", "Content": "So I'm trying to do a small refresher in design patterns and I reached the Observer pattern. And I'm encounter a circular import error that I'm not sure how to solve. At first I had two files: `observers.py` and `subjects.py`, which each contained the abstract class and some concrete ones. But because each  had to know about the other, I got a circler import error. I tried to put them in the same file, but than the first cant use the second. Also tried to put the Observer in the \"subjects.py\" file, textualy before \"Subject\", that worked, but not clear to me why. I know that in compiled languages, they just use an interface, but we dont have it in Python. Tried to solved it in a various ways, but want to hear others, how you think this can be solved and opinons on this. The base classes are: class Subject(ABC): @abstractmethod def attach(self, observer: Observer) -> None: # The Observer is in the method parameters, so we need to import it pass class Observer(ABC): @abstractmethod def update(self, subject: Subject) -> None: # The Subjectis in the method parameters, so we need to import it pass"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "New project: A blend of Rust and Python: speeding up Python encryption", "Author": "u/radumarias", "Content": "[ https://github.com/radumarias/rencrypt-python](https://github.com/radumarias/rencrypt-python) * **What My Project Does** A Python encryption library implemented in Rust. It supports AEAD with AES-GCM and ChaCha20Poly1305. It uses ring to handle encryption.If offers slightly higher speed compared to other Python libs, especially for small chunks of data. The API also tries to be easy to use but it's more optimized for speed than usability. So if you are open to experiment and want to achieve the highest possible encryption speed, consider giving it a try. * **Target Audience** This is just a toy project as a learning experience * **Comparison** This is slightly faster than PyFLocker which from my benchmarks is the faster among other Python libs like cryptography, NaCl (libsodium), PyCryptodome"},
{"Title": "AndroidWorld ‚Äî Build and test AI agents on Android", "Author": "u/crawles89", "Content": "What it does: It is for building AI agents that perform tasks for you on Android using LLMs. Agents read the screen and perform actions like clicking, typing, and dragging. It includes a test suite of 116 tasks across 20 real-world apps to evaluate agent performance. Think of each task like a unit test, with a setup, evaluation, and tear down procedure. Every task is written in Python. The most powerful agents should be able to pass all of them. Target Audience: Anyone looking to experiment with LLM for controlling Android UIs. You can download any app you‚Äôd like and test out the default agent, M3A, on it. Just give it a task like ‚ÄúShow my most recent purchases on Amazon.‚Äù You can also build your own agent. Comparison For desktop OSes, there is OSWorld , although it requires costly commercial software (VMWare) to run. AndroidWorld only requires free Android emulator. While this is OSS and for research, the closest commercial product would be the Rabbit R1 . They should test their agent on AndroidWorld to improve accuracy before shipping again :P Link to repo: https://github.com/google-research/android_world"},
{"Title": "Rio: WebApps in pure Python ‚Äì Thanks and Feedback wanted!", "Author": "u/Sn3llius", "Content": "Hey everyone, I'm a Rio developer, and I just wanted to say thanks for all the feedback we've received so far! Since our launch, we've implemented a lot of the features you asked for! As requested, we are currently working on an in-depth technical description of Rio, explaining how it works under the hood. So stay tuned! We are looking forward to your feedback, so let us hear from you! :) GitHub"},
{"Title": "2024 StackOverflow Survey", "Author": "u/monorepo", "Content": "This years SO survey is out now. It includes questions for Python tooling and frameworks. Contribute when you can, it closes soon. It takes ~10 minutes to finish. Link to the survey: https://stackoverflow.az1.qualtrics.com/jfe/form/SV_6rJVT6XXsfTo1JI"},
{"Title": "cachebox: The fastest caching library written in Rust", "Author": "u/SpareRevolution1487", "Content": "What my library does You can easily and powerfully perform caching and memoizing operations in your Python projects using my library. This library is written in Rust, which makes its performance very fast and efficient. By using this library, you can use 7 different caching algorithms that allow you to choose the best algorithm based on your needs. One prominent feature of this library is its simplicity to work with. You just need to import the library into your project and then behave with it like a dictionary. Therefore, if you are looking for a powerful, fast, and simple library for caching and memoizing in Python, my library will be responsive to your needs. By using this library, you can improve the performance of your program and significantly reduce the execution time of your Python code. Target Audience For anyone who needs caching and values speed Comparison When compared to other caching libraries: It's very faster than others (about 5-20x) It's very simple and easy to use It's completely thread-safe (uses RwLock) It uses lower memory than others You can see benchmark here: https://github.com/awolverp/cachebox-benchmark More Info My project github: https://github.com/awolverp/cachebox"},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "PyData Amsterdam 2024 Call for Proposals closes on Sunday, June 2", "Author": "u/PyDataAmsterdam", "Content": "Hey all, we will close the Call for Proposals portal this Sunday, June 2 , for our PyData Amsterdam 2024 Conference which will take place on September 18-20 in Amsterdam. We are looking for presentations that can captivate our audience, provide invaluable insights, and foster community learning. Don't miss this chance to speak on stage in front of over 800 attendees in the field of Data & AI. Submit a talk here > https://amsterdam2024.pydata.org/cfp/cfp"},
{"Title": "Zango - New python framework for building enterprise ready business apps. Salesforce alternative.", "Author": "u/CanaryHill", "Content": "What My Project Does Zango, built on top of Django, is further opinionated towards building enterprise ready custom business apps. Includes additional batteries for out of the box enterprise readiness and rapid app development. Growing ecosystem of packages that serves as building blocks of apps. Zango also enables multi-tenancy where each tenant, representing an app/microservices, can be deployed independently on the same underlying monolith. Tenants have logically seperated db, codebase as well as deployment. This significantly cuts down per app hosting cost and enables microservices pattern without the cost overhead. Target Audience Enterprises: Benefits from the open core concept. No vendor lock-ins. Rapid development with out-of-the-box enterprise readiness. Startups: Get productive from day-1. Leverage packages to reach MVP really fast and not be constrained by limit on customizability (as with low-code/no-code solutions). Lowest cost of hosting if you have multiple apps or building microservices. Consulting/ Development companies: Increase development efficiency and optimize on hosting cost. You: If you are looking to develop any bespoke app, give it a try :) Comparison Web dev frameworks(e.g. Django): Not opinionated for enterprise readiness/ business apps. Zango enables faster development, lower opex and and built-in compliance and enterprise readiness Proprietary platforms (e.g. Salesforce): No vendor lock-in. Faster development Low-Code / No-Code: Limited customizability. More Info Know more at the project's Github repo: https://github.com/Healthlane-Technologies/Zango"},
{"Title": "pyDSLR: Easy-to-use wrapper around libgphoto2 to control your DSLR/DSLM from Linux/MacOS", "Author": "u/Zahlii", "Content": "What the Project Does The idea is to provide an easy to use (and fully typed, including camera settings!) abstraction around libgphoto2, allowing even non-tech-savy users to write Python scripts/sequences to take pictures. Generally, it supports all cameras that libgphoto2 also supports! Possible use cases are: Source code/examples available here (this one can be used to automatically take an image once a lightning strike is detected): https://github.com/Zahlii/pyDSLR/blob/main/examples/lightning_trigger.py Lightning trigger (showcased) Bulb capture (showcased) High Speed capture (e.g. using computer vision to detect animals and use the camera as part of a wildlife trap, partly showcased) Photo booths Timelapses (also for cameras that don't naturally support them) Focus bracketing (also for cameras that don't natively support them) Astro stacking (Taking hundreds of long exposures with fixed settings after another) With a computer-controllable astro mount we could also track the camera based on preview images Target Audience For now, mainly Python hobby photographers, but in the future hopefully also less tech savy hobbysts. Right now it is obviously still a work in progress (with only types available for my Canon R6II), and I am inviting people to reach out to me if they are interested in participating or have cameras to add to our types :) Comparison with Other Libraries When compared to other library around it: We wrap python-gphoto2's low level API gphoto2-cffi is an alternative, but not maintained in 7 years, lacks typing support and doesn't provide much benefits over existing low-level APIs"},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "The Python on Microcontrollers (and Raspberry Pi) Newsletter, a weekly news and project resource", "Author": "u/HP7933", "Content": "The Python on Microcontrollers (and Raspberry Pi) Newsletter: subscribe for free With the Python on Microcontrollers newsletter, you get all the latest information on Python running on hardware in one place! MicroPython, CircuitPython and Python on single Board Computers like Raspberry Pi & many more. The Python on Microcontrollers newsletter is the place for the latest news. It arrives Monday morning with all the week‚Äôs happenings. No advertising, no spam, easy to unsubscribe. 11,019 subscribers - the largest Python on hardware newsletter out there. Catch all the weekly news on Python for Microcontrollers with adafruitdaily.com . This ad-free, spam-free weekly email is filled with CircuitPython, MicroPython, and Python information that you may have missed, all in one place! Ensure you catch the weekly Python on Hardware roundup ‚Äì you can cancel anytime . Try our spam-free newsletter today! https://www.adafruitdaily.com/"},
{"Title": "From poetry to docker - easy way", "Author": "u/nicoloboschi", "Content": "Poetry plugin to generate Dockerfile and images automatically This project lets you generate a docker image or just a Dockerfile for your poetry application without manual setup It is meant for production images. https://github.com/nicoloboschi/poetry-dockerize-plugin https://pypi.org/project/poetry-dockerize-plugin/ Get started with poetry self add poetry-dockerize-plugin@latest This command generates a production-ready, optimized python image: poetry dockerize or to generate a Dockerfile poetry dockerize --generate"},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "A \"new\" Object & Vector Database for Python", "Author": "u/greenrobot_de", "Content": "ObjectBox ( GitHub ) is an embedded database for Python objects and high-dimensional vectors. Today is it's first stable release for Python developers. It's very lightweight similar to SQLite, but built for objects so it's faster as there's no SQL layer in-between. It's the very first vector database that also runs on smaller low-memory devices. The article comes with first benchmarks and hints at the LangChain integration."},
{"Title": "musicnotes: Python module for playing musical instruments!", "Author": "u/must1088", "Content": "https://github.com/must108/musicnotes What My Project Does musicnotes is a small open-source project that lets you play musical instruments (currently, only piano and guitar) in your Python programs. I created this project as I wanted to create a simple and useful open-source project for beginner developers to easily contribute to the project. I know it's hard to find good open-source projects for new developers. Target Audience Developers looking to add sounds to small games, or just have fun while learning Python in general. This library could also be used to teach Python and coding in a fun way. This project was also made to allow new Python developers to easily contribute to open-source! Feel free to star the repository, and download with pip install musicnotes ! You can also create a pull request with any changes you find useful, and visit the GitHub repository if you find any setbacks while using this module. There are a few things that can be worked on listed in the README of the repository if you're looking for a place to get started. Comparison This project is very simple and easy to use, and is easy to contribute to as well, which is one of the primary goals of the project."},
{"Title": "TerminalTextEffects (TTE) - A terminal visual effects engine, application, and library.", "Author": "u/XUtYwYzz", "Content": "I saw the words 'visual effects', just give me GIFs Understandable, visit the Effects Showroom first. Then come back if you like what you see. What My Project Does TerminalTextEffects (TTE) is a terminal visual effects engine. TTE can be installed as a system application to produce effects in your terminal, or as a Python library to enable effects within your Python scripts/applications. TTE includes a growing library of built-in effects which showcase the engine's features. Use cases: Invoke at terminal launch to produce an animation (ex: fetch). Alias system commands to animate output. Invoke on SSH session to blow people's minds when they log in. Use in your project to produce animated prompts, logos, etc. Target Audience TTE is a terminal toy (and now a Python library) that anybody can use to add visual flair to their terminal or projects. It works best in Linux but is functional in the new Windows Terminal. Every effect allows for significant customization including color gradient stops and directions as well as many effect-specific options. Customization is exposed via command-line arguments and through the Config class interface. The effect examples shown in the documentation represent a single configuration. Your experience can be very different with a little tweaking to match your system theme and preferences. Comparison I don't know of any other projects like TTE. It's a completely useless and over-engineered side-project that's turned into a whole thing. Have fun. More Info The GitHub README has some effect examples, installation instructions and some basic quick-start info."},
{"Title": "Preferred method to run python in VS Code", "Author": "u/SuperMB13", "Content": "Been working on a python tool for VS Code. Curious to get peoples' opinion on how they run python files (not notebooks) within VS Code. Do you typically run files python by: Typing the python command into the integrated terminal Clicking the run button at the top of the file Pressing F5 for debugging Pressing Ctrl+F5 for run but not debug Creating a custom keyboard shortcut Other Let me know your thoughts, I appreciate the insights!"},
{"Title": "Gloe: A lightweight lib to create readable and type-safe pipelines", "Author": "u/justme_sam", "Content": "Have you ever faced a moment when your code is a mess of nested classes and functions, and you have to dig through dozens of levels to understand a simple function? Gloe (pronounced like ‚Äúglow‚Äù) is a library designed to assist you organize your code into a type-safe flow, making it flat and linear. What My Project Does Here‚Äôs what it can do for you: Write type-safe pipelines with pure Python. Express your code as a set of atomic and extensible units of responsibility called transformers . Validate the input and output of transformers, and changes between them during execution. Mix sync and async¬†code without worrying about its concurrent nature. Keep your code readable and maintainable , even for complex flows . Visualize you pipelines and the data flowing through them. Use it anywhere without changing your existing workflow. Target Audience : any Python developer who sees their code as a flow (a series of sequential operations) and wants to improve its readability and maintainability. It's production-ready! Comparison : Currently, unlike platforms like¬†Air Flow¬†that include scheduler backends for task orchestration, Gloe‚Äôs primary purpose is to aid in development. The graph structure aims to make the code¬†more flat and readable. Example of usage in a server: send_promotion = ( get_users >> ( filter_basic_subscription >> send_basic_subscription_promotion_email, filter_premium_subscription >> send_premium_subscription_promotion_email, ) >> log_emails_result ) @users_router.post('/send-promotion/{role}') def send_promotion_emails_route(role: str): return send_promotion(role) Full code . Links : github.com/ideos/gloe gloe.ideos.com.br"},
{"Title": "Book Management Restful-API", "Author": "u/ThePawners", "Content": "What My Project Does: This project aims to provide a simple and efficient way to manage a collection of books through various API endpoints. This API allows you to: Get a list of all books. Add a new book. Get a book by its isbn. Update an existing book by its isbn. Delete a book by its isbn. API Endpoints: GET /api/v1/books - Retrieve all books. POST /api/v1/books - Add a new book. GET /api/v1/books/<ISBN> - Retrieve a book by its ISBN. PUT /api/v1/books/<ISBN> - Update a book by its ISBN. DELETE /api/v1/books/<ISBN> - Delete a book by its ISBN. Target Audience: Anyone who is interested to integrate book management api into their applications. Website API: Book Management API GitHub Repo: Book-Management-API on GitHub Follow Me: IG: @nordszamora Threads: @nordszamora Tiktok: @nordszamora Github: @nordszamora"},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "SH1106 OLED Screen App Framework for Raspberry Pi - Now on PyPI", "Author": "u/danyourmaster", "Content": "What it does: Today, I released the first working version of my SH1106 app framework for Raspberry Pi on PyPI! The SH1106 is an affordable OLED screen, costing under $3, and it's perfect for projects of all sizes. This package enables the creation of apps for it with graphics support, state management, image conversion utilities, and custom fonts. Check it out here: SH1106 Framework on PyPI . Target audience: The package is mainly aimed at hobbyists who want to create small projects using the SH1106 OLED without having to manually write a lot of the graphics code typically needed on top of standard packages. I am also developing a hardware synthesizer keyboard from scratch that utilizes this framework extensively. So far, the framework handles the massive scaling required for this project excellently in terms of both code organization and performance. Comparison: This package offers several advantages over other SH1106 packages: Improved Rendering Speed: It significantly speeds up the rendering time for a given frame by writing all graphical operations to a pixel array, which is then loaded onto the screen using low-level functions from the excellent luma.oled package. Efficient Resource Management: All images and fonts are pre-loaded during the initialization of the framework, reducing the processing time during rendering. State Management: A simple yet effective state management system is implemented, making app creation straightforward from the start. You can also check out the project on GitHub: SH1106 Framework on GitHub . I'd love to answer any questions you have in the comments! I hope you find some cool uses for it. Cheers! :)"},
{"Title": "Crowbar - Package Management without Venv", "Author": "u/coryalanfitz", "Content": "https://github.com/coryfitz/crowbar What it does: I'm working on a way of simplifying your Python dependency management. Basically, it handles virtual environments so you don‚Äôt have to think about them. First: pip install crowbar-package-manager Basically you just install and run things with the crowbar command rather than pip: crowbar install package_name And then you also run things with the crowbar command rather than using \"python\" - crowbar then runs the program based on the packages in the local environment rather than having to activate your virtual environment. It's inspired by npm if you've used that with js. Target audience: Anyone who currently uses the standard package management tools (requirements.txt, pip, etc) and wants to automate some of those processes. Comparison: The workflow is most similar to Poetry but there are a couple of major differences - for one thing, Crowbar only does package management; it doesn't create a project structure for you. Also, Poetry puts all of your environments in a central repository - Crowbar keeps it in your project folder. Unlike Poetry or any of the other dependency management tools out there, you don't have to buy into a completely different way of structuring your dependencies or your projects. A project that you use Crowbar on is identical to one where you used pip, venv, and requirements.txt - and if you try Crowbar and decide you don't like it, just activate your virtual environment like normal."},
{"Title": "An IDE with the same step by step functionality as in Matlab", "Author": "u/Cerricola", "Content": "When working with Matlab I love how I can run the code step by step to debug it. Even being able to \"step in\" functions and loops. Then, I was looking to an IDE with a similar functionality for Python. Nowadays I'm using Spyder."},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Spotify Lyrics visualizer", "Author": "u/Mews75", "Content": "What My Project Does Because spotify made their lyrics menu a premium only feature, I thought I'd make my own replacement for it. The app connects to your spotify account, fetches the lyrics from various websites, and then syncs them automatically to what is currently playing. Basically does the exact same as the lyrics menu used to do. Target Audience Anyone who wants to see the lyrics to songs really. Comparison Most other apps that I've found are either browser only, or don't actually sync the lyrics to the song, they just show the entire lyrics at once. In comparison, my app shows the lyrics line by line, synced with the song, and also has (in my opinion lol) a fairly nice looking ui. It's also very easy to use for non programmers too, since you can just download an executable to use the app. It's available for free here https://github.com/Mews/spotify-lyrics"},
{"Title": "I created an unofficial module for the ShipEngine API", "Author": "u/Status_Bid_1604", "Content": "What My Project Does Simplifies the interaction with the ShipEngine API with most response and requests built as objects, which in my opinion makes interaction much easier. This is my first released package so all criticism and feedback is very welcome. Target Audience Anyone who deals with the current ShipEngine API using Python. Comparison There is an official ShipEngine API module that is created by the company but I have found it somewhat lack luster with no way to create batches or bulk shipments (and other missing functionality), this is much more suited to accomplishing that task. Links https://github.com/Sen-tio/unofficial-shipengine"},
{"Title": "Xenharmlib - An advanced music theory library that supports microtonality", "Author": "u/realretooth", "Content": "Introducing Xenharmlib (Source code here ) What My Project Does (taken from the docs) Xenharmlib is a music theory library for the exploration and research of microtonality, diatonic set theory, non-standard notations, and many more. The library implements a superset of Western classical music theory, so you can also use it to compose and analyze music in the boundaries of the common practice period or 20th century Western music. Target Audience Composers who want to get answers to theoretical questions pertaining to structures of musical scales, note intervals, frequencies and frequency ratios in equal division tunings. People who want to explore microtonality or non-western musical theory in general. Comparison * mingus Xenharmlib is pretty much on-par with features in mingus, however extends those features to all sorts of equal division temperaments. * pytuning supports more slightly tuning methods and export formats, however does not support microtonal notation or note / interval calculation * music21 is much more mature in providing an analytical toolset, however supports only traditional western equal temperament"},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "AI planner: AI tool for efficient event scheduling in Google Calendar.", "Author": "u/nginx26", "Content": "Good evening! I have created a new projectfor adding events to google calendar based on the text a user inputs. What My Project Does The project is a tool that uses large language models to understand the user's input and add events to the user's Google Calendar based on the user's input. It uses Ollama for natural language understanding and Google Calendar API for adding events to the user's calendar. How My Project Works Ollama uses Llama 3 with pre-instructions to act as a calendar event planner. The tool uses the model to generate responses to extract the event's details from the user's input inserted in the Web Interface. tool then asks the user to confirm the details extracted from the user's input and adds the event to the user's Google Calendar (example shown here ) References Checkout my github repository AIPlanner for more details about the project."},
{"Title": "PyPods: A lightweight solution to execute Python dependencies in an isolated fashion.", "Author": "u/Brilliant_Emphasis63", "Content": "Introducing PyPods What My Project Does A Python library designed to manage monolithic project architectures by isolating dependencies. Traditionally, monolithic architectures cluster all dependencies into one project, creating complexities and potential conflicts. PyPods offers a solution by isolating these dependencies and enabling the main project to communicate with them via remote procedure calls. This approach eliminates the need to install dependencies directly in the main project. Feel free to take a look and I am happy to receive some feedback! Target Audience Production grade. Comparison This solution is inspired by Babashka pods in the Clojure world."},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "PyData Amsterdam 2024 Call for Proposals closes on Sunday, June 2", "Author": "u/PyDataAmsterdam", "Content": "Hey all, we will close the Call for Proposals portal this Sunday, June 2 , for our PyData Amsterdam 2024 Conference which will take place on September 18-20 in Amsterdam. We are looking for presentations that can captivate our audience, provide invaluable insights, and foster community learning. Don't miss this chance to speak on stage in front of over 800 attendees in the field of Data & AI. Submit a talk here > https://amsterdam2024.pydata.org/cfp/cfp"},
{"Title": "Zango - New python framework for building enterprise ready business apps. Salesforce alternative.", "Author": "u/CanaryHill", "Content": "What My Project Does Zango, built on top of Django, is further opinionated towards building enterprise ready custom business apps. Includes additional batteries for out of the box enterprise readiness and rapid app development. Growing ecosystem of packages that serves as building blocks of apps. Zango also enables multi-tenancy where each tenant, representing an app/microservices, can be deployed independently on the same underlying monolith. Tenants have logically seperated db, codebase as well as deployment. This significantly cuts down per app hosting cost and enables microservices pattern without the cost overhead. Target Audience Enterprises: Benefits from the open core concept. No vendor lock-ins. Rapid development with out-of-the-box enterprise readiness. Startups: Get productive from day-1. Leverage packages to reach MVP really fast and not be constrained by limit on customizability (as with low-code/no-code solutions). Lowest cost of hosting if you have multiple apps or building microservices. Consulting/ Development companies: Increase development efficiency and optimize on hosting cost. You: If you are looking to develop any bespoke app, give it a try :) Comparison Web dev frameworks(e.g. Django): Not opinionated for enterprise readiness/ business apps. Zango enables faster development, lower opex and and built-in compliance and enterprise readiness Proprietary platforms (e.g. Salesforce): No vendor lock-in. Faster development Low-Code / No-Code: Limited customizability. More Info Know more at the project's Github repo: https://github.com/Healthlane-Technologies/Zango"},
{"Title": "pyDSLR: Easy-to-use wrapper around libgphoto2 to control your DSLR/DSLM from Linux/MacOS", "Author": "u/Zahlii", "Content": "What the Project Does The idea is to provide an easy to use (and fully typed, including camera settings!) abstraction around libgphoto2, allowing even non-tech-savy users to write Python scripts/sequences to take pictures. Generally, it supports all cameras that libgphoto2 also supports! Possible use cases are: Source code/examples available here (this one can be used to automatically take an image once a lightning strike is detected): https://github.com/Zahlii/pyDSLR/blob/main/examples/lightning_trigger.py Lightning trigger (showcased) Bulb capture (showcased) High Speed capture (e.g. using computer vision to detect animals and use the camera as part of a wildlife trap, partly showcased) Photo booths Timelapses (also for cameras that don't naturally support them) Focus bracketing (also for cameras that don't natively support them) Astro stacking (Taking hundreds of long exposures with fixed settings after another) With a computer-controllable astro mount we could also track the camera based on preview images Target Audience For now, mainly Python hobby photographers, but in the future hopefully also less tech savy hobbysts. Right now it is obviously still a work in progress (with only types available for my Canon R6II), and I am inviting people to reach out to me if they are interested in participating or have cameras to add to our types :) Comparison with Other Libraries When compared to other library around it: We wrap python-gphoto2's low level API gphoto2-cffi is an alternative, but not maintained in 7 years, lacks typing support and doesn't provide much benefits over existing low-level APIs"},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "The Python on Microcontrollers (and Raspberry Pi) Newsletter, a weekly news and project resource", "Author": "u/HP7933", "Content": "The Python on Microcontrollers (and Raspberry Pi) Newsletter: subscribe for free With the Python on Microcontrollers newsletter, you get all the latest information on Python running on hardware in one place! MicroPython, CircuitPython and Python on single Board Computers like Raspberry Pi & many more. The Python on Microcontrollers newsletter is the place for the latest news. It arrives Monday morning with all the week‚Äôs happenings. No advertising, no spam, easy to unsubscribe. 11,019 subscribers - the largest Python on hardware newsletter out there. Catch all the weekly news on Python for Microcontrollers with adafruitdaily.com . This ad-free, spam-free weekly email is filled with CircuitPython, MicroPython, and Python information that you may have missed, all in one place! Ensure you catch the weekly Python on Hardware roundup ‚Äì you can cancel anytime . Try our spam-free newsletter today! https://www.adafruitdaily.com/"},
{"Title": "From poetry to docker - easy way", "Author": "u/nicoloboschi", "Content": "Poetry plugin to generate Dockerfile and images automatically This project lets you generate a docker image or just a Dockerfile for your poetry application without manual setup It is meant for production images. https://github.com/nicoloboschi/poetry-dockerize-plugin https://pypi.org/project/poetry-dockerize-plugin/ Get started with poetry self add poetry-dockerize-plugin@latest This command generates a production-ready, optimized python image: poetry dockerize or to generate a Dockerfile poetry dockerize --generate"},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "A \"new\" Object & Vector Database for Python", "Author": "u/greenrobot_de", "Content": "ObjectBox ( GitHub ) is an embedded database for Python objects and high-dimensional vectors. Today is it's first stable release for Python developers. It's very lightweight similar to SQLite, but built for objects so it's faster as there's no SQL layer in-between. It's the very first vector database that also runs on smaller low-memory devices. The article comes with first benchmarks and hints at the LangChain integration."},
{"Title": "musicnotes: Python module for playing musical instruments!", "Author": "u/must1088", "Content": "https://github.com/must108/musicnotes What My Project Does musicnotes is a small open-source project that lets you play musical instruments (currently, only piano and guitar) in your Python programs. I created this project as I wanted to create a simple and useful open-source project for beginner developers to easily contribute to the project. I know it's hard to find good open-source projects for new developers. Target Audience Developers looking to add sounds to small games, or just have fun while learning Python in general. This library could also be used to teach Python and coding in a fun way. This project was also made to allow new Python developers to easily contribute to open-source! Feel free to star the repository, and download with pip install musicnotes ! You can also create a pull request with any changes you find useful, and visit the GitHub repository if you find any setbacks while using this module. There are a few things that can be worked on listed in the README of the repository if you're looking for a place to get started. Comparison This project is very simple and easy to use, and is easy to contribute to as well, which is one of the primary goals of the project."},
{"Title": "TerminalTextEffects (TTE) - A terminal visual effects engine, application, and library.", "Author": "u/XUtYwYzz", "Content": "I saw the words 'visual effects', just give me GIFs Understandable, visit the Effects Showroom first. Then come back if you like what you see. What My Project Does TerminalTextEffects (TTE) is a terminal visual effects engine. TTE can be installed as a system application to produce effects in your terminal, or as a Python library to enable effects within your Python scripts/applications. TTE includes a growing library of built-in effects which showcase the engine's features. Use cases: Invoke at terminal launch to produce an animation (ex: fetch). Alias system commands to animate output. Invoke on SSH session to blow people's minds when they log in. Use in your project to produce animated prompts, logos, etc. Target Audience TTE is a terminal toy (and now a Python library) that anybody can use to add visual flair to their terminal or projects. It works best in Linux but is functional in the new Windows Terminal. Every effect allows for significant customization including color gradient stops and directions as well as many effect-specific options. Customization is exposed via command-line arguments and through the Config class interface. The effect examples shown in the documentation represent a single configuration. Your experience can be very different with a little tweaking to match your system theme and preferences. Comparison I don't know of any other projects like TTE. It's a completely useless and over-engineered side-project that's turned into a whole thing. Have fun. More Info The GitHub README has some effect examples, installation instructions and some basic quick-start info."},
{"Title": "Preferred method to run python in VS Code", "Author": "u/SuperMB13", "Content": "Been working on a python tool for VS Code. Curious to get peoples' opinion on how they run python files (not notebooks) within VS Code. Do you typically run files python by: Typing the python command into the integrated terminal Clicking the run button at the top of the file Pressing F5 for debugging Pressing Ctrl+F5 for run but not debug Creating a custom keyboard shortcut Other Let me know your thoughts, I appreciate the insights!"},
{"Title": "Gloe: A lightweight lib to create readable and type-safe pipelines", "Author": "u/justme_sam", "Content": "Have you ever faced a moment when your code is a mess of nested classes and functions, and you have to dig through dozens of levels to understand a simple function? Gloe (pronounced like ‚Äúglow‚Äù) is a library designed to assist you organize your code into a type-safe flow, making it flat and linear. What My Project Does Here‚Äôs what it can do for you: Write type-safe pipelines with pure Python. Express your code as a set of atomic and extensible units of responsibility called transformers . Validate the input and output of transformers, and changes between them during execution. Mix sync and async¬†code without worrying about its concurrent nature. Keep your code readable and maintainable , even for complex flows . Visualize you pipelines and the data flowing through them. Use it anywhere without changing your existing workflow. Target Audience : any Python developer who sees their code as a flow (a series of sequential operations) and wants to improve its readability and maintainability. It's production-ready! Comparison : Currently, unlike platforms like¬†Air Flow¬†that include scheduler backends for task orchestration, Gloe‚Äôs primary purpose is to aid in development. The graph structure aims to make the code¬†more flat and readable. Example of usage in a server: send_promotion = ( get_users >> ( filter_basic_subscription >> send_basic_subscription_promotion_email, filter_premium_subscription >> send_premium_subscription_promotion_email, ) >> log_emails_result ) @users_router.post('/send-promotion/{role}') def send_promotion_emails_route(role: str): return send_promotion(role) Full code . Links : github.com/ideos/gloe gloe.ideos.com.br"},
{"Title": "Book Management Restful-API", "Author": "u/ThePawners", "Content": "What My Project Does: This project aims to provide a simple and efficient way to manage a collection of books through various API endpoints. This API allows you to: Get a list of all books. Add a new book. Get a book by its isbn. Update an existing book by its isbn. Delete a book by its isbn. API Endpoints: GET /api/v1/books - Retrieve all books. POST /api/v1/books - Add a new book. GET /api/v1/books/<ISBN> - Retrieve a book by its ISBN. PUT /api/v1/books/<ISBN> - Update a book by its ISBN. DELETE /api/v1/books/<ISBN> - Delete a book by its ISBN. Target Audience: Anyone who is interested to integrate book management api into their applications. Website API: Book Management API GitHub Repo: Book-Management-API on GitHub Follow Me: IG: @nordszamora Threads: @nordszamora Tiktok: @nordszamora Github: @nordszamora"},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "SH1106 OLED Screen App Framework for Raspberry Pi - Now on PyPI", "Author": "u/danyourmaster", "Content": "What it does: Today, I released the first working version of my SH1106 app framework for Raspberry Pi on PyPI! The SH1106 is an affordable OLED screen, costing under $3, and it's perfect for projects of all sizes. This package enables the creation of apps for it with graphics support, state management, image conversion utilities, and custom fonts. Check it out here: SH1106 Framework on PyPI . Target audience: The package is mainly aimed at hobbyists who want to create small projects using the SH1106 OLED without having to manually write a lot of the graphics code typically needed on top of standard packages. I am also developing a hardware synthesizer keyboard from scratch that utilizes this framework extensively. So far, the framework handles the massive scaling required for this project excellently in terms of both code organization and performance. Comparison: This package offers several advantages over other SH1106 packages: Improved Rendering Speed: It significantly speeds up the rendering time for a given frame by writing all graphical operations to a pixel array, which is then loaded onto the screen using low-level functions from the excellent luma.oled package. Efficient Resource Management: All images and fonts are pre-loaded during the initialization of the framework, reducing the processing time during rendering. State Management: A simple yet effective state management system is implemented, making app creation straightforward from the start. You can also check out the project on GitHub: SH1106 Framework on GitHub . I'd love to answer any questions you have in the comments! I hope you find some cool uses for it. Cheers! :)"},
{"Title": "Crowbar - Package Management without Venv", "Author": "u/coryalanfitz", "Content": "https://github.com/coryfitz/crowbar What it does: I'm working on a way of simplifying your Python dependency management. Basically, it handles virtual environments so you don‚Äôt have to think about them. First: pip install crowbar-package-manager Basically you just install and run things with the crowbar command rather than pip: crowbar install package_name And then you also run things with the crowbar command rather than using \"python\" - crowbar then runs the program based on the packages in the local environment rather than having to activate your virtual environment. It's inspired by npm if you've used that with js. Target audience: Anyone who currently uses the standard package management tools (requirements.txt, pip, etc) and wants to automate some of those processes. Comparison: The workflow is most similar to Poetry but there are a couple of major differences - for one thing, Crowbar only does package management; it doesn't create a project structure for you. Also, Poetry puts all of your environments in a central repository - Crowbar keeps it in your project folder. Unlike Poetry or any of the other dependency management tools out there, you don't have to buy into a completely different way of structuring your dependencies or your projects. A project that you use Crowbar on is identical to one where you used pip, venv, and requirements.txt - and if you try Crowbar and decide you don't like it, just activate your virtual environment like normal."},
{"Title": "An IDE with the same step by step functionality as in Matlab", "Author": "u/Cerricola", "Content": "When working with Matlab I love how I can run the code step by step to debug it. Even being able to \"step in\" functions and loops. Then, I was looking to an IDE with a similar functionality for Python. Nowadays I'm using Spyder."},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Spotify Lyrics visualizer", "Author": "u/Mews75", "Content": "What My Project Does Because spotify made their lyrics menu a premium only feature, I thought I'd make my own replacement for it. The app connects to your spotify account, fetches the lyrics from various websites, and then syncs them automatically to what is currently playing. Basically does the exact same as the lyrics menu used to do. Target Audience Anyone who wants to see the lyrics to songs really. Comparison Most other apps that I've found are either browser only, or don't actually sync the lyrics to the song, they just show the entire lyrics at once. In comparison, my app shows the lyrics line by line, synced with the song, and also has (in my opinion lol) a fairly nice looking ui. It's also very easy to use for non programmers too, since you can just download an executable to use the app. It's available for free here https://github.com/Mews/spotify-lyrics"},
{"Title": "I created an unofficial module for the ShipEngine API", "Author": "u/Status_Bid_1604", "Content": "What My Project Does Simplifies the interaction with the ShipEngine API with most response and requests built as objects, which in my opinion makes interaction much easier. This is my first released package so all criticism and feedback is very welcome. Target Audience Anyone who deals with the current ShipEngine API using Python. Comparison There is an official ShipEngine API module that is created by the company but I have found it somewhat lack luster with no way to create batches or bulk shipments (and other missing functionality), this is much more suited to accomplishing that task. Links https://github.com/Sen-tio/unofficial-shipengine"},
{"Title": "Xenharmlib - An advanced music theory library that supports microtonality", "Author": "u/realretooth", "Content": "Introducing Xenharmlib (Source code here ) What My Project Does (taken from the docs) Xenharmlib is a music theory library for the exploration and research of microtonality, diatonic set theory, non-standard notations, and many more. The library implements a superset of Western classical music theory, so you can also use it to compose and analyze music in the boundaries of the common practice period or 20th century Western music. Target Audience Composers who want to get answers to theoretical questions pertaining to structures of musical scales, note intervals, frequencies and frequency ratios in equal division tunings. People who want to explore microtonality or non-western musical theory in general. Comparison * mingus Xenharmlib is pretty much on-par with features in mingus, however extends those features to all sorts of equal division temperaments. * pytuning supports more slightly tuning methods and export formats, however does not support microtonal notation or note / interval calculation * music21 is much more mature in providing an analytical toolset, however supports only traditional western equal temperament"},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "AI planner: AI tool for efficient event scheduling in Google Calendar.", "Author": "u/nginx26", "Content": "Good evening! I have created a new projectfor adding events to google calendar based on the text a user inputs. What My Project Does The project is a tool that uses large language models to understand the user's input and add events to the user's Google Calendar based on the user's input. It uses Ollama for natural language understanding and Google Calendar API for adding events to the user's calendar. How My Project Works Ollama uses Llama 3 with pre-instructions to act as a calendar event planner. The tool uses the model to generate responses to extract the event's details from the user's input inserted in the Web Interface. tool then asks the user to confirm the details extracted from the user's input and adds the event to the user's Google Calendar (example shown here ) References Checkout my github repository AIPlanner for more details about the project."},
{"Title": "PyPods: A lightweight solution to execute Python dependencies in an isolated fashion.", "Author": "u/Brilliant_Emphasis63", "Content": "Introducing PyPods What My Project Does A Python library designed to manage monolithic project architectures by isolating dependencies. Traditionally, monolithic architectures cluster all dependencies into one project, creating complexities and potential conflicts. PyPods offers a solution by isolating these dependencies and enabling the main project to communicate with them via remote procedure calls. This approach eliminates the need to install dependencies directly in the main project. Feel free to take a look and I am happy to receive some feedback! Target Audience Production grade. Comparison This solution is inspired by Babashka pods in the Clojure world."},
{"Title": "I made a desktop chat app :)", "Author": "u/Reasonable-Zone-7909", "Content": "What My Project Does Hi! This is my first time doing a python project more than a few hours in size. I made a chat app which features E2E encryption using a passcode and has a multiclient architecture. All comments are welcome! Target Audience It is just a toy project for my portfolio. Comparison Compared to other chat clients, this one uses a passphrase to encrypt all data, with the passphrase being chosen out of the app, for instance on a dinner. But I think that IRC already has this, so it doesn't differ much XD. Git link: https://github.com/xxzoltanxx/Balvan-Chat"},
{"Title": "Rye-Tui, a Text-based User Interface (TUI) to manage rye projects", "Author": "u/Zaloog1337", "Content": "Hi everyone, Ive reached a state of my current project, where I want to share it with you, and gather some feedback. This is my first time using rye and I am surprised, how Hassle-Free building a package with it went. Source Code: github Installation python -m pip install rye-tui for CLI Tools I recommend using pipx or rye. pipx install rye-tui rye install rye-tui After Installation you can open the TUI using trye in your Terminal. On first use a config file is generated. After that use trye again in your rye managed project What My Project Does A Text-based User Interface (TUI) for rye written in python using Textual Current State Currently rye-tui supports the following functionalities of rye: creating new projects (flag-support coming soon) adding normal and dev dependencies (flag-support coming soon) pinning versions Syncing (flag-support coming soon) changing rye's configuration (sources and default coming soon) Target Audience Python developers and rye users who like a UI to manage their rye projects. Comparison To my knowledge there is no similar tool for rye. Maybe the Anaconda UI comes closest for Anaconda Users. Last Words Feel free to try(e) it out. Happy to hear your feedback."},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "I built a pipeline sending my wife and I SMSs twice a week with budgeting advice generated by AI", "Author": "u/MoodAppropriate4108", "Content": "What My Project Does : I built a pipeline of¬†Dagger¬†modules to send my wife and I SMSs twice a week with actionable financial advice generated by AI based on data from bank accounts regarding our daily spending. Details: Dagger is an open source programmable CI/CD engine. I built each step in the pipeline as a Dagger method. Dagger spins up ephemeral containers, running everything within its own container. I use GitHub Actions to trigger dagger methods that; retrieve data from a source filter for new transactions Categorizes transactions using a zero shot model, facebook/bart-large-mnli through the HuggingFace API. This process is optimized by sending data in dynamically sized batches asynchronously. Writes the data to a MongoDB database Retrieves the data, using Atlas search to aggregate the data by week and categories Sends the data to openAI to generate financial advice. In this module, I implement a memory using LangChain. I store this memory in MongoDB to persist the memory between build runs. I designed the database to rewrite the data whenever I receive new data. The memory keeps track of feedback given, enabling the advice to improve based on feedback This response is sent via SMS through the TextBelt API Full Blog: https://emmanuelsibanda.hashnode.dev/a-dagger-pipeline-sending-weekly-smss-with-financial-advice-generated-by-ai Video Demo: https://youtu.be/S45n89gzH4Y GitHub Repo: https://github.com/EmmS21/daggerverse Target Audience: Personal project (family and friends) Comparison : We have too many budgeting apps and wanted to receive this advice via SMS, personalizing it based on our changing financial goals A screenshot of the message sent: https://ibb.co/Qk1wXQK"},
{"Title": "JSX Syntax inside Python files. (Packed)", "Author": "u/RevolutionaryPen4661", "Content": "There was a JSX-style syntax preprocessor for Python called \" Packed ,\" which allowed us to write JSX inside Python (*.pyx and *.py) files. It's unclear why they chose *.pyx for the file extension, as it conflicts with the naming of Cythonic file extensions (I have checked their issues). This project might have thrived with sufficient contributions and could have changed the way apps are built. However, the project is now archived on GitHub. The last commit was 5 years ago (LICENSE), and the last development commit was 9 years ago. This repository needs someone to revive it, but I don't have enough experience to take on that task. Even though I don't have enough information, we should start with Rust + Python to build a compiler (aka. template replacer) (this doesn't compile Python but replaces all JSX with a dictionary) and cleaner syntax. Integration with Django (Packed has an example too), Flask, FastAPI, Robyn etc. We may also need plugins for the language server, I recommend supporting with *.pyh or *.psx (a fork renamed name) the extension file name (Derived from Python + HTML). VSCODE and NVIM Extensions are required to build support for this. The existing modern syntax of native Python will continue to support this syntax. I made a Handlebars Extension for the community back in the day of 2022 but I don't want to continue the development for it because later I disliked the syntax of handlebars (opinion, you're point of view may contrast with my thoughts). We can use emmet for writing easy HTML. @packed def tag(self): share = get_share_link() return <a href={share}>Share on internet</a> The main point of view is that somehow make returnable components as üëÜ instead of doing this üëá def app(): return div(div(p(\"Hello World\")),span(\"Not a Good Approach for someone (opinion)\"))"},
{"Title": "We built open-source SDK for adding custom code interpreters to AI apps", "Author": "u/mlejva", "Content": "What My Project Does Hey everyone! I'm the CEO of the company that built an SDK that makes it easy to build custom code interpreters for AI apps . We're a company called E2B [0]. We're building and open-source [1] secure environments for running untrusted AI-generated code and AI agents. We call these environments sandboxes and they are built on top of micro VM called Firecracker [2]. We specifically decided to use Firecrackers instead of containers because of their security and ability to do snapshots. You can think of us as giving small cloud computers to LLMs. We recently created a dedicated SDK for building custom code interpreters in Python or JS/TS. We saw this need after a lot of our users have been adding code execution capabilities to their AI apps with our core SDK [3]. These use cases were often centered around AI data analysis so code interpreter-like behavior made sense The way our code interpret SDK works is by spawning an E2B sandbox with Jupyter Server. We then communicate with this Jupyter server through Jupyter Kernel messaging protocol [4]. Here's our cookbook showing how to add code interpreter to different models [5]. We don't do any wrapping around LLM, any prompting, or any agent-like framework. We leave all of that to our users. We're really just a boring code execution layer that sits at the bottom. We're building for the future software that will be building another software. Our long-term plan is to build an automated AWS for AI apps and agents where AI can build and deploy its own software while giving developers powerful observability into what's happening inside our sandboxes. With everything being open-source. Happy to answer any questions and hear feedback! Target Audience You can use it in production. We have companies using us in production already. Comparison Alternatives we usually see are serverless functions or Docker containers. Both have security issues. With serverless functions you can leak data between users and with containers you don't really have true isolation. Containers were made for packaging and portability, not security. Links https://github.com/e2b-dev/code-interpreter [0] https://e2b.dev/ [1] https://github.com/e2b-dev [2] https://github.com/firecracker-microvm/firecracker [3] https://e2b.dev/docs [4] https://jupyter-client.readthedocs.io/en/latest/messaging.html [5] https://github.com/e2b-dev/e2b-cookbook"},
{"Title": "I made a small Python script that uses NASA'S APOD API to set cool backgrounds on a Windows machine", "Author": "u/Potato_eating_a_dog", "Content": "https://github.com/william7491681/APOD_Wallpaper_Script What my project does NASA has a ton of accessible API's, one of which being the APOD (Astronomy Picture Of the Day) API. I made a script to get the last 9 pictures of the day and set them as my Windows 10 background, and then used task scheduler to have the script re-run every day at noon and whenever the computer boots up. It's fairly hard coded for my setup (specific file paths, 1920x1080 monitor, etc), but it shouldn't be too hard to change if one wanted to. Target audience Anyone who likes space backgrounds Comparison Idk, automod made me put this section"},
{"Title": "DeepFusion: a highly modular Deep Learning Framework.", "Author": "u/atharvaaalok1", "Content": "What My Project Does: Hello all, I am a student at Stanford University, I was on a gap year due to medical conditions and to utilitze my time I was studying deep learning. And Voila... I've developed a deep learning library, DeepFusion ! Details: It's customizable and has an easily accessible and highly intuitive codebase. One can just dive right in and effortlessly understand the source code. You can download it from: github at https://github.com/atharvaaalok/deepfusion or install using pip install deepfusion (easy!) For a series of examples explaining the usage and features refer demo or tutorials . Target Audience: Machine learning and python enthusiasts. Comparison: DeepFusion allows explicit access to all activations in a neural network, therefore, making applications such as neural style transfer much easier to perform. It also provides an easy user interface for forward and backward pass profiling, multiple loss functions, automated training, gpu training etc."},
{"Title": "TPC-H Cloud Benchmarks: Spark, Dask, DuckDB, Polars", "Author": "u/mrocklin", "Content": "I hit publish on a blogpost last week on running Spark, Dask, DuckDB, and Polars on the TPC-H benchmark across a variety of scales (10 GiB, 100 GiB, 1 TiB, 10 TiB), both locally on a Macbook Pro and on the cloud.¬† It‚Äôs a broad set of configurations.¬† The results are interesting. No project wins uniformly.¬† They all perform differently at different scales: DuckDB and Polars are crazy fast on local machines Dask and DuckDB seem to win on cloud and at scale Dask ends up being most robust, especially at scale DuckDB does shockingly well on large datasets on a single large machine Spark performs oddly poorly, despite being the standard choice üò¢ Tons of charts in this post to try to make sense of the data.¬† If folks are curious, here‚Äôs the post: https://docs.coiled.io/blog/tpch.html And here's the code. Performance isn‚Äôt everything of course.¬† Each project has its die-hard fans/critics for loads of different reasons. I'd be curious to hear if people want to defend/critique their project of choice."},
{"Title": "MyTimer v1.3: A Geeky Timer for Terminal Enthusiasts", "Author": "u/sepandhaghighi", "Content": "GitHub Repo: https://github.com/sepandhaghighi/mytimer What My Project Does: MyTimer is a Python CLI project that provides a simple, efficient timer for terminal users, particularly targeting the geek community. It allows users to set timers directly from their command line interface, offering a distraction-free experience. mytimer --hour=12 --minute=34 --second=56 --alarm --alarm-repeat=5 ___    ______          ______   _     _         _______  _______ (___)  (_____ \\        (_____ \\ | |   (_)       (_______)(_______) _     ____) )   _    _____) )| |_____    _    ______   ______ | |   / ____/   (_)  (_____ ( |_____  |  (_)  (_____ \\ |  ___ \\ _| |_ | (_____    _    _____) )      | |   _    _____) )| |___) ) (_____)|_______)  (_)  (______/       |_|  (_)  (______/ |______/ Target Audience: Developers who spend a significant amount of time working in the terminal :) Comparison: MyTimer supports more features compared to countdown MyTimer offers a greater variety of faces and functions than timer-cli"},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Thank You PyConUS 2024 !!!", "Author": "u/ryanstephendavis", "Content": "First timer this year, currently at the airport leaving Pittsburgh after 6 days of PyCon... I've never seen such an intelligent, inclusive, humble, diverse, and inspiring group of human beings.  The Python community serves as a beautiful model of what tech culture should strive towards. I could go on and on about how much fun I had, but in short, thanks to all the volunteers, staff, and FOSS developers that have cultivated such an amazing culture."},
{"Title": "Speed improvements in Polars over Pandas", "Author": "u/zzoetrop_1999", "Content": "I'm giving a talk on polars in July. It's been pretty fast for us, but I'm curious to hear some examples of improvements other people have seen. I got one process down from over three minutes to around 10 seconds. Also curious whether people have switched over to using polars instead of pandas or they reserve it for specific use cases."},
{"Title": "Interfacing Custom USB endpoints using Python!", "Author": "u/shekhuu", "Content": "Hi everyone, I was building something that required me to communicate over USB to Raspberry Pi Pico using Pyusb Python. So I decided to make a blog post about it showing the concepts, process, and source code. Check out the blog post here! Check out the source code here!"},
{"Title": "Mystique: Sparse data matching for Python tests", "Author": "u/jonocodes", "Content": "What My Project Does I made this library to help assert test responses inline while directing the comparison to be as rigid or lax as it needs to be. Motivation I write a lot of tests that assert values in complex nested dictionaries. But really I only need to check some parts in the response, not all of it. I often find myself transforming the response or maliciously extracting the important parts I need - in order to satisfy the assertions. This gets messy and can make tests hard to follow. Target Audience Anyone who writes tests. This is particularly useful if you generate fake data in your tests with something like Faker, Factory Boy, or Model Bakery. Comparison I have not found a like-project. Searched high and low in PyPI. If such a library existed, I would not have written one myself. Feedback appreciated. See PyPI project for basic use and github tests for more complex examples ."},
{"Title": "I made python audioplayer with FFmpeg and Qt6", "Author": "u/WyseTwist", "Content": "Midnight Player - a simple python audiplayer for playing audio What My Project Does: My project is just an audio player for playing music, it can play audio from folders, supports different audio formats like Flac, Mp3 and can show some information about the track. Player uses subprocess to access ffmpeg, then it decodes the audio file into pcm format, then plays this file using sounddevice library. the use of these libraries is to ensure that the audio file does not lose quality during processing. Target Audience: This project was made to increase experience in python programming and to understand how the audio playback process works, but the project is also useful for people who are interested in learning the structure of the audio player as it is open source. Comparison: First of all you should understand that this player is not trying to compete with large-scale projects like AIMP because I developed this project alone and the project was written in a short period of time. But if you compare with other python audio players on github you will notice that many people use wrong libraries like qmediaplayer or pygame mixer to create their audio player, which are not designed for wide support of audio formats, my project is much more complex to operate audio file. Packages and source code can be found here: https://github.com/Niamorro/Midnight-Player"},
{"Title": "How to publish a Python package with GitHub Actions using Poetry", "Author": "u/johnfraney", "Content": "Hi! I've been enjoying using PyPI's trusted publishing for the Python packages I maintain and I threw together a little post showing how I'm using that along with Poetry to publish a package from GitHub https://johnfraney.ca/blog/how-to-publish-a-python-package-with-poetry-and-github-actions/ If you've got any tips for publishing a Python package, I'd be happy to hear those, too"},
{"Title": "Reforger Whitelist Py", "Author": "u/PyjamaZombie", "Content": "My project below, to put it simply, periodically checks the console.log for when a player join event occurs, when it does, it extracts the player's identifiers ( player_name and identity_id ). This is then checked against either, a JSON or, a database. I have incorporated standard logging, command-line arguments and threading to handle each player process individually. The target audience for this is the Arma Reforger community, for which, the application is made for. Currently, to my knowledge, there is no application like this available to the Arma Reforger community. I am very open to feedback, contributions and advice as want to expand this as much as possible! https://github.com/BreathXV/ReforgerWhitelistPy"},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "I made a Traversible Tree in Python", "Author": "u/frankstan33", "Content": "Comparison It is inspired from the existing tree command on linux and windows too So basically it is just like the tree command, it shows you a tree of the current directory structure. What My Project Does It basically gives you a birds eye view of your dir structure and quickly navigate to the folder you want to without having to know its path or doing cd ../../.. many times. There are a bunch of command line args such as setting the paths, flags to show dot directories, set head height (no. of parent dirs shown) and tail height (depth). You can traverse around the tree using various key presses (inspired from vim keybindings) and based on the given argument (-o, -c or --copy) you can output the value (the node to which you traversed), cd into it and have it copied into your clipboard.J I had created this for my assignment and had a lot of fun with it. Tried to implement as much clean code and good design as I could but its still a mess and active work in progress tbh (added unit tests lol). And the rendering is still a little slow. Do check it out: pranavpa8788/trav: A Traversible Tree command line program (github.com) and let me know what you guys think. It is built with support for Windows and Linux, some installation stuff might be needed though, and I'll update those steps soon in the github page Target Audience For anyone really, especially if you use a lot of terminal (Had to add the titles because my post was getting auto-deleted lol) Link to video demo: https://streamable.com/ds911k"},
{"Title": "Programmable Semantics (Eval, Semicolon, Assignment) for Python", "Author": "u/aoeu512", "Content": "I've seen programmable semantics (eval-hacking, macros) in LISPs and in Haskell-likes(Monads/Template Haskell), the overall techinque in OOP languages is called \"Aspect Oriented Programming\".  Has this kind of thing been discussed before, and is it Pythonic it could allow a lot of Python code to be shorter.  Python has sys.set_trace that sort of allows some form of programmable semantics but its mostly for debugging. Programmable assignment(variables) are like setters/getters/properties, but instead of being run on o.x = 5, you could run them on \"all local assignments\" isnside a context manager or in a decorated function.  On every assignment you could do stuff like log the values, update dependencies, notify objects, do a database transaction, do persistance, calculate other values, without having to explicitly do so for every assignment statement. Programmable semicolons (such as Haskell Monads, or reprogramming Lisp do/progn/let) could allow you to have the same code run either synchronous, async, get undo/history support, break on error, rollback, logging in between lines, changing local/global variables in between each line, database access in between lines, checking values for errors, ignoring certain statements, etc...  You can think of a semicolon like an \"unrolled for loop\"/iterator ran for each code line.  It would be like async but you can change a piece of code to be sync or async at run time by changing the context manager you are in.  Programmable \"call\" can change the default call operation in a context manager for all functions and be similar to semicolons. Programmable eval would allow you to change the order of operations, choose to ignore certain functions, allow you replace certain expensive expressions with others, allow you to keep a trace of all evaluations taking place, you can turn an expression/program into an interator allowing you to pretty cool stuff."},
{"Title": "try... except... finally!", "Author": "u/young-and-ignorant", "Content": "Haven't seen this syntax used very often and was wondering why. During error handling, if you have something to run independent of the success, you can use finally. from your_library import DataProcess engine = DataProcess() try: engine.io() engine.process() engine.some_more_io() except Exception as e: engine.revert() raise e finally: engine.cleanup() VS from your_library import DataProcess engine = DataProcess() try: engine.io() engine.process() engine.some_more_io() except Exception as e: engine.revert() engine.cleanup() raise e engine.cleanup() VS from your_library import DataProcess from contextlib import contextmanager @contextmanager def process_data(engine: DataProcess): try: engine.io() yield engine except Exception as e: engine.revert() raise e finally: engine.cleanup() proc = DataProcess() with process_data(proc) as engine: engine.process() engine.some_more_io()"},
{"Title": "Durable Python - Infrastructure failures should not stop the process", "Author": "u/autokitteh", "Content": "Durable Python enables developers to write Python code while an underlying system ensures reliability and resilience. It automatically handles state persistence, fault tolerance, and retry mechanisms, allowing developers to focus on business logic without worrying about infrastructure concerns. Consider the following code, in case the process terminates in the middle of execution, in case the process is killed or due to hardware failure, the process will not complete. import requests import time SLEEP_SECONDS = 3 URL = \"http://localhost:9980/webtools/api/msgs\" def on_http_get(data): for i in range(10): print(\"Loop iteration: %d of 10\" % (i + 1)) # Send a POST request to the application requests.post(URL, data = \"This is my \" + str(i) + \" iteration...\") time.sleep(SLEEP_SECONDS) But actually, I would like the process to survive restarts and continue from the spot it terminated, especially if it's a long running process. For this we need Durable Python. I was wondering which use cases can take advantage of this technology."},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Dive into DevOps ebook Humble Bundle supporting the Python Software Foundation", "Author": "u/AlSweigart", "Content": "https://www.humblebundle.com/books/dive-into-dev-ops-no-starch-books Be sure to click on \"Adjust Donation\" and \"Custom Amount\" and then max out the amount going to the Python Software Foundation. (From $1.75 to $24.50!) For $30 you get the following ebooks from No Starch Press: Automate the Boring Stuff with Python, 2nd Edition DevOps for the Desperate How Linux Works, 3rd Edition The Book of Kubernetes PowerShell for Sysadmins Practical Vulnerability Management Practical SQL, 2nd Edition Practical Linux Forensics Eloquent JavaScript, 3rd Edition Cybersecurity for Small Networks The Linux Command Line, 2nd Edition Web Security for Developers MySQL Crash Course Designing Secure Software Network Programming with Go Practice of Network Security Monitoring Network Flow Analysis Absolute FreeBSD, 3rd Edition Absolute OpenBSD, 2nd Edition Linux Firewalls Pentesting Azure Applications The Book of PF, 3rd Edition"},
{"Title": "I made a desktop chat app :)", "Author": "u/Reasonable-Zone-7909", "Content": "What My Project Does Hi! This is my first time doing a python project more than a few hours in size. I made a chat app which features E2E encryption using a passcode and has a multiclient architecture. All comments are welcome! Target Audience It is just a toy project for my portfolio. Comparison Compared to other chat clients, this one uses a passphrase to encrypt all data, with the passphrase being chosen out of the app, for instance on a dinner. But I think that IRC already has this, so it doesn't differ much XD. Git link: https://github.com/xxzoltanxx/Balvan-Chat"},
{"Title": "Rye-Tui, a Text-based User Interface (TUI) to manage rye projects", "Author": "u/Zaloog1337", "Content": "Hi everyone, Ive reached a state of my current project, where I want to share it with you, and gather some feedback. This is my first time using rye and I am surprised, how Hassle-Free building a package with it went. Source Code: github Installation python -m pip install rye-tui for CLI Tools I recommend using pipx or rye. pipx install rye-tui rye install rye-tui After Installation you can open the TUI using trye in your Terminal. On first use a config file is generated. After that use trye again in your rye managed project What My Project Does A Text-based User Interface (TUI) for rye written in python using Textual Current State Currently rye-tui supports the following functionalities of rye: creating new projects (flag-support coming soon) adding normal and dev dependencies (flag-support coming soon) pinning versions Syncing (flag-support coming soon) changing rye's configuration (sources and default coming soon) Target Audience Python developers and rye users who like a UI to manage their rye projects. Comparison To my knowledge there is no similar tool for rye. Maybe the Anaconda UI comes closest for Anaconda Users. Last Words Feel free to try(e) it out. Happy to hear your feedback."},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "I built a pipeline sending my wife and I SMSs twice a week with budgeting advice generated by AI", "Author": "u/MoodAppropriate4108", "Content": "What My Project Does : I built a pipeline of¬†Dagger¬†modules to send my wife and I SMSs twice a week with actionable financial advice generated by AI based on data from bank accounts regarding our daily spending. Details: Dagger is an open source programmable CI/CD engine. I built each step in the pipeline as a Dagger method. Dagger spins up ephemeral containers, running everything within its own container. I use GitHub Actions to trigger dagger methods that; retrieve data from a source filter for new transactions Categorizes transactions using a zero shot model, facebook/bart-large-mnli through the HuggingFace API. This process is optimized by sending data in dynamically sized batches asynchronously. Writes the data to a MongoDB database Retrieves the data, using Atlas search to aggregate the data by week and categories Sends the data to openAI to generate financial advice. In this module, I implement a memory using LangChain. I store this memory in MongoDB to persist the memory between build runs. I designed the database to rewrite the data whenever I receive new data. The memory keeps track of feedback given, enabling the advice to improve based on feedback This response is sent via SMS through the TextBelt API Full Blog: https://emmanuelsibanda.hashnode.dev/a-dagger-pipeline-sending-weekly-smss-with-financial-advice-generated-by-ai Video Demo: https://youtu.be/S45n89gzH4Y GitHub Repo: https://github.com/EmmS21/daggerverse Target Audience: Personal project (family and friends) Comparison : We have too many budgeting apps and wanted to receive this advice via SMS, personalizing it based on our changing financial goals A screenshot of the message sent: https://ibb.co/Qk1wXQK"},
{"Title": "JSX Syntax inside Python files. (Packed)", "Author": "u/RevolutionaryPen4661", "Content": "There was a JSX-style syntax preprocessor for Python called \" Packed ,\" which allowed us to write JSX inside Python (*.pyx and *.py) files. It's unclear why they chose *.pyx for the file extension, as it conflicts with the naming of Cythonic file extensions (I have checked their issues). This project might have thrived with sufficient contributions and could have changed the way apps are built. However, the project is now archived on GitHub. The last commit was 5 years ago (LICENSE), and the last development commit was 9 years ago. This repository needs someone to revive it, but I don't have enough experience to take on that task. Even though I don't have enough information, we should start with Rust + Python to build a compiler (aka. template replacer) (this doesn't compile Python but replaces all JSX with a dictionary) and cleaner syntax. Integration with Django (Packed has an example too), Flask, FastAPI, Robyn etc. We may also need plugins for the language server, I recommend supporting with *.pyh or *.psx (a fork renamed name) the extension file name (Derived from Python + HTML). VSCODE and NVIM Extensions are required to build support for this. The existing modern syntax of native Python will continue to support this syntax. I made a Handlebars Extension for the community back in the day of 2022 but I don't want to continue the development for it because later I disliked the syntax of handlebars (opinion, you're point of view may contrast with my thoughts). We can use emmet for writing easy HTML. @packed def tag(self): share = get_share_link() return <a href={share}>Share on internet</a> The main point of view is that somehow make returnable components as üëÜ instead of doing this üëá def app(): return div(div(p(\"Hello World\")),span(\"Not a Good Approach for someone (opinion)\"))"},
{"Title": "We built open-source SDK for adding custom code interpreters to AI apps", "Author": "u/mlejva", "Content": "What My Project Does Hey everyone! I'm the CEO of the company that built an SDK that makes it easy to build custom code interpreters for AI apps . We're a company called E2B [0]. We're building and open-source [1] secure environments for running untrusted AI-generated code and AI agents. We call these environments sandboxes and they are built on top of micro VM called Firecracker [2]. We specifically decided to use Firecrackers instead of containers because of their security and ability to do snapshots. You can think of us as giving small cloud computers to LLMs. We recently created a dedicated SDK for building custom code interpreters in Python or JS/TS. We saw this need after a lot of our users have been adding code execution capabilities to their AI apps with our core SDK [3]. These use cases were often centered around AI data analysis so code interpreter-like behavior made sense The way our code interpret SDK works is by spawning an E2B sandbox with Jupyter Server. We then communicate with this Jupyter server through Jupyter Kernel messaging protocol [4]. Here's our cookbook showing how to add code interpreter to different models [5]. We don't do any wrapping around LLM, any prompting, or any agent-like framework. We leave all of that to our users. We're really just a boring code execution layer that sits at the bottom. We're building for the future software that will be building another software. Our long-term plan is to build an automated AWS for AI apps and agents where AI can build and deploy its own software while giving developers powerful observability into what's happening inside our sandboxes. With everything being open-source. Happy to answer any questions and hear feedback! Target Audience You can use it in production. We have companies using us in production already. Comparison Alternatives we usually see are serverless functions or Docker containers. Both have security issues. With serverless functions you can leak data between users and with containers you don't really have true isolation. Containers were made for packaging and portability, not security. Links https://github.com/e2b-dev/code-interpreter [0] https://e2b.dev/ [1] https://github.com/e2b-dev [2] https://github.com/firecracker-microvm/firecracker [3] https://e2b.dev/docs [4] https://jupyter-client.readthedocs.io/en/latest/messaging.html [5] https://github.com/e2b-dev/e2b-cookbook"},
{"Title": "I made a small Python script that uses NASA'S APOD API to set cool backgrounds on a Windows machine", "Author": "u/Potato_eating_a_dog", "Content": "https://github.com/william7491681/APOD_Wallpaper_Script What my project does NASA has a ton of accessible API's, one of which being the APOD (Astronomy Picture Of the Day) API. I made a script to get the last 9 pictures of the day and set them as my Windows 10 background, and then used task scheduler to have the script re-run every day at noon and whenever the computer boots up. It's fairly hard coded for my setup (specific file paths, 1920x1080 monitor, etc), but it shouldn't be too hard to change if one wanted to. Target audience Anyone who likes space backgrounds Comparison Idk, automod made me put this section"},
{"Title": "DeepFusion: a highly modular Deep Learning Framework.", "Author": "u/atharvaaalok1", "Content": "What My Project Does: Hello all, I am a student at Stanford University, I was on a gap year due to medical conditions and to utilitze my time I was studying deep learning. And Voila... I've developed a deep learning library, DeepFusion ! Details: It's customizable and has an easily accessible and highly intuitive codebase. One can just dive right in and effortlessly understand the source code. You can download it from: github at https://github.com/atharvaaalok/deepfusion or install using pip install deepfusion (easy!) For a series of examples explaining the usage and features refer demo or tutorials . Target Audience: Machine learning and python enthusiasts. Comparison: DeepFusion allows explicit access to all activations in a neural network, therefore, making applications such as neural style transfer much easier to perform. It also provides an easy user interface for forward and backward pass profiling, multiple loss functions, automated training, gpu training etc."},
{"Title": "TPC-H Cloud Benchmarks: Spark, Dask, DuckDB, Polars", "Author": "u/mrocklin", "Content": "I hit publish on a blogpost last week on running Spark, Dask, DuckDB, and Polars on the TPC-H benchmark across a variety of scales (10 GiB, 100 GiB, 1 TiB, 10 TiB), both locally on a Macbook Pro and on the cloud.¬† It‚Äôs a broad set of configurations.¬† The results are interesting. No project wins uniformly.¬† They all perform differently at different scales: DuckDB and Polars are crazy fast on local machines Dask and DuckDB seem to win on cloud and at scale Dask ends up being most robust, especially at scale DuckDB does shockingly well on large datasets on a single large machine Spark performs oddly poorly, despite being the standard choice üò¢ Tons of charts in this post to try to make sense of the data.¬† If folks are curious, here‚Äôs the post: https://docs.coiled.io/blog/tpch.html And here's the code. Performance isn‚Äôt everything of course.¬† Each project has its die-hard fans/critics for loads of different reasons. I'd be curious to hear if people want to defend/critique their project of choice."},
{"Title": "MyTimer v1.3: A Geeky Timer for Terminal Enthusiasts", "Author": "u/sepandhaghighi", "Content": "GitHub Repo: https://github.com/sepandhaghighi/mytimer What My Project Does: MyTimer is a Python CLI project that provides a simple, efficient timer for terminal users, particularly targeting the geek community. It allows users to set timers directly from their command line interface, offering a distraction-free experience. mytimer --hour=12 --minute=34 --second=56 --alarm --alarm-repeat=5 ___    ______          ______   _     _         _______  _______ (___)  (_____ \\        (_____ \\ | |   (_)       (_______)(_______) _     ____) )   _    _____) )| |_____    _    ______   ______ | |   / ____/   (_)  (_____ ( |_____  |  (_)  (_____ \\ |  ___ \\ _| |_ | (_____    _    _____) )      | |   _    _____) )| |___) ) (_____)|_______)  (_)  (______/       |_|  (_)  (______/ |______/ Target Audience: Developers who spend a significant amount of time working in the terminal :) Comparison: MyTimer supports more features compared to countdown MyTimer offers a greater variety of faces and functions than timer-cli"},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Thank You PyConUS 2024 !!!", "Author": "u/ryanstephendavis", "Content": "First timer this year, currently at the airport leaving Pittsburgh after 6 days of PyCon... I've never seen such an intelligent, inclusive, humble, diverse, and inspiring group of human beings.  The Python community serves as a beautiful model of what tech culture should strive towards. I could go on and on about how much fun I had, but in short, thanks to all the volunteers, staff, and FOSS developers that have cultivated such an amazing culture."},
{"Title": "Speed improvements in Polars over Pandas", "Author": "u/zzoetrop_1999", "Content": "I'm giving a talk on polars in July. It's been pretty fast for us, but I'm curious to hear some examples of improvements other people have seen. I got one process down from over three minutes to around 10 seconds. Also curious whether people have switched over to using polars instead of pandas or they reserve it for specific use cases."},
{"Title": "Interfacing Custom USB endpoints using Python!", "Author": "u/shekhuu", "Content": "Hi everyone, I was building something that required me to communicate over USB to Raspberry Pi Pico using Pyusb Python. So I decided to make a blog post about it showing the concepts, process, and source code. Check out the blog post here! Check out the source code here!"},
{"Title": "Mystique: Sparse data matching for Python tests", "Author": "u/jonocodes", "Content": "What My Project Does I made this library to help assert test responses inline while directing the comparison to be as rigid or lax as it needs to be. Motivation I write a lot of tests that assert values in complex nested dictionaries. But really I only need to check some parts in the response, not all of it. I often find myself transforming the response or maliciously extracting the important parts I need - in order to satisfy the assertions. This gets messy and can make tests hard to follow. Target Audience Anyone who writes tests. This is particularly useful if you generate fake data in your tests with something like Faker, Factory Boy, or Model Bakery. Comparison I have not found a like-project. Searched high and low in PyPI. If such a library existed, I would not have written one myself. Feedback appreciated. See PyPI project for basic use and github tests for more complex examples ."},
{"Title": "I made python audioplayer with FFmpeg and Qt6", "Author": "u/WyseTwist", "Content": "Midnight Player - a simple python audiplayer for playing audio What My Project Does: My project is just an audio player for playing music, it can play audio from folders, supports different audio formats like Flac, Mp3 and can show some information about the track. Player uses subprocess to access ffmpeg, then it decodes the audio file into pcm format, then plays this file using sounddevice library. the use of these libraries is to ensure that the audio file does not lose quality during processing. Target Audience: This project was made to increase experience in python programming and to understand how the audio playback process works, but the project is also useful for people who are interested in learning the structure of the audio player as it is open source. Comparison: First of all you should understand that this player is not trying to compete with large-scale projects like AIMP because I developed this project alone and the project was written in a short period of time. But if you compare with other python audio players on github you will notice that many people use wrong libraries like qmediaplayer or pygame mixer to create their audio player, which are not designed for wide support of audio formats, my project is much more complex to operate audio file. Packages and source code can be found here: https://github.com/Niamorro/Midnight-Player"},
{"Title": "How to publish a Python package with GitHub Actions using Poetry", "Author": "u/johnfraney", "Content": "Hi! I've been enjoying using PyPI's trusted publishing for the Python packages I maintain and I threw together a little post showing how I'm using that along with Poetry to publish a package from GitHub https://johnfraney.ca/blog/how-to-publish-a-python-package-with-poetry-and-github-actions/ If you've got any tips for publishing a Python package, I'd be happy to hear those, too"},
{"Title": "Reforger Whitelist Py", "Author": "u/PyjamaZombie", "Content": "My project below, to put it simply, periodically checks the console.log for when a player join event occurs, when it does, it extracts the player's identifiers ( player_name and identity_id ). This is then checked against either, a JSON or, a database. I have incorporated standard logging, command-line arguments and threading to handle each player process individually. The target audience for this is the Arma Reforger community, for which, the application is made for. Currently, to my knowledge, there is no application like this available to the Arma Reforger community. I am very open to feedback, contributions and advice as want to expand this as much as possible! https://github.com/BreathXV/ReforgerWhitelistPy"},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "I made a Traversible Tree in Python", "Author": "u/frankstan33", "Content": "Comparison It is inspired from the existing tree command on linux and windows too So basically it is just like the tree command, it shows you a tree of the current directory structure. What My Project Does It basically gives you a birds eye view of your dir structure and quickly navigate to the folder you want to without having to know its path or doing cd ../../.. many times. There are a bunch of command line args such as setting the paths, flags to show dot directories, set head height (no. of parent dirs shown) and tail height (depth). You can traverse around the tree using various key presses (inspired from vim keybindings) and based on the given argument (-o, -c or --copy) you can output the value (the node to which you traversed), cd into it and have it copied into your clipboard.J I had created this for my assignment and had a lot of fun with it. Tried to implement as much clean code and good design as I could but its still a mess and active work in progress tbh (added unit tests lol). And the rendering is still a little slow. Do check it out: pranavpa8788/trav: A Traversible Tree command line program (github.com) and let me know what you guys think. It is built with support for Windows and Linux, some installation stuff might be needed though, and I'll update those steps soon in the github page Target Audience For anyone really, especially if you use a lot of terminal (Had to add the titles because my post was getting auto-deleted lol) Link to video demo: https://streamable.com/ds911k"},
{"Title": "Programmable Semantics (Eval, Semicolon, Assignment) for Python", "Author": "u/aoeu512", "Content": "I've seen programmable semantics (eval-hacking, macros) in LISPs and in Haskell-likes(Monads/Template Haskell), the overall techinque in OOP languages is called \"Aspect Oriented Programming\".  Has this kind of thing been discussed before, and is it Pythonic it could allow a lot of Python code to be shorter.  Python has sys.set_trace that sort of allows some form of programmable semantics but its mostly for debugging. Programmable assignment(variables) are like setters/getters/properties, but instead of being run on o.x = 5, you could run them on \"all local assignments\" isnside a context manager or in a decorated function.  On every assignment you could do stuff like log the values, update dependencies, notify objects, do a database transaction, do persistance, calculate other values, without having to explicitly do so for every assignment statement. Programmable semicolons (such as Haskell Monads, or reprogramming Lisp do/progn/let) could allow you to have the same code run either synchronous, async, get undo/history support, break on error, rollback, logging in between lines, changing local/global variables in between each line, database access in between lines, checking values for errors, ignoring certain statements, etc...  You can think of a semicolon like an \"unrolled for loop\"/iterator ran for each code line.  It would be like async but you can change a piece of code to be sync or async at run time by changing the context manager you are in.  Programmable \"call\" can change the default call operation in a context manager for all functions and be similar to semicolons. Programmable eval would allow you to change the order of operations, choose to ignore certain functions, allow you replace certain expensive expressions with others, allow you to keep a trace of all evaluations taking place, you can turn an expression/program into an interator allowing you to pretty cool stuff."},
{"Title": "try... except... finally!", "Author": "u/young-and-ignorant", "Content": "Haven't seen this syntax used very often and was wondering why. During error handling, if you have something to run independent of the success, you can use finally. from your_library import DataProcess engine = DataProcess() try: engine.io() engine.process() engine.some_more_io() except Exception as e: engine.revert() raise e finally: engine.cleanup() VS from your_library import DataProcess engine = DataProcess() try: engine.io() engine.process() engine.some_more_io() except Exception as e: engine.revert() engine.cleanup() raise e engine.cleanup() VS from your_library import DataProcess from contextlib import contextmanager @contextmanager def process_data(engine: DataProcess): try: engine.io() yield engine except Exception as e: engine.revert() raise e finally: engine.cleanup() proc = DataProcess() with process_data(proc) as engine: engine.process() engine.some_more_io()"},
{"Title": "Durable Python - Infrastructure failures should not stop the process", "Author": "u/autokitteh", "Content": "Durable Python enables developers to write Python code while an underlying system ensures reliability and resilience. It automatically handles state persistence, fault tolerance, and retry mechanisms, allowing developers to focus on business logic without worrying about infrastructure concerns. Consider the following code, in case the process terminates in the middle of execution, in case the process is killed or due to hardware failure, the process will not complete. import requests import time SLEEP_SECONDS = 3 URL = \"http://localhost:9980/webtools/api/msgs\" def on_http_get(data): for i in range(10): print(\"Loop iteration: %d of 10\" % (i + 1)) # Send a POST request to the application requests.post(URL, data = \"This is my \" + str(i) + \" iteration...\") time.sleep(SLEEP_SECONDS) But actually, I would like the process to survive restarts and continue from the spot it terminated, especially if it's a long running process. For this we need Durable Python. I was wondering which use cases can take advantage of this technology."},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Dive into DevOps ebook Humble Bundle supporting the Python Software Foundation", "Author": "u/AlSweigart", "Content": "https://www.humblebundle.com/books/dive-into-dev-ops-no-starch-books Be sure to click on \"Adjust Donation\" and \"Custom Amount\" and then max out the amount going to the Python Software Foundation. (From $1.75 to $24.50!) For $30 you get the following ebooks from No Starch Press: Automate the Boring Stuff with Python, 2nd Edition DevOps for the Desperate How Linux Works, 3rd Edition The Book of Kubernetes PowerShell for Sysadmins Practical Vulnerability Management Practical SQL, 2nd Edition Practical Linux Forensics Eloquent JavaScript, 3rd Edition Cybersecurity for Small Networks The Linux Command Line, 2nd Edition Web Security for Developers MySQL Crash Course Designing Secure Software Network Programming with Go Practice of Network Security Monitoring Network Flow Analysis Absolute FreeBSD, 3rd Edition Absolute OpenBSD, 2nd Edition Linux Firewalls Pentesting Azure Applications The Book of PF, 3rd Edition"},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "A Beginner's Guide to Unit Testing with Pytest", "Author": "u/Stanulilic", "Content": "Hey r/python ! I wrote a guide on how to use Pytest, covering a bunch of important features like designing tests, filtering tests, parameterizing tests, fixtures, and more. Check it out on this link ."},
{"Title": "Reactive programming for Python with live-cells-py", "Author": "u/alex-gutev", "Content": "live-cells-py (Live Cells Python) is a reactive programming library which I ported from Live Cells for Dart. What my project Does: You can declare cells which are observable containers for data: import live_cells as lc a = lc.mutable(0) Cells can be defined as a function of other cells: a = lc.mutable(0) b = lc.mutable(1) c = lc.computed(lambda: a() + b()) c is defined as the sum of the values of cells a and b . The value of c is automatically recomputed when the value of either a or b changes. The definition of c can be simplified to the following: c = a + b Which reads like an ordinary variable definition You can define a watch function which runs whenever the value of a cell changes: lc.watch(lambda: print(f'The sum is {c()}')) This watch function, which prints the value of c to standard output, is run automatically whenever the value of c changes. More complex computed cells and watch functions can be defined using decorators: n = lc.mutable(5) @lc.computed def n_factorial(): result = 1 m = n() while m > 0: result *= m m -= 1 return m @lc.watch def watch_factorial(): print(f'{n()}! = {n_factorial()}') I've found this paradigm to be very useful for handling events and keeping the state of an application, be it a GUI desktop application, systems software or a server, in sync between its various components, which is why I ported this library to Python so I can use the same paradigm, with a similar API, on the backend as well. Target Audience This project is intended for those who are looking for a declarative solution to handling and reacting to events in Python applications that is simple and intuitive to use and doesn't require excessive boilerplate. Particularly if you're used to working with signals in JavaScript, you will quickly pick up this library. Comparison The de-facto standard for reactive programming is the ReactiveX (RX) series of libraries available for various programming languages. The main difference between RxPy and Live Cells is in the design of the API, with the main difference being that cells are self-subscribing. Referring to the examples shown in the previous sections, you do not have to explicitly \"connect\", \"subscribe\" to cells nor do you need a \"map\" or \"zip\" construct to build more complicated reactive pipelines. Instead you simply reference whatever you need and the subscription to the dependencies is handled automatically by the library. The source code and package is available at: https://github.com/alex-gutev/live_cells_py https://pypi.org/project/live-cells-py/ The documentation is available at: https://alex-gutev.github.io/live_cells_py/basics/cells.html"},
{"Title": "The possibility to build Android apps with python professionally is here and needs your support.", "Author": "u/klargstein", "Content": "You guys really need to check this. I believe new comers to python would love to tinker with the android ecosystem from the safety of python :-) Imgur: https://imgur.com/gallery/DtfwOVi https://www.kickstarter.com/projects/kivyschool/the-pain-free-python-on-android-essentials-course Edit: added imgur link."},
{"Title": "You should only use licensed version of python", "Author": "u/vikashgraja", "Content": "I‚Äôm an intern in a company and I automated some processes using python. My company‚Äôs IT wing said that as long as it is a licensed software you can use it in our company. In my mind I was like where the f I‚Äôm going to get a license for an open source software. Note : They mention that another team has been using licensed python. I thought either IT is so stupid or that team is so smart that they brought license for pycharm or anaconda (claim that it is a Python license) and fooled IT. If I am wrong then tell me where I can get that license. And I am also looking for job in data analyst."},
{"Title": "Dash Pip Components", "Author": "u/Soolsily", "Content": "What My Project Does Hey everyone, just released 8 new pip components for plotly and dash including: Full Calendar Component - A Full Calendar Component for Dash Dash Summernote - A rich text WYSIWYG Editor for Dash Dash Emoji Mart - A Slack-like Emoji Picker for Dash Dash Charty - A Charting Library for Dash Dash Image Gallery - A Image Gallery Component for Dash Dash Swiper - A Swiper Component for Dash Dash Insta Stories - An Instagram Stories Component for Dash Dash Credit Cards - A Credit Card Component for Dash Documentation can be found here: https://pip-docs.onrender.com/ The repo for the github can be found here: https://github.com/pip-install-python/pip-docs Target Audience Plotly dash and Python developers. Comparison All these are new components for the dash framework, but based on javascript or react projects which were forked and edited to work specifically for dash."},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "HS-transform, python package for hyperbolic S-transform in signal processing", "Author": "u/Ok_Employ_2414", "Content": "I made a python package for S-transform with Hyperbolic window (Hyperbolic S-transform or HSTransform package). This is my first time publishing a python package, so the project is still far from stable and still under beta release. What my project does: This transformation is applied to signal processing, analyzing transient changes of a signal during very short-time. Some special use case can be in power system signal, or Geophysical signal analysis, or MRI ... Target audience: anyone who is interested in signal processing or power system analysis or geographical analysis. Comparison: The comparison with Wavelet Transform has been shown. (which shows more potential in detecting transient changes) I would highly appreciate some feedback, before progressing further. HSTransform¬†is available on pypi . Link to source code in github Quick Usage import numpy as np from hstransform import HSTransform # Create input signal (for example: Voltage signal) t = np.linspace(0, 10, 100) # timeseries V_m = 220*np.sqrt(2)  # peak voltage f_V = 50  # frequency phi_V = 0  # phase V_clean = V_m * np.sin(2 * np.pi * f_V * t + phi_V) # Create voltage sag/dip (80% of the nominal voltage for 0.15 second) V_sag = np.where((t >= 2) & (t <= 3.5), 0.5 * V_clean, V_clean) # Create an instance of HSTransform hs = HSTransform() # Perform the transform signal = V_sag S_transformed = hs.fit_transform(t, signal)"},
{"Title": "I made a cheatsheet for pydash", "Author": "u/poopatroopa3", "Content": "https://brunodantas.github.io/pydash-cheatsheet/en/ What my project does: pydash is a library with great potential to make you code more Functional and simple. I made this cheatsheet a while ago to highlight some of the most useful functions of the library, since there are so many. I hope it's useful. Target audience: anyone who is interested in pydash, functional programming, not reinventing the wheel. Comparison: on Google you can find cheatsheets for Lodash, which is the original Javascript library which pydash is inspired by, but no cheatsheets for pydash itself. Note that many pydash functions are already implemented in modern Python, so I did not include those in the cheatsheet. I made this programatically using Material for Mkdocs , which I also recommend. https://github.com/brunodantas/pydash-cheatsheet"},
{"Title": "IconMatch - find icons and letters positions from images!", "Author": "u/TraditionalDistrict9", "Content": "Hey all, I am not the original creator, but found that 4yo project, and decided to revive it! What my project does: IconMatch is library allowing you to extract icons and letter positions from image or from display! There is also realtime demo on repo showcasing how it works! Target Audience : For all detecting objects from display! Comparison : I did not find other project like that - but it was my first find too! It is also not OCR! https://github.com/NativeSensors/IconMatch Have fun!"},
{"Title": "Tutorial: Simple Pretty Maps That Will Improve Your Python Streamlit Skills", "Author": "u/jgloewen", "Content": "Interactive web applications for data visualization improve user engagement and understanding. These days, Streamlit is a very popular framework used to provide web applications for data science. It is a terrific programming tool to have in you Python knowledge toolbox. Here‚Äôs a fun and practical tutorial on how to create a simple interactive and dynamic Streamlit application. This application generates a beautiful and original map using the prettymaps library. Free article: HERE"},
{"Title": "Interactive 8-Puzzle Game", "Author": "u/MohamedElfatih", "Content": "What does my project do Hi everyone I‚Äôm excited to share a project I‚Äôve been working on: an interactive 8-puzzle game built using Python and Pygame. This project also includes several solvers based on classic search algorithms. Technical details: Python: the primary language in the project. Pygame: for rendering and handling user interaction. Search algorithms: implement depth first search (dfs) and A star search for solving the puzzle. By default A star search is used because it finds the solution faster than dfs Target Audience This is a toy project I did for fun. You can find the project in GitHub: link I would love to get your feedback, contributions, and if you find it interesting or helpful, please give it a star on GitHub. Your support and feedback will help me improve and add more features!Thank you for checking out my project!"},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Mastering Python: 7 Strategies for Writing Clear, Organized, and Efficient Code", "Author": "u/Slow_Scene_7972", "Content": "Optimize Your Python Workflow: Proven Techniques for Crafting Production-Ready Code Link"},
{"Title": "Enhanced EPUB Version of \"An Understated Dominance\" Available for Fans!", "Author": "u/abdirxhmxn", "Content": "Hey everyone! I recently completed a fun project that I think fans of \"An Understated Dominance\" will appreciate, especially those who have struggled with reading the novel online due to website clutter and distractions. I've created a clean, distraction-free EPUB version of the novel that you can download and read on your preferred device. You can get it here: EPUB Download . Why I did this: The original website hosting the novel has a lot of distractions that can take away from the reading experience. My goal was to provide a seamless reading experience that allows you to fully immerse yourself in the story without unnecessary interruptions. Project Details: For those interested in the technical side, here's a brief overview of how I created the EPUB: Web Scraping: I used Python with libraries like requests and BeautifulSoup to scrape all 2240 chapters of the novel. Concurrency: Leveraged concurrent.futures to speed up the scraping process. Retries: Implemented retry logic with tenacity to handle any request failures. EPUB Creation: Used ebooklib to compile the scraped content into an EPUB file, complete with chapter headers and a cover image. You can check out the full code for this project on my GitHub: GitHub Repository . Feedback & Collaboration: I‚Äôd love to hear your thoughts on the EPUB. Feel free to download, read, and let me know if you encounter any issues or have suggestions for improvements. If you're interested in collaborating on similar projects or have any ideas, I'm all ears! Enjoy the novel, and happy reading!"},
{"Title": "Picodi - Simplifying Dependency Injection in Python", "Author": "u/yakimka", "Content": "What My Project Does Picodi is a lightweight and easy-to-use Dependency Injection ( DI ) library for Python. Picodi supports both synchronous and asynchronous contexts and offers features like resource lifecycle management. Think about Picodi as a decorator that helps you manage your dependencies without the need for a full-blown DI container. Key Features üåü Simple and lightweight üì¶ Zero dependencies ‚è±Ô∏è Supports both sync and async contexts üîÑ Resource lifecycle management üîç Type hints support üêç Python & PyPy 3.10+ support Quick Start Here‚Äôs a quick example of how Picodi works: import asyncio from collections.abc import Callable from datetime import date from typing import Any import httpx from picodi import Provide, init_resources, inject, resource, shutdown_resources from picodi.helpers import get_value def get_settings() -> dict: return { \"nasa_api\": { \"api_key\": \"DEMO_KEY\", \"base_url\": \"https://api.nasa.gov\", \"timeout\": 10, } } @inject def get_setting(path: str, settings: dict = Provide(get_settings)) -> Callable[[], Any]: value = get_value(path, settings) return lambda: value @resource @inject async def get_nasa_client( api_key: str = Provide(get_setting(\"nasa_api.api_key\")), base_url: str = Provide(get_setting(\"nasa_api.base_url\")), timeout: int = Provide(get_setting(\"nasa_api.timeout\")), ) -> httpx.AsyncClient: async with httpx.AsyncClient( base_url=base_url, params={\"api_key\": api_key}, timeout=timeout ) as client: yield client @inject async def get_apod( date: date, client: httpx.AsyncClient = Provide(get_nasa_client) ) -> dict[str, Any]: response = await client.get(\"/planetary/apod\", params={\"date\": date.isoformat()}) response.raise_for_status() return response.json() async def main(): await init_resources() apod_data = await get_apod(date(2011, 7, 19)) print(\"Title:\", apod_data[\"title\"]) await shutdown_resources() if __name__ == \"__main__\": asyncio.run(main()) This example demonstrates how Picodi handles dependency injection for both synchronous and asynchronous functions, manages resource lifecycles, and provides a clean and efficient way to structure your code. For more examples and detailed documentation, check out the GitHub repository Target Audience Picodi is perfect for developers who want to simplify dependency management in their Python applications, but don't want to deal with the complexity of larger DI frameworks. Picodi can help you write cleaner and more maintainable code. Comparison Unlike other DI libraries, Picodi does not have wiring, a large set of different types of providers, or the concept of a container. Picodi prioritizes simplicity, so it includes only the most essential features: dependency injection, resource lifecycle management, and dependency overriding. Get Involved Picodi is still in the experimental stage, and I'm looking for feedback from the community. If you have any suggestions, encounter any issues, or want to contribute, please check out the GitHub repository and let me know."},
{"Title": "Homoiconic Python Code", "Author": "u/moonbunR", "Content": "Homoiconic, what does it mean? In simple terms, homoiconic code is when code is treated as data and can be manipulated as you would data. This means the code can be changed, new functions and variables added, the code can generate new code or even examine and modify its own structure and behavior all while it is running. That‚Äôs why homoiconic languages like Lisp are so powerful. But what if we can make a homoiconic python code, where the code and the data are one and the same and can be modified in the same way? This guide does a good job in trying to explain how you would create a python version of the ‚ÄúLisp in Lisp‚Äù code which would give you access to all those homoiconic features that Lisp brags of like the macro systems, the expressiveness and flexibility, the metaprogramming etc. while still using python. What do you guys think of this?"},
{"Title": "The best Python CLI library, arguably.", "Author": "u/AND_MY_HAX", "Content": "What My Project Does https://github.com/treykeown/arguably arguably makes it super simple to define complex CLIs. It uses your function signatures and docstrings to set everything up. Here's how it works: Adding the @arguably.command decorator to a function makes it appear on the CLI. If multiple functions are decorated, they'll all be set up as subcommands. You can even set up multiple levels of subcommands. The function name, signature, and docstring are used to automatically set up the CLI Call arguably.run() to parse the arguments and invoke the appropriate command A small example: #!/usr/bin/env python3 import arguably @arguably.command def some_function(required, not_required=2, *others: int, option: float = 3.14): \"\"\" this function is on the command line! Args: required: a required argument not_required: this one isn't required, since it has a default value *others: all the other positional arguments go here option: [-x] keyword-only args are options, short name is in brackets \"\"\" print(f\"{required=}, {not_required=}, {others=}, {option=}\") if __name__ == \"__main__\": arguably.run() becomes user@machine:~$ ./readme-1.py -h usage: readme-1.py [-h] [-x OPTION] required [not-required] [others ...] this function is on the command line! positional arguments: required             a required parameter (type: str) not-required         this one isn't required, since it has a default (type: int, default: 2) others               all the other positional arguments go here (type: int) options: -h, --help           show this help message and exit -x, --option OPTION  an option, short name is in brackets (type: float, default: 3.14) It can easily hand some very complex cases, like passing in QEMU-style arguments to automatically instantiated different types of classes: user@machine:~$ ./readme-2.py --nic tap,model=e1000 --nic user,hostfwd=tcp::10022-:22 nic=[TapNic(model='e1000'), UserNic(hostfwd='tcp::10022-:22')] You can also auto-generate a CLI for your script through python3 -m arguably your_script.py , more on that here . Target Audience If you're writing a script or tool, and you need a quick and effective way to run it from the command line, arguably was made for you. It's great for things where a CLI is essential, but doesn't need tons of customization. arguably makes some opinionated decisions that keep things simple for you, but doesn't expose ways of handling things like error messages. I put in the work to create GitHub workflows, documentation, and proper tests for arguably . I want this to be useful for the community at large, and a tool that you can rely on. Let me know if you're having trouble with your use case! Comparison There are plenty of other tools for making CLIs out there. My goal was to build one that's unobtrusive and easy to integrate. I wrote a whole page on the project goals here: https://treykeown.github.io/arguably/why/ A quick comparison: argparse - this is what arguably uses under the hood. The end user experience should be similar - arguably just aims to make it easy to set up. click - a powerhouse with all the tools you'd ever want. Use this if you need extensive customization and don't mind some verbosity. typer - also a great option, and some aspects are similar design-wise. It also uses functions with a decorator to set up commands, and also uses the function signature. A bit more verbose, though like click , has more customization options. fire - super easy to generate CLIs. arguably tries to improve on this by utilizing type hints for argument conversion, and being a little more of a middle ground between this and the more traditional ways of writing CLIs in Python. This project has been a labor of love to make CLI generation as easy as it should be. Thanks for checking it out!"},
{"Title": "LinkedIn-Learning-Downloader v1.1", "Author": "u/YounesWinter", "Content": "With Python i created a tool that enables users to download LinkedIn Learning courses, including the often overlooked but incredibly valuable exercise files. This feature sets our project apart, offering a complete learning experience by providing both the course videos and the materials needed for practical application. What great about it and beyond other tools in the same genre concerned LinkedIn Learning Downloaders, now you can download the whole courses from a path link. this is was never possible without Python. For more detailed information, visit the repo : https://github.com/M0r0cc4nGh0st/LinkedIn-Learning-Downloader"},
{"Title": "FastAPI Backend Template for SaaS products", "Author": "u/shekhuu", "Content": "Hello there, I just created a template for creating a backend for your SaaS products. What my project does: It is a FastAPI project/template for creating SaaS backends and admin dashboards. Comparison: Out of the box, it supports Licence key generation and validation. OAuth 2 authentication with scopes. Endpoints with pagination and filters to easily integrate with an admin dashboard. Passwords are securely stored using hashing. used PostgreSQL for database Target Audience: Production Check it here! Update 1 : Added pre-commit hooks, tox for testing and linting."},
{"Title": "this.s and this.d", "Author": "u/VladTbk", "Content": "Recently, I found out about the this \"Easter egg\" in python3. Adding import this into a py file will print \"The Zen of Python\" by Tim Peters. Also, this has two attributes: this.s and this.d , which I guess form the actual Easter egg. this.s returns an encrypted version of \"The Zen\" and this.d well, see for yourself, maybe you'll solve the puzzle."},
{"Title": "I made a Mandelbrot Zoom using Python", "Author": "u/AbideByReason", "Content": "I made a YouTube video which previews the zoom and explains the code, which you can find here: https://youtu.be/HtNUFdh2sjg What my project does: it creates a Mandelbrot Zoom. Comparison: it uses Pillow and consists of just 2 main blocks of code: one is the main function that finds which points are in the Mandelbrot Set and the other is the main loop that applies appropriate colors to each image. It gives the option of being black and white OR in color. It works fairly well but can definitely be faster if parallelized. I'd love to hear any suggestions on how it can be improved. Target Audience: fun/toy project Source code is here: https://github.com/AbideByReason/Python_Notebooks/tree/main"},
{"Title": "sjvisualizer: a python package to animate time-series data", "Author": "u/PieChartPirate", "Content": "What the project does: data animation library for time-series data. Currently it supports the following chart types: Bar races Animated Pie Charts Animated Line Charts Animated Stacked Area Charts Animated (World) Maps You can find some simple example charts here: https://www.sjdataviz.com/software It is on pypi, you can install it using: pip install sjvisualizer It is fully based on TkInter to draw the graph shapes to the screen, which gives a lot of flexibility. You can also mix and match the different chart types in a single animation. Target audience: people interested in data animation for presentations or social media content creation Alternatives: I only know one alternative which is bar-chart-race, the ways sjvisualizer is better: Smoother animation, bar-chart-race isn't the quite choppy I would say Load custom icons for each data category (flag icons for countries for example) Number of supported chart types Mix and match different chart types in a single animation, have a bar race to show the ranking, and a smaller pie chart showing the percentages of the whole Based on TkInter, easy to add custom elements through the standard python GUI library Topics to improve (contributions welcome): Documentation Improve built in screen recorder, performance takes a hit when using the built in screen recorder Additional chart types: bubble charts, lollipop charts, etc Improve the way data can be loaded into the library (currently only supports reading into a dataframe from Excel) Sorry for the long post, you can find it here on GitHub: https://github.com/SjoerdTilmans/sjvisualizer"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "The Python on Microcontrollers (and Raspberry Pi) Newsletter, a weekly news and project resource", "Author": "u/HP7933", "Content": "The Python on Microcontrollers (and Raspberry Pi) Newsletter: subscribe for free With the Python on Microcontrollers newsletter, you get all the latest information on Python running on hardware in one place! MicroPython, CircuitPython and Python on single Board Computers like Raspberry Pi & many more. The Python on Microcontrollers newsletter is the place for the latest news. It arrives Monday morning with all the week‚Äôs happenings. No advertising, no spam, easy to unsubscribe. 10,998 subscribers - the largest Python on hardware newsletter out there. (2 more for 11k!) Catch all the weekly news on Python for Microcontrollers with adafruitdaily.com . This ad-free, spam-free weekly email is filled with CircuitPython, MicroPython, and Python information that you may have missed, all in one place! Ensure you catch the weekly Python on Hardware roundup‚Äì you can cancel anytime ‚Äì try our spam-free newsletter today! https://www.adafruitdaily.com/"},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "A Beginner's Guide to Unit Testing with Pytest", "Author": "u/Stanulilic", "Content": "Hey r/python ! I wrote a guide on how to use Pytest, covering a bunch of important features like designing tests, filtering tests, parameterizing tests, fixtures, and more. Check it out on this link ."},
{"Title": "Reactive programming for Python with live-cells-py", "Author": "u/alex-gutev", "Content": "live-cells-py (Live Cells Python) is a reactive programming library which I ported from Live Cells for Dart. What my project Does: You can declare cells which are observable containers for data: import live_cells as lc a = lc.mutable(0) Cells can be defined as a function of other cells: a = lc.mutable(0) b = lc.mutable(1) c = lc.computed(lambda: a() + b()) c is defined as the sum of the values of cells a and b . The value of c is automatically recomputed when the value of either a or b changes. The definition of c can be simplified to the following: c = a + b Which reads like an ordinary variable definition You can define a watch function which runs whenever the value of a cell changes: lc.watch(lambda: print(f'The sum is {c()}')) This watch function, which prints the value of c to standard output, is run automatically whenever the value of c changes. More complex computed cells and watch functions can be defined using decorators: n = lc.mutable(5) @lc.computed def n_factorial(): result = 1 m = n() while m > 0: result *= m m -= 1 return m @lc.watch def watch_factorial(): print(f'{n()}! = {n_factorial()}') I've found this paradigm to be very useful for handling events and keeping the state of an application, be it a GUI desktop application, systems software or a server, in sync between its various components, which is why I ported this library to Python so I can use the same paradigm, with a similar API, on the backend as well. Target Audience This project is intended for those who are looking for a declarative solution to handling and reacting to events in Python applications that is simple and intuitive to use and doesn't require excessive boilerplate. Particularly if you're used to working with signals in JavaScript, you will quickly pick up this library. Comparison The de-facto standard for reactive programming is the ReactiveX (RX) series of libraries available for various programming languages. The main difference between RxPy and Live Cells is in the design of the API, with the main difference being that cells are self-subscribing. Referring to the examples shown in the previous sections, you do not have to explicitly \"connect\", \"subscribe\" to cells nor do you need a \"map\" or \"zip\" construct to build more complicated reactive pipelines. Instead you simply reference whatever you need and the subscription to the dependencies is handled automatically by the library. The source code and package is available at: https://github.com/alex-gutev/live_cells_py https://pypi.org/project/live-cells-py/ The documentation is available at: https://alex-gutev.github.io/live_cells_py/basics/cells.html"},
{"Title": "The possibility to build Android apps with python professionally is here and needs your support.", "Author": "u/klargstein", "Content": "You guys really need to check this. I believe new comers to python would love to tinker with the android ecosystem from the safety of python :-) Imgur: https://imgur.com/gallery/DtfwOVi https://www.kickstarter.com/projects/kivyschool/the-pain-free-python-on-android-essentials-course Edit: added imgur link."},
{"Title": "You should only use licensed version of python", "Author": "u/vikashgraja", "Content": "I‚Äôm an intern in a company and I automated some processes using python. My company‚Äôs IT wing said that as long as it is a licensed software you can use it in our company. In my mind I was like where the f I‚Äôm going to get a license for an open source software. Note : They mention that another team has been using licensed python. I thought either IT is so stupid or that team is so smart that they brought license for pycharm or anaconda (claim that it is a Python license) and fooled IT. If I am wrong then tell me where I can get that license. And I am also looking for job in data analyst."},
{"Title": "Dash Pip Components", "Author": "u/Soolsily", "Content": "What My Project Does Hey everyone, just released 8 new pip components for plotly and dash including: Full Calendar Component - A Full Calendar Component for Dash Dash Summernote - A rich text WYSIWYG Editor for Dash Dash Emoji Mart - A Slack-like Emoji Picker for Dash Dash Charty - A Charting Library for Dash Dash Image Gallery - A Image Gallery Component for Dash Dash Swiper - A Swiper Component for Dash Dash Insta Stories - An Instagram Stories Component for Dash Dash Credit Cards - A Credit Card Component for Dash Documentation can be found here: https://pip-docs.onrender.com/ The repo for the github can be found here: https://github.com/pip-install-python/pip-docs Target Audience Plotly dash and Python developers. Comparison All these are new components for the dash framework, but based on javascript or react projects which were forked and edited to work specifically for dash."},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "HS-transform, python package for hyperbolic S-transform in signal processing", "Author": "u/Ok_Employ_2414", "Content": "I made a python package for S-transform with Hyperbolic window (Hyperbolic S-transform or HSTransform package). This is my first time publishing a python package, so the project is still far from stable and still under beta release. What my project does: This transformation is applied to signal processing, analyzing transient changes of a signal during very short-time. Some special use case can be in power system signal, or Geophysical signal analysis, or MRI ... Target audience: anyone who is interested in signal processing or power system analysis or geographical analysis. Comparison: The comparison with Wavelet Transform has been shown. (which shows more potential in detecting transient changes) I would highly appreciate some feedback, before progressing further. HSTransform¬†is available on pypi . Link to source code in github Quick Usage import numpy as np from hstransform import HSTransform # Create input signal (for example: Voltage signal) t = np.linspace(0, 10, 100) # timeseries V_m = 220*np.sqrt(2)  # peak voltage f_V = 50  # frequency phi_V = 0  # phase V_clean = V_m * np.sin(2 * np.pi * f_V * t + phi_V) # Create voltage sag/dip (80% of the nominal voltage for 0.15 second) V_sag = np.where((t >= 2) & (t <= 3.5), 0.5 * V_clean, V_clean) # Create an instance of HSTransform hs = HSTransform() # Perform the transform signal = V_sag S_transformed = hs.fit_transform(t, signal)"},
{"Title": "I made a cheatsheet for pydash", "Author": "u/poopatroopa3", "Content": "https://brunodantas.github.io/pydash-cheatsheet/en/ What my project does: pydash is a library with great potential to make you code more Functional and simple. I made this cheatsheet a while ago to highlight some of the most useful functions of the library, since there are so many. I hope it's useful. Target audience: anyone who is interested in pydash, functional programming, not reinventing the wheel. Comparison: on Google you can find cheatsheets for Lodash, which is the original Javascript library which pydash is inspired by, but no cheatsheets for pydash itself. Note that many pydash functions are already implemented in modern Python, so I did not include those in the cheatsheet. I made this programatically using Material for Mkdocs , which I also recommend. https://github.com/brunodantas/pydash-cheatsheet"},
{"Title": "IconMatch - find icons and letters positions from images!", "Author": "u/TraditionalDistrict9", "Content": "Hey all, I am not the original creator, but found that 4yo project, and decided to revive it! What my project does: IconMatch is library allowing you to extract icons and letter positions from image or from display! There is also realtime demo on repo showcasing how it works! Target Audience : For all detecting objects from display! Comparison : I did not find other project like that - but it was my first find too! It is also not OCR! https://github.com/NativeSensors/IconMatch Have fun!"},
{"Title": "Tutorial: Simple Pretty Maps That Will Improve Your Python Streamlit Skills", "Author": "u/jgloewen", "Content": "Interactive web applications for data visualization improve user engagement and understanding. These days, Streamlit is a very popular framework used to provide web applications for data science. It is a terrific programming tool to have in you Python knowledge toolbox. Here‚Äôs a fun and practical tutorial on how to create a simple interactive and dynamic Streamlit application. This application generates a beautiful and original map using the prettymaps library. Free article: HERE"},
{"Title": "Interactive 8-Puzzle Game", "Author": "u/MohamedElfatih", "Content": "What does my project do Hi everyone I‚Äôm excited to share a project I‚Äôve been working on: an interactive 8-puzzle game built using Python and Pygame. This project also includes several solvers based on classic search algorithms. Technical details: Python: the primary language in the project. Pygame: for rendering and handling user interaction. Search algorithms: implement depth first search (dfs) and A star search for solving the puzzle. By default A star search is used because it finds the solution faster than dfs Target Audience This is a toy project I did for fun. You can find the project in GitHub: link I would love to get your feedback, contributions, and if you find it interesting or helpful, please give it a star on GitHub. Your support and feedback will help me improve and add more features!Thank you for checking out my project!"},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Mastering Python: 7 Strategies for Writing Clear, Organized, and Efficient Code", "Author": "u/Slow_Scene_7972", "Content": "Optimize Your Python Workflow: Proven Techniques for Crafting Production-Ready Code Link"},
{"Title": "Enhanced EPUB Version of \"An Understated Dominance\" Available for Fans!", "Author": "u/abdirxhmxn", "Content": "Hey everyone! I recently completed a fun project that I think fans of \"An Understated Dominance\" will appreciate, especially those who have struggled with reading the novel online due to website clutter and distractions. I've created a clean, distraction-free EPUB version of the novel that you can download and read on your preferred device. You can get it here: EPUB Download . Why I did this: The original website hosting the novel has a lot of distractions that can take away from the reading experience. My goal was to provide a seamless reading experience that allows you to fully immerse yourself in the story without unnecessary interruptions. Project Details: For those interested in the technical side, here's a brief overview of how I created the EPUB: Web Scraping: I used Python with libraries like requests and BeautifulSoup to scrape all 2240 chapters of the novel. Concurrency: Leveraged concurrent.futures to speed up the scraping process. Retries: Implemented retry logic with tenacity to handle any request failures. EPUB Creation: Used ebooklib to compile the scraped content into an EPUB file, complete with chapter headers and a cover image. You can check out the full code for this project on my GitHub: GitHub Repository . Feedback & Collaboration: I‚Äôd love to hear your thoughts on the EPUB. Feel free to download, read, and let me know if you encounter any issues or have suggestions for improvements. If you're interested in collaborating on similar projects or have any ideas, I'm all ears! Enjoy the novel, and happy reading!"},
{"Title": "Picodi - Simplifying Dependency Injection in Python", "Author": "u/yakimka", "Content": "What My Project Does Picodi is a lightweight and easy-to-use Dependency Injection ( DI ) library for Python. Picodi supports both synchronous and asynchronous contexts and offers features like resource lifecycle management. Think about Picodi as a decorator that helps you manage your dependencies without the need for a full-blown DI container. Key Features üåü Simple and lightweight üì¶ Zero dependencies ‚è±Ô∏è Supports both sync and async contexts üîÑ Resource lifecycle management üîç Type hints support üêç Python & PyPy 3.10+ support Quick Start Here‚Äôs a quick example of how Picodi works: import asyncio from collections.abc import Callable from datetime import date from typing import Any import httpx from picodi import Provide, init_resources, inject, resource, shutdown_resources from picodi.helpers import get_value def get_settings() -> dict: return { \"nasa_api\": { \"api_key\": \"DEMO_KEY\", \"base_url\": \"https://api.nasa.gov\", \"timeout\": 10, } } @inject def get_setting(path: str, settings: dict = Provide(get_settings)) -> Callable[[], Any]: value = get_value(path, settings) return lambda: value @resource @inject async def get_nasa_client( api_key: str = Provide(get_setting(\"nasa_api.api_key\")), base_url: str = Provide(get_setting(\"nasa_api.base_url\")), timeout: int = Provide(get_setting(\"nasa_api.timeout\")), ) -> httpx.AsyncClient: async with httpx.AsyncClient( base_url=base_url, params={\"api_key\": api_key}, timeout=timeout ) as client: yield client @inject async def get_apod( date: date, client: httpx.AsyncClient = Provide(get_nasa_client) ) -> dict[str, Any]: response = await client.get(\"/planetary/apod\", params={\"date\": date.isoformat()}) response.raise_for_status() return response.json() async def main(): await init_resources() apod_data = await get_apod(date(2011, 7, 19)) print(\"Title:\", apod_data[\"title\"]) await shutdown_resources() if __name__ == \"__main__\": asyncio.run(main()) This example demonstrates how Picodi handles dependency injection for both synchronous and asynchronous functions, manages resource lifecycles, and provides a clean and efficient way to structure your code. For more examples and detailed documentation, check out the GitHub repository Target Audience Picodi is perfect for developers who want to simplify dependency management in their Python applications, but don't want to deal with the complexity of larger DI frameworks. Picodi can help you write cleaner and more maintainable code. Comparison Unlike other DI libraries, Picodi does not have wiring, a large set of different types of providers, or the concept of a container. Picodi prioritizes simplicity, so it includes only the most essential features: dependency injection, resource lifecycle management, and dependency overriding. Get Involved Picodi is still in the experimental stage, and I'm looking for feedback from the community. If you have any suggestions, encounter any issues, or want to contribute, please check out the GitHub repository and let me know."},
{"Title": "Homoiconic Python Code", "Author": "u/moonbunR", "Content": "Homoiconic, what does it mean? In simple terms, homoiconic code is when code is treated as data and can be manipulated as you would data. This means the code can be changed, new functions and variables added, the code can generate new code or even examine and modify its own structure and behavior all while it is running. That‚Äôs why homoiconic languages like Lisp are so powerful. But what if we can make a homoiconic python code, where the code and the data are one and the same and can be modified in the same way? This guide does a good job in trying to explain how you would create a python version of the ‚ÄúLisp in Lisp‚Äù code which would give you access to all those homoiconic features that Lisp brags of like the macro systems, the expressiveness and flexibility, the metaprogramming etc. while still using python. What do you guys think of this?"},
{"Title": "The best Python CLI library, arguably.", "Author": "u/AND_MY_HAX", "Content": "What My Project Does https://github.com/treykeown/arguably arguably makes it super simple to define complex CLIs. It uses your function signatures and docstrings to set everything up. Here's how it works: Adding the @arguably.command decorator to a function makes it appear on the CLI. If multiple functions are decorated, they'll all be set up as subcommands. You can even set up multiple levels of subcommands. The function name, signature, and docstring are used to automatically set up the CLI Call arguably.run() to parse the arguments and invoke the appropriate command A small example: #!/usr/bin/env python3 import arguably @arguably.command def some_function(required, not_required=2, *others: int, option: float = 3.14): \"\"\" this function is on the command line! Args: required: a required argument not_required: this one isn't required, since it has a default value *others: all the other positional arguments go here option: [-x] keyword-only args are options, short name is in brackets \"\"\" print(f\"{required=}, {not_required=}, {others=}, {option=}\") if __name__ == \"__main__\": arguably.run() becomes user@machine:~$ ./readme-1.py -h usage: readme-1.py [-h] [-x OPTION] required [not-required] [others ...] this function is on the command line! positional arguments: required             a required parameter (type: str) not-required         this one isn't required, since it has a default (type: int, default: 2) others               all the other positional arguments go here (type: int) options: -h, --help           show this help message and exit -x, --option OPTION  an option, short name is in brackets (type: float, default: 3.14) It can easily hand some very complex cases, like passing in QEMU-style arguments to automatically instantiated different types of classes: user@machine:~$ ./readme-2.py --nic tap,model=e1000 --nic user,hostfwd=tcp::10022-:22 nic=[TapNic(model='e1000'), UserNic(hostfwd='tcp::10022-:22')] You can also auto-generate a CLI for your script through python3 -m arguably your_script.py , more on that here . Target Audience If you're writing a script or tool, and you need a quick and effective way to run it from the command line, arguably was made for you. It's great for things where a CLI is essential, but doesn't need tons of customization. arguably makes some opinionated decisions that keep things simple for you, but doesn't expose ways of handling things like error messages. I put in the work to create GitHub workflows, documentation, and proper tests for arguably . I want this to be useful for the community at large, and a tool that you can rely on. Let me know if you're having trouble with your use case! Comparison There are plenty of other tools for making CLIs out there. My goal was to build one that's unobtrusive and easy to integrate. I wrote a whole page on the project goals here: https://treykeown.github.io/arguably/why/ A quick comparison: argparse - this is what arguably uses under the hood. The end user experience should be similar - arguably just aims to make it easy to set up. click - a powerhouse with all the tools you'd ever want. Use this if you need extensive customization and don't mind some verbosity. typer - also a great option, and some aspects are similar design-wise. It also uses functions with a decorator to set up commands, and also uses the function signature. A bit more verbose, though like click , has more customization options. fire - super easy to generate CLIs. arguably tries to improve on this by utilizing type hints for argument conversion, and being a little more of a middle ground between this and the more traditional ways of writing CLIs in Python. This project has been a labor of love to make CLI generation as easy as it should be. Thanks for checking it out!"},
{"Title": "LinkedIn-Learning-Downloader v1.1", "Author": "u/YounesWinter", "Content": "With Python i created a tool that enables users to download LinkedIn Learning courses, including the often overlooked but incredibly valuable exercise files. This feature sets our project apart, offering a complete learning experience by providing both the course videos and the materials needed for practical application. What great about it and beyond other tools in the same genre concerned LinkedIn Learning Downloaders, now you can download the whole courses from a path link. this is was never possible without Python. For more detailed information, visit the repo : https://github.com/M0r0cc4nGh0st/LinkedIn-Learning-Downloader"},
{"Title": "FastAPI Backend Template for SaaS products", "Author": "u/shekhuu", "Content": "Hello there, I just created a template for creating a backend for your SaaS products. What my project does: It is a FastAPI project/template for creating SaaS backends and admin dashboards. Comparison: Out of the box, it supports Licence key generation and validation. OAuth 2 authentication with scopes. Endpoints with pagination and filters to easily integrate with an admin dashboard. Passwords are securely stored using hashing. used PostgreSQL for database Target Audience: Production Check it here! Update 1 : Added pre-commit hooks, tox for testing and linting."},
{"Title": "this.s and this.d", "Author": "u/VladTbk", "Content": "Recently, I found out about the this \"Easter egg\" in python3. Adding import this into a py file will print \"The Zen of Python\" by Tim Peters. Also, this has two attributes: this.s and this.d , which I guess form the actual Easter egg. this.s returns an encrypted version of \"The Zen\" and this.d well, see for yourself, maybe you'll solve the puzzle."},
{"Title": "I made a Mandelbrot Zoom using Python", "Author": "u/AbideByReason", "Content": "I made a YouTube video which previews the zoom and explains the code, which you can find here: https://youtu.be/HtNUFdh2sjg What my project does: it creates a Mandelbrot Zoom. Comparison: it uses Pillow and consists of just 2 main blocks of code: one is the main function that finds which points are in the Mandelbrot Set and the other is the main loop that applies appropriate colors to each image. It gives the option of being black and white OR in color. It works fairly well but can definitely be faster if parallelized. I'd love to hear any suggestions on how it can be improved. Target Audience: fun/toy project Source code is here: https://github.com/AbideByReason/Python_Notebooks/tree/main"},
{"Title": "sjvisualizer: a python package to animate time-series data", "Author": "u/PieChartPirate", "Content": "What the project does: data animation library for time-series data. Currently it supports the following chart types: Bar races Animated Pie Charts Animated Line Charts Animated Stacked Area Charts Animated (World) Maps You can find some simple example charts here: https://www.sjdataviz.com/software It is on pypi, you can install it using: pip install sjvisualizer It is fully based on TkInter to draw the graph shapes to the screen, which gives a lot of flexibility. You can also mix and match the different chart types in a single animation. Target audience: people interested in data animation for presentations or social media content creation Alternatives: I only know one alternative which is bar-chart-race, the ways sjvisualizer is better: Smoother animation, bar-chart-race isn't the quite choppy I would say Load custom icons for each data category (flag icons for countries for example) Number of supported chart types Mix and match different chart types in a single animation, have a bar race to show the ranking, and a smaller pie chart showing the percentages of the whole Based on TkInter, easy to add custom elements through the standard python GUI library Topics to improve (contributions welcome): Documentation Improve built in screen recorder, performance takes a hit when using the built in screen recorder Additional chart types: bubble charts, lollipop charts, etc Improve the way data can be loaded into the library (currently only supports reading into a dataframe from Excel) Sorry for the long post, you can find it here on GitHub: https://github.com/SjoerdTilmans/sjvisualizer"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "The Python on Microcontrollers (and Raspberry Pi) Newsletter, a weekly news and project resource", "Author": "u/HP7933", "Content": "The Python on Microcontrollers (and Raspberry Pi) Newsletter: subscribe for free With the Python on Microcontrollers newsletter, you get all the latest information on Python running on hardware in one place! MicroPython, CircuitPython and Python on single Board Computers like Raspberry Pi & many more. The Python on Microcontrollers newsletter is the place for the latest news. It arrives Monday morning with all the week‚Äôs happenings. No advertising, no spam, easy to unsubscribe. 10,998 subscribers - the largest Python on hardware newsletter out there. (2 more for 11k!) Catch all the weekly news on Python for Microcontrollers with adafruitdaily.com . This ad-free, spam-free weekly email is filled with CircuitPython, MicroPython, and Python information that you may have missed, all in one place! Ensure you catch the weekly Python on Hardware roundup‚Äì you can cancel anytime ‚Äì try our spam-free newsletter today! https://www.adafruitdaily.com/"},
{"Title": "Auto Data Analysis python packages to know", "Author": "u/mehul_gupta1997", "Content": "Check this video tutorial to explore different AutoEDA python packages like pandas-profiling, sweetviz, dataprep,etc which can enable automatic data analysis within minutes without any effort : https://youtu.be/Z7RgmM4cI2I?si=8GGM50qqlN0lGzry"},
{"Title": "Folks who know the internals: Where does operator precedence \"happen\"?", "Author": "u/BeerIsTheMindKiller", "Content": "Hey! Messing around with instaviz, cool library, highly recommend. You can visualize a function's bytecode as well as AST and some other stuff. i entered this: def f(): x = 1 + 2 - 10**2 return x I was expecting the AST nodes for 1 + 2 - 10**2 to be rearranged somehow, with 10**2 being moved to the left hand of the expression, because exponents get evaluated before addition/subtraction. but no! just looks like this: ... (more tree up here) BinOp |                    \\                 \\ BinOp          Sub             BinOp |    \\      \\                           /    |    \\ 1  ADD  2                       10 POW 2 I was assuming operator precedence was implemented as the AST level. Seems no - I would assume that the tree would've had the 10 POW 2 on the left. Does it happen at the control flow graph phase? I can imagine the interpreter itself handles it. danke danke danke danke"},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "pip time machine", "Author": "u/rejectedlesbian", "Content": "https://github.com/nevakrien/time_machine_pip this is a fairly simple project barely anything to it but I think its promising the idea is to put pip in a time machine so it can not use package versions that were made after the project is made. I am doing this by proxiying pypi and cutting out the newer versions. initial tests show that pip respects the proxy and works like you would expect"},
{"Title": "Building an LLM chat application using RAG Agent", "Author": "u/iryna_kondr", "Content": "Motivation Chatbots are among the most popular applications of large language models (LLMs). Often, an LLM's internal knowledge base is adequate for answering users questions. However, in those cases, the model may generate outdated, incorrect, or too generic responses when specificity is expected. These challenges can be partially addressed by supplementing the LLM with an external knowledge base and employing the retrieval-augmented generation (RAG) technique. However, if user queries are complex, it may be necessary to break the task into several sub-parts. In such cases, relying solely on the RAG technique may not be sufficient, and the use of agents may be required. The fundamental concept of agents involves using a language model to determine a sequence of actions (including the usage of external tools) and their order. One possible action could be retrieving data from an external knowledge base in response to a user's query. In this tutorial, we will develop a simple Agent that accesses multiple data sources and invokes data retrieval when needed. We will use a Dingo framework that allows the development of LLM pipelines and autonomous agents. RAG Agent Architecture and Technical Stack The application will consist of the following components: Streamlit : provides a frontend interface for users to interact with a chatbot. FastAPI : facilitates communication between the frontend and backend. Dingo Agent :¬†agent powered by GPT-4 Turbo model from OpenAI that has access to provided knowledge bases and invokes data retrieval from them if needed. LLMs docs: a vector store containing documentation about the recently released Phi-3 (from Microsoft) and Llama 3 (from Meta) models. Audio gen docs: a vector store containing documentation about the recently released OpenVoice model from MyShell . Embedding V3 small model from OpenAI: computes text embeddings. QDrant : vector database that stores embedded chunks of text. Implementation Step 0: Install the Dingo framework: pip install agent-dingo Set the OPENAI_API_KEY environment variable to your OpenAI API key: export OPENAI_API_KEY=your-api-key Step 1: Create a component.py file, and initialize an embedding model, a chat model, and two vector stores: one for storing documentation of Llama 3 and Phi-3, and another for storing documentation of OpenVoice. # component.py from agent_dingo.rag.embedders.openai import OpenAIEmbedder from agent_dingo.rag.vector_stores.qdrant import Qdrant from agent_dingo.llm.openai import OpenAI # Initialize an embedding model embedder = OpenAIEmbedder(model=\"text-embedding-3-small\") # Initialize a vector store with information about Phi-3 and Llama 3 models llm_vector_store = Qdrant(collection_name=\"llm\", embedding_size=1536, path=\"./qdrant_db_llm\") # Initialize a vector store with information about OpenVoice model audio_gen_vector_store = Qdrant(collection_name=\"audio_gen\", embedding_size=1536, path=\"./qdrant_db_audio_gen\") # Initialize an LLM llm = OpenAI(model = \"gpt-3.5-turbo\") Step 2: Create a build.py file. Parse, chunk into smaller pieces, and embed websites containing documentation of the above-mentioned models. The embedded chunks are used to populate the corresponding vector stores. # build.py from components import llm_vector_store, audio_gen_vector_store, embedder from agent_dingo.rag.readers.web import WebpageReader from agent_dingo.rag.chunkers.recursive import RecursiveChunker # Read the content of the websites reader = WebpageReader() phi_3_docs = reader.read(\"https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/\") llama_3_docs = reader.read(\"https://ai.meta.com/blog/meta-llama-3/\") openvoice_docs = reader.read(\"https://research.myshell.ai/open-voice\") # Chunk the documents chunker = RecursiveChunker(chunk_size=512) phi_3_chunks = chunker.chunk(phi_3_docs) llama_3_chunks = chunker.chunk(llama_3_docs) openvoice_chunks = chunker.chunk(openvoice_docs) # Embed the chunks for doc in [phi_3_chunks, llama_3_chunks, openvoice_chunks]: embedder.embed_chunks(doc) # Populate LLM vector store with embedded chunks about Phi-3 and Llama 3 for chunk in [phi_3_chunks, llama_3_chunks]: llm_vector_store.upsert_chunks(chunk) # Populate audio gen vector store with embedded chunks about OpenVoice audio_gen_vector_store.upsert_chunks(openvoice_chunks) Run the script: python build.py At this step, we have successfully created vector stores. Step 3: Create serve.py file, and build a RAG pipeline. To access the pipeline from the Streamlit application, we can serve it using the serve_pipeline function, which provides a REST API compatible with the OpenAI API. # serve.py from agent_dingo.agent import Agent from agent_dingo.serve import serve_pipeline from components import llm_vector_store, audio_gen_vector_store, embedder, llm agent = Agent(llm, max_function_calls=3) # Define a function that an agent can call if needed u/agent.function def retrieve(topic: str, query: str) -> str: \"\"\"Retrieves the documents from the vector store based on the similarity to the query. This function is to be used to retrieve the additional information in order to answer users' queries. Parameters ---------- topic : str The topic, can be either \"large_language_models\" or \"audio_generation_models\". \"large_language_models\" covers the documentation of Phi-3 family of models from Microsoft and Llama 3 model from Meta. \"audio_generation_models\" covers the documentation of OpenVoice voice cloning model from MyShell. Enum: [\"large_language_models\", \"audio_generation_models\"] query : str A string that is used for similarity search of document chunks. Returns ------- str JSON-formatted string with retrieved chunks. \"\"\" print(f'called retrieve with topic {topic} and query {query}') if topic == \"large_language_models\": vs = llm_vector_store elif topic == \"audio_generation_models\": vs = audio_gen_vector_store else: return \"Unknown topic. The topic must be one of `large_language_models` or `audio_generation_models`\" query_embedding = embedder.embed(query)[0] retrieved_chunks = vs.retrieve(k=5, query=query_embedding) print(f'retrieved data: {retrieved_chunks}') return str([chunk.content for chunk in retrieved_chunks]) # Create a pipeline pipeline = agent.as_pipeline() # Serve the pipeline serve_pipeline( {\"gpt-agent\": pipeline}, host=\"127.0.0.1\", port=8000, is_async=False, ) Run the script: python serve.py At this stage, we have an openai-compatible backend with a model named gpt-agent , running on http://127.0.0.1:8000/ . The Streamlit application will send requests to this backend. Step 4: Create app.py file, and build a chatbot UI: # app.py import streamlit as st from openai import OpenAI st.title(\"ü¶ä Agent\") # provide any string as an api_key parameter client = OpenAI(base_url=\"http://127.0.0.1:8000\", api_key=\"123\") if \"openai_model\" not in st.session_state: st.session_state[\"openai_model\"] = \"gpt-agent\" if \"messages\" not in st.session_state: st.session_state.messages = [] for message in st.session_state.messages: avatar = \"ü¶ä\" if message[\"role\"] == \"assistant\" else \"üë§\" with st.chat_message(message[\"role\"], avatar=avatar): st.markdown(message[\"content\"]) if prompt := st.chat_input(\"How can I assist you today?\"): st.session_state.messages.append({\"role\": \"user\", \"content\": prompt}) with st.chat_message(\"user\", avatar=\"üë§\"): st.markdown(prompt) with st.chat_message(\"assistant\", avatar=\"ü¶ä\"): stream = client.chat.completions.create( model=st.session_state[\"openai_model\"], messages=[ {\"role\": m[\"role\"], \"content\": m[\"content\"]} for m in st.session_state.messages ], stream=False, ) response = st.write_stream((i for i in stream.choices[0].message.content)) st.session_state.messages.append({\"role\": \"assistant\", \"content\": response}) Run the application: streamlit run app.py üéâ¬†We have successfully built an Agent that is augmented with the technical documentation of several newly released generative models and can retrieve information from these documents if necessary.¬†Let‚Äôs ask some technical questions, and check the generated output: Conclusion In this tutorial, we have developed a RAG agent that can access external knowledge bases, selectively decide whether to access the external data, which data source to use (and how many times), and how to rewrite the user's query before retrieving the data. It can be seen that the Dingo framework enhances the development of LLM-based applications by allowing developers to quickly and easily create application prototypes."},
{"Title": "What changes needs to be done when I change the version of Wergzeug from 2.3.8 to 3.0.0 ?", "Author": "u/These_Shoe3594", "Content": "What are all the changes needs to be done when I change the version of Wergzeug from 2.3.8 to 3.0.0 ? There are some CVE fixes available in the latest 3.x version of werkzueg. To take the fixes as part of my code, we want to upgrade the version. When I do so, I‚Äôve faced lot of breakages. I found some on documents and release notes. But it would be easier if someone already did some changes regarding this."},
{"Title": "Langchain using llama3 to build recommendation system", "Author": "u/yngwieHero", "Content": "Hi, Recently I played a bit with LLMs, specifcally exploring ways of running the models locally and building prompts using LangChain. As a result ended up coding a small recommendation system, powered with Llama3-7b model, which suggests topics to read on HackerNews. Wanted to share my experiences, so I wrote a small article where I described all my findings. Hope you'll like it: https://lukaszksiezak.github.io/ScrapyToLLM/ Github repo: https://github.com/lukaszksiezak/ScrapyToLLM What the project does: It's a Python application which uses scrapy to scrape HackerNews page. Scraped articles are pipelined to redis, which is then feeding Llama3 using langchain. Prompter is configured to serve a user articles which are matching his request. Target Audience : I think it suits the best all the people who are looking for a Hello World projects using LLMs. I think it also reveals some difficulties related to LLM tech, what potential problems could be found in production systems. Comparison: Recommendation systems are widely used and known, however LLMs are the ones which may work out of the box when appropriate prompt is given. It's kind of interesting to explore various usages of the technology and take part in fast grow of that stack. Cheers."},
{"Title": "PyCon US 2024 is here!", "Author": "u/monorepo", "Content": "Official Event"},
{"Title": "Blat AI generates Python code to do web-scraping (code based on Scrapy framework)", "Author": "u/arnaupv", "Content": "Miguel Algorri and Arnau Pont V√≠lchez here, blat co-founders! Target Audience People who need to collect public data from the web (pricing, articles, reviews, leads etc). What does our Project Do? At blat we aim to deliver production-ready web scraping code in minutes (written in Python, Scrapy framework). This is feasible thanks to our Web Scraping AI Agent üß†. Here our CLI to interact with the Web Scraping AI Agent (github) . Too good to be true? Check our video Comparison There are lots of other tools in the market, like Zyte , Apify , Kadoa . All those are great tools for web scraping purposes. The main difference with our competitors is that we give you the Python code that's ready to use (you host it, you run it). Also, once created, the code does not use AI for parsing HTMLs, so it's more efficient and deterministic. What are we looking for? We encourage you to register as a alpha testers üí™¬†if you are willing to have a better and more automated web scraping experience. Here our CLI to interact with the Web Scraping AI Agent (github)"},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Apple Health data exploration with Atlas, Clickhouse, Vega-Altair, Quarto", "Author": "u/__tosh", "Content": "What My Project Does A few days ago I wrote a simple python script (\" Atlas \") that turns the Apple Health export.xml file (which is about 1 GB in my case, with about 10 years of data) into a very simple parquet file (a bit like a compressed CSV) that is also way smaller (40 MB). The parquet file has 5 columns: type (e.g. \"CyclingDistance\") value (e.g. \"12.100\") and 3 datetime timestamps: start end created This makes it way easier to do data exploration. Here are a few example charts I generated using Clickhouse (chDB) and Vega-Altair in a Quarto notebook. Step Count: https://x.com/__tosh/status/1785397655784337684 Environmental Noise: https://x.com/__tosh/status/1787530483208786029 Sleep Duration & States: https://x.com/__tosh/status/1786505867438768254 Coffee Consumption: https://x.com/__tosh/status/1783906333911076996 Coffee after 17:00: https://twitter.com/__tosh/status/1789304034442043421/photo/1 Target Audience For everyone who would like to explore their own Apple Health data or see how to work with a simple .parquet file using Clickhouse (chDB), Vega-Altair and Quarto. Quarto notebook: https://github.com/atlaslib/atlas/blob/main/examples/apple-health-exploration-clickhouse-chdb-altair-quarto/index.qmd In the repo on Github I've added also added instructions for how to get your export.xml file from Apple Health and how to install the python script via pip to use it as a command line tool: https://github.com/atlaslib/atlas (‚≠êÔ∏è star to stay tuned for updates) Curious if you have charts that you would be interested in. Happy to add more examples over the next days! Comparison This is me playing around with the data and wrapping the script up in a pip package to make it easier for others to install and use. You can also explore the data in the Apple Health app but why would you if you can also explore it with your favorite programming language?"},
{"Title": "I made a python bot that plays minesweeper", "Author": "u/_dwightshrute", "Content": "Hello, I made this Minesweeper bot that I wanted to share with you all. What My Project Does - The bot takes a screenshot of the board and runs a classification algorithm to extract the contents of the board. It then analyzes the board, finds as many mines as it can, and sends clicks. If it cannot find any mines then it guesses the most probable position of a mine. Target Audience - It's a toy project for anyone interested in algorithms or problem-solving. Comparison - This is just my attempt at making a minesweeper bot. I'm sure there are many bots out there that are much more efficient than this. do let me know, if you feel anything can be done better :)"},
{"Title": "Production grade AI Web apps, just using python ?", "Author": "u/prime_danger", "Content": "Hey guys, I have worked on building multiple ai/ml usecases and their specific backends. But now I want build interfaces for easy and quick integration. I saw a blog which used FastUI which looks quick decent but when I tried it just showed me a Json of elements on the page. Are there any other libraries I should use? ü§î"},
{"Title": "Is PyGame still alive?", "Author": "u/pyeri", "Content": "So it was a long time ago in the good old Python 2.x days (circa 2010 probably) that I had learned PyGame with some tutorials at my former work place. But nowadays since I mostly freelance with business apps, I never felt the need for it. But since such a game development project is on the horizon after all these years, I was wondering if PyGame can still be up for the task with Python 3.x? Or is there a better Python library available these days? I don't need any advanced gaming features of modern day VFX or anything, all I need is some basic Mario/Luigi style graphics, that's all!"},
{"Title": "Track the size of your PyInstaller packages in CI", "Author": "u/bencherdev", "Content": "If you have ever wanted to track the size of your PyInstaller packages in CI, Bencher now supports tracking your package size: https://bencher.dev/docs/how-to/track-file-size/"},
{"Title": "I created a Python script that makes it easier to track how your baby's sleep is improving", "Author": "u/BX1959", "Content": "My wife and I use the Huckleberry app to track our baby's sleep periods. Although the free version of the app allows you to view a number of sleep-related metrics, I also wanted to see whether his longest nightly sleep stretches were getting longer over time. Therefore, I created a Python project to help me answer this and other questions I had about my baby's sleep. What My Project Does This project reads in data from a Huckleberry .csv export (or a separate custom .csv file); analyzes its sleep information; and then produces a number of visualizations . Personally, I've found that running the code and viewing its output helps reassure me that our baby is making progress with sleep, even if he seems to have some setbacks now and then! I hope you'll find it useful as well in evaluating the effectiveness of your sleep training approach. Target Audience This project can be useful for any parent who wishes to see how his or her baby's sleep is improving over time. (It could be used for other age ranges as well, but the code and visualizations are geared towards infant sleep data.) The project's readme has instructions on using the code to track your own baby's sleep data. Comparison This project is released under the open-source MIT license, so you are welcome to use and modify it for free. (I imagine that this is not the case for many sleep analysis tools.) As noted earlier, the project allows you to see how your baby's longest daily sleep stretch has improved over time. (The longer your baby sleeps at any given point, the longer you get to sleep, so I think this metric is of great interest to most parents!) I don't think the free version of Huckleberry includes this data in line chart form, though you can get a sense of this improvement by scrolling through your daily sleep data. This script also separates individaul sleep entries into their respective daytime and nighttime components. For instance, if your baby slept from 6 AM to 9 AM, and you've specified the nighttime period to end at 7 AM (the default setting), the script will treat this entry as one hour of nighttime sleep and 2 hours of daytime sleep. I don't think Huckleberry offers this same functionality, though I could be wrong. (Note: The sample data shown within the project is completely made up using another Python script , and is not meant to reflect normal sleep patterns in infants.)"},
{"Title": "ChatGPT can talk with all my Python notebooks", "Author": "u/pp314159", "Content": "I'm working on an open-source framework for converting Python notebooks into web applications, it is called Mercury . Recently, I have added an option to execute notebooks with REST API. You can pass paramters in POST request body to the notebook, execute all cells and return JSON as response. I'm also running a SaaS service, Mercury Cloud where you can deploy notebooks with one-click with unique website domain. What My Project Does It makes Python notebooks extermely easy to integrate with custom ChatGPT, so GPT can execute Python notebooks and get response. It is 3-steps process: Create Python notebook, with Mercury widgets to accept parameters and return JSON response. Deploy notebook online - it can be done in 1 minute with Mercury Cloud Configure ChatGPT Actions - it is quick, because Mercury automatically generates OpenAPI schema based on your notebooks. You can read more in article how ChatGPT is talking with all my Python notebooks . Target Audience This solution is perfect for people: that would like to quickly create custom API for ChatGPT with Python, that don't want to manage server by themself. Comparison I think that building API with Python notebooks is alternative for full REST API development with Django, Flask or FastAPI, if you quickly need few endpoints that will expose your Python code. Examples I have created example notebooks that are used by ChatGPT: notebook to send email directly from ChatGPT notebook to query Postgres database with SQL from ChatGPT notebook to access Google Sheets in custom ChatGPT"},
{"Title": "Framework to use for backend", "Author": "u/Clickyz", "Content": "Hello guys I recently decided to move from nodejs(expressjs) to python for general purposes but mostly for backend. I have couple of questions. Will i regret my migration to python? :) Which framework you suggest for backend solo dev? And what tips are you suggesting me in general to get used to python."},
{"Title": "New Python-only abstractions for extracting data from apis", "Author": "u/Thinker_Assignment", "Content": "Hey there, you are probably familiar with REST APIs. We at dlt library added a new way to get data from apis (and dlt can already load it with best practice to db or parquet). We already did some internal hackathons but we would appreciate your feedback so we can improve it further - Our new REST API Source is a short, declarative configuration driven way of creating sources. - Our new REST API Client is a collection of Python helpers used by the above source, which you can also use as a standalone, config-free, imperative high-level abstraction for building pipelines. You can read more about the source here or go to our docs for the REST APIClient info https://github.com/dlt-hub/verified-sources/tree/master/sources/rest_api PS: see you at Pycon Pittsburgh!"},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Interactive Data Visualization with Python: A Showcase of Plotly Dash", "Author": "u/AbhishekSuryavanshee", "Content": "What My Project Does: I'm excited to introduce my latest project built with Python ‚Äì an interactive data visualization application using Plotly Dash. This project aims to empower users to explore and analyze datasets dynamically through interactive visualizations. By leveraging Plotly Dash's capabilities, users can interact with data in real-time, customize visualizations on the fly, and gain deeper insights with just a few clicks. Target Audience: This project caters to a wide range of users, from data enthusiasts and analysts to professionals seeking to communicate insights effectively. Whether you're a data scientist exploring patterns in large datasets or a business analyst presenting findings to stakeholders, this tool is designed to streamline your workflow and enhance your data storytelling capabilities. It's suitable for both production-grade applications and educational purposes, offering a versatile platform for data visualization tasks of varying complexity. Comparison: Unlike traditional static charts or cumbersome data exploration tools, this Plotly Dash application stands out for its interactivity and flexibility. While existing alternatives may offer basic charting functionalities, they often lack the dynamic capabilities required for in-depth data exploration. With Plotly Dash, users can manipulate charts in real-time, zoom in on specific data points, filter datasets dynamically, and even integrate interactive components like dropdowns and sliders for a more immersive experience. This project takes data visualization to the next level by providing a user-friendly interface coupled with powerful interactivity, setting it apart as a top choice for visualizing and analyzing datasets. Source Code: You can access the source code for this project on GitHub: Interactive Data Visualization with Plotly Dash Website: For more information and to see the project in action, visit : https://www.aspiresoftserv.com/"},
{"Title": "Declarative GUI Slint v1.6 released with Design Mode (WYSIWYG) Improvements", "Author": "u/madnirua", "Content": "https://slint.dev/blog/slint-1.6-released Slint is a declarative GUI toolkit to build native user interfaces for desktop and embedded applications. Find more information at https://slint.dev/ or check out the source code at https://github.com/slint-ui/slint . EDIT: The Python APIs are currently in alpha. More info -- https://github.com/slint-ui/slint/tree/master/api/python"},
{"Title": "Implementing your own pypi clone", "Author": "u/chione99", "Content": "Hi, Just want to know how difficult is it to manage your own pypi clone and how do you recommend to create a seperation between dev and prod systems."},
{"Title": "Manage Your Squid Proxy Server Efficiently with This Python Script", "Author": "u/SAV_NC", "Content": "ü¶ë Squid Proxy Manager Script Hello fellow Python enthusiasts! I've created a Python script that makes managing your Squid Proxy Server a breeze. If you're looking for an efficient and straightforward way to interact with your Squid server remotely, this script is for you. üéâ What My Project Does The Squid Proxy Manager script allows you to manage your Squid Proxy Server remotely using a simple command-line interface. Here are some of the key features: Check Squid Service Status : Quickly check if your Squid service is running or not. Start/Stop/Restart Service : Easily control the Squid service remotely. View Logs : Access the latest entries in your Squid access logs. View Configuration : Display the current Squid configuration file. Update Configuration : Replace the existing Squid configuration with a new one. Reload Service : Reload the Squid service to apply changes without restarting. Target Audience This script is designed for anyone who manages a Squid Proxy Server and prefers a command-line tool for remote management. If you are comfortable using Python and SSH, this tool will streamline your workflow and enhance your productivity. Differences Here are some aspects that make this Squid Proxy Manager script stand out: Remote Management : Manage your Squid server without needing physical access, thanks to SSH connectivity. Ease of Use : The script provides a simple and intuitive command-line interface, making it easy to perform various tasks. Comprehensive Features : From checking service status to updating configurations and viewing logs, this script covers all essential Squid management tasks. Error Handling and Logging : Detailed logging and error handling ensure you know exactly what's happening and can troubleshoot issues effectively. üöÄ Usage Installation : Ensure you have the required libraries installed: pip install paramiko termcolor Running the Script : Use the script with appropriate arguments to manage your Squid Proxy Server. Here's an example command to check the Squid service status: ./squid_proxy_manager.py 192.168.2.111 22 username password --check-status Updating Configuration : Create a new configuration file (e.g., new_squid.conf ) with your desired settings. Run the script to update the Squid configuration: ./squid_proxy_manager.py 192.168.2.111 22 username password --update-config new_squid.conf üíª Script Example Here's a snippet of the script to give you an idea of its simplicity and functionality: #!/usr/bin/env python3 import paramiko import argparse import logging import sys import os from termcolor import colored class SquidProxyManager: def __init__(self, hostname, port, username, password): self.hostname = hostname self.port = port self.username = username self.password = password self.client = paramiko.SSHClient() self.client.set_missing_host_key_policy(paramiko.AutoAddPolicy()) def connect(self): try: logging.info(colored(\"Attempting to connect to {}:{}\".format(self.hostname, self.port), 'cyan')) self.client.connect(self.hostname, port=self.port, username=self.username, password=self.password) logging.info(colored(f\"Connected to {self.hostname} on port {self.port}\", 'green')) except Exception as e: logging.error(colored(f\"Failed to connect: {e}\", 'red')) sys.exit(1) def disconnect(self): self.client.close() logging.info(colored(\"Disconnected from the server\", 'green')) def execute_command(self, command): logging.info(colored(\"Executing command: {}\".format(command), 'cyan')) try: stdin, stdout, stderr = self.client.exec_command(command) stdout.channel.recv_exit_status() out = stdout.read().decode() err = stderr.read().decode() if err: logging.error(colored(f\"Error executing command '{command}': {err}\", 'red')) else: logging.info(colored(f\"Successfully executed command '{command}'\", 'green')) return out, err except Exception as e: logging.error(colored(f\"Exception during command execution '{command}': {e}\", 'red')) return \"\", str(e) # More functions here... def parse_args(): parser = argparse.ArgumentParser(description=\"Squid Proxy Manager\") parser.add_argument('hostname', help=\"IP address of the Squid proxy server\") parser.add_argument('port', type=int, help=\"Port number for SSH connection\") parser.add_argument('username', help=\"SSH username\") parser.add_argument('password', help=\"SSH password\") parser.add_argument('--check-status', action='store_true', help=\"Check Squid service status\") parser.add.add_argument('--start', action='store_true', help=\"Start Squid service\") parser.add.add_argument('--stop', action='store_true', help=\"Stop Squid service\") parser.add.add_argument('--restart', action='store_true', help=\"Restart Squid service\") parser.add.add_argument('--view-logs', action='store_true', help=\"View Squid logs\") parser.add.add_argument('--view-config', action='store_true', help=\"View Squid configuration\") parser.add.add_argument('--update-config', help=\"Update Squid configuration with provided data\") parser.add.add_argument('--reload', action='store_true', help=\"Reload Squid service\") return parser.parse_args() def main(): logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') args = parse_args() logging.info(colored(\"Initializing Squid Proxy Manager script\", 'cyan')) manager = SquidProxyManager(args.hostname, args.port, args.username, args.password) manager.connect() try: if args.check_status: manager.check_squid_status() if args.start: manager.start_squid() if args.stop: manager.stop_squid() if args.restart: manager.restart_squid() if args.view_logs: manager.view_squid_logs() if args.view_config: manager.view_squid_config() if args.update_config: if not args.update_config.endswith('.conf'): logging.error(colored(\"The provided file must have a .conf extension\", 'red')) elif not os.path.isfile(args.update_config): logging.error(colored(f\"Configuration file {args.update_config} not found\", 'red')) else: try: with open(args.update_config, 'r') as config_file: config_data = config_file.read() manager.update_squid_config(config_data) except Exception as e: logging.error(colored(f\"Error reading configuration file {args.update_config}: {e}\", 'red')) if args.reload: manager.reload_squid() finally: manager.disconnect() logging.info(colored(\"Squid Proxy Manager operations completed\", 'green')) if __name__ == \"__main__\": main() üåü Benefits Remote Management : No need to be physically present to manage your Squid server. Ease of Use : Simple command-line interface for quick operations. Versatility : Supports various Squid management tasks, from checking status to updating configurations and viewing logs. üì¢ Get Involved! If you find this script useful, feel free to give it a try and share your feedback. Contributions and suggestions are always welcome! Comments however, that are unhelpful and serve no purpose to better the script or the author in their python scripting abilities are not welcome! Keep the nasty to yourself. Access the script You can find the script here on GitHub. Happy coding! üöÄ"},
{"Title": "modern_colorthief - Modified Median Cut Quantization algorithm in rust + python", "Author": "u/BasePlate_Admin", "Content": "github documentation What my project does : It gets the dominant color/color palette from given image. Target Audience: Anyone Usage modern_colorthief exposes two functions get_color and get_palette Here is how to use get_color: from modern_colorthief import get_color # Path to any image path = ... print(get_color(path)) # returns tuple[int,int,int] Here is how to use get_palette: from modern_colorthief import get_color # Path to any image path = ... print(get_palette(path)) # returns list[tuple[int,int,int]] Goals: Bring color-thief-rs to python Benchmarks: Written in deatils Gist: Python Took:            0.09976800000004005 CPP Took:               0.008461299999908078 RUST Took:              0.008549499994842336 Python Took:            0.0960583999985829 CPP Took:               0.008564600000681821 RUST Took:              0.007692700004554354 Differences With fast-colorthief Supports more architectures. ( pybind11 vs pyo3 ) Doesn't have a hard dependency on numpy Code is simple compared to fast-colorthief's CPP codebase Automated tooling powered by maturin and github-actions The size of fast-colorthief is 52kb-60kb. With color-thief-py Superior execution time (nearly 100x) Doesn't have a hard dependency on pillow color-thief's codebase is not in par with modern python versions If you like this project please star this repository"},
{"Title": "Auto Data Analysis python packages to know", "Author": "u/mehul_gupta1997", "Content": "Check this video tutorial to explore different AutoEDA python packages like pandas-profiling, sweetviz, dataprep,etc which can enable automatic data analysis within minutes without any effort : https://youtu.be/Z7RgmM4cI2I?si=8GGM50qqlN0lGzry"},
{"Title": "Folks who know the internals: Where does operator precedence \"happen\"?", "Author": "u/BeerIsTheMindKiller", "Content": "Hey! Messing around with instaviz, cool library, highly recommend. You can visualize a function's bytecode as well as AST and some other stuff. i entered this: def f(): x = 1 + 2 - 10**2 return x I was expecting the AST nodes for 1 + 2 - 10**2 to be rearranged somehow, with 10**2 being moved to the left hand of the expression, because exponents get evaluated before addition/subtraction. but no! just looks like this: ... (more tree up here) BinOp |                    \\                 \\ BinOp          Sub             BinOp |    \\      \\                           /    |    \\ 1  ADD  2                       10 POW 2 I was assuming operator precedence was implemented as the AST level. Seems no - I would assume that the tree would've had the 10 POW 2 on the left. Does it happen at the control flow graph phase? I can imagine the interpreter itself handles it. danke danke danke danke"},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "pip time machine", "Author": "u/rejectedlesbian", "Content": "https://github.com/nevakrien/time_machine_pip this is a fairly simple project barely anything to it but I think its promising the idea is to put pip in a time machine so it can not use package versions that were made after the project is made. I am doing this by proxiying pypi and cutting out the newer versions. initial tests show that pip respects the proxy and works like you would expect"},
{"Title": "Building an LLM chat application using RAG Agent", "Author": "u/iryna_kondr", "Content": "Motivation Chatbots are among the most popular applications of large language models (LLMs). Often, an LLM's internal knowledge base is adequate for answering users questions. However, in those cases, the model may generate outdated, incorrect, or too generic responses when specificity is expected. These challenges can be partially addressed by supplementing the LLM with an external knowledge base and employing the retrieval-augmented generation (RAG) technique. However, if user queries are complex, it may be necessary to break the task into several sub-parts. In such cases, relying solely on the RAG technique may not be sufficient, and the use of agents may be required. The fundamental concept of agents involves using a language model to determine a sequence of actions (including the usage of external tools) and their order. One possible action could be retrieving data from an external knowledge base in response to a user's query. In this tutorial, we will develop a simple Agent that accesses multiple data sources and invokes data retrieval when needed. We will use a Dingo framework that allows the development of LLM pipelines and autonomous agents. RAG Agent Architecture and Technical Stack The application will consist of the following components: Streamlit : provides a frontend interface for users to interact with a chatbot. FastAPI : facilitates communication between the frontend and backend. Dingo Agent :¬†agent powered by GPT-4 Turbo model from OpenAI that has access to provided knowledge bases and invokes data retrieval from them if needed. LLMs docs: a vector store containing documentation about the recently released Phi-3 (from Microsoft) and Llama 3 (from Meta) models. Audio gen docs: a vector store containing documentation about the recently released OpenVoice model from MyShell . Embedding V3 small model from OpenAI: computes text embeddings. QDrant : vector database that stores embedded chunks of text. Implementation Step 0: Install the Dingo framework: pip install agent-dingo Set the OPENAI_API_KEY environment variable to your OpenAI API key: export OPENAI_API_KEY=your-api-key Step 1: Create a component.py file, and initialize an embedding model, a chat model, and two vector stores: one for storing documentation of Llama 3 and Phi-3, and another for storing documentation of OpenVoice. # component.py from agent_dingo.rag.embedders.openai import OpenAIEmbedder from agent_dingo.rag.vector_stores.qdrant import Qdrant from agent_dingo.llm.openai import OpenAI # Initialize an embedding model embedder = OpenAIEmbedder(model=\"text-embedding-3-small\") # Initialize a vector store with information about Phi-3 and Llama 3 models llm_vector_store = Qdrant(collection_name=\"llm\", embedding_size=1536, path=\"./qdrant_db_llm\") # Initialize a vector store with information about OpenVoice model audio_gen_vector_store = Qdrant(collection_name=\"audio_gen\", embedding_size=1536, path=\"./qdrant_db_audio_gen\") # Initialize an LLM llm = OpenAI(model = \"gpt-3.5-turbo\") Step 2: Create a build.py file. Parse, chunk into smaller pieces, and embed websites containing documentation of the above-mentioned models. The embedded chunks are used to populate the corresponding vector stores. # build.py from components import llm_vector_store, audio_gen_vector_store, embedder from agent_dingo.rag.readers.web import WebpageReader from agent_dingo.rag.chunkers.recursive import RecursiveChunker # Read the content of the websites reader = WebpageReader() phi_3_docs = reader.read(\"https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/\") llama_3_docs = reader.read(\"https://ai.meta.com/blog/meta-llama-3/\") openvoice_docs = reader.read(\"https://research.myshell.ai/open-voice\") # Chunk the documents chunker = RecursiveChunker(chunk_size=512) phi_3_chunks = chunker.chunk(phi_3_docs) llama_3_chunks = chunker.chunk(llama_3_docs) openvoice_chunks = chunker.chunk(openvoice_docs) # Embed the chunks for doc in [phi_3_chunks, llama_3_chunks, openvoice_chunks]: embedder.embed_chunks(doc) # Populate LLM vector store with embedded chunks about Phi-3 and Llama 3 for chunk in [phi_3_chunks, llama_3_chunks]: llm_vector_store.upsert_chunks(chunk) # Populate audio gen vector store with embedded chunks about OpenVoice audio_gen_vector_store.upsert_chunks(openvoice_chunks) Run the script: python build.py At this step, we have successfully created vector stores. Step 3: Create serve.py file, and build a RAG pipeline. To access the pipeline from the Streamlit application, we can serve it using the serve_pipeline function, which provides a REST API compatible with the OpenAI API. # serve.py from agent_dingo.agent import Agent from agent_dingo.serve import serve_pipeline from components import llm_vector_store, audio_gen_vector_store, embedder, llm agent = Agent(llm, max_function_calls=3) # Define a function that an agent can call if needed u/agent.function def retrieve(topic: str, query: str) -> str: \"\"\"Retrieves the documents from the vector store based on the similarity to the query. This function is to be used to retrieve the additional information in order to answer users' queries. Parameters ---------- topic : str The topic, can be either \"large_language_models\" or \"audio_generation_models\". \"large_language_models\" covers the documentation of Phi-3 family of models from Microsoft and Llama 3 model from Meta. \"audio_generation_models\" covers the documentation of OpenVoice voice cloning model from MyShell. Enum: [\"large_language_models\", \"audio_generation_models\"] query : str A string that is used for similarity search of document chunks. Returns ------- str JSON-formatted string with retrieved chunks. \"\"\" print(f'called retrieve with topic {topic} and query {query}') if topic == \"large_language_models\": vs = llm_vector_store elif topic == \"audio_generation_models\": vs = audio_gen_vector_store else: return \"Unknown topic. The topic must be one of `large_language_models` or `audio_generation_models`\" query_embedding = embedder.embed(query)[0] retrieved_chunks = vs.retrieve(k=5, query=query_embedding) print(f'retrieved data: {retrieved_chunks}') return str([chunk.content for chunk in retrieved_chunks]) # Create a pipeline pipeline = agent.as_pipeline() # Serve the pipeline serve_pipeline( {\"gpt-agent\": pipeline}, host=\"127.0.0.1\", port=8000, is_async=False, ) Run the script: python serve.py At this stage, we have an openai-compatible backend with a model named gpt-agent , running on http://127.0.0.1:8000/ . The Streamlit application will send requests to this backend. Step 4: Create app.py file, and build a chatbot UI: # app.py import streamlit as st from openai import OpenAI st.title(\"ü¶ä Agent\") # provide any string as an api_key parameter client = OpenAI(base_url=\"http://127.0.0.1:8000\", api_key=\"123\") if \"openai_model\" not in st.session_state: st.session_state[\"openai_model\"] = \"gpt-agent\" if \"messages\" not in st.session_state: st.session_state.messages = [] for message in st.session_state.messages: avatar = \"ü¶ä\" if message[\"role\"] == \"assistant\" else \"üë§\" with st.chat_message(message[\"role\"], avatar=avatar): st.markdown(message[\"content\"]) if prompt := st.chat_input(\"How can I assist you today?\"): st.session_state.messages.append({\"role\": \"user\", \"content\": prompt}) with st.chat_message(\"user\", avatar=\"üë§\"): st.markdown(prompt) with st.chat_message(\"assistant\", avatar=\"ü¶ä\"): stream = client.chat.completions.create( model=st.session_state[\"openai_model\"], messages=[ {\"role\": m[\"role\"], \"content\": m[\"content\"]} for m in st.session_state.messages ], stream=False, ) response = st.write_stream((i for i in stream.choices[0].message.content)) st.session_state.messages.append({\"role\": \"assistant\", \"content\": response}) Run the application: streamlit run app.py üéâ¬†We have successfully built an Agent that is augmented with the technical documentation of several newly released generative models and can retrieve information from these documents if necessary.¬†Let‚Äôs ask some technical questions, and check the generated output: Conclusion In this tutorial, we have developed a RAG agent that can access external knowledge bases, selectively decide whether to access the external data, which data source to use (and how many times), and how to rewrite the user's query before retrieving the data. It can be seen that the Dingo framework enhances the development of LLM-based applications by allowing developers to quickly and easily create application prototypes."},
{"Title": "What changes needs to be done when I change the version of Wergzeug from 2.3.8 to 3.0.0 ?", "Author": "u/These_Shoe3594", "Content": "What are all the changes needs to be done when I change the version of Wergzeug from 2.3.8 to 3.0.0 ? There are some CVE fixes available in the latest 3.x version of werkzueg. To take the fixes as part of my code, we want to upgrade the version. When I do so, I‚Äôve faced lot of breakages. I found some on documents and release notes. But it would be easier if someone already did some changes regarding this."},
{"Title": "Langchain using llama3 to build recommendation system", "Author": "u/yngwieHero", "Content": "Hi, Recently I played a bit with LLMs, specifcally exploring ways of running the models locally and building prompts using LangChain. As a result ended up coding a small recommendation system, powered with Llama3-7b model, which suggests topics to read on HackerNews. Wanted to share my experiences, so I wrote a small article where I described all my findings. Hope you'll like it: https://lukaszksiezak.github.io/ScrapyToLLM/ Github repo: https://github.com/lukaszksiezak/ScrapyToLLM What the project does: It's a Python application which uses scrapy to scrape HackerNews page. Scraped articles are pipelined to redis, which is then feeding Llama3 using langchain. Prompter is configured to serve a user articles which are matching his request. Target Audience : I think it suits the best all the people who are looking for a Hello World projects using LLMs. I think it also reveals some difficulties related to LLM tech, what potential problems could be found in production systems. Comparison: Recommendation systems are widely used and known, however LLMs are the ones which may work out of the box when appropriate prompt is given. It's kind of interesting to explore various usages of the technology and take part in fast grow of that stack. Cheers."},
{"Title": "PyCon US 2024 is here!", "Author": "u/monorepo", "Content": "Official Event"},
{"Title": "Blat AI generates Python code to do web-scraping (code based on Scrapy framework)", "Author": "u/arnaupv", "Content": "Miguel Algorri and Arnau Pont V√≠lchez here, blat co-founders! Target Audience People who need to collect public data from the web (pricing, articles, reviews, leads etc). What does our Project Do? At blat we aim to deliver production-ready web scraping code in minutes (written in Python, Scrapy framework). This is feasible thanks to our Web Scraping AI Agent üß†. Here our CLI to interact with the Web Scraping AI Agent (github) . Too good to be true? Check our video Comparison There are lots of other tools in the market, like Zyte , Apify , Kadoa . All those are great tools for web scraping purposes. The main difference with our competitors is that we give you the Python code that's ready to use (you host it, you run it). Also, once created, the code does not use AI for parsing HTMLs, so it's more efficient and deterministic. What are we looking for? We encourage you to register as a alpha testers üí™¬†if you are willing to have a better and more automated web scraping experience. Here our CLI to interact with the Web Scraping AI Agent (github)"},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Apple Health data exploration with Atlas, Clickhouse, Vega-Altair, Quarto", "Author": "u/__tosh", "Content": "What My Project Does A few days ago I wrote a simple python script (\" Atlas \") that turns the Apple Health export.xml file (which is about 1 GB in my case, with about 10 years of data) into a very simple parquet file (a bit like a compressed CSV) that is also way smaller (40 MB). The parquet file has 5 columns: type (e.g. \"CyclingDistance\") value (e.g. \"12.100\") and 3 datetime timestamps: start end created This makes it way easier to do data exploration. Here are a few example charts I generated using Clickhouse (chDB) and Vega-Altair in a Quarto notebook. Step Count: https://x.com/__tosh/status/1785397655784337684 Environmental Noise: https://x.com/__tosh/status/1787530483208786029 Sleep Duration & States: https://x.com/__tosh/status/1786505867438768254 Coffee Consumption: https://x.com/__tosh/status/1783906333911076996 Coffee after 17:00: https://twitter.com/__tosh/status/1789304034442043421/photo/1 Target Audience For everyone who would like to explore their own Apple Health data or see how to work with a simple .parquet file using Clickhouse (chDB), Vega-Altair and Quarto. Quarto notebook: https://github.com/atlaslib/atlas/blob/main/examples/apple-health-exploration-clickhouse-chdb-altair-quarto/index.qmd In the repo on Github I've added also added instructions for how to get your export.xml file from Apple Health and how to install the python script via pip to use it as a command line tool: https://github.com/atlaslib/atlas (‚≠êÔ∏è star to stay tuned for updates) Curious if you have charts that you would be interested in. Happy to add more examples over the next days! Comparison This is me playing around with the data and wrapping the script up in a pip package to make it easier for others to install and use. You can also explore the data in the Apple Health app but why would you if you can also explore it with your favorite programming language?"},
{"Title": "I made a python bot that plays minesweeper", "Author": "u/_dwightshrute", "Content": "Hello, I made this Minesweeper bot that I wanted to share with you all. What My Project Does - The bot takes a screenshot of the board and runs a classification algorithm to extract the contents of the board. It then analyzes the board, finds as many mines as it can, and sends clicks. If it cannot find any mines then it guesses the most probable position of a mine. Target Audience - It's a toy project for anyone interested in algorithms or problem-solving. Comparison - This is just my attempt at making a minesweeper bot. I'm sure there are many bots out there that are much more efficient than this. do let me know, if you feel anything can be done better :)"},
{"Title": "Production grade AI Web apps, just using python ?", "Author": "u/prime_danger", "Content": "Hey guys, I have worked on building multiple ai/ml usecases and their specific backends. But now I want build interfaces for easy and quick integration. I saw a blog which used FastUI which looks quick decent but when I tried it just showed me a Json of elements on the page. Are there any other libraries I should use? ü§î"},
{"Title": "Is PyGame still alive?", "Author": "u/pyeri", "Content": "So it was a long time ago in the good old Python 2.x days (circa 2010 probably) that I had learned PyGame with some tutorials at my former work place. But nowadays since I mostly freelance with business apps, I never felt the need for it. But since such a game development project is on the horizon after all these years, I was wondering if PyGame can still be up for the task with Python 3.x? Or is there a better Python library available these days? I don't need any advanced gaming features of modern day VFX or anything, all I need is some basic Mario/Luigi style graphics, that's all!"},
{"Title": "Track the size of your PyInstaller packages in CI", "Author": "u/bencherdev", "Content": "If you have ever wanted to track the size of your PyInstaller packages in CI, Bencher now supports tracking your package size: https://bencher.dev/docs/how-to/track-file-size/"},
{"Title": "I created a Python script that makes it easier to track how your baby's sleep is improving", "Author": "u/BX1959", "Content": "My wife and I use the Huckleberry app to track our baby's sleep periods. Although the free version of the app allows you to view a number of sleep-related metrics, I also wanted to see whether his longest nightly sleep stretches were getting longer over time. Therefore, I created a Python project to help me answer this and other questions I had about my baby's sleep. What My Project Does This project reads in data from a Huckleberry .csv export (or a separate custom .csv file); analyzes its sleep information; and then produces a number of visualizations . Personally, I've found that running the code and viewing its output helps reassure me that our baby is making progress with sleep, even if he seems to have some setbacks now and then! I hope you'll find it useful as well in evaluating the effectiveness of your sleep training approach. Target Audience This project can be useful for any parent who wishes to see how his or her baby's sleep is improving over time. (It could be used for other age ranges as well, but the code and visualizations are geared towards infant sleep data.) The project's readme has instructions on using the code to track your own baby's sleep data. Comparison This project is released under the open-source MIT license, so you are welcome to use and modify it for free. (I imagine that this is not the case for many sleep analysis tools.) As noted earlier, the project allows you to see how your baby's longest daily sleep stretch has improved over time. (The longer your baby sleeps at any given point, the longer you get to sleep, so I think this metric is of great interest to most parents!) I don't think the free version of Huckleberry includes this data in line chart form, though you can get a sense of this improvement by scrolling through your daily sleep data. This script also separates individaul sleep entries into their respective daytime and nighttime components. For instance, if your baby slept from 6 AM to 9 AM, and you've specified the nighttime period to end at 7 AM (the default setting), the script will treat this entry as one hour of nighttime sleep and 2 hours of daytime sleep. I don't think Huckleberry offers this same functionality, though I could be wrong. (Note: The sample data shown within the project is completely made up using another Python script , and is not meant to reflect normal sleep patterns in infants.)"},
{"Title": "ChatGPT can talk with all my Python notebooks", "Author": "u/pp314159", "Content": "I'm working on an open-source framework for converting Python notebooks into web applications, it is called Mercury . Recently, I have added an option to execute notebooks with REST API. You can pass paramters in POST request body to the notebook, execute all cells and return JSON as response. I'm also running a SaaS service, Mercury Cloud where you can deploy notebooks with one-click with unique website domain. What My Project Does It makes Python notebooks extermely easy to integrate with custom ChatGPT, so GPT can execute Python notebooks and get response. It is 3-steps process: Create Python notebook, with Mercury widgets to accept parameters and return JSON response. Deploy notebook online - it can be done in 1 minute with Mercury Cloud Configure ChatGPT Actions - it is quick, because Mercury automatically generates OpenAPI schema based on your notebooks. You can read more in article how ChatGPT is talking with all my Python notebooks . Target Audience This solution is perfect for people: that would like to quickly create custom API for ChatGPT with Python, that don't want to manage server by themself. Comparison I think that building API with Python notebooks is alternative for full REST API development with Django, Flask or FastAPI, if you quickly need few endpoints that will expose your Python code. Examples I have created example notebooks that are used by ChatGPT: notebook to send email directly from ChatGPT notebook to query Postgres database with SQL from ChatGPT notebook to access Google Sheets in custom ChatGPT"},
{"Title": "Framework to use for backend", "Author": "u/Clickyz", "Content": "Hello guys I recently decided to move from nodejs(expressjs) to python for general purposes but mostly for backend. I have couple of questions. Will i regret my migration to python? :) Which framework you suggest for backend solo dev? And what tips are you suggesting me in general to get used to python."},
{"Title": "New Python-only abstractions for extracting data from apis", "Author": "u/Thinker_Assignment", "Content": "Hey there, you are probably familiar with REST APIs. We at dlt library added a new way to get data from apis (and dlt can already load it with best practice to db or parquet). We already did some internal hackathons but we would appreciate your feedback so we can improve it further - Our new REST API Source is a short, declarative configuration driven way of creating sources. - Our new REST API Client is a collection of Python helpers used by the above source, which you can also use as a standalone, config-free, imperative high-level abstraction for building pipelines. You can read more about the source here or go to our docs for the REST APIClient info https://github.com/dlt-hub/verified-sources/tree/master/sources/rest_api PS: see you at Pycon Pittsburgh!"},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Interactive Data Visualization with Python: A Showcase of Plotly Dash", "Author": "u/AbhishekSuryavanshee", "Content": "What My Project Does: I'm excited to introduce my latest project built with Python ‚Äì an interactive data visualization application using Plotly Dash. This project aims to empower users to explore and analyze datasets dynamically through interactive visualizations. By leveraging Plotly Dash's capabilities, users can interact with data in real-time, customize visualizations on the fly, and gain deeper insights with just a few clicks. Target Audience: This project caters to a wide range of users, from data enthusiasts and analysts to professionals seeking to communicate insights effectively. Whether you're a data scientist exploring patterns in large datasets or a business analyst presenting findings to stakeholders, this tool is designed to streamline your workflow and enhance your data storytelling capabilities. It's suitable for both production-grade applications and educational purposes, offering a versatile platform for data visualization tasks of varying complexity. Comparison: Unlike traditional static charts or cumbersome data exploration tools, this Plotly Dash application stands out for its interactivity and flexibility. While existing alternatives may offer basic charting functionalities, they often lack the dynamic capabilities required for in-depth data exploration. With Plotly Dash, users can manipulate charts in real-time, zoom in on specific data points, filter datasets dynamically, and even integrate interactive components like dropdowns and sliders for a more immersive experience. This project takes data visualization to the next level by providing a user-friendly interface coupled with powerful interactivity, setting it apart as a top choice for visualizing and analyzing datasets. Source Code: You can access the source code for this project on GitHub: Interactive Data Visualization with Plotly Dash Website: For more information and to see the project in action, visit : https://www.aspiresoftserv.com/"},
{"Title": "Declarative GUI Slint v1.6 released with Design Mode (WYSIWYG) Improvements", "Author": "u/madnirua", "Content": "https://slint.dev/blog/slint-1.6-released Slint is a declarative GUI toolkit to build native user interfaces for desktop and embedded applications. Find more information at https://slint.dev/ or check out the source code at https://github.com/slint-ui/slint . EDIT: The Python APIs are currently in alpha. More info -- https://github.com/slint-ui/slint/tree/master/api/python"},
{"Title": "Implementing your own pypi clone", "Author": "u/chione99", "Content": "Hi, Just want to know how difficult is it to manage your own pypi clone and how do you recommend to create a seperation between dev and prod systems."},
{"Title": "Manage Your Squid Proxy Server Efficiently with This Python Script", "Author": "u/SAV_NC", "Content": "ü¶ë Squid Proxy Manager Script Hello fellow Python enthusiasts! I've created a Python script that makes managing your Squid Proxy Server a breeze. If you're looking for an efficient and straightforward way to interact with your Squid server remotely, this script is for you. üéâ What My Project Does The Squid Proxy Manager script allows you to manage your Squid Proxy Server remotely using a simple command-line interface. Here are some of the key features: Check Squid Service Status : Quickly check if your Squid service is running or not. Start/Stop/Restart Service : Easily control the Squid service remotely. View Logs : Access the latest entries in your Squid access logs. View Configuration : Display the current Squid configuration file. Update Configuration : Replace the existing Squid configuration with a new one. Reload Service : Reload the Squid service to apply changes without restarting. Target Audience This script is designed for anyone who manages a Squid Proxy Server and prefers a command-line tool for remote management. If you are comfortable using Python and SSH, this tool will streamline your workflow and enhance your productivity. Differences Here are some aspects that make this Squid Proxy Manager script stand out: Remote Management : Manage your Squid server without needing physical access, thanks to SSH connectivity. Ease of Use : The script provides a simple and intuitive command-line interface, making it easy to perform various tasks. Comprehensive Features : From checking service status to updating configurations and viewing logs, this script covers all essential Squid management tasks. Error Handling and Logging : Detailed logging and error handling ensure you know exactly what's happening and can troubleshoot issues effectively. üöÄ Usage Installation : Ensure you have the required libraries installed: pip install paramiko termcolor Running the Script : Use the script with appropriate arguments to manage your Squid Proxy Server. Here's an example command to check the Squid service status: ./squid_proxy_manager.py 192.168.2.111 22 username password --check-status Updating Configuration : Create a new configuration file (e.g., new_squid.conf ) with your desired settings. Run the script to update the Squid configuration: ./squid_proxy_manager.py 192.168.2.111 22 username password --update-config new_squid.conf üíª Script Example Here's a snippet of the script to give you an idea of its simplicity and functionality: #!/usr/bin/env python3 import paramiko import argparse import logging import sys import os from termcolor import colored class SquidProxyManager: def __init__(self, hostname, port, username, password): self.hostname = hostname self.port = port self.username = username self.password = password self.client = paramiko.SSHClient() self.client.set_missing_host_key_policy(paramiko.AutoAddPolicy()) def connect(self): try: logging.info(colored(\"Attempting to connect to {}:{}\".format(self.hostname, self.port), 'cyan')) self.client.connect(self.hostname, port=self.port, username=self.username, password=self.password) logging.info(colored(f\"Connected to {self.hostname} on port {self.port}\", 'green')) except Exception as e: logging.error(colored(f\"Failed to connect: {e}\", 'red')) sys.exit(1) def disconnect(self): self.client.close() logging.info(colored(\"Disconnected from the server\", 'green')) def execute_command(self, command): logging.info(colored(\"Executing command: {}\".format(command), 'cyan')) try: stdin, stdout, stderr = self.client.exec_command(command) stdout.channel.recv_exit_status() out = stdout.read().decode() err = stderr.read().decode() if err: logging.error(colored(f\"Error executing command '{command}': {err}\", 'red')) else: logging.info(colored(f\"Successfully executed command '{command}'\", 'green')) return out, err except Exception as e: logging.error(colored(f\"Exception during command execution '{command}': {e}\", 'red')) return \"\", str(e) # More functions here... def parse_args(): parser = argparse.ArgumentParser(description=\"Squid Proxy Manager\") parser.add_argument('hostname', help=\"IP address of the Squid proxy server\") parser.add_argument('port', type=int, help=\"Port number for SSH connection\") parser.add_argument('username', help=\"SSH username\") parser.add_argument('password', help=\"SSH password\") parser.add_argument('--check-status', action='store_true', help=\"Check Squid service status\") parser.add.add_argument('--start', action='store_true', help=\"Start Squid service\") parser.add.add_argument('--stop', action='store_true', help=\"Stop Squid service\") parser.add.add_argument('--restart', action='store_true', help=\"Restart Squid service\") parser.add.add_argument('--view-logs', action='store_true', help=\"View Squid logs\") parser.add.add_argument('--view-config', action='store_true', help=\"View Squid configuration\") parser.add.add_argument('--update-config', help=\"Update Squid configuration with provided data\") parser.add.add_argument('--reload', action='store_true', help=\"Reload Squid service\") return parser.parse_args() def main(): logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') args = parse_args() logging.info(colored(\"Initializing Squid Proxy Manager script\", 'cyan')) manager = SquidProxyManager(args.hostname, args.port, args.username, args.password) manager.connect() try: if args.check_status: manager.check_squid_status() if args.start: manager.start_squid() if args.stop: manager.stop_squid() if args.restart: manager.restart_squid() if args.view_logs: manager.view_squid_logs() if args.view_config: manager.view_squid_config() if args.update_config: if not args.update_config.endswith('.conf'): logging.error(colored(\"The provided file must have a .conf extension\", 'red')) elif not os.path.isfile(args.update_config): logging.error(colored(f\"Configuration file {args.update_config} not found\", 'red')) else: try: with open(args.update_config, 'r') as config_file: config_data = config_file.read() manager.update_squid_config(config_data) except Exception as e: logging.error(colored(f\"Error reading configuration file {args.update_config}: {e}\", 'red')) if args.reload: manager.reload_squid() finally: manager.disconnect() logging.info(colored(\"Squid Proxy Manager operations completed\", 'green')) if __name__ == \"__main__\": main() üåü Benefits Remote Management : No need to be physically present to manage your Squid server. Ease of Use : Simple command-line interface for quick operations. Versatility : Supports various Squid management tasks, from checking status to updating configurations and viewing logs. üì¢ Get Involved! If you find this script useful, feel free to give it a try and share your feedback. Contributions and suggestions are always welcome! Comments however, that are unhelpful and serve no purpose to better the script or the author in their python scripting abilities are not welcome! Keep the nasty to yourself. Access the script You can find the script here on GitHub. Happy coding! üöÄ"},
{"Title": "modern_colorthief - Modified Median Cut Quantization algorithm in rust + python", "Author": "u/BasePlate_Admin", "Content": "github documentation What my project does : It gets the dominant color/color palette from given image. Target Audience: Anyone Usage modern_colorthief exposes two functions get_color and get_palette Here is how to use get_color: from modern_colorthief import get_color # Path to any image path = ... print(get_color(path)) # returns tuple[int,int,int] Here is how to use get_palette: from modern_colorthief import get_color # Path to any image path = ... print(get_palette(path)) # returns list[tuple[int,int,int]] Goals: Bring color-thief-rs to python Benchmarks: Written in deatils Gist: Python Took:            0.09976800000004005 CPP Took:               0.008461299999908078 RUST Took:              0.008549499994842336 Python Took:            0.0960583999985829 CPP Took:               0.008564600000681821 RUST Took:              0.007692700004554354 Differences With fast-colorthief Supports more architectures. ( pybind11 vs pyo3 ) Doesn't have a hard dependency on numpy Code is simple compared to fast-colorthief's CPP codebase Automated tooling powered by maturin and github-actions The size of fast-colorthief is 52kb-60kb. With color-thief-py Superior execution time (nearly 100x) Doesn't have a hard dependency on pillow color-thief's codebase is not in par with modern python versions If you like this project please star this repository"},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Frame - a new language for programming state machines in Python", "Author": "u/framelanger", "Content": "Hey, I am (re)releasing a project called Frame that I've been working on to create a language and transpiler to easily create state machines/automata in Python. It also is able to generate UML documentation as well. This project is for people who are interested in programming state machines for a wide range of purposes such as game programming, workflows, MBSE modeling as well as school projects for comp sci theory. It is also useful simply for generating flow documentation. The Framepiler (Frame transpiler) is in beta at this time. It would be great to get feedback from the Python community on any gaps in key functionality or bugs. Low-code/no-code workflow tools are often problematic for creating state machine like flows. Frame is intended to give a textual way to accomplish the same thing, but without having to \"draw\" your software and with the ability to use all the standard devops tooling and processes for \"normal\" development processes. There is also a VSCode extension and a playground environment to experiment in. Very much hoping to connect with people who might find this interesting and useful. If that is you, please take a look at the Overview and the Getting Started articles. Here is a link to the GitHub Framepiler Project as well. Please LMK if you have any questions or interest in the project. Thanks! Mark"},
{"Title": "SQLPage - a Python library to add string token based pagination easily", "Author": "u/kernelslayer", "Content": "What My Project Does  - This is a Python package to easily add string token based pagination. Currently it supports SQLModel and SQLAlchemy ORMs. Recently I wanted to add pagination in one of my Python projects and in the API response, I had to return a string next page token. Now I could not find a straight-forward way of doing this in Python. All of the tutorials or blog posts I saw, there in the response the server always returned a page_number , page_size , and total_elements and then the onus was on the calling service to adjust this accordingly. Comparison - The current packages and methods requires some changes in the app layer as well. I tried using a few but those did not satisfy the use case and were also a bit harder to implement. I could not find a easy to use option. The present ones returned integers instead of a string token I wanted it to be simpler, just like OpenSearch - you call its search API and it returns 10 elements and a next_page_token and then for the next 10 (or you configure this using the size parameter) you use the next_page_token in the subsequent request to get to the new page. I ended up doing a lot of if-else checks and encoding and decoding, so I decided to create this library. Target Audience - This is production ready, have been using it in one of my projects. Hope some of you folks find it useful :) Here is the link to the PyPi repository and here is the GitHub repo"},
{"Title": "UXsim 1.3.0 released with vehicle tracking and improved vehicle routing", "Author": "u/Balance-", "Content": "Main Changes Add GUI functions Vehicle tracking: You can now track a specific vehicle to see their route Dataframe viewer: Stats can be confirmed Improve vehicle routing functions Add example of routing optimization Change documentation 's theme for better indexing UXsim UXsim is a free, open-source macroscopic and mesoscopic network traffic flow simulator written in Python. It simulates the movements of car travelers and traffic congestion in road networks. It is suitable for simulating large-scale (e.g., city-scale) traffic phenomena. UXsim is especially useful for scientific and educational purposes because of its simple, lightweight, and customizable features, but users are free to use UXsim for any purpose."},
{"Title": "Resume Screening Chatbot using RAG Fusion", "Author": "u/Babe_My_Name_Is_Hung", "Content": "Hi everyone! I recently finished a small side project for my graduating thesis, which is about experimenting with RAG-based frameworks in improving resume screening. What my project does: The project for the thesis is a GPT-4 Chatbot with RAG Fusion retrieval. Given a job description as input, the system retrieves the most relevant candidate profiles to perform follow-up tasks such as analysis, summarization, and decision-making, which can assist the screening process better. The revolving idea is that the similarity-based retrieval process can effectively narrow the initial large pool of applicants down to the most relevant resumes. However, this simple similarity ranking should not be used to evaluate a candidate's actual ability. Therefore, the top resumes are used to augment the GPT-4 Chatbot so it can be conditioned on these profiles and perform further downstream tasks. Target audience: The repo contains the link to my paper and the notebooks that were used to design the prototype program and conduct some experiments. For the newcomers to RAG/RAG Fusion, or people who are just interested in building a RAG-based chatbots, this can be especially helpful. Feel free to check them out too! Comparison: I'm not sure if there's any similar project out there, but the program is sort of designed to move the resume screening process away from existing keyword-based methods. It's much more versatile in use cases and also more effective in handling resumes. The project is very far from being perfect. Because of that, I share this with the hope to receive suggestions and feedback from you. If you have time, please give the project a visit here: GitHub"},
{"Title": "Giving New Life to JModelica: Bringing Powerful Modelica Simulations to Python", "Author": "u/foadsf", "Content": "Five years ago, I posted about JModelica, a fantastic open-source tool for simulating complex systems that combined the ease of Python with the strength of Modelica. Sadly, the project went quiet, but I'm thrilled to share that, thanks to the dedication of a few folks (myself included!), JModelica is back! You can find the revived project on GitHub: https://github.com/JModelica/JModelica . What JModelica Does: JModelica provides a way to write complex simulations using the Modelica language, which is known for its ability to handle differential equations and model physical systems beautifully. The magic of JModelica lies in its Python integration‚Äîyou can solve your Modelica models and access the results directly in Python for in-depth analysis, visualization, and even optimization using libraries you already love! Target Audience: This project is geared toward anyone interested in modeling and simulating complex systems, particularly those with a background in engineering, physics, or related fields. If you've struggled with Python's ODE solvers or wish for a more elegant way to model physical interactions, JModelica offers a compelling solution. It's ready for research, educational projects, and even more ambitious endeavors! Comparison: JModelica stands alongside OpenModelica as a champion of open-source Modelica tools. While OpenModelica is known for its user-friendly graphical interface, JModelica shines in its seamless integration with Python, giving you the best of both worlds! It's a powerful alternative to proprietary software like Simulink, providing transparency, flexibility, and a thriving community. We're actively working on squashing bugs, adding features, and making JModelica more accessible across different platforms (Windows and macOS support are on the horizon!). Anyone interested in contributing is welcome! Whether you're a Modelica expert or a curious newcomer, this project has a place for you. Check out the GitHub repository to explore the code, open issues, or submit pull requests."},
{"Title": "CLI to embed code snippets in your README, from actual (testable) code", "Author": "u/realazthat", "Content": "What My Project Does What My Project Does: snipinator is a CLI to embed (testable) snippets from your codebase into your README, using Jinja2 and functions provided by snipinator to assist with embedding code, shell output, etc. Please provide any feedback in the comments or GH issues. Target Audience Target Audience: Developers of {GitHub,other} projects that have a README. It works for me, it might work for you. Comparison Features: Supports anything Jinja2 supports. First-class support for python source code. Can include python function signatures, docstrings, entire function source code, classes. Snip from any source code language . Put delimiter markers into the code (e.g # START_SNIPPET , # END_TEMPLATE ), and use snippet() . First-class support for Markdown templates (with backtickify , decomentify ). Can include shell output . Supports ANSI colors with SVG output. More robust references/links to local files using path() . I keep a table of similar projects in my README at realazthat/snipinator: Related Projects . Not complete, and not necessarily up to date. Make a PR to README.md.jinja , (see realazthat/snipinator/Contributions ) to insert/modify the table. Project Stars Last Update Language Platform Similarity X Obviousness mdx-js/mdx 16.8k 2024/04/17 JS N/A ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê zakhenry/embedme 222 2023/11/08 JS N/A ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê cmacmackin/markdown-include 95 2023/02/07 Python N/A ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê BurdetteLamar/markdown_helper 38 2020/03/16 Ruby N/A ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê SimonCropp/MarkdownSnippets 23 2024/04/23 .NET N/A ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê endocode/snippetextractor 4 2014/08/16 C++ N/A ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê polywrap/doc-snippets 3 2023/09/26 JS N/A ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê JulianCataldo/remark-embed 2 2022/09/22 JS N/A ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê xrd/oreilly-snippets 2 2015/10/15 Ruby N/A ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê DamonOehlman/injectcode 1 2021/08/01 JS N/A ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê electrovir/markdown-code-example-inserter 1 2024/02/19 JS N/A ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê andersfischernielsen/Simple-Embedded-Markdown-Code-Snippets 1 2021/02/12 JS N/A ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê ildar-shaimordanov/git-markdown-snippet 0 2021/09/14 Perl N/A ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê teyc/markdown-snippet 0 2024/01/22 Powershell N/A ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê marc-bouvier-graveyard/baldir_markdown 0 2020/06/15 Python N/A ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê dineshsonachalam/markdown-autodocs 176 2022/09/19 JS GH Action ‚≠ê‚≠ê‚≠ê‚≠ê tokusumi/markdown-embed-code 28 2022/01/05 Python GH Action ‚≠ê‚≠ê‚≠ê‚≠ê sammndhr/gridsome-remark-embed-snippet 2 2021/06/14 JS Gridsome ‚≠ê‚≠ê‚≠ê‚≠ê NativeScript/markdown-snippet-injector 4 2019/01/24 JS N/A ‚≠ê‚≠ê‚≠ê‚≠ê fuxingloh/remark-code-import-replace 0 2022/12/21 JS Remark? ‚≠ê‚≠ê‚≠ê‚≠ê szkiba/mdcode 15 2014/02/12 Go N/A ‚≠ê‚≠ê‚≠ê devincornell/pymddoc 0 2023/12/01 Python Python ‚≠ê‚≠ê‚≠ê shiftkey/scribble ( docs ) 40 2013/08/08 .NET N/A ‚≠ê‚≠ê calebpeterson/jest-transformer-test-md 2 2020/08/21 JS Jest Tests ‚≠ê‚≠ê tjstankus/commitate 0 2014/05/29 Ruby N/A ‚≠ê GitHub Docs: Creating a permanent link to a code snippet N/A N/A N/A N/A ‚≠ê javierfernandes/markdown-exercises 1 2017/05/01 JS N/A ‚≠ê gatsby-remark-embed-snippet N/A (55k) 2024/01/23 JS Gatsby ‚≠ê ARMmbed/snippet 6 2021/08/05 Python N/A ? drewavis/markdowninclude 1 2024/04/06 JS VSCode Extension ? romnn/embedme 0 2024/04/18 Go N/A ? The 5 star projects have the bare minimum of being able to embed a file, and run via CLI. Snipinator does have other features (such as shell() ), implemented as I needed them (and listed below) which I do not think any of these have in combination. Some of these projects are not CLIs. mdx-js/mdx is the closest in terms of flexibility, but it is JS + components, which may not be everyone's cup of tea. Usage: Example template README: (./snipinator/examples/EXAMPLE.md.jinja2): # A README Here is a code snippet: <!--{{ pysnippet(path='snipinator/examples/code.py', symbol='MyClass', backtickify='py', decomentify='nl') }}--> Note that `code.py` has a test: {{path('./snipinator/examples/code_test.py', link='md')}}. Generating the README: $ python -m snipinator.cli -t snipinator/examples/EXAMPLE.md.jinja2 <!-- WARNING: This file is auto-generated by snipinator. Do not edit directly. SOURCE: `snipinator/examples/EXAMPLE.md.jinja2`. --> # A README Here is a code snippet: <!----> ```py class MyClass: \"\"\"This is a global class\"\"\" def __init__(self, name): self.name = name def MyClassMethod(self): \"\"\"This is a method of MyClass\"\"\" print(self.name) ``` <!----> Note that `code.py` has a test: [./snipinator/examples/code_test.py](./snipinator/examples/code_test.py)."},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "map_plotter - abstracts complexity of creating intensity plots overlaid onto global map", "Author": "u/teamamentum", "Content": "What My Project Does Overlaying intensity plots onto a geographical map using cartopy/matplotlib can be complex. So we created this map_plotter package to abstract away that complexity for a common use case. Installation (opinionated use of conda to avoid cartopy dependency hell and install precompiled binaries) conda install cartopy git clone git@github.com:amentumspace/map_plotter.git cd map_plotter pip install . Usage import map_plotter map_plotter.plot(lons_g, lats_g, variable, units=\"m/s\", img_name=\"image.png\", save=True, plot=True, title=\"something\", zlims=[0,10]) Whereby: lons_g and lats_g represent 2D matrices / grids of longitudes and latitudes. values is the matrix of values to be plotted (same grid dimensions). units and img_name (self explanatory). save & plot boolean flags to save the file and plot to screen, respectively. zlims define the color scale minimum and maximum. Target Audience Python developers or data scientists or scientists or any Pythonista wanting a simple way to quickly plot an intensity map onto a geographical map. Comparison Differs from using cartopy and matplotlib in its ease-of-use, but it is less customisable (can't change projections, colors). Regardless, it's convenient and at least provides a starting point for customisation. Similar functionality can be had from geopandas or folium (although cartopy/matplotlib suited our needs better)."},
{"Title": "2,000 lines of Python code to make this scrolling ASCII art animation: \"The Forbidden Zone\"", "Author": "u/AlSweigart", "Content": "What My Project Does This is a music video of the output of a Python program: https://www.youtube.com/watch?v=Sjk4UMpJqVs I'm the author of Automate the Boring Stuff with Python and I teach people to code. As part of that, I created something I call \"scroll art\". Scroll art is a program that prints text from a loop, eventually filling the screen and causing the text to scroll up. (Something like those BASIC programs that are 10 PRINT \"HELLO\"; 20 GOTO 10) Once printed, text cannot be erased, it can only be scrolled up. It's an easy and artistic way for beginners to get into coding, but it's surprising how sophisticated they can become. The source code for this animation is here: https://github.com/asweigart/scrollart/blob/main/python/forbiddenzone.py (read the comments at the top to figure out how to run it with the forbiddenzonecontrol.py program which is also in that repo) The output text is procedurally generated from random numbers, so like a lava lamp, it is unpredictable and never exactly the same twice. This video is a collection of scroll art to the music of \"The Forbidden Zone,\" which was released in 1980 by the band Oingo Boingo, led by Danny Elfman (known for composing the theme song to The Simpsons.) It was used in a cult classic movie of the same name, but also the intro for the short-run Dilbert animated series. Target Audience Anyone (including beginners) who wants ideas for creating generative art without needing to know a ton of math or graphics concepts. You can make scroll art with print() and loops and random numbers. But there's a surprising amount of sophistication you can put into these programs as well. Comparison Because it's just text, scroll art doesn't have such a high barrier to entry compared with many computer graphics and generative artwork. The constraints lower expectations and encourage creativity within a simple context. I've produced scroll art examples on https://scrollart.org I also gave a talk on scroll art at PyTexas 2024: https://www.youtube.com/watch?v=SyKUBXJLL50"},
{"Title": "Reviewing Dataframe Changes? Looking for Your Preferred Methods!", "Author": "u/rageagainistjg", "Content": "After playing around with a dataframe‚Äîapplying filters or other transformations‚ÄîI'm curious about your methods for reviewing the changes. In VS Code, the variable explorer is quite handy for a quick look at the modified dataframe. Alternatively, when working in a Jupyter notebook within VS Code, exporting the data to an Excel file provides a detailed view and allows for an easy deep dive into the results. What are your preferred practices for ensuring your data adjustments are precisely what you intended?"},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Library for automatic Cython 3.0 code annotations generation.", "Author": "u/Cool-Nefariousness76", "Content": "Hi everybody, over the last year I've been developing a library that adds some Cython 3.0 annotations to existing python code. What My Project Does: For example if it sees a for i in range(): in a function it recognizes i as an integer and adds a i = cython.declare(cython.int) line at the beginning of the function. It actually uses the built-in ast module under the hood for parsing, I found it a super useful library! Target Audience: It is a side project I made mainly for fun. I don't know if it can be of interest to anybody, or if it could have some potential utility. Comparison: I did not find anything similar. There are a lot of very cool projects like mypyc for example, but nothing that does this tiny little code generation specific to Cython. The link to the repository is here: https://github.com/nucccc/markarth"},
{"Title": "APScheduler vs Schedule package", "Author": "u/kostakos14", "Content": "Hey folks, looking to use one library to implement some background scheduling logic on my application. I find in Google search APScheduler to be frequently mentioned, but I can see the Schedule package has more GH stars. Was curious if anybody has used one of them, and which one would you recommend based on your own experience."},
{"Title": "Introducing Notolog: Python Markdown Editor built with PySide6", "Author": "u/devotaku", "Content": "Excited to share my personal open-source project: Notolog - Python Markdown Editor (MIT License). The main motivation for developing another markdown editor was my passion for learning new things and enhancing my development skills in Python. I developed it in my spare time over a few months, despite having no prior experience in creating full-scale Python applications. What My Project Does ‚àó Multiplatform ‚àó Markdown async syntax highlighting created by me ‚àó Several pre-installed color themes ‚àó Supports English and 17 other languages right out of the box ‚àó Integration with OpenAI API for AI-assisted features ‚àó Optional file encryption/decryption Target Audience Primarily developers who write markdown documents and notes. Comparison This is more of a personal learning project, so it's hard to compare it directly with others. How to install Discover Notolog on GitHub üåü and PyPI . Installation is as easy as running a single command: pip install notolog"},
{"Title": "I've developed a library for send metrics to zabbix asynchronously", "Author": "u/Royal_Section4889", "Content": "I have been using zabbix for monitoring a lot of metrics in my work, none of the most popular zabbix were capable of doing async tasks, so I've developed some simple package capable of doing this. Tests, examples and how-tos can be found here: https://github.com/gustavofbreunig/zabbix-sender-async What My Project Does Send zabbix sender messages using asyncio tasks. Target Audience SysAdmins who use Zabbix to monitor a large number of metrics. Comparison Instead of doing traditional way, using these abandoned library: https://github.com/adubkov/py-zabbix from pyzabbix import ZabbixMetric, ZabbixSender # Send metrics to zabbix trapper packet = [ ZabbixMetric('hostname1', 'test[cpu_usage]', 2), ZabbixMetric('hostname1', 'test[system_status]', \"OK\"), ZabbixMetric('hostname1', 'test[disk_io]', '0.1'), ZabbixMetric('hostname1', 'test[cpu_usage]', 20, 1411598020), ] result = ZabbixSender(use_config=True).send(packet) You can do this: async def sendmetrics(): sender = AsyncSender('localhost', 10051) metric = ItemData(host='hostname', key='test.metric.text', value='test package import') result = await sender.send(metric)"},
{"Title": "I made a Python text to speech library - Pyt2s", "Author": "u/asksumanth", "Content": "What my project does : It supports services like IBM Watson, Acapela and Stream labs' demo websites to convert your text to speech. Target audience : It's a toy project and would not recommend you to use in Production. Comparison : It's wayyyyy easy to use. Just pip install and use in your project. No extra setup required like other libraries. Also supports various languages and voices and accents. Check docs for more. Here is the link to repository. Please go do check it out and star it if it's helpful to you guys. Thank you. I made this library taking inspiration from this php tts library by chrisjp."},
{"Title": "Interactive plots in the terminal", "Author": "u/Spiffidimus", "Content": "I made a library to create interactive plots in the terminal ( pip install itrm ). It uses braille characters (by default) to display the data with sub-character resolution. There are several keybindings for moving a vertical cursor left and right, for zooming in or out on data, and for changing which curve to focus on. There are occasions (such as when working with a server) where MatPlotLib is not an option and the terminal is the only available tool. But, in my opinion, it is actually faster to use this tool (itrm) to zoom in on interesting parts of data and analyze patterns than using other tools like MatPlotLib. In fact, with large data sets (~1 million points), this tool actually renders faster than MatPlotLib. Please check it out and let know what you think."},
{"Title": "Hi! I've published a Python client for IBKR REST and WebSocket APIs - IBind. Hope you like it üëã", "Author": "u/VoyZan", "Content": "Hi! I want to share a library I've built recently. IBind is a REST and WebSocket Python client for Interactive Brokers Client Portal Web API . It is directed at IBKR users. You can find IBind on GitHub: https://github.com/Voyz/ibind What My Project Does: It is a REST and WebSocket API for the Interactive Brokers' Web API. I'm particularly proud of a few things in this release: The REST and WebSocket API clients are based on an abstract base class RestClient and WsClient accordingly. These could be implemented to use some other Web APIs in a relatively straightforward way. I have in fact used a version of that WsClient for a cryptocurrency WebSocket API, and it is nice to see it adapt to a different environment. I've covered most of the codebase with automated tests (appx 80%). Contrary to some of my other libraries, these are mainly integration tests which feel to provide a stronger test coverage than only unit tests. I've learned how to use class mixins in this project, and it aids the maintainability by a lot! The REST client itself is pretty barebone, but has a lot of mixin classes - all corresponding to the endpoint categories the broker uses, making it easy to search for the right piece of code and documentation. There's a lot of things that make this client as plug-and-play as possible. The broker requires the user to specify a bunch of things - account ids, certificates, URLs, etc. - which the class either reads from the environment variables or assumes (given that some things would be common for most users). In either case, all these are customisable by parameters if needed, but it is nice to just write client = IbkrClient() in various projects having set just a couple of env vars. I think the documentation is pretty in-depth but readable. It's always hard to judge whether docs are well written, but I think it is nicely broken down. Also, I managed to use pydoc-markdown package to create API reference in markdown, which works nicely with the GitHub Wiki. I'd prefer it to be even easier, but compared to Sphinx and readthedocs it's a much quicker job. The WebSocket class does a ton to keep the connection alive and recover from connection losses. Maintaining active subscriptions after a re-connect can be a real pain, and I think this class does it in a nice and reliable way. I've tested it for various types of connectivity loss, and it manages to recover and re-establish the WebSocket data stream. Pretty crucial in the trading environment. I made a nice logo for it ü•≥ Target Audience: Traders using IBKR who want to automate their trading through this Web API. Comparison (A brief comparison explaining how it differs from existing alternatives.) : There are two similar libraries that I know of. They aren't bad, but seem not very well maintained and incomplete: https://github.com/areed1192/interactive-brokers-api - outdated and stale, last update 3 years ago https://github.com/utilmon/EasyIB - stale and incomplete The library I've published covers a much wider range of endpoints, adds WebSocket support and a bunch of wrapper methods to simplify the usage of the API. IBind has a bunch of features that make using the IBKR APIs much easier. Some of these are: REST: Automated question/answer handling - streamlining placing orders. Parallel requests - speeding up collection of price data. Rate limiting - guarding against account bans. Conid unpacking - helping to find the right contract. WebSocket: WebSocket thread lifecycle handling - ensuring the connection is alive. Thread-safe Queue data stream - exposing the collected data in a safe way. Internal subscription tracking - recreating subscriptions upon re-connections. Health monitoring - Acting on unusual ping or heartbeat. REST Example: from ibind import IbkrClient # Construct the client client = IbkrClient() print(client.tickle().data) WebSocket Example: from ibind import IbkrWsKey, IbkrWsClient # Construct the client. ws_client = IbkrWsClient(start=True) # Choose the WebSocket channel key = IbkrWsKey.PNL # Subscribe to the PNL channel ws_client.subscribe(channel=key.channel) print(ws_client.get(key)) I just wanted to share my experience of publishing Open Source. For some reason I get a lot of motivation when I can publish code that makes peoples' lives easier. The library could use some code review on it, so if you‚Äôd feel like reading some code and helping out - drop me a message. Other than that, happy to answer any questions, and - if you are an algo trader - let me know if you get a chance to use it. Thanks for reading!"},
{"Title": "Python Streamlit Spotlight Tutorial: an Interactive Dashboard using UNHCR Refugee Data", "Author": "u/jgloewen", "Content": "Python Streamlit is a terrific tool for creating interactive data visualizations. It packages all your visualizations up into a neat little application - including charts and maps - and displays them in your default browser. No muss, no fuss. Recently, I found a new dataset (to me) on the UN High Commission for Refugees (UNHCR) website. It contains country-to-country movements for refugees both from origin country and country of asylum Using this dataset, here's a step-by-step on how to code a Python Streamlit application that has: A dropdown menu to select by country A second dropdown menu to select by year Radio buttons (2) to select country of origin or county of asylum A global choropleth map to display the results by country and year. Free article HERE ."},
{"Title": "IP subnet or IP calculator tool need feedback", "Author": "u/nicanorflavier", "Content": "Hey folks, I've been dabbling with a Python project recently that's all about making life easier for us I.T. people. It's a nifty little tool that calculates IP subnets and does IP calculations from the command or CLI. Here's the GitHub link and the code: https://github.com/nicanorflavier/ipnet I‚Äôm pretty stoked about it, but I know there‚Äôs always room for improvement. So, I thought, better to turn to than the wise minds of this python community? I‚Äôm all ears for any feedback, tips, tricks, or advice you guys might have. Thanks a ton in advance!"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Pre-commit hook to keep coverage badge in README up to date", "Author": "u/60percentcocoa", "Content": "Wrote this as a tool to keep README coverage badges up to date without relying on 3rd party services or having to do anything extra, thought others might get some utility out of it: coverage-pre-commit . A .coverage file is expected at the root of the project, generated by running coverage run directly or using a plugin such as pytest-cov when running tests. Most convenient when used as a pre-push hook imo. Feel free to opine, be it positive or negative!"},
{"Title": "PyWolt: Wolt food delivery service API wrapper", "Author": "u/daivushe1", "Content": "I'm thrilled to share my first open-source project with you all: PyWolt ! üéâ PyWolt is a Python library that makes it super easy to interact with the Wolt API. What My Project Does: Discover Venues: Find nearby spots to grab a bite. Explore Menus: Dive into a venue's menu and pick your favorites. Target Audience: Software Engineers : Professionals who build web or mobile applications, particularly those in the food delivery or restaurant industry, looking to incorporate Wolt's services seamlessly into their platforms. Data Scientists/Analysts : Individuals analyzing food delivery data, consumer behavior, or market trends, who may utilize PyWolt to gather data from Wolt's API for analysis and insights. Students/Learners : Those studying Python programming, web development, or API integration, who can use PyWolt as a practical example or learning tool to understand how to interact with RESTful APIs in Python. Freelancers/Entrepreneurs : Independent developers or startup founders looking to build new food-related applications or services leveraging Wolt's platform without reinventing the wheel. Comparison: woltcheck : only offers a script to check if a wolt restaurant is ready to deliver to your location. what-to-eat : a pretty neat cli tool that offers all of pywolt's functionality. In my opinion it overcomplicates things a little, and doesn't offer straight-forward RESTful functionality to interact with the API itself."},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Frame - a new language for programming state machines in Python", "Author": "u/framelanger", "Content": "Hey, I am (re)releasing a project called Frame that I've been working on to create a language and transpiler to easily create state machines/automata in Python. It also is able to generate UML documentation as well. This project is for people who are interested in programming state machines for a wide range of purposes such as game programming, workflows, MBSE modeling as well as school projects for comp sci theory. It is also useful simply for generating flow documentation. The Framepiler (Frame transpiler) is in beta at this time. It would be great to get feedback from the Python community on any gaps in key functionality or bugs. Low-code/no-code workflow tools are often problematic for creating state machine like flows. Frame is intended to give a textual way to accomplish the same thing, but without having to \"draw\" your software and with the ability to use all the standard devops tooling and processes for \"normal\" development processes. There is also a VSCode extension and a playground environment to experiment in. Very much hoping to connect with people who might find this interesting and useful. If that is you, please take a look at the Overview and the Getting Started articles. Here is a link to the GitHub Framepiler Project as well. Please LMK if you have any questions or interest in the project. Thanks! Mark"},
{"Title": "SQLPage - a Python library to add string token based pagination easily", "Author": "u/kernelslayer", "Content": "What My Project Does  - This is a Python package to easily add string token based pagination. Currently it supports SQLModel and SQLAlchemy ORMs. Recently I wanted to add pagination in one of my Python projects and in the API response, I had to return a string next page token. Now I could not find a straight-forward way of doing this in Python. All of the tutorials or blog posts I saw, there in the response the server always returned a page_number , page_size , and total_elements and then the onus was on the calling service to adjust this accordingly. Comparison - The current packages and methods requires some changes in the app layer as well. I tried using a few but those did not satisfy the use case and were also a bit harder to implement. I could not find a easy to use option. The present ones returned integers instead of a string token I wanted it to be simpler, just like OpenSearch - you call its search API and it returns 10 elements and a next_page_token and then for the next 10 (or you configure this using the size parameter) you use the next_page_token in the subsequent request to get to the new page. I ended up doing a lot of if-else checks and encoding and decoding, so I decided to create this library. Target Audience - This is production ready, have been using it in one of my projects. Hope some of you folks find it useful :) Here is the link to the PyPi repository and here is the GitHub repo"},
{"Title": "UXsim 1.3.0 released with vehicle tracking and improved vehicle routing", "Author": "u/Balance-", "Content": "Main Changes Add GUI functions Vehicle tracking: You can now track a specific vehicle to see their route Dataframe viewer: Stats can be confirmed Improve vehicle routing functions Add example of routing optimization Change documentation 's theme for better indexing UXsim UXsim is a free, open-source macroscopic and mesoscopic network traffic flow simulator written in Python. It simulates the movements of car travelers and traffic congestion in road networks. It is suitable for simulating large-scale (e.g., city-scale) traffic phenomena. UXsim is especially useful for scientific and educational purposes because of its simple, lightweight, and customizable features, but users are free to use UXsim for any purpose."},
{"Title": "Resume Screening Chatbot using RAG Fusion", "Author": "u/Babe_My_Name_Is_Hung", "Content": "Hi everyone! I recently finished a small side project for my graduating thesis, which is about experimenting with RAG-based frameworks in improving resume screening. What my project does: The project for the thesis is a GPT-4 Chatbot with RAG Fusion retrieval. Given a job description as input, the system retrieves the most relevant candidate profiles to perform follow-up tasks such as analysis, summarization, and decision-making, which can assist the screening process better. The revolving idea is that the similarity-based retrieval process can effectively narrow the initial large pool of applicants down to the most relevant resumes. However, this simple similarity ranking should not be used to evaluate a candidate's actual ability. Therefore, the top resumes are used to augment the GPT-4 Chatbot so it can be conditioned on these profiles and perform further downstream tasks. Target audience: The repo contains the link to my paper and the notebooks that were used to design the prototype program and conduct some experiments. For the newcomers to RAG/RAG Fusion, or people who are just interested in building a RAG-based chatbots, this can be especially helpful. Feel free to check them out too! Comparison: I'm not sure if there's any similar project out there, but the program is sort of designed to move the resume screening process away from existing keyword-based methods. It's much more versatile in use cases and also more effective in handling resumes. The project is very far from being perfect. Because of that, I share this with the hope to receive suggestions and feedback from you. If you have time, please give the project a visit here: GitHub"},
{"Title": "Giving New Life to JModelica: Bringing Powerful Modelica Simulations to Python", "Author": "u/foadsf", "Content": "Five years ago, I posted about JModelica, a fantastic open-source tool for simulating complex systems that combined the ease of Python with the strength of Modelica. Sadly, the project went quiet, but I'm thrilled to share that, thanks to the dedication of a few folks (myself included!), JModelica is back! You can find the revived project on GitHub: https://github.com/JModelica/JModelica . What JModelica Does: JModelica provides a way to write complex simulations using the Modelica language, which is known for its ability to handle differential equations and model physical systems beautifully. The magic of JModelica lies in its Python integration‚Äîyou can solve your Modelica models and access the results directly in Python for in-depth analysis, visualization, and even optimization using libraries you already love! Target Audience: This project is geared toward anyone interested in modeling and simulating complex systems, particularly those with a background in engineering, physics, or related fields. If you've struggled with Python's ODE solvers or wish for a more elegant way to model physical interactions, JModelica offers a compelling solution. It's ready for research, educational projects, and even more ambitious endeavors! Comparison: JModelica stands alongside OpenModelica as a champion of open-source Modelica tools. While OpenModelica is known for its user-friendly graphical interface, JModelica shines in its seamless integration with Python, giving you the best of both worlds! It's a powerful alternative to proprietary software like Simulink, providing transparency, flexibility, and a thriving community. We're actively working on squashing bugs, adding features, and making JModelica more accessible across different platforms (Windows and macOS support are on the horizon!). Anyone interested in contributing is welcome! Whether you're a Modelica expert or a curious newcomer, this project has a place for you. Check out the GitHub repository to explore the code, open issues, or submit pull requests."},
{"Title": "CLI to embed code snippets in your README, from actual (testable) code", "Author": "u/realazthat", "Content": "What My Project Does What My Project Does: snipinator is a CLI to embed (testable) snippets from your codebase into your README, using Jinja2 and functions provided by snipinator to assist with embedding code, shell output, etc. Please provide any feedback in the comments or GH issues. Target Audience Target Audience: Developers of {GitHub,other} projects that have a README. It works for me, it might work for you. Comparison Features: Supports anything Jinja2 supports. First-class support for python source code. Can include python function signatures, docstrings, entire function source code, classes. Snip from any source code language . Put delimiter markers into the code (e.g # START_SNIPPET , # END_TEMPLATE ), and use snippet() . First-class support for Markdown templates (with backtickify , decomentify ). Can include shell output . Supports ANSI colors with SVG output. More robust references/links to local files using path() . I keep a table of similar projects in my README at realazthat/snipinator: Related Projects . Not complete, and not necessarily up to date. Make a PR to README.md.jinja , (see realazthat/snipinator/Contributions ) to insert/modify the table. Project Stars Last Update Language Platform Similarity X Obviousness mdx-js/mdx 16.8k 2024/04/17 JS N/A ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê zakhenry/embedme 222 2023/11/08 JS N/A ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê cmacmackin/markdown-include 95 2023/02/07 Python N/A ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê BurdetteLamar/markdown_helper 38 2020/03/16 Ruby N/A ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê SimonCropp/MarkdownSnippets 23 2024/04/23 .NET N/A ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê endocode/snippetextractor 4 2014/08/16 C++ N/A ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê polywrap/doc-snippets 3 2023/09/26 JS N/A ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê JulianCataldo/remark-embed 2 2022/09/22 JS N/A ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê xrd/oreilly-snippets 2 2015/10/15 Ruby N/A ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê DamonOehlman/injectcode 1 2021/08/01 JS N/A ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê electrovir/markdown-code-example-inserter 1 2024/02/19 JS N/A ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê andersfischernielsen/Simple-Embedded-Markdown-Code-Snippets 1 2021/02/12 JS N/A ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê ildar-shaimordanov/git-markdown-snippet 0 2021/09/14 Perl N/A ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê teyc/markdown-snippet 0 2024/01/22 Powershell N/A ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê marc-bouvier-graveyard/baldir_markdown 0 2020/06/15 Python N/A ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê dineshsonachalam/markdown-autodocs 176 2022/09/19 JS GH Action ‚≠ê‚≠ê‚≠ê‚≠ê tokusumi/markdown-embed-code 28 2022/01/05 Python GH Action ‚≠ê‚≠ê‚≠ê‚≠ê sammndhr/gridsome-remark-embed-snippet 2 2021/06/14 JS Gridsome ‚≠ê‚≠ê‚≠ê‚≠ê NativeScript/markdown-snippet-injector 4 2019/01/24 JS N/A ‚≠ê‚≠ê‚≠ê‚≠ê fuxingloh/remark-code-import-replace 0 2022/12/21 JS Remark? ‚≠ê‚≠ê‚≠ê‚≠ê szkiba/mdcode 15 2014/02/12 Go N/A ‚≠ê‚≠ê‚≠ê devincornell/pymddoc 0 2023/12/01 Python Python ‚≠ê‚≠ê‚≠ê shiftkey/scribble ( docs ) 40 2013/08/08 .NET N/A ‚≠ê‚≠ê calebpeterson/jest-transformer-test-md 2 2020/08/21 JS Jest Tests ‚≠ê‚≠ê tjstankus/commitate 0 2014/05/29 Ruby N/A ‚≠ê GitHub Docs: Creating a permanent link to a code snippet N/A N/A N/A N/A ‚≠ê javierfernandes/markdown-exercises 1 2017/05/01 JS N/A ‚≠ê gatsby-remark-embed-snippet N/A (55k) 2024/01/23 JS Gatsby ‚≠ê ARMmbed/snippet 6 2021/08/05 Python N/A ? drewavis/markdowninclude 1 2024/04/06 JS VSCode Extension ? romnn/embedme 0 2024/04/18 Go N/A ? The 5 star projects have the bare minimum of being able to embed a file, and run via CLI. Snipinator does have other features (such as shell() ), implemented as I needed them (and listed below) which I do not think any of these have in combination. Some of these projects are not CLIs. mdx-js/mdx is the closest in terms of flexibility, but it is JS + components, which may not be everyone's cup of tea. Usage: Example template README: (./snipinator/examples/EXAMPLE.md.jinja2): # A README Here is a code snippet: <!--{{ pysnippet(path='snipinator/examples/code.py', symbol='MyClass', backtickify='py', decomentify='nl') }}--> Note that `code.py` has a test: {{path('./snipinator/examples/code_test.py', link='md')}}. Generating the README: $ python -m snipinator.cli -t snipinator/examples/EXAMPLE.md.jinja2 <!-- WARNING: This file is auto-generated by snipinator. Do not edit directly. SOURCE: `snipinator/examples/EXAMPLE.md.jinja2`. --> # A README Here is a code snippet: <!----> ```py class MyClass: \"\"\"This is a global class\"\"\" def __init__(self, name): self.name = name def MyClassMethod(self): \"\"\"This is a method of MyClass\"\"\" print(self.name) ``` <!----> Note that `code.py` has a test: [./snipinator/examples/code_test.py](./snipinator/examples/code_test.py)."},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "map_plotter - abstracts complexity of creating intensity plots overlaid onto global map", "Author": "u/teamamentum", "Content": "What My Project Does Overlaying intensity plots onto a geographical map using cartopy/matplotlib can be complex. So we created this map_plotter package to abstract away that complexity for a common use case. Installation (opinionated use of conda to avoid cartopy dependency hell and install precompiled binaries) conda install cartopy git clone git@github.com:amentumspace/map_plotter.git cd map_plotter pip install . Usage import map_plotter map_plotter.plot(lons_g, lats_g, variable, units=\"m/s\", img_name=\"image.png\", save=True, plot=True, title=\"something\", zlims=[0,10]) Whereby: lons_g and lats_g represent 2D matrices / grids of longitudes and latitudes. values is the matrix of values to be plotted (same grid dimensions). units and img_name (self explanatory). save & plot boolean flags to save the file and plot to screen, respectively. zlims define the color scale minimum and maximum. Target Audience Python developers or data scientists or scientists or any Pythonista wanting a simple way to quickly plot an intensity map onto a geographical map. Comparison Differs from using cartopy and matplotlib in its ease-of-use, but it is less customisable (can't change projections, colors). Regardless, it's convenient and at least provides a starting point for customisation. Similar functionality can be had from geopandas or folium (although cartopy/matplotlib suited our needs better)."},
{"Title": "2,000 lines of Python code to make this scrolling ASCII art animation: \"The Forbidden Zone\"", "Author": "u/AlSweigart", "Content": "What My Project Does This is a music video of the output of a Python program: https://www.youtube.com/watch?v=Sjk4UMpJqVs I'm the author of Automate the Boring Stuff with Python and I teach people to code. As part of that, I created something I call \"scroll art\". Scroll art is a program that prints text from a loop, eventually filling the screen and causing the text to scroll up. (Something like those BASIC programs that are 10 PRINT \"HELLO\"; 20 GOTO 10) Once printed, text cannot be erased, it can only be scrolled up. It's an easy and artistic way for beginners to get into coding, but it's surprising how sophisticated they can become. The source code for this animation is here: https://github.com/asweigart/scrollart/blob/main/python/forbiddenzone.py (read the comments at the top to figure out how to run it with the forbiddenzonecontrol.py program which is also in that repo) The output text is procedurally generated from random numbers, so like a lava lamp, it is unpredictable and never exactly the same twice. This video is a collection of scroll art to the music of \"The Forbidden Zone,\" which was released in 1980 by the band Oingo Boingo, led by Danny Elfman (known for composing the theme song to The Simpsons.) It was used in a cult classic movie of the same name, but also the intro for the short-run Dilbert animated series. Target Audience Anyone (including beginners) who wants ideas for creating generative art without needing to know a ton of math or graphics concepts. You can make scroll art with print() and loops and random numbers. But there's a surprising amount of sophistication you can put into these programs as well. Comparison Because it's just text, scroll art doesn't have such a high barrier to entry compared with many computer graphics and generative artwork. The constraints lower expectations and encourage creativity within a simple context. I've produced scroll art examples on https://scrollart.org I also gave a talk on scroll art at PyTexas 2024: https://www.youtube.com/watch?v=SyKUBXJLL50"},
{"Title": "Reviewing Dataframe Changes? Looking for Your Preferred Methods!", "Author": "u/rageagainistjg", "Content": "After playing around with a dataframe‚Äîapplying filters or other transformations‚ÄîI'm curious about your methods for reviewing the changes. In VS Code, the variable explorer is quite handy for a quick look at the modified dataframe. Alternatively, when working in a Jupyter notebook within VS Code, exporting the data to an Excel file provides a detailed view and allows for an easy deep dive into the results. What are your preferred practices for ensuring your data adjustments are precisely what you intended?"},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Library for automatic Cython 3.0 code annotations generation.", "Author": "u/Cool-Nefariousness76", "Content": "Hi everybody, over the last year I've been developing a library that adds some Cython 3.0 annotations to existing python code. What My Project Does: For example if it sees a for i in range(): in a function it recognizes i as an integer and adds a i = cython.declare(cython.int) line at the beginning of the function. It actually uses the built-in ast module under the hood for parsing, I found it a super useful library! Target Audience: It is a side project I made mainly for fun. I don't know if it can be of interest to anybody, or if it could have some potential utility. Comparison: I did not find anything similar. There are a lot of very cool projects like mypyc for example, but nothing that does this tiny little code generation specific to Cython. The link to the repository is here: https://github.com/nucccc/markarth"},
{"Title": "APScheduler vs Schedule package", "Author": "u/kostakos14", "Content": "Hey folks, looking to use one library to implement some background scheduling logic on my application. I find in Google search APScheduler to be frequently mentioned, but I can see the Schedule package has more GH stars. Was curious if anybody has used one of them, and which one would you recommend based on your own experience."},
{"Title": "Introducing Notolog: Python Markdown Editor built with PySide6", "Author": "u/devotaku", "Content": "Excited to share my personal open-source project: Notolog - Python Markdown Editor (MIT License). The main motivation for developing another markdown editor was my passion for learning new things and enhancing my development skills in Python. I developed it in my spare time over a few months, despite having no prior experience in creating full-scale Python applications. What My Project Does ‚àó Multiplatform ‚àó Markdown async syntax highlighting created by me ‚àó Several pre-installed color themes ‚àó Supports English and 17 other languages right out of the box ‚àó Integration with OpenAI API for AI-assisted features ‚àó Optional file encryption/decryption Target Audience Primarily developers who write markdown documents and notes. Comparison This is more of a personal learning project, so it's hard to compare it directly with others. How to install Discover Notolog on GitHub üåü and PyPI . Installation is as easy as running a single command: pip install notolog"},
{"Title": "I've developed a library for send metrics to zabbix asynchronously", "Author": "u/Royal_Section4889", "Content": "I have been using zabbix for monitoring a lot of metrics in my work, none of the most popular zabbix were capable of doing async tasks, so I've developed some simple package capable of doing this. Tests, examples and how-tos can be found here: https://github.com/gustavofbreunig/zabbix-sender-async What My Project Does Send zabbix sender messages using asyncio tasks. Target Audience SysAdmins who use Zabbix to monitor a large number of metrics. Comparison Instead of doing traditional way, using these abandoned library: https://github.com/adubkov/py-zabbix from pyzabbix import ZabbixMetric, ZabbixSender # Send metrics to zabbix trapper packet = [ ZabbixMetric('hostname1', 'test[cpu_usage]', 2), ZabbixMetric('hostname1', 'test[system_status]', \"OK\"), ZabbixMetric('hostname1', 'test[disk_io]', '0.1'), ZabbixMetric('hostname1', 'test[cpu_usage]', 20, 1411598020), ] result = ZabbixSender(use_config=True).send(packet) You can do this: async def sendmetrics(): sender = AsyncSender('localhost', 10051) metric = ItemData(host='hostname', key='test.metric.text', value='test package import') result = await sender.send(metric)"},
{"Title": "I made a Python text to speech library - Pyt2s", "Author": "u/asksumanth", "Content": "What my project does : It supports services like IBM Watson, Acapela and Stream labs' demo websites to convert your text to speech. Target audience : It's a toy project and would not recommend you to use in Production. Comparison : It's wayyyyy easy to use. Just pip install and use in your project. No extra setup required like other libraries. Also supports various languages and voices and accents. Check docs for more. Here is the link to repository. Please go do check it out and star it if it's helpful to you guys. Thank you. I made this library taking inspiration from this php tts library by chrisjp."},
{"Title": "Interactive plots in the terminal", "Author": "u/Spiffidimus", "Content": "I made a library to create interactive plots in the terminal ( pip install itrm ). It uses braille characters (by default) to display the data with sub-character resolution. There are several keybindings for moving a vertical cursor left and right, for zooming in or out on data, and for changing which curve to focus on. There are occasions (such as when working with a server) where MatPlotLib is not an option and the terminal is the only available tool. But, in my opinion, it is actually faster to use this tool (itrm) to zoom in on interesting parts of data and analyze patterns than using other tools like MatPlotLib. In fact, with large data sets (~1 million points), this tool actually renders faster than MatPlotLib. Please check it out and let know what you think."},
{"Title": "Hi! I've published a Python client for IBKR REST and WebSocket APIs - IBind. Hope you like it üëã", "Author": "u/VoyZan", "Content": "Hi! I want to share a library I've built recently. IBind is a REST and WebSocket Python client for Interactive Brokers Client Portal Web API . It is directed at IBKR users. You can find IBind on GitHub: https://github.com/Voyz/ibind What My Project Does: It is a REST and WebSocket API for the Interactive Brokers' Web API. I'm particularly proud of a few things in this release: The REST and WebSocket API clients are based on an abstract base class RestClient and WsClient accordingly. These could be implemented to use some other Web APIs in a relatively straightforward way. I have in fact used a version of that WsClient for a cryptocurrency WebSocket API, and it is nice to see it adapt to a different environment. I've covered most of the codebase with automated tests (appx 80%). Contrary to some of my other libraries, these are mainly integration tests which feel to provide a stronger test coverage than only unit tests. I've learned how to use class mixins in this project, and it aids the maintainability by a lot! The REST client itself is pretty barebone, but has a lot of mixin classes - all corresponding to the endpoint categories the broker uses, making it easy to search for the right piece of code and documentation. There's a lot of things that make this client as plug-and-play as possible. The broker requires the user to specify a bunch of things - account ids, certificates, URLs, etc. - which the class either reads from the environment variables or assumes (given that some things would be common for most users). In either case, all these are customisable by parameters if needed, but it is nice to just write client = IbkrClient() in various projects having set just a couple of env vars. I think the documentation is pretty in-depth but readable. It's always hard to judge whether docs are well written, but I think it is nicely broken down. Also, I managed to use pydoc-markdown package to create API reference in markdown, which works nicely with the GitHub Wiki. I'd prefer it to be even easier, but compared to Sphinx and readthedocs it's a much quicker job. The WebSocket class does a ton to keep the connection alive and recover from connection losses. Maintaining active subscriptions after a re-connect can be a real pain, and I think this class does it in a nice and reliable way. I've tested it for various types of connectivity loss, and it manages to recover and re-establish the WebSocket data stream. Pretty crucial in the trading environment. I made a nice logo for it ü•≥ Target Audience: Traders using IBKR who want to automate their trading through this Web API. Comparison (A brief comparison explaining how it differs from existing alternatives.) : There are two similar libraries that I know of. They aren't bad, but seem not very well maintained and incomplete: https://github.com/areed1192/interactive-brokers-api - outdated and stale, last update 3 years ago https://github.com/utilmon/EasyIB - stale and incomplete The library I've published covers a much wider range of endpoints, adds WebSocket support and a bunch of wrapper methods to simplify the usage of the API. IBind has a bunch of features that make using the IBKR APIs much easier. Some of these are: REST: Automated question/answer handling - streamlining placing orders. Parallel requests - speeding up collection of price data. Rate limiting - guarding against account bans. Conid unpacking - helping to find the right contract. WebSocket: WebSocket thread lifecycle handling - ensuring the connection is alive. Thread-safe Queue data stream - exposing the collected data in a safe way. Internal subscription tracking - recreating subscriptions upon re-connections. Health monitoring - Acting on unusual ping or heartbeat. REST Example: from ibind import IbkrClient # Construct the client client = IbkrClient() print(client.tickle().data) WebSocket Example: from ibind import IbkrWsKey, IbkrWsClient # Construct the client. ws_client = IbkrWsClient(start=True) # Choose the WebSocket channel key = IbkrWsKey.PNL # Subscribe to the PNL channel ws_client.subscribe(channel=key.channel) print(ws_client.get(key)) I just wanted to share my experience of publishing Open Source. For some reason I get a lot of motivation when I can publish code that makes peoples' lives easier. The library could use some code review on it, so if you‚Äôd feel like reading some code and helping out - drop me a message. Other than that, happy to answer any questions, and - if you are an algo trader - let me know if you get a chance to use it. Thanks for reading!"},
{"Title": "Python Streamlit Spotlight Tutorial: an Interactive Dashboard using UNHCR Refugee Data", "Author": "u/jgloewen", "Content": "Python Streamlit is a terrific tool for creating interactive data visualizations. It packages all your visualizations up into a neat little application - including charts and maps - and displays them in your default browser. No muss, no fuss. Recently, I found a new dataset (to me) on the UN High Commission for Refugees (UNHCR) website. It contains country-to-country movements for refugees both from origin country and country of asylum Using this dataset, here's a step-by-step on how to code a Python Streamlit application that has: A dropdown menu to select by country A second dropdown menu to select by year Radio buttons (2) to select country of origin or county of asylum A global choropleth map to display the results by country and year. Free article HERE ."},
{"Title": "IP subnet or IP calculator tool need feedback", "Author": "u/nicanorflavier", "Content": "Hey folks, I've been dabbling with a Python project recently that's all about making life easier for us I.T. people. It's a nifty little tool that calculates IP subnets and does IP calculations from the command or CLI. Here's the GitHub link and the code: https://github.com/nicanorflavier/ipnet I‚Äôm pretty stoked about it, but I know there‚Äôs always room for improvement. So, I thought, better to turn to than the wise minds of this python community? I‚Äôm all ears for any feedback, tips, tricks, or advice you guys might have. Thanks a ton in advance!"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Pre-commit hook to keep coverage badge in README up to date", "Author": "u/60percentcocoa", "Content": "Wrote this as a tool to keep README coverage badges up to date without relying on 3rd party services or having to do anything extra, thought others might get some utility out of it: coverage-pre-commit . A .coverage file is expected at the root of the project, generated by running coverage run directly or using a plugin such as pytest-cov when running tests. Most convenient when used as a pre-push hook imo. Feel free to opine, be it positive or negative!"},
{"Title": "PyWolt: Wolt food delivery service API wrapper", "Author": "u/daivushe1", "Content": "I'm thrilled to share my first open-source project with you all: PyWolt ! üéâ PyWolt is a Python library that makes it super easy to interact with the Wolt API. What My Project Does: Discover Venues: Find nearby spots to grab a bite. Explore Menus: Dive into a venue's menu and pick your favorites. Target Audience: Software Engineers : Professionals who build web or mobile applications, particularly those in the food delivery or restaurant industry, looking to incorporate Wolt's services seamlessly into their platforms. Data Scientists/Analysts : Individuals analyzing food delivery data, consumer behavior, or market trends, who may utilize PyWolt to gather data from Wolt's API for analysis and insights. Students/Learners : Those studying Python programming, web development, or API integration, who can use PyWolt as a practical example or learning tool to understand how to interact with RESTful APIs in Python. Freelancers/Entrepreneurs : Independent developers or startup founders looking to build new food-related applications or services leveraging Wolt's platform without reinventing the wheel. Comparison: woltcheck : only offers a script to check if a wolt restaurant is ready to deliver to your location. what-to-eat : a pretty neat cli tool that offers all of pywolt's functionality. In my opinion it overcomplicates things a little, and doesn't offer straight-forward RESTful functionality to interact with the API itself."},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "I made a React-like web framework for Python üëã", "Author": "u/Rawing7", "Content": "I'm Paul, one of the creators of Rio. Over the years I've tried many different established python GUI frameworks, but none of them really satisfied me. So I teamed up with a few like minded developers and spent the last few months to create our own framework. Rio is the result of this effort. What My Project Does Rio is a brand new GUI framework that lets you create modern web apps in just a few lines of Python. Our goal is to simplify web and app development, so you can focus on the things you care about, instead of wasting countless hours on frustrating user interface details. We do this by following the core principles of Python that we all know and love. Python is supposed to be simple and compact - and so is Rio. There is no need to learn any additional languages such as HTML, CSS or JavaScript, because all of the UI, Logic, Components and even layouting is done entirely in Python. There‚Äôs not even a distinction between front-end and back-end. Rio handles all of the communication transparently for you. Key Features Full-Stack Web Development: Rio handles front-end and backend for you. In fact, you won't even notice they exist. Create your UI, and Rio will take care of the rest. Python Native: Rio apps are written in 100% Python, meaning you don't need to write a single line of CSS or JavaScript. Modern Python: We embrace modern Python features, such as type annotations and asynchrony. This keeps your code clean and maintainable, and helps your code editor help you out with code completions and type checking. Python Debugger Compatible: Since Rio runs on Python, you can connect directly to the running process with a debugger. This makes it easy to identify and fix bugs in your code. Declarative Interface: Rio apps are built using reusable components, inspired by React, Flutter & Vue. They're declaratively combined to create modular and maintainable UIs. Batteries included: Over 50 builtin components based on Google's Material Design Demo Video Target Audience Whether you need to build dashboards, CRUD apps, or just want to make a personal website, Rio makes it possible without any web development knowledge. Because Rio was developed from the ground up for Python programmers, it was designed to be concise and readable, just like Python itself. Comparison Rio doesn't just serve HTML templates like you might be used to from frameworks like Flask. In Rio you define components as simple dataclasses with a React/Flutter style build method. Rio continuously watches your attributes for changes and updates the UI as necessary. class MyComponent(rio.Component): clicks: int = 0 def _on_press(self) -> None: self.clicks += 1 def build(self) -> rio.Component: return rio.Column( rio.Button('Click me', on_press=self._on_press), rio.Text(f'You clicked the button {self.clicks} time(s)'), ) app = rio.App(build=MyComponent) app.run_in_browser() Notice how there is no need for any explicit HTTP requests. In fact there isn't even a distinction between frontend and backend. Rio handles all communication transparently for you. Unlike ancient libraries like Tkinter, Rio ships with over 50 builtin components in Google's Material Design. Moreover the same exact codebase can be used for both local apps and websites. We Want Your Feedback! The first alpha version of Rio is available on PyPi now: pip install rio-ui rio new my-project --template tic-tac-toe cd my-project rio run Discord GitHub Tutorial Website Let us know what you think - any feedback, ideas, or even a helping hand are hugely welcome! Just hop on our Discord server and say hello!"},
{"Title": "The new REPL in Python 3.13.0 beta 1", "Author": "u/treyhunner", "Content": "Python 3.13.0 beta 1 was released today. The feature I'm most excited about is the new Python REPL. Here's a summary of my favorite features in the new REPL along with animated gifs . The TLDR: Support for block-leveling history and block-level editing Pasting code (even with blank lines within it) works as expected now Typing exit will exit (no more Use exit() or Ctrl-D (i.e. EOF) to exit message)"},
{"Title": "Calculating Virtual Cycling Power With Python", "Author": "u/TokenChingy", "Content": "I was doing some light reading and stumbled across Steve Gribbles Power vs Speed Calculator and thought I'd give it a go at rebuilding it based on his Physics model using Python. Then I wrote an article about. Thought I'd share it with you all: Calculating Virtual Cycling Power (jasonlei.com)"},
{"Title": "InterProcessPyObjects: Fast IPC for Sharing and Modifying Objects Across Processes", "Author": "u/FI_Mihej", "Content": "InterProcessPyObjects Python package github.com/FI-Mihej/InterProcessPyObjects If you like the project, consider giving it a star on GitHub to show your support and help further development. :) pypi.org/project/InterProcessPyObjects What My Project Does InterProcessPyObjects is a part of the Cengal library. If you have any questions or would like to participate in discussions, feel free to join the Cengal Discord . Your support and involvement are greatly appreciated as Cengal evolves. This high-performance package delivers blazing-fast inter-process communication through shared memory, enabling Python objects to be shared across processes with exceptional efficiency. By minimizing the need for frequent serialization-deserialization, it enhances overall speed and responsiveness. The package offers a comprehensive suite of functionalities designed to support a diverse array of Python types and facilitate asynchronous IPC, optimizing performance for demanding applications. Target Audience This project is designed for production environments, offering a stable API suitable for developers looking to implement fast inter-process communication. Whether you're building complex systems or require robust data sharing and modification across processes, InterProcessPyObjects is ready to meet your needs. Comparison Comparison with multiprocessing.shared_memory While both InterProcessPyObjects and multiprocessing.shared_memory facilitate inter-process communication, there are several key differences to note. Unlike multiprocessing.shared_memory, InterProcessPyObjects offers the following enhancements: High-Performance Mutable Objects: Both connected processes can modify shared objects at runtime, and these changes are immediately reflected on the other side. This feature not only increases flexibility but also delivers exceptional performance, with the capability to handle up to several million changes per second. Synchronization Features: Ensures that operations are thread-safe and data integrity is maintained across processes. Message Queue: Integrates a system for queuing messages, making communication between processes more structured and reliable. Extended Type Support: Supports a broad range of data types, including custom classes, which goes beyond the basic types typically handled by multiprocessing.shared_memory. These features make InterProcessPyObjects a more robust option for developers requiring advanced inter-process communication capabilities. API State Stable. Guaranteed not to have breaking changes in the future. (see github.com/FI-Mihej/InterProcessPyObjects?tab=readme-ov-file#api-state for details) Key Features Shared Memory Communication: Enables sharing of Python objects directly between processes using shared memory. Utilizes a linked list of global messages to inform connected processes about new shared objects. Lock-Free Synchronization: Uses memory barriers for efficient communication, avoiding slow syscalls. Ensures each process can access and modify shared memory without contention. Supported Python Types: Handles various Python data structures including: Basic types: None , bool , 64-bit int , large int (arbitrary precision integers), float , complex , bytes , bytearray , str . Standard types: Decimal , slice , datetime , timedelta , timezone , date , time Containers: tuple , list , classes inherited from: AbstractSet ( frozenset ), MutableSet ( set ), Mapping and MutableMapping ( dict ). Pickable classes instances: custom classes including dataclass Allows mutable containers (lists, sets, mappings) to save basic types ( None , bool , 64 bit int , float ) internally, optimizing memory use and speed. NumPy and Torch Support: Supports numpy arrays by creating shared bytes objects coupled with independent arrays. Supports torch tensors by coupling them with shared numpy arrays. Custom Class Support: Projects pickable custom classes instances (including dataclasses ) onto shared dictionaries in shared memory. Modifies the class instance to override attribute access methods, managing data fields within the shared dictionary. supports classes with or without __dict__ attr supports classes with or without __slots__ attr Asyncio Compatibility: Provides a wrapper module for async-await functionality, integrating seamlessly with asyncio. Ensures asynchronous operations work smoothly with the package's lock-free approach. Main principles only one process has access to the shared memory at the same time working cycle: work on your tasks acquire access to shared memory work with shared memory as fast as possible (read and/or update data structures in shared memory) release access to shared memory continue your work on other tasks do not forget to manually destroy your shared objects when they are not needed already feel free to not destroy your shared object if you need it for a whole run and/or do not care about the shared memory waste data will not be preserved between Creator's sessions. Shared memory will be wiped just before Creator finished its work with a shared memory instance (Consumer's session will be finished already at this point) Examples An async examples (with asyncio): sender.py receiver.py shared_objects__types.py Receiver.py performance measurements CPU: i5-3570@3.40GHz (Ivy Bridge) RAM: 32 GBytes, DDR3, dual channel, 655 MHz OS: Ubuntu 20.04.6 LTS under WSL2. Windows 10 async with ashared_memory_context_manager.if_has_messages() as shared_memory: # Taking a message with an object from the queue. sso: SomeSharedObject = shared_memory.value.take_message()  # 5_833 iterations/seconds # We create local variables once in order to access them many times in the future, ensuring high performance. # Applying a principle that is widely recommended for improving Python code. company_metrics: List = sso.company_info.company_metrics  # 12_479 iterations/seconds some_employee: Employee = sso.company_info.some_employee  # 10_568 iterations/seconds data_dict: Dict = sso.data_dict  # 16_362 iterations/seconds numpy_ndarray: np.ndarray = data_dict['key3']  # 26_223 iterations/seconds # Optimal work with shared data (through local variables): async with ashared_memory_context_manager as shared_memory: # List k = company_metrics[CompanyMetrics.avg_salary]  # 1_535_267 iterations/seconds k = company_metrics[CompanyMetrics.employees]  # 1_498_278 iterations/seconds k = company_metrics[CompanyMetrics.in_a_good_state]  # 1_154_454 iterations/seconds k = company_metrics[CompanyMetrics.websites]  # 380_258 iterations/seconds company_metrics[CompanyMetrics.annual_income] = 2_000_000.0  # 1_380_983 iterations/seconds company_metrics[CompanyMetrics.employees] = 20  # 1_352_799 iterations/seconds company_metrics[CompanyMetrics.avg_salary] = 5_000.0  # 1_300_966 iterations/seconds company_metrics[CompanyMetrics.in_a_good_state] = None  # 1_224_573 iterations/seconds company_metrics[CompanyMetrics.in_a_good_state] = False  # 1_213_175 iterations/seconds company_metrics[CompanyMetrics.avg_salary] += 1.1  # 299_415 iterations/seconds company_metrics[CompanyMetrics.employees] += 1  # 247_476 iterations/seconds company_metrics[CompanyMetrics.emails] = tuple()  # 55_335 iterations/seconds (memory allocation performance is planned to be improved) company_metrics[CompanyMetrics.emails] = ('sails@company.com',)  # 30_314 iterations/seconds (memory allocation performance is planned to be improved) company_metrics[CompanyMetrics.emails] = ('sails@company.com', 'support@company.com')  # 20_860 iterations/seconds (memory allocation performance is planned to be improved) company_metrics[CompanyMetrics.websites] = ['http://company.com', 'http://company.org']  # 10_465 iterations/seconds (memory allocation performance is planned to be improved) # Method call on a shared object that changes a property through the method some_employee.increase_years_of_employment()  # 80548 iterations/seconds # Object properties k = sso.int_value  # 850_098 iterations/seconds k = sso.str_value  # 228_966 iterations/seconds sso.int_value = 200  # 207_480 iterations/seconds sso.int_value += 1  # 152_263 iterations/seconds sso.str_value = 'Hello. '  # 52_390 iterations/seconds (memory allocation performance is planned to be improved) sso.str_value += '!'  # 35_823 iterations/seconds (memory allocation performance is planned to be improved) # Numpy.ndarray numpy_ndarray += 10  # 403_646 iterations/seconds numpy_ndarray -= 15  # 402_107 iterations/seconds # Dict k = data_dict['key1']  # 87_558 iterations/seconds k = data_dict[('key', 2)]  # 49_338 iterations/seconds data_dict['key1'] = 200  # 86_744 iterations/seconds data_dict['key1'] += 3  # 41_409 iterations/seconds data_dict['key1'] *= 1  # 40_927 iterations/seconds data_dict[('key', 2)] = 'value2'  # 31_460 iterations/seconds (memory allocation performance is planned to be improved) data_dict[('key', 2)] = data_dict[('key', 2)] + 'd'  # 18_972 iterations/seconds (memory allocation performance is planned to be improved) data_dict[('key', 2)] = 'value2'  # 10_941 iterations/seconds (memory allocation performance is planned to be improved) data_dict[('key', 2)] += 'd'  # 16_568 iterations/seconds (memory allocation performance is planned to be improved) # An example of non-optimal work with shared data (without using a local variables): async with ashared_memory_context_manager as shared_memory: # An example of a non-optimal method call (without using a local variable) that changes a property through the method sso.company_info.some_employee.increase_years_of_employment()  # 9_418 iterations/seconds # An example of non-optimal work with object properties (without using local variables) k = sso.company_info.income  # 20_445 iterations/seconds sso.company_info.income = 3_000_000.0  # 13_899 iterations/seconds sso.company_info.income *= 1.1  # 17_272 iterations/seconds sso.company_info.income += 500_000.0  # 18_376 iterations/seconds # Example of non-optimal usage of numpy.ndarray without a proper local variable data_dict['key3'] += 10  # 6_319 iterations/seconds # Notify the sender about the completion of work on the shared object async with ashared_memory_context_manager as shared_memory: sso.some_processing_stage_control = True  # 298_968 iterations/seconds Throughput Benchmarks CPU: i5-3570@3.40GHz (Ivy Bridge) RAM: 32 GBytes, DDR3, dual channel, 655 MHz OS: Ubuntu 20.04.6 LTS under WSL2. Windows 10 Refference results (sysbench) sysbench memory --memory-oper=write run 5499.28 MiB/sec Benchmarks results table GiB/s Approach sync/async Throughput GiB/s InterProcessPyObjects (sync) sync 3.770 InterProcessPyObjects + uvloop async 3.222 InterProcessPyObjects + asyncio async 3.079 multiprocessing.shared_memory * sync 2.685 uvloop.UnixDomainSockets async 0.966 asyncio + cengal.Streams async 0.942 uvloop.Streams async 0.922 asyncio.Streams async 0.784 asyncio.UnixDomainSockets async 0.708 multiprocessing.Queue sync 0.669 multiprocessing.Pipe sync 0.469 * multiprocessing.shared_memory.py - simple implementation. This is a simple implementation because it uses a similar approach to the one used in uvloop.* , asyncio.* , multiprocessing.Queue , and multiprocessing.Pipe benchmarking scripts. Similar implementations are expected to be used by the majority of projects. Todo Connect more than two processes Use third-party fast hashing implementations instead of or in addition to built in hash() call Continuous performance improvements Conclusion This Python package provides a robust solution for inter-process communication, supporting a variety of Python data structures, types, and third-party libraries. Its lock-free synchronization and asyncio compatibility make it an ideal choice for high-performance, concurrent execution. Based on Cengal This is a stand-alone package for a specific Cengal module. Package is designed to offer users the ability to install specific Cengal functionality without the burden of the library's full set of dependencies. The core of this approach lies in our 'cengal-light' package, which houses both Python and compiled Cengal modules. The 'cengal' package itself serves as a lightweight shell, devoid of its own modules, but dependent on 'cengal-light[full]' for a complete Cengal library installation with all required dependencies. An equivalent import: from cengal.hardware.memory.shared_memory import * from cengal.parallel_execution.asyncio.ashared_memory_manager import * Cengal library can be installed by: pip install cengal https://github.com/FI-Mihej/Cengal https://pypi.org/project/cengal/ Projects using Cengal CengalPolyBuild - A Comprehensive and Hackable Build System for Multilingual Python Packages: Cython (including automatic conversion from Python to Cython), C/C++, Objective-C, Go, and Nim, with ongoing expansions to include additional languages. (Planned to be released soon) cengal_app_dir_path_finder - A Python module offering a unified API for easy retrieval of OS-specific application directories, enhancing data management across Windows, Linux, and macOS cengal_cpu_info - Extended, cached CPU info with consistent output format. cengal_memory_barriers - Fast cross-platform memory barriers for Python. flet_async - wrapper which makes Flet async and brings booth Cengal.coroutines and asyncio to Flet (Flutter based UI) justpy_containers - wrapper around JustPy in order to bring more security and more production-needed features to JustPy (VueJS based UI) Bensbach - decompiler from Unreal Engine 3 bytecode to a Lisp-like script and compiler back to Unreal Engine 3 bytecode. Made for a game modding purposes Realistic-Damage-Model-mod-for-Long-War - Mod for both the original XCOM:EW and the mod Long War. Was made with a Bensbach, which was made with Cengal SmartCATaloguer.com - TagDB based catalog of images (tags), music albums (genre tags) and apps (categories) License Licensed under the Apache License, Version 2.0."},
{"Title": "AzuracastPy: An Unofficial Python Wrapper for the Azuracast API.", "Author": "u/ARandomBoiIsMe", "Content": "Source code What My Project Does: It acts as a wrapper for the AzuraCast API, providing custom functions and classes for more straightforward use of the API in python projects. Target Audience: Python users who are interested in programmatically interacting with online radios hosted on AzuraCast . Comparison: The idea of API Wrappers is not new. However, I noticed that the only existing wrapper for this API is written in PHP , which I am not experienced with. I created this project so I, and other python programmers by extension, could have an easier time working with the API. This is my first \"major\" programming project, so thoughts and feedback are welcome and greatly appreciated. PS: Shoutout to PRAW for \"inspiring\" basically everything about the project's structure and functionality."},
{"Title": "diskcache: This key-value store library is faster than Redis and Memcached üòÆ (built by Grant Jenks)", "Author": "u/RevolutionaryPen4661", "Content": "PYPI (From the README, Released Last Year, Edited by Grammarly) Github pip install diskcache The cloud-based computing of 2024 puts a premium on memory. Gigabytes of space are left on disks as processes vie for memory. Memcached (and sometimes Redis) is used as a cache among these processes. Wouldn‚Äôt it be nice to leverage empty disk space for caching? Django is Python‚Äôs most popular web framework and has several caching backends. Unfortunately, the file-based cache in Django is essentially broken. The culling method is random and large caches repeatedly scan a cache directory which slows linearly with growth. Can you allow it to take sixty milliseconds to store a key in a cache with a thousand items? Is it that fast? In [1]: import pylibmc In [2]: client = pylibmc.Client(['127.0.0.1'], binary=True) In [3]: client[b'key'] = b'value' In [4]: %timeit client[b'key'] 10000 loops, best of 3: 25.4 ¬µs per loop In [5]: import diskcache as dc In [6]: cache = dc.Cache('tmp') In [7]: cache[b'key'] = b'value' In [8]: %timeit cache[b'key'] 100000 loops, best of 3: 11.8 ¬µs per loop"},
{"Title": "Who is using quart framework for microservices?", "Author": "u/Dry_Raspberry4514", "Content": "I am using quart framework ( https://quart.palletsprojects.com ) for a number of microservices in a SaaS application. However, I hardly hear anything about this framework on any social media platform which seems to be dominated by FastAPI. Also I'm unable to find which all projects/companies are using this framework. All this is leading to anxiety around the future of this project. Are there any well known projects / companies which are using this framework for microservices?"},
{"Title": "Why is Plotly so cumbersome to tweak?", "Author": "u/olive_oil_for_you", "Content": "I made this visualisation with this code . I have three questions: Is Plotly supposed to be this cumbersome to tweak? Would other libraries require the same amount of code to add the details I did? Can my code be reduced in size? Maybe it's me who is complicating things with Plotly and there are easier ways to do what I am doing. Any R enthusiast who can tell me how much shorter this code would look like with ggplot2? I asked ChatGPT but the result was garbage. Bonus question: This took me an entire morning. Is it normal to be \"that slow\" to plot a simple figure?"},
{"Title": "I connected LLM to Python runtime and generated unit-tests (OpenSource)", "Author": "u/Financial_Muffin396", "Content": "Hi all, I initially started this adventure by trying to automate bug fixes with the help of LLMs. However, I received feedback saying the fixes aren't always correct, leading to the question: why bother reviewing PRs that might add more issues? (It's really hard for LLMs to say \"I don't know\"). So, I decided to focus on reliability perfecting unit tests. The source code is available at: https://github.com/CaptureFlow/captureflow-py What My Project Does: It incorporates a tracer client-side Python library and a backend that accumulates such traces and is capable of proposing code improvements and generating tests for your repository. It traverses the execution graph, extracts relevant parts, enriches them with implementation data from the GitHub API, and then generates tests with the help of GPT4. Target Audience: Python users interested in discovering what LLMs can achieve when given detailed runtime information from your app. Generally, this approach somewhat reverses the concept of TDD, but in my day job I deal with many legacy apps that have poor test coverage, and having it really helps. I suspect I‚Äôm not alone in finding value in this. Comparison: I think idea of using LLMs to generate tests is not new, but generating them actually based on application's performance is inspired by Jane Street's article on how they automated test boilerplate creation and recent Facebook's research paper . Disclaimer: More work needs to be done to make it work for any Python app and not just a subset of FastAPI servers. I'm curious if you folks would find it useful. Example: you can check this PR for reference. feedback / stars / contributions are welcome."},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Seeking Your Input: Let's Co-Create FreeCodeCamp for python together", "Author": "u/globalwarming_isreal", "Content": "Hello all, This is probably my first post here. I usuallly lurk around here and Django subreddits. I've been brewing up an idea and I need your input before I take the plunge! Picture this: a website like FreeCodeCamp but for python and related technologies, a learning oasis where anyone can kickstart their journey from Python newbie to job-ready pro, and it's all free! But here's the thing, I want this to be our platform, crafted with your needs and dreams in mind. So, before I start, I need to know: would this be something that gets you excited? Imagine quizzes helping you find your starting point, interactive challenges that keep you in the zone, and a supportive community to cheer you on every step of the way. Plus, videos, written tutorials, and a progress tracker to keep you motivated! What would make you go, \"Wow, I need this in my life!\"? What features would you love to see? Any suggestions or wild ideas ? My aim is to give back to the community by assisting new learners in navigating common pitfalls when they start their Python journey"},
{"Title": "Tutorial on Creating Useful Data Visuals with Python seaborn and matplotlib libraries", "Author": "u/jgloewen", "Content": "With the current global deluge of data and information, there has never been a more important to visualize your data in a clear and simple manner. Python is a terrific tool to help us do this. The key to this lies in choosing the the right data visualization techniques to tell the most interesting and relevant story. Three useful visuals are: small multiples heat maps stacked area charts In this tutorial, using pandas, seaborn, and matplotlib.pyplot, we create the Python code for each data visual Link to free Tutorial"},
{"Title": "Rethinking String Encoding: a 37.5% space efficient string encoding than UTF-8 in Apache Fury", "Author": "u/Shawn-Yang25", "Content": "In rpc/serialization systems, we often need to send namespace/path/filename/fieldName/packageName/moduleName/className/enumValue string between processes. Those strings are mostly ascii strings. In order to transfer between processes, we encode such strings using utf-8 encodings. Such encoding will take one byte for every char, which is not space efficient actually. If we take a deeper look, we will found that most chars are lowercase chars, ., $ and _, which can be expressed in a much smaller range 0~32. But one byte can represent range 0~255, the significant bits are wasted, and this cost is not ignorable. In a dynamic serialization framework, such meta will take considerable cost compared to actual data. So we proposed a new string encoding which we called meta string encoding in Fury. It will encode most chars using 5 bits instead of 8 bits in utf-8 encoding, which can bring 37.5% space cost savings compared to utf-8 encoding. For string can't be represented by 5 bits, we also proposed encoding using 6 bits which can bring 25% space cost savings More details can be found in: https://fury.apache.org/blog/fury_meta_string_37_5_percent_space_efficient_encoding_than_utf8 and https://github.com/apache/incubator-fury/blob/main/docs/specification/xlang_serialization_spec.md#meta-string"},
{"Title": "Python script to convert Spotify Artists to Playlists", "Author": "u/RJW-20", "Content": "I've made my first bit of useful software and I wanted to share it here. I'd love some feedback (and it would be amazing to hear if someone has used it!) What My Project Does: Using the third party requests package, the script interacts with the Spotify web API to request all albums from the given Artist, then all the tracks from all of those albums. It then goes through the list to remove any duplicates and also tries to remove any unwanted versions (only done by examining the name of the track, since Spotify does not attribute a version type to its tracks). Once that's done a playlist is then created on your Spotify account with the name of the Artist and all the tracks are posted there in chronological (essentially per album) order. Target Audience: Anyone who struggles like me when they find a new Artist and they want to listen to every conceivable song from them! Link to GitHub: https://github.com/RJW20/spotify-artist-to-playlist"},
{"Title": "List of Sites that Packages Need to Connect to?", "Author": "u/BullCityPicker", "Content": "I'm doing most of my work behind a government firewall, and I'm having trouble connecting to certain sites.   I can do the usual \"pip\" installs just fine, but I'm talking about packages that need to download data to do their job.  An example is the NLTK (Natural Language Toolkit) package, which downloads dictionaries, lookup tables for sentiment analysis, and so on.  I know what sites to open up for that particular problem (pastebin.com and nltk.org), but I wonder if anybody's made a list of such sites for different packages. I can ask for the two sites I know about to be opened up, but I'd like to have a more comprehensive list so I don't have to go through the red tape multiple times."},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Pip 24.1 beta released, and it's a big one", "Author": "u/zurtex", "Content": "I'd like to call attention to pip 24.1 beta asit is unusual for the pip team to release betas: https://pip.pypa.io/en/latest/news/#b1-2024-05-06 https://pypi.org/project/pip/24.1b1/ You can install with: python -m pip install pip==24.1b1 In particular they have upgraded their vendored version of packaging from 21.3 to 24.0, this was a big effort and fixed many bugs , included significant performance improvements, and will allow pip to support free threaded packages . However, it also means legacy versions and specifiers are no longer compatible with pip. Because this was such a big land the pip maintainers have released a beta in the hopes people will test their workflows, and if something fails in an expected way report their steps as best as possible back to pip: https://github.com/pypa/pip/issues I've been testing, and contributing a little bit, to the improved performance in this release, it is most noticeable on large dependency trees or long backtracking. For example, a dry run of \"apache-airflow[all]\" using cached packages on my machine goes from ~418 seconds to ~185 seconds."},
{"Title": "How Python Asyncio Works: Recreating it from Scratch", "Author": "u/jpjacobpadilla", "Content": "Do you understand how asyncio works behind the scenes? Read this article and see how you can use Python generators to create your own version of asyncio, and then use the __await__ dunder method to use the async/await keywords to come full circle! https://jacobpadilla.com/articles/recreating-asyncio"},
{"Title": "Build tool support for PySide / PyQt", "Author": "u/jmacey", "Content": "Just interested in how people approach this, typically I just use VSCode or QtCreator to build simple projects. However I now want to automate some of the build process such as running uic, and rcc. I've tried to use CMake but can't seem to get it to work without a lot of custom scripting (for example the AUTOUIC etc functions need c++ projects), can't see any info on running uic in QtCreator (which would be ideal but python support is really just an after thought). I could write some Makefiles but this is a little ad-hoc and also confuses the IDE's (and at the end of the day I want a simple process for my students to use and I already teach cmake for C++ dev). So I guess my questions are what workflows do people use, can you recommend any tools to help, or do you just have a per project script to run uic and rcc? (I may cross post this in both qt and python subreddits as I'm not sure where it fits best)"},
{"Title": "relax-py - Web framework for htmx with hot module replacement", "Author": "u/MindLopsided4430", "Content": "Excited to finally showcase this! It's still pretty rough around the edges, but I'm finally happy enough with the feature set and curious to see what the community thinks about a framework like this. Code: github.com/crpier/relax-py Documentation: crpier.github.io/relax-py What My Project Does relax-py is a Python framework for building full-stack applications with htmx It provides tools for writing HTML in a manner similar to simple_html (which also inspired the decision to use standard Python to write HTML, rather than use Jinja2 or to make something like templ work in Python) It has: Hot Module Replacement (meaning, when you update the code that generates HTML templates, the browser also updates instantly) - see the video in the documentation for a quick demo of this URL resolution with type hinting - you can get the URL of an endpoint to use in your templates by using the function that handles that URL, and get help from static typing (for example, for putting path parameters in the URL) Helpers for dependency injection In essence, this framework is just a bunch of decorators and functions over starlette , meaning everything that starlette has can be used alongside the framework. Target Audience Developers interested in building web applications with htmx that like new shiny things and static typing support Comparison As far as I know, the only other backend framework that has Hot Module Replacement is turbo in Ruby on Rails, but there might be something I missed. As for other points of comparison with other frameworks: Django relax is less opinionated about what's done in the backend (.e.g there is preference to what ORM is used) using standard Python code to generate HTML has nicer static typing the URL resolution is more complex and provides errors in the IDE by way of static typing the component decorator provides nicer ways to reuse template functions and helpers for interoperability with JavaScript templ in Go templ allows writing actual HTML in go files, but requires an additional compilation step plugins for whatever IDE/code editor is used are needed parsing templ files FastAPI (with something to generate HTML like simple_html or Jinja2) since FastAPI is built for RESTful APIs, it lacks niceties like URL resolution, or a mechanism to manage the sprawling mess of interconnected HTML components that apps tend to develop dependency injection in FastAPI is \"encouraged\" to happen in the path functions, but in relax it's meant to happen at any level of the app (either in path functions, or in service-level functions, or in util functions) simple_html (with a backend like Flask or FastAPI): the main differences between simple_html and the relax.html module are that CSS classes are provided as a list of strings - this makes it easier to reuse them in different components, and will make it easier to implement other helpers in the future, like a Python version of tailwind-merge , or a formatter that sorts tailwind classes htmx-related attributes are included in the elements inserting children to an HTML element is done after instantiating the element, making it easier to reuse components Here's the code again: github.com/crpier/relax-py There's more details in the documentation: crpier.github.io/relax-py While this framework is definitely not production ready, in the \"Other\" page of the documentation there's an example app built with this framework, which shows how it can be used in conjuction with some real-life scenarios (production environment for tailwind with plugins, working in a bunch of interactivity with JavaScript, in either separate js files and inline scripts, Dockerfiles and deployments, authentication and authorization, configuration etc.) Please let me know what you think (are there better alternatives, is writing HTML in standard Python a deal-breaker, is investing in making something templ in Python worth it?) Hope you're intrigued by this!"},
{"Title": "python-oracledb 2.2 and the VECTOR type in Oracle Database 23ai", "Author": "u/cjbj", "Content": "python-oracledb 2.2, the Oracle Database driver, has been released with support for Oracle Database 23ai features such as the VECTOR and BOOLEAN data types, Implicit Connection Pooling, and improved connection performance.  See the release announcement ."},
{"Title": "I've started writing Python bindings for lexertl", "Author": "u/BenHanson", "Content": "See https://github.com/BenHanson/pylexertl I will see about registering as an official library when I am happy I have completed all the bindings. I added all the missing functions for the rules objects today, so things are in reasonable shape already. My python experience has been limited up until now, but it is big for my new role. I have a runtime parser generator https://github.com/BenHanson/parsertl17 which I also plan to add bindings for. I hope this is of interest to somebody! What My Project Does Allows you to build lexical analysers at runtime and use them to lex text (in this case utf-8) Target Audience The C++ library has been used in production for over 10 years. Comparison I'm not aware of any competing library."},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Introducing PgQueuer: A Minimalist Python Job Queue Built on PostgreSQL", "Author": "u/GabelSnabel", "Content": "What My Project Does PgQueuer is a Python library designed to manage job queues using  PostgreSQL features. It leverages PostgreSQL's native LISTEN/NOTIFY, along with advanced locking mechanisms, to handle job queues efficiently. This allows for real-time job processing, concurrency, and reliable task execution without the need for a separate queuing system. Target Audience PgQueuer is ideal for developers and teams who already use PostgreSQL in their projects and are looking for a simple, integrated way to handle background tasks and job queues. It's designed for production use, offering a dependable solution that scales seamlessly with existing PostgreSQL databases. Comparison Unlike many other job queue solutions that require additional services or complex setups (such as Redis or RabbitMQ), PgQueuer operates directly within PostgreSQL. This removes the overhead of integrating and maintaining separate systems for job management. How PgQueuer stands out Integration Simplicity : Integrates directly with existing PostgreSQL setups without additional infrastructure. Efficiency : Uses PostgreSQL‚Äôs FOR UPDATE SKIP LOCKED for high concurrency, allowing multiple workers to process tasks simultaneously without conflict. Real-time Updates : Utilizes PostgreSQL's LISTEN/NOTIFY for immediate job processing updates, reducing latency compared to polling-based systems. Request for Feedback on Useful Features Im always looking to improve PgQueuer and make it more useful for our users. If you have any features you'd like to see, or if there's something you think could be improved, please let me know! Your feedback is invaluable! Share your thoughts, suggestions, or feature requests either here in the comments or via GitHub ."},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "I made a React-like web framework for Python üëã", "Author": "u/Rawing7", "Content": "I'm Paul, one of the creators of Rio. Over the years I've tried many different established python GUI frameworks, but none of them really satisfied me. So I teamed up with a few like minded developers and spent the last few months to create our own framework. Rio is the result of this effort. What My Project Does Rio is a brand new GUI framework that lets you create modern web apps in just a few lines of Python. Our goal is to simplify web and app development, so you can focus on the things you care about, instead of wasting countless hours on frustrating user interface details. We do this by following the core principles of Python that we all know and love. Python is supposed to be simple and compact - and so is Rio. There is no need to learn any additional languages such as HTML, CSS or JavaScript, because all of the UI, Logic, Components and even layouting is done entirely in Python. There‚Äôs not even a distinction between front-end and back-end. Rio handles all of the communication transparently for you. Key Features Full-Stack Web Development: Rio handles front-end and backend for you. In fact, you won't even notice they exist. Create your UI, and Rio will take care of the rest. Python Native: Rio apps are written in 100% Python, meaning you don't need to write a single line of CSS or JavaScript. Modern Python: We embrace modern Python features, such as type annotations and asynchrony. This keeps your code clean and maintainable, and helps your code editor help you out with code completions and type checking. Python Debugger Compatible: Since Rio runs on Python, you can connect directly to the running process with a debugger. This makes it easy to identify and fix bugs in your code. Declarative Interface: Rio apps are built using reusable components, inspired by React, Flutter & Vue. They're declaratively combined to create modular and maintainable UIs. Batteries included: Over 50 builtin components based on Google's Material Design Demo Video Target Audience Whether you need to build dashboards, CRUD apps, or just want to make a personal website, Rio makes it possible without any web development knowledge. Because Rio was developed from the ground up for Python programmers, it was designed to be concise and readable, just like Python itself. Comparison Rio doesn't just serve HTML templates like you might be used to from frameworks like Flask. In Rio you define components as simple dataclasses with a React/Flutter style build method. Rio continuously watches your attributes for changes and updates the UI as necessary. class MyComponent(rio.Component): clicks: int = 0 def _on_press(self) -> None: self.clicks += 1 def build(self) -> rio.Component: return rio.Column( rio.Button('Click me', on_press=self._on_press), rio.Text(f'You clicked the button {self.clicks} time(s)'), ) app = rio.App(build=MyComponent) app.run_in_browser() Notice how there is no need for any explicit HTTP requests. In fact there isn't even a distinction between frontend and backend. Rio handles all communication transparently for you. Unlike ancient libraries like Tkinter, Rio ships with over 50 builtin components in Google's Material Design. Moreover the same exact codebase can be used for both local apps and websites. We Want Your Feedback! The first alpha version of Rio is available on PyPi now: pip install rio-ui rio new my-project --template tic-tac-toe cd my-project rio run Discord GitHub Tutorial Website Let us know what you think - any feedback, ideas, or even a helping hand are hugely welcome! Just hop on our Discord server and say hello!"},
{"Title": "The new REPL in Python 3.13.0 beta 1", "Author": "u/treyhunner", "Content": "Python 3.13.0 beta 1 was released today. The feature I'm most excited about is the new Python REPL. Here's a summary of my favorite features in the new REPL along with animated gifs . The TLDR: Support for block-leveling history and block-level editing Pasting code (even with blank lines within it) works as expected now Typing exit will exit (no more Use exit() or Ctrl-D (i.e. EOF) to exit message)"},
{"Title": "Calculating Virtual Cycling Power With Python", "Author": "u/TokenChingy", "Content": "I was doing some light reading and stumbled across Steve Gribbles Power vs Speed Calculator and thought I'd give it a go at rebuilding it based on his Physics model using Python. Then I wrote an article about. Thought I'd share it with you all: Calculating Virtual Cycling Power (jasonlei.com)"},
{"Title": "InterProcessPyObjects: Fast IPC for Sharing and Modifying Objects Across Processes", "Author": "u/FI_Mihej", "Content": "InterProcessPyObjects Python package github.com/FI-Mihej/InterProcessPyObjects If you like the project, consider giving it a star on GitHub to show your support and help further development. :) pypi.org/project/InterProcessPyObjects What My Project Does InterProcessPyObjects is a part of the Cengal library. If you have any questions or would like to participate in discussions, feel free to join the Cengal Discord . Your support and involvement are greatly appreciated as Cengal evolves. This high-performance package delivers blazing-fast inter-process communication through shared memory, enabling Python objects to be shared across processes with exceptional efficiency. By minimizing the need for frequent serialization-deserialization, it enhances overall speed and responsiveness. The package offers a comprehensive suite of functionalities designed to support a diverse array of Python types and facilitate asynchronous IPC, optimizing performance for demanding applications. Target Audience This project is designed for production environments, offering a stable API suitable for developers looking to implement fast inter-process communication. Whether you're building complex systems or require robust data sharing and modification across processes, InterProcessPyObjects is ready to meet your needs. Comparison Comparison with multiprocessing.shared_memory While both InterProcessPyObjects and multiprocessing.shared_memory facilitate inter-process communication, there are several key differences to note. Unlike multiprocessing.shared_memory, InterProcessPyObjects offers the following enhancements: High-Performance Mutable Objects: Both connected processes can modify shared objects at runtime, and these changes are immediately reflected on the other side. This feature not only increases flexibility but also delivers exceptional performance, with the capability to handle up to several million changes per second. Synchronization Features: Ensures that operations are thread-safe and data integrity is maintained across processes. Message Queue: Integrates a system for queuing messages, making communication between processes more structured and reliable. Extended Type Support: Supports a broad range of data types, including custom classes, which goes beyond the basic types typically handled by multiprocessing.shared_memory. These features make InterProcessPyObjects a more robust option for developers requiring advanced inter-process communication capabilities. API State Stable. Guaranteed not to have breaking changes in the future. (see github.com/FI-Mihej/InterProcessPyObjects?tab=readme-ov-file#api-state for details) Key Features Shared Memory Communication: Enables sharing of Python objects directly between processes using shared memory. Utilizes a linked list of global messages to inform connected processes about new shared objects. Lock-Free Synchronization: Uses memory barriers for efficient communication, avoiding slow syscalls. Ensures each process can access and modify shared memory without contention. Supported Python Types: Handles various Python data structures including: Basic types: None , bool , 64-bit int , large int (arbitrary precision integers), float , complex , bytes , bytearray , str . Standard types: Decimal , slice , datetime , timedelta , timezone , date , time Containers: tuple , list , classes inherited from: AbstractSet ( frozenset ), MutableSet ( set ), Mapping and MutableMapping ( dict ). Pickable classes instances: custom classes including dataclass Allows mutable containers (lists, sets, mappings) to save basic types ( None , bool , 64 bit int , float ) internally, optimizing memory use and speed. NumPy and Torch Support: Supports numpy arrays by creating shared bytes objects coupled with independent arrays. Supports torch tensors by coupling them with shared numpy arrays. Custom Class Support: Projects pickable custom classes instances (including dataclasses ) onto shared dictionaries in shared memory. Modifies the class instance to override attribute access methods, managing data fields within the shared dictionary. supports classes with or without __dict__ attr supports classes with or without __slots__ attr Asyncio Compatibility: Provides a wrapper module for async-await functionality, integrating seamlessly with asyncio. Ensures asynchronous operations work smoothly with the package's lock-free approach. Main principles only one process has access to the shared memory at the same time working cycle: work on your tasks acquire access to shared memory work with shared memory as fast as possible (read and/or update data structures in shared memory) release access to shared memory continue your work on other tasks do not forget to manually destroy your shared objects when they are not needed already feel free to not destroy your shared object if you need it for a whole run and/or do not care about the shared memory waste data will not be preserved between Creator's sessions. Shared memory will be wiped just before Creator finished its work with a shared memory instance (Consumer's session will be finished already at this point) Examples An async examples (with asyncio): sender.py receiver.py shared_objects__types.py Receiver.py performance measurements CPU: i5-3570@3.40GHz (Ivy Bridge) RAM: 32 GBytes, DDR3, dual channel, 655 MHz OS: Ubuntu 20.04.6 LTS under WSL2. Windows 10 async with ashared_memory_context_manager.if_has_messages() as shared_memory: # Taking a message with an object from the queue. sso: SomeSharedObject = shared_memory.value.take_message()  # 5_833 iterations/seconds # We create local variables once in order to access them many times in the future, ensuring high performance. # Applying a principle that is widely recommended for improving Python code. company_metrics: List = sso.company_info.company_metrics  # 12_479 iterations/seconds some_employee: Employee = sso.company_info.some_employee  # 10_568 iterations/seconds data_dict: Dict = sso.data_dict  # 16_362 iterations/seconds numpy_ndarray: np.ndarray = data_dict['key3']  # 26_223 iterations/seconds # Optimal work with shared data (through local variables): async with ashared_memory_context_manager as shared_memory: # List k = company_metrics[CompanyMetrics.avg_salary]  # 1_535_267 iterations/seconds k = company_metrics[CompanyMetrics.employees]  # 1_498_278 iterations/seconds k = company_metrics[CompanyMetrics.in_a_good_state]  # 1_154_454 iterations/seconds k = company_metrics[CompanyMetrics.websites]  # 380_258 iterations/seconds company_metrics[CompanyMetrics.annual_income] = 2_000_000.0  # 1_380_983 iterations/seconds company_metrics[CompanyMetrics.employees] = 20  # 1_352_799 iterations/seconds company_metrics[CompanyMetrics.avg_salary] = 5_000.0  # 1_300_966 iterations/seconds company_metrics[CompanyMetrics.in_a_good_state] = None  # 1_224_573 iterations/seconds company_metrics[CompanyMetrics.in_a_good_state] = False  # 1_213_175 iterations/seconds company_metrics[CompanyMetrics.avg_salary] += 1.1  # 299_415 iterations/seconds company_metrics[CompanyMetrics.employees] += 1  # 247_476 iterations/seconds company_metrics[CompanyMetrics.emails] = tuple()  # 55_335 iterations/seconds (memory allocation performance is planned to be improved) company_metrics[CompanyMetrics.emails] = ('sails@company.com',)  # 30_314 iterations/seconds (memory allocation performance is planned to be improved) company_metrics[CompanyMetrics.emails] = ('sails@company.com', 'support@company.com')  # 20_860 iterations/seconds (memory allocation performance is planned to be improved) company_metrics[CompanyMetrics.websites] = ['http://company.com', 'http://company.org']  # 10_465 iterations/seconds (memory allocation performance is planned to be improved) # Method call on a shared object that changes a property through the method some_employee.increase_years_of_employment()  # 80548 iterations/seconds # Object properties k = sso.int_value  # 850_098 iterations/seconds k = sso.str_value  # 228_966 iterations/seconds sso.int_value = 200  # 207_480 iterations/seconds sso.int_value += 1  # 152_263 iterations/seconds sso.str_value = 'Hello. '  # 52_390 iterations/seconds (memory allocation performance is planned to be improved) sso.str_value += '!'  # 35_823 iterations/seconds (memory allocation performance is planned to be improved) # Numpy.ndarray numpy_ndarray += 10  # 403_646 iterations/seconds numpy_ndarray -= 15  # 402_107 iterations/seconds # Dict k = data_dict['key1']  # 87_558 iterations/seconds k = data_dict[('key', 2)]  # 49_338 iterations/seconds data_dict['key1'] = 200  # 86_744 iterations/seconds data_dict['key1'] += 3  # 41_409 iterations/seconds data_dict['key1'] *= 1  # 40_927 iterations/seconds data_dict[('key', 2)] = 'value2'  # 31_460 iterations/seconds (memory allocation performance is planned to be improved) data_dict[('key', 2)] = data_dict[('key', 2)] + 'd'  # 18_972 iterations/seconds (memory allocation performance is planned to be improved) data_dict[('key', 2)] = 'value2'  # 10_941 iterations/seconds (memory allocation performance is planned to be improved) data_dict[('key', 2)] += 'd'  # 16_568 iterations/seconds (memory allocation performance is planned to be improved) # An example of non-optimal work with shared data (without using a local variables): async with ashared_memory_context_manager as shared_memory: # An example of a non-optimal method call (without using a local variable) that changes a property through the method sso.company_info.some_employee.increase_years_of_employment()  # 9_418 iterations/seconds # An example of non-optimal work with object properties (without using local variables) k = sso.company_info.income  # 20_445 iterations/seconds sso.company_info.income = 3_000_000.0  # 13_899 iterations/seconds sso.company_info.income *= 1.1  # 17_272 iterations/seconds sso.company_info.income += 500_000.0  # 18_376 iterations/seconds # Example of non-optimal usage of numpy.ndarray without a proper local variable data_dict['key3'] += 10  # 6_319 iterations/seconds # Notify the sender about the completion of work on the shared object async with ashared_memory_context_manager as shared_memory: sso.some_processing_stage_control = True  # 298_968 iterations/seconds Throughput Benchmarks CPU: i5-3570@3.40GHz (Ivy Bridge) RAM: 32 GBytes, DDR3, dual channel, 655 MHz OS: Ubuntu 20.04.6 LTS under WSL2. Windows 10 Refference results (sysbench) sysbench memory --memory-oper=write run 5499.28 MiB/sec Benchmarks results table GiB/s Approach sync/async Throughput GiB/s InterProcessPyObjects (sync) sync 3.770 InterProcessPyObjects + uvloop async 3.222 InterProcessPyObjects + asyncio async 3.079 multiprocessing.shared_memory * sync 2.685 uvloop.UnixDomainSockets async 0.966 asyncio + cengal.Streams async 0.942 uvloop.Streams async 0.922 asyncio.Streams async 0.784 asyncio.UnixDomainSockets async 0.708 multiprocessing.Queue sync 0.669 multiprocessing.Pipe sync 0.469 * multiprocessing.shared_memory.py - simple implementation. This is a simple implementation because it uses a similar approach to the one used in uvloop.* , asyncio.* , multiprocessing.Queue , and multiprocessing.Pipe benchmarking scripts. Similar implementations are expected to be used by the majority of projects. Todo Connect more than two processes Use third-party fast hashing implementations instead of or in addition to built in hash() call Continuous performance improvements Conclusion This Python package provides a robust solution for inter-process communication, supporting a variety of Python data structures, types, and third-party libraries. Its lock-free synchronization and asyncio compatibility make it an ideal choice for high-performance, concurrent execution. Based on Cengal This is a stand-alone package for a specific Cengal module. Package is designed to offer users the ability to install specific Cengal functionality without the burden of the library's full set of dependencies. The core of this approach lies in our 'cengal-light' package, which houses both Python and compiled Cengal modules. The 'cengal' package itself serves as a lightweight shell, devoid of its own modules, but dependent on 'cengal-light[full]' for a complete Cengal library installation with all required dependencies. An equivalent import: from cengal.hardware.memory.shared_memory import * from cengal.parallel_execution.asyncio.ashared_memory_manager import * Cengal library can be installed by: pip install cengal https://github.com/FI-Mihej/Cengal https://pypi.org/project/cengal/ Projects using Cengal CengalPolyBuild - A Comprehensive and Hackable Build System for Multilingual Python Packages: Cython (including automatic conversion from Python to Cython), C/C++, Objective-C, Go, and Nim, with ongoing expansions to include additional languages. (Planned to be released soon) cengal_app_dir_path_finder - A Python module offering a unified API for easy retrieval of OS-specific application directories, enhancing data management across Windows, Linux, and macOS cengal_cpu_info - Extended, cached CPU info with consistent output format. cengal_memory_barriers - Fast cross-platform memory barriers for Python. flet_async - wrapper which makes Flet async and brings booth Cengal.coroutines and asyncio to Flet (Flutter based UI) justpy_containers - wrapper around JustPy in order to bring more security and more production-needed features to JustPy (VueJS based UI) Bensbach - decompiler from Unreal Engine 3 bytecode to a Lisp-like script and compiler back to Unreal Engine 3 bytecode. Made for a game modding purposes Realistic-Damage-Model-mod-for-Long-War - Mod for both the original XCOM:EW and the mod Long War. Was made with a Bensbach, which was made with Cengal SmartCATaloguer.com - TagDB based catalog of images (tags), music albums (genre tags) and apps (categories) License Licensed under the Apache License, Version 2.0."},
{"Title": "AzuracastPy: An Unofficial Python Wrapper for the Azuracast API.", "Author": "u/ARandomBoiIsMe", "Content": "Source code What My Project Does: It acts as a wrapper for the AzuraCast API, providing custom functions and classes for more straightforward use of the API in python projects. Target Audience: Python users who are interested in programmatically interacting with online radios hosted on AzuraCast . Comparison: The idea of API Wrappers is not new. However, I noticed that the only existing wrapper for this API is written in PHP , which I am not experienced with. I created this project so I, and other python programmers by extension, could have an easier time working with the API. This is my first \"major\" programming project, so thoughts and feedback are welcome and greatly appreciated. PS: Shoutout to PRAW for \"inspiring\" basically everything about the project's structure and functionality."},
{"Title": "diskcache: This key-value store library is faster than Redis and Memcached üòÆ (built by Grant Jenks)", "Author": "u/RevolutionaryPen4661", "Content": "PYPI (From the README, Released Last Year, Edited by Grammarly) Github pip install diskcache The cloud-based computing of 2024 puts a premium on memory. Gigabytes of space are left on disks as processes vie for memory. Memcached (and sometimes Redis) is used as a cache among these processes. Wouldn‚Äôt it be nice to leverage empty disk space for caching? Django is Python‚Äôs most popular web framework and has several caching backends. Unfortunately, the file-based cache in Django is essentially broken. The culling method is random and large caches repeatedly scan a cache directory which slows linearly with growth. Can you allow it to take sixty milliseconds to store a key in a cache with a thousand items? Is it that fast? In [1]: import pylibmc In [2]: client = pylibmc.Client(['127.0.0.1'], binary=True) In [3]: client[b'key'] = b'value' In [4]: %timeit client[b'key'] 10000 loops, best of 3: 25.4 ¬µs per loop In [5]: import diskcache as dc In [6]: cache = dc.Cache('tmp') In [7]: cache[b'key'] = b'value' In [8]: %timeit cache[b'key'] 100000 loops, best of 3: 11.8 ¬µs per loop"},
{"Title": "Who is using quart framework for microservices?", "Author": "u/Dry_Raspberry4514", "Content": "I am using quart framework ( https://quart.palletsprojects.com ) for a number of microservices in a SaaS application. However, I hardly hear anything about this framework on any social media platform which seems to be dominated by FastAPI. Also I'm unable to find which all projects/companies are using this framework. All this is leading to anxiety around the future of this project. Are there any well known projects / companies which are using this framework for microservices?"},
{"Title": "Why is Plotly so cumbersome to tweak?", "Author": "u/olive_oil_for_you", "Content": "I made this visualisation with this code . I have three questions: Is Plotly supposed to be this cumbersome to tweak? Would other libraries require the same amount of code to add the details I did? Can my code be reduced in size? Maybe it's me who is complicating things with Plotly and there are easier ways to do what I am doing. Any R enthusiast who can tell me how much shorter this code would look like with ggplot2? I asked ChatGPT but the result was garbage. Bonus question: This took me an entire morning. Is it normal to be \"that slow\" to plot a simple figure?"},
{"Title": "I connected LLM to Python runtime and generated unit-tests (OpenSource)", "Author": "u/Financial_Muffin396", "Content": "Hi all, I initially started this adventure by trying to automate bug fixes with the help of LLMs. However, I received feedback saying the fixes aren't always correct, leading to the question: why bother reviewing PRs that might add more issues? (It's really hard for LLMs to say \"I don't know\"). So, I decided to focus on reliability perfecting unit tests. The source code is available at: https://github.com/CaptureFlow/captureflow-py What My Project Does: It incorporates a tracer client-side Python library and a backend that accumulates such traces and is capable of proposing code improvements and generating tests for your repository. It traverses the execution graph, extracts relevant parts, enriches them with implementation data from the GitHub API, and then generates tests with the help of GPT4. Target Audience: Python users interested in discovering what LLMs can achieve when given detailed runtime information from your app. Generally, this approach somewhat reverses the concept of TDD, but in my day job I deal with many legacy apps that have poor test coverage, and having it really helps. I suspect I‚Äôm not alone in finding value in this. Comparison: I think idea of using LLMs to generate tests is not new, but generating them actually based on application's performance is inspired by Jane Street's article on how they automated test boilerplate creation and recent Facebook's research paper . Disclaimer: More work needs to be done to make it work for any Python app and not just a subset of FastAPI servers. I'm curious if you folks would find it useful. Example: you can check this PR for reference. feedback / stars / contributions are welcome."},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Seeking Your Input: Let's Co-Create FreeCodeCamp for python together", "Author": "u/globalwarming_isreal", "Content": "Hello all, This is probably my first post here. I usuallly lurk around here and Django subreddits. I've been brewing up an idea and I need your input before I take the plunge! Picture this: a website like FreeCodeCamp but for python and related technologies, a learning oasis where anyone can kickstart their journey from Python newbie to job-ready pro, and it's all free! But here's the thing, I want this to be our platform, crafted with your needs and dreams in mind. So, before I start, I need to know: would this be something that gets you excited? Imagine quizzes helping you find your starting point, interactive challenges that keep you in the zone, and a supportive community to cheer you on every step of the way. Plus, videos, written tutorials, and a progress tracker to keep you motivated! What would make you go, \"Wow, I need this in my life!\"? What features would you love to see? Any suggestions or wild ideas ? My aim is to give back to the community by assisting new learners in navigating common pitfalls when they start their Python journey"},
{"Title": "Tutorial on Creating Useful Data Visuals with Python seaborn and matplotlib libraries", "Author": "u/jgloewen", "Content": "With the current global deluge of data and information, there has never been a more important to visualize your data in a clear and simple manner. Python is a terrific tool to help us do this. The key to this lies in choosing the the right data visualization techniques to tell the most interesting and relevant story. Three useful visuals are: small multiples heat maps stacked area charts In this tutorial, using pandas, seaborn, and matplotlib.pyplot, we create the Python code for each data visual Link to free Tutorial"},
{"Title": "Rethinking String Encoding: a 37.5% space efficient string encoding than UTF-8 in Apache Fury", "Author": "u/Shawn-Yang25", "Content": "In rpc/serialization systems, we often need to send namespace/path/filename/fieldName/packageName/moduleName/className/enumValue string between processes. Those strings are mostly ascii strings. In order to transfer between processes, we encode such strings using utf-8 encodings. Such encoding will take one byte for every char, which is not space efficient actually. If we take a deeper look, we will found that most chars are lowercase chars, ., $ and _, which can be expressed in a much smaller range 0~32. But one byte can represent range 0~255, the significant bits are wasted, and this cost is not ignorable. In a dynamic serialization framework, such meta will take considerable cost compared to actual data. So we proposed a new string encoding which we called meta string encoding in Fury. It will encode most chars using 5 bits instead of 8 bits in utf-8 encoding, which can bring 37.5% space cost savings compared to utf-8 encoding. For string can't be represented by 5 bits, we also proposed encoding using 6 bits which can bring 25% space cost savings More details can be found in: https://fury.apache.org/blog/fury_meta_string_37_5_percent_space_efficient_encoding_than_utf8 and https://github.com/apache/incubator-fury/blob/main/docs/specification/xlang_serialization_spec.md#meta-string"},
{"Title": "Python script to convert Spotify Artists to Playlists", "Author": "u/RJW-20", "Content": "I've made my first bit of useful software and I wanted to share it here. I'd love some feedback (and it would be amazing to hear if someone has used it!) What My Project Does: Using the third party requests package, the script interacts with the Spotify web API to request all albums from the given Artist, then all the tracks from all of those albums. It then goes through the list to remove any duplicates and also tries to remove any unwanted versions (only done by examining the name of the track, since Spotify does not attribute a version type to its tracks). Once that's done a playlist is then created on your Spotify account with the name of the Artist and all the tracks are posted there in chronological (essentially per album) order. Target Audience: Anyone who struggles like me when they find a new Artist and they want to listen to every conceivable song from them! Link to GitHub: https://github.com/RJW20/spotify-artist-to-playlist"},
{"Title": "List of Sites that Packages Need to Connect to?", "Author": "u/BullCityPicker", "Content": "I'm doing most of my work behind a government firewall, and I'm having trouble connecting to certain sites.   I can do the usual \"pip\" installs just fine, but I'm talking about packages that need to download data to do their job.  An example is the NLTK (Natural Language Toolkit) package, which downloads dictionaries, lookup tables for sentiment analysis, and so on.  I know what sites to open up for that particular problem (pastebin.com and nltk.org), but I wonder if anybody's made a list of such sites for different packages. I can ask for the two sites I know about to be opened up, but I'd like to have a more comprehensive list so I don't have to go through the red tape multiple times."},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Pip 24.1 beta released, and it's a big one", "Author": "u/zurtex", "Content": "I'd like to call attention to pip 24.1 beta asit is unusual for the pip team to release betas: https://pip.pypa.io/en/latest/news/#b1-2024-05-06 https://pypi.org/project/pip/24.1b1/ You can install with: python -m pip install pip==24.1b1 In particular they have upgraded their vendored version of packaging from 21.3 to 24.0, this was a big effort and fixed many bugs , included significant performance improvements, and will allow pip to support free threaded packages . However, it also means legacy versions and specifiers are no longer compatible with pip. Because this was such a big land the pip maintainers have released a beta in the hopes people will test their workflows, and if something fails in an expected way report their steps as best as possible back to pip: https://github.com/pypa/pip/issues I've been testing, and contributing a little bit, to the improved performance in this release, it is most noticeable on large dependency trees or long backtracking. For example, a dry run of \"apache-airflow[all]\" using cached packages on my machine goes from ~418 seconds to ~185 seconds."},
{"Title": "How Python Asyncio Works: Recreating it from Scratch", "Author": "u/jpjacobpadilla", "Content": "Do you understand how asyncio works behind the scenes? Read this article and see how you can use Python generators to create your own version of asyncio, and then use the __await__ dunder method to use the async/await keywords to come full circle! https://jacobpadilla.com/articles/recreating-asyncio"},
{"Title": "Build tool support for PySide / PyQt", "Author": "u/jmacey", "Content": "Just interested in how people approach this, typically I just use VSCode or QtCreator to build simple projects. However I now want to automate some of the build process such as running uic, and rcc. I've tried to use CMake but can't seem to get it to work without a lot of custom scripting (for example the AUTOUIC etc functions need c++ projects), can't see any info on running uic in QtCreator (which would be ideal but python support is really just an after thought). I could write some Makefiles but this is a little ad-hoc and also confuses the IDE's (and at the end of the day I want a simple process for my students to use and I already teach cmake for C++ dev). So I guess my questions are what workflows do people use, can you recommend any tools to help, or do you just have a per project script to run uic and rcc? (I may cross post this in both qt and python subreddits as I'm not sure where it fits best)"},
{"Title": "relax-py - Web framework for htmx with hot module replacement", "Author": "u/MindLopsided4430", "Content": "Excited to finally showcase this! It's still pretty rough around the edges, but I'm finally happy enough with the feature set and curious to see what the community thinks about a framework like this. Code: github.com/crpier/relax-py Documentation: crpier.github.io/relax-py What My Project Does relax-py is a Python framework for building full-stack applications with htmx It provides tools for writing HTML in a manner similar to simple_html (which also inspired the decision to use standard Python to write HTML, rather than use Jinja2 or to make something like templ work in Python) It has: Hot Module Replacement (meaning, when you update the code that generates HTML templates, the browser also updates instantly) - see the video in the documentation for a quick demo of this URL resolution with type hinting - you can get the URL of an endpoint to use in your templates by using the function that handles that URL, and get help from static typing (for example, for putting path parameters in the URL) Helpers for dependency injection In essence, this framework is just a bunch of decorators and functions over starlette , meaning everything that starlette has can be used alongside the framework. Target Audience Developers interested in building web applications with htmx that like new shiny things and static typing support Comparison As far as I know, the only other backend framework that has Hot Module Replacement is turbo in Ruby on Rails, but there might be something I missed. As for other points of comparison with other frameworks: Django relax is less opinionated about what's done in the backend (.e.g there is preference to what ORM is used) using standard Python code to generate HTML has nicer static typing the URL resolution is more complex and provides errors in the IDE by way of static typing the component decorator provides nicer ways to reuse template functions and helpers for interoperability with JavaScript templ in Go templ allows writing actual HTML in go files, but requires an additional compilation step plugins for whatever IDE/code editor is used are needed parsing templ files FastAPI (with something to generate HTML like simple_html or Jinja2) since FastAPI is built for RESTful APIs, it lacks niceties like URL resolution, or a mechanism to manage the sprawling mess of interconnected HTML components that apps tend to develop dependency injection in FastAPI is \"encouraged\" to happen in the path functions, but in relax it's meant to happen at any level of the app (either in path functions, or in service-level functions, or in util functions) simple_html (with a backend like Flask or FastAPI): the main differences between simple_html and the relax.html module are that CSS classes are provided as a list of strings - this makes it easier to reuse them in different components, and will make it easier to implement other helpers in the future, like a Python version of tailwind-merge , or a formatter that sorts tailwind classes htmx-related attributes are included in the elements inserting children to an HTML element is done after instantiating the element, making it easier to reuse components Here's the code again: github.com/crpier/relax-py There's more details in the documentation: crpier.github.io/relax-py While this framework is definitely not production ready, in the \"Other\" page of the documentation there's an example app built with this framework, which shows how it can be used in conjuction with some real-life scenarios (production environment for tailwind with plugins, working in a bunch of interactivity with JavaScript, in either separate js files and inline scripts, Dockerfiles and deployments, authentication and authorization, configuration etc.) Please let me know what you think (are there better alternatives, is writing HTML in standard Python a deal-breaker, is investing in making something templ in Python worth it?) Hope you're intrigued by this!"},
{"Title": "python-oracledb 2.2 and the VECTOR type in Oracle Database 23ai", "Author": "u/cjbj", "Content": "python-oracledb 2.2, the Oracle Database driver, has been released with support for Oracle Database 23ai features such as the VECTOR and BOOLEAN data types, Implicit Connection Pooling, and improved connection performance.  See the release announcement ."},
{"Title": "I've started writing Python bindings for lexertl", "Author": "u/BenHanson", "Content": "See https://github.com/BenHanson/pylexertl I will see about registering as an official library when I am happy I have completed all the bindings. I added all the missing functions for the rules objects today, so things are in reasonable shape already. My python experience has been limited up until now, but it is big for my new role. I have a runtime parser generator https://github.com/BenHanson/parsertl17 which I also plan to add bindings for. I hope this is of interest to somebody! What My Project Does Allows you to build lexical analysers at runtime and use them to lex text (in this case utf-8) Target Audience The C++ library has been used in production for over 10 years. Comparison I'm not aware of any competing library."},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Introducing PgQueuer: A Minimalist Python Job Queue Built on PostgreSQL", "Author": "u/GabelSnabel", "Content": "What My Project Does PgQueuer is a Python library designed to manage job queues using  PostgreSQL features. It leverages PostgreSQL's native LISTEN/NOTIFY, along with advanced locking mechanisms, to handle job queues efficiently. This allows for real-time job processing, concurrency, and reliable task execution without the need for a separate queuing system. Target Audience PgQueuer is ideal for developers and teams who already use PostgreSQL in their projects and are looking for a simple, integrated way to handle background tasks and job queues. It's designed for production use, offering a dependable solution that scales seamlessly with existing PostgreSQL databases. Comparison Unlike many other job queue solutions that require additional services or complex setups (such as Redis or RabbitMQ), PgQueuer operates directly within PostgreSQL. This removes the overhead of integrating and maintaining separate systems for job management. How PgQueuer stands out Integration Simplicity : Integrates directly with existing PostgreSQL setups without additional infrastructure. Efficiency : Uses PostgreSQL‚Äôs FOR UPDATE SKIP LOCKED for high concurrency, allowing multiple workers to process tasks simultaneously without conflict. Real-time Updates : Utilizes PostgreSQL's LISTEN/NOTIFY for immediate job processing updates, reducing latency compared to polling-based systems. Request for Feedback on Useful Features Im always looking to improve PgQueuer and make it more useful for our users. If you have any features you'd like to see, or if there's something you think could be improved, please let me know! Your feedback is invaluable! Share your thoughts, suggestions, or feature requests either here in the comments or via GitHub ."},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Reboot Your Router with a Python Script", "Author": "u/SAV_NC", "Content": "Hello r/python , I've developed a Python script that allows you to reboot your router remotely via SSH! This script handles the countdown and checks when the router is back online after a reboot. What My Project Does: Key Features: Automated Router Reboot: Remotely trigger a reboot of your router. Monitoring: After sending the reboot command, the script counts down from 350 seconds and starts checking the router's status by pinging it after the first 100 seconds have passed. Flexibility: You can pass arguments dynamically (router IP, username, password, and port) or use hardcoded values within the script. Method of Execution: To execute the script from the command line: python3 reboot-router.py --ip <router_ip> --username <username> --password <password> --port <port_number> Default values are set, but it's highly recommended to pass arguments to the script for security reasons. Target Audience: This script is intended for: Tech Enthusiasts and Home Users who enjoy managing their home network setups and want a quick way to automate router management. Requirements: Required Modules and Programs: Python 3: The script is written in Python 3. Ensure you have Python 3.6 or newer installed. subprocess and argparse modules: These are standard libraries in Python and should be available with your Python installation. sshpass: This utility is used for noninteractive password authentication with SSH. Install it using your package manager, e.g., sudo apt-get install sshpass for Debian/Ubuntu. Important Router Configuration: Before using this script, make sure your router is configured to: Enable SSH Access: Ensure SSH is turned on and configured to accept password authentication. This setting is usually found under the Administration tab in your router settings. Allow ICMP Echo (Ping) Requests: Some routers disable ICMP Echo requests by default for security. You must enable Respond ICMP Echo (ping) Request from WAN under the Firewall tab. Comparison: Unlike many GUI-based tools, this script provides a simple, lightweight command-line solution easily integrated into larger automation workflows or triggered manually without logging into the router interface. For People New to Python: If you're new to scripting or network management, be cautious about storing sensitive information like passwords directly in scripts. While hardcoded values can be used for ease and demonstration, the best practice is to pass these securely as arguments to prevent exposure. Access to the script You can access the script on my GitHub page here Feel free to use, modify, and share this script! I look forward to your feedback and enhancements! Cheers -J"},
{"Title": "Opinions sought: Modernising the Apache NiFi Python client", "Author": "u/Samausi", "Content": "Hello folks, I am the author of NiPyAPI , the defacto Python client for the Apache NiFi project. For those unfamiliar, Apache NiFi is an open-source ETL tool designed around the flow-based programming paradigm. It excels at continuously moving and managing arbitrary data flows between disparate systems with low latency at large scale, and is often contrasted with Airflow. Amongst many features, NiFi allows for Command & Control via a native UI, enabling live edits to data routing and transformation. NiFi also enforces an API-first approach, and produces a swagger definition during code compilation, which allowed creation of a Python client to enable automated Data Flow testing and a Design & Deploy approach by running it through Swagger Codegen 2.3 tool shaped by mustache templates. This produces a very verbose low-level client, which I then leverage in higher-level operational functions and demo scripts in the library. However it's always bugged me that this produces literally 10's of thousands of lines of boilerplate, so potentially replacing the low level client with something more modern would ideally massively reduce the codebase without impacting the high-level functionality of the library. Now, I first started it mid last decade with Python2 for broad compatibility and times have moved on significantly (as has my skill with Python) but it's finally time to drop Python2 support and move to a better build & release process, which opens the door to a larger update, and I am frankly not across modern best-practices around ClientGen so I come seeking r/Python 's advice: How would you modernise this venerable artefact? I could simply move to a newer version of Swagger Codegen, or move into the OpenAPI Generator, I could get fancy with something like Fern - or I could leave well enough alone and just accept that the tons of boilerplate are at least very readable."},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Python Test 220: Getting the most out of PyCon, including juggling - Rob Ludwick", "Author": "u/variedthoughts", "Content": "Listen at https://podcast.pythontest.com/episodes/220-juggling-pycon Even if you never get a chance to go to PyCon, I hope this interview helps you get a feel for the welcoming aspect of the Python community. The juggling at PyCon is one of the inspirations for PythonPeople.fm , one of PythonTests's sibling podcasts. Do you have any conference tips to add?"},
{"Title": "Suggestions for python libraries to contribute to", "Author": "u/JimJimBerry", "Content": "Hey, python folks ! I have been coding in python for around 3 years, 2 years professionally. I have worked with asyncio, typing and other stuff that is needed to build a server. I was looking for a small but impactful enough open source core python library/application to work on. I tried cpython but it seems to be beyond my capability at the moment. As for my interests I was interested in lower level stuff as well as libraries like asyncio and celery. Any suggestions for libraries that could use a bit of help and teach me some stuff as well would be appreciated"},
{"Title": "Project: Simple Interactive Python Streamlit Maps With NASA GIS Data", "Author": "u/jgloewen", "Content": "Python Streamlit is terrific for putting together interactive dashboards. Combined with the geopandas library, streamlit can easily display GIS data points on a map for you. Forest fires in my home province of British Columbia, Canada have been really bad recently. NASA has a terrific dataset that keeps track of forest fires by country. Can I use Streamlit to access this dataset and display a map off all the fires within a certain area (BC) for a particular time frame (2021)? And can I give the user the ability to choose a month? You bet! Let me step you through how! FREE tutorial (with code): https://johnloewen.substack.com/p/simple-interactive-python-streamlit"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "New book! The Quick Python Book, Fourth Edition by Naomi Ceder", "Author": "u/ManningBooks", "Content": "Hello everybody, Thank you for having us here, and a huge \"Thank you\" to the moderators for letting us post. We have just released the latest edition of The Quick Python Book by the one-and-only Naomi Ceder, and I wanted to share that news with the community. Many of you are already familiar with Naomi's work and her massive contributions to the world of Python programming language. The Quick Python Book has aided over 100,000 developers in mastering Python. The Fourth Edition of the book has been revised to include the latest features, control structures, and libraries of Python, along with new coverage of working with AI-generated Python code. Naomi, the author, has beautifully balanced the details of the language with the insights and advice required to accomplish any task. Her personal touch has made learning Python an enjoyable experience for countless developers. üìö You can find the book here: https://mng.bz/aEQj üìñ Get into the liveBook: https://mng.bz/gvee And last but not the least, get 46% off with code: receder46 Hope you find the book helpful. Thank you. Cheers,"},
{"Title": "typedattr: Autocompletion and typechecking for CLI script arguments, using standard argparse syntax", "Author": "u/gings7", "Content": "Excited to share my pypi package typedparser I have been working on for around 1 year now. What My Project Does : It enables writing CLI scripts and create an \"args\" variable with autocompleted members and type checks, but still keeps the simple and universally understood syntax of the stdlib argarse module. Target Audience : For stability, I battletested it in my research projects and added automatic builds as well as 80%+ test coverage. So I believe it is pretty stable. Comparison : For typing functionality it uses the attrs package as backend. It also provides some additional features for object and dictionary manipulation. Of course there are many other CLI argument packages out there, but this one stands out in that it tries to keep the syntax of the argparse standard library as much as possible, making it easy for others to figure out what your script does. Check it out and let me know what you think."},
{"Title": "Dash vs Reflex vs Others", "Author": "u/Sea_Split_1182", "Content": "Where can I find a decent comparison (pros and cons) of these 5 solutions? They seem to be solving the same problem, which is, afaiu, separating the frontend ‚Äòannoyance‚Äô from Python scripting  / math. ‚Å†Reflex (used to be called Pynecone) https://reflex.dev ‚Å†Streamlit https://streamlit.io ‚Å†Gradio https://gradio.app ‚Å†Dash https://dash.plotly.com ‚Å†Panel https://panel.holoviz.org/ ‚Å†Anvil https://anvil.works/ Quarto My use case: user access the web app, choose some parameters, selects things that go or not into a model. Python returns results of my math. Needs to be somewhat eye-candy and I need to use a lot of pictures to get the user input (i.e. ‚Äúwhich of these figures you like most? 1,2,3. User clicks on ‚Äú3‚Äù, 3 is considered in the model."},
{"Title": "I made a python package that can parse Excel Formula Strings into dictionary structures!", "Author": "u/MPGaming9000", "Content": "What my project does: It basically takes a formula string like you'd get from Openpyxl like \"=SUM(A1:B2)\" and breaks it all out into a dictionary structure for you to then navigate through, modify, and then reformat that modified structure back into an excel friendly formula string again! Target Audience: (People who modify Excel formula strings in automated spreadsheet modification scripts. Or people who need to analyze formulas in a spreadsheet to do some kind of logic based on that analysis). Disclaimer: For most people some simple regex pattern matching and str replaces would be fine to modify formulas but if you need a more structured approach to working with these strings, this package has you covered! How does it differ compared to other projects: There are libraries like Openpyxl that allow you to tokenize and translate formulas but that's currently where it ends. It doesn't allow you to systematically parse out a formula and replace those pieces and add new structures and what not into it. Currently the best you can really do is translate formulas and anything other than that would need to rely on regex string matching logic or string replacements. (Which still would be fine for most people, but this just adds another layer of organization and scalability to the format). More info about it here: https://github.com/Voltaic314/ExcelFormulaParser To install, just do: pip install ExcelFormulaParser Thank you for reading this!! Hope you guys find it useful if you're ever systematically modifying (or analyzing) spreadsheets!"},
{"Title": "Hatch v1.10.0 - UV support, new test command and built-in script runner", "Author": "u/Ofekmeister", "Content": "Hello everyone! I'd like to announce version 1.10.0: https://hatch.pypa.io/latest/blog/2024/05/02/hatch-v1100/ Feel free to provide any feedback either here or as a discussion on the repo: https://github.com/pypa/hatch"},
{"Title": "What does your python development setup look like?", "Author": "u/Working_Noise_6043", "Content": "I'd like to explore other people's setup and perhaps try need things or extra tools. What kind IDE, any extra tools to make it easier for you, etc. Looking forward to everyone's responses!"},
{"Title": "Tutorial on Building a Server-to-Server Zoom App with Python", "Author": "u/SleekEagle", "Content": "I made a tutorial on how to build a server-to-server Zoom OAuth application using Python . This application can transcribe Zoom meeting recordings, print the transcripts to the terminal, and save the transcripts as text files. video tutorial repo written tutorial This tutorial covers: Setting up OAuth authentication for server-to-server apps Utilizing the Zoom API to access recordings Implementing automatic transcription using Python"},
{"Title": "Multipart File Uploads to S3 with Python", "Author": "u/tylersavery", "Content": "I created this tutorial after overcoming a difficult challenge myself: uploading 5GB+ files to AWS. This approach allows the browser to securely upload directly to an S3 bucket without the file having to travel through the backend server. The implementation is written in python (backend) and vanilla js (frontend)."},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Suggestions for a self-hosted authentication as a service?", "Author": "u/FlyingRaijinEX", "Content": "I have a simple backend REST API service that is serving a few ML models. I have made it \"secured\" by implementing an API key in order call those endpoints. I was wondering, how common it is for people to use services that can be self-hosted as their authentication/authorization. If it is common and reliable, what are the best options to go for? I've read that building your own authentication/authorization service with email verification, password reset, and social auth can be a pain. Also, did some googling and found this General - Fief . Has anyone ever tried using this? If so, how was the experience? Thanks in advance."},
{"Title": "One pytest marker to track the performance of your tests", "Author": "u/toodarktoshine", "Content": "Hello Pythonistas! I just wrote a blog post about measuring performance inside pytest test cases. We dive into why it‚Äôs important to test for performance and how to integrate the measurements in the CI. Here is the link to the blog: https://codspeed.io/blog/one-pytest-marker-to-track-the-performance-of-your-tests"},
{"Title": "How to create architecture diagrams from code in Jupyter Notebook", "Author": "u/writer_on_rails", "Content": "Hello world,I wrote an article about creating diagrams from code on Jupyter Notebook inside VS Code. It will give you a brief on the setup and also an overview of concepts. Within 5 minutes, you should be able to start making cool architecture diagrams. [TO MODERATOR: This link does not contain any paywalled or paid content. All the contents are available for free] Article link: https://ashgaikwad.substack.com/p/how-to-create-architecture-diagrams"},
{"Title": "Starter Code for a LLM-based AI Assistant", "Author": "u/2bytesgoat", "Content": "Hey everyone üëã TL;DR Since everyone is talking about the Humane AI Pin and the Rabbit R1, I decided to make a short 5 minute tutorial on how people can setup and customize their own little AI assistant on their machine. I've uploaded a video tutorial here: https://www.youtube.com/watch?v=2fD_SAouoOs&ab_channel=2BytesGoat And the Github code is here: https://github.com/2BYTESGOAT/AI-ASSISTANT Longer version What my project does: It's the starter code for an AI assistant that you can run locally. More precisely, it's a ChatGPT / Llama 2 agent that has access to Google Search and can get businesses nearby based on your location. The tool can be easily extended to support other APIs. Target audience : Pythoneers that are curious about LLMs and LLM related libraries. Comparison : It was inspired by projects such as the Humane AI Pin and the Rabbit R1. Though it's a inferior version to those, it serves more as a playground for people to develop their own AI assistants."},
{"Title": "The Python on Microcontrollers (and Raspberry Pi) Newsletter, a weekly news and project resource", "Author": "u/HP7933", "Content": "The Python on Microcontrollers (and Raspberry Pi) Newsletter: subscribe for free With the Python on Microcontrollers newsletter, you get all the latest information on Python running on hardware in one place! MicroPython, CircuitPython and Python on single Board Computers like Raspberry Pi & many more. The Python on Microcontrollers newsletter is the place for the latest news. It arrives Monday morning with all the week‚Äôs happenings. No advertising, no spam, easy to unsubscribe. 10,958 subscribers - the largest Python on hardware newsletter out there. Catch all the weekly news on Python for Microcontrollers with adafruitdaily.com . This ad-free, spam-free weekly email is filled with CircuitPython, MicroPython, and Python information that you may have missed, all in one place! Ensure you catch the weekly Python on Hardware roundup‚Äì you can cancel anytime ‚Äì try our spam-free newsletter today! https://www.adafruitdaily.com/"},
{"Title": "How to auto-instrument Python servers w/ OpenTelemetry for performance & error monitoring", "Author": "u/__boba__", "Content": "Hi everyone! We've recently written up a guide for anyone running a Python server (ex. Flask, Django, FastAPI) to instrument their app to debug slow downs and errors using the CNCF OpenTelemetry project and their Python instrumentation package. It's really straightforward to get started with just a few lines of added code and commands to any Python project, so hopefully helps some people out as they're looking to add better instrumentation to their servers. Here's the tutorial: https://www.hyperdx.io/blog/opentelemetry-python-server-auto-instrumentation"},
{"Title": "k8sAI - my open-source GPT CLI tool for Kubernetes!", "Author": "u/Wild_Plantain528", "Content": "What my project does: I wanted to share an open-source project I‚Äôve been working on called k8sAI. It‚Äôs a personal AI Kubernetes expert that can answer questions about your cluster, suggests commands, and even executes relevant kubectl commands to help diagnose and suggest fixes to your cluster, all in the CLI! Target Audience: As a relative newcomer to k8s, this tool has really streamlined my workflow. I can ask questions about my cluster, k8sAI will run kubectl commands to gather info, and then answer those question. It‚Äôs also found several issues in my cluster for me - all I‚Äôve had to do is point it in the right direction. I‚Äôve really enjoyed making and using this so I thought it could be useful for others. Added bonus is that you don‚Äôt need to copy and paste into ChatGPT anymore! k8sAI operates with read-only kubectl commands to make sure your cluster stays safe. All you need is an OpenAI API key and a valid kubectl config. Start chatting with k8sAI using: $ pip install k8sAI $ k8sAI chat or to fix an issue: $ k8sAI fix -p=\"take a look at the failing pod in the test namespace\" Would love to get any feedback you guys have! Here's the repo for anyone who wants to take a look Comparison: I found a tool (k8sGPT) that I enjoyed using, but I felt it was still missing a few pieces on the chatbot side. You can't chat back and forth with k8sGPT and it doesn't suggest commands for you to execute, so I decided to make this."},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "PkgInspect - Inspect Local/External Python Packages", "Author": "u/yousefabuz", "Content": "GitHub What My Project Does PkgInspect is a comprehensive tool designed to inspect and compare Python packages and Python versions effortlessly. It equips users with a comprehensive set of tools and utility classes to retrieve essential information from installed Python packages, compare versions seamlessly, and extract various details about Python installations with ease. Target Audience Developers and Python enthusiasts looking to streamline the process of inspecting Python packages, comparing versions, and extracting vital information from Python installations will find PkgInspect invaluable. Many current modules such as importlib_metadata and pkg_resources are fairly limited on what items can be inspected and retrieved for a specified python package. Also noticed pkg_resources has also deprecated some of its important retrieval methods. Comparison PkgInspect stands out from other Python package inspection tools due to its robust features. Unlike traditional methods that require manual inspection and comparison, PkgInspect automates the process, saving developers valuable time and effort. With PkgInspect , you can effortlessly retrieve package information, compare versions across different Python installations, and extract crucial details with just a few simple commands. Key Features Inspect Packages : Retrieve comprehensive information about installed Python packages. Compare Versions : Seamlessly compare package data across different Python versions. Retrieve Installed Pythons : Identify and list installed Python versions effortlessly. Inspect PyPI Packages : Gather detailed information about packages from the Python Package Index (PyPI). Fetch Available Updates : Stay up-to-date with available updates for a package from the current version. List Inspection Fieldnames : Access a list of available fieldnames for package inspection. Retrieve Package Metrics : Extract OS statistics about a package effortlessly. Fetch GitHub Statistics : Retrieve insightful statistics about a package from GitHub effortlessly. Retrieve all Python Packages : Easily list all installed Python packages for a given Python version. Main Components Core Modules PkgInspect : Inspects Python packages and retrieves package information. PkgVersions : Retrieves and compares package data across different Python versions. PkgMetrics : Extracts OS statistics about a package. Functions inspect_package : Inspects a Python package and retrieves package information. inspect_pypi : Inspects a package from the Python Package Index (PyPI). get_available_updates : Fetches available updates for a package from the current version. get_installed_pythons : Identifies and lists installed Python versions. get_version_packages : Lists all installed Python packages for a given Python version. pkg_version_compare : Compares package data across different Python versions. Inspection Field Options Any other field name will be treated as a file name to inspect from the packages' site-path directory. - `short_meta` (dict[str, Any]): Returns a dictionary of the most important metadata fields. - If only one field is needed, you can use any of the following metadata fields. - Possible Fields instead of `short_meta`: - `Metadata-Version` (PackageVersion) - `Name` (str) - `Summary` (str) - `Author-email` (str) - `Home-page` (str) - `Download-URL` (str) - `Platform(s)` (set) - `Author` (str) - `Classifier(s)` (set) - `Description-Content-Type` (str) - `short_license` (str): Returns the name of the license being used. - `metadata` (str): Returns the contents of the METADATA file. - `installer` (str): Returns the installer tool used for installation. - `license` (str): Returns the contents of the LICENSE file. - `record` (str): Returns the list of installed files. - `wheel` (str): Returns information about the Wheel distribution format. - `requested` (str): Returns information about the requested installation. - `authors` (str): Returns the contents of the AUTHORS.md file. - `entry_points` (str): Returns the contents of the entry_points.txt file. - `top_level` (str): Returns the contents of the top_level.txt file. - `source_file` (str): Returns the source file path for the specified package. - `source_code` (str): Returns the source code contents for the specified package. - `doc` (str): Returns the documentation for the specified package. - `Pkg` Custom Class Fields - `PkgInspect fields`: Possible Fields from the `PkgInspect` class. - `site_path` (Path): Returns the site path of the package. - `package_paths` (Iterable[Path]): Returns the package paths of the package. - `package_versions` (Generator[tuple[str, tuple[tuple[Any, str]]]]): Returns the package versions of the package. - `pyversions` (tuple[Path]): Returns the Python versions of the package. - `installed_pythons` (TupleOfPkgVersions): Returns the installed Python versions of the package. - `site_packages` (Iterable[str]): Returns the site packages of the package. - `islatest_version` (bool): Returns True if the package is the latest version. - `isinstalled_version` (bool): Returns True if the package is the installed version. - `installed_version` (PackageVersion): Returns the installed version of the package. - `available_updates` (TupleOfPkgVersions): Returns the available updates of the package. - `PkgVersions fields`: Possible Fields from the `PkgVersions` class. - `initial_version` (PackageVersion): Returns the initial version of the package. - `installed_version` (PackageVersion): Returns the installed version of the package. - `latest_version` (PackageVersion): Returns the latest version of the package. - `total_versions` (int): Returns the total number of versions of the package. - `version_history` (TupleOfPkgVersions): Returns the version history of the specified package. - `package_url`: Returns the URL of the package on PyPI. - `github_stats_url` (str): Returns the GitHub statistics URL of the package. - `github_stats` (dict[str, Any]): Returns the GitHub statistics of the package. - The GitHub statistics are returned as a dictionary \\ containing the following fields which can accessed using the `item` parameter: - `Forks` (int): Returns the number of forks on GitHub. - `Stars` (int): Returns the number of stars on GitHub. - `Watchers` (int): Returns the number of watchers on GitHub. - `Contributors` (int): Returns the number of contributors on GitHub. - `Dependencies` (int): Returns the number of dependencies on GitHub. - `Dependent repositories` (int): Returns the number of dependent repositories on GitHub. - `Dependent packages` (int): Returns the number of dependent packages on GitHub. - `Repository size` (NamedTuple): Returns the size of the repository on GitHub. - `SourceRank` (int): Returns the SourceRank of the package on GitHub. - `Total releases` (int): Returns the total number of releases on GitHub. - `PkgMetrics fields`: Possible Fields from the `PkgMetrics` class. - `all_metric_stats` (dict[str, Any]): Returns all the OS statistics of the package. - `total_size` (int): Returns the total size of the package. - `date_installed` (datetime): Returns the date the package was installed. - `pypistats fields`: Possible Fields from the `pypistats` module. - `all-pypi-stats` (dict[str, Any]): Returns all the statistics of the package on PyPI into a single dictionary. - `stats-overall` (dict[str, Any]): Returns the overall statistics of the package on PyPI. - `stats-major` (dict[str, Any]): Returns the major version statistics of the package on PyPI. - `stats-minor` (dict[str, Any]): Returns the minor version statistics of the package on PyPI. - `stats-recent` (dict[str, Any]): Returns the recent statistics of the package on PyPI. - `stats-system` (dict[str, Any]): Returns the system statistics of the package on PyPI. Downsides & Limitations My algorithms are fairly well but do come with some important downsides. PkgInspect will ONLY inspect packages that are python files or contains a dist-info folder in the site-packages folder for a given Python version. Was not able to efficiently figure out a way to retrieve all necessary packages without containing unrelevant folders/files. Some personal packages may be skipped otherwise. Beta (pre-releases) has not been implemented yet. As many files may be handled, the runtime may be slow for some users. The demand for a project like this is not so much in-demand but have noticed many people, including my self, still seeking for a project like this. However, this type of project does seem to exceed my experience level with Python and algorithms (hence the downsides) so not entirely sure how far this project may come in the future. Was hoping for it to be GUI based if possible. Usage Examples from pkg_inspect import inspect_package inspect_package(\"pkg_inspect\", itemOrfile=\"initial_version\") # Output (Format - DateTimeAndVersion): ('May 02, 2024', '0.1.0') inspect_package(\"pkg_inspect\", itemOrfile=\"version_history\") # Output (Format - tuple[DateTimeAndVersion]): (('May 02, 2024', '0.1.2'), ('May 02, 2024', '0.1.1'), ('May 02, 2024', '0.1.0')) inspect_package(\"pkg_inspect\", pyversion=\"3.12\", itemOrfile=\"short_meta\") # Output (Format dict[str, Any]): {'Author': 'Yousef Abuzahrieh', 'Author-email': 'yousefzahrieh17@gmail.com', 'Classifiers': {'Development Status 4 Beta', 'Intended Audience Developers', 'License OSI Approved Apache Software License', 'Operating System OS Independent', 'Programming Language Python 3', 'Programming Language Python 3 Only', 'Topic Utilities'}, 'Description-Content-Type': 'text/markdown', 'Download-URL': 'https://github.com/yousefabuz17/PkgInspect.git', 'Home-page': 'https://github.com/yousefabuz17/PkgInspect', 'License': 'Apache Software License', 'Metadata-Version': <Version('2.1')>, 'Name': 'pkg-inspect', 'Platforms': {'Windows', 'MacOS', 'Linux'}, 'Summary': 'A comprehensive tools to inspect Python packages and Python ' 'installations.'} inspect_package(\"pandas\", pyversion=\"3.12\", itemOrfile=\"github_stats\") # Output (Format - dict[str, Any]): {'Contributors': '1.09K', 'Dependencies': 3, 'Dependent packages': '41.3K', 'Dependent repositories': '38.4K', 'Forks': '17.3K', 'Repository size': Stats(symbolic='338.000 KB (Kilobytes)', calculated_size=338.0, bytes_size=346112.0), 'SourceRank': 32, 'Stars': '41.9K', 'Total releases': 126, 'Watchers': 1116}"},
{"Title": "Reboot Your Router with a Python Script", "Author": "u/SAV_NC", "Content": "Hello r/python , I've developed a Python script that allows you to reboot your router remotely via SSH! This script handles the countdown and checks when the router is back online after a reboot. What My Project Does: Key Features: Automated Router Reboot: Remotely trigger a reboot of your router. Monitoring: After sending the reboot command, the script counts down from 350 seconds and starts checking the router's status by pinging it after the first 100 seconds have passed. Flexibility: You can pass arguments dynamically (router IP, username, password, and port) or use hardcoded values within the script. Method of Execution: To execute the script from the command line: python3 reboot-router.py --ip <router_ip> --username <username> --password <password> --port <port_number> Default values are set, but it's highly recommended to pass arguments to the script for security reasons. Target Audience: This script is intended for: Tech Enthusiasts and Home Users who enjoy managing their home network setups and want a quick way to automate router management. Requirements: Required Modules and Programs: Python 3: The script is written in Python 3. Ensure you have Python 3.6 or newer installed. subprocess and argparse modules: These are standard libraries in Python and should be available with your Python installation. sshpass: This utility is used for noninteractive password authentication with SSH. Install it using your package manager, e.g., sudo apt-get install sshpass for Debian/Ubuntu. Important Router Configuration: Before using this script, make sure your router is configured to: Enable SSH Access: Ensure SSH is turned on and configured to accept password authentication. This setting is usually found under the Administration tab in your router settings. Allow ICMP Echo (Ping) Requests: Some routers disable ICMP Echo requests by default for security. You must enable Respond ICMP Echo (ping) Request from WAN under the Firewall tab. Comparison: Unlike many GUI-based tools, this script provides a simple, lightweight command-line solution easily integrated into larger automation workflows or triggered manually without logging into the router interface. For People New to Python: If you're new to scripting or network management, be cautious about storing sensitive information like passwords directly in scripts. While hardcoded values can be used for ease and demonstration, the best practice is to pass these securely as arguments to prevent exposure. Access to the script You can access the script on my GitHub page here Feel free to use, modify, and share this script! I look forward to your feedback and enhancements! Cheers -J"},
{"Title": "Opinions sought: Modernising the Apache NiFi Python client", "Author": "u/Samausi", "Content": "Hello folks, I am the author of NiPyAPI , the defacto Python client for the Apache NiFi project. For those unfamiliar, Apache NiFi is an open-source ETL tool designed around the flow-based programming paradigm. It excels at continuously moving and managing arbitrary data flows between disparate systems with low latency at large scale, and is often contrasted with Airflow. Amongst many features, NiFi allows for Command & Control via a native UI, enabling live edits to data routing and transformation. NiFi also enforces an API-first approach, and produces a swagger definition during code compilation, which allowed creation of a Python client to enable automated Data Flow testing and a Design & Deploy approach by running it through Swagger Codegen 2.3 tool shaped by mustache templates. This produces a very verbose low-level client, which I then leverage in higher-level operational functions and demo scripts in the library. However it's always bugged me that this produces literally 10's of thousands of lines of boilerplate, so potentially replacing the low level client with something more modern would ideally massively reduce the codebase without impacting the high-level functionality of the library. Now, I first started it mid last decade with Python2 for broad compatibility and times have moved on significantly (as has my skill with Python) but it's finally time to drop Python2 support and move to a better build & release process, which opens the door to a larger update, and I am frankly not across modern best-practices around ClientGen so I come seeking r/Python 's advice: How would you modernise this venerable artefact? I could simply move to a newer version of Swagger Codegen, or move into the OpenAPI Generator, I could get fancy with something like Fern - or I could leave well enough alone and just accept that the tons of boilerplate are at least very readable."},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Python Test 220: Getting the most out of PyCon, including juggling - Rob Ludwick", "Author": "u/variedthoughts", "Content": "Listen at https://podcast.pythontest.com/episodes/220-juggling-pycon Even if you never get a chance to go to PyCon, I hope this interview helps you get a feel for the welcoming aspect of the Python community. The juggling at PyCon is one of the inspirations for PythonPeople.fm , one of PythonTests's sibling podcasts. Do you have any conference tips to add?"},
{"Title": "Suggestions for python libraries to contribute to", "Author": "u/JimJimBerry", "Content": "Hey, python folks ! I have been coding in python for around 3 years, 2 years professionally. I have worked with asyncio, typing and other stuff that is needed to build a server. I was looking for a small but impactful enough open source core python library/application to work on. I tried cpython but it seems to be beyond my capability at the moment. As for my interests I was interested in lower level stuff as well as libraries like asyncio and celery. Any suggestions for libraries that could use a bit of help and teach me some stuff as well would be appreciated"},
{"Title": "Project: Simple Interactive Python Streamlit Maps With NASA GIS Data", "Author": "u/jgloewen", "Content": "Python Streamlit is terrific for putting together interactive dashboards. Combined with the geopandas library, streamlit can easily display GIS data points on a map for you. Forest fires in my home province of British Columbia, Canada have been really bad recently. NASA has a terrific dataset that keeps track of forest fires by country. Can I use Streamlit to access this dataset and display a map off all the fires within a certain area (BC) for a particular time frame (2021)? And can I give the user the ability to choose a month? You bet! Let me step you through how! FREE tutorial (with code): https://johnloewen.substack.com/p/simple-interactive-python-streamlit"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "New book! The Quick Python Book, Fourth Edition by Naomi Ceder", "Author": "u/ManningBooks", "Content": "Hello everybody, Thank you for having us here, and a huge \"Thank you\" to the moderators for letting us post. We have just released the latest edition of The Quick Python Book by the one-and-only Naomi Ceder, and I wanted to share that news with the community. Many of you are already familiar with Naomi's work and her massive contributions to the world of Python programming language. The Quick Python Book has aided over 100,000 developers in mastering Python. The Fourth Edition of the book has been revised to include the latest features, control structures, and libraries of Python, along with new coverage of working with AI-generated Python code. Naomi, the author, has beautifully balanced the details of the language with the insights and advice required to accomplish any task. Her personal touch has made learning Python an enjoyable experience for countless developers. üìö You can find the book here: https://mng.bz/aEQj üìñ Get into the liveBook: https://mng.bz/gvee And last but not the least, get 46% off with code: receder46 Hope you find the book helpful. Thank you. Cheers,"},
{"Title": "typedattr: Autocompletion and typechecking for CLI script arguments, using standard argparse syntax", "Author": "u/gings7", "Content": "Excited to share my pypi package typedparser I have been working on for around 1 year now. What My Project Does : It enables writing CLI scripts and create an \"args\" variable with autocompleted members and type checks, but still keeps the simple and universally understood syntax of the stdlib argarse module. Target Audience : For stability, I battletested it in my research projects and added automatic builds as well as 80%+ test coverage. So I believe it is pretty stable. Comparison : For typing functionality it uses the attrs package as backend. It also provides some additional features for object and dictionary manipulation. Of course there are many other CLI argument packages out there, but this one stands out in that it tries to keep the syntax of the argparse standard library as much as possible, making it easy for others to figure out what your script does. Check it out and let me know what you think."},
{"Title": "Dash vs Reflex vs Others", "Author": "u/Sea_Split_1182", "Content": "Where can I find a decent comparison (pros and cons) of these 5 solutions? They seem to be solving the same problem, which is, afaiu, separating the frontend ‚Äòannoyance‚Äô from Python scripting  / math. ‚Å†Reflex (used to be called Pynecone) https://reflex.dev ‚Å†Streamlit https://streamlit.io ‚Å†Gradio https://gradio.app ‚Å†Dash https://dash.plotly.com ‚Å†Panel https://panel.holoviz.org/ ‚Å†Anvil https://anvil.works/ Quarto My use case: user access the web app, choose some parameters, selects things that go or not into a model. Python returns results of my math. Needs to be somewhat eye-candy and I need to use a lot of pictures to get the user input (i.e. ‚Äúwhich of these figures you like most? 1,2,3. User clicks on ‚Äú3‚Äù, 3 is considered in the model."},
{"Title": "I made a python package that can parse Excel Formula Strings into dictionary structures!", "Author": "u/MPGaming9000", "Content": "What my project does: It basically takes a formula string like you'd get from Openpyxl like \"=SUM(A1:B2)\" and breaks it all out into a dictionary structure for you to then navigate through, modify, and then reformat that modified structure back into an excel friendly formula string again! Target Audience: (People who modify Excel formula strings in automated spreadsheet modification scripts. Or people who need to analyze formulas in a spreadsheet to do some kind of logic based on that analysis). Disclaimer: For most people some simple regex pattern matching and str replaces would be fine to modify formulas but if you need a more structured approach to working with these strings, this package has you covered! How does it differ compared to other projects: There are libraries like Openpyxl that allow you to tokenize and translate formulas but that's currently where it ends. It doesn't allow you to systematically parse out a formula and replace those pieces and add new structures and what not into it. Currently the best you can really do is translate formulas and anything other than that would need to rely on regex string matching logic or string replacements. (Which still would be fine for most people, but this just adds another layer of organization and scalability to the format). More info about it here: https://github.com/Voltaic314/ExcelFormulaParser To install, just do: pip install ExcelFormulaParser Thank you for reading this!! Hope you guys find it useful if you're ever systematically modifying (or analyzing) spreadsheets!"},
{"Title": "Hatch v1.10.0 - UV support, new test command and built-in script runner", "Author": "u/Ofekmeister", "Content": "Hello everyone! I'd like to announce version 1.10.0: https://hatch.pypa.io/latest/blog/2024/05/02/hatch-v1100/ Feel free to provide any feedback either here or as a discussion on the repo: https://github.com/pypa/hatch"},
{"Title": "What does your python development setup look like?", "Author": "u/Working_Noise_6043", "Content": "I'd like to explore other people's setup and perhaps try need things or extra tools. What kind IDE, any extra tools to make it easier for you, etc. Looking forward to everyone's responses!"},
{"Title": "Tutorial on Building a Server-to-Server Zoom App with Python", "Author": "u/SleekEagle", "Content": "I made a tutorial on how to build a server-to-server Zoom OAuth application using Python . This application can transcribe Zoom meeting recordings, print the transcripts to the terminal, and save the transcripts as text files. video tutorial repo written tutorial This tutorial covers: Setting up OAuth authentication for server-to-server apps Utilizing the Zoom API to access recordings Implementing automatic transcription using Python"},
{"Title": "Multipart File Uploads to S3 with Python", "Author": "u/tylersavery", "Content": "I created this tutorial after overcoming a difficult challenge myself: uploading 5GB+ files to AWS. This approach allows the browser to securely upload directly to an S3 bucket without the file having to travel through the backend server. The implementation is written in python (backend) and vanilla js (frontend)."},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Suggestions for a self-hosted authentication as a service?", "Author": "u/FlyingRaijinEX", "Content": "I have a simple backend REST API service that is serving a few ML models. I have made it \"secured\" by implementing an API key in order call those endpoints. I was wondering, how common it is for people to use services that can be self-hosted as their authentication/authorization. If it is common and reliable, what are the best options to go for? I've read that building your own authentication/authorization service with email verification, password reset, and social auth can be a pain. Also, did some googling and found this General - Fief . Has anyone ever tried using this? If so, how was the experience? Thanks in advance."},
{"Title": "One pytest marker to track the performance of your tests", "Author": "u/toodarktoshine", "Content": "Hello Pythonistas! I just wrote a blog post about measuring performance inside pytest test cases. We dive into why it‚Äôs important to test for performance and how to integrate the measurements in the CI. Here is the link to the blog: https://codspeed.io/blog/one-pytest-marker-to-track-the-performance-of-your-tests"},
{"Title": "How to create architecture diagrams from code in Jupyter Notebook", "Author": "u/writer_on_rails", "Content": "Hello world,I wrote an article about creating diagrams from code on Jupyter Notebook inside VS Code. It will give you a brief on the setup and also an overview of concepts. Within 5 minutes, you should be able to start making cool architecture diagrams. [TO MODERATOR: This link does not contain any paywalled or paid content. All the contents are available for free] Article link: https://ashgaikwad.substack.com/p/how-to-create-architecture-diagrams"},
{"Title": "Starter Code for a LLM-based AI Assistant", "Author": "u/2bytesgoat", "Content": "Hey everyone üëã TL;DR Since everyone is talking about the Humane AI Pin and the Rabbit R1, I decided to make a short 5 minute tutorial on how people can setup and customize their own little AI assistant on their machine. I've uploaded a video tutorial here: https://www.youtube.com/watch?v=2fD_SAouoOs&ab_channel=2BytesGoat And the Github code is here: https://github.com/2BYTESGOAT/AI-ASSISTANT Longer version What my project does: It's the starter code for an AI assistant that you can run locally. More precisely, it's a ChatGPT / Llama 2 agent that has access to Google Search and can get businesses nearby based on your location. The tool can be easily extended to support other APIs. Target audience : Pythoneers that are curious about LLMs and LLM related libraries. Comparison : It was inspired by projects such as the Humane AI Pin and the Rabbit R1. Though it's a inferior version to those, it serves more as a playground for people to develop their own AI assistants."},
{"Title": "The Python on Microcontrollers (and Raspberry Pi) Newsletter, a weekly news and project resource", "Author": "u/HP7933", "Content": "The Python on Microcontrollers (and Raspberry Pi) Newsletter: subscribe for free With the Python on Microcontrollers newsletter, you get all the latest information on Python running on hardware in one place! MicroPython, CircuitPython and Python on single Board Computers like Raspberry Pi & many more. The Python on Microcontrollers newsletter is the place for the latest news. It arrives Monday morning with all the week‚Äôs happenings. No advertising, no spam, easy to unsubscribe. 10,958 subscribers - the largest Python on hardware newsletter out there. Catch all the weekly news on Python for Microcontrollers with adafruitdaily.com . This ad-free, spam-free weekly email is filled with CircuitPython, MicroPython, and Python information that you may have missed, all in one place! Ensure you catch the weekly Python on Hardware roundup‚Äì you can cancel anytime ‚Äì try our spam-free newsletter today! https://www.adafruitdaily.com/"},
{"Title": "How to auto-instrument Python servers w/ OpenTelemetry for performance & error monitoring", "Author": "u/__boba__", "Content": "Hi everyone! We've recently written up a guide for anyone running a Python server (ex. Flask, Django, FastAPI) to instrument their app to debug slow downs and errors using the CNCF OpenTelemetry project and their Python instrumentation package. It's really straightforward to get started with just a few lines of added code and commands to any Python project, so hopefully helps some people out as they're looking to add better instrumentation to their servers. Here's the tutorial: https://www.hyperdx.io/blog/opentelemetry-python-server-auto-instrumentation"},
{"Title": "k8sAI - my open-source GPT CLI tool for Kubernetes!", "Author": "u/Wild_Plantain528", "Content": "What my project does: I wanted to share an open-source project I‚Äôve been working on called k8sAI. It‚Äôs a personal AI Kubernetes expert that can answer questions about your cluster, suggests commands, and even executes relevant kubectl commands to help diagnose and suggest fixes to your cluster, all in the CLI! Target Audience: As a relative newcomer to k8s, this tool has really streamlined my workflow. I can ask questions about my cluster, k8sAI will run kubectl commands to gather info, and then answer those question. It‚Äôs also found several issues in my cluster for me - all I‚Äôve had to do is point it in the right direction. I‚Äôve really enjoyed making and using this so I thought it could be useful for others. Added bonus is that you don‚Äôt need to copy and paste into ChatGPT anymore! k8sAI operates with read-only kubectl commands to make sure your cluster stays safe. All you need is an OpenAI API key and a valid kubectl config. Start chatting with k8sAI using: $ pip install k8sAI $ k8sAI chat or to fix an issue: $ k8sAI fix -p=\"take a look at the failing pod in the test namespace\" Would love to get any feedback you guys have! Here's the repo for anyone who wants to take a look Comparison: I found a tool (k8sGPT) that I enjoyed using, but I felt it was still missing a few pieces on the chatbot side. You can't chat back and forth with k8sGPT and it doesn't suggest commands for you to execute, so I decided to make this."},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "PkgInspect - Inspect Local/External Python Packages", "Author": "u/yousefabuz", "Content": "GitHub What My Project Does PkgInspect is a comprehensive tool designed to inspect and compare Python packages and Python versions effortlessly. It equips users with a comprehensive set of tools and utility classes to retrieve essential information from installed Python packages, compare versions seamlessly, and extract various details about Python installations with ease. Target Audience Developers and Python enthusiasts looking to streamline the process of inspecting Python packages, comparing versions, and extracting vital information from Python installations will find PkgInspect invaluable. Many current modules such as importlib_metadata and pkg_resources are fairly limited on what items can be inspected and retrieved for a specified python package. Also noticed pkg_resources has also deprecated some of its important retrieval methods. Comparison PkgInspect stands out from other Python package inspection tools due to its robust features. Unlike traditional methods that require manual inspection and comparison, PkgInspect automates the process, saving developers valuable time and effort. With PkgInspect , you can effortlessly retrieve package information, compare versions across different Python installations, and extract crucial details with just a few simple commands. Key Features Inspect Packages : Retrieve comprehensive information about installed Python packages. Compare Versions : Seamlessly compare package data across different Python versions. Retrieve Installed Pythons : Identify and list installed Python versions effortlessly. Inspect PyPI Packages : Gather detailed information about packages from the Python Package Index (PyPI). Fetch Available Updates : Stay up-to-date with available updates for a package from the current version. List Inspection Fieldnames : Access a list of available fieldnames for package inspection. Retrieve Package Metrics : Extract OS statistics about a package effortlessly. Fetch GitHub Statistics : Retrieve insightful statistics about a package from GitHub effortlessly. Retrieve all Python Packages : Easily list all installed Python packages for a given Python version. Main Components Core Modules PkgInspect : Inspects Python packages and retrieves package information. PkgVersions : Retrieves and compares package data across different Python versions. PkgMetrics : Extracts OS statistics about a package. Functions inspect_package : Inspects a Python package and retrieves package information. inspect_pypi : Inspects a package from the Python Package Index (PyPI). get_available_updates : Fetches available updates for a package from the current version. get_installed_pythons : Identifies and lists installed Python versions. get_version_packages : Lists all installed Python packages for a given Python version. pkg_version_compare : Compares package data across different Python versions. Inspection Field Options Any other field name will be treated as a file name to inspect from the packages' site-path directory. - `short_meta` (dict[str, Any]): Returns a dictionary of the most important metadata fields. - If only one field is needed, you can use any of the following metadata fields. - Possible Fields instead of `short_meta`: - `Metadata-Version` (PackageVersion) - `Name` (str) - `Summary` (str) - `Author-email` (str) - `Home-page` (str) - `Download-URL` (str) - `Platform(s)` (set) - `Author` (str) - `Classifier(s)` (set) - `Description-Content-Type` (str) - `short_license` (str): Returns the name of the license being used. - `metadata` (str): Returns the contents of the METADATA file. - `installer` (str): Returns the installer tool used for installation. - `license` (str): Returns the contents of the LICENSE file. - `record` (str): Returns the list of installed files. - `wheel` (str): Returns information about the Wheel distribution format. - `requested` (str): Returns information about the requested installation. - `authors` (str): Returns the contents of the AUTHORS.md file. - `entry_points` (str): Returns the contents of the entry_points.txt file. - `top_level` (str): Returns the contents of the top_level.txt file. - `source_file` (str): Returns the source file path for the specified package. - `source_code` (str): Returns the source code contents for the specified package. - `doc` (str): Returns the documentation for the specified package. - `Pkg` Custom Class Fields - `PkgInspect fields`: Possible Fields from the `PkgInspect` class. - `site_path` (Path): Returns the site path of the package. - `package_paths` (Iterable[Path]): Returns the package paths of the package. - `package_versions` (Generator[tuple[str, tuple[tuple[Any, str]]]]): Returns the package versions of the package. - `pyversions` (tuple[Path]): Returns the Python versions of the package. - `installed_pythons` (TupleOfPkgVersions): Returns the installed Python versions of the package. - `site_packages` (Iterable[str]): Returns the site packages of the package. - `islatest_version` (bool): Returns True if the package is the latest version. - `isinstalled_version` (bool): Returns True if the package is the installed version. - `installed_version` (PackageVersion): Returns the installed version of the package. - `available_updates` (TupleOfPkgVersions): Returns the available updates of the package. - `PkgVersions fields`: Possible Fields from the `PkgVersions` class. - `initial_version` (PackageVersion): Returns the initial version of the package. - `installed_version` (PackageVersion): Returns the installed version of the package. - `latest_version` (PackageVersion): Returns the latest version of the package. - `total_versions` (int): Returns the total number of versions of the package. - `version_history` (TupleOfPkgVersions): Returns the version history of the specified package. - `package_url`: Returns the URL of the package on PyPI. - `github_stats_url` (str): Returns the GitHub statistics URL of the package. - `github_stats` (dict[str, Any]): Returns the GitHub statistics of the package. - The GitHub statistics are returned as a dictionary \\ containing the following fields which can accessed using the `item` parameter: - `Forks` (int): Returns the number of forks on GitHub. - `Stars` (int): Returns the number of stars on GitHub. - `Watchers` (int): Returns the number of watchers on GitHub. - `Contributors` (int): Returns the number of contributors on GitHub. - `Dependencies` (int): Returns the number of dependencies on GitHub. - `Dependent repositories` (int): Returns the number of dependent repositories on GitHub. - `Dependent packages` (int): Returns the number of dependent packages on GitHub. - `Repository size` (NamedTuple): Returns the size of the repository on GitHub. - `SourceRank` (int): Returns the SourceRank of the package on GitHub. - `Total releases` (int): Returns the total number of releases on GitHub. - `PkgMetrics fields`: Possible Fields from the `PkgMetrics` class. - `all_metric_stats` (dict[str, Any]): Returns all the OS statistics of the package. - `total_size` (int): Returns the total size of the package. - `date_installed` (datetime): Returns the date the package was installed. - `pypistats fields`: Possible Fields from the `pypistats` module. - `all-pypi-stats` (dict[str, Any]): Returns all the statistics of the package on PyPI into a single dictionary. - `stats-overall` (dict[str, Any]): Returns the overall statistics of the package on PyPI. - `stats-major` (dict[str, Any]): Returns the major version statistics of the package on PyPI. - `stats-minor` (dict[str, Any]): Returns the minor version statistics of the package on PyPI. - `stats-recent` (dict[str, Any]): Returns the recent statistics of the package on PyPI. - `stats-system` (dict[str, Any]): Returns the system statistics of the package on PyPI. Downsides & Limitations My algorithms are fairly well but do come with some important downsides. PkgInspect will ONLY inspect packages that are python files or contains a dist-info folder in the site-packages folder for a given Python version. Was not able to efficiently figure out a way to retrieve all necessary packages without containing unrelevant folders/files. Some personal packages may be skipped otherwise. Beta (pre-releases) has not been implemented yet. As many files may be handled, the runtime may be slow for some users. The demand for a project like this is not so much in-demand but have noticed many people, including my self, still seeking for a project like this. However, this type of project does seem to exceed my experience level with Python and algorithms (hence the downsides) so not entirely sure how far this project may come in the future. Was hoping for it to be GUI based if possible. Usage Examples from pkg_inspect import inspect_package inspect_package(\"pkg_inspect\", itemOrfile=\"initial_version\") # Output (Format - DateTimeAndVersion): ('May 02, 2024', '0.1.0') inspect_package(\"pkg_inspect\", itemOrfile=\"version_history\") # Output (Format - tuple[DateTimeAndVersion]): (('May 02, 2024', '0.1.2'), ('May 02, 2024', '0.1.1'), ('May 02, 2024', '0.1.0')) inspect_package(\"pkg_inspect\", pyversion=\"3.12\", itemOrfile=\"short_meta\") # Output (Format dict[str, Any]): {'Author': 'Yousef Abuzahrieh', 'Author-email': 'yousefzahrieh17@gmail.com', 'Classifiers': {'Development Status 4 Beta', 'Intended Audience Developers', 'License OSI Approved Apache Software License', 'Operating System OS Independent', 'Programming Language Python 3', 'Programming Language Python 3 Only', 'Topic Utilities'}, 'Description-Content-Type': 'text/markdown', 'Download-URL': 'https://github.com/yousefabuz17/PkgInspect.git', 'Home-page': 'https://github.com/yousefabuz17/PkgInspect', 'License': 'Apache Software License', 'Metadata-Version': <Version('2.1')>, 'Name': 'pkg-inspect', 'Platforms': {'Windows', 'MacOS', 'Linux'}, 'Summary': 'A comprehensive tools to inspect Python packages and Python ' 'installations.'} inspect_package(\"pandas\", pyversion=\"3.12\", itemOrfile=\"github_stats\") # Output (Format - dict[str, Any]): {'Contributors': '1.09K', 'Dependencies': 3, 'Dependent packages': '41.3K', 'Dependent repositories': '38.4K', 'Forks': '17.3K', 'Repository size': Stats(symbolic='338.000 KB (Kilobytes)', calculated_size=338.0, bytes_size=346112.0), 'SourceRank': 32, 'Stars': '41.9K', 'Total releases': 126, 'Watchers': 1116}"},
{"Title": "2,000 free sign ups available for the \"Automate the Boring Stuff with Python\" online course. (May 20", "Author": "u/AlSweigart", "Content": "If you want to learn to code, I've released 2,000 free sign ups for my course following my Automate the Boring Stuff with Python book (each has 1,000 sign ups, use the other one if one is sold out): *The sign ups are all used up, but you can still watch all the videos for free. Read below! https://udemy.com/course/automate/?couponCode=MAY2024FREE https://udemy.com/course/automate/?couponCode=MAY2024FREE2 If you are reading this after the sign ups are used up, you can always find the first 15 of the course's 50 videos are free on YouTube if you want to preview them. YOU CAN ALSO WATCH THE VIDEOS WITHOUT SIGNING UP FOR THE COURSE. All of the videos on the course webpage have \"preview\" turned on. Scroll down to find and click \"Expand All Sections\" and then click the preview link. You won't have access to the forums and other materials, but you can watch the videos. NOTE: Be sure to BUY the course for $0, and not sign up for Udemy's subscription plan. The subscription plan is free for the first seven days and then they charge you. It's selected by default. If you are on a laptop and can't click the BUY checkbox, try shrinking the browser window. Some have reported it works in mobile view. I'm also working on another Udemy course that follows my recent book \"Beyond the Basic Stuff with Python\". So far I have the first 15 of the planned 56 videos done. You can watch them for free on YouTube. Frequently Asked Questions: ( read this before posting questions ) This course is for beginners and assumes no previous programming experience, but the second half is useful for experienced programmers who want to learn about various third-party Python modules. If you don't have time to take the course now, that's fine. Signing up gives you lifetime access so you can work on it at your own pace. This Udemy course covers roughly the same content as the 1st edition book (the book has a little bit more, but all the basics are covered in the online course), which you can read for free online at https://inventwithpython.com The 2nd edition of Automate the Boring Stuff with Python is free online: https://automatetheboringstuff.com/2e/ I do plan on updating the Udemy course, but it'll take a while because I have other book projects I'm working on. If you sign up for this Udemy course, you'll get the updated content automatically once I finish it. It won't be a separate course. It's totally fine to start on the first edition and then read the second edition later. I'll be writing a blog post to guide first edition readers to the parts of the second edition they should read. You're not too old to learn to code. You don't need to be \"good at math\" to be good at coding. Signing up is the first step. Actually finishing the course is the next. :) There are several ways to get/stay motivated. I suggest getting a \"gym buddy\" to learn with. Check out r/ProgrammingBuddies"},
{"Title": "Best book for GUI development in Python", "Author": "u/Bekhyam", "Content": "Can you guys suggest some very good book for GUI development in Python? I'm currently working on a visualizer that needs many features to plot data on a 3D and 2D space. Using PyQt for this as it has threading support."},
{"Title": "ConfigClass - simple dataclass inspired configuration", "Author": "u/TheTerrasque", "Content": "What My Project Does I'm making a simple configclass for handling configuration in smaller projects and scripts. Goal is to be as simple to start with as creating a dataclass. The module itself works off dataclass and when you use it you just define a dataclass as normal, but decorate it with @configclass() instead. Example: from configclass import configclass @configclass() class Settings: foo: bool = False url: str = \"\" footoo: bool = True my_model: str = \"model.pt\" setting = Settings.load() print(setting.foo, setting.footoo, setting.my_model) From that you got JSON config file support (config.json) YAML config file support (config.yaml) Command line support (argparse) Env variables support (CONFIG_SETTINGNAME) It also support nested structures via nested dataclass classes. Comparison It's meant as a quick and lightweight alternative to larger and more comprehensive config systems, for the small programs and scripts where you'd just use a dataclass, and maybe load the values from a config file. Target Audience Since it's pretty new and raw I wouldn't recommend it for heavy production settings or complex projects. That said, it should work fine for most cases. While I've worked with python for quite some time, this is the first time I've tried making a package, so I'd like some feedback on the project and it's structure before I push it anywhere. It'd also be nice to stress test it and shake out some bugs. More info and code at https://github.com/TheTerrasque/python-configclass"},
{"Title": "tach - a Python tool to enforce modular design", "Author": "u/the1024", "Content": "https://github.com/Never-Over/tach What My Project Does tach is a lightweight Python tool that enforces boundaries and dependencies in your Python project. Inspired by nx , tach helps you maintain a decoupled and modular Python codebase. An earlier version of this tool was called modguard, which we shared here . By default, Python allows you to import and use anything, anywhere. Over time, this results in modules that were intended to be separate getting tightly coupled together, and domain boundaries breaking down. We experienced this first-hand at a unicorn startup, where the eng team paused development for over a year in an attempt to split up packages into independent services. This attempt ultimately failed. This problem occurs because: It's much easier to add to an existing package rather than create a new one Junior devs have a limited understanding of the existing architecture External pressure leading to shortcuts and overlooking best practices Efforts we've seen to fix this problem always came up short. A patchwork of solutions would attempt to solve this from different angles, such as developer education, CODEOWNERs, standard guides, refactors, and more. However, none of these addressed the root cause. With tach , you can: Declare your packages ( package.yml ) Define dependencies between packages ( tach.yml ) Enforce those dependencies ( tach check ) You can also enforce a strict interface for each package. This means that only imports that are directly listed in __init__.py can be imported by other packages. tach is: fully open source able to be adopted incrementally ( tach init and tach add ) implemented with no runtime footprint interoperable with your existing tooling We hope you give it a try! We'd love any feedback. GitHub Target Audience Python developers who want to maintain quality while shipping quickly Comparison This tool is an evolution of a tool we previously built, modguard . It's very similar to nx's module boundaries tool, although they don't support Python."},
{"Title": "Just an Appreciation Post for the Python Standard Library", "Author": "u/SpeakerOk1974", "Content": "Approaching a year into my journey of learning Python (I come from a C background) I finally understand the beauty and ubiquity of the langauge. Initially, I was hesitant to learn a dynamically typed and interpreted language when job requirements required me to learn it (I help support a team of Power Systems Engineers, and our professional software uses python for scripting). My first attempts were extremely unpythonic and it felt like I was fighting the language (index based looping, declaring variables before use, C style procedural code on top of Pandas antipatterns) and the lack of brackets I found appalling. Then I had my first code review with a helpful Senior engineer. We refactored my code together and something beautiful came together. He then told me to read the Zen of Python. It was love at first \"import this\". I was hooked. Every waking moment for weeks I was learning all I could muster about python. Now later on in my journey, and having written several complex systems in python over the course of the past 6 months, it is truly incredible what you can accomplish with just the standard library. Need a temporary file? There is a module for that. Want to serialize data? Multiple modules just for that purpose. Embarrassingly parallel basic scripting tasks? 3 lines of code later and it is now multiprocessed. The list goes on and on. Archimedes once famously said \"Give me a lever long enough and a fulcrum on which to place it, and I shall move the world\". With an understanding of all that is available out of the box as the fulcrum so to speak, it is amazing the breadth of problems that the standard library proves to be a long enough lever. On our compute cluster, we try to keep dependencies to an absolute minimum. Sticking to the standard library, Pandas, and the provided API for the software with a clever enough implementation rarely if ever feels like a limitation. So next time you feel the need to pip install something new, be sure to check the Python documentation. You just might already have all the tools you need already at your disposal!"},
{"Title": "ext-message: Send files over Discord direct messages", "Author": "u/FeLoNy111", "Content": "Hey all! I recently made a library for sending text files over Discord DMs https://pypi.org/project/ext-message/ https://github.com/jwjeffr/ext-message/ What my project does This code is a command-line interface for sending text files to Discord DMs from a bot. Target audience My personal use-case is for my research, where I run long simulations (~72 hours) on a computing cluster. These simulations generate log files, so I can send myself the log file after the simulation, letting me: - Know that the simulation is done - Check that the simulation ran as intended without having to SSH into the cluster. As of now, those running similarly heavy code looking to be notified when it finishes is my target audience, but I would love to hear other creative use cases to expand that audience! Comparison Not sure what's out there in terms of other libraries for doing similar things. This was partially for production and partially a toy project forcing me to become more familiar with discord.py"},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "pytest-ndb - debugging pytest tests in a notebook", "Author": "u/rhshadrach", "Content": "What My Project Does : Allows users to run a pytest test from a notebook or other REPL and capture local variables for inspection / debugging. While I think users should very often prefer debugging tests with a debugger, in certain situations where one deals with a nontrivial amount of data or many complex objects, being able to inspect and manipulate objects in a notebook can be helpful. Target Audience : Primarily data scientists, but more generally anyone who tests with large/complex objects that can be difficult to understand in a debugger. Comparison: I'm not aware of any alternatives, other than copy-and-pasting your pytest tests in a notebook. Repo with examples: https://github.com/rhshadrach/pytest-ndb"},
{"Title": "Google laysoff Python maintainer team", "Author": "u/jmreagle", "Content": "https://www.hindustantimes.com/business/google-layoffs-sundar-pichai-led-company-fires-entire-python-team-for-cheaper-labour-101714379453603.html Are there any ramifications for the Python community outside of  Google?"},
{"Title": "pyrseus - concurrent.futures extensions for easier troubleshooting", "Author": "u/Mental-Elephant-6215", "Content": "https://github.com/dalleyg/pyrseus What My Project Does Pyrseus extends Python‚Äôs concurrent.futures asynchronous and concurrent programming package with: a collection of non-concurrent executors for light workloads and troubleshooting, ExecutorCtx, a factory for easily switching between different executors, and a collection of ready-built ExecutorCtx plugins, supporting executors from concurrent.futures, ipyparallel, loky, mpi4py, and itself. Where relevant, optional cloudpickle-enhanced plugins are also provided. Full documentation is available at https://pyrseus.readthedocs.io/en/latest/ . Target Audience Python developers who already use concurrent.futures that want at least one of the following: an easier way to troubleshoot problems with their tasks without having to rewrite the control code, an easier way to switch between concurrent and non-concurrent execution without having to rewrite the control code, and/or an easier way to dynamically change what executor class is used at runtime. Comparison InlineExecutor is similar to various open source classes that already exist, but almost always as internal implementation details in much larger projects. NoCatchExecutor is a less common class in other libraries, since it intentionally has non-standard error handling. We include it because it can be useful in situations where one wants true fail-fast behavior. We're not aware of the other pickle-testing serial executors existing elsewhere. ExecutorCtx ties together the executors provided by Pyrseus. We're not aware of any open source libraries that support (a) both serial and concurrent executors, and (b) do so as a standalone package instead of as part of a much larger package. It's worth giving special mention to the loky project. It provides (a) a better version of concurrent.futures.ProcessPoolExecutor and (b) a reusable executor factory. Pyrseus can use loky's executors as backends, if it's installed. Our package extends loky by making its reusable executors usable as context managers. We also supply non-concurrent executors and make it easy to switch to them at runtime, as discussed above. See the notebooks and the plugins list at https://pyrseus.readthedocs.io/en/latest/ for more details."},
{"Title": "python irl in london england", "Author": "u/ProsodySpeaks", "Content": "are there any groups, meets, events etc i can get involved with locally in london uk? i've got moderate python chops as well as a wide range of other skills and tools, notably construction and event organisation/production/management, but also 3d design and print, some electronics and others. i work flexible hours and honestly am looking to develop a network and experiences to land my first job that at least involves some coding - i'm happy to commit significant time and resource to interesting projects to get eventually my foot or at least my nose in a door somewhere. (i dont mean i would stop contributing to the project once i land a job!) thanks!"},
{"Title": "Analyzing Python Compression Libraries: zlib, LZ4, Brotli, and Zstandard", "Author": "u/dhilip-siva", "Content": "Source Code: https://github.com/dhilipsiva/py-compress-compare Analyzing Python Compression Libraries: zlib, LZ4, Brotli, and Zstandard When dealing with large volumes of data, compression can be a critical factor in enhancing performance, reducing storage costs, and speeding up network transfers. In this blog post, we will dive into a comparison of four popular Python compression libraries‚Äîzlib, LZ4, Brotli, and Zstandard‚Äîusing a real-world dataset to evaluate their performance in terms of compression ratio and time efficiency. The Experiment Setup Our test involved a dataset roughly 581 KB in size, named sample_data.json. We executed compression and decompression using each library as follows: Compression was performed 1000 times. Decompression was repeated 10,000 times. This rigorous testing framework ensures that we obtain a solid understanding of each library's performance under heavy load. Compression Ratio The compression ratio is a key metric that represents how effectively a compression algorithm can reduce the size of the input data. Here‚Äôs how each library scored: Zlib achieved a compression ratio of 27.84, LZ4 came in at 18.23, Brotli impressed with a ratio of 64.78, Zstandard offered a ratio of 43.42. From these results, Brotli leads with the highest compression ratio, indicating its superior efficiency in data size reduction. Zstandard also shows strong performance, while LZ4, though lower, still provides a reasonable reduction. Compression Time Efficiency isn't just about space savings; time is equally crucial. Here‚Äôs how long each library took to compress the data: Zlib: 7.34 seconds, LZ4: 0.13 seconds, Brotli: 204.18 seconds, Zstandard: 0.15 seconds. LZ4 and Zstandard excel in speed, with LZ4 being slightly faster. Zlib offers a middle ground, but Brotli, despite its high compression efficiency, takes significantly longer, which could be a drawback for real-time applications. Decompression Time Decompression time is vital for applications where data needs to be rapidly restored to its original state: Zlib: 11.99 seconds, LZ4: 0.46 seconds, Brotli: 0.99 seconds, Zstandard: 0.46 seconds. Again, LZ4 and Zstandard show excellent performance, both under half a second. Brotli presents a decent time despite its lengthy compression time, while zlib lags behind in this aspect. Conclusion Each library has its strengths and weaknesses: Brotli is your go-to for maximum compression but at the cost of time, making it suitable for applications where compression time is less critical. Zstandard offers a great balance between compression ratio and speed, recommended for a wide range of applications. LZ4 shines in speed, ideal for scenarios requiring rapid data processing. Zlib provides moderate performance across the board. Choosing the right library depends on your specific needs, whether it‚Äôs speed, space, or a balance of both. This experiment provides a clear picture of what to expect from these libraries, helping you make an informed decision based on your application's requirements."},
{"Title": "I wrote a book on Python Regular Expressions - and made it free!", "Author": "u/code_x_7777", "Content": "Hi! I've just sorted through some of my books on Leanpub and decided to make the book on Python Regular Expressions free to download for a couple of days. Why? Just for fun and learning and to get some valuable feedback. Free ebook link: https://leanpub.com/regexpython/ Regex Video Tutorials: https://blog.finxter.com/python-regex/ This book helps you learn regular expressions chapter by chapter. Each chapter first teaches the concepts and then asks you to solve Python puzzles (\"Guess the code!\") to master the concepts. The main idea is to keep it lightweight and fun. Here's the ToC: Introduction Applications About Puzzle Learning Basics Special Symbols Character Sets Dot Regex Asterisk Quantifier Plus Quantifier ? Quantifier Quantifier Differentiation Greediness Line and String Boundaries OR Regex AND Regex NOT Regex Matching Groups Split Method Substitution Method Compile Method Bonus Puzzles Final Remarks I hope you enjoy learning from the book! Feel free to send me your feedback. Happy learning \"R[e-x]+\"! ü§ì"},
{"Title": "How Malicious Python Code Gains Execution", "Author": "u/louis11", "Content": "The primary vector for malicious code running in software developer environments (e.g., local system, CI/CD runners, production servers, etc.) is software dependencies. This is third-party code which often means open-source software, also known as running code from strangers on the internet. The prized goal for attackers is arbitrary code execution. It‚Äôs the stuff high CVE scores are made of and often the topic of how vulnerabilities can turn into exploits. It‚Äôs the foothold needed to run cryptominers, steal secrets, or encrypt data for ransom. It‚Äôs no wonder why threat actors want it, but how do they get it? Sutton‚Äôs Law makes it obvious why they go after open-source software: because executing arbitrary code is easy there. This is a series examining the methods malicious Python code gains execution. Some of the methods are obvious and some are potentially undiscovered or at least not found in the wild, yet. What they all mostly have in common is the reliance on a software dependency in the form of a Python package, which is where we begin. https://blog.phylum.io/how-malicious-python-code-gains-execution/"},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Medical imaging Python jobs", "Author": "u/Durysik", "Content": "Hi folks! I am currently looking for a US (west coast best) based job where I could grow as a python programmer, specifically analysing medical images. Looking online I found some labs where they work with images, but I thought I'd ask (not sure if it's the right sub tho) cause I'm sure this way I'll hear about many places that deal with Medical Imaging. Thanks :)"},
{"Title": "tab-pal: a TUI for adding and editing custom colour palettes in Tableau", "Author": "u/Ok-Frosting7364", "Content": "Source code : https://github.com/ben-n93/tab-pal What my Project Does Makes it quicker, easier and prettier to create custom colour palettes in Tableau. Rather than having to find your Preferences.tps file every time and edit the XML you can just launch tab-pal from the command-line and make any changes you want (adding/deleting new palettes, adding/removing colours. You also get a preview of the colours you've added. Target Audience Data/business analysts and data scientists and who work with Tableau. Comparison There are some websites that let you dynamically create the Preferences.tps XML file however this still requires you to locate the file and copy and paste  over it with any changes you make"},
{"Title": "A Free GPT-Powered Financial Analysis Tool", "Author": "u/Starks-Technology", "Content": "Link to GitHub Page What My Project Does I created a simple financial analysis tool using the SimFin API and the GPT API. This tool summarizes financial information and makes it digestable for people who don't know a lot about finance. Target Audience This is more of a toy project for those interested in AI and Finance. It's meant for people without a ton of GPT experience to see a tangible use-case of generative AI within the field. Comparison Even though this is a simple tool, I don't know of any existing alternative. Example Input: Analyze Apple's q1 2023 earnings Output: AI Assistant:  Based on the provided financial information, here is a summary of the company's finances: Balance Sheet:- Total Assets: $346,747,000,000- Total Liabilities: $290,020,000,000- Total Equity: $56,727,000,000The company has a strong balance sheet with a significant amount of assets compared to its liabilities. This indicates a healthy financial position. Cash Flow:- Net Cash from Operating Activities: $34,005,000,000- Net Cash from Investing Activities: -$1,445,000,000- Net Cash from Financing Activities: -$35,563,000,000- Net Change in Cash: -$3,003,000,000The company generated positive cash flow from its operating activities, but had negative cash flow from investing and financing activities. As a result, there was a decrease in cash during the period. Profitability Metrics:- Gross Profit Margin: 42.96%- Operating Margin: 30.74%- Net Profit Margin: 25.61%- Return on Equity: 52.88%- Return on Assets: 8.65%- Return on Invested Capital: 15.28%The company has healthy profitability metrics, indicating efficient operations and good returns on investment. Liquidity Metrics:- Current Ratio: 1.01The company has a current ratio slightly above 1, which suggests it has enough current assets to cover its short-term liabilities. However, it is important to note that a current ratio of exactly 1 may indicate limited liquidity. Solvency Metrics:- Liabilities to Equity Ratio: 5.11- Debt Ratio: 32.04%The company has a relatively high liabilities to equity ratio, indicating a significant amount of debt compared to equity. The debt ratio is moderate, suggesting that a significant portion of the company's assets is financed by debt. Cash Flow Metrics:- Free Cash Flow: $39,273,000,000- Free Cash Flow to Net Income: 1.31- Cash Return on Invested Capital: 17.77%The company has positive free cash flow, indicating its ability to generate cash after accounting for capital expenditures. The free cash flow to net income ratio suggests that the company is efficient in converting its net income into free cash flow. The cash return on invested capital is also positive, indicating good returns on the capital invested. Other Important Metrics:- Piotroski F-Score: 4- Net Debt / EBITDA: 1.53- Dividend Payout Ratio: 12.56%The Piotroski F-Score of 4 suggests that the company has a moderate financial strength. The net debt to EBITDA ratio indicates the company's ability to repay its debt, with a ratio of 1.53. The dividend payout ratio suggests that the company distributes a portion of its earnings as dividends.Based on the provided information, the company appears to be in a strong financial position with healthy profitability metrics and positive cash flow. However, it is important to conduct further research and analysis to fully understand the company's financial health and prospects. Feel free to fork it, make PRs for it,or leave feedback!"},
{"Title": "I made a Tkinter \"DevTools\" to inspect and modify widgets in your running app in real-time", "Author": "u/254hypebeast", "Content": "source: https://github.com/ObaraEmmanuel/Formation pypi: https://pypi.org/project/formation-studio/ What My Project Does Allows you to inspect widgets in your running Tk app in real-time. You can view the widget hierarchy, modify widget attributes, adjust widget layout and run arbitrary code to interact with your widgets through the embedded Python REPL console. It works just like DevTools in a browser. This debugger is part of the Formation studio project which is a drag-n-drop graphical UI builder for Tkinter. Target Audience Any Tk developer seeking to have an easier time debugging their UI or seeking to experiment with the Tk framework with minimal effort. Comparison There is no project currently doing this same thing. Usage It comes bundled with Formation Studio so the installation is as simple as pip install formation-studio You don't have to change anything in your code. Simply use the following command and the debugger will attach itself to your app: formation-dbg /path/to/your/tk/app.py In the embedded python REPL console you can access a simple debugger API as follows: # Access a list of all widgets currently selected widgets = debugger.selection # Access the root widget usually a Tk object root = debugger.root"},
{"Title": "I made a Python CLI Tools for Competitive Programming", "Author": "u/Old-Manufacturer6209", "Content": "Source Code: https://github.com/JeanExtreme002/FastSnake PyPI: https://pypi.org/project/FastSnake/ What My Project Does FastSnake is a command-line tool that allows you to easily create, expand, run, and test Python solutions for competitive programming problems. üêçüèÅ This project provides useful CLI tools for competitive programming, such as test case generators, algorithms and data structures, tools for platforms Codeforces and AtCoder , and other features that assist you during the development and testing of solutions, besides building a nice directory structure to develop your solutions. ü§ì Target Audience This project was developed for programming competitors, focusing on users of CodeForces and AtCoder, who need to quickly develop solutions for complex problems Comparison There is no project currently doing this same thing. How to Install Just type the following commands: $ pip install fastsnake $ fastsnake -v Documentation Explore the documentation of FastSnake package at repository's project."},
{"Title": "Meteor Science using Python - Creating a \"weird\" coordinate system", "Author": "u/MrAstroThomas", "Content": "Hey everyone, I am creating \"Space Science with Python\" tutorials on YouTube with free and open accessible Python code on GitHub . It is not fancy or a \"super high animation quality YouTube production\". I am just an astrophysicists (what a difficult word to write), working in industry who continues with science as a hobby. Ha! And I have some crazy niche knowledge that is not covered by Gemini or chatGPT (yet). Anyway, I am creating now a tutorial on meteor science and wanted to show you how to create a coordinate system that co-rotates with the Earth while it is revolving around the Sun. Why is this interesting? Well, you may have heard about certain meteor streams like the Perseids in August or the Geminids in December. Dedicated streams that are associated with e.g. a particular comet or asteroid. However there are meteors that appear \"random\". So called sporadics. These sporadics have certain source regions, like e.g. the Apex. Cool, what the heck is an Apex? The Apex is Earth's \"flight direction\" in the Solar System. Imagine viewing the Sun and Earth from top of the Solar System: Apex is rotating with the Earth. Thus, it is not a fixed coordinate system. Take this image from Sky & Telescope that helps you imaging this stuff. Now the resulting regions of interests can be seen on this NASA page . In a sky map / plot you see different sources. ... but wouldn't it be cool to do it yourself? With your own data and Python code? And that's where I try to jump in. Check out the code and the corresponding video . I am looking forward to any feedback / comment"},
{"Title": "SecretScraper: highly configurable web crawler/scraper for extracting sensitive data from websites", "Author": "u/PadishahIII", "Content": "Hi, I'm a cybersecurity enthusiastic. And I've made a web crawler/scraper tool to extract links and sensitive information against target websites. You can find it here: https://github.com/PadishahIII/SecretScraper . What My Project Does SecretScraper is a highly configurable web scraper tool that crawls links, extracts subdomains from target websites and finds sensitive data using regular expressions. The features included in the SecretScraper are: Web crawler: extract links using both DOM hierarchy and regex Support for domain whitelist and blacklist Support multiple targets, enter target URLs from a file Scalable customisation: header, proxy, timeout, cookie, scrape depth, follow redirect, etc. Built-in regex to search for sensitive information: hyperscan is employed for higher performance Flexible configuration in yaml format Target Audience SecretScraper is made for penetration tester or web developer who can use this tool for info-gathering and finding any sensitive data or route of any website. Comparison A similar project is LinkFinder , an awesome python script written to discover endpoints and their parameters in JavaScript files. But I was expecting a project with more general use and more functionality. So I am developing this project half for practice and half with the intension of integrating it in a larger design. Use Case There is full documentation available in Github: https://github.com/PadishahIII/SecretScraper . Simply install via pip install secretscraper and see secretscraper --help ."},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "TypeIs does what I thought TypeGuard would do in Python", "Author": "u/Ok_Analysis_4910", "Content": "While it's unfortunate to have two constructs‚ÄîTypeGuard and TypeIs‚Äîwith slightly different behaviors, I'm glad that the latter is less surprising. https://rednafi.com/python/typeguard_vs_typeis/"},
{"Title": "Are PEP 744 goals very modest?", "Author": "u/MrMrsPotts", "Content": "Pypy has been able to speed up pure python code by a factor of 5 or more for a number of years. The only disadvantage it has is the difficulty in handling C extensions which are very commonly used in practice. https://peps.python.org/pep-0744 seems to be talking about speed ups of 5-10%. Why are the goals so much more modest than what pypy can already achieve?"},
{"Title": "2,000 free sign ups available for the \"Automate the Boring Stuff with Python\" online course. (May 20", "Author": "u/AlSweigart", "Content": "If you want to learn to code, I've released 2,000 free sign ups for my course following my Automate the Boring Stuff with Python book (each has 1,000 sign ups, use the other one if one is sold out): *The sign ups are all used up, but you can still watch all the videos for free. Read below! https://udemy.com/course/automate/?couponCode=MAY2024FREE https://udemy.com/course/automate/?couponCode=MAY2024FREE2 If you are reading this after the sign ups are used up, you can always find the first 15 of the course's 50 videos are free on YouTube if you want to preview them. YOU CAN ALSO WATCH THE VIDEOS WITHOUT SIGNING UP FOR THE COURSE. All of the videos on the course webpage have \"preview\" turned on. Scroll down to find and click \"Expand All Sections\" and then click the preview link. You won't have access to the forums and other materials, but you can watch the videos. NOTE: Be sure to BUY the course for $0, and not sign up for Udemy's subscription plan. The subscription plan is free for the first seven days and then they charge you. It's selected by default. If you are on a laptop and can't click the BUY checkbox, try shrinking the browser window. Some have reported it works in mobile view. I'm also working on another Udemy course that follows my recent book \"Beyond the Basic Stuff with Python\". So far I have the first 15 of the planned 56 videos done. You can watch them for free on YouTube. Frequently Asked Questions: ( read this before posting questions ) This course is for beginners and assumes no previous programming experience, but the second half is useful for experienced programmers who want to learn about various third-party Python modules. If you don't have time to take the course now, that's fine. Signing up gives you lifetime access so you can work on it at your own pace. This Udemy course covers roughly the same content as the 1st edition book (the book has a little bit more, but all the basics are covered in the online course), which you can read for free online at https://inventwithpython.com The 2nd edition of Automate the Boring Stuff with Python is free online: https://automatetheboringstuff.com/2e/ I do plan on updating the Udemy course, but it'll take a while because I have other book projects I'm working on. If you sign up for this Udemy course, you'll get the updated content automatically once I finish it. It won't be a separate course. It's totally fine to start on the first edition and then read the second edition later. I'll be writing a blog post to guide first edition readers to the parts of the second edition they should read. You're not too old to learn to code. You don't need to be \"good at math\" to be good at coding. Signing up is the first step. Actually finishing the course is the next. :) There are several ways to get/stay motivated. I suggest getting a \"gym buddy\" to learn with. Check out r/ProgrammingBuddies"},
{"Title": "Best book for GUI development in Python", "Author": "u/Bekhyam", "Content": "Can you guys suggest some very good book for GUI development in Python? I'm currently working on a visualizer that needs many features to plot data on a 3D and 2D space. Using PyQt for this as it has threading support."},
{"Title": "ConfigClass - simple dataclass inspired configuration", "Author": "u/TheTerrasque", "Content": "What My Project Does I'm making a simple configclass for handling configuration in smaller projects and scripts. Goal is to be as simple to start with as creating a dataclass. The module itself works off dataclass and when you use it you just define a dataclass as normal, but decorate it with @configclass() instead. Example: from configclass import configclass @configclass() class Settings: foo: bool = False url: str = \"\" footoo: bool = True my_model: str = \"model.pt\" setting = Settings.load() print(setting.foo, setting.footoo, setting.my_model) From that you got JSON config file support (config.json) YAML config file support (config.yaml) Command line support (argparse) Env variables support (CONFIG_SETTINGNAME) It also support nested structures via nested dataclass classes. Comparison It's meant as a quick and lightweight alternative to larger and more comprehensive config systems, for the small programs and scripts where you'd just use a dataclass, and maybe load the values from a config file. Target Audience Since it's pretty new and raw I wouldn't recommend it for heavy production settings or complex projects. That said, it should work fine for most cases. While I've worked with python for quite some time, this is the first time I've tried making a package, so I'd like some feedback on the project and it's structure before I push it anywhere. It'd also be nice to stress test it and shake out some bugs. More info and code at https://github.com/TheTerrasque/python-configclass"},
{"Title": "tach - a Python tool to enforce modular design", "Author": "u/the1024", "Content": "https://github.com/Never-Over/tach What My Project Does tach is a lightweight Python tool that enforces boundaries and dependencies in your Python project. Inspired by nx , tach helps you maintain a decoupled and modular Python codebase. An earlier version of this tool was called modguard, which we shared here . By default, Python allows you to import and use anything, anywhere. Over time, this results in modules that were intended to be separate getting tightly coupled together, and domain boundaries breaking down. We experienced this first-hand at a unicorn startup, where the eng team paused development for over a year in an attempt to split up packages into independent services. This attempt ultimately failed. This problem occurs because: It's much easier to add to an existing package rather than create a new one Junior devs have a limited understanding of the existing architecture External pressure leading to shortcuts and overlooking best practices Efforts we've seen to fix this problem always came up short. A patchwork of solutions would attempt to solve this from different angles, such as developer education, CODEOWNERs, standard guides, refactors, and more. However, none of these addressed the root cause. With tach , you can: Declare your packages ( package.yml ) Define dependencies between packages ( tach.yml ) Enforce those dependencies ( tach check ) You can also enforce a strict interface for each package. This means that only imports that are directly listed in __init__.py can be imported by other packages. tach is: fully open source able to be adopted incrementally ( tach init and tach add ) implemented with no runtime footprint interoperable with your existing tooling We hope you give it a try! We'd love any feedback. GitHub Target Audience Python developers who want to maintain quality while shipping quickly Comparison This tool is an evolution of a tool we previously built, modguard . It's very similar to nx's module boundaries tool, although they don't support Python."},
{"Title": "Just an Appreciation Post for the Python Standard Library", "Author": "u/SpeakerOk1974", "Content": "Approaching a year into my journey of learning Python (I come from a C background) I finally understand the beauty and ubiquity of the langauge. Initially, I was hesitant to learn a dynamically typed and interpreted language when job requirements required me to learn it (I help support a team of Power Systems Engineers, and our professional software uses python for scripting). My first attempts were extremely unpythonic and it felt like I was fighting the language (index based looping, declaring variables before use, C style procedural code on top of Pandas antipatterns) and the lack of brackets I found appalling. Then I had my first code review with a helpful Senior engineer. We refactored my code together and something beautiful came together. He then told me to read the Zen of Python. It was love at first \"import this\". I was hooked. Every waking moment for weeks I was learning all I could muster about python. Now later on in my journey, and having written several complex systems in python over the course of the past 6 months, it is truly incredible what you can accomplish with just the standard library. Need a temporary file? There is a module for that. Want to serialize data? Multiple modules just for that purpose. Embarrassingly parallel basic scripting tasks? 3 lines of code later and it is now multiprocessed. The list goes on and on. Archimedes once famously said \"Give me a lever long enough and a fulcrum on which to place it, and I shall move the world\". With an understanding of all that is available out of the box as the fulcrum so to speak, it is amazing the breadth of problems that the standard library proves to be a long enough lever. On our compute cluster, we try to keep dependencies to an absolute minimum. Sticking to the standard library, Pandas, and the provided API for the software with a clever enough implementation rarely if ever feels like a limitation. So next time you feel the need to pip install something new, be sure to check the Python documentation. You just might already have all the tools you need already at your disposal!"},
{"Title": "ext-message: Send files over Discord direct messages", "Author": "u/FeLoNy111", "Content": "Hey all! I recently made a library for sending text files over Discord DMs https://pypi.org/project/ext-message/ https://github.com/jwjeffr/ext-message/ What my project does This code is a command-line interface for sending text files to Discord DMs from a bot. Target audience My personal use-case is for my research, where I run long simulations (~72 hours) on a computing cluster. These simulations generate log files, so I can send myself the log file after the simulation, letting me: - Know that the simulation is done - Check that the simulation ran as intended without having to SSH into the cluster. As of now, those running similarly heavy code looking to be notified when it finishes is my target audience, but I would love to hear other creative use cases to expand that audience! Comparison Not sure what's out there in terms of other libraries for doing similar things. This was partially for production and partially a toy project forcing me to become more familiar with discord.py"},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "pytest-ndb - debugging pytest tests in a notebook", "Author": "u/rhshadrach", "Content": "What My Project Does : Allows users to run a pytest test from a notebook or other REPL and capture local variables for inspection / debugging. While I think users should very often prefer debugging tests with a debugger, in certain situations where one deals with a nontrivial amount of data or many complex objects, being able to inspect and manipulate objects in a notebook can be helpful. Target Audience : Primarily data scientists, but more generally anyone who tests with large/complex objects that can be difficult to understand in a debugger. Comparison: I'm not aware of any alternatives, other than copy-and-pasting your pytest tests in a notebook. Repo with examples: https://github.com/rhshadrach/pytest-ndb"},
{"Title": "Google laysoff Python maintainer team", "Author": "u/jmreagle", "Content": "https://www.hindustantimes.com/business/google-layoffs-sundar-pichai-led-company-fires-entire-python-team-for-cheaper-labour-101714379453603.html Are there any ramifications for the Python community outside of  Google?"},
{"Title": "pyrseus - concurrent.futures extensions for easier troubleshooting", "Author": "u/Mental-Elephant-6215", "Content": "https://github.com/dalleyg/pyrseus What My Project Does Pyrseus extends Python‚Äôs concurrent.futures asynchronous and concurrent programming package with: a collection of non-concurrent executors for light workloads and troubleshooting, ExecutorCtx, a factory for easily switching between different executors, and a collection of ready-built ExecutorCtx plugins, supporting executors from concurrent.futures, ipyparallel, loky, mpi4py, and itself. Where relevant, optional cloudpickle-enhanced plugins are also provided. Full documentation is available at https://pyrseus.readthedocs.io/en/latest/ . Target Audience Python developers who already use concurrent.futures that want at least one of the following: an easier way to troubleshoot problems with their tasks without having to rewrite the control code, an easier way to switch between concurrent and non-concurrent execution without having to rewrite the control code, and/or an easier way to dynamically change what executor class is used at runtime. Comparison InlineExecutor is similar to various open source classes that already exist, but almost always as internal implementation details in much larger projects. NoCatchExecutor is a less common class in other libraries, since it intentionally has non-standard error handling. We include it because it can be useful in situations where one wants true fail-fast behavior. We're not aware of the other pickle-testing serial executors existing elsewhere. ExecutorCtx ties together the executors provided by Pyrseus. We're not aware of any open source libraries that support (a) both serial and concurrent executors, and (b) do so as a standalone package instead of as part of a much larger package. It's worth giving special mention to the loky project. It provides (a) a better version of concurrent.futures.ProcessPoolExecutor and (b) a reusable executor factory. Pyrseus can use loky's executors as backends, if it's installed. Our package extends loky by making its reusable executors usable as context managers. We also supply non-concurrent executors and make it easy to switch to them at runtime, as discussed above. See the notebooks and the plugins list at https://pyrseus.readthedocs.io/en/latest/ for more details."},
{"Title": "python irl in london england", "Author": "u/ProsodySpeaks", "Content": "are there any groups, meets, events etc i can get involved with locally in london uk? i've got moderate python chops as well as a wide range of other skills and tools, notably construction and event organisation/production/management, but also 3d design and print, some electronics and others. i work flexible hours and honestly am looking to develop a network and experiences to land my first job that at least involves some coding - i'm happy to commit significant time and resource to interesting projects to get eventually my foot or at least my nose in a door somewhere. (i dont mean i would stop contributing to the project once i land a job!) thanks!"},
{"Title": "Analyzing Python Compression Libraries: zlib, LZ4, Brotli, and Zstandard", "Author": "u/dhilip-siva", "Content": "Source Code: https://github.com/dhilipsiva/py-compress-compare Analyzing Python Compression Libraries: zlib, LZ4, Brotli, and Zstandard When dealing with large volumes of data, compression can be a critical factor in enhancing performance, reducing storage costs, and speeding up network transfers. In this blog post, we will dive into a comparison of four popular Python compression libraries‚Äîzlib, LZ4, Brotli, and Zstandard‚Äîusing a real-world dataset to evaluate their performance in terms of compression ratio and time efficiency. The Experiment Setup Our test involved a dataset roughly 581 KB in size, named sample_data.json. We executed compression and decompression using each library as follows: Compression was performed 1000 times. Decompression was repeated 10,000 times. This rigorous testing framework ensures that we obtain a solid understanding of each library's performance under heavy load. Compression Ratio The compression ratio is a key metric that represents how effectively a compression algorithm can reduce the size of the input data. Here‚Äôs how each library scored: Zlib achieved a compression ratio of 27.84, LZ4 came in at 18.23, Brotli impressed with a ratio of 64.78, Zstandard offered a ratio of 43.42. From these results, Brotli leads with the highest compression ratio, indicating its superior efficiency in data size reduction. Zstandard also shows strong performance, while LZ4, though lower, still provides a reasonable reduction. Compression Time Efficiency isn't just about space savings; time is equally crucial. Here‚Äôs how long each library took to compress the data: Zlib: 7.34 seconds, LZ4: 0.13 seconds, Brotli: 204.18 seconds, Zstandard: 0.15 seconds. LZ4 and Zstandard excel in speed, with LZ4 being slightly faster. Zlib offers a middle ground, but Brotli, despite its high compression efficiency, takes significantly longer, which could be a drawback for real-time applications. Decompression Time Decompression time is vital for applications where data needs to be rapidly restored to its original state: Zlib: 11.99 seconds, LZ4: 0.46 seconds, Brotli: 0.99 seconds, Zstandard: 0.46 seconds. Again, LZ4 and Zstandard show excellent performance, both under half a second. Brotli presents a decent time despite its lengthy compression time, while zlib lags behind in this aspect. Conclusion Each library has its strengths and weaknesses: Brotli is your go-to for maximum compression but at the cost of time, making it suitable for applications where compression time is less critical. Zstandard offers a great balance between compression ratio and speed, recommended for a wide range of applications. LZ4 shines in speed, ideal for scenarios requiring rapid data processing. Zlib provides moderate performance across the board. Choosing the right library depends on your specific needs, whether it‚Äôs speed, space, or a balance of both. This experiment provides a clear picture of what to expect from these libraries, helping you make an informed decision based on your application's requirements."},
{"Title": "I wrote a book on Python Regular Expressions - and made it free!", "Author": "u/code_x_7777", "Content": "Hi! I've just sorted through some of my books on Leanpub and decided to make the book on Python Regular Expressions free to download for a couple of days. Why? Just for fun and learning and to get some valuable feedback. Free ebook link: https://leanpub.com/regexpython/ Regex Video Tutorials: https://blog.finxter.com/python-regex/ This book helps you learn regular expressions chapter by chapter. Each chapter first teaches the concepts and then asks you to solve Python puzzles (\"Guess the code!\") to master the concepts. The main idea is to keep it lightweight and fun. Here's the ToC: Introduction Applications About Puzzle Learning Basics Special Symbols Character Sets Dot Regex Asterisk Quantifier Plus Quantifier ? Quantifier Quantifier Differentiation Greediness Line and String Boundaries OR Regex AND Regex NOT Regex Matching Groups Split Method Substitution Method Compile Method Bonus Puzzles Final Remarks I hope you enjoy learning from the book! Feel free to send me your feedback. Happy learning \"R[e-x]+\"! ü§ì"},
{"Title": "How Malicious Python Code Gains Execution", "Author": "u/louis11", "Content": "The primary vector for malicious code running in software developer environments (e.g., local system, CI/CD runners, production servers, etc.) is software dependencies. This is third-party code which often means open-source software, also known as running code from strangers on the internet. The prized goal for attackers is arbitrary code execution. It‚Äôs the stuff high CVE scores are made of and often the topic of how vulnerabilities can turn into exploits. It‚Äôs the foothold needed to run cryptominers, steal secrets, or encrypt data for ransom. It‚Äôs no wonder why threat actors want it, but how do they get it? Sutton‚Äôs Law makes it obvious why they go after open-source software: because executing arbitrary code is easy there. This is a series examining the methods malicious Python code gains execution. Some of the methods are obvious and some are potentially undiscovered or at least not found in the wild, yet. What they all mostly have in common is the reliance on a software dependency in the form of a Python package, which is where we begin. https://blog.phylum.io/how-malicious-python-code-gains-execution/"},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Medical imaging Python jobs", "Author": "u/Durysik", "Content": "Hi folks! I am currently looking for a US (west coast best) based job where I could grow as a python programmer, specifically analysing medical images. Looking online I found some labs where they work with images, but I thought I'd ask (not sure if it's the right sub tho) cause I'm sure this way I'll hear about many places that deal with Medical Imaging. Thanks :)"},
{"Title": "tab-pal: a TUI for adding and editing custom colour palettes in Tableau", "Author": "u/Ok-Frosting7364", "Content": "Source code : https://github.com/ben-n93/tab-pal What my Project Does Makes it quicker, easier and prettier to create custom colour palettes in Tableau. Rather than having to find your Preferences.tps file every time and edit the XML you can just launch tab-pal from the command-line and make any changes you want (adding/deleting new palettes, adding/removing colours. You also get a preview of the colours you've added. Target Audience Data/business analysts and data scientists and who work with Tableau. Comparison There are some websites that let you dynamically create the Preferences.tps XML file however this still requires you to locate the file and copy and paste  over it with any changes you make"},
{"Title": "A Free GPT-Powered Financial Analysis Tool", "Author": "u/Starks-Technology", "Content": "Link to GitHub Page What My Project Does I created a simple financial analysis tool using the SimFin API and the GPT API. This tool summarizes financial information and makes it digestable for people who don't know a lot about finance. Target Audience This is more of a toy project for those interested in AI and Finance. It's meant for people without a ton of GPT experience to see a tangible use-case of generative AI within the field. Comparison Even though this is a simple tool, I don't know of any existing alternative. Example Input: Analyze Apple's q1 2023 earnings Output: AI Assistant:  Based on the provided financial information, here is a summary of the company's finances: Balance Sheet:- Total Assets: $346,747,000,000- Total Liabilities: $290,020,000,000- Total Equity: $56,727,000,000The company has a strong balance sheet with a significant amount of assets compared to its liabilities. This indicates a healthy financial position. Cash Flow:- Net Cash from Operating Activities: $34,005,000,000- Net Cash from Investing Activities: -$1,445,000,000- Net Cash from Financing Activities: -$35,563,000,000- Net Change in Cash: -$3,003,000,000The company generated positive cash flow from its operating activities, but had negative cash flow from investing and financing activities. As a result, there was a decrease in cash during the period. Profitability Metrics:- Gross Profit Margin: 42.96%- Operating Margin: 30.74%- Net Profit Margin: 25.61%- Return on Equity: 52.88%- Return on Assets: 8.65%- Return on Invested Capital: 15.28%The company has healthy profitability metrics, indicating efficient operations and good returns on investment. Liquidity Metrics:- Current Ratio: 1.01The company has a current ratio slightly above 1, which suggests it has enough current assets to cover its short-term liabilities. However, it is important to note that a current ratio of exactly 1 may indicate limited liquidity. Solvency Metrics:- Liabilities to Equity Ratio: 5.11- Debt Ratio: 32.04%The company has a relatively high liabilities to equity ratio, indicating a significant amount of debt compared to equity. The debt ratio is moderate, suggesting that a significant portion of the company's assets is financed by debt. Cash Flow Metrics:- Free Cash Flow: $39,273,000,000- Free Cash Flow to Net Income: 1.31- Cash Return on Invested Capital: 17.77%The company has positive free cash flow, indicating its ability to generate cash after accounting for capital expenditures. The free cash flow to net income ratio suggests that the company is efficient in converting its net income into free cash flow. The cash return on invested capital is also positive, indicating good returns on the capital invested. Other Important Metrics:- Piotroski F-Score: 4- Net Debt / EBITDA: 1.53- Dividend Payout Ratio: 12.56%The Piotroski F-Score of 4 suggests that the company has a moderate financial strength. The net debt to EBITDA ratio indicates the company's ability to repay its debt, with a ratio of 1.53. The dividend payout ratio suggests that the company distributes a portion of its earnings as dividends.Based on the provided information, the company appears to be in a strong financial position with healthy profitability metrics and positive cash flow. However, it is important to conduct further research and analysis to fully understand the company's financial health and prospects. Feel free to fork it, make PRs for it,or leave feedback!"},
{"Title": "I made a Tkinter \"DevTools\" to inspect and modify widgets in your running app in real-time", "Author": "u/254hypebeast", "Content": "source: https://github.com/ObaraEmmanuel/Formation pypi: https://pypi.org/project/formation-studio/ What My Project Does Allows you to inspect widgets in your running Tk app in real-time. You can view the widget hierarchy, modify widget attributes, adjust widget layout and run arbitrary code to interact with your widgets through the embedded Python REPL console. It works just like DevTools in a browser. This debugger is part of the Formation studio project which is a drag-n-drop graphical UI builder for Tkinter. Target Audience Any Tk developer seeking to have an easier time debugging their UI or seeking to experiment with the Tk framework with minimal effort. Comparison There is no project currently doing this same thing. Usage It comes bundled with Formation Studio so the installation is as simple as pip install formation-studio You don't have to change anything in your code. Simply use the following command and the debugger will attach itself to your app: formation-dbg /path/to/your/tk/app.py In the embedded python REPL console you can access a simple debugger API as follows: # Access a list of all widgets currently selected widgets = debugger.selection # Access the root widget usually a Tk object root = debugger.root"},
{"Title": "I made a Python CLI Tools for Competitive Programming", "Author": "u/Old-Manufacturer6209", "Content": "Source Code: https://github.com/JeanExtreme002/FastSnake PyPI: https://pypi.org/project/FastSnake/ What My Project Does FastSnake is a command-line tool that allows you to easily create, expand, run, and test Python solutions for competitive programming problems. üêçüèÅ This project provides useful CLI tools for competitive programming, such as test case generators, algorithms and data structures, tools for platforms Codeforces and AtCoder , and other features that assist you during the development and testing of solutions, besides building a nice directory structure to develop your solutions. ü§ì Target Audience This project was developed for programming competitors, focusing on users of CodeForces and AtCoder, who need to quickly develop solutions for complex problems Comparison There is no project currently doing this same thing. How to Install Just type the following commands: $ pip install fastsnake $ fastsnake -v Documentation Explore the documentation of FastSnake package at repository's project."},
{"Title": "Meteor Science using Python - Creating a \"weird\" coordinate system", "Author": "u/MrAstroThomas", "Content": "Hey everyone, I am creating \"Space Science with Python\" tutorials on YouTube with free and open accessible Python code on GitHub . It is not fancy or a \"super high animation quality YouTube production\". I am just an astrophysicists (what a difficult word to write), working in industry who continues with science as a hobby. Ha! And I have some crazy niche knowledge that is not covered by Gemini or chatGPT (yet). Anyway, I am creating now a tutorial on meteor science and wanted to show you how to create a coordinate system that co-rotates with the Earth while it is revolving around the Sun. Why is this interesting? Well, you may have heard about certain meteor streams like the Perseids in August or the Geminids in December. Dedicated streams that are associated with e.g. a particular comet or asteroid. However there are meteors that appear \"random\". So called sporadics. These sporadics have certain source regions, like e.g. the Apex. Cool, what the heck is an Apex? The Apex is Earth's \"flight direction\" in the Solar System. Imagine viewing the Sun and Earth from top of the Solar System: Apex is rotating with the Earth. Thus, it is not a fixed coordinate system. Take this image from Sky & Telescope that helps you imaging this stuff. Now the resulting regions of interests can be seen on this NASA page . In a sky map / plot you see different sources. ... but wouldn't it be cool to do it yourself? With your own data and Python code? And that's where I try to jump in. Check out the code and the corresponding video . I am looking forward to any feedback / comment"},
{"Title": "SecretScraper: highly configurable web crawler/scraper for extracting sensitive data from websites", "Author": "u/PadishahIII", "Content": "Hi, I'm a cybersecurity enthusiastic. And I've made a web crawler/scraper tool to extract links and sensitive information against target websites. You can find it here: https://github.com/PadishahIII/SecretScraper . What My Project Does SecretScraper is a highly configurable web scraper tool that crawls links, extracts subdomains from target websites and finds sensitive data using regular expressions. The features included in the SecretScraper are: Web crawler: extract links using both DOM hierarchy and regex Support for domain whitelist and blacklist Support multiple targets, enter target URLs from a file Scalable customisation: header, proxy, timeout, cookie, scrape depth, follow redirect, etc. Built-in regex to search for sensitive information: hyperscan is employed for higher performance Flexible configuration in yaml format Target Audience SecretScraper is made for penetration tester or web developer who can use this tool for info-gathering and finding any sensitive data or route of any website. Comparison A similar project is LinkFinder , an awesome python script written to discover endpoints and their parameters in JavaScript files. But I was expecting a project with more general use and more functionality. So I am developing this project half for practice and half with the intension of integrating it in a larger design. Use Case There is full documentation available in Github: https://github.com/PadishahIII/SecretScraper . Simply install via pip install secretscraper and see secretscraper --help ."},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "TypeIs does what I thought TypeGuard would do in Python", "Author": "u/Ok_Analysis_4910", "Content": "While it's unfortunate to have two constructs‚ÄîTypeGuard and TypeIs‚Äîwith slightly different behaviors, I'm glad that the latter is less surprising. https://rednafi.com/python/typeguard_vs_typeis/"},
{"Title": "Are PEP 744 goals very modest?", "Author": "u/MrMrsPotts", "Content": "Pypy has been able to speed up pure python code by a factor of 5 or more for a number of years. The only disadvantage it has is the difficulty in handling C extensions which are very commonly used in practice. https://peps.python.org/pep-0744 seems to be talking about speed ups of 5-10%. Why are the goals so much more modest than what pypy can already achieve?"},
{"Title": "Pure Python Physics Engine", "Author": "u/More-Tower9993", "Content": "What My Project Does The Physics Engine Called PhysEng, provides an easy to use environment and visualization combo in which to try out different physics or even provide a template to design your own accelleration/velocity fields. Besides the visualization aspect and numpy the basic functions of the Engine are written completely in 100% python. The features included in the Engine are: Particles, Soft Bodies, Anchor points Built in Fields: Drag, Uniform Force Fields, Gravity Between Particles, Octree Gravity etc Make your own: There are standard templates included in the Examples to design your own fields Springs - Construct Soft Bodies using Springs. (Built in soft bodies: Cloth and ball Target Audience PhysEng is made for people who just want to try out different simple simulations or want to design their own physics. Comparison Looking through github I could never really find a simple and easy-to-use library that did not require me to install some weird libraries or that felt like it was hiding some process from me through using packages. This package is a solution to this since everything is written in python nothing is a secret and can be directed easily. Get PhysEng There is full documentation available in the Github repo: https://github.com/levi2234/PhysEng"},
{"Title": "In what way do you try out small things when developing?", "Author": "u/HatWithAChat", "Content": "I've noticed at work that my coworkers and I try out small things in different ways. Small things like if you want to try that adding two datetimes together behaves in the way you expect. Some people use jupyter notebook for this and others run python interactively in a separate command prompt. I usually run debug in whatever IDE I'm using and then letting it stop at the code I'm currently developing and then using the debug console to test things out. Sometimes this means just leaving the debugger at a breakpoint for half an hour while I continue writing code. Is my way of doing it weird or does it have any disadvantages? How do you usually test out things on the go in a good way?"},
{"Title": "ArchiveFile: Unified interface for tar, zip, sevenzip, and rar files", "Author": "u/PredatorOwl", "Content": "What My Project Does archivefile is a wrapper around tarfile , zipfile , py7zr , and rarfile . The above libraries are excellent when you are dealing with a single archive format but things quickly get annoying when you have a bunch of mixed archives such as .zip , .7z , .cbr , .tar.gz , etc because each library has a slightly different syntax and quirks which you need to deal with. archivefile wraps the common methods from the above libraries to provide a unified interface that takes care of said differences under the hood. However, it's not as powerful as the libraries it wraps due to lack of support for features that are unique to a specific archive format and library. Target audience Anyone who's using python to deal with different archive formats Comparison ZipFile, TarFile, RarFile, and py7zr - These are libraries that mine wraps since each of them can only deal with a single archive format shutil - Shutil can only deal with zipfile and tarfile and only allows full packing or full extraction. patool - Excellent library that deals with wider range of formats than mine but in doing so it provides less granular control over each ArchiveFile falls somewhere between the powerful dedicated library and the far less powerful universal libaries. Links Repository: https://github.com/Ravencentric/archivefile Docs: https://ravencentric.github.io/archivefile"},
{"Title": "I made an easy and secure data lake for Pandas", "Author": "u/realstoned", "Content": "What My Project Does Shoots is essentially a \"data lake\" where you can easily store pandas dataframes, and retrieve them later or from different locations or in different tools. Shoots has a client and a server. After choosing a place to run the server, you can easily use the client to \"put\" and \"get\" dataframes. Shoots supports SQL, allowing you to put very large dataframes, and then use a query to only get a subset. Shoots also allows you to resample on the server. # put a dataframe, uploads it to the server df = pd.read\\_csv('sensor\\_data.csv') shoots.put(\"sensor\\_data\", dataframe=df, mode=PutMode.REPLACE) # retrieve the whole data frame df0 = shoots.get(\"sensor\\_data\") print(df0) # or use sql to retrieve just some of the data sql = 'select \"Sensor\\_1\" from sensor\\_data where \"Sensor\\_2\" < .2' df1 = shoots.get(\"sensor\\_data\", sql=sql) Target Audience Shoots is designed to be used in production by data scientists and other python devs using pandas. The server is configurable to run in various settings, including locally on a laptop if desired. It is useful for anyone who wants to share dataframes, or store dataframes so they can be easily accessed from different sources. Comparison To my knowledge, Shoots is the only data lake with a client that is 100% pandas native. The get() method returns pandas dataframes natively, so there is no cumbersome translations such as required from typical databases and data lakes. The server is build on top of Apache Arrow Flight, and is very efficient with storage because it uses Parquet as the storage format natively. While the Shoots client does all of the heavy listing, if desired, the server can be accessed with any Apache Flight client library, so other languages are supported by the server. Get Shoots There is full documentation available in the Github repo: https://github.com/rickspencer3/shoots It is packaged for Pypi as well: ( https://pypi.org/project/shoots/ ) ```pip install shoots\""},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Share Proejct: NLLB-200 Distill 350M en-ko", "Author": "u/SaeChan5", "Content": "Hello , I'm excited to share a project that was initially intended to use in my graduating product(Capstone) What My Proeject Does I made NLLB-200 Distill 350M model to translating English to Korean Target Audience GPU servers are quite expensive, so I made it for university students who can't cost the server (like me.) Comparison It's even smaller and faster the other NLLB-200 model. so it can be run with CPU! more details are in my page If you know Korean, please give me a lot of feedback https://github.com/newfull5/NLLB-200-Distilled-350M-en-ko thank you!!"},
{"Title": "ASCII plot backend package for matplotlib", "Author": "u/jetpack_away", "Content": "Hi I've made a package called mpl_ascii which is a backend for matplotlib. You can find it here: https://github.com/chriscave/mpl_ascii I would love to share it with others and see what you guys think What it is It is a backend for matplotlib that converts your plots into ASCII characters. At the moment I have only made support for: bar charts, scatter plots and line plots but if there's demand for more then I would love to keep working on it. Target Audience: Anyone using matplotlib to create plots who might also want to track how their plots change with their codebase (i.e. version control). Comparison: There are a few plotting libraries that produce ASCII plots but I have only come across this one that is a backend for matplotlib: https://github.com/gooofy/drawilleplot . I think it's a great package and it is really clever code but I found it a little lacking when you have multiple colours in a plot. Let me know if you know of other matploblib backends that does similar things. Use case: A use case I can think of is for version controlling your plots. Having your plot as a txt format means it can be much easier to see the diff and the files you are committing are much smaller. Since it is only a backend to matplotlib then you only need to switch to it and you don't need to recreate your plots in a different plotting library. Thanks for reading and let me know what you think! :)"},
{"Title": "Cross platform python3 shebang", "Author": "u/tedkotz", "Content": "There is no shebang line that actually works across platforms for python 3. I would like one that works on unmodified : Debian shell (Dropped python2, falls under PEP 394) Older Linux shells that still have python pointing to python2 (PEP 394) Windows cmd.exe shell (this really just means one that will work with PEP 397) Gitbash for Windows (sort of a weird half sibling that respects shebangs) The best work around I have found is: use #!/usr/bin/env python3 on Windows copy python.exe to python3.exe Then make sure both are in your path for unix-like shells. Debian make sure python-is-python2 or python-is-python3 is installed, in case you come upon a #!/usr/bin/env python . As Windows adopts more and more Unix-like behavior, and more distros drop python2, having completely different \"portability\" rules is going to become a larger problem. A significant compatibility enhancement would be if the official python packages for Windows just included a python3.exe to comply with PEP 394. This could be a copy of python.exe like my workaround, or one could be a minimal executable that just hands off to the other or to py . An alternative would be adding py and pyw from PEP 397 to PEP 394. and having people move to the shebang #!/usr/bin/env py -3 . The belt and suspenders compatibility approach is all platforms should have a py , pyw , and python3 executable that can launch python3 scripts if requested. And python should be an executable than runs some version of python. I am curious what others are using out there? Do others launch python scripts from inside gitbash? do you have a seperate window for running the script and git actions? Are you manually choosing the python executable on the command line?"},
{"Title": "American Airlines scraper made in Python with only http requests", "Author": "u/JohnBalvin", "Content": "Hello wonderful community, Today I'll present to you pyaair, a scraper made pure on Python https://github.com/johnbalvin/pyaair Easy instalation ` ` `pip install pyaair ` ` ` Easy Usage ` ` ` airports=pyaair.airports(\"miami\",\"\") ` ` ` Always remember, only use selenium, puppeteer, playwright etc when it's strictly necesary Let me know what you think, thanks About me: I'm full stack developer specialized on web scraping and backend, with 6-7 years of experience"},
{"Title": "I made a Python app that turns your Figma design into code", "Author": "u/axorax", "Content": "Link: https://github.com/Axorax/tkforge Hey, my name is Axorax. I have been programming for a few years now. I started making a lot more projects in Python recently and this is one of them. I decided to call the project TkForge. What My Project Does TkForge allows you to turn your Figma design into code. So, you can make the UI for an app in Figma and add input fields, buttons and much more and name them properly then you can run TkForge to convert your Figma design into code. The names need to be the element that you want. For example; if you want a button element then you can name it \"button\" or \"button Hello World!\". The \"Hello World!\" portion will just get ignored. All of the text after the first space is ignored. However, for some elements, they matter. Like, if you want a textbox element with the placeholder text of \"Hello\" then you need to name it \"textbox Hello\". Target Audience It is meant for anyone who wants to create a GUI in Python easily. Dealing with Tkinter can be a pain a lot of times and it also takes a long time. Using TkForge, you can make better UI's and also get a lot of work done in a short amount of time. Anyone who is new to Python or even an expert can use TkForge to speed up their development times and get a better result. You can TkForge in your production app or for a demo app and really anywhere if you are also using Python. Comparison There is another project called TkDesigner that does the same sort of thing. But TkForge is able to generate better code. TkForge also supports a lot more elements. Placeholder text for textbox and textarea are not built-in to Python. But TkForge has support for those even though using them requires you to handle a lot of situations by yourself (TkForge provides functions for these situations to be handled correctly, you need to implement them were needed yourself). Thank you for reading! <3"},
{"Title": "milkcow - First package/library", "Author": "u/Samuel_G_Reynoso", "Content": "Excited to share milkcow, my first python package. I'd love any feedback, and to continue to build out the parts of this package that show potential. https://pypi.org/project/milkcow/ https://github.com/SamReynoso/milkcow What MilkCow Does Milkcow automates database creation and offers in-memory key-value mapping for data handling. Whether you're building middleware, local storage, or multiprocessing scripts. Target Audience MilkCow is designed for developers looking to streamline the development process. It caters to those who want to simplify data. Comparison Milkcow aims for simplicity. Milkcow offers a way for making it easier for developers to get started. Additional functionalities, including database creation and the in-memory datastore, enhancing its usability. from milkcow import ObjectCow oc = ObjectCow(Record) oc.push('Bob', records) objs = oc.new('Bob') k, v = oc.items() for k in oc.keys() new = oc.new(k) from milkcow import MilkCow mc = MilkCow(Record) mc.pull('Bob') mc.push('Alice', list[Record]) sender = mc.sender.new_sender() sender = mc.sender.all_sender() sender = mc.sender.keyed_sender('Alice') sender.send()"},
{"Title": "Python Quality Standards", "Author": "u/nicomarcan", "Content": "Hey, happy Friday (don't push to prod). Me and some friends are building a no-code platform to run code improvement agents (really in BETA) . We want to have a quality agent for each language, and I would really appreciate your feedback on python best practices and standards. The agents are created by defining the steps that you want to apply in natural language. Right now our Python agent has the following steps: Use descriptive naming for functions and variables. Add Type Hints. Add proper docstrings. Make docstrings follow PEP-257 standard. All variables and functions should be snake_case. Add proper input validation that checks for type and not null. If the input validation fails raise an Exception. Add useful logs for debugging with the logging library. In case you want to check our tool, we have a free playground right now at GitGud and are working on github PR integrations. Happy coding and thank you in advance for your help! Edit: Of course the steps are really basic right now, we are still testing the POC, so any feedback would be really appreciated"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "How To Build a Social Media Sentiment Analysis Pipeline With FastAPI And Generative AI", "Author": "u/arthurdelerue25", "Content": "Social media like Reddit, Hacker News, Twitter, etc. contain tons of genuine discussions that you might want to analyze automatically with sentiment analysis. For example you might want to monitor what people say about you, your product, your competitors, etc. I made a technical article that shows how to implement such a sentiment analysis pipeline using the following steps: Implement social media listening Integrate the data in your system with an API webhook processed in Python/FastAPI Analyze the sentiment thanks to generative AI models like GPT-4, LLaMA 3, ChatDolphin, etc. Here it is: https://kwatch.io/how-to-build-a-social-media-sentiment-analysis-pipeline I hope you will find it useful. If you have some comments about how to improve this pipeline I would love to hear them! Arthur"},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Python Interview Cheat Sheet Website!", "Author": "u/ixatrap", "Content": "Hey everyone, I‚Äôve recently launched a new website aimed at helping fellow programmers ace their Python interviews. It‚Äôs not just limited to Python though; it also covers essential topics like big-O notation, object-oriented programming, design patterns, and more! I‚Äôd love to hear your thoughts and feedback on the content, layout, and anything else you think could be improved. Check it out here https://hlop3z.github.io/interviews-python/ and let me know what you think. Your input is invaluable in making this resource the best it can be. Thanks in advance for your time and insights! üöÄüêç Note: It‚Äôs mainly to be used in a computer or tablet. You can see it in your mobile, but some sections won‚Äôt look as intended."},
{"Title": "Python Test 219: Building Django Apps & SaaS Pegasus - Cory Zue", "Author": "u/variedthoughts", "Content": "Listen at podcast.pythontest.com/219 When starting a SaaS project using Django, there are tons of decisions. I've asked Cory Zue, creator of SaaS Pegasus, to help me sift through some common SaaS/Django decisions."},
{"Title": "üî≠ OpenTelemetry Architecture: Python SDK Overview", "Author": "u/roma-glushko", "Content": "Hey folks, I have just posted an article for those who want to go a little bit beyond the basic usage of OTEL and understand how it works under the hood. The post quickly touches on: - üî≠ History and the idea of OpenTelemetry - üßµ Distributed traces & spans. How span collection happens on the service side - üíº Baggage & trace ctx propagation - üìà Metrics collection. Views & aggregations. Metrics readers - üìë OTEL Logging integration - ü§ù Semantic conventions and why that is important Blog Post: https://www.romaglushko.com/blog/opentelemetry-sdk/ Let me know what do you think and hope this is helpful for someone üôå"},
{"Title": "How to create a Discord Bot to launch a Minecraft server automatically", "Author": "u/chadicus-gigo", "Content": "Hi y'all, This might be off topic a bit from the normal stuff that gets posted here, but a while back I was messing around on a Minecraft server that I was hosting on Aternos (a free online Minecraft server host). Now since the server wasn't running on my computer (and since Aternos has a timeout policy to save server resources) it became annoying for my buddies to hop on the server if I was busy. They'd have to ask me to manually start the server from the webpage. So I had a free Saturday a couple of months ago and decided to remedy this problem. My source code is hosted on a GIGO Dev environment if y'all wanna check it out: https://www.gigo.dev/challenge/1732810200471044096 But to summarize it quickly... I used Playwright and the Discord API in Python to simulate a browser navigating to an Aternos server and starting it. For this tutorial you will need to make an Aternos acct, but their platform is free and very useful if you want to spin up a Minecraft server quickly without using your own resources. You simply need to configure a trigger command or @ the bot and set up a webhook for the the discord server you want to use. There's a full tutorial on how I set it up in the link along with the project structure, but truth be told the basic implementation is pretty simple and can be tweaked to work really however you want. Just wanted to share this to see if anyone had done something similar before, or if I'm just insane and made this mundane problem into a way bigger endeavor than it should be"},
{"Title": "What are your favourite pre-commit hooks and why?", "Author": "u/LatterConcentrate6", "Content": "Just getting started with pre-commit and I think it's awesome. Looking to find out what other code automation tools people are using. Let me know what works for you and why. Thanks!"},
{"Title": "Sensor-App: A Sensor Data Displaying/Streaming Android App written in Python", "Author": "u/StoneSteel_1", "Content": "Sensor-App is an Android App that's main focus is to help create a real-time mobile sensor data stream for computer applications, data collection, AR, VR, etc. Github: SensorApp Features of Sensor-App Real-Time Sensor Data display Faster Real-Time Sensor Data Streaming via TCP Sockets Simple and Easy setup of Data Streaming Server What my Project Does My project is aimed to help provide a Real-Time Mobile Sensor data streaming service. Target Audience Computer Programmers, Data Scientists, AR and VR enthusiasts Remarks This Application was made with help of Beeware Tools. I made this Application to test out abilities of Beeware Tools, and get Experience in Android App develpoment with python Currently the Project only contains Accelerometer, and I will update it soon to support other sensors too. I am always open hear advice, constructive criticism about my project. I would like to hear your opinion of my project :} Thanks for Reading, hope you try out my project :)"},
{"Title": "UXsim 1.2.0 released with support for (self-driving) taxis and shared mobility", "Author": "u/Balance-", "Content": "Version 1.2.0 of UXsim is released, which allows simulating taxis, shared mobility and self-driving taxis! Main Changes in 1.2.0 Add taxi (aka. shared mobility) functions A standard vehicle in UXsim just travel from A to B and disappear. This is like a private owned vehicle. From this update, a Vehicle with mode=\"taxi\" behave like a taxi. Specifically, they travel through a network by passing through specific nodes that are dynamically updated, simulating passenger pickup and drop-off. New sub-module uxsim.TaxiHandler handles these matters. Built-in vehicle-to-passneger matching methods are also available. This addresses Issue #41 From now on, we follow the Semantic Versioning rigorously. UXsim UXsim is a free, open-source macroscopic and mesoscopic network traffic flow simulator written in Python. It simulates the movements of car travelers and traffic congestion in road networks. It is suitable for simulating large-scale (e.g., city-scale) traffic phenomena. UXsim is especially useful for scientific and educational purposes because of its simple, lightweight, and customizable features, but users are free to use UXsim for any purpose."},
{"Title": "Best way to grade Jupyter-Notebooks?", "Author": "u/flying_Wahale", "Content": "I recently took a job with a professor, that includes helping with the grading of biweekly assignments. So I basically have now 30 Notebooks that I have to grade. Top of my head I can think of these approaches: Convert to PDF and write into the PDF Duplicate the Notebook and write the comments in extra blocks Create a .txt file with all my note Does anybody have experience with this and can share their workflow?"},
{"Title": "What is your best Tkinter project that you put a lot of effort into creating?", "Author": "u/_-Dan_-", "Content": "I want to hear your description of the project you are most proud of. I was just trying to find something on the Internet, but I didn't find much of interest. The only difficult project was CustomTkinter , which, as far as I know, was done entirely in Tkinter. Since it is possible to create something this complex, I'm sure you have something cool too. Note: I am expecting to hear projects more complex than a calculator :)"},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Pure Python Physics Engine", "Author": "u/More-Tower9993", "Content": "What My Project Does The Physics Engine Called PhysEng, provides an easy to use environment and visualization combo in which to try out different physics or even provide a template to design your own accelleration/velocity fields. Besides the visualization aspect and numpy the basic functions of the Engine are written completely in 100% python. The features included in the Engine are: Particles, Soft Bodies, Anchor points Built in Fields: Drag, Uniform Force Fields, Gravity Between Particles, Octree Gravity etc Make your own: There are standard templates included in the Examples to design your own fields Springs - Construct Soft Bodies using Springs. (Built in soft bodies: Cloth and ball Target Audience PhysEng is made for people who just want to try out different simple simulations or want to design their own physics. Comparison Looking through github I could never really find a simple and easy-to-use library that did not require me to install some weird libraries or that felt like it was hiding some process from me through using packages. This package is a solution to this since everything is written in python nothing is a secret and can be directed easily. Get PhysEng There is full documentation available in the Github repo: https://github.com/levi2234/PhysEng"},
{"Title": "In what way do you try out small things when developing?", "Author": "u/HatWithAChat", "Content": "I've noticed at work that my coworkers and I try out small things in different ways. Small things like if you want to try that adding two datetimes together behaves in the way you expect. Some people use jupyter notebook for this and others run python interactively in a separate command prompt. I usually run debug in whatever IDE I'm using and then letting it stop at the code I'm currently developing and then using the debug console to test things out. Sometimes this means just leaving the debugger at a breakpoint for half an hour while I continue writing code. Is my way of doing it weird or does it have any disadvantages? How do you usually test out things on the go in a good way?"},
{"Title": "ArchiveFile: Unified interface for tar, zip, sevenzip, and rar files", "Author": "u/PredatorOwl", "Content": "What My Project Does archivefile is a wrapper around tarfile , zipfile , py7zr , and rarfile . The above libraries are excellent when you are dealing with a single archive format but things quickly get annoying when you have a bunch of mixed archives such as .zip , .7z , .cbr , .tar.gz , etc because each library has a slightly different syntax and quirks which you need to deal with. archivefile wraps the common methods from the above libraries to provide a unified interface that takes care of said differences under the hood. However, it's not as powerful as the libraries it wraps due to lack of support for features that are unique to a specific archive format and library. Target audience Anyone who's using python to deal with different archive formats Comparison ZipFile, TarFile, RarFile, and py7zr - These are libraries that mine wraps since each of them can only deal with a single archive format shutil - Shutil can only deal with zipfile and tarfile and only allows full packing or full extraction. patool - Excellent library that deals with wider range of formats than mine but in doing so it provides less granular control over each ArchiveFile falls somewhere between the powerful dedicated library and the far less powerful universal libaries. Links Repository: https://github.com/Ravencentric/archivefile Docs: https://ravencentric.github.io/archivefile"},
{"Title": "I made an easy and secure data lake for Pandas", "Author": "u/realstoned", "Content": "What My Project Does Shoots is essentially a \"data lake\" where you can easily store pandas dataframes, and retrieve them later or from different locations or in different tools. Shoots has a client and a server. After choosing a place to run the server, you can easily use the client to \"put\" and \"get\" dataframes. Shoots supports SQL, allowing you to put very large dataframes, and then use a query to only get a subset. Shoots also allows you to resample on the server. # put a dataframe, uploads it to the server df = pd.read\\_csv('sensor\\_data.csv') shoots.put(\"sensor\\_data\", dataframe=df, mode=PutMode.REPLACE) # retrieve the whole data frame df0 = shoots.get(\"sensor\\_data\") print(df0) # or use sql to retrieve just some of the data sql = 'select \"Sensor\\_1\" from sensor\\_data where \"Sensor\\_2\" < .2' df1 = shoots.get(\"sensor\\_data\", sql=sql) Target Audience Shoots is designed to be used in production by data scientists and other python devs using pandas. The server is configurable to run in various settings, including locally on a laptop if desired. It is useful for anyone who wants to share dataframes, or store dataframes so they can be easily accessed from different sources. Comparison To my knowledge, Shoots is the only data lake with a client that is 100% pandas native. The get() method returns pandas dataframes natively, so there is no cumbersome translations such as required from typical databases and data lakes. The server is build on top of Apache Arrow Flight, and is very efficient with storage because it uses Parquet as the storage format natively. While the Shoots client does all of the heavy listing, if desired, the server can be accessed with any Apache Flight client library, so other languages are supported by the server. Get Shoots There is full documentation available in the Github repo: https://github.com/rickspencer3/shoots It is packaged for Pypi as well: ( https://pypi.org/project/shoots/ ) ```pip install shoots\""},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Share Proejct: NLLB-200 Distill 350M en-ko", "Author": "u/SaeChan5", "Content": "Hello , I'm excited to share a project that was initially intended to use in my graduating product(Capstone) What My Proeject Does I made NLLB-200 Distill 350M model to translating English to Korean Target Audience GPU servers are quite expensive, so I made it for university students who can't cost the server (like me.) Comparison It's even smaller and faster the other NLLB-200 model. so it can be run with CPU! more details are in my page If you know Korean, please give me a lot of feedback https://github.com/newfull5/NLLB-200-Distilled-350M-en-ko thank you!!"},
{"Title": "ASCII plot backend package for matplotlib", "Author": "u/jetpack_away", "Content": "Hi I've made a package called mpl_ascii which is a backend for matplotlib. You can find it here: https://github.com/chriscave/mpl_ascii I would love to share it with others and see what you guys think What it is It is a backend for matplotlib that converts your plots into ASCII characters. At the moment I have only made support for: bar charts, scatter plots and line plots but if there's demand for more then I would love to keep working on it. Target Audience: Anyone using matplotlib to create plots who might also want to track how their plots change with their codebase (i.e. version control). Comparison: There are a few plotting libraries that produce ASCII plots but I have only come across this one that is a backend for matplotlib: https://github.com/gooofy/drawilleplot . I think it's a great package and it is really clever code but I found it a little lacking when you have multiple colours in a plot. Let me know if you know of other matploblib backends that does similar things. Use case: A use case I can think of is for version controlling your plots. Having your plot as a txt format means it can be much easier to see the diff and the files you are committing are much smaller. Since it is only a backend to matplotlib then you only need to switch to it and you don't need to recreate your plots in a different plotting library. Thanks for reading and let me know what you think! :)"},
{"Title": "Cross platform python3 shebang", "Author": "u/tedkotz", "Content": "There is no shebang line that actually works across platforms for python 3. I would like one that works on unmodified : Debian shell (Dropped python2, falls under PEP 394) Older Linux shells that still have python pointing to python2 (PEP 394) Windows cmd.exe shell (this really just means one that will work with PEP 397) Gitbash for Windows (sort of a weird half sibling that respects shebangs) The best work around I have found is: use #!/usr/bin/env python3 on Windows copy python.exe to python3.exe Then make sure both are in your path for unix-like shells. Debian make sure python-is-python2 or python-is-python3 is installed, in case you come upon a #!/usr/bin/env python . As Windows adopts more and more Unix-like behavior, and more distros drop python2, having completely different \"portability\" rules is going to become a larger problem. A significant compatibility enhancement would be if the official python packages for Windows just included a python3.exe to comply with PEP 394. This could be a copy of python.exe like my workaround, or one could be a minimal executable that just hands off to the other or to py . An alternative would be adding py and pyw from PEP 397 to PEP 394. and having people move to the shebang #!/usr/bin/env py -3 . The belt and suspenders compatibility approach is all platforms should have a py , pyw , and python3 executable that can launch python3 scripts if requested. And python should be an executable than runs some version of python. I am curious what others are using out there? Do others launch python scripts from inside gitbash? do you have a seperate window for running the script and git actions? Are you manually choosing the python executable on the command line?"},
{"Title": "American Airlines scraper made in Python with only http requests", "Author": "u/JohnBalvin", "Content": "Hello wonderful community, Today I'll present to you pyaair, a scraper made pure on Python https://github.com/johnbalvin/pyaair Easy instalation ` ` `pip install pyaair ` ` ` Easy Usage ` ` ` airports=pyaair.airports(\"miami\",\"\") ` ` ` Always remember, only use selenium, puppeteer, playwright etc when it's strictly necesary Let me know what you think, thanks About me: I'm full stack developer specialized on web scraping and backend, with 6-7 years of experience"},
{"Title": "I made a Python app that turns your Figma design into code", "Author": "u/axorax", "Content": "Link: https://github.com/Axorax/tkforge Hey, my name is Axorax. I have been programming for a few years now. I started making a lot more projects in Python recently and this is one of them. I decided to call the project TkForge. What My Project Does TkForge allows you to turn your Figma design into code. So, you can make the UI for an app in Figma and add input fields, buttons and much more and name them properly then you can run TkForge to convert your Figma design into code. The names need to be the element that you want. For example; if you want a button element then you can name it \"button\" or \"button Hello World!\". The \"Hello World!\" portion will just get ignored. All of the text after the first space is ignored. However, for some elements, they matter. Like, if you want a textbox element with the placeholder text of \"Hello\" then you need to name it \"textbox Hello\". Target Audience It is meant for anyone who wants to create a GUI in Python easily. Dealing with Tkinter can be a pain a lot of times and it also takes a long time. Using TkForge, you can make better UI's and also get a lot of work done in a short amount of time. Anyone who is new to Python or even an expert can use TkForge to speed up their development times and get a better result. You can TkForge in your production app or for a demo app and really anywhere if you are also using Python. Comparison There is another project called TkDesigner that does the same sort of thing. But TkForge is able to generate better code. TkForge also supports a lot more elements. Placeholder text for textbox and textarea are not built-in to Python. But TkForge has support for those even though using them requires you to handle a lot of situations by yourself (TkForge provides functions for these situations to be handled correctly, you need to implement them were needed yourself). Thank you for reading! <3"},
{"Title": "milkcow - First package/library", "Author": "u/Samuel_G_Reynoso", "Content": "Excited to share milkcow, my first python package. I'd love any feedback, and to continue to build out the parts of this package that show potential. https://pypi.org/project/milkcow/ https://github.com/SamReynoso/milkcow What MilkCow Does Milkcow automates database creation and offers in-memory key-value mapping for data handling. Whether you're building middleware, local storage, or multiprocessing scripts. Target Audience MilkCow is designed for developers looking to streamline the development process. It caters to those who want to simplify data. Comparison Milkcow aims for simplicity. Milkcow offers a way for making it easier for developers to get started. Additional functionalities, including database creation and the in-memory datastore, enhancing its usability. from milkcow import ObjectCow oc = ObjectCow(Record) oc.push('Bob', records) objs = oc.new('Bob') k, v = oc.items() for k in oc.keys() new = oc.new(k) from milkcow import MilkCow mc = MilkCow(Record) mc.pull('Bob') mc.push('Alice', list[Record]) sender = mc.sender.new_sender() sender = mc.sender.all_sender() sender = mc.sender.keyed_sender('Alice') sender.send()"},
{"Title": "Python Quality Standards", "Author": "u/nicomarcan", "Content": "Hey, happy Friday (don't push to prod). Me and some friends are building a no-code platform to run code improvement agents (really in BETA) . We want to have a quality agent for each language, and I would really appreciate your feedback on python best practices and standards. The agents are created by defining the steps that you want to apply in natural language. Right now our Python agent has the following steps: Use descriptive naming for functions and variables. Add Type Hints. Add proper docstrings. Make docstrings follow PEP-257 standard. All variables and functions should be snake_case. Add proper input validation that checks for type and not null. If the input validation fails raise an Exception. Add useful logs for debugging with the logging library. In case you want to check our tool, we have a free playground right now at GitGud and are working on github PR integrations. Happy coding and thank you in advance for your help! Edit: Of course the steps are really basic right now, we are still testing the POC, so any feedback would be really appreciated"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "How To Build a Social Media Sentiment Analysis Pipeline With FastAPI And Generative AI", "Author": "u/arthurdelerue25", "Content": "Social media like Reddit, Hacker News, Twitter, etc. contain tons of genuine discussions that you might want to analyze automatically with sentiment analysis. For example you might want to monitor what people say about you, your product, your competitors, etc. I made a technical article that shows how to implement such a sentiment analysis pipeline using the following steps: Implement social media listening Integrate the data in your system with an API webhook processed in Python/FastAPI Analyze the sentiment thanks to generative AI models like GPT-4, LLaMA 3, ChatDolphin, etc. Here it is: https://kwatch.io/how-to-build-a-social-media-sentiment-analysis-pipeline I hope you will find it useful. If you have some comments about how to improve this pipeline I would love to hear them! Arthur"},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Python Interview Cheat Sheet Website!", "Author": "u/ixatrap", "Content": "Hey everyone, I‚Äôve recently launched a new website aimed at helping fellow programmers ace their Python interviews. It‚Äôs not just limited to Python though; it also covers essential topics like big-O notation, object-oriented programming, design patterns, and more! I‚Äôd love to hear your thoughts and feedback on the content, layout, and anything else you think could be improved. Check it out here https://hlop3z.github.io/interviews-python/ and let me know what you think. Your input is invaluable in making this resource the best it can be. Thanks in advance for your time and insights! üöÄüêç Note: It‚Äôs mainly to be used in a computer or tablet. You can see it in your mobile, but some sections won‚Äôt look as intended."},
{"Title": "Python Test 219: Building Django Apps & SaaS Pegasus - Cory Zue", "Author": "u/variedthoughts", "Content": "Listen at podcast.pythontest.com/219 When starting a SaaS project using Django, there are tons of decisions. I've asked Cory Zue, creator of SaaS Pegasus, to help me sift through some common SaaS/Django decisions."},
{"Title": "üî≠ OpenTelemetry Architecture: Python SDK Overview", "Author": "u/roma-glushko", "Content": "Hey folks, I have just posted an article for those who want to go a little bit beyond the basic usage of OTEL and understand how it works under the hood. The post quickly touches on: - üî≠ History and the idea of OpenTelemetry - üßµ Distributed traces & spans. How span collection happens on the service side - üíº Baggage & trace ctx propagation - üìà Metrics collection. Views & aggregations. Metrics readers - üìë OTEL Logging integration - ü§ù Semantic conventions and why that is important Blog Post: https://www.romaglushko.com/blog/opentelemetry-sdk/ Let me know what do you think and hope this is helpful for someone üôå"},
{"Title": "How to create a Discord Bot to launch a Minecraft server automatically", "Author": "u/chadicus-gigo", "Content": "Hi y'all, This might be off topic a bit from the normal stuff that gets posted here, but a while back I was messing around on a Minecraft server that I was hosting on Aternos (a free online Minecraft server host). Now since the server wasn't running on my computer (and since Aternos has a timeout policy to save server resources) it became annoying for my buddies to hop on the server if I was busy. They'd have to ask me to manually start the server from the webpage. So I had a free Saturday a couple of months ago and decided to remedy this problem. My source code is hosted on a GIGO Dev environment if y'all wanna check it out: https://www.gigo.dev/challenge/1732810200471044096 But to summarize it quickly... I used Playwright and the Discord API in Python to simulate a browser navigating to an Aternos server and starting it. For this tutorial you will need to make an Aternos acct, but their platform is free and very useful if you want to spin up a Minecraft server quickly without using your own resources. You simply need to configure a trigger command or @ the bot and set up a webhook for the the discord server you want to use. There's a full tutorial on how I set it up in the link along with the project structure, but truth be told the basic implementation is pretty simple and can be tweaked to work really however you want. Just wanted to share this to see if anyone had done something similar before, or if I'm just insane and made this mundane problem into a way bigger endeavor than it should be"},
{"Title": "What are your favourite pre-commit hooks and why?", "Author": "u/LatterConcentrate6", "Content": "Just getting started with pre-commit and I think it's awesome. Looking to find out what other code automation tools people are using. Let me know what works for you and why. Thanks!"},
{"Title": "Sensor-App: A Sensor Data Displaying/Streaming Android App written in Python", "Author": "u/StoneSteel_1", "Content": "Sensor-App is an Android App that's main focus is to help create a real-time mobile sensor data stream for computer applications, data collection, AR, VR, etc. Github: SensorApp Features of Sensor-App Real-Time Sensor Data display Faster Real-Time Sensor Data Streaming via TCP Sockets Simple and Easy setup of Data Streaming Server What my Project Does My project is aimed to help provide a Real-Time Mobile Sensor data streaming service. Target Audience Computer Programmers, Data Scientists, AR and VR enthusiasts Remarks This Application was made with help of Beeware Tools. I made this Application to test out abilities of Beeware Tools, and get Experience in Android App develpoment with python Currently the Project only contains Accelerometer, and I will update it soon to support other sensors too. I am always open hear advice, constructive criticism about my project. I would like to hear your opinion of my project :} Thanks for Reading, hope you try out my project :)"},
{"Title": "UXsim 1.2.0 released with support for (self-driving) taxis and shared mobility", "Author": "u/Balance-", "Content": "Version 1.2.0 of UXsim is released, which allows simulating taxis, shared mobility and self-driving taxis! Main Changes in 1.2.0 Add taxi (aka. shared mobility) functions A standard vehicle in UXsim just travel from A to B and disappear. This is like a private owned vehicle. From this update, a Vehicle with mode=\"taxi\" behave like a taxi. Specifically, they travel through a network by passing through specific nodes that are dynamically updated, simulating passenger pickup and drop-off. New sub-module uxsim.TaxiHandler handles these matters. Built-in vehicle-to-passneger matching methods are also available. This addresses Issue #41 From now on, we follow the Semantic Versioning rigorously. UXsim UXsim is a free, open-source macroscopic and mesoscopic network traffic flow simulator written in Python. It simulates the movements of car travelers and traffic congestion in road networks. It is suitable for simulating large-scale (e.g., city-scale) traffic phenomena. UXsim is especially useful for scientific and educational purposes because of its simple, lightweight, and customizable features, but users are free to use UXsim for any purpose."},
{"Title": "Best way to grade Jupyter-Notebooks?", "Author": "u/flying_Wahale", "Content": "I recently took a job with a professor, that includes helping with the grading of biweekly assignments. So I basically have now 30 Notebooks that I have to grade. Top of my head I can think of these approaches: Convert to PDF and write into the PDF Duplicate the Notebook and write the comments in extra blocks Create a .txt file with all my note Does anybody have experience with this and can share their workflow?"},
{"Title": "What is your best Tkinter project that you put a lot of effort into creating?", "Author": "u/_-Dan_-", "Content": "I want to hear your description of the project you are most proud of. I was just trying to find something on the Internet, but I didn't find much of interest. The only difficult project was CustomTkinter , which, as far as I know, was done entirely in Tkinter. Since it is possible to create something this complex, I'm sure you have something cool too. Note: I am expecting to hear projects more complex than a calculator :)"},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Zillow scraper made pure in Python", "Author": "u/JohnBalvin", "Content": "Hello everyone., on today new scraper I created the python version for the zillow scraper. https://github.com/johnbalvin/pyzill What My Project Does The library will get zillow listings and details. I didn't created a defined structured like on the Go version just because it's not as easy to maintain this kind of projects on python like on Go. It is made on pure python with HTTP requests, so no selenium, puppeteer, playwright etc. or none of those automation libraries that I hate. Target Audience This project target could be real state agents probably, so lets say you want to track the real price history of properties around an area, you can use it track it Comparison There are libraries similar outhere but they look outdated, most of the time, scraping projects need to ne on constant maintance due to changed on the page or api pip install pyzill Let me know what ou think, thanks about me: I'm full stack developer specialized on web scraping and backend, with 6-7 years of experience"},
{"Title": "Created Netlify DNS Manager: CLI Tool for uploading zone Files to Netlify or exporting Netlify DNS", "Author": "u/snorkell_", "Content": "What My Project Does? Command-line tool to simplify the management of DNS records for domains hosted on Netlify, whether you want to migrate to Netlify or migrate away from Netlify. Import Zone File from Godaddy, NameCheap or others to Netlify - helpful in migrating nameservers. Export DNS records from Netlify as zonefile to be import it to Godaddy, Namecheap or other servers. Zonefile is a list of all the DNS records for a given domain Here is the url - https://github.com/sumansaurabh/netlify-dns-manager Target Audience (e.g., Is it meant for production, just a toy project, etc.) Anyone who is intended to use Netlify DNS. Comparison (A brief comparison explaining how it differs from existing alternatives.) There is no such tool that can help in managing DNS in Netlify - hence I have created it."},
{"Title": "Py2wasm: A Python to Wasm compiler 3x faster than pyiodide", "Author": "u/desmoulinmichel", "Content": "Take the excellent nuitka , compile python code to C, turn it into web assembly, and you got Python in the browser, without the usual runtime overhead: https://wasmer.io/posts/py2wasm-a-python-to-wasm-compiler While the doc states you can get this effect by doing: pip install py2wasm py2wasm myprogram.py -o myprogram.wasm wasmer run myprogram.wasm You still need the wasmer WASM runtime ( curl https://get.wasmer.io -sSfL | sh for Unix users, iwr https://win.wasmer.io -useb | iex for Windows user with Powershell), however. But more than that, since you need nuikta, it means you need a C compiler installed. While in Ubuntu it's a just an sudo apt install build-essential , it will require a bit more work on Windows and Mac."},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Runtime type checking performance", "Author": "u/gerardwx", "Content": "I'm trying to decide whether to use typeguard or stick to assert isinstance in the places where I care. Has anyone done benchmarking testing of the overhead of using type guards \"at\"typeguard decorator ?"},
{"Title": "pwdgen v2 -a simple password generator", "Author": "u/RRTheGuy", "Content": "https://github.com/RadoTheProgrammer/pwdgen What my project does My project generate simple, strong, memorable and easy-to-type passwords. The system is simple: it generate 2 pronounceable words separated by a special char, with a number at start or end. I tried creating a password generator that combines simplicity, security, memorability, and ease of type. This should be secure enough because it can generate 4e15 possibilities of passwords and uses the secrets module. Target audience For anyone who need to have passwords easily. Comparison Most passwords manager generate completely passwords with completely random characters that aren't very easy to memorize or tape. Examples include Dashlane , Norton , Avast . Or other like Bitwarden generate passwords that are not really fast-to-type. The mine generate sth like 7Xy-Bonuwucete 0Qubyby+Pomafy , or 7Zuxogu:Lebuwo . Usage You can install it with pip install pwd-generator and use the cli version: pwdgen To use it in a python code import pwdgen print(pwdgen.generate()) Changes from last post I already made another post for this, but this was not well received because my code use the random module. I updated it to use the secrets module."},
{"Title": "I analyzed & visualized 7 years of music royalties with polars & Vega-Altair. Code in article", "Author": "u/fjogurpiano", "Content": "Hi all, I used polars (a Rust-powered ‚ÄúBlazingly Fast DataFrame Library‚Äù) to analyze seven years of my own music royalties data. Vega-Altair (Python wrapper for Vega-Lite) powers the (often interactive) visualizations. Link to the article: https://osc.garden/blog/data-analysis-music-streaming/ It was a lot of fun learning polars and setting up the graphs with Vega-Altair. Would love to hear any comments and suggestions. Link to r/dataisbeautiful post: https://www.reddit.com/r/dataisbeautiful/comments/1ca7x6z/oc_my_music_needs_more_than_200k_plays_on/"},
{"Title": "find_where - my first Python package published to PyPi", "Author": "u/dan_ohn", "Content": "PyPi Source What My Project Does find_where is a Python package that provides a function to find values in dictionaries where a specified key matches a given value, similar to filtering in SQL. Target Audience This is my first attempt at creating a Python package so I would describe this as a toy project at this stage but am definitely looking for feedback from the wider community. Comparison I mainly wrote this package because I kept on writing the same iterable based code when trying to find a value, given a key: data = { \"people\": [ {\"first_name\": \"John\", \"last_name\": \"Smith\", \"age\": 25}, {\"first_name\": \"Alice\", \"last_name\": \"Jones\", \"age\": 32}, ] } first_name = None for result in data[\"people\"]: if result[\"age\"] == 32: first_name = result[\"first_name\"] break print(first_name) When using find_where, you can simply run: from find_where import find_where data = { \"people\": [ {\"first_name\": \"John\", \"last_name\": \"Smith\", \"age\": 25}, {\"first_name\": \"Alice\", \"last_name\": \"Jones\", \"age\": 32}, ] } first_name = find_where(data[\"people\"], \"first_name\", age=32) print(first_name) Appreciate any feedback, constructive or otherwise!"},
{"Title": "I now know again why I stopped using mamba / conda for setting up virtual environments", "Author": "u/RareRandomRedditor", "Content": "I have started at a new job and had the idea that it would probably be clever to set up my developing environment in exactly the same way as my predecessor did. Because: This should help resolving errors quicker in the transition period His code was good and clean and it appears that he knows what he is doing we were using mostly the same tools (VScode etc.) anyways. He set up his virtual environments (VE)s with conda/mamba. I vaguely remembered that I also used to do that but then stopped for some reason and switched to the virtualenv package. But I did not remember why anymore. So I just set up my VEs in the same way, it should not really make any difference anyways (so I thought). Well, fast forward about two weeks and now I have VEs that occasionally (but not always) exist twice in the same folders under the same name (according to mamba info --envs) and that are at the same time completely empty (according to mamba list) and contain all packages I have installed anywhere, ever (according to pip list). I usually install packages via pip and I assume this may have fucked things up in combination with mamba? I'll probably switch back to virtualenv again and add a \"do not use conda/mamba VEs !!!\" in my notes. I am working on Windows. Is mamba better on Linux?"},
{"Title": "Pandas Python Introduction", "Author": "u/cyprusgreekstudent", "Content": "I'm sharing some of the Python tutorials I made.  I teach Ukrainian teenagers and university students Python programming and data science for free. Here is an introduction to Pandas . It shows: How to read data into a Pandas dataframe from a CSV file. Show how to print out the column names. See and set the index. Print sections of the dataframe. Do basic statistics. Create a dataframe from a dictionary."},
{"Title": "Inline templating engine", "Author": "u/TheRealMrMatt", "Content": "I was wondering if anyone has come across anything like https://github.com/a-h/templ or https://hono.dev/guides/jsx , but for python. For context, I am familiar with jinja2, mako, etc. but find them to be unintuitive due to the loose coupling of logic (ex: database calls) and templating (ex: generating a list from the database results). Therefore, I am looking for a \"inline\" templating solution."},
{"Title": "I made Cria - Run LLMs (AI) locally and programmatically with as little friction as possible", "Author": "u/reformedbillclinton", "Content": "https://github.com/leftmove/cria My name is Anonyo, and I am a seventeen year old from Southeast Michigan. This is my second open source project. I built Cria, a Python library that allows you to run LLMs programmatically through Python. Cria is designed so there is as little friction as possible ‚Äî getting started takes just five lines of code. I created this library because I was using OpenAI in my project, and kept running into rate limits. With local LLMs getting better and better, I sought to switch, but found command line configurations to be limited. Running and configuring LLMs can be trivial, but programs like ollama make it easier. The only problem I found with ollama though, was the lack of features in its Python client. To counteract this, I built Cria ‚Äî a program that is concise, efficient, and most importantly programmable. The name is based off of Meta's llama, as baby llamas are called crias. Cria solves pain points of ollama - it manages the ollama server for you, saves your message history, streams responses by default, and allows easier management of multiple LLMs at once. The target audience for this project is anyone that uses AI. This library was built in a weekend, and is meant to make running LLMs easier for anyone that wants to go local. I built this for myself, but it could be used at scale as ollama runs its own server (there would of course have to be extra configuration though). In Python, while there are libraries to run LLMs, they are feature limited. Ollama provides the most extensive toolset, and so Cria uses that. Ollama has its own Python library, but I found that to be cumbersome when running multiple LLMs. There are similar libraries, but existing alternatives involve strictly the command line (from what I've seen). While this isn't my first open source project, it's my first Python library. The code may be a little rough around the edges, so I would appreciate any and all suggestions. Thank you!"},
{"Title": "[tutorial] Data imputation on real-time data source", "Author": "u/oli_k", "Content": "Hi there, My team at Bytewax and I have been working on a series of hands-on guides on streaming data and I am excited to share how one can handle missing values in real-time in Python. While some parts of the guide are simplified, for example, we use a random number generator as an input source, the algorithmic part is production-ready. We are taking advantage of the Numpy library + stateful operators in Bytewax. Other input sources are available, too. https://bytewax.io/guides/handling-missing-values"},
{"Title": "Molly - a data quality monitoring library specifically designed for time series data", "Author": "u/flaviaouyang", "Content": "What My Project Does Molly is designed to monitor time series data in a SQL database such as MySQL or PostgreSQL. Currently supported data quality feature: - staleness: when the data was last updated - completeness: whether any data is missing in a given date range Currently support messaging service: - Slack Web API Currently support scheduling service: - Apache Airflow - crontab Target Audience people in need of a plug-n-play dq monitoring service Comparison It's open source. It's free. It's written in pure python. And it's made for modern data stack so you can try it out with minimum effort. I'd appreciate it if you could check it out on GitHub. All feedback and/or contributions are welcomed! GitHub: https://github.com/flaviaouyang/molly"},
{"Title": "What is currently the fastest/state-of-the-art ODE solver in Python?", "Author": "u/D_vd_P", "Content": "For my application, Scipy's solvers are not fast enough so I am looking to speed up by using another package. These are some packages I have found so far: NumbaLSODA DifferentialEquations.jl Torchquad (Although doesn't solve ODEs, just integrates quickly, but that could be wrapped I guess) Torchdiffeq PyDSTool I will be experimenting with these packages, but I was wondering whether anyone has experience with them and has done work on them before. Also, I am wondering whether there are any packages I should check out?"},
{"Title": "Announcing The Python Logging Book & Course Kickstarter", "Author": "u/driscollis", "Content": "New developers print out strings to their terminal. It‚Äôs how we learn! But printing out to the terminal isn‚Äôt what you do with most professional applications. In those cases, you log into files. Sometimes, you log into multiple locations at once. These logs may serve as an audit trail for compliance purposes or help the engineers debug what went wrong. Python Logging teaches you how to log in the Python programming language. Python is one of the most popular programming languages in the world. Python comes with a logging module that makes logging easy. What You‚Äôll Learn In this book, you will learn how about the following: Logger objects Log levels Log handlers Formatting your logs Log configuration Logging decorators Rotating logs Logging and concurrency and more! Book formats The finished book will be made available in the following formats: paperback (at the appropriate reward level) PDF epub The paperback is a 6‚Ä≥ x 9‚Ä≥ book and is approximately 150 pages long. You can check out the book's campaign on Kickstarter . Let me know if you have any questions!"},
{"Title": "Most-watched PyData conference & meetup talks from 2023", "Author": "u/ad81923", "Content": "Just came across this list and I think it's amazing: https://techtalksweekly.substack.com/p/all-pydata-2023-talks"},
{"Title": "Is Pythonista (Python for iOS/iPadOS) no longer with active development and support?", "Author": "u/br_web", "Content": "I have not seen updates in almost a year and there is no activity in the support forum? Is there a replacement forum? Thanks"},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Jobs that utilize Jupyter Notebook?", "Author": "u/Shadowforce426", "Content": "I have been programming for a few years now and have on and off had jobs in the industry. I used Jupyter Notebook in undergrad for a course almost a decade ago and I found it really cool. Back then I really didn‚Äôt know what I was doing and now I do. I think it‚Äôs cool how it makes it feel more like a TI calculator (I studied math originally) What are jobs that utilize this? What can I do or practice to put myself in a better position to land one?"},
{"Title": "How to Clean Data and work with outliers and Missing Data in Python Pandas", "Author": "u/cyprusgreekstudent", "Content": "I made a detailed tutorial on how to work with missing data, outliers, and how to lean up data in Pandas Python.  This explains how to clean up your data so that your statistical results and trainin data is not skewed. https://github.com/werowe/HypatiaAcademy/blob/master/pandas/pandas_missing_data.ipynb"},
{"Title": "Tkinter Variable class C/Python garbage collection mismatch", "Author": "u/ranemaeker", "Content": "Tkinter is going to drive me crazy. Let me lay this out for you: Tkinter is the GUI interface I am using, which is a python frontend for a C library It holds \"widgets\" in a window. Some widgets have variables (StringVar, BooleanVar, etc - not \"variables\" like code variables but a class of updatable data holders for the widgets to link to the Tcl backend) The Vars have incremental names like PYVAR1, PYVAR93, etc by default. Names can be provided to the Vars via keywords upon initialization.  (Note I have not been able to find any reason to not do this - hence this post) Names should absolutely not be provided to the Vars unless they are unique.  Because on python garbage collection (or \"del Var\" command) the Vars are told to destroy() themselves and then this command is passed to C \"destroy this Var.\" But the garbage collection in C (or rather Tcl) is slightly delayed from Python... So if I immediately I instantiate a different Var with a different value (or same value, who knows) but reusing the same name, the Var has been deleted in Python (yey!) but it has not been deleted in C yet (uh oh).  C says to Python oh hey I got this Var already, use it, don't make a new one. So python goes to use it and then OOPS NEVERMIND LOL IT'S DELETED IN C HAHAHA So now I have this Var that I can't access in Python because it's a Schrodinger's Variable: it both exists and does not exist.  Wonky behavior too, like check boxes that are in no state at all, neither checked nor unchecked, and I can't access the variable to see what state it is in because the variable that existed before the one I'm using no longer actually exists.  So the Var I can access looks 100% fine!  But if I put a value checker in just the right place I can catch where the wonky behavior exists. This has been a frustrating several weeks. Anyway.  I have not been able to find any information about this and thought I'd share.  Hopefully this helps someone avoid the mistake I made.  I have since set all Var names to be the incremental default names. Edit: after quite a bit of experimentation I have discovered the issue is not in the Variable naming but in the fact that I had the Variables stored as attributes in my custom widget classes.  The attributes were not being cleaned up properly on widget destroy().  The simple solution is to pop the variable attribute from the widget dict before child.destroy()."},
{"Title": "I created an AI tool for using python to process excel and other files", "Author": "u/smallSohoSolo", "Content": "Hi, I'm Owen, and I've developed an AI tool for data processing. I spend a significant amount of time processing daily data from numerous Excel files, which requires me to write a lot of Python code. I have been using GitHub Copilot to assist with this, but I am exploring ways to eliminate the need to write code manually. The tool is called Tipis AI, and you can find it here: https://github.com/tipisai/tipis-fe What My Project Does Therefore, I have developed an AI that generates Python code for data processing. I've also created a Python runtime environment to execute this code. You can use this tool to process any type of files. Target Audience Any worker. Someone needs to process files. Comparison You can perform complex calculations and summarizations on files using natural language descriptions, without needing to master complex Python knowledge If you have any suggestions for AI in data processing, please let me know, and I might include them in this product.I am very interested in code generation."},
{"Title": "Ray on Golem MVP Launch: the Best Way of Running Python Code on the Golem Network", "Author": "u/GolemSM", "Content": "Golem Network has just released Ray on Golem MVP, a significant milestone in integrating Ray with Golem's decentralized infrastructure! üéâ Check out all the details about the release and next steps in our blog post: üìù https://blog.golem.network/announcing-ray-on-golem-mvp-launch-the-recommended-way-of-running-python-code-on-the-golem-network/ üëâ Golem is looking for engaged beta testers to run diverse applications and collaborate on refining the solution. Interested? Find all the information on the brand-new Ray on Golem site: üîó https://www.golem.network/ray"},
{"Title": "Should I use pydantic for all my classes?", "Author": "u/uh_sorry_i_dont_know", "Content": "Pydantic makes your code safer by making it strongly typed. You can no longer input a wrongly typed argument without getting an error (if pydantic can't convert it). This is great but to me it seems that sometimes standard python classes still seem preferable. Perhaps it's because I'm not using it correctly but my code for a pydantic class is much longer then for a normal class. Especially if you are working with computed attributes. Then you have to start using special decorators and for every computed attribute you have to declare a function with \"def ...\" Instead of in an init function just being able to write attribute_3 = attribute 1 + attribute 2. So I'm just wondering are you using pydantic for all your classes? And how do you handle computed fields in pydantic especially upon instantiation I find it hard to implement."},
{"Title": "Zillow scraper made pure in Python", "Author": "u/JohnBalvin", "Content": "Hello everyone., on today new scraper I created the python version for the zillow scraper. https://github.com/johnbalvin/pyzill What My Project Does The library will get zillow listings and details. I didn't created a defined structured like on the Go version just because it's not as easy to maintain this kind of projects on python like on Go. It is made on pure python with HTTP requests, so no selenium, puppeteer, playwright etc. or none of those automation libraries that I hate. Target Audience This project target could be real state agents probably, so lets say you want to track the real price history of properties around an area, you can use it track it Comparison There are libraries similar outhere but they look outdated, most of the time, scraping projects need to ne on constant maintance due to changed on the page or api pip install pyzill Let me know what ou think, thanks about me: I'm full stack developer specialized on web scraping and backend, with 6-7 years of experience"},
{"Title": "Created Netlify DNS Manager: CLI Tool for uploading zone Files to Netlify or exporting Netlify DNS", "Author": "u/snorkell_", "Content": "What My Project Does? Command-line tool to simplify the management of DNS records for domains hosted on Netlify, whether you want to migrate to Netlify or migrate away from Netlify. Import Zone File from Godaddy, NameCheap or others to Netlify - helpful in migrating nameservers. Export DNS records from Netlify as zonefile to be import it to Godaddy, Namecheap or other servers. Zonefile is a list of all the DNS records for a given domain Here is the url - https://github.com/sumansaurabh/netlify-dns-manager Target Audience (e.g., Is it meant for production, just a toy project, etc.) Anyone who is intended to use Netlify DNS. Comparison (A brief comparison explaining how it differs from existing alternatives.) There is no such tool that can help in managing DNS in Netlify - hence I have created it."},
{"Title": "Py2wasm: A Python to Wasm compiler 3x faster than pyiodide", "Author": "u/desmoulinmichel", "Content": "Take the excellent nuitka , compile python code to C, turn it into web assembly, and you got Python in the browser, without the usual runtime overhead: https://wasmer.io/posts/py2wasm-a-python-to-wasm-compiler While the doc states you can get this effect by doing: pip install py2wasm py2wasm myprogram.py -o myprogram.wasm wasmer run myprogram.wasm You still need the wasmer WASM runtime ( curl https://get.wasmer.io -sSfL | sh for Unix users, iwr https://win.wasmer.io -useb | iex for Windows user with Powershell), however. But more than that, since you need nuikta, it means you need a C compiler installed. While in Ubuntu it's a just an sudo apt install build-essential , it will require a bit more work on Windows and Mac."},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Runtime type checking performance", "Author": "u/gerardwx", "Content": "I'm trying to decide whether to use typeguard or stick to assert isinstance in the places where I care. Has anyone done benchmarking testing of the overhead of using type guards \"at\"typeguard decorator ?"},
{"Title": "pwdgen v2 -a simple password generator", "Author": "u/RRTheGuy", "Content": "https://github.com/RadoTheProgrammer/pwdgen What my project does My project generate simple, strong, memorable and easy-to-type passwords. The system is simple: it generate 2 pronounceable words separated by a special char, with a number at start or end. I tried creating a password generator that combines simplicity, security, memorability, and ease of type. This should be secure enough because it can generate 4e15 possibilities of passwords and uses the secrets module. Target audience For anyone who need to have passwords easily. Comparison Most passwords manager generate completely passwords with completely random characters that aren't very easy to memorize or tape. Examples include Dashlane , Norton , Avast . Or other like Bitwarden generate passwords that are not really fast-to-type. The mine generate sth like 7Xy-Bonuwucete 0Qubyby+Pomafy , or 7Zuxogu:Lebuwo . Usage You can install it with pip install pwd-generator and use the cli version: pwdgen To use it in a python code import pwdgen print(pwdgen.generate()) Changes from last post I already made another post for this, but this was not well received because my code use the random module. I updated it to use the secrets module."},
{"Title": "I analyzed & visualized 7 years of music royalties with polars & Vega-Altair. Code in article", "Author": "u/fjogurpiano", "Content": "Hi all, I used polars (a Rust-powered ‚ÄúBlazingly Fast DataFrame Library‚Äù) to analyze seven years of my own music royalties data. Vega-Altair (Python wrapper for Vega-Lite) powers the (often interactive) visualizations. Link to the article: https://osc.garden/blog/data-analysis-music-streaming/ It was a lot of fun learning polars and setting up the graphs with Vega-Altair. Would love to hear any comments and suggestions. Link to r/dataisbeautiful post: https://www.reddit.com/r/dataisbeautiful/comments/1ca7x6z/oc_my_music_needs_more_than_200k_plays_on/"},
{"Title": "find_where - my first Python package published to PyPi", "Author": "u/dan_ohn", "Content": "PyPi Source What My Project Does find_where is a Python package that provides a function to find values in dictionaries where a specified key matches a given value, similar to filtering in SQL. Target Audience This is my first attempt at creating a Python package so I would describe this as a toy project at this stage but am definitely looking for feedback from the wider community. Comparison I mainly wrote this package because I kept on writing the same iterable based code when trying to find a value, given a key: data = { \"people\": [ {\"first_name\": \"John\", \"last_name\": \"Smith\", \"age\": 25}, {\"first_name\": \"Alice\", \"last_name\": \"Jones\", \"age\": 32}, ] } first_name = None for result in data[\"people\"]: if result[\"age\"] == 32: first_name = result[\"first_name\"] break print(first_name) When using find_where, you can simply run: from find_where import find_where data = { \"people\": [ {\"first_name\": \"John\", \"last_name\": \"Smith\", \"age\": 25}, {\"first_name\": \"Alice\", \"last_name\": \"Jones\", \"age\": 32}, ] } first_name = find_where(data[\"people\"], \"first_name\", age=32) print(first_name) Appreciate any feedback, constructive or otherwise!"},
{"Title": "I now know again why I stopped using mamba / conda for setting up virtual environments", "Author": "u/RareRandomRedditor", "Content": "I have started at a new job and had the idea that it would probably be clever to set up my developing environment in exactly the same way as my predecessor did. Because: This should help resolving errors quicker in the transition period His code was good and clean and it appears that he knows what he is doing we were using mostly the same tools (VScode etc.) anyways. He set up his virtual environments (VE)s with conda/mamba. I vaguely remembered that I also used to do that but then stopped for some reason and switched to the virtualenv package. But I did not remember why anymore. So I just set up my VEs in the same way, it should not really make any difference anyways (so I thought). Well, fast forward about two weeks and now I have VEs that occasionally (but not always) exist twice in the same folders under the same name (according to mamba info --envs) and that are at the same time completely empty (according to mamba list) and contain all packages I have installed anywhere, ever (according to pip list). I usually install packages via pip and I assume this may have fucked things up in combination with mamba? I'll probably switch back to virtualenv again and add a \"do not use conda/mamba VEs !!!\" in my notes. I am working on Windows. Is mamba better on Linux?"},
{"Title": "Pandas Python Introduction", "Author": "u/cyprusgreekstudent", "Content": "I'm sharing some of the Python tutorials I made.  I teach Ukrainian teenagers and university students Python programming and data science for free. Here is an introduction to Pandas . It shows: How to read data into a Pandas dataframe from a CSV file. Show how to print out the column names. See and set the index. Print sections of the dataframe. Do basic statistics. Create a dataframe from a dictionary."},
{"Title": "Inline templating engine", "Author": "u/TheRealMrMatt", "Content": "I was wondering if anyone has come across anything like https://github.com/a-h/templ or https://hono.dev/guides/jsx , but for python. For context, I am familiar with jinja2, mako, etc. but find them to be unintuitive due to the loose coupling of logic (ex: database calls) and templating (ex: generating a list from the database results). Therefore, I am looking for a \"inline\" templating solution."},
{"Title": "I made Cria - Run LLMs (AI) locally and programmatically with as little friction as possible", "Author": "u/reformedbillclinton", "Content": "https://github.com/leftmove/cria My name is Anonyo, and I am a seventeen year old from Southeast Michigan. This is my second open source project. I built Cria, a Python library that allows you to run LLMs programmatically through Python. Cria is designed so there is as little friction as possible ‚Äî getting started takes just five lines of code. I created this library because I was using OpenAI in my project, and kept running into rate limits. With local LLMs getting better and better, I sought to switch, but found command line configurations to be limited. Running and configuring LLMs can be trivial, but programs like ollama make it easier. The only problem I found with ollama though, was the lack of features in its Python client. To counteract this, I built Cria ‚Äî a program that is concise, efficient, and most importantly programmable. The name is based off of Meta's llama, as baby llamas are called crias. Cria solves pain points of ollama - it manages the ollama server for you, saves your message history, streams responses by default, and allows easier management of multiple LLMs at once. The target audience for this project is anyone that uses AI. This library was built in a weekend, and is meant to make running LLMs easier for anyone that wants to go local. I built this for myself, but it could be used at scale as ollama runs its own server (there would of course have to be extra configuration though). In Python, while there are libraries to run LLMs, they are feature limited. Ollama provides the most extensive toolset, and so Cria uses that. Ollama has its own Python library, but I found that to be cumbersome when running multiple LLMs. There are similar libraries, but existing alternatives involve strictly the command line (from what I've seen). While this isn't my first open source project, it's my first Python library. The code may be a little rough around the edges, so I would appreciate any and all suggestions. Thank you!"},
{"Title": "[tutorial] Data imputation on real-time data source", "Author": "u/oli_k", "Content": "Hi there, My team at Bytewax and I have been working on a series of hands-on guides on streaming data and I am excited to share how one can handle missing values in real-time in Python. While some parts of the guide are simplified, for example, we use a random number generator as an input source, the algorithmic part is production-ready. We are taking advantage of the Numpy library + stateful operators in Bytewax. Other input sources are available, too. https://bytewax.io/guides/handling-missing-values"},
{"Title": "Molly - a data quality monitoring library specifically designed for time series data", "Author": "u/flaviaouyang", "Content": "What My Project Does Molly is designed to monitor time series data in a SQL database such as MySQL or PostgreSQL. Currently supported data quality feature: - staleness: when the data was last updated - completeness: whether any data is missing in a given date range Currently support messaging service: - Slack Web API Currently support scheduling service: - Apache Airflow - crontab Target Audience people in need of a plug-n-play dq monitoring service Comparison It's open source. It's free. It's written in pure python. And it's made for modern data stack so you can try it out with minimum effort. I'd appreciate it if you could check it out on GitHub. All feedback and/or contributions are welcomed! GitHub: https://github.com/flaviaouyang/molly"},
{"Title": "What is currently the fastest/state-of-the-art ODE solver in Python?", "Author": "u/D_vd_P", "Content": "For my application, Scipy's solvers are not fast enough so I am looking to speed up by using another package. These are some packages I have found so far: NumbaLSODA DifferentialEquations.jl Torchquad (Although doesn't solve ODEs, just integrates quickly, but that could be wrapped I guess) Torchdiffeq PyDSTool I will be experimenting with these packages, but I was wondering whether anyone has experience with them and has done work on them before. Also, I am wondering whether there are any packages I should check out?"},
{"Title": "Announcing The Python Logging Book & Course Kickstarter", "Author": "u/driscollis", "Content": "New developers print out strings to their terminal. It‚Äôs how we learn! But printing out to the terminal isn‚Äôt what you do with most professional applications. In those cases, you log into files. Sometimes, you log into multiple locations at once. These logs may serve as an audit trail for compliance purposes or help the engineers debug what went wrong. Python Logging teaches you how to log in the Python programming language. Python is one of the most popular programming languages in the world. Python comes with a logging module that makes logging easy. What You‚Äôll Learn In this book, you will learn how about the following: Logger objects Log levels Log handlers Formatting your logs Log configuration Logging decorators Rotating logs Logging and concurrency and more! Book formats The finished book will be made available in the following formats: paperback (at the appropriate reward level) PDF epub The paperback is a 6‚Ä≥ x 9‚Ä≥ book and is approximately 150 pages long. You can check out the book's campaign on Kickstarter . Let me know if you have any questions!"},
{"Title": "Most-watched PyData conference & meetup talks from 2023", "Author": "u/ad81923", "Content": "Just came across this list and I think it's amazing: https://techtalksweekly.substack.com/p/all-pydata-2023-talks"},
{"Title": "Is Pythonista (Python for iOS/iPadOS) no longer with active development and support?", "Author": "u/br_web", "Content": "I have not seen updates in almost a year and there is no activity in the support forum? Is there a replacement forum? Thanks"},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Jobs that utilize Jupyter Notebook?", "Author": "u/Shadowforce426", "Content": "I have been programming for a few years now and have on and off had jobs in the industry. I used Jupyter Notebook in undergrad for a course almost a decade ago and I found it really cool. Back then I really didn‚Äôt know what I was doing and now I do. I think it‚Äôs cool how it makes it feel more like a TI calculator (I studied math originally) What are jobs that utilize this? What can I do or practice to put myself in a better position to land one?"},
{"Title": "How to Clean Data and work with outliers and Missing Data in Python Pandas", "Author": "u/cyprusgreekstudent", "Content": "I made a detailed tutorial on how to work with missing data, outliers, and how to lean up data in Pandas Python.  This explains how to clean up your data so that your statistical results and trainin data is not skewed. https://github.com/werowe/HypatiaAcademy/blob/master/pandas/pandas_missing_data.ipynb"},
{"Title": "Tkinter Variable class C/Python garbage collection mismatch", "Author": "u/ranemaeker", "Content": "Tkinter is going to drive me crazy. Let me lay this out for you: Tkinter is the GUI interface I am using, which is a python frontend for a C library It holds \"widgets\" in a window. Some widgets have variables (StringVar, BooleanVar, etc - not \"variables\" like code variables but a class of updatable data holders for the widgets to link to the Tcl backend) The Vars have incremental names like PYVAR1, PYVAR93, etc by default. Names can be provided to the Vars via keywords upon initialization.  (Note I have not been able to find any reason to not do this - hence this post) Names should absolutely not be provided to the Vars unless they are unique.  Because on python garbage collection (or \"del Var\" command) the Vars are told to destroy() themselves and then this command is passed to C \"destroy this Var.\" But the garbage collection in C (or rather Tcl) is slightly delayed from Python... So if I immediately I instantiate a different Var with a different value (or same value, who knows) but reusing the same name, the Var has been deleted in Python (yey!) but it has not been deleted in C yet (uh oh).  C says to Python oh hey I got this Var already, use it, don't make a new one. So python goes to use it and then OOPS NEVERMIND LOL IT'S DELETED IN C HAHAHA So now I have this Var that I can't access in Python because it's a Schrodinger's Variable: it both exists and does not exist.  Wonky behavior too, like check boxes that are in no state at all, neither checked nor unchecked, and I can't access the variable to see what state it is in because the variable that existed before the one I'm using no longer actually exists.  So the Var I can access looks 100% fine!  But if I put a value checker in just the right place I can catch where the wonky behavior exists. This has been a frustrating several weeks. Anyway.  I have not been able to find any information about this and thought I'd share.  Hopefully this helps someone avoid the mistake I made.  I have since set all Var names to be the incremental default names. Edit: after quite a bit of experimentation I have discovered the issue is not in the Variable naming but in the fact that I had the Variables stored as attributes in my custom widget classes.  The attributes were not being cleaned up properly on widget destroy().  The simple solution is to pop the variable attribute from the widget dict before child.destroy()."},
{"Title": "I created an AI tool for using python to process excel and other files", "Author": "u/smallSohoSolo", "Content": "Hi, I'm Owen, and I've developed an AI tool for data processing. I spend a significant amount of time processing daily data from numerous Excel files, which requires me to write a lot of Python code. I have been using GitHub Copilot to assist with this, but I am exploring ways to eliminate the need to write code manually. The tool is called Tipis AI, and you can find it here: https://github.com/tipisai/tipis-fe What My Project Does Therefore, I have developed an AI that generates Python code for data processing. I've also created a Python runtime environment to execute this code. You can use this tool to process any type of files. Target Audience Any worker. Someone needs to process files. Comparison You can perform complex calculations and summarizations on files using natural language descriptions, without needing to master complex Python knowledge If you have any suggestions for AI in data processing, please let me know, and I might include them in this product.I am very interested in code generation."},
{"Title": "Ray on Golem MVP Launch: the Best Way of Running Python Code on the Golem Network", "Author": "u/GolemSM", "Content": "Golem Network has just released Ray on Golem MVP, a significant milestone in integrating Ray with Golem's decentralized infrastructure! üéâ Check out all the details about the release and next steps in our blog post: üìù https://blog.golem.network/announcing-ray-on-golem-mvp-launch-the-recommended-way-of-running-python-code-on-the-golem-network/ üëâ Golem is looking for engaged beta testers to run diverse applications and collaborate on refining the solution. Interested? Find all the information on the brand-new Ray on Golem site: üîó https://www.golem.network/ray"},
{"Title": "Should I use pydantic for all my classes?", "Author": "u/uh_sorry_i_dont_know", "Content": "Pydantic makes your code safer by making it strongly typed. You can no longer input a wrongly typed argument without getting an error (if pydantic can't convert it). This is great but to me it seems that sometimes standard python classes still seem preferable. Perhaps it's because I'm not using it correctly but my code for a pydantic class is much longer then for a normal class. Especially if you are working with computed attributes. Then you have to start using special decorators and for every computed attribute you have to declare a function with \"def ...\" Instead of in an init function just being able to write attribute_3 = attribute 1 + attribute 2. So I'm just wondering are you using pydantic for all your classes? And how do you handle computed fields in pydantic especially upon instantiation I find it hard to implement."},
{"Title": "Intermediate Project: Python Plotly Data Visuals: Dropdowns and Range Sliders For User Interaction", "Author": "u/jgloewen", "Content": "Interactive data visualization is a powerful tool that can significantly enhance the analysis and interpretation of complex datasets. With Python, the Plotly library offers various features that can be used to create interactive publication-quality graphs. This project demonstrates how to use 2 of these awesome features: Range Slider Dropdown Menu A good dataset is essential to demonstrate this style of interactivity. For this project, we will use the UN population projection data to sort and visualize by country for various age groups. FREE ARTICLE: https://johnloewen.substack.com/p/python-plotly-combining-dropdowns-and-range-sliders-for-user-interaction-658dc6fd9c71"},
{"Title": "My latest TILs about Python", "Author": "u/JCx64", "Content": "After 10+ years working with it, I keep discovering new features. This is a list of the most recent ones: https://jcarlosroldan.com/post/329"},
{"Title": "Single file, hot reloading python server with raw websockets and inotify", "Author": "u/tootac", "Content": "A project to build a local server without any external dependencies with main features of hot reloading browser on source modification. The idea is to make it simple, standalone and work without any setup. The project uses regular sockets, websockets, inotify and a bit of javascript on the fly embedding to allow users to achieve automatic synchronization with browser. The article describes hot to monitor file changes with direct loading of libc and requesting kernel notify on change. Free article and code: https://hereket.com/posts/linux_live_reload_python_server/"},
{"Title": "I made a team of AI manage my YouTube channel and my work with Python based framework CrewAI", "Author": "u/fx2mx3", "Content": "I found this awesome AI framework for python called \"Crew AI\", that allows us to create assistants, or agents in the CrewAI lingo, and assign specific tasks. I started my tiny youtube channel 3 months and for a youtuber, especially a novice one like myself, there are a bunch of tasks such as checking what other similar channels are doing, which topics are trending and if a video idea is good or not. There is also the part of coming up with the video content idea itself, create a catchy title, suitable youtube tags and finding the appropriate forum to talk about! You folks get the picture. It's a great project for anyone seeking to automate tasks not just related to a youtube channel, but this could easily be adapted to your daily job tasks, or even a startup idea. I have written a full article about how the project unfolds and also a youtube video for how I did it step by step, which you are welcome to check it out! The links are as follows: The medium article: https://medium.digitalmirror.uk/create-an-ai-team-to-manage-your-youtube-channel-5dc1e6c9b31b The YouTube video: https://youtu.be/5JoVeYcxgpU And of course the source code: https://github.com/fmiguelmmartins/crewaiyoutube.git As always, if you would like to drop some feedback so I can improve with time I would be grateful! Cheers"},
{"Title": "Anycrc - Python CRC computation library", "Author": "u/marzoogy", "Content": "Link: https://github.com/marzooqy/anycrc What My Project Does This is a Cython module with bindings to the crcany library. It supports calculating CRC hashes of arbitary sizes as well as updating a crc hash over time, and I believe that it is the fastest generic CRC library available for Python. Target Audience Anyone that needs to compute CRCs. Comparison with Alternatives crcmod-plus: A modernized version of the old crcmod library. It's highly customizable and fast. fastcrc: A library with bindings to the crc-rs library. Has a limited selection of CRCs to use. binascii: The standard library module comes with support for only CRC32 and CRC-CCITT, and at least the CRC32 implementation used is pretty fast according to my testing. CrcEngine: If you are looking for a pure Python implementation of CRC, then this is a good option. It is however one to two orders of magnitude slower than the libraries that use C/Rust bindings. crc: Popular but extremely inefficient CRC library. It does several things that severely slow it down, such as using an entire class to represent a byte. It's three orders of magnitude slower than the faster options."},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Returns Python library for FP", "Author": "u/yinshangyi", "Content": "Hello! I work in big data space (data engineering), I mainly used Java, Scala and Python. I have been learning functional programming in greater depth and I found this Python library which seems pretty cool. https://github.com/dry-python/returns I've used it at work for implementing an Either based error handling. It seems a great library. Any of you have used it? Any thoughts? For sure, I prefer doing FP in Scala but given the job market isn't too kind too Scala and FP languages in general. What are your thoughts to bring FP (at least parts of it) to the Python world? Some people in the TypeScript world seem to take that direction: https://github.com/Effect-TS/effect"},
{"Title": "Introducing Stockdex: A Superior Python Package for Real-Time Financial Data Retrieval", "Author": "u/nginx26", "Content": "Greetings! I've previously introduced my Python package in this post. Since then, I've significantly enhanced its performance and expanded its capabilities. What My Project Does A lightweight python package designed for the efficient retrieval of financial data from various sources like Yahoo Finance, Nasdaq, Digrin and JustETF websites (somewhat similar to yfinance). This tool provides functionality akin to yfinance but with broader data access. Main benefits of stockdex over yfinance Fresh data : Yahoo Finance often delays updates to financial data by several days while Nasdaq and other sources typically update on the day reports are released. Stockdex enables access to this fresher data, such as quarterly earnings. Broader Data Sources : Unlike yfinance which relies solely on the Yahoo Finance API, Stockdex aggregates data from multiple platforms including Digrin, JustETF, Nasdaq, and Yahoo Finance. For specific examples of data retrieval, refer to this readme . Access to Historical Data : Yahoo Finance limits access to the most recent five annual or four quarterly reports. Stockdex , however, taps into sources that maintain extensive historical archives not available through Yahoo Finance. Target Audience The package is targeted at people who are interested in financial analysis using python. Explore more: Github Repo Link Pypi link"},
{"Title": "Python icon library with 5237 high-quality icons (Tabler Icons wrapper)", "Author": "u/niklashnng", "Content": "What My Project Does: pytablericons is a Python wrapper for the tabler-icons project, which provides a set of 5237 free MIT-licensed high-quality SVG icons for web projects with each icon being designed on a 24x24 grid and a 2px stroke. It allows you to load any of those SVG icons directly into a Pillow PNG Image in any size and color without losing quality and without having to download the icons manually or handling the SVG conversion yourself. For the outline icons, the stroke width is customizable as well. All of this is done in just a single line and since the icon is a Pillow Image, it can then be used easily with basically anything (e.g. PyQt5, PyQt6, PySide2, PySide6, Tkinter, etc.) Target Audience: This is useful for any Python developer who is working on UI projects and wants to easily display modern icons in any size and color without losing quality and without having to download each icon manually. Comparison : / Links: Preview: https://github.com/niklashenning/pytablericons/assets/58544929/e13fb020-4d5f-4e28-bd5f-0d5659bd6582 GitHub: https://github.com/niklashenning/pytablericons"},
{"Title": "A word search game generator", "Author": "u/RRTheGuy", "Content": "https://github.com/RadoTheProgrammer/motsmeles/ What my project does It generate word search games with custom words, dimensions. Target audience For people who want to get fun with some word searching Comparison This is a python library, it's open source, and you can use it in a python code, or with the command-line. Also, you have freedom if you want to have diagonal or reversed words."},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "What's the best library to find differences in two text files and visulaize it?", "Author": "u/TheGupta", "Content": "difflib is popular. Most of the answers on Stack Overflow uses it. But, I think Google's diff-match-patch is better. See an example here . Or, am I missing something?"},
{"Title": "Alternative data managment tool to replace the need to use Jupyter Notebook for Python", "Author": "u/IllogicalLunarBear", "Content": "What My Project Does: For everyone who says they use jupyter Notebook for Python to do chunks of code work‚Ä¶ I developed my own data management system for that kind of stuff that provides a wrapper over serialization that essentially creates pythonic pointers to data on any network drive. I just grab my data that I want to grab. Provides autocomplete and C# like attributes. I can dynamically build any data storage class using UML formatted infrastructure-as-code to build the front end. The back end is completely dynamic. Here is the pypi link. Note that I developed this to handle genomic data that was generated over the course of a week of 24/7 computation, that I break into multiple logical chunks. You dont need to rely on Jupyter to hold your data and you can retrieve it months after generating it. This software routes the data to whatever means you use. It works with databases, pickles, yaml. You just have to add your interface in the data routing class. I just did yaml to start. This is just a wrapper around everything so you can declare your class like normal and at variable declaration you have data permanence instantly with no user interaction. If you pass Y=1 and power off your computer and turn it back on again Y still equals 1 when you call that variable instantly Target Audience (e.g., Is it meant for production, just a toy project, etc.) production as I use it for my genomic research on my project with Stanford Comparison (A brief comparison explaining how it differs from existing alternatives.): There is no alternatve to this project as far as i know. It replaces the need for Jupyter Notebook to hold your data as you can retrieve it months after generating it. PyPi: https://pypi.org/project/data-nut-squirrel/ Source Code: https://github.com/LunarFawn/rna_squirrel Edit: Here is an example of implementation where I actually gave a shit about documentation. https://pypi.org/project/serena-rna-tool/"},
{"Title": "Ruff 0.4.0 just dropped, with a faster parser and a new language server", "Author": "u/QueasyEntrance6269", "Content": "Release notes here , seems to be a 20-40% improvement around the board. This version features a new hand-written parser (rather than a generated one) that is much faster and offers better error messages. It also comes with a new rust-native language server inspired by rust-analyzer, that is multithreaded. I think they‚Äôre challenging Pylance‚Äôs throne, wouldn‚Äôt be surprised if the team goes after type checking next."},
{"Title": "Understanding State Machines in Python Through a Practical Example", "Author": "u/pemidi", "Content": "Hey everyone! I've written an article that simplifies the concept of state machines using Python, with a practical example related to order statuses. If you've ever been confused about state machines or just want a refresher with a real-world application, this might be just what you're looking for. Check it out and let me know what you think! Read the full article here I'm here for any questions or discussions"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Project: An Interactive Python Dashboard For Data Storytelling UN Food Security", "Author": "u/jgloewen", "Content": "Python‚Äôs Plotly Dash is a powerful tool for creating interactive data visualizations. As a Comp Sci professor, I use it extensively for interactive dashboards. Its usefulness lies in its ability to create web-based applications directly from Python code, without the need for additional web development skills. Using UN food security data, let me show you an example of how you can create an interesting and useful Python Plotly dashboard that tells a data story using: A bubble chart: with variable-sized markers, a bubble chart can represent the severity of undernourishment within a given country. Each bubble visually represents the scale of undernourishment. A horizontal bar chart: with its rigid organized structure, a horizontal bar chart provides a clear ordered list that emphasizes the 10 most undernourished countries by percentage of population. A dropdown menu: by Year, allowing the user to see the changing story over a period of time. These two data visualizations offer a dual perspective on the global picture of undernourishment: one that is geographically broad and another that is focused and comparative. Here's a step-by-step project on how to put this all together. FREE ARTICLE: https://johnloewen.substack.com/p/combining-data-visuals-an-interactive"},
{"Title": "I made a Python desktop app using PyWebview (similar to ElectronJS) & ReactJS that tracks time.", "Author": "u/zynix", "Content": "https://github.com/devdave/pyminder What does it do? The application is a very simple time tracker broken down by Client->Project->Task.  In addition it has a summary report view plus a simple CRUD like data manager.     The bigger goal was to see what it entailed to mix Python w/ReactJS and if it was easier using HTML+TypeScript over QT, TK, or some other unique API. Target Audience While I am using PyMinder to track my own work it wasn't meant for a general audience.   Instead I am sharing this in case anyone else has a need to make a desktop application and looking for alternatives. Comparison I have decades of experience with HTML & Ecma script plus I have found very little benefit is gained learning yet another way to make a user interface.   In addition there is more resources for HTML+JS tools and support. One thing to note, if the debug console is closed, the app generally consumes only 300MB.   I was expecting it to be more given the abstraction layers and behind the scenes it is an instrumented web browser instance. Future sub project 90% of the code is type annotated but that 10% is a \"here be monsters\" component that scans a Python class and makes a Typescript API-Bridge.   Transformer works but it is very much hacked together with 2 unit tests that barely cover 5% of it. That said, I would like to rip that part out, clean it up, and make it its own thing as I could see some use for a Python to Typescript adapter library."},
{"Title": "Import pdf into python GUI and measure length within PDF", "Author": "u/Most-Guarantee956", "Content": "I'm interested to create a software where I can import some construction drawings and measure the distance within the PDF as a first step of my work. Tried a few open source pdf but none of them still allows me to have a correct measurement on the PDF, anyone have an idea?"},
{"Title": "PyPDFForm now lets you create widgets without Acrobat", "Author": "u/chinapandaman", "Content": "Hello ! Earlier this year I made a post about my open source project PyPDFForm and got some really nice feedbacks from you guys. I have been since then continuously working on it and I'd love to share you two really cool features that were newly added to the library. The first one is, like what the title says, the library finally supports creating a subset of widgets through code now. One of the previous hard dependency for PyPDFForm was that it requires a PDF template that was prepared using another tool, namely Adobe Acrobat or some web based ones like DocFly. Well now, at least for text field, checkbox, and dropdown, you can do it through plain Python code. I personally find this a huge milestone of the project and if you are interested in this new feature checkout the documentation here . The second new feature, which is actually not quite new because this was how PyPDFForm worked back in its ancestral stage, is that now you can fill a PDF form \"in place\", meaning when you fill it this way, the output PDF will look like as if it's filled manually. The reason why this got removed was because of a bug related to text field, where when filled this way the text that got put into a text field will not show up unless the text field is actually clicked by mouse and selected. Fortunately I was enlightened by another user from this sub last time I posted here and he/she gave me a solution on this weird text field bug. Thanks to that, I was able to bring this old but yet new feature back to the library for those who just wants to simply fill a PDF form. Again if you are interested, it's documented here . If you are interested in any of these new features or just the library in general, feel free to go checkout the newest release of the project, try it, test it, and leave comments or suggestions. And of course if you are willing, a star on GitHub is always kindly appreciated."},
{"Title": "Open Sourcing a Python Project the Right Way in 2024", "Author": "u/Vegetable_Study3730", "Content": "I recently open-sourced a package and the tooling was a bit of a struggle. I decided to write down the steps & all the tools needed to open-source a Python package in a scalable way that invite users and contributors. https://jonathanadly.com/open-sourcing-a-python-project-the-right-way-in-2024 Happy to hear your feedback!"},
{"Title": "Python Wrapper For Meta AI (Llama 3)", "Author": "u/Significant-Turn4107", "Content": "Here is just a small wrapper to interact with the new MetaAI chat bot assistant with Python ( https://www.meta.ai/ ), which is running the newly release Llama 3 model. Another nice thing is that its directly connected with Bing Search so you will be able to get the latest informations. https://github.com/Strvm/meta-ai-api"},
{"Title": "Has PyPI ceased all support?", "Author": "u/baekalfen", "Content": "Does anyone know what‚Äôs going on behind the scenes of PyPI (who maintains `pip install`)? It seems like they‚Äôve stopped processing support tickets over a month ago. I‚Äôve hit the data limit for my package [PyBoy]( https://github.com/baekalfen/pyboy ), and I immediately posted a ticket to get the limit increased (as others have successfully done). But after more than a month, and several attempts at contacting the support team, I‚Äôve heard nothing back, and I‚Äôve run out of options. Does anyone know what‚Äôs happening, or how to get a comment from PyPI? My ticket is this one: https://github.com/pypi/support/issues/3757"},
{"Title": "Achieve true parallelism in Python 3.12", "Author": "u/ThatsAHumanPerson", "Content": "Article link: https://rishiraj.me/articles/2024-04/python_subinterpreter_parallelism I have written an article, which should be helpful to folks at all experience levels, covering various multi-tasking paradigms in computers, and how they apply in CPython, with its unique limitations like the Global Interpreter Lock. Using this knowledge, we look at traditional ways to achieve \"true parallelism\" (i.e. multiple tasks running at the same time) in Python. Finally, we build a solution utilizing newer concepts in Python 3.12 to run any arbitrary pure Python code in parallel across multiple threads. All the code used to achieve this, along with the benchmarking code are available in the repository linked in the blog-post. This is my first time writing a technical post in Python. Any feedback would be really appreciated! üòä"},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Intermediate Project: Python Plotly Data Visuals: Dropdowns and Range Sliders For User Interaction", "Author": "u/jgloewen", "Content": "Interactive data visualization is a powerful tool that can significantly enhance the analysis and interpretation of complex datasets. With Python, the Plotly library offers various features that can be used to create interactive publication-quality graphs. This project demonstrates how to use 2 of these awesome features: Range Slider Dropdown Menu A good dataset is essential to demonstrate this style of interactivity. For this project, we will use the UN population projection data to sort and visualize by country for various age groups. FREE ARTICLE: https://johnloewen.substack.com/p/python-plotly-combining-dropdowns-and-range-sliders-for-user-interaction-658dc6fd9c71"},
{"Title": "My latest TILs about Python", "Author": "u/JCx64", "Content": "After 10+ years working with it, I keep discovering new features. This is a list of the most recent ones: https://jcarlosroldan.com/post/329"},
{"Title": "Single file, hot reloading python server with raw websockets and inotify", "Author": "u/tootac", "Content": "A project to build a local server without any external dependencies with main features of hot reloading browser on source modification. The idea is to make it simple, standalone and work without any setup. The project uses regular sockets, websockets, inotify and a bit of javascript on the fly embedding to allow users to achieve automatic synchronization with browser. The article describes hot to monitor file changes with direct loading of libc and requesting kernel notify on change. Free article and code: https://hereket.com/posts/linux_live_reload_python_server/"},
{"Title": "I made a team of AI manage my YouTube channel and my work with Python based framework CrewAI", "Author": "u/fx2mx3", "Content": "I found this awesome AI framework for python called \"Crew AI\", that allows us to create assistants, or agents in the CrewAI lingo, and assign specific tasks. I started my tiny youtube channel 3 months and for a youtuber, especially a novice one like myself, there are a bunch of tasks such as checking what other similar channels are doing, which topics are trending and if a video idea is good or not. There is also the part of coming up with the video content idea itself, create a catchy title, suitable youtube tags and finding the appropriate forum to talk about! You folks get the picture. It's a great project for anyone seeking to automate tasks not just related to a youtube channel, but this could easily be adapted to your daily job tasks, or even a startup idea. I have written a full article about how the project unfolds and also a youtube video for how I did it step by step, which you are welcome to check it out! The links are as follows: The medium article: https://medium.digitalmirror.uk/create-an-ai-team-to-manage-your-youtube-channel-5dc1e6c9b31b The YouTube video: https://youtu.be/5JoVeYcxgpU And of course the source code: https://github.com/fmiguelmmartins/crewaiyoutube.git As always, if you would like to drop some feedback so I can improve with time I would be grateful! Cheers"},
{"Title": "Anycrc - Python CRC computation library", "Author": "u/marzoogy", "Content": "Link: https://github.com/marzooqy/anycrc What My Project Does This is a Cython module with bindings to the crcany library. It supports calculating CRC hashes of arbitary sizes as well as updating a crc hash over time, and I believe that it is the fastest generic CRC library available for Python. Target Audience Anyone that needs to compute CRCs. Comparison with Alternatives crcmod-plus: A modernized version of the old crcmod library. It's highly customizable and fast. fastcrc: A library with bindings to the crc-rs library. Has a limited selection of CRCs to use. binascii: The standard library module comes with support for only CRC32 and CRC-CCITT, and at least the CRC32 implementation used is pretty fast according to my testing. CrcEngine: If you are looking for a pure Python implementation of CRC, then this is a good option. It is however one to two orders of magnitude slower than the libraries that use C/Rust bindings. crc: Popular but extremely inefficient CRC library. It does several things that severely slow it down, such as using an entire class to represent a byte. It's three orders of magnitude slower than the faster options."},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Returns Python library for FP", "Author": "u/yinshangyi", "Content": "Hello! I work in big data space (data engineering), I mainly used Java, Scala and Python. I have been learning functional programming in greater depth and I found this Python library which seems pretty cool. https://github.com/dry-python/returns I've used it at work for implementing an Either based error handling. It seems a great library. Any of you have used it? Any thoughts? For sure, I prefer doing FP in Scala but given the job market isn't too kind too Scala and FP languages in general. What are your thoughts to bring FP (at least parts of it) to the Python world? Some people in the TypeScript world seem to take that direction: https://github.com/Effect-TS/effect"},
{"Title": "Introducing Stockdex: A Superior Python Package for Real-Time Financial Data Retrieval", "Author": "u/nginx26", "Content": "Greetings! I've previously introduced my Python package in this post. Since then, I've significantly enhanced its performance and expanded its capabilities. What My Project Does A lightweight python package designed for the efficient retrieval of financial data from various sources like Yahoo Finance, Nasdaq, Digrin and JustETF websites (somewhat similar to yfinance). This tool provides functionality akin to yfinance but with broader data access. Main benefits of stockdex over yfinance Fresh data : Yahoo Finance often delays updates to financial data by several days while Nasdaq and other sources typically update on the day reports are released. Stockdex enables access to this fresher data, such as quarterly earnings. Broader Data Sources : Unlike yfinance which relies solely on the Yahoo Finance API, Stockdex aggregates data from multiple platforms including Digrin, JustETF, Nasdaq, and Yahoo Finance. For specific examples of data retrieval, refer to this readme . Access to Historical Data : Yahoo Finance limits access to the most recent five annual or four quarterly reports. Stockdex , however, taps into sources that maintain extensive historical archives not available through Yahoo Finance. Target Audience The package is targeted at people who are interested in financial analysis using python. Explore more: Github Repo Link Pypi link"},
{"Title": "Python icon library with 5237 high-quality icons (Tabler Icons wrapper)", "Author": "u/niklashnng", "Content": "What My Project Does: pytablericons is a Python wrapper for the tabler-icons project, which provides a set of 5237 free MIT-licensed high-quality SVG icons for web projects with each icon being designed on a 24x24 grid and a 2px stroke. It allows you to load any of those SVG icons directly into a Pillow PNG Image in any size and color without losing quality and without having to download the icons manually or handling the SVG conversion yourself. For the outline icons, the stroke width is customizable as well. All of this is done in just a single line and since the icon is a Pillow Image, it can then be used easily with basically anything (e.g. PyQt5, PyQt6, PySide2, PySide6, Tkinter, etc.) Target Audience: This is useful for any Python developer who is working on UI projects and wants to easily display modern icons in any size and color without losing quality and without having to download each icon manually. Comparison : / Links: Preview: https://github.com/niklashenning/pytablericons/assets/58544929/e13fb020-4d5f-4e28-bd5f-0d5659bd6582 GitHub: https://github.com/niklashenning/pytablericons"},
{"Title": "A word search game generator", "Author": "u/RRTheGuy", "Content": "https://github.com/RadoTheProgrammer/motsmeles/ What my project does It generate word search games with custom words, dimensions. Target audience For people who want to get fun with some word searching Comparison This is a python library, it's open source, and you can use it in a python code, or with the command-line. Also, you have freedom if you want to have diagonal or reversed words."},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "What's the best library to find differences in two text files and visulaize it?", "Author": "u/TheGupta", "Content": "difflib is popular. Most of the answers on Stack Overflow uses it. But, I think Google's diff-match-patch is better. See an example here . Or, am I missing something?"},
{"Title": "Alternative data managment tool to replace the need to use Jupyter Notebook for Python", "Author": "u/IllogicalLunarBear", "Content": "What My Project Does: For everyone who says they use jupyter Notebook for Python to do chunks of code work‚Ä¶ I developed my own data management system for that kind of stuff that provides a wrapper over serialization that essentially creates pythonic pointers to data on any network drive. I just grab my data that I want to grab. Provides autocomplete and C# like attributes. I can dynamically build any data storage class using UML formatted infrastructure-as-code to build the front end. The back end is completely dynamic. Here is the pypi link. Note that I developed this to handle genomic data that was generated over the course of a week of 24/7 computation, that I break into multiple logical chunks. You dont need to rely on Jupyter to hold your data and you can retrieve it months after generating it. This software routes the data to whatever means you use. It works with databases, pickles, yaml. You just have to add your interface in the data routing class. I just did yaml to start. This is just a wrapper around everything so you can declare your class like normal and at variable declaration you have data permanence instantly with no user interaction. If you pass Y=1 and power off your computer and turn it back on again Y still equals 1 when you call that variable instantly Target Audience (e.g., Is it meant for production, just a toy project, etc.) production as I use it for my genomic research on my project with Stanford Comparison (A brief comparison explaining how it differs from existing alternatives.): There is no alternatve to this project as far as i know. It replaces the need for Jupyter Notebook to hold your data as you can retrieve it months after generating it. PyPi: https://pypi.org/project/data-nut-squirrel/ Source Code: https://github.com/LunarFawn/rna_squirrel Edit: Here is an example of implementation where I actually gave a shit about documentation. https://pypi.org/project/serena-rna-tool/"},
{"Title": "Ruff 0.4.0 just dropped, with a faster parser and a new language server", "Author": "u/QueasyEntrance6269", "Content": "Release notes here , seems to be a 20-40% improvement around the board. This version features a new hand-written parser (rather than a generated one) that is much faster and offers better error messages. It also comes with a new rust-native language server inspired by rust-analyzer, that is multithreaded. I think they‚Äôre challenging Pylance‚Äôs throne, wouldn‚Äôt be surprised if the team goes after type checking next."},
{"Title": "Understanding State Machines in Python Through a Practical Example", "Author": "u/pemidi", "Content": "Hey everyone! I've written an article that simplifies the concept of state machines using Python, with a practical example related to order statuses. If you've ever been confused about state machines or just want a refresher with a real-world application, this might be just what you're looking for. Check it out and let me know what you think! Read the full article here I'm here for any questions or discussions"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Project: An Interactive Python Dashboard For Data Storytelling UN Food Security", "Author": "u/jgloewen", "Content": "Python‚Äôs Plotly Dash is a powerful tool for creating interactive data visualizations. As a Comp Sci professor, I use it extensively for interactive dashboards. Its usefulness lies in its ability to create web-based applications directly from Python code, without the need for additional web development skills. Using UN food security data, let me show you an example of how you can create an interesting and useful Python Plotly dashboard that tells a data story using: A bubble chart: with variable-sized markers, a bubble chart can represent the severity of undernourishment within a given country. Each bubble visually represents the scale of undernourishment. A horizontal bar chart: with its rigid organized structure, a horizontal bar chart provides a clear ordered list that emphasizes the 10 most undernourished countries by percentage of population. A dropdown menu: by Year, allowing the user to see the changing story over a period of time. These two data visualizations offer a dual perspective on the global picture of undernourishment: one that is geographically broad and another that is focused and comparative. Here's a step-by-step project on how to put this all together. FREE ARTICLE: https://johnloewen.substack.com/p/combining-data-visuals-an-interactive"},
{"Title": "I made a Python desktop app using PyWebview (similar to ElectronJS) & ReactJS that tracks time.", "Author": "u/zynix", "Content": "https://github.com/devdave/pyminder What does it do? The application is a very simple time tracker broken down by Client->Project->Task.  In addition it has a summary report view plus a simple CRUD like data manager.     The bigger goal was to see what it entailed to mix Python w/ReactJS and if it was easier using HTML+TypeScript over QT, TK, or some other unique API. Target Audience While I am using PyMinder to track my own work it wasn't meant for a general audience.   Instead I am sharing this in case anyone else has a need to make a desktop application and looking for alternatives. Comparison I have decades of experience with HTML & Ecma script plus I have found very little benefit is gained learning yet another way to make a user interface.   In addition there is more resources for HTML+JS tools and support. One thing to note, if the debug console is closed, the app generally consumes only 300MB.   I was expecting it to be more given the abstraction layers and behind the scenes it is an instrumented web browser instance. Future sub project 90% of the code is type annotated but that 10% is a \"here be monsters\" component that scans a Python class and makes a Typescript API-Bridge.   Transformer works but it is very much hacked together with 2 unit tests that barely cover 5% of it. That said, I would like to rip that part out, clean it up, and make it its own thing as I could see some use for a Python to Typescript adapter library."},
{"Title": "Import pdf into python GUI and measure length within PDF", "Author": "u/Most-Guarantee956", "Content": "I'm interested to create a software where I can import some construction drawings and measure the distance within the PDF as a first step of my work. Tried a few open source pdf but none of them still allows me to have a correct measurement on the PDF, anyone have an idea?"},
{"Title": "PyPDFForm now lets you create widgets without Acrobat", "Author": "u/chinapandaman", "Content": "Hello ! Earlier this year I made a post about my open source project PyPDFForm and got some really nice feedbacks from you guys. I have been since then continuously working on it and I'd love to share you two really cool features that were newly added to the library. The first one is, like what the title says, the library finally supports creating a subset of widgets through code now. One of the previous hard dependency for PyPDFForm was that it requires a PDF template that was prepared using another tool, namely Adobe Acrobat or some web based ones like DocFly. Well now, at least for text field, checkbox, and dropdown, you can do it through plain Python code. I personally find this a huge milestone of the project and if you are interested in this new feature checkout the documentation here . The second new feature, which is actually not quite new because this was how PyPDFForm worked back in its ancestral stage, is that now you can fill a PDF form \"in place\", meaning when you fill it this way, the output PDF will look like as if it's filled manually. The reason why this got removed was because of a bug related to text field, where when filled this way the text that got put into a text field will not show up unless the text field is actually clicked by mouse and selected. Fortunately I was enlightened by another user from this sub last time I posted here and he/she gave me a solution on this weird text field bug. Thanks to that, I was able to bring this old but yet new feature back to the library for those who just wants to simply fill a PDF form. Again if you are interested, it's documented here . If you are interested in any of these new features or just the library in general, feel free to go checkout the newest release of the project, try it, test it, and leave comments or suggestions. And of course if you are willing, a star on GitHub is always kindly appreciated."},
{"Title": "Open Sourcing a Python Project the Right Way in 2024", "Author": "u/Vegetable_Study3730", "Content": "I recently open-sourced a package and the tooling was a bit of a struggle. I decided to write down the steps & all the tools needed to open-source a Python package in a scalable way that invite users and contributors. https://jonathanadly.com/open-sourcing-a-python-project-the-right-way-in-2024 Happy to hear your feedback!"},
{"Title": "Python Wrapper For Meta AI (Llama 3)", "Author": "u/Significant-Turn4107", "Content": "Here is just a small wrapper to interact with the new MetaAI chat bot assistant with Python ( https://www.meta.ai/ ), which is running the newly release Llama 3 model. Another nice thing is that its directly connected with Bing Search so you will be able to get the latest informations. https://github.com/Strvm/meta-ai-api"},
{"Title": "Has PyPI ceased all support?", "Author": "u/baekalfen", "Content": "Does anyone know what‚Äôs going on behind the scenes of PyPI (who maintains `pip install`)? It seems like they‚Äôve stopped processing support tickets over a month ago. I‚Äôve hit the data limit for my package [PyBoy]( https://github.com/baekalfen/pyboy ), and I immediately posted a ticket to get the limit increased (as others have successfully done). But after more than a month, and several attempts at contacting the support team, I‚Äôve heard nothing back, and I‚Äôve run out of options. Does anyone know what‚Äôs happening, or how to get a comment from PyPI? My ticket is this one: https://github.com/pypi/support/issues/3757"},
{"Title": "Achieve true parallelism in Python 3.12", "Author": "u/ThatsAHumanPerson", "Content": "Article link: https://rishiraj.me/articles/2024-04/python_subinterpreter_parallelism I have written an article, which should be helpful to folks at all experience levels, covering various multi-tasking paradigms in computers, and how they apply in CPython, with its unique limitations like the Global Interpreter Lock. Using this knowledge, we look at traditional ways to achieve \"true parallelism\" (i.e. multiple tasks running at the same time) in Python. Finally, we build a solution utilizing newer concepts in Python 3.12 to run any arbitrary pure Python code in parallel across multiple threads. All the code used to achieve this, along with the benchmarking code are available in the repository linked in the blog-post. This is my first time writing a technical post in Python. Any feedback would be really appreciated! üòä"},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "MPCode is a scripting programming language", "Author": "u/weksoftware", "Content": "MPCode is a small open-source scripting programming language written in python 3.12 The repository with the same name on github.com allows you to modify the language to your own needs, execute small scripts, write programmes and libraries. The language itself already has functions, object groups, logical operators, loops and variables. The current functionality has documentation in three languages (English, Italian and Russian) and some small code samples. It would be very nice to get feedback and ideas from you. You can also develop your own projects based on it, as the project is licensed by MIT. Link to the repository: https://github.com/weksoftware/MPCode"},
{"Title": "Web API Security Champion: Broken Object Level Authorization (OWASP TOP 10) for Python Developers", "Author": "u/theowni", "Content": "Explaining one of the most common web API vulnerability classes‚Ää-‚ÄäBroken Object Level Authorization in a practical manner. Providing a case study example based on the Damn Vulnerable RESTaurant API, including methods for identifying and preventing these vulnerabilities. https://devsec-blog.com/2024/04/web-api-security-champion-broken-object-level-authorization-owasp-top-10/"},
{"Title": "chatpdb - gpt for your python debugger", "Author": "u/the1024", "Content": "https://github.com/Never-Over/chatpdb Do you ever copy code, errors, or stack traces into ChatGPT? We did, and found it frustrating to always have to manually find, copy, and paste each relevant piece of information. On top of that, being forced to change tools would switch our focus and cause us to lose our flow. That‚Äôs why we built chatpdb - a python debugger with ChatGPT! Simply use chatpdb like you would use ipdb or pdb ; it‚Äôs a drop in replacement with the exact same functionality. The only addition is the new y keyword, which will trigger a response from ChatGPT. Here‚Äôs how it works: > /Programming/test-chatpdb/lib.py(2)echo_platform() 1 def echo_platform(platform: str): ----> 2     print(\"You are running on:\" + platform) 3 ipdb> y The exception occurred because the function `echo_platform` tries to concatenate the string \"You are running on:\" with the `platform` variable, which is `None`. [...] In this example, chatpdb correctly diagnoses an error in the current function as being caused by a bug in the calling function. chatpdb automatically passes relevant context about your program, including the source code, stack trace, and error information if available to ChatGPT. If you have a more specific question you can also supply a prompt to y : > /Programming/test-chatpdb/lib.py(2)echo_platform() 1 def echo_platform(platform: str): ----> 2     print(\"You are running on:\" + platform) 3 ipdb> y \"Why is platform coming through as None?\" The variable `platform` is coming through as `None` because the environment variable `\"PLATFORM\"` is not set in your system's environment variables. [...] It‚Äôs easy to install and set up: pip install chatpdb export OPENAI_API_KEY=.... import chatpdb chatpdb.set_trace() What my project does: A drop-in replacement for ipdb or pdb, with ChatGPT built in. Target audience: anyone who currently uses ChatGPT to debug or ask questions about their code. Comparison: Copilot offers similar functionality, but is a much heavier tool and harder to turn off/ignore when you don‚Äôt want it. stackexplain is probably the closest tool; however it lacks the ability to hook into your program in any other way than fully running the python process. chatpdb meets you where you are ‚Äì AI tooling that‚Äôs only invoked when you need it. We hope you give it a try! We‚Äôd love any feedback or suggestions. Docs"},
{"Title": "groupby in Python| Python Hacks you might now know.", "Author": "u/hasanul_islam", "Content": "Groupby in itertools module of python works differently that you might have thought. In this video, I have tried to explain very clearly about this feature. You are requested to watch the video. If you like, you can share the video and subscribe my youtube channel for further hacks. https://youtu.be/sX8G8qNwxjc?si=UTdHvbDKIMfOGhpr"},
{"Title": "Handling of long-running processes with timeouts in Flask + Gunicorn API app", "Author": "u/zacky2004", "Content": "I run an API (Python + Flask + Gunicorn) which has an endpoint that's calls a nested processes, which can sometimes take 6 seconds, to sometimes 60+ seconds to complete. The time it takes isn't a measure of my API's performance really, but rather its bottlenecked by an external service that the process has to rely on. Im looking for any suggested libraries that can safely handle this secondary process asynchronously with a long timeout. Ideally the API endpoint is called, and it won't have to wait for this process to finish if it takes more than 30+ seconds. Here's a mockup of what the chain of commands look like. Process 1: core.slurm.SlurmUser - addSlurmUser took: 1.6083331108093262. This process then calls core.slurm.SlurmUser - addScratch (which takes sometimes 6 seconds, sometimes 30+ seconds) Child Process 2: core.slurm.SlurmUser - addScratch took: 29.492394208908081 Any advice? Am I missing something that I can take advantage of already? The App is built using Flask + Gunicorn workers. Wondering If Celery is something that can nicely integrate with my current setup, or is it something that would replace Gunicorn in this scenario?"},
{"Title": "Deploying Multi-Module Python Applications on GCP: Service Recommendations?", "Author": "u/ste042024", "Content": "I'm working on a project involving a Python application with multiple interacting modules and am looking for some advice on deploying it efficiently on Google Cloud Platform . Here's a brief overview of what the application entails: Functionality Overview: ‚Ä¢ Data Fetching : One module fetches data from external websites. ‚Ä¢ Data Extraction : Another module extracts data from another GCP project. ‚Ä¢ Data Processing : Variables are processed across various modules. ‚Ä¢ CSV Output : The app generates and stores CSV files. ‚Ä¢ Scheduled Running : The application should run automatically once daily. I would greatly appreciate your insights on the following questions: What are the best GCP services to use for this type of application? Recommended Python libraries for handling web data retrieval in this case? How to proceed step by step? I intend to first deploy my Python application using placeholder inputs and simple printed outputs. Then, I plan to integrate the data fetching component and implement the creation and storage of CSV files. Does this strategy seem logical to you? Currently, I only tried to deploy my Python code using Google Cloud Functions, but I find this method impractical because I must deploy each module separately. I would prefer a solution that allows me to deploy all my code at once, where the Python modules can smoothly pass variables among themselves. Thank you in advance for your help!"},
{"Title": "ScrumMD: A CLI Scrum tool written in Python", "Author": "u/thelochok", "Content": "I got grumpy with our Scrum process, and I thought about what kinda tool I'd love to work with... so I started making it in my beloved Python. I think it's mature enough I'd like to start giving it to other people. It's called ScrumMD. It's open source, and you can already install it with pip (pip install scrummd) if you've got Python 3.10+. Documented, with tutorials on https://scrummd.readthedocs.io/en/stable/ and source on https://github.com/lkingsford/scrummd What my project does Short version is that it's some tools to support you storing all of your Scrum cards (or, I guess, other cards - tickets perhaps?) in markdown format on a local machine. There's intentionally a lot of flexibility - so, every card needs a summary, but everything else is fair game. You can configure to require fields for some collections (like needing status in stories), or limit fields (like requiring status be 'Done' or 'In Progress'). Target Audience Limited, but public. It's chief audience is software engineers who work in self-organising teams. It's for teams who use processes like Scrum, but don't need the bureaucracy layers. Honestly - I know it's niche. Heck, I won't even be using it at work myself. But, if I could, I would - because I actually like using it. Comparison I'm not aware of any CLI tools that do this kinda thing - particularly with local markdown files. The chief tool it would replace is a project/scrum management tool like Jira, or an issue tracker like the one that comes with GitHub. Both of those are far more friendly to a non-technical audience than ScrumMD is. Technical things Technically - it's pretty darned vanilla. The only non-included bits I'm using are a toml library on Python 3.10, Sphynx for Doco, Setup Tools and Pytest. I use Pylint and Mypy for my own sake. Wasn't anything technically novel going on - although it's the first time I've tried submitting anything to PyPi myself. It's been fun to try setting up CI in GitHub - I'd used a number of other CI/CD frameworks, but I think I like GitHub actions. Happy to talk through anything I've done here, and would really be genuinely pleased to be forced to talk my way through any design decisions or hear ways I could have structured it better."},
{"Title": "Why Jupyter notebook", "Author": "u/IllogicalLunarBear", "Content": "I did a search and read previous discussions in this sub about why Jupyter labs exists. From what I understand it is for quick code to try things out especially in data science ‚Ä¶. Thing is, can‚Äôt you get the same thing via a decent IDE with autocomplete and quality debug tools in some quick Python code like in VSCODE? I do that all the time‚Ä¶. I develop RNA computational software in Python and I abandoned Jupyter labs very early on as it just was not well suited to the job of bespoke data science. So many limitations and don‚Äôt get me started with widgets‚Ä¶ I‚Äôve been  developing code for decades and I have spent 3 days trying to understand widgets and I think I finally understand how to modify a text box and get a value from it‚Ä¶. You have to observe (why?), then act on, then route through some output and then you can get the info? It‚Äôs like someone was drunk while trying to emulate visual C#. Edit: before anyone else tells me to read vscode docs like I‚Äôm an idiot‚Ä¶ I am unable to remote into the repo I‚Äôm using now for reasons. Not everyone can remote in and the question is not about remoting in. It‚Äôs still garbage in VSCODE‚Ä¶ just use regular python. Edit2: for everyone who says they use it to do chunks of code work‚Ä¶ I developed my own data management system for that kind of stuff that provides a wrapper over serialization that essentially creates pythonic pointers to data on any network drive. I just grab my data that I want to grab. Provides autocomplete and C# like attributes. I can dynamically build any data storage class using UML formatted infrastructure-as-code to build the front end. The back end is completely dynamic. Here is the pypi link. Note that I developed this to handle genomic data that was generated over the course of a week of 24/7 computation. https://pypi.org/project/data-nut-squirrel/"},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Big O Cheat Sheet: the time complexities of operations Python's data structures", "Author": "u/treyhunner", "Content": "I made a cheat sheet of all common operations on Python's many data structures. This include both the built-in data structures and all common standard library data structures. The time complexities of different data structures in Python If you're unfamiliar with time complexity and Big O notation, be sure to read the first section and the last two sections. I also recommend Ned Batchelder's talk/article that explains this topic more deeply."},
{"Title": "Transforming Images to Art with Python: My Journey with Stable Diffusion", "Author": "u/Maleficent_Yak_993", "Content": "Hey Python community! I created a stable diffusion pipeline to convert reference images to prompt and used it along with text prompt to generate variations of reference image. Explainer video here: https://www.youtube.com/watch?v=x9VryjEcxzk"},
{"Title": "bridge ‚Äî automatic infrastructure for Django", "Author": "u/the1024", "Content": "https://github.com/Never-Over/bridge The Problem We built bridge to solve the most frustrating part of any new project ‚Äî infrastructure. Whenever you spin up a new Django project, you usually have to manually configure Postgres, background workers, a task queue, and more. The problem is amplified when you go to deploy your application ‚Äî hosting providers don‚Äôt understand anything about what you‚Äôve configured already, so you have to run through an even more complicated process to set up the same infrastructure in a deployed environment. What My Project Does bridge is a pip-installable package that spins up all of the infrastructure you need, and automatically connects it to your Django project. By adding a single line to your Django project's settings.py file, bridge configures everything for you ‚Äî this means you don‚Äôt need to mess with DATABASES , BROKER_URL , or other environment variables to connect to these services. bridge also gives you the access you need to manage these services, including a database and Redis shell, as well as a Flower instance for monitoring background tasks. When you‚Äôre ready to deploy, bridge can handle that as well. By running bridge init render , bridge will write all of the configuration necessary to deploy your application on Render , including a button to trigger deploys straight from your README. If you don‚Äôt want all of these services, (say you already have a database, and just want to add background workers) bridge supports that too! It can automate everything you need and nothing you don‚Äôt. Target Audience This tool is intended for Django developers who are getting new projects off the ground. Comparison Compared to how Django functions on its own, bridge abstracts and manages all of the services you'd need to manually set up and configure. Compared to other boilerplate or starter pack repos, bridge is more lightweight and doesn't force you to change your application code. It is focused only on handling all of the required dependencies, startup, and teardown of your infrastructure. How it Works bridge is built on top of Docker, so you get fully isolated and up-to-date versions of Postgres and Redis from the beginning. Celery and Flower need to run on top of your app code, so we hook into runserver to spin these up as background processes. If you need to spin things down, bridge stop will conveniently shut down all services that it‚Äôs started. Coming Soon In the future, we want to add support for more services (jupyter, mail/mailhog etc), more hosting providers (Heroku, Railway, etc.), and more configuration (env vars, optional dependencies, etc). bridge is and always will be fully open source. Please give it a try and we‚Äôd love any feedback! Github Docs PyPI"},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "UXsim 1.1.1 released: Significantly increased performance for the network traffic flow simulator", "Author": "u/Balance-", "Content": "Version 1.1.1 of UXsim is released, which improves performance significantly. Main Changes in 1.1.1 Add setting to adjust vehicle logging time interval via World.vehicle_logging_timestep_interval By lowering the interval (e.g., World.vehicle_logging_timestep_interval=2), the simulation time can be reduced (~20% speed up), and we can obtain vehicle trajectory data with slightly less accuracy. The logging setting does not affect the internal simulation accuracy. Only the outputted trajectories are affected. By setting World.vehicle_logging_timestep_interval=-1, the record_log is turned off, and the simulation time can be significantly reduced (~40% speed up). This addresses Issue #58 Correct route choice behavior Vehicle.links_prefer and Vehicle.links_avoid work correctly now. UXsim UXsim is a free, open-source macroscopic and mesoscopic network traffic flow simulator written in Python. It simulates the movements of car travelers and traffic congestion in road networks. It is suitable for simulating large-scale (e.g., city-scale) traffic phenomena. UXsim is especially useful for scientific and educational purposes because of its simple, lightweight, and customizable features, but users are free to use UXsim for any purpose."},
{"Title": "Commercial pip packages?", "Author": "u/Normal_Antenna", "Content": "I‚Äôm aware there is a big open source community aspect to the python community. So my question is: ‚Äúwould uploading commercial software as a pip package offend the community?‚Äù I‚Äôm hesitant if this is normal, and if users would consider trials for components that would then make their project ask for a license to work after some time. Just wondering if it‚Äôs worth the effort to make products more accessible, or if it would just rub users the wrong way."},
{"Title": "Question: Tool for editing figures in Python", "Author": "u/Dangerous_Law1678", "Content": "So I'm trying to switch from MATLAB to Python for my numerical simulations. One of the things I like about MATLAB is the ability to edit figures by adding stuff (eg arrows, lines, text ... ) to the figure within the software itself. This is what I mean. It's a very handy tool in my line of work and I use it often. I was wondering if there's any similar tool for Python. I use matplotlib and seaborn. Any suggestions would be great. Thanks guys!"},
{"Title": "How fast can Python parse 1 billion rows of data? (1brc)", "Author": "u/mercer22", "Content": "https://www.youtube.com/watch?v=utTaPW32gKY I made a video summarizing the top techniques used by the Python community in the recently popular One Billion Row Challenge (1brc, https://github.com/gunnarmorling/1brc ). I adapted one of the top Python submissions into the fastest pure Python approach for the 1brc (using only built-in libraries). Also, I tested a few awesome libraries (polars, duckdb) to see how well they can carve through the challenge's 1 billion rows of input data. If anyone wants to try to speed up my solution, then feel free to fork this repo https://github.com/dougmercer-yt/1brc and give it a shot!"},
{"Title": "Some days I really love python...", "Author": "u/Jazzlike-Compote4463", "Content": "I have a problem that I was terrible at listening to new music, everything I listen to is from around 2000 - 2010. There is a radio station here in the UK called BBC 6 music that has a decent playlist but since I only really listen in the car its not always appropriate to note the track names down - and my memory is terrible so I always forget by the time I get home. Luckily they put the playlist online but it's just text links so I can't add it to my music service of choice - YouTube Music. Thankfully the page is pretty simple so I wrote a basic ass scraper that will grab the HTML, parse through the tracks with beautful soup, then use the ytmusicapi package to remove the old tracks from my playlist and recreate it with the freshly scraped data, railway then makes it dead easy to deploy and schedule it to run once a day. This thing took me probably less than an hour and a half to create and will hopefully open me up to some quality new tunes!"},
{"Title": "Meta Used Monolithic Architecture Using Python to Ship Threads in Only Five Months", "Author": "u/rgancarz", "Content": "https://www.infoq.com/news/2024/04/meta-threads-instagram-5-months/ Zahan Malkani talked during QCon London 2024 about Meta's journey from identifying the opportunity in the market to shipping the Threads application only five months later. The company leveraged Instagram's existing monolithic architecture, written in Python and PHP, and quickly iterated to create a new text-first microblogging service in record time."},
{"Title": "float('inf') is bad practice", "Author": "u/Librarian-Rare", "Content": "I don't know why float('inf') was chosen as the way the language accesses infinity since this is using a magic string. Why couldn't it just be float.inf? That way magic string is avoided. Feels too basic of a best practice to be simply passed up. Anyone know the reason behind this?"},
{"Title": "Using Pandas 2 and different datetime erros", "Author": "u/technically_right_", "Content": "Hey Folks, I am working on a project that uses a bit old pandas version (1.5.3). I am trying to update it and use a more recent version (2.2.2). Its the first time I use pandas 2 btw. I simply ran all my unit tests and got multiple and different errors all concerning some datetime aspects. I tried troubleshooting by searching the errors and look in pandas release note but I find really complicated. It feels like the documentation is really exhaustive but I could really use a note on principal errors one can get switching to pandas 2. Does anyone have any handy blogpost, article, documentation that specifies this ? My focus is mainly on datetime errors when differences computed or comparaisons Thanks"},
{"Title": "kmeans-tjdwill: A K-Means Implementation that Maintains Data Association", "Author": "u/tjdwill", "Content": "Hello, I recently published my first Python package on PyPi called kmeans-tjdwill ( source ). This is a k-means clustering implementation I wrote as a toy project that eventually was used extensively throughout my Master's degree, especially in my thesis. After a refactor and some optimization, I wanted to post here to hopefully elicit feedback and commentary from more experienced, professional software engineers. What My Project Does The central function is kmeans.cluster which segments input data into groups based on proximity to centroids. Users may either specify the initial means themselves or allow them to be chosen from among the input data. There is also a function for viewing the clustering process for 2-D and 3-D data, and, finally, a function that uses kmeans.cluster to perform image segmentation. Visual examples of both are found on the landing page of the Github repo linked above. Comparison This k-means clustering implementation is different from others I've seen in that it maintains what I call‚Äîfor want of a better term‚Äî data association . The example I use is with object detection data. Say we have object detection data where each detection is of form [x_min, y_min, x_max, y_max, conf, label_id] Normally, if we want to cluster based on the first four elements (the bounding box data), we would lose track of the remaining two elements as the data is shuffled around during clustering. If we were to instead cluster based on all of the elements, the centroid values would be affected by the extra elements because we'd be clustering 6-D data instead of 4-D. This package provides a way to cluster based on the first four elements while also maintaining the association with the remaining data through the ndim parameter. This means that once the data is done clustering, you can then use the remaining data for further processing. In this project's implementation of image segmentation, for example, I leverage the above such that each (R,G,B) pixel remembers its  original image coordinates. This was very useful for replacing each pixel in a given cluster with the centroidal color value. Target Audience While I did use the first iteration of this project through multiple projects of my Master's degree, it is certainly not on the level of industry. Hopefully, this project can be used for exploration by those learning the concept. I've never had a code review before, so I appreciate any feedback or suggestions for improvement. Thank you for your time."},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "zpy can now use uv as a backend to replace Python's venv module and pip-tools", "Author": "u/AndydeCleyre", "Content": "Hello! What My Project Does zpy is a set of Zsh functions, mostly wrapping pip-tools or uv, for the simple and practical management of Python virtual environments, dependency specifications, and isolated Python app installation. You may find it a suitable alternative to poetry, pipenv, pipx, or unaided pip-tools or uv. There's a focus on use of good old requirements.txt files, and extensive tab completion assistance. It's not new, but I just made a new release that can use uv as a backend, making it much faster (and hipper, obviously). Target Audience (e.g., Is it meant for production, just a toy project, etc.) I'd say it's for folks who enjoy Zsh and tab completion, and a preference for \"vanilla\" and standards-based Python environment definitions. It's more oriented toward personal developer environments, not deployments. But it should work with the same requirements and pyproject.toml files that are used in production, with whatever tools you want to use there (e.g. pip ). Comparison (A brief comparison explaining how it differs from existing alternatives.) You may find poetry or pipenv a good fit, and that's ok. I have personally found them to conflict with my needs and preferences: both introduce new-syntax files neither can do what pipx does poetry is a bit rigid, only working with installable packages as projects (possibly outdated info, it was this way last time I tried) pipenv is problematically nosy and weird sometimes, e.g. using unrelated files above the current folder without consent neither fully embrace the rich completions and other features that Zsh offers I have had only good experience with pipx, and replacing its features here was not an initial goal. I just couldn't resist when the other functions provided components for a transparent pipx substitute, with excellent tab completion. One difference from using plain uv is that this manages venvs in an external directory, and not necessarily just one venv per project folder. More Notes If you have zpy installed, you can install uv with the pipz command, and from then on zpy will use uv instead of Python's venv module and pip-tools: % pipz install uv If you have any questions, please ask! I personally use it in combination with mise (for Python runtime management) and flit (for package publishing), but aim to keep it rather agnostic and interoperable. Here's some more explanation copied from the readme : Guiding Ideas: You should not have to manually specify the dependencies anywhere other than *requirements.in files Folks who want to use your code shouldn't have to install any new-fangled less-standard tools (pipenv, poetry, pip-tools, zpy, etc.); pip install -r *requirements.txt ought to be sufficient It's nice to keep the venv folder outside of the project itself Not every manageable project needs a pyproject.toml or to be packaged Lockfiles are good Tab completion is wonderful These functions don't : need to be used exclusively need to be used by everyone on the same project do what mise/pyenv/asdf-vm or flit do best (but do work with them if you choose) conflict with anything else your team cares to do with your code; If they can be a friendlier neighbor to your workflows, file an issue overview screenshot"},
{"Title": "Announcing pixi-kernel - Jupyter kernels using Pixi for reproducible notebooks", "Author": "u/renanengmec", "Content": "What My Project Does? Hello everyone, I'm proud to announce a brand-new Python library named pixi-kernel : https://github.com/renan-r-santos/pixi-kernel allowing you to run Jupyter kernels using Pixi for reproducible notebooks. By the way, if you haven't heard of the Pixi package manager, check it out at https://pixi.sh/latest/ .It supports conda and PyPI (through uv ) packages, pyproject.toml config, git, path and editable installs and it is really fast. Target Audience JupyterLab users, production-ready. Comparison The main differences compared to similar solutions such as poetry-kernel are the ability to install conda-forge packages and using kernels in languages other than Python. Compared to nb-conda-kernels , the difference is the use of the Pixi package manager which allows per-directory dependency locking."},
{"Title": "MPCode is a scripting programming language", "Author": "u/weksoftware", "Content": "MPCode is a small open-source scripting programming language written in python 3.12 The repository with the same name on github.com allows you to modify the language to your own needs, execute small scripts, write programmes and libraries. The language itself already has functions, object groups, logical operators, loops and variables. The current functionality has documentation in three languages (English, Italian and Russian) and some small code samples. It would be very nice to get feedback and ideas from you. You can also develop your own projects based on it, as the project is licensed by MIT. Link to the repository: https://github.com/weksoftware/MPCode"},
{"Title": "Web API Security Champion: Broken Object Level Authorization (OWASP TOP 10) for Python Developers", "Author": "u/theowni", "Content": "Explaining one of the most common web API vulnerability classes‚Ää-‚ÄäBroken Object Level Authorization in a practical manner. Providing a case study example based on the Damn Vulnerable RESTaurant API, including methods for identifying and preventing these vulnerabilities. https://devsec-blog.com/2024/04/web-api-security-champion-broken-object-level-authorization-owasp-top-10/"},
{"Title": "chatpdb - gpt for your python debugger", "Author": "u/the1024", "Content": "https://github.com/Never-Over/chatpdb Do you ever copy code, errors, or stack traces into ChatGPT? We did, and found it frustrating to always have to manually find, copy, and paste each relevant piece of information. On top of that, being forced to change tools would switch our focus and cause us to lose our flow. That‚Äôs why we built chatpdb - a python debugger with ChatGPT! Simply use chatpdb like you would use ipdb or pdb ; it‚Äôs a drop in replacement with the exact same functionality. The only addition is the new y keyword, which will trigger a response from ChatGPT. Here‚Äôs how it works: > /Programming/test-chatpdb/lib.py(2)echo_platform() 1 def echo_platform(platform: str): ----> 2     print(\"You are running on:\" + platform) 3 ipdb> y The exception occurred because the function `echo_platform` tries to concatenate the string \"You are running on:\" with the `platform` variable, which is `None`. [...] In this example, chatpdb correctly diagnoses an error in the current function as being caused by a bug in the calling function. chatpdb automatically passes relevant context about your program, including the source code, stack trace, and error information if available to ChatGPT. If you have a more specific question you can also supply a prompt to y : > /Programming/test-chatpdb/lib.py(2)echo_platform() 1 def echo_platform(platform: str): ----> 2     print(\"You are running on:\" + platform) 3 ipdb> y \"Why is platform coming through as None?\" The variable `platform` is coming through as `None` because the environment variable `\"PLATFORM\"` is not set in your system's environment variables. [...] It‚Äôs easy to install and set up: pip install chatpdb export OPENAI_API_KEY=.... import chatpdb chatpdb.set_trace() What my project does: A drop-in replacement for ipdb or pdb, with ChatGPT built in. Target audience: anyone who currently uses ChatGPT to debug or ask questions about their code. Comparison: Copilot offers similar functionality, but is a much heavier tool and harder to turn off/ignore when you don‚Äôt want it. stackexplain is probably the closest tool; however it lacks the ability to hook into your program in any other way than fully running the python process. chatpdb meets you where you are ‚Äì AI tooling that‚Äôs only invoked when you need it. We hope you give it a try! We‚Äôd love any feedback or suggestions. Docs"},
{"Title": "groupby in Python| Python Hacks you might now know.", "Author": "u/hasanul_islam", "Content": "Groupby in itertools module of python works differently that you might have thought. In this video, I have tried to explain very clearly about this feature. You are requested to watch the video. If you like, you can share the video and subscribe my youtube channel for further hacks. https://youtu.be/sX8G8qNwxjc?si=UTdHvbDKIMfOGhpr"},
{"Title": "Handling of long-running processes with timeouts in Flask + Gunicorn API app", "Author": "u/zacky2004", "Content": "I run an API (Python + Flask + Gunicorn) which has an endpoint that's calls a nested processes, which can sometimes take 6 seconds, to sometimes 60+ seconds to complete. The time it takes isn't a measure of my API's performance really, but rather its bottlenecked by an external service that the process has to rely on. Im looking for any suggested libraries that can safely handle this secondary process asynchronously with a long timeout. Ideally the API endpoint is called, and it won't have to wait for this process to finish if it takes more than 30+ seconds. Here's a mockup of what the chain of commands look like. Process 1: core.slurm.SlurmUser - addSlurmUser took: 1.6083331108093262. This process then calls core.slurm.SlurmUser - addScratch (which takes sometimes 6 seconds, sometimes 30+ seconds) Child Process 2: core.slurm.SlurmUser - addScratch took: 29.492394208908081 Any advice? Am I missing something that I can take advantage of already? The App is built using Flask + Gunicorn workers. Wondering If Celery is something that can nicely integrate with my current setup, or is it something that would replace Gunicorn in this scenario?"},
{"Title": "Deploying Multi-Module Python Applications on GCP: Service Recommendations?", "Author": "u/ste042024", "Content": "I'm working on a project involving a Python application with multiple interacting modules and am looking for some advice on deploying it efficiently on Google Cloud Platform . Here's a brief overview of what the application entails: Functionality Overview: ‚Ä¢ Data Fetching : One module fetches data from external websites. ‚Ä¢ Data Extraction : Another module extracts data from another GCP project. ‚Ä¢ Data Processing : Variables are processed across various modules. ‚Ä¢ CSV Output : The app generates and stores CSV files. ‚Ä¢ Scheduled Running : The application should run automatically once daily. I would greatly appreciate your insights on the following questions: What are the best GCP services to use for this type of application? Recommended Python libraries for handling web data retrieval in this case? How to proceed step by step? I intend to first deploy my Python application using placeholder inputs and simple printed outputs. Then, I plan to integrate the data fetching component and implement the creation and storage of CSV files. Does this strategy seem logical to you? Currently, I only tried to deploy my Python code using Google Cloud Functions, but I find this method impractical because I must deploy each module separately. I would prefer a solution that allows me to deploy all my code at once, where the Python modules can smoothly pass variables among themselves. Thank you in advance for your help!"},
{"Title": "ScrumMD: A CLI Scrum tool written in Python", "Author": "u/thelochok", "Content": "I got grumpy with our Scrum process, and I thought about what kinda tool I'd love to work with... so I started making it in my beloved Python. I think it's mature enough I'd like to start giving it to other people. It's called ScrumMD. It's open source, and you can already install it with pip (pip install scrummd) if you've got Python 3.10+. Documented, with tutorials on https://scrummd.readthedocs.io/en/stable/ and source on https://github.com/lkingsford/scrummd What my project does Short version is that it's some tools to support you storing all of your Scrum cards (or, I guess, other cards - tickets perhaps?) in markdown format on a local machine. There's intentionally a lot of flexibility - so, every card needs a summary, but everything else is fair game. You can configure to require fields for some collections (like needing status in stories), or limit fields (like requiring status be 'Done' or 'In Progress'). Target Audience Limited, but public. It's chief audience is software engineers who work in self-organising teams. It's for teams who use processes like Scrum, but don't need the bureaucracy layers. Honestly - I know it's niche. Heck, I won't even be using it at work myself. But, if I could, I would - because I actually like using it. Comparison I'm not aware of any CLI tools that do this kinda thing - particularly with local markdown files. The chief tool it would replace is a project/scrum management tool like Jira, or an issue tracker like the one that comes with GitHub. Both of those are far more friendly to a non-technical audience than ScrumMD is. Technical things Technically - it's pretty darned vanilla. The only non-included bits I'm using are a toml library on Python 3.10, Sphynx for Doco, Setup Tools and Pytest. I use Pylint and Mypy for my own sake. Wasn't anything technically novel going on - although it's the first time I've tried submitting anything to PyPi myself. It's been fun to try setting up CI in GitHub - I'd used a number of other CI/CD frameworks, but I think I like GitHub actions. Happy to talk through anything I've done here, and would really be genuinely pleased to be forced to talk my way through any design decisions or hear ways I could have structured it better."},
{"Title": "Why Jupyter notebook", "Author": "u/IllogicalLunarBear", "Content": "I did a search and read previous discussions in this sub about why Jupyter labs exists. From what I understand it is for quick code to try things out especially in data science ‚Ä¶. Thing is, can‚Äôt you get the same thing via a decent IDE with autocomplete and quality debug tools in some quick Python code like in VSCODE? I do that all the time‚Ä¶. I develop RNA computational software in Python and I abandoned Jupyter labs very early on as it just was not well suited to the job of bespoke data science. So many limitations and don‚Äôt get me started with widgets‚Ä¶ I‚Äôve been  developing code for decades and I have spent 3 days trying to understand widgets and I think I finally understand how to modify a text box and get a value from it‚Ä¶. You have to observe (why?), then act on, then route through some output and then you can get the info? It‚Äôs like someone was drunk while trying to emulate visual C#. Edit: before anyone else tells me to read vscode docs like I‚Äôm an idiot‚Ä¶ I am unable to remote into the repo I‚Äôm using now for reasons. Not everyone can remote in and the question is not about remoting in. It‚Äôs still garbage in VSCODE‚Ä¶ just use regular python. Edit2: for everyone who says they use it to do chunks of code work‚Ä¶ I developed my own data management system for that kind of stuff that provides a wrapper over serialization that essentially creates pythonic pointers to data on any network drive. I just grab my data that I want to grab. Provides autocomplete and C# like attributes. I can dynamically build any data storage class using UML formatted infrastructure-as-code to build the front end. The back end is completely dynamic. Here is the pypi link. Note that I developed this to handle genomic data that was generated over the course of a week of 24/7 computation. https://pypi.org/project/data-nut-squirrel/"},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Big O Cheat Sheet: the time complexities of operations Python's data structures", "Author": "u/treyhunner", "Content": "I made a cheat sheet of all common operations on Python's many data structures. This include both the built-in data structures and all common standard library data structures. The time complexities of different data structures in Python If you're unfamiliar with time complexity and Big O notation, be sure to read the first section and the last two sections. I also recommend Ned Batchelder's talk/article that explains this topic more deeply."},
{"Title": "Transforming Images to Art with Python: My Journey with Stable Diffusion", "Author": "u/Maleficent_Yak_993", "Content": "Hey Python community! I created a stable diffusion pipeline to convert reference images to prompt and used it along with text prompt to generate variations of reference image. Explainer video here: https://www.youtube.com/watch?v=x9VryjEcxzk"},
{"Title": "bridge ‚Äî automatic infrastructure for Django", "Author": "u/the1024", "Content": "https://github.com/Never-Over/bridge The Problem We built bridge to solve the most frustrating part of any new project ‚Äî infrastructure. Whenever you spin up a new Django project, you usually have to manually configure Postgres, background workers, a task queue, and more. The problem is amplified when you go to deploy your application ‚Äî hosting providers don‚Äôt understand anything about what you‚Äôve configured already, so you have to run through an even more complicated process to set up the same infrastructure in a deployed environment. What My Project Does bridge is a pip-installable package that spins up all of the infrastructure you need, and automatically connects it to your Django project. By adding a single line to your Django project's settings.py file, bridge configures everything for you ‚Äî this means you don‚Äôt need to mess with DATABASES , BROKER_URL , or other environment variables to connect to these services. bridge also gives you the access you need to manage these services, including a database and Redis shell, as well as a Flower instance for monitoring background tasks. When you‚Äôre ready to deploy, bridge can handle that as well. By running bridge init render , bridge will write all of the configuration necessary to deploy your application on Render , including a button to trigger deploys straight from your README. If you don‚Äôt want all of these services, (say you already have a database, and just want to add background workers) bridge supports that too! It can automate everything you need and nothing you don‚Äôt. Target Audience This tool is intended for Django developers who are getting new projects off the ground. Comparison Compared to how Django functions on its own, bridge abstracts and manages all of the services you'd need to manually set up and configure. Compared to other boilerplate or starter pack repos, bridge is more lightweight and doesn't force you to change your application code. It is focused only on handling all of the required dependencies, startup, and teardown of your infrastructure. How it Works bridge is built on top of Docker, so you get fully isolated and up-to-date versions of Postgres and Redis from the beginning. Celery and Flower need to run on top of your app code, so we hook into runserver to spin these up as background processes. If you need to spin things down, bridge stop will conveniently shut down all services that it‚Äôs started. Coming Soon In the future, we want to add support for more services (jupyter, mail/mailhog etc), more hosting providers (Heroku, Railway, etc.), and more configuration (env vars, optional dependencies, etc). bridge is and always will be fully open source. Please give it a try and we‚Äôd love any feedback! Github Docs PyPI"},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "UXsim 1.1.1 released: Significantly increased performance for the network traffic flow simulator", "Author": "u/Balance-", "Content": "Version 1.1.1 of UXsim is released, which improves performance significantly. Main Changes in 1.1.1 Add setting to adjust vehicle logging time interval via World.vehicle_logging_timestep_interval By lowering the interval (e.g., World.vehicle_logging_timestep_interval=2), the simulation time can be reduced (~20% speed up), and we can obtain vehicle trajectory data with slightly less accuracy. The logging setting does not affect the internal simulation accuracy. Only the outputted trajectories are affected. By setting World.vehicle_logging_timestep_interval=-1, the record_log is turned off, and the simulation time can be significantly reduced (~40% speed up). This addresses Issue #58 Correct route choice behavior Vehicle.links_prefer and Vehicle.links_avoid work correctly now. UXsim UXsim is a free, open-source macroscopic and mesoscopic network traffic flow simulator written in Python. It simulates the movements of car travelers and traffic congestion in road networks. It is suitable for simulating large-scale (e.g., city-scale) traffic phenomena. UXsim is especially useful for scientific and educational purposes because of its simple, lightweight, and customizable features, but users are free to use UXsim for any purpose."},
{"Title": "Commercial pip packages?", "Author": "u/Normal_Antenna", "Content": "I‚Äôm aware there is a big open source community aspect to the python community. So my question is: ‚Äúwould uploading commercial software as a pip package offend the community?‚Äù I‚Äôm hesitant if this is normal, and if users would consider trials for components that would then make their project ask for a license to work after some time. Just wondering if it‚Äôs worth the effort to make products more accessible, or if it would just rub users the wrong way."},
{"Title": "Question: Tool for editing figures in Python", "Author": "u/Dangerous_Law1678", "Content": "So I'm trying to switch from MATLAB to Python for my numerical simulations. One of the things I like about MATLAB is the ability to edit figures by adding stuff (eg arrows, lines, text ... ) to the figure within the software itself. This is what I mean. It's a very handy tool in my line of work and I use it often. I was wondering if there's any similar tool for Python. I use matplotlib and seaborn. Any suggestions would be great. Thanks guys!"},
{"Title": "How fast can Python parse 1 billion rows of data? (1brc)", "Author": "u/mercer22", "Content": "https://www.youtube.com/watch?v=utTaPW32gKY I made a video summarizing the top techniques used by the Python community in the recently popular One Billion Row Challenge (1brc, https://github.com/gunnarmorling/1brc ). I adapted one of the top Python submissions into the fastest pure Python approach for the 1brc (using only built-in libraries). Also, I tested a few awesome libraries (polars, duckdb) to see how well they can carve through the challenge's 1 billion rows of input data. If anyone wants to try to speed up my solution, then feel free to fork this repo https://github.com/dougmercer-yt/1brc and give it a shot!"},
{"Title": "Some days I really love python...", "Author": "u/Jazzlike-Compote4463", "Content": "I have a problem that I was terrible at listening to new music, everything I listen to is from around 2000 - 2010. There is a radio station here in the UK called BBC 6 music that has a decent playlist but since I only really listen in the car its not always appropriate to note the track names down - and my memory is terrible so I always forget by the time I get home. Luckily they put the playlist online but it's just text links so I can't add it to my music service of choice - YouTube Music. Thankfully the page is pretty simple so I wrote a basic ass scraper that will grab the HTML, parse through the tracks with beautful soup, then use the ytmusicapi package to remove the old tracks from my playlist and recreate it with the freshly scraped data, railway then makes it dead easy to deploy and schedule it to run once a day. This thing took me probably less than an hour and a half to create and will hopefully open me up to some quality new tunes!"},
{"Title": "Meta Used Monolithic Architecture Using Python to Ship Threads in Only Five Months", "Author": "u/rgancarz", "Content": "https://www.infoq.com/news/2024/04/meta-threads-instagram-5-months/ Zahan Malkani talked during QCon London 2024 about Meta's journey from identifying the opportunity in the market to shipping the Threads application only five months later. The company leveraged Instagram's existing monolithic architecture, written in Python and PHP, and quickly iterated to create a new text-first microblogging service in record time."},
{"Title": "float('inf') is bad practice", "Author": "u/Librarian-Rare", "Content": "I don't know why float('inf') was chosen as the way the language accesses infinity since this is using a magic string. Why couldn't it just be float.inf? That way magic string is avoided. Feels too basic of a best practice to be simply passed up. Anyone know the reason behind this?"},
{"Title": "Using Pandas 2 and different datetime erros", "Author": "u/technically_right_", "Content": "Hey Folks, I am working on a project that uses a bit old pandas version (1.5.3). I am trying to update it and use a more recent version (2.2.2). Its the first time I use pandas 2 btw. I simply ran all my unit tests and got multiple and different errors all concerning some datetime aspects. I tried troubleshooting by searching the errors and look in pandas release note but I find really complicated. It feels like the documentation is really exhaustive but I could really use a note on principal errors one can get switching to pandas 2. Does anyone have any handy blogpost, article, documentation that specifies this ? My focus is mainly on datetime errors when differences computed or comparaisons Thanks"},
{"Title": "kmeans-tjdwill: A K-Means Implementation that Maintains Data Association", "Author": "u/tjdwill", "Content": "Hello, I recently published my first Python package on PyPi called kmeans-tjdwill ( source ). This is a k-means clustering implementation I wrote as a toy project that eventually was used extensively throughout my Master's degree, especially in my thesis. After a refactor and some optimization, I wanted to post here to hopefully elicit feedback and commentary from more experienced, professional software engineers. What My Project Does The central function is kmeans.cluster which segments input data into groups based on proximity to centroids. Users may either specify the initial means themselves or allow them to be chosen from among the input data. There is also a function for viewing the clustering process for 2-D and 3-D data, and, finally, a function that uses kmeans.cluster to perform image segmentation. Visual examples of both are found on the landing page of the Github repo linked above. Comparison This k-means clustering implementation is different from others I've seen in that it maintains what I call‚Äîfor want of a better term‚Äî data association . The example I use is with object detection data. Say we have object detection data where each detection is of form [x_min, y_min, x_max, y_max, conf, label_id] Normally, if we want to cluster based on the first four elements (the bounding box data), we would lose track of the remaining two elements as the data is shuffled around during clustering. If we were to instead cluster based on all of the elements, the centroid values would be affected by the extra elements because we'd be clustering 6-D data instead of 4-D. This package provides a way to cluster based on the first four elements while also maintaining the association with the remaining data through the ndim parameter. This means that once the data is done clustering, you can then use the remaining data for further processing. In this project's implementation of image segmentation, for example, I leverage the above such that each (R,G,B) pixel remembers its  original image coordinates. This was very useful for replacing each pixel in a given cluster with the centroidal color value. Target Audience While I did use the first iteration of this project through multiple projects of my Master's degree, it is certainly not on the level of industry. Hopefully, this project can be used for exploration by those learning the concept. I've never had a code review before, so I appreciate any feedback or suggestions for improvement. Thank you for your time."},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "zpy can now use uv as a backend to replace Python's venv module and pip-tools", "Author": "u/AndydeCleyre", "Content": "Hello! What My Project Does zpy is a set of Zsh functions, mostly wrapping pip-tools or uv, for the simple and practical management of Python virtual environments, dependency specifications, and isolated Python app installation. You may find it a suitable alternative to poetry, pipenv, pipx, or unaided pip-tools or uv. There's a focus on use of good old requirements.txt files, and extensive tab completion assistance. It's not new, but I just made a new release that can use uv as a backend, making it much faster (and hipper, obviously). Target Audience (e.g., Is it meant for production, just a toy project, etc.) I'd say it's for folks who enjoy Zsh and tab completion, and a preference for \"vanilla\" and standards-based Python environment definitions. It's more oriented toward personal developer environments, not deployments. But it should work with the same requirements and pyproject.toml files that are used in production, with whatever tools you want to use there (e.g. pip ). Comparison (A brief comparison explaining how it differs from existing alternatives.) You may find poetry or pipenv a good fit, and that's ok. I have personally found them to conflict with my needs and preferences: both introduce new-syntax files neither can do what pipx does poetry is a bit rigid, only working with installable packages as projects (possibly outdated info, it was this way last time I tried) pipenv is problematically nosy and weird sometimes, e.g. using unrelated files above the current folder without consent neither fully embrace the rich completions and other features that Zsh offers I have had only good experience with pipx, and replacing its features here was not an initial goal. I just couldn't resist when the other functions provided components for a transparent pipx substitute, with excellent tab completion. One difference from using plain uv is that this manages venvs in an external directory, and not necessarily just one venv per project folder. More Notes If you have zpy installed, you can install uv with the pipz command, and from then on zpy will use uv instead of Python's venv module and pip-tools: % pipz install uv If you have any questions, please ask! I personally use it in combination with mise (for Python runtime management) and flit (for package publishing), but aim to keep it rather agnostic and interoperable. Here's some more explanation copied from the readme : Guiding Ideas: You should not have to manually specify the dependencies anywhere other than *requirements.in files Folks who want to use your code shouldn't have to install any new-fangled less-standard tools (pipenv, poetry, pip-tools, zpy, etc.); pip install -r *requirements.txt ought to be sufficient It's nice to keep the venv folder outside of the project itself Not every manageable project needs a pyproject.toml or to be packaged Lockfiles are good Tab completion is wonderful These functions don't : need to be used exclusively need to be used by everyone on the same project do what mise/pyenv/asdf-vm or flit do best (but do work with them if you choose) conflict with anything else your team cares to do with your code; If they can be a friendlier neighbor to your workflows, file an issue overview screenshot"},
{"Title": "Announcing pixi-kernel - Jupyter kernels using Pixi for reproducible notebooks", "Author": "u/renanengmec", "Content": "What My Project Does? Hello everyone, I'm proud to announce a brand-new Python library named pixi-kernel : https://github.com/renan-r-santos/pixi-kernel allowing you to run Jupyter kernels using Pixi for reproducible notebooks. By the way, if you haven't heard of the Pixi package manager, check it out at https://pixi.sh/latest/ .It supports conda and PyPI (through uv ) packages, pyproject.toml config, git, path and editable installs and it is really fast. Target Audience JupyterLab users, production-ready. Comparison The main differences compared to similar solutions such as poetry-kernel are the ability to install conda-forge packages and using kernels in languages other than Python. Compared to nb-conda-kernels , the difference is the use of the Pixi package manager which allows per-directory dependency locking."},
{"Title": "motsmeles - word search generator", "Author": "u/RRTheGuy", "Content": "https://github.com/RadoTheProgrammer/motsmeles/ Note: This is written in french. What my project does It generate word search games with custom words, dimensions. Target audience For people who want to get fun with some word searching Comparison This is a python library, it's open source, and you can use it in a python code, or with the command-line. Also, you have freedom if you want to have diagonal or reversed words. Next steps A GUI with pygame to generate and use it in a user-friendly interface."},
{"Title": "Heavy Stack - Top to Bottom Python, Dev Container, Hot Reloading, More", "Author": "u/JamesHutchisonReal", "Content": "Trying this again a third time because automod flagged it as a resource and the mods do not appear to be responding. https://github.com/heavy-resume/heavy-stack What it does: The Heavy Stack is a template web framework that is top to bottom Python using Sanic / (custom) ReactPy / Brython. It's a complete solution that just works out of the box. Target Audience: This is production capable but a little on the early side. Iterating is quite fast. Comparison: Compared to other web frameworks: Just works out of the box, even has a docker image ready to go for the server and your postgres database that uses PG Vector. ReactPy is server side, so instead of creating API calls, you just use `heavy_use_effect` and `heavy_event` to respond to things, do async loads, etc. It uses a websocket. If you're familiar with ReactPy already, the difference is that there's `heavy` versions that inject a database connection to the async functions and let you stuff in user context. The custom version of ReactPy that ships with this reconnects the websocket and attempts to restore the client state. It also supports recording and replaying your actions and uses locust to do load testing with those actions. Hot reloading means if you change a code file you see your changes about 2 seconds after saving without having to refresh the page. For tests it means the test runs near instantly after the initial load. Description: This is a dev container / GitHub Codespaces centric repo, so you'll be best served by using VS Code and either codespaces or something compatible the dev container standard. It's intended to be a template repo which you just run a script to rename to whatever project you want. Here are the features that work \"out of the box\" if you use the dev container or codespace: Pre-made dev container, docker files Top to Bottom Python (Sanic, custom ReactPy, Brython) PostgreSQL, PG Vector, CockroachDB SQL Model (SQLAlchemy + Pydantic) Hot reloading, both server and tests User action recording and playback for load testing Time tracking based on file changes Established patterns and examples, script to generate boilerplate around tables and their domain models There's some known issues and planned updates that will come at a later time, so for the sake of not overselling it, this should be treated like an alpha product. I'd love to see contributions though, especially when it comes to creating off-the-shelf components for ReactPy, or fixing the ReactPy bugs or getting that massive ReactPy PR mentioned in the readme piece-wise merged into the main project. Thanks for your time! I hope this is helpful for some people!"},
{"Title": "Behavior of AttributeError in @property and __getattr__", "Author": "u/Hatteras-", "Content": "I'm sure that many people have encountered the following (code for reproduction below) behavior:  if during the execution of `@property`, an `AttributeError` is raised and the same class implements `__getattr__` then `__getattr__` is invoked with the name of the property resulting in a confusing message: `AttributeError: 'Foo' object has no attribute 'something'`. If we remove `__getattr__` then we get a more meaningful and correct message: `AttributeError: 'Foo' object has no attribute 'bar'`. from dataclasses import dataclass @dataclass class Foo: val = 'foo_value' class Test: def __init__(self): self.foo = Foo() def __getattr__(self, name): return getattr(self.foo, name) @property def something(self): return self.foo.bar t = Test() t.something The error message is misleading and the behavior catches many people (myself including) in surprise and results in many wasted hours of debugging. I've seen people recommend using a custom decorator (instead of `@property` that catches attribute error) or wrapping your `@property` bodies in `try/except`. But it seems to me that this should be dealt with on language level. I'm trying to understand a few things: Is this behavior in Python intentional (i.e. is there a good reason) or just stems from the fact that `__getattr__` is invoked when `AttributeError` is raised when an instance attribute is accessed? If not intentional, is there a relatively easy way to detect `AttributeErrors` cause by `@property` execution in CPython? If so, why hasn't this been handled on language level? I've searched CPython issues on Github and I saw someone trying to correct documentation but couldn't find issues asking about this behavior or suggestions to fix - am I missing something?"},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Is Litestar production ready 2024?", "Author": "u/I_will_delete_myself", "Content": "At least a couple things that seems is that Litestar appears to be a fast  Django lite. Some batteries included, but not too many and opinionated. It's uses Rust so of course it's going to be fast, not that it really matters that much since IO > CPU for a website IMO. Which any async framework addresses this. I haven't heard many people using it though despite it having a lot of batteries included, but not dominating everything with first party support. Anyone using it in production in 2024? Edit: Fact check it appers it isn't written in Rust, I probably confused it with something else or it might just be data serialization"},
{"Title": "Demystifying list comprehensions in Python", "Author": "u/lyubolp", "Content": "In this article, I explain list comprehensions, as this is something people new to Python struggle with. Demystifying list comprehensions in Python"},
{"Title": "Using Polars to Write Multiple LazyFrames to Delta from One Source File", "Author": "u/EngiNerd9000", "Content": "I'm working on an ETL job that reads in a JSONL file, splits it into multiple Polars LazyFrames, collects those to DataFrames, and then writes them to a Delta Table. Prior to doing this with Polars, I would generally complete a task like this with either Pandas or PySpark depending on the size of data I was dealing with. I'm looking for any suggestions or advice the community has on doing this as efficiently as possible using Polars."},
{"Title": "ChatGPT / Python", "Author": "u/meisghost", "Content": "A lot of people are starting to rely on ChatGPT to write code for them from time to time. What I have personally noticed is ChatGPT writes modules so heavy its ridiculous. Last night I was working on an issue a small but technical issue, and asked it for help, it spat out, I kid you not 200+ lines of code and I just thought to myself this cannot be right, it was so heavy I was shaking my head. After some trial and error I got a very elegant piece done in 15 lines of code. Way faster, cleaner and yes reusable. So, it's nice that you can ask CGPT a question from time to time but to rely on it for code structure at this point, in my opinion, no good. Nice to be here!"},
{"Title": "privates.py - Stop others from touching your privates!", "Author": "u/ZeroIntensity", "Content": "Quick example: from privates import private @private class Hello: __readonly__ = \"bar\",  # No need for @property! __protected__ = \"foo\", def __init__(self): self.bar = \"hello world!\" self.foo = \"foo\" # Hello is now only usable from this module Repo: https://github.com/ZeroIntensity/privates.py"},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Using FastAPI REST along with RabbitMQ", "Author": "u/Accembler", "Content": "I have searched for a workable solution for the marriage of FastAPI async REST with RabbitMQ async interop in one application. Here is a fully workable code I came up with (intentionally simplified). class MessageBrokerManager: async def consume(self, loop): try: pika.BlockingConnection( parameters=pika.ConnectionParameters( host=self._host, port=self._port, credentials=pika.PlainCredentials( username=self._username, password=self._password, ), ) ) except Exception as e: log_info_fail(\"Failed to connect to RabbitMQ\") # return empty task return asyncio.create_task(asyncio.sleep(0)) self._connection = await connect_robust( host=self._host, port=self._port, login=self._username, password=self._password, loop=loop, ) self._channel = await self._connection.channel() queue = await self._channel.declare_queue(self._queue_name, durable=True) await queue.consume(callback=MessageBrokerManager.on_message, no_ack=True) return self._connection @asynccontextmanager async def lifespan(app: FastAPI): try: loop = asyncio.get_running_loop() task = loop.create_task(MessageBrokerManager().consume(loop)) await task except Exception as e: pass yield app = FastAPI(lifespan=lifespan) app.include_router(some_rest_router) Dependencies: pika>=1.3.2 aio-pika>=9.4.1 fastapi==0.109.2"},
{"Title": "constable - automatically inject print statements into your functions for debugging variables", "Author": "u/saurabh0719", "Content": "What My Project Does constable automatically injects print statements, during runtime, into your function code to give you a live step by step replay of each variable assignment. Github repo - https://github.com/saurabh0719/constable Target Audience Can be used in testing/debugging environments. Do not leave this hanging around in production! Comparison Use pdb for all purposes that matter. Or an object state tracker. Example - import constable @constable.trace('a', 'b') def example(a, b): a = a + b c = a a = \"Experimenting with the AST\" b = c + b a = c + b return a example(5, 6) Output - constable: example: line 5 a = a + b a = 11 type(a) = <class 'int'> constable: example: line 7 a = \"Experimenting with the AST\" a = Experimenting with the AST type(a) = <class 'str'> constable: example: line 8 b = c + b b = 17 type(b) = <class 'int'> constable: example: line 9 a = c + b a = 28 type(a) = <class 'int'> constable: example: line 3 to 10 args: (5, 6) kwargs: {} returned: 28 execution time: 0.00018480 seconds"},
{"Title": "Segmentation pipeline using MONAI and Pytorch", "Author": "u/zacky2004", "Content": "For my machine learning project, I created an 'end to end' segmentation pipeline using MONAI and Pytorch for the deep learning portion, and the Optuna hyper parameter optimization library for hyperparam optimization / search. The entire project is fully encapsulated/packaged using Poetry, so its really easy to install and use. The pipeline is fully customizable in terms of which models to use, model parameters, optimizers, as well as hyperparameters via configuration files. It also takes advantage of automatic mixed-precision for accelerated compute. I also have it integrated with Aimstack for experiment tracking and visualization. What My Project Does Segmentation pipeline used for creating segmentation masks of diagnostic medical images in Nifti format. Streamlines the process of training, inference and hyperparameter search using Optuna and Aimstack integration. Modularized configuration files allow for specific use-case modification. Target Audience Primarily meant for medical image and computer vision research & teaching. Not meant for clinical use. Comparison Use a single pipeline for training, inference and visualization, quickly prototype models, and even benchmark HPC GPU performance. Overall, this was an amazing learning experience for me. I'm new to the world of ML, and I learned a lot while developing this. I've been running this on an HPC cluster with both A100 and H100 GPUs. What do you guy's think? https://github.com/adnan-umich/monai-train"},
{"Title": "Pandas - value replacement or interpolation", "Author": "u/DeskAdministrative42", "Content": "Hi all, here trying to replace values for a range of dates in a data frame copy I've made where DateTime is the index and the values I need to mass replace are in the 'Value' column. The reason why is I need to interpolate between two dates in a time series, where the data I have is incorrect due to measurement error.  I need to replace it with interpolated data to make a more representative data set for forecasting Can anyone advise?"},
{"Title": "Topic ideas for talk", "Author": "u/jackh53453", "Content": "I‚Äôm thinking about giving a talk at my Python group but struggling to think of an interesting topic. Any suggestions or recommendations of ways of brainstorming? I like running so was thinking about a talk about using the Strava API, maybe get the top running shoes, but not sure if that‚Äôd interest other people. Topic ideas please!"},
{"Title": "resvg-py, safe bindings for resvg project", "Author": "u/BasePlate_Admin", "Content": "Hi everyone, Github Documentation What my project does : For a long time, i had a problem of rendering svg to png format. Specially after my project required opengraph image generation. Vercel's OG supports this functionality but that's JavaScript. So therefore i created this bindings to resvg library (same library used by vercel og) Targer Audience Developers Usage Install it like this: pip install resvg_py Then use it like this: import resvg_py svg_string = \"\"\" <svg width=\"300\" height=\"130\" xmlns=\"http://www.w3.org/2000/svg\"> <rect width=\"200\" height=\"100\" x=\"10\" y=\"10\" rx=\"20\" ry=\"20\" fill=\"blue\" /> </svg> \"\"\" print(resvg_py.svg_to_base64(svg_string)) Goals: To enable all the features available in resvg but don‚Äôt write to the disk, everything must be done in memory. Use the bare minimum amount of packages, in both python side and rust side Make the package as user friendly as possible Comparison I don't think theres any project that has safe bindings for resvg Please do note that this is my first time writing a package in rust."},
{"Title": "pydantic + dataloader = pydantic-resolve", "Author": "u/TurbulentAd8020", "Content": "https://github.com/allmonday/pydantic-resolve supports both pydantic v1 and v2. What My Project Does : a declaractive, schema based way for fetching and reorgnizing and changing data, from simple to complicated. Target Audience : production, web development Comparison : graphql frameworks. much simple, define specific schemas at backend. and can adjust data in backward stage of traversal. Introduction : I've used FastAPI for around two years, and like the pydantic as well. The idea of generating openapi.json from pydantic (response_model) is facinating, it help frontend generate clients based on it and simpilify the integration. I also use strawberry with FastAPI in some scenario, and enjoy the benefits from dataloaders. with pydantic you can define nested data structures but usually we need to construct the fields manually, or with the help of ORM relationship. So one day it comes with an idea, what if we put pydantic and aiodataloader together? pydantic + aiodataloader ? starts from some root data, and then let resolve / dataloader fetching the descendants? apis from restful service can provide root data, or single field query dataloader from gql can provide batch query for children and descendants. class MySite(BaseModel): name: str blogs: list[Blog] = [] async def resolve_blogs(self): return await get_blogs() comment_count: int = 0 def post_comment_count(self): # >> it will wait until all blogs are resolved return sum([b.comment_count for b in self.blogs]) class Blog(BaseModel): id: int title: str comments: list[Comment] = [] async def resolve_comments(self): return await query_comments(self.id) comment_count: int = 0 def post_comment_count(self): return len(self.comments) class Comment(BaseModel): id: int content: str looks pretty like graphql but absolutely in pydantic. executing is also very simple: async def main(): my_blog_site = MySite(name: \"tangkikodo's blog\") my_blog_site = await Resolver().resolve(my_blog_site) print(my_blog_site.json(indent=2)) using resolve and contexts related params can handle 90% features in graphql, it can also handle self-referencing data, like calculating the full path of each nodes of a tree. class Tree(BaseModel): name: str children: List[Tree] = [] path: str = '' def resolve_path(self, parent): if parent is not None: return f'{parent.path}/{self.name}' return self.name The most interesting part is post methods. the shortage of graphql or orm relationship is that we can only read the data by the structure they provided. for the fetched nested result, there is lack of ability to modify it in scope of each node. so it's always difficult to transform it. in daily frontend requirements, we need to merge, pick, flat, transform all kinds of nested data from backend, which means quite a lot of works. with post method, this become very simple. take tree for example, we can declarativly calculate the sum of descendants of each node. class Tree(BaseModel): count: int children: List[Tree] = [] total: int = 0 def post_total(self): return self.count + sum([c.total for c in self.children]) take blog site for example, post_comments can calculate the comment count of each blog. with collector https://allmonday.github.io/pydantic-resolve/reference_api/#collectors , post field can collect data from it's deep descendants. this provide a huge flexibility for reorganizing the data structure. This project is already in production environment, and has been tested for 1 year. hope to be helpful for you and welcome your suggestions!"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Example Data Pipeline with Prefect, Delta Lake, and Dask", "Author": "u/dask-jeeves", "Content": "I‚Äôm an OSS developer (primarily working on Dask) and lately I‚Äôve been talking to users about how they‚Äôre using Dask for ETL-style production workflows and this inspired me to make something myself. I wanted a simple example that met the following criteria: Run locally (optionally) . Should be easy to try out locally and easily scalable. Scalable to cloud . I didn‚Äôt want to think hard about cloud deployment. Python forward . I wanted to use tools familiar to Python users, not an ETL expert. The resulting data pipeline uses Prefect for workflow orchestration, Dask to scale the data processing across a cluster, Delta Lake for storage, and Coiled to deploy Dask on the cloud. I really like the outcome, but wanted to get more balanced feedback since lately I‚Äôve been more on the side of building these tools rather than using them heavily for data engineering. Some questions I‚Äôve had include: Prefect vs. Airflow vs. Dagster? For the users I‚Äôve been working with at Coiled, Prefect is the most commonly used tool. I also know Dagster is quite popular and could easily be swapped into this example. DeltaLake or something else? To be honest I mostly see vanilla Parquet in the wild, but I‚Äôve been curious about Delta for a while and mostly wanted an excuse to try it out (pandas and Dask support improved a lot with delta-rs). Anyway, if people have a chance to read things over and give feedback I‚Äôd welcome constructive critique. Code: https://github.com/coiled/etl-tpch Blog post: https://docs.coiled.io/blog/easy-scalable-production-etl.html"},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "JSON to Python - VSCode extension", "Author": "u/MichalMietus", "Content": "Hi, I just published my mini extension to VSCode with a command to convert a JSON object file to Python, something I often missed. I hope you will find it useful! https://marketplace.visualstudio.com/items?itemName=BringFuture.json-to-python What My Project Does: Converts JSON to Python Target Audience: VSCode coders Comparison: Formatters, etc. I could not find a tool to do exactly that"},
{"Title": "Python 3.12.3 Released", "Author": "u/chinawcswing", "Content": "https://www.python.org/downloads/release/python-3123/ 3.12.3 is the latest maintenance release, containing more than 300 bugfixes, build improvements and documentation changes since 3.12.2."},
{"Title": "Awesome Python Library: Tenacity", "Author": "u/pysk00l", "Content": "When writing code or tests in Python, one issue I had was when the code would fail due to random things like network issues or external peripherals not responding in time. So I had to add extra code to retry the code, but this added unnecessary complexity. Thats when I discovered the Tenacity library and it saved me hours and a lot of useless boilerplate code. Link: https://tenacity.readthedocs.io/en/latest/ I wrote a blog on it with some examples: https://new.pythonforengineers.com/blog/awesome-python-library-tenacity/"},
{"Title": "Build and Publish Your Python Project", "Author": "u/Shay-Hill", "Content": "In another article, I outlined how to handle Python imports and dependencies in Poetry. Poetry has a lot to recommend it: writes a pyproject.toml file for you creates an environment for you pins top-level dependencies in pyproject.toml (nice if that‚Äôs what you want) automates a lot of adding and removing dependencies all of the documentation is in one place But there are a few minor cons: adds minor complexity to ci adds minor complexity to tox some tools have a hard time finding your environment no PEP 621 compliance anthonywritescode doesn‚Äôt use it, and he knows more than most of the rest of us combined. So, here is an alternative (with its own minor pros and cons). I intend this as a simple reference you can paste commands out of. It will get you through the usual workflow of creating a Python library and hosting it on PyPI. https://shayallenhill.com/setuptools"},
{"Title": "\"Gracefully\" Implementing Graceful Shutdowns - FastApi | Kubernetes | WebSockets | Black | Ruff Show", "Author": "u/Jainal09", "Content": "Overview When deploying a new version of an application, it is essential to ensure that no tasks are lost during the deployment process. This is especially important for applications that use WebSockets to maintain real-time connections with clients. In such cases, abruptly terminating the application can lead to data loss and client disconnections. What My Project Does? This project demonstrates how to implement graceful shutdowns using FastAPI and Kubernetes, that use WebSockets client connections and internal background queues. https://github.com/jainal09/fastapi-gracefulshutdown-websockets Target Audience Production Use Cases"},
{"Title": "motsmeles - word search generator", "Author": "u/RRTheGuy", "Content": "https://github.com/RadoTheProgrammer/motsmeles/ Note: This is written in french. What my project does It generate word search games with custom words, dimensions. Target audience For people who want to get fun with some word searching Comparison This is a python library, it's open source, and you can use it in a python code, or with the command-line. Also, you have freedom if you want to have diagonal or reversed words. Next steps A GUI with pygame to generate and use it in a user-friendly interface."},
{"Title": "Heavy Stack - Top to Bottom Python, Dev Container, Hot Reloading, More", "Author": "u/JamesHutchisonReal", "Content": "Trying this again a third time because automod flagged it as a resource and the mods do not appear to be responding. https://github.com/heavy-resume/heavy-stack What it does: The Heavy Stack is a template web framework that is top to bottom Python using Sanic / (custom) ReactPy / Brython. It's a complete solution that just works out of the box. Target Audience: This is production capable but a little on the early side. Iterating is quite fast. Comparison: Compared to other web frameworks: Just works out of the box, even has a docker image ready to go for the server and your postgres database that uses PG Vector. ReactPy is server side, so instead of creating API calls, you just use `heavy_use_effect` and `heavy_event` to respond to things, do async loads, etc. It uses a websocket. If you're familiar with ReactPy already, the difference is that there's `heavy` versions that inject a database connection to the async functions and let you stuff in user context. The custom version of ReactPy that ships with this reconnects the websocket and attempts to restore the client state. It also supports recording and replaying your actions and uses locust to do load testing with those actions. Hot reloading means if you change a code file you see your changes about 2 seconds after saving without having to refresh the page. For tests it means the test runs near instantly after the initial load. Description: This is a dev container / GitHub Codespaces centric repo, so you'll be best served by using VS Code and either codespaces or something compatible the dev container standard. It's intended to be a template repo which you just run a script to rename to whatever project you want. Here are the features that work \"out of the box\" if you use the dev container or codespace: Pre-made dev container, docker files Top to Bottom Python (Sanic, custom ReactPy, Brython) PostgreSQL, PG Vector, CockroachDB SQL Model (SQLAlchemy + Pydantic) Hot reloading, both server and tests User action recording and playback for load testing Time tracking based on file changes Established patterns and examples, script to generate boilerplate around tables and their domain models There's some known issues and planned updates that will come at a later time, so for the sake of not overselling it, this should be treated like an alpha product. I'd love to see contributions though, especially when it comes to creating off-the-shelf components for ReactPy, or fixing the ReactPy bugs or getting that massive ReactPy PR mentioned in the readme piece-wise merged into the main project. Thanks for your time! I hope this is helpful for some people!"},
{"Title": "Behavior of AttributeError in @property and __getattr__", "Author": "u/Hatteras-", "Content": "I'm sure that many people have encountered the following (code for reproduction below) behavior:  if during the execution of `@property`, an `AttributeError` is raised and the same class implements `__getattr__` then `__getattr__` is invoked with the name of the property resulting in a confusing message: `AttributeError: 'Foo' object has no attribute 'something'`. If we remove `__getattr__` then we get a more meaningful and correct message: `AttributeError: 'Foo' object has no attribute 'bar'`. from dataclasses import dataclass @dataclass class Foo: val = 'foo_value' class Test: def __init__(self): self.foo = Foo() def __getattr__(self, name): return getattr(self.foo, name) @property def something(self): return self.foo.bar t = Test() t.something The error message is misleading and the behavior catches many people (myself including) in surprise and results in many wasted hours of debugging. I've seen people recommend using a custom decorator (instead of `@property` that catches attribute error) or wrapping your `@property` bodies in `try/except`. But it seems to me that this should be dealt with on language level. I'm trying to understand a few things: Is this behavior in Python intentional (i.e. is there a good reason) or just stems from the fact that `__getattr__` is invoked when `AttributeError` is raised when an instance attribute is accessed? If not intentional, is there a relatively easy way to detect `AttributeErrors` cause by `@property` execution in CPython? If so, why hasn't this been handled on language level? I've searched CPython issues on Github and I saw someone trying to correct documentation but couldn't find issues asking about this behavior or suggestions to fix - am I missing something?"},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Is Litestar production ready 2024?", "Author": "u/I_will_delete_myself", "Content": "At least a couple things that seems is that Litestar appears to be a fast  Django lite. Some batteries included, but not too many and opinionated. It's uses Rust so of course it's going to be fast, not that it really matters that much since IO > CPU for a website IMO. Which any async framework addresses this. I haven't heard many people using it though despite it having a lot of batteries included, but not dominating everything with first party support. Anyone using it in production in 2024? Edit: Fact check it appers it isn't written in Rust, I probably confused it with something else or it might just be data serialization"},
{"Title": "Demystifying list comprehensions in Python", "Author": "u/lyubolp", "Content": "In this article, I explain list comprehensions, as this is something people new to Python struggle with. Demystifying list comprehensions in Python"},
{"Title": "Using Polars to Write Multiple LazyFrames to Delta from One Source File", "Author": "u/EngiNerd9000", "Content": "I'm working on an ETL job that reads in a JSONL file, splits it into multiple Polars LazyFrames, collects those to DataFrames, and then writes them to a Delta Table. Prior to doing this with Polars, I would generally complete a task like this with either Pandas or PySpark depending on the size of data I was dealing with. I'm looking for any suggestions or advice the community has on doing this as efficiently as possible using Polars."},
{"Title": "ChatGPT / Python", "Author": "u/meisghost", "Content": "A lot of people are starting to rely on ChatGPT to write code for them from time to time. What I have personally noticed is ChatGPT writes modules so heavy its ridiculous. Last night I was working on an issue a small but technical issue, and asked it for help, it spat out, I kid you not 200+ lines of code and I just thought to myself this cannot be right, it was so heavy I was shaking my head. After some trial and error I got a very elegant piece done in 15 lines of code. Way faster, cleaner and yes reusable. So, it's nice that you can ask CGPT a question from time to time but to rely on it for code structure at this point, in my opinion, no good. Nice to be here!"},
{"Title": "privates.py - Stop others from touching your privates!", "Author": "u/ZeroIntensity", "Content": "Quick example: from privates import private @private class Hello: __readonly__ = \"bar\",  # No need for @property! __protected__ = \"foo\", def __init__(self): self.bar = \"hello world!\" self.foo = \"foo\" # Hello is now only usable from this module Repo: https://github.com/ZeroIntensity/privates.py"},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Using FastAPI REST along with RabbitMQ", "Author": "u/Accembler", "Content": "I have searched for a workable solution for the marriage of FastAPI async REST with RabbitMQ async interop in one application. Here is a fully workable code I came up with (intentionally simplified). class MessageBrokerManager: async def consume(self, loop): try: pika.BlockingConnection( parameters=pika.ConnectionParameters( host=self._host, port=self._port, credentials=pika.PlainCredentials( username=self._username, password=self._password, ), ) ) except Exception as e: log_info_fail(\"Failed to connect to RabbitMQ\") # return empty task return asyncio.create_task(asyncio.sleep(0)) self._connection = await connect_robust( host=self._host, port=self._port, login=self._username, password=self._password, loop=loop, ) self._channel = await self._connection.channel() queue = await self._channel.declare_queue(self._queue_name, durable=True) await queue.consume(callback=MessageBrokerManager.on_message, no_ack=True) return self._connection @asynccontextmanager async def lifespan(app: FastAPI): try: loop = asyncio.get_running_loop() task = loop.create_task(MessageBrokerManager().consume(loop)) await task except Exception as e: pass yield app = FastAPI(lifespan=lifespan) app.include_router(some_rest_router) Dependencies: pika>=1.3.2 aio-pika>=9.4.1 fastapi==0.109.2"},
{"Title": "constable - automatically inject print statements into your functions for debugging variables", "Author": "u/saurabh0719", "Content": "What My Project Does constable automatically injects print statements, during runtime, into your function code to give you a live step by step replay of each variable assignment. Github repo - https://github.com/saurabh0719/constable Target Audience Can be used in testing/debugging environments. Do not leave this hanging around in production! Comparison Use pdb for all purposes that matter. Or an object state tracker. Example - import constable @constable.trace('a', 'b') def example(a, b): a = a + b c = a a = \"Experimenting with the AST\" b = c + b a = c + b return a example(5, 6) Output - constable: example: line 5 a = a + b a = 11 type(a) = <class 'int'> constable: example: line 7 a = \"Experimenting with the AST\" a = Experimenting with the AST type(a) = <class 'str'> constable: example: line 8 b = c + b b = 17 type(b) = <class 'int'> constable: example: line 9 a = c + b a = 28 type(a) = <class 'int'> constable: example: line 3 to 10 args: (5, 6) kwargs: {} returned: 28 execution time: 0.00018480 seconds"},
{"Title": "Segmentation pipeline using MONAI and Pytorch", "Author": "u/zacky2004", "Content": "For my machine learning project, I created an 'end to end' segmentation pipeline using MONAI and Pytorch for the deep learning portion, and the Optuna hyper parameter optimization library for hyperparam optimization / search. The entire project is fully encapsulated/packaged using Poetry, so its really easy to install and use. The pipeline is fully customizable in terms of which models to use, model parameters, optimizers, as well as hyperparameters via configuration files. It also takes advantage of automatic mixed-precision for accelerated compute. I also have it integrated with Aimstack for experiment tracking and visualization. What My Project Does Segmentation pipeline used for creating segmentation masks of diagnostic medical images in Nifti format. Streamlines the process of training, inference and hyperparameter search using Optuna and Aimstack integration. Modularized configuration files allow for specific use-case modification. Target Audience Primarily meant for medical image and computer vision research & teaching. Not meant for clinical use. Comparison Use a single pipeline for training, inference and visualization, quickly prototype models, and even benchmark HPC GPU performance. Overall, this was an amazing learning experience for me. I'm new to the world of ML, and I learned a lot while developing this. I've been running this on an HPC cluster with both A100 and H100 GPUs. What do you guy's think? https://github.com/adnan-umich/monai-train"},
{"Title": "Pandas - value replacement or interpolation", "Author": "u/DeskAdministrative42", "Content": "Hi all, here trying to replace values for a range of dates in a data frame copy I've made where DateTime is the index and the values I need to mass replace are in the 'Value' column. The reason why is I need to interpolate between two dates in a time series, where the data I have is incorrect due to measurement error.  I need to replace it with interpolated data to make a more representative data set for forecasting Can anyone advise?"},
{"Title": "Topic ideas for talk", "Author": "u/jackh53453", "Content": "I‚Äôm thinking about giving a talk at my Python group but struggling to think of an interesting topic. Any suggestions or recommendations of ways of brainstorming? I like running so was thinking about a talk about using the Strava API, maybe get the top running shoes, but not sure if that‚Äôd interest other people. Topic ideas please!"},
{"Title": "resvg-py, safe bindings for resvg project", "Author": "u/BasePlate_Admin", "Content": "Hi everyone, Github Documentation What my project does : For a long time, i had a problem of rendering svg to png format. Specially after my project required opengraph image generation. Vercel's OG supports this functionality but that's JavaScript. So therefore i created this bindings to resvg library (same library used by vercel og) Targer Audience Developers Usage Install it like this: pip install resvg_py Then use it like this: import resvg_py svg_string = \"\"\" <svg width=\"300\" height=\"130\" xmlns=\"http://www.w3.org/2000/svg\"> <rect width=\"200\" height=\"100\" x=\"10\" y=\"10\" rx=\"20\" ry=\"20\" fill=\"blue\" /> </svg> \"\"\" print(resvg_py.svg_to_base64(svg_string)) Goals: To enable all the features available in resvg but don‚Äôt write to the disk, everything must be done in memory. Use the bare minimum amount of packages, in both python side and rust side Make the package as user friendly as possible Comparison I don't think theres any project that has safe bindings for resvg Please do note that this is my first time writing a package in rust."},
{"Title": "pydantic + dataloader = pydantic-resolve", "Author": "u/TurbulentAd8020", "Content": "https://github.com/allmonday/pydantic-resolve supports both pydantic v1 and v2. What My Project Does : a declaractive, schema based way for fetching and reorgnizing and changing data, from simple to complicated. Target Audience : production, web development Comparison : graphql frameworks. much simple, define specific schemas at backend. and can adjust data in backward stage of traversal. Introduction : I've used FastAPI for around two years, and like the pydantic as well. The idea of generating openapi.json from pydantic (response_model) is facinating, it help frontend generate clients based on it and simpilify the integration. I also use strawberry with FastAPI in some scenario, and enjoy the benefits from dataloaders. with pydantic you can define nested data structures but usually we need to construct the fields manually, or with the help of ORM relationship. So one day it comes with an idea, what if we put pydantic and aiodataloader together? pydantic + aiodataloader ? starts from some root data, and then let resolve / dataloader fetching the descendants? apis from restful service can provide root data, or single field query dataloader from gql can provide batch query for children and descendants. class MySite(BaseModel): name: str blogs: list[Blog] = [] async def resolve_blogs(self): return await get_blogs() comment_count: int = 0 def post_comment_count(self): # >> it will wait until all blogs are resolved return sum([b.comment_count for b in self.blogs]) class Blog(BaseModel): id: int title: str comments: list[Comment] = [] async def resolve_comments(self): return await query_comments(self.id) comment_count: int = 0 def post_comment_count(self): return len(self.comments) class Comment(BaseModel): id: int content: str looks pretty like graphql but absolutely in pydantic. executing is also very simple: async def main(): my_blog_site = MySite(name: \"tangkikodo's blog\") my_blog_site = await Resolver().resolve(my_blog_site) print(my_blog_site.json(indent=2)) using resolve and contexts related params can handle 90% features in graphql, it can also handle self-referencing data, like calculating the full path of each nodes of a tree. class Tree(BaseModel): name: str children: List[Tree] = [] path: str = '' def resolve_path(self, parent): if parent is not None: return f'{parent.path}/{self.name}' return self.name The most interesting part is post methods. the shortage of graphql or orm relationship is that we can only read the data by the structure they provided. for the fetched nested result, there is lack of ability to modify it in scope of each node. so it's always difficult to transform it. in daily frontend requirements, we need to merge, pick, flat, transform all kinds of nested data from backend, which means quite a lot of works. with post method, this become very simple. take tree for example, we can declarativly calculate the sum of descendants of each node. class Tree(BaseModel): count: int children: List[Tree] = [] total: int = 0 def post_total(self): return self.count + sum([c.total for c in self.children]) take blog site for example, post_comments can calculate the comment count of each blog. with collector https://allmonday.github.io/pydantic-resolve/reference_api/#collectors , post field can collect data from it's deep descendants. this provide a huge flexibility for reorganizing the data structure. This project is already in production environment, and has been tested for 1 year. hope to be helpful for you and welcome your suggestions!"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Example Data Pipeline with Prefect, Delta Lake, and Dask", "Author": "u/dask-jeeves", "Content": "I‚Äôm an OSS developer (primarily working on Dask) and lately I‚Äôve been talking to users about how they‚Äôre using Dask for ETL-style production workflows and this inspired me to make something myself. I wanted a simple example that met the following criteria: Run locally (optionally) . Should be easy to try out locally and easily scalable. Scalable to cloud . I didn‚Äôt want to think hard about cloud deployment. Python forward . I wanted to use tools familiar to Python users, not an ETL expert. The resulting data pipeline uses Prefect for workflow orchestration, Dask to scale the data processing across a cluster, Delta Lake for storage, and Coiled to deploy Dask on the cloud. I really like the outcome, but wanted to get more balanced feedback since lately I‚Äôve been more on the side of building these tools rather than using them heavily for data engineering. Some questions I‚Äôve had include: Prefect vs. Airflow vs. Dagster? For the users I‚Äôve been working with at Coiled, Prefect is the most commonly used tool. I also know Dagster is quite popular and could easily be swapped into this example. DeltaLake or something else? To be honest I mostly see vanilla Parquet in the wild, but I‚Äôve been curious about Delta for a while and mostly wanted an excuse to try it out (pandas and Dask support improved a lot with delta-rs). Anyway, if people have a chance to read things over and give feedback I‚Äôd welcome constructive critique. Code: https://github.com/coiled/etl-tpch Blog post: https://docs.coiled.io/blog/easy-scalable-production-etl.html"},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "JSON to Python - VSCode extension", "Author": "u/MichalMietus", "Content": "Hi, I just published my mini extension to VSCode with a command to convert a JSON object file to Python, something I often missed. I hope you will find it useful! https://marketplace.visualstudio.com/items?itemName=BringFuture.json-to-python What My Project Does: Converts JSON to Python Target Audience: VSCode coders Comparison: Formatters, etc. I could not find a tool to do exactly that"},
{"Title": "Python 3.12.3 Released", "Author": "u/chinawcswing", "Content": "https://www.python.org/downloads/release/python-3123/ 3.12.3 is the latest maintenance release, containing more than 300 bugfixes, build improvements and documentation changes since 3.12.2."},
{"Title": "Awesome Python Library: Tenacity", "Author": "u/pysk00l", "Content": "When writing code or tests in Python, one issue I had was when the code would fail due to random things like network issues or external peripherals not responding in time. So I had to add extra code to retry the code, but this added unnecessary complexity. Thats when I discovered the Tenacity library and it saved me hours and a lot of useless boilerplate code. Link: https://tenacity.readthedocs.io/en/latest/ I wrote a blog on it with some examples: https://new.pythonforengineers.com/blog/awesome-python-library-tenacity/"},
{"Title": "Build and Publish Your Python Project", "Author": "u/Shay-Hill", "Content": "In another article, I outlined how to handle Python imports and dependencies in Poetry. Poetry has a lot to recommend it: writes a pyproject.toml file for you creates an environment for you pins top-level dependencies in pyproject.toml (nice if that‚Äôs what you want) automates a lot of adding and removing dependencies all of the documentation is in one place But there are a few minor cons: adds minor complexity to ci adds minor complexity to tox some tools have a hard time finding your environment no PEP 621 compliance anthonywritescode doesn‚Äôt use it, and he knows more than most of the rest of us combined. So, here is an alternative (with its own minor pros and cons). I intend this as a simple reference you can paste commands out of. It will get you through the usual workflow of creating a Python library and hosting it on PyPI. https://shayallenhill.com/setuptools"},
{"Title": "\"Gracefully\" Implementing Graceful Shutdowns - FastApi | Kubernetes | WebSockets | Black | Ruff Show", "Author": "u/Jainal09", "Content": "Overview When deploying a new version of an application, it is essential to ensure that no tasks are lost during the deployment process. This is especially important for applications that use WebSockets to maintain real-time connections with clients. In such cases, abruptly terminating the application can lead to data loss and client disconnections. What My Project Does? This project demonstrates how to implement graceful shutdowns using FastAPI and Kubernetes, that use WebSockets client connections and internal background queues. https://github.com/jainal09/fastapi-gracefulshutdown-websockets Target Audience Production Use Cases"},
{"Title": "All Python conference talks from 2023 ordered by the number of views", "Author": "u/TechTalksWeekly", "Content": "Hello r/python üëã! Back in January, I've compiled a list of the most watched PyCon talks from 2023. I've received tons of positive feedback via DM, upvotes, and comments, so I decided to put together another compilation. This time around, I've gathered not only PyCon, but all Python 2023 talks across +100 conferences (here's the list ) that include PyCon (all locations), PyData (all locations), EuroPython , Conf42 , and many more to give you a complete overview of the landscape. The list is gigantic and includes over 850 talks **!** What's more, I've created a Google Sheets version of this post that gives more convenient sorting and filtering options. If it turns to be useful, don't forget to share the list with friends / colleagues! \"Tutorials - Mario Munoz: Web Development With A Python-backed Frontend: Featuring HTMX and Tailwind\" ‚∏± +12k views ‚∏± 02h 26m 00s \"Thomas Bierhance: Polars - make the switch to lightning-fast dataframes\" ‚∏± +11k views ‚∏± 00h 29m 49s \"Mariatta Wijaya: Welcome to PyCon US 2023\" ‚∏± +11k views ‚∏± 00h 29m 29s \"Scaling Python for Machine Learning: Beyond Data Parallelism ‚Ä¢ Holden Karau ‚Ä¢ GOTO 2023\" ‚∏± +10k views ‚∏± 00h 39m 19s \"Tutorials - Matt Harrison: Getting Started with Polars\" ‚∏± +10k views ‚∏± 02h 14m 28s \"Keynote Speaker - Ned Batchelder\" ‚∏± +7k views ‚∏± 00h 47m 02s \"G√°bor Sz√°rnyas - DuckDB: The Power of a Data Warehouse in your Python Process\" ‚∏± +7k views ‚∏± 00h 55m 27s \"Talks - Samuel Colvin: How Pydantic V2 leverages Rust's Superpowers\" ‚∏± +6k views ‚∏± 00h 45m 55s \"Delta-rs, Apache Arrow, Polars, WASM: Is Rust the Future of Analytics?\" ‚∏± +6k views ‚∏± 00h 31m 03s \"Tutorials - Simon Willison: Data analysis with SQLite and Python\" ‚∏± +5k views ‚∏± 02h 45m 54s \"Talks - Hynek Schlawack: Subclassing, Composition, Python, and You\" ‚∏± +5k views ‚∏± 00h 45m 42s \"Writing Python Bindings for C++ Libraries: Easy-to-use Performance - Saksham Sharma - CppCon 2023\" ‚∏± +5k views ‚∏± 01h 01m 30s \"Pedro Holanda - DuckDB: Bringing analytical SQL directly to your Python shell\" ‚∏± +5k views ‚∏± 00h 29m 31s \"Tutorials - Lisa Carpenter: How to create beautiful interactive GUIs and web apps\" ‚∏± +5k views ‚∏± 01h 58m 05s \"Tutorials - Reuven M. Lerner: Comprehending comprehensions\" ‚∏± +5k views ‚∏± 02h 17m 33s \"Keynote Speaker - Guido van Rossum\" ‚∏± +5k views ‚∏± 00h 30m 51s \"PyData Heidelberg #11 - TimeSeries Forecasting & LLM Langchain\" ‚∏± +4k views ‚∏± 02h 39m 06s \"Keynote Speaker - James Powell\" ‚∏± +4k views ‚∏± 00h 25m 43s \"Use Spark from anywhere: A Spark client in Python powered by Spark Connect\" ‚∏± +4k views ‚∏± 00h 56m 58s \"Cython 3 ‚Äì Python at the speed of C ‚Äî Stefan Behnel\" ‚∏± +4k views ‚∏± 00h 31m 15s \"Patrick Bl√∂baum: Performing Root Cause Analysis with DoWhy, a Causal Machine-Learning Library\" ‚∏± +4k views ‚∏± 00h 44m 38s \"Max Mergenthaler and Fede Garza - Quantifying Uncertainty in Time Series Forecasting\" ‚∏± +4k views ‚∏± 00h 37m 25s \"Talks - Bruce Eckel: Rethinking Objects\" ‚∏± +4k views ‚∏± 00h 32m 06s \"Rust for Python data engineers ‚Äî Karim Jedda\" ‚∏± +4k views ‚∏± 00h 27m 30s \"Matt Harrison - An Introduction to Pandas 2, Polars, and DuckDB | PyData Global 2023\" ‚∏± +4k views ‚∏± 01h 22m 15s \"Tutorials - Ted Patrick: Writing Serverless Python Web Apps with PyScript\" ‚∏± +3k views ‚∏± 02h 55m 29s \"Tutorials - Trey Hunner: Intro to Python for Brand New Programmers\" ‚∏± +3k views ‚∏± 02h 16m 41s \"Dr. Thomas Wiecki: Bayesian Marketing Science - Solving Marketing's 3 Biggest Problems\" ‚∏± +3k views ‚∏± 00h 30m 35s [**\"Ga√´l Varoquaux: Prepping Tables for Machine Learning Gets Easier PyData S√ºdwest]\"** ‚∏± +3k views ‚∏± 00h 57m 54s \"Talks - Reuven M. Lerner: Generators, coroutines and nanoservices\" ‚∏± +3k views ‚∏± 00h 26m 28s \"Talks - Brett Cannon: Python's syntactic sugar\" ‚∏± +3k views ‚∏± 00h 31m 11s \"Bruno Vollmer: BLE and Python - How to build a simple BLE project on Linux with Python\" ‚∏± +3k views ‚∏± 00h 29m 40s \"Vasileios Mourtakos - A data engineering framework in Python\" ‚∏± +3k views ‚∏± 00h 35m 55s \"Talks - Eric Snow: A Per-Interpreter GIL: Concurrency and Parallelism with Subinterpreters\" ‚∏± +3k views ‚∏± 00h 30m 29s \"Talks - ≈Åukasz Langa: Working Around the GIL with asyncio\" ‚∏± +2k views ‚∏± 00h 44m 26s \"Hajime Takeda - Media Mix Modeling:How to Measure the Effectiveness of Advertising\" ‚∏± +2k views ‚∏± 00h 30m 37s \"Tutorials - Mike M√ºller: The How and Why of Object-oriented Programming in Python\" ‚∏± +2k views ‚∏± 02h 45m 34s \"Shahriyar Rzayev: Building Hexagonal Python Services\" ‚∏± +2k views ‚∏± 01h 29m 50s \"Performance tips by the FastAPI Expert ‚Äî Marcelo Trylesinski\" ‚∏± +2k views ‚∏± 00h 24m 59s \"Solving Multi-Objective Constrained Optimisation Problems using Pymoo ‚Äî Pranjal Biyani\" ‚∏± +2k views ‚∏± 00h 44m 23s \"Carsten Binnig: Towards Learned Database Systems\" ‚∏± +2k views ‚∏± 00h 43m 34s \"Talks - Brandt Bucher: Inside CPython 3.11's new specializing, adaptive interpreter\" ‚∏± +2k views ‚∏± 00h 23m 01s \"Carl Kadie - A Perfect, Infinite-Precision, Game Physics in Python | PyData Seattle 2023\" ‚∏± +2k views ‚∏± 00h 36m 26s \"From crontab to celery with no regrets by Marco Pavanelli\" ‚∏± +2k views ‚∏± 00h 32m 03s \"Talks - Al Sweigart: An Overview of the Python Code Tool Landscape 2023\" ‚∏± +2k views ‚∏± 00h 26m 58s \"Reinventing Machine Learning with Transformers and Hugging Face by Keynote speaker Julien Simon\" ‚∏± +2k views ‚∏± 01h 08m 52s \"Empower your Spring Applications with Python Features on GraalVM by Johannes Link @ Spring I/O 2023\" ‚∏± +2k views ‚∏± 00h 53m 00s \"Marysia Winkels - Data Storytelling through Visualization\" ‚∏± +2k views ‚∏± 00h 30m 57s \"What polars does for you ‚Äî Ritchie Vink\" ‚∏± +2k views ‚∏± 00h 27m 45s \"Beyond Toy Datasets: Timeseries Forecasting for Real Business Problems - Robert Haase\" ‚∏± +2k views ‚∏± 00h 33m 55s \"PyData Online - An AI assistant for football analytics - Petar Veliƒçkoviƒá (Google DeepMind)\" ‚∏± +2k views ‚∏± 01h 02m 40s \"Talks - Mark Shannon: How we are making CPython faster. Past, present and future.\" ‚∏± +2k views ‚∏± 00h 29m 11s \"JIm Dowling - Build a production ML system with only Python on free serverless services\" ‚∏± +2k views ‚∏± 01h 22m 35s \"Writing a Python interpreter from scratch, in half an hour ‚Äî Tushar Sadhwani\" ‚∏± +2k views ‚∏± 00h 43m 38s \"Dominika Basaj & Barbara Rychalska - Creating behavioral profiles of your customer\" ‚∏± +2k views ‚∏± 00h 26m 55s \"Stephan Sahm: Accelerate Python with Julia\" ‚∏± +2k views ‚∏± 01h 27m 14s \"Why Delta Lake is the Best Storage Format for Pandas Analyses\" ‚∏± +2k views ‚∏± 00h 28m 25s \"Tutorials - Geir Arne Hjelle: Introduction to Decorators: Power Up Your Python Code\" ‚∏± +2k views ‚∏± 02h 21m 20s \"Jeroen Overschie - How to create a Devcontainer for your Python project üê≥\" ‚∏± +2k views ‚∏± 00h 36m 01s \"Wesley Boelrijk - Lowering the barrier for ML monitoring\" ‚∏± +2k views ‚∏± 00h 36m 53s \"Talks - Dan Craig: Testing Spacecraft with Pytest\" ‚∏± +2k views ‚∏± 00h 30m 12s \"Keynote Speaker - Margaret Mitchell\" ‚∏± +2k views ‚∏± 00h 42m 29s \"Talks - Moshe Zadka: pyproject.toml, packaging, and you\" ‚∏± +2k views ‚∏± 00h 30m 06s \"Tutorials - Juhi, Dana: Intro to Hugging Face: Fine-tuning BERT for NLP tasks\" ‚∏± +2k views ‚∏± 02h 09m 52s \"Tutorials - Patrick Arminio: Build a production ready GraphQL API using Python\" ‚∏± +2k views ‚∏± 02h 23m 14s \"Ryan Curtin - Lightweight, low-overhead, high-performance: machine learning directly in C++\" ‚∏± +2k views ‚∏± 00h 43m 10s \"Python with Spark Connect\" ‚∏± +2k views ‚∏± 00h 33m 27s \"David Qiu - Jupyter AI ‚Äî Bringing Generative AI to Jupyter | PyData Seattle 2023\" ‚∏± +1k views ‚∏± 00h 39m 29s \"Polars: A highly optimized dataframe library | Matt Harrison | Conf42 Machine Learning 2023\" ‚∏± +1k views ‚∏± 00h 20m 31s \"Nico Kreiling: Raised by Pandas, striving for more: An opinionated introduction to Polars\" ‚∏± +1k views ‚∏± 00h 29m 47s \"Jiang et al. - Automated Machine Learning & Tuning with FLAML | PyData Seattle 2023\" ‚∏± +1k views ‚∏± 01h 21m 54s \"Alexander CS Hendorf: 5 Things about fastAPI I wish we had known beforehand\" ‚∏± +1k views ‚∏± 00h 32m 31s \"Duarte Carmo - MLOps for the rest of us- A poor man's guide to putting models in production\" ‚∏± +1k views ‚∏± 00h 26m 04s [**\"Ines Montani (spaCy) - Large Language Models from Prototype to Production PyData S√ºdwest]\"** ‚∏± +1k views ‚∏± 00h 39m 55s \"Robin Raymond: Rusty Python - A Case Study\" ‚∏± +1k views ‚∏± 00h 28m 47s \"Tutorials - Leah Berg, Ray: Feature Engineering is for Everyone!\" ‚∏± +1k views ‚∏± 02h 16m 35s \"Giles Weaver & Ian Ozsvald - Pandas 2, Dask or Polars? Tackling larger data on a single machine\" ‚∏± +1k views ‚∏± 00h 16m 22s \"Talks - Glyph: How To Keep A Secret\" ‚∏± +1k views ‚∏± 00h 26m 19s \"Maximilian M. - SHAPtivating Insights: unravelling blackbox AI models\" ‚∏± +1k views ‚∏± 00h 35m 02s \"Vahan Huroyan - Recent Developments in Self-Supervised Learning for Computer Vision\" ‚∏± +1k views ‚∏± 00h 36m 25s \"Lucas Durand - Building an Interactive Network Graph to Understand Communities | PyData Seattle 2023\" ‚∏± +1k views ‚∏± 01h 26m 14s \"Big PyData BBQ #5: LLMs feat. Ines Montani (spaCy) & Alejandro Saucedo (Zalando)\" ‚∏± +1k views ‚∏± 03h 48m 21s \"Stop using print! Understanding and using the \"logging\" module ‚Äî Reuven M. Lerner\" ‚∏± +1k views ‚∏± 00h 29m 32s \"Joris Van den Bossche & Patrick Hoefler: Pandas 2.0 and beyond\" ‚∏± +1k views ‚∏± 00h 37m 07s \"Structured output with large language models / Uri Goren (Argmax)\" ‚∏± +1k views ‚∏± 00h 25m 09s \"PyData Chicago April 2023 Meetup | Design and Analysis of (Backtest) Experiments with R and Python\" ‚∏± +1k views ‚∏± 00h 50m 19s \"Sponsor Presentation - Python & Bloomberg: An Open Source Duo\" ‚∏± +1k views ‚∏± 00h 57m 08s \"Soumith Chintala - Keynote: AI & the stuff built for AI - are they actually useful for data science?\" ‚∏± +1k views ‚∏± 00h 31m 54s \"The Future of Microprocessors ‚Äî Sophie Wilson\" ‚∏± +1k views ‚∏± 00h 56m 45s \"Jia Yu - How Apache Sedona is Revolutionizing Geospatial Data Analysis | PyData Seattle 2023\" ‚∏± +1k views ‚∏± 00h 46m 12s \"Lightning Talks - April 21, 5pm\" ‚∏± +1k views ‚∏± 01h 04m 16s \"How to Leverage the Full Potential of LLMs for Your Business with Langchain - Leon Ruddat\" ‚∏± +1k views ‚∏± 00h 34m 40s \"Jay Chia & Sammy Sidhu: Daft - The Distributed Python Dataframe for Complex Data\" ‚∏± +1k views ‚∏± 00h 23m 01s \"Talks - A. Jesse Jiryu Davis: Consistency and isolation for Python programmers\" ‚∏± +1k views ‚∏± 00h 24m 23s \"\"Python deployment with Docker and Poetry\" - Cristian Heredia (PyBay 2023)\" ‚∏± +1k views ‚∏± 00h 12m 24s \"Peterson & Qin - Contextual Multi-Arm Bandit and its applications to digital experiments | PyData\" ‚∏± +1k views ‚∏± 00h 44m 54s \"Tutorials - Ron Nathaniel: How To Troubleshoot and Monitor Applications using OpenTelemetry\" ‚∏± +1k views ‚∏± 02h 01m 57s \"Keynote Speaker - Python Steering Council\" ‚∏± +1k views ‚∏± 00h 20m 42s \"An unbiased evaluation of environment management and packaging tools ‚Äî Anna-Lena Popkes\" ‚∏± +1k views ‚∏± 00h 43m 14s \"Vincent Gosselin - Turning your Data/AI algorithms into full web apps in no time with Taipy\" ‚∏± +1k views ‚∏± 00h 38m 18s \"Malte Tichy - Knowing what you don‚Äôt know matters- Uncertainty-aware model rating\" ‚∏± +1k views ‚∏± 00h 30m 05s \"Talks - Andrew Godwin: Reconciling Everything\" ‚∏± +1k views ‚∏± 00h 30m 16s \"Vincent Warmerdam - Bulk Labelling Techniques\" ‚∏± +1k views ‚∏± 00h 32m 55s \"Hugo Bowne-Anderson - Full-stack Machine Learning and Generative AI for Data Scientists\" ‚∏± +1k views ‚∏± 01h 28m 30s \"J.J. Allaire - Keynote: Dashboards with Jupyter and Quarto | PyData NYC 2023\" ‚∏± +1k views ‚∏± 00h 40m 15s \"Alejandro Saucedo - Industrial Strength DALLE-E:Scaling Complex Large Text & Image Models\" ‚∏± +1k views ‚∏± 00h 28m 07s \"Subclassing, Composition, Python, and You ‚Äî Hynek Schlawack\" ‚∏± +1k views ‚∏± 00h 44m 59s \"Tutorials -Zac Hatfield-Dodds, Ryan Soklaski: Introduction to Property-Based Testing\" ‚∏± +1k views ‚∏± 01h 31m 20s [**\"Furkan M. Torun - Become a Data Storyteller with Streamlit! PyData Prague 2023-11-20]\"** ‚∏± +1k views ‚∏± 00h 23m 26s \"Anna-Lena Popkes: An unbiased evaluation of environment management and packaging tools\" ‚∏± +1k views ‚∏± 00h 43m 53s \"PyData Chicago March 2023 Meetup | Monte Carlo with QMCPy for Vector Functions of Integrals\" ‚∏± +1k views ‚∏± 00h 46m 35s \"James Powell - Simple Simulators with pandas and Generator Coroutines | PyData NYC 2023\" ‚∏± +1k views ‚∏± 01h 21m 43s \"Polars is the Pandas killer / Igor Mintz (Viz.ai)\" ‚∏± +1k views ‚∏± 00h 21m 46s \"Diving into Event-Driven Architectures with Python ‚Äî Marc-Andr√© Lemburg\" ‚∏± +1k views ‚∏± 00h 30m 59s \"Using Embeddings and Deep Neural Networks as a technique for AutoML Demand Forecasting PyData SW\" ‚∏± +1k views ‚∏± 00h 36m 06s \"Leland McInnes - Data Mapping for Data Exploration | PyData Seattle 2023\" ‚∏± +1k views ‚∏± 00h 38m 16s \"Eduardo Blancas - Using embedded SQL engines for plotting massive datasets on a laptop\" ‚∏± +1k views ‚∏± 00h 27m 00s \"Large Language Models: From Prototype to Production ‚Äî Ines Montani\" ‚∏± +1k views ‚∏± 00h 40m 20s \"Tutorials - Pavithra Eswaramoorthy, Dharhas Pothina: Data of Unusual Size: Interactive Visualization\" ‚∏± +1k views ‚∏± 03h 06m 45s \"Moritz Meister - Data Validation for Feature pipelines: Using Great Expectations and Hopsworks\" ‚∏± +1k views ‚∏± 00h 27m 40s \"Inge van den Ende-Leveraging conformal prediction for calibrated probabilistic time series forecasts\" ‚∏± +1k views ‚∏± 00h 31m 33s \"Nick Sorros - A Tour of Large Language Models\" ‚∏± +1k views ‚∏± 00h 46m 38s \"Thomas Frauholz: From notebook to pipeline in no time with LineaPy\" ‚∏± +1k views ‚∏± 00h 43m 17s \"EuroPython 2023 Opening Session\" ‚∏± +1k views ‚∏± 00h 20m 58s \"Optimizing Ad Conversions with DS / Yael Kiselman (DigitalTurbine)\" ‚∏± +1k views ‚∏± 00h 51m 44s \"Harizo Rajaona - A Tour of the Many DataFrame Frameworks\" ‚∏± +1k views ‚∏± 00h 34m 59s \"Andrei Alekseev - Why does everyone need to develop a machine learning package?\" ‚∏± +1k views ‚∏± 00h 28m 07s \"Ties de Kok - Going beyond ChatGPT: introduction to prompt engineering & LLMs | PyData Seattle 2023\" ‚∏± +1k views ‚∏± 00h 59m 56s \"PyData Boston Sept session 1: Mike Woodward - Data sci done wrong: how/why scientists make mistakes.\" ‚∏± +1k views ‚∏± 00h 27m 14s \"Joe Cheng - Shiny: Data-centric web applications in Python | PyData Seattle 2023\" ‚∏± +1k views ‚∏± 00h 43m 56s \"Tutorials - Cheuk Ting Ho: Power up your work with compiling and profiling\" ‚∏± +1k views ‚∏± 01h 33m 02s \"Talks -Algorithmic ideas, engineering tricks, and trivia behind CPython's new sorting algorithm\" ‚∏± +1k views ‚∏± 00h 29m 39s \"Talks - Dawn Wages: Supercharge your Python Development Environment with VS Code + Dev Container\" ‚∏± +1k views ‚∏± 00h 31m 01s \"Egor Romanov - Performance of Vector Databases\" ‚∏± +1k views ‚∏± 00h 43m 09s \"Alexander CS Hendorf - Ten Years of Community Organizer | PyData NYC 2023\" ‚∏± +1k views ‚∏± 00h 42m 41s \"Talks - Nicholas H.Tollervey, Paul Everitt: Build Yourself a PyScript\" ‚∏± +1k views ‚∏± 00h 39m 28s \"Emil Rijcken - FuzzyTM: a Python package for fuzzy topic models\" ‚∏± +1k views ‚∏± 00h 28m 12s \"Nikolas Markou - Artificial Intelligence for Vision: A walkthrough of recent breakthroughs\" ‚∏± +1k views ‚∏± 00h 42m 10s \"Would Rust make you a better Pythonista? ‚Äî Alexys Jacob\" ‚∏± +1k views ‚∏± 00h 47m 23s \"Simon Pressler: Getting started with JAX\" ‚∏± +1k views ‚∏± 00h 29m 49s \"Sponsor Presentation - How to build stunning Data Science Web applications in Python\" ‚∏± +1k views ‚∏± 00h 54m 10s \"Pavel Pleskov - All about ML competitions!\" ‚∏± +1k views ‚∏± 01h 03m 13s \"Laszlo Sragner - Code Smells in Data Science: What can we do about them? | PyData London 2023\" ‚∏± +1k views ‚∏± 00h 39m 19s \"Talks - Jodie Burchell: Vectorize using linear algebra and NumPy to make your Python code fast\" ‚∏± +1k views ‚∏± 00h 28m 32s \"Comparing Elixir and Python when working with Simple Neural Networks - A. Neto & L. C. Tavano\" ‚∏± +1k views ‚∏± 00h 38m 11s \"Talks - Valerio Maggio: Pythonic functional ( iter )tools for your data challenges\" ‚∏± +1k views ‚∏± 00h 32m 38s \"Topaz Gilad - Classification Through Regression:Unlock the True Potential of Your Labels\" ‚∏± +1k views ‚∏± 00h 28m 13s \"Running Python packages in the browser with Pyodide ‚Äî Roman Yurchak\" ‚∏± +1k views ‚∏± 00h 27m 49s \"J.J. Allaire - Publishing Jupyter Notebooks with Quarto | PyData Seattle 2023\" ‚∏± +1k views ‚∏± 00h 40m 47s \"ipyvizzu-story- a open-source tool to build create+share animated data stories w/ python in jupyter\" ‚∏± +1k views ‚∏± 01h 25m 54s Due to Reddit post length limit, see the remaining talks in this post or google sheet ."},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Python 3.12.3 - The third maintenance release of Python 3.12", "Author": "u/Neustradamus", "Content": "https://www.python.org/downloads/release/python-3123/"},
{"Title": "Adnexus DI/IoC Framework", "Author": "u/Nictec", "Content": "Repo: https://github.com/Nictec/Adnexus What My Project Does: Adnexus is a declarative and fully typed DI/IoC framework for python. It is inspired by python-dependency-injector because this project is no longer maintained. Target Audience: Currently Adnexus is in heavy development. So expect heavy errors and breaking changes. Eventually if we reach 1.0.0 the project will be ready for production. Comparison: The main difference to existing (and maintained) projects is the \"declarative\" part of the framework. Injectables are declared direclty in the container class using Providers. This takes away the \"magic\" of IoC because it is clear where your dependencies come from. Furthermore all dependencies can be directly configured using the integrated config system (for details see the repo) Please try it and give me Feedback! Contributions are always welcome just open a pull request or an issue."},
{"Title": "PGCacheWatch - Supercharge Your Caching Strategy", "Author": "u/GabelSnabel", "Content": "Hey Python enthusiasts! üëã I'm thrilled to introduce PGCacheWatch , my recent project designed to enhance your PostgreSQL databases by bringing real-time event notifications right into your applications. Vastly improved cache invalidation efficiency, without the need for adding any new services or infrastructure. What My Project Does PGCacheWatch capitalizes on PostgreSQL's built-in NOTIFY/LISTEN features to deliver instant, real-time notifications about database events, ensuring your application's cache remains fresh and synchronized with your database. Target Audience This project is tailored for developers and architects striving to maintain high-performance and scalable Python applications with real-time data demands. Whether you're developing web applications using FastAPI, orchestrating data processing pipelines, or any application dependent on PostgreSQL for data storage, PGCacheWatch simplifies cache invalidation management. Comparison PGCacheWatch differentiates itself from other caching and notification mechanisms by: Zero Additional Infrastructure: Utilizing PostgreSQL's existing functionalities, it circumvents the necessity for external tools or services. Key Features Instant Database Change Notifications: Ensures your application is immediately informed about database events as they occur, keeping data consistently updated. Adaptable Cache Invalidation Strategies: Offers a range of strategies to precisely control the cache invalidation process, striking a balance between performance and data fidelity. Optimized for Async Python: Conceived for the asynchronous Python ecosystem, making it a superb fit for applications utilizing asyncpg and FastAPI. Join Me on GitHub I'm keen to see how PGCacheWatch can bolster your projects. Your feedback and contributions are greatly appreciated as I aspire to continually enhance and broaden the library's functionality. Explore PGCacheWatch on GitHub"},
{"Title": "Advanced Macro & Calorie Calculator", "Author": "u/MCTrovato", "Content": "What My Project Does A macro-nutrient and calorie calculator to help me achieve body composition goals. This calculator is compatible with metric and imperial units. It supports weight changes based on percentage and fixed values (lbs, kg). This is useful when losing weight in a fat loss phase as a percentage, and for gaining at a fixed value during a muscle growth phase. The recommendations are built using scientific research based on lean body mass. If the user's body fat percentage is unknown, then it will be calculated using BMI and the Deurenberg formula. General guidance is also available on the project page . Key Features: Compatible with metric and imperial units Weight change based on percentage or fixed values Recommendations based on scientific research Body Fat Percentage Calculator Choose where to place remaining calories (after minimum fat & protein targets reached) Safety Limits (Max Change & Caloric Intake Below BMR) Target Audience This calculator is ready for production. Ideally, any end user should be able to use this program, not just developers. I hope that this calculator can help the general public. Comparison There are many other macro calculators available. However, I feel that this one is a step beyond other calculators. This calculator will always make sure that the end user reaches their minimum protein and fat amounts. It also also allows the user to select where to place any remaining calories. Some may prefer to place remaining calories in carbs, while others may wish to stay in ketosis and place remaining calories in fat, or even a mix of all macro-nutrients. It also has built in safety mechanism to keep calories and macros adequate for health status. Project Page: Advanced Macro Calculator on GitHub"},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Using Jupyter Notebooks with Flox", "Author": "u/z_mitchell", "Content": "https://flox.dev/blog/jupyter-remote-env Hey everyone, we released Flox 1.0 a few weeks ago and one of the cool things you can do is set up environments with a bunch of dependencies already set up for running a Jupyter notebook. Take a look to see how it all works!"},
{"Title": "automathon: A Python library for simulating and visualizing finite automata", "Author": "u/fexx3l", "Content": "What my project does automathon is a Python library for simulating and visualizing finite automata, is easy to use and have a docs website to read more about the functions that are implemented for DFAs and NFAs. During my CS degree, I learned about automata and this became one of my favorite topics, that's why I started to work on this project, I wanted to create something simple and easy to use. Target audience Students, professors and any developer that could need to use automata. Comparison I don't know if there's any python library that let you simulate finite automata, but I'm sure you will love this one, there's a blog talking about automata and have examples using automathon, you can read it, it's written in French."},
{"Title": "pwdgen - a simple password generator", "Author": "u/RRTheGuy", "Content": "What my project does My project generate simple, strong, memorable and easy-to-type passwords. Target audience For anyone who need to get passwords easily. Comparison Most passwords manager generate completely passwords with completely random characters that aren't very easy to memorize or tape. Examples include Dashlane , Norton , Avast . Or other like Bitwarden generate passwords that are not really fast-to-type. The mine generate sth like 6Nixe#Becokace 0Qubyby+Pomafy , or 7Zuxogu:Lebuwo . I wanted to make a password generator that combines simplicity, security, memorability, and ease of type. Usage You can install it with pip install pwd-generator and use the cli version: pwdgen To use it in a python code import pwdgen print(pwdgen.generate()) Source code The source code is on github"},
{"Title": "Paid libraries may kill Python", "Author": "u/Glxblt76", "Content": "It appeared too easy to me that we could create entire GUI from Python. I waited for the other shoe to drop, and it dropped. Now Anaconda and PySimpleGUI require payment if ever you want to use the libraries professionally. I wonder whether this will be the end of Python. It seems enshittification is finally reaching Python, where people previously used libraries for free and when they update them find out that they have to pay or stop using the libraries."},
{"Title": "async_api_caller - simplifies asynchronous web API calling", "Author": "u/teamamentum", "Content": "What My Project Does Making asynchronous web API calls with asyncio and aiohttp can be complicated. This async_api_caller package abstracts away that complexity for a common case of needing to make multiple web API calls while varying query parameters. Installation git clone git@github.com:amentumspace/async_api_caller.git cd async_api_caller/ pip install . Usage import async_api_caller url = \"https://ocean.amentum.io/gebco\" headers = {'API-Key': API_KEY} param_list = [ { \"latitude\": 42, \"longitude\": 42 },{ \"latitude\": 43, \"longitude\": 43 } ] responses_json = async_api_caller.run( url, headers, param_list ) Target Audience Python developers or data scientists or scientists or any Pythonista wanting a simple way to make asynchronous web API calls. Comparison Differs from existing alternatives in its simplicity. Simple like grequests without needing gevents. It really just abstracts the usual boilerplate needed to use asyncio with aiohttp ."},
{"Title": "magic-di: Dependency Injector with minimal boilerplate code, built-in support for FastAPI and Celery", "Author": "u/R1ngoB", "Content": "I‚Äôm excited to share something we‚Äôve been working on at Wolt: a new dependency injection library for Python ‚Äú magic-di ‚Äù. Born from our experience with a large service that has many components such as: API, event consumers, background workers, and cron jobs. magic-di aims to simplify dependency management without the hassle. from magic_di.fastapi import inject_app, Provide app = inject_app(FastAPI()) @app.get(path=\"/hello-reddit\") def hello_reddit(service: Provide[Service]) -> dict: return { \"is_connected\": service.is_connected(), } What Project Does magic-di cuts through the complexity of dependency management by adhering to a ‚Äúzero-config‚Äù philosophy. It was developed to address the challenges we faced in managing code dependencies across various deployments, aiming for a straightforward solution. Core Concepts Zero Config : Setup is a breeze with magic-di. There‚Äôs no need to wrangle with injector configurations. All parameters in dependencies should either be injected or fetched from the environment, file, or any other source Connectable Dependencies : Dependencies should implement a Connectable interface, which mandates __connect__ and __disconnect__ methods. This neat feature ensures that your dependencies are properly initialized and shut down. Dependency Order Resolution : magic-di ensures dependencies are injected in the correct order and ready to use right away once it‚Äôs injected Target Audience magic-di is ideal for Python developers looking for a straightforward dependency injection solution that helps to think only about business logic and not about boilerplate code. It‚Äôs designed to keep things simple, focusing on getting the job done without unnecessary configuration. Comparison magic-di differs from existing solutions by focusing on simplicity and practicality: Compared to FastAPI‚Äôs built-in DI: FastAPI couples your logic to its ecosystem. magic-di offers a framework-agnostic approach, making it easier to integrate without tying your code to a specific framework. Compared to python-dependency-injector : This library can be heavy on boilerplate. magic-di aims to reduce boilerplate to the bare minimum, advocating for a design where dependencies are self-configuring. Goals Our main goal with magic-di is to provide a DI tool that‚Äôs easy to integrate and simplifies the developer‚Äôs workflow. We wanted to address the common pain points in dependency management without introducing new complexities. In essence, magic-di is about making dependency injection straightforward and hassle-free for Python developers. It‚Äôs a tool born from practical needs, and while it may not revolutionize the field, it certainly aims to improve how we handle dependencies in our projects. We believe magic-di can make a positive impact on your projects, and we‚Äôre eager for you to try it out and share your thoughts. Your feedback is crucial as we continue to refine and improve this tool Read more about magic-di: https://github.com/woltapp/magic-di"},
{"Title": "3-dimensional/ heatmap peak picking?", "Author": "u/robbie_s1", "Content": "Hello python community. I‚Äôm pretty new to scripting and come from a pure chemistry background. I‚Äôm working with 3-D MALDI-trapped ion mobility mass spectrometry data and I‚Äôve been looking for a solid algorithm that can peak pick from two ‚Äòtime-resolved‚Äô dimensions (mass and mobility), and the intensity arrays associated with those two dimensions. I need to peak pick from a heatmap essentially. Does anyone know of any good 3-D peak picking algorithms, or otherwise know how I can approach this? Thank you all!"},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Do folks ever use Pandas when they should use SQL?", "Author": "u/ChristianPacifist", "Content": "I think I see this a lot. I believe SQL, including like dataframe SQL packages, should always be used over Pandas when possible unless one is doing something Pandas is more suited to or unless someone requires broader Python control structures or features. I am a big believer in sticking to SQL as a baseline data manipulation framework and only using other frameworks if SQL is lacking. One of the biggest ways to accumulate technical debt is solving problems in multiple ways that are not standardized that not all people know."},
{"Title": "Automatically extract phone call insights with LLMs - quick tutorial", "Author": "u/SleekEagle", "Content": "Hey everyone! I wrote this quick Python script showing how you can automatically extract information from phone calls with LLMS. The call I use is a phone call received by a home building company where the caller is inquiring about an estimate for the cost to build a home on land he may purchase. Here's what the LLM extracted: SUMMARY: - The caller is interested in getting an estimate for building a house on a property he is looking to purchase in Westchester. ACTION ITEMS: - Have someone call the customer back today to discuss building estimate. - Set up time for builder to assess potential property site prior to purchase. CONTACT INFORMATION: Name: Lindstrom, Kenny Phone number: 610-265-1715 If you skip to around 4:25 in this video you can see that all the information is extracted accurately! You can check out the code in this GitHub repository . It's really straightforward and only requires a handful of lines of code - feel free to drop any questions if you have any!"},
{"Title": "With the demise of open Pysimplegui is anyone interested in helping productionise this alternative?", "Author": "u/Black-DVD-Archiver", "Content": "I wrote my own Pyside6 alternative to Pysimple gui that I use in my projects starting some years back after some unsatisfactory interactions with the author of Pysimple gui. The GIT source for my GUI wrapper is in https://github.com/David-Worboys/Black-DVD-Archiver/blob/main/qtgui.py And the open source project it is used in is https://github.com/David-Worboys/Black-DVD-Archiver Just look for the layout methods in the .py files to see how it is used. https://github.com/David-Worboys/Black-DVD-Archiver/blob/main/video_cutter.py#L2391 is a good place to start, line 953,. Note size measurements are generally in chars, but pixels can be used Make no mistake there is a lot of work to produuctionise this and make a Python library, including, most importantly, doco of which there is none. If anyone is interested I will set up a new Git project for it.. I am living on borrowed time and have to focus more on my other projectsWith the demise of open Pysimplegui is anyone interested in helping productionise this alternative? Help I wrote my own Pyside6 alternative to Pysimple gui that  I use in my projects starting some years back after some unsatisfactory interactions with the author of Pysimple gui. The GIT source for my GUI wrapper is in https://github.com/David-Worboys/Black-DVD-Archiver/blob/main/qtgui.py And the open source project it is used in is https://github.com/David-Worboys/Black-DVD-Archiver Just look for the layout methods in the .py files to see how it is used. https://github.com/David-Worboys/Black-DVD-Archiver/blob/main/video_cutter.py#L2391 is a good place to start, line 953,. Note size measurements are generally in chars, but pixels can be used Make no mistake there is a lot of work to produuctionise this and make a Python library, including, most importantly, doco of which there is none. If anyone is interested I will set up a new Git project for it.. I am living on borrowed time and have to focus more on my other projects"},
{"Title": "Automated system testing on GUI app (Nodezator - Public domain generalist Python node editor)", "Author": "u/KennedyRichard", "Content": "Hello, everyone. I'm Kennedy Guerra, 33, open-source software maintainer. Just wanted to share a new feature I'm working on for the next version of Nodezator ( GitHub | website ), my public domain generalist Python node editor. In case you don't know Nodezator, the next few sections present it briefly. If you want to know about the new feature I'm working on, you can skip straight to the last subsection, which contains a detailed explanation and links to videos and extra info about the feature, called Automated System Testing. What my project does Nodezator is a generalist Python node editor (a desktop app) that allows developers to build and execute visual graphs whose nodes represent Python callables (your own and/or callables from Python standard or third-party libraries). Target audience Python developers in all areas. Intermediate Python knowledge is advised. You can also use it as a no-code interface, since people editing/executing the graph only have to deal with widgets and basic node editing operations like connecting/disconnecting sockets, creating or deleting nodes, etc. Comparison Nodezator has actually many features that are either absent in other similar apps or are very rarely found: it is dedicated to the public domain with much love devs only need to define Python functions and the app automatically turns them into nodes (yes, no need to subclass anything) instead of writing your own functions, you can also feed existing functions from third-party libraries and the app will also turn them into nodes, virtually making Nodezator an \"everything nodes\" solution the graphs created can be converted back into Python code with the click of a button exporting to SVG, HTML+SVG and PNG is also available in the same way functions can receive other functions, nodes can receive other nodes as arguments, allowing the creation of powerful operations (functions are first-class citizens after all, so the nodes that represent them are as well) a comprehensive user manual available both inside the app and online . New feature on development branch: Automated System Testing What I wanted to highlight today, though, is a feature I'm currently working on, called automated system testing , which is available in the development branch. Here's the link to a GitHub devlog post that contains a video demonstrating the feature in action: https://github.com/IndiePython/nodezator/discussions/72#discussioncomment-8997584 On the post you can also read more about the feature and how to reproduce the behaviour inside the app. Here's a brief explanation: once the user picks the tests to be performed in a dedicated form and clicks \"Start system testing session\", the app takes over and performs all the tests automatically by controlling the GUI. At the end of each test case, the app checks the state for expected values. After all test cases are performed, the app shows a report. I'm very relieved this feature is finally implemented and should soon be released (it needs a lot of polishing and is only available in the development branch, but is already functional). The reason for this is that as the project grows and becomes more and more complex, I needed to automated this kind of tests to ensure a more healthy/feasible development and more stable releases. That is, there are so many different tasks performed in the app that testing it manually takes a lot of time. As we all know, development requires a lot of iteration, so the enormous amount of time required to test everything manually would be spent many times over the implementation of even the simplest changes. Once this feature is released, we'll be able to automatically test everything and thus get feedback much earlier regarding the impact of our changes throughout the plethora of possible tasks that comprise Nodezator's workflow. Of course we also have unit tests, and they are important, but they are useful for testing units of behaviour, not their synergy when fully integrated in the working app as the user operates it. Once released, this feature will help me and other contributors check much sooner and with more precision whether our work is breaking any existing tasks/workflows/features. Sooner because the automated tests can be executed as fast as possible, as shown in the video, not only in normal time. With precision because the exact mouse and keyboard interactions are reproduced in the exact same way every time each test runs. This app is part of the Indie Python project, a personal project of mine to create and maintain completely free and open-source software (apps and games) and also release instructional/educational content related to them. Such software/content is created and maintained with much love, so please, consider supporting us if you have the means: https://patreon.com/KennedyRichard , https://github.com/sponsors/KennedyRichard , https://indiepython.com/donate . Thank you for your attention, let me know if you have any questions."},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Gmail Filter Squasher project", "Author": "u/unexpected_possum", "Content": "In Gmail I realized that I had way too many filters that performed exactly the same action, but they simply had different \"from\" addresses. To solve this I created this little project that merges similar filters into a single one. It does require a lot of setup due to having to authenticate into Gmail, but if someone is on the same situation as me, it could be helpful. Link to the project: github What My Project Does It connects into a Gmail account, reads their filters and squashes them into a single one. The program requires a manual setup of creating an OAuth credentials screen and downloading relevant credentials, which is cumbersome. But it's a one time effort as you can keep on using the application as long as you keep access to the GCP project. Note that it only merges filters if their criteria is a single \"from\" statement and they perform exactly the same action. Target Audience It's just a toy project of mine that I did for my sole benefit, but thought that it could be of use to anyone else in my same situation. Comparison I tried to look up for an already-existing solution for this issue before implementing it myself, but I couldn't find it. Relevant Gmail support post . Please do let me know if you have any suggestion or advice. Link to the project: github"},
{"Title": "I made my very first python library! It converts reddit posts to text format for feeding to LLM's!", "Author": "u/NFeruch", "Content": "Hello everyone, I've been programming for about 4 years now and this is my first ever library that I created! What My Project Does It's called Reddit2Text, and it converts a reddit post (and all its comments) into a single, clean, easy to copy/paste string. I often like to ask ChatGPT about reddit posts, but copying all the relevant information among a large amount of comments is difficult/impossible. I searched for a tool or library that would help me do this and was astonished to find no such thing! I took it into my own hands and decided to make it myself. Target Audience This project is useable in its current state, and always looking for more feedback/features from the community! Comparison There are no other similar alternatives AFAIK Here is the GitHub repo: https://github.com/NFeruch/reddit2text It's also available to download through pip/pypi :D Some basic features: Gathers the authors, upvotes, and text for the OP and every single comment Specify the max depth for how many comments you want Change the delimiter for the comment nesting Here is an example truncated output: https://pastebin.com/mmHFJtcc Under the hood, I relied heavily on the PRAW library (python reddit api wrapper) to do the actual interfacing with the Reddit API. I took it a step further though, by combining all these moving parts and raw outputs into something that's easily useable and very simple.Could you see yourself using something like this?"},
{"Title": "Would there be general interest for a library that ensure external application API call safety?", "Author": "u/SpeakerOk1974", "Content": "Python has become a popular inclusion as a scripting language for many professional software packages. However, the APIs presented by these applications may be unsafe and/or do not include pythonic error handling. As someone working in the energy sector at my day job, I often find myself wrestling issues external to my python code in our power system simulator (usually memory related) that due to the implementation of the API can error at runtime below the exception handling mechanism of the interpreter. I will have have to implement a library for this purpose either internally or as an open-source side project. Traditionally, we handle these errors by isolating all potentially unsafe API calls in their own function and running that function in a separate process with multiprocessing and a handler function orchestrating the process and handling the exit codes. Although effective, this basic solution doesn't have the level of functionality needed (no logging, no stdout/stderr redirection, no exceptions, etc) and brings a decent amount of boilerplate to simple scripts. I wanted to post here and see if this is an issue others in the community face interacting with external code written in lower level languages they cannot freely modify. This isn't a detailed proposal more so trying to generate discussion around the idea, but essentially the module would provide decorators to wrap the external calls safely, with extra features like logging, custom exception classes and more to allow pythonic handling of errors. TLDR: Would there be interest in a library centered around pain-free handling of runtime errors that happen below the exception handling mechanism with extra convenience features?"},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "PyPDFCompare PDF Comparison CLI Tool", "Author": "u/Motox2019", "Content": "First time poster here, Ill preface this post by saying I am not a software engineer by any metric. I am a hobbyist programmer and I mainly just create programs I find particularly useful in my day job as a designer. Figured I would share my handy PDF comparison tool, see if anyone else can find it useful as well. This tool is mostly designed around comparing CAD drawings. It probably isn't great for comparing text documents at the moment though I plan to add this functionality. It works by comparing the pdf's page by page in a raster image format. It creates several output files of different styles: Markup Page Differences Page Overlay Page The markup page is the main page (File 1) with outlines around any changes and some boxes to highlight larger areas. The differences page is essentially the main page - the secondary page. Any changes that were \"removed\" since the previous version end up in red and anything that was \"added\" is in blue. The overlay page is the secondary page in red with the main page slapped on top showing differences quite nicely in red. Don't judge too harshly :) https://github.com/TMan9654/PyPDFCompare"},
{"Title": "All Python conference talks from 2023 ordered by the number of views", "Author": "u/TechTalksWeekly", "Content": "Hello r/python üëã! Back in January, I've compiled a list of the most watched PyCon talks from 2023. I've received tons of positive feedback via DM, upvotes, and comments, so I decided to put together another compilation. This time around, I've gathered not only PyCon, but all Python 2023 talks across +100 conferences (here's the list ) that include PyCon (all locations), PyData (all locations), EuroPython , Conf42 , and many more to give you a complete overview of the landscape. The list is gigantic and includes over 850 talks **!** What's more, I've created a Google Sheets version of this post that gives more convenient sorting and filtering options. If it turns to be useful, don't forget to share the list with friends / colleagues! \"Tutorials - Mario Munoz: Web Development With A Python-backed Frontend: Featuring HTMX and Tailwind\" ‚∏± +12k views ‚∏± 02h 26m 00s \"Thomas Bierhance: Polars - make the switch to lightning-fast dataframes\" ‚∏± +11k views ‚∏± 00h 29m 49s \"Mariatta Wijaya: Welcome to PyCon US 2023\" ‚∏± +11k views ‚∏± 00h 29m 29s \"Scaling Python for Machine Learning: Beyond Data Parallelism ‚Ä¢ Holden Karau ‚Ä¢ GOTO 2023\" ‚∏± +10k views ‚∏± 00h 39m 19s \"Tutorials - Matt Harrison: Getting Started with Polars\" ‚∏± +10k views ‚∏± 02h 14m 28s \"Keynote Speaker - Ned Batchelder\" ‚∏± +7k views ‚∏± 00h 47m 02s \"G√°bor Sz√°rnyas - DuckDB: The Power of a Data Warehouse in your Python Process\" ‚∏± +7k views ‚∏± 00h 55m 27s \"Talks - Samuel Colvin: How Pydantic V2 leverages Rust's Superpowers\" ‚∏± +6k views ‚∏± 00h 45m 55s \"Delta-rs, Apache Arrow, Polars, WASM: Is Rust the Future of Analytics?\" ‚∏± +6k views ‚∏± 00h 31m 03s \"Tutorials - Simon Willison: Data analysis with SQLite and Python\" ‚∏± +5k views ‚∏± 02h 45m 54s \"Talks - Hynek Schlawack: Subclassing, Composition, Python, and You\" ‚∏± +5k views ‚∏± 00h 45m 42s \"Writing Python Bindings for C++ Libraries: Easy-to-use Performance - Saksham Sharma - CppCon 2023\" ‚∏± +5k views ‚∏± 01h 01m 30s \"Pedro Holanda - DuckDB: Bringing analytical SQL directly to your Python shell\" ‚∏± +5k views ‚∏± 00h 29m 31s \"Tutorials - Lisa Carpenter: How to create beautiful interactive GUIs and web apps\" ‚∏± +5k views ‚∏± 01h 58m 05s \"Tutorials - Reuven M. Lerner: Comprehending comprehensions\" ‚∏± +5k views ‚∏± 02h 17m 33s \"Keynote Speaker - Guido van Rossum\" ‚∏± +5k views ‚∏± 00h 30m 51s \"PyData Heidelberg #11 - TimeSeries Forecasting & LLM Langchain\" ‚∏± +4k views ‚∏± 02h 39m 06s \"Keynote Speaker - James Powell\" ‚∏± +4k views ‚∏± 00h 25m 43s \"Use Spark from anywhere: A Spark client in Python powered by Spark Connect\" ‚∏± +4k views ‚∏± 00h 56m 58s \"Cython 3 ‚Äì Python at the speed of C ‚Äî Stefan Behnel\" ‚∏± +4k views ‚∏± 00h 31m 15s \"Patrick Bl√∂baum: Performing Root Cause Analysis with DoWhy, a Causal Machine-Learning Library\" ‚∏± +4k views ‚∏± 00h 44m 38s \"Max Mergenthaler and Fede Garza - Quantifying Uncertainty in Time Series Forecasting\" ‚∏± +4k views ‚∏± 00h 37m 25s \"Talks - Bruce Eckel: Rethinking Objects\" ‚∏± +4k views ‚∏± 00h 32m 06s \"Rust for Python data engineers ‚Äî Karim Jedda\" ‚∏± +4k views ‚∏± 00h 27m 30s \"Matt Harrison - An Introduction to Pandas 2, Polars, and DuckDB | PyData Global 2023\" ‚∏± +4k views ‚∏± 01h 22m 15s \"Tutorials - Ted Patrick: Writing Serverless Python Web Apps with PyScript\" ‚∏± +3k views ‚∏± 02h 55m 29s \"Tutorials - Trey Hunner: Intro to Python for Brand New Programmers\" ‚∏± +3k views ‚∏± 02h 16m 41s \"Dr. Thomas Wiecki: Bayesian Marketing Science - Solving Marketing's 3 Biggest Problems\" ‚∏± +3k views ‚∏± 00h 30m 35s [**\"Ga√´l Varoquaux: Prepping Tables for Machine Learning Gets Easier PyData S√ºdwest]\"** ‚∏± +3k views ‚∏± 00h 57m 54s \"Talks - Reuven M. Lerner: Generators, coroutines and nanoservices\" ‚∏± +3k views ‚∏± 00h 26m 28s \"Talks - Brett Cannon: Python's syntactic sugar\" ‚∏± +3k views ‚∏± 00h 31m 11s \"Bruno Vollmer: BLE and Python - How to build a simple BLE project on Linux with Python\" ‚∏± +3k views ‚∏± 00h 29m 40s \"Vasileios Mourtakos - A data engineering framework in Python\" ‚∏± +3k views ‚∏± 00h 35m 55s \"Talks - Eric Snow: A Per-Interpreter GIL: Concurrency and Parallelism with Subinterpreters\" ‚∏± +3k views ‚∏± 00h 30m 29s \"Talks - ≈Åukasz Langa: Working Around the GIL with asyncio\" ‚∏± +2k views ‚∏± 00h 44m 26s \"Hajime Takeda - Media Mix Modeling:How to Measure the Effectiveness of Advertising\" ‚∏± +2k views ‚∏± 00h 30m 37s \"Tutorials - Mike M√ºller: The How and Why of Object-oriented Programming in Python\" ‚∏± +2k views ‚∏± 02h 45m 34s \"Shahriyar Rzayev: Building Hexagonal Python Services\" ‚∏± +2k views ‚∏± 01h 29m 50s \"Performance tips by the FastAPI Expert ‚Äî Marcelo Trylesinski\" ‚∏± +2k views ‚∏± 00h 24m 59s \"Solving Multi-Objective Constrained Optimisation Problems using Pymoo ‚Äî Pranjal Biyani\" ‚∏± +2k views ‚∏± 00h 44m 23s \"Carsten Binnig: Towards Learned Database Systems\" ‚∏± +2k views ‚∏± 00h 43m 34s \"Talks - Brandt Bucher: Inside CPython 3.11's new specializing, adaptive interpreter\" ‚∏± +2k views ‚∏± 00h 23m 01s \"Carl Kadie - A Perfect, Infinite-Precision, Game Physics in Python | PyData Seattle 2023\" ‚∏± +2k views ‚∏± 00h 36m 26s \"From crontab to celery with no regrets by Marco Pavanelli\" ‚∏± +2k views ‚∏± 00h 32m 03s \"Talks - Al Sweigart: An Overview of the Python Code Tool Landscape 2023\" ‚∏± +2k views ‚∏± 00h 26m 58s \"Reinventing Machine Learning with Transformers and Hugging Face by Keynote speaker Julien Simon\" ‚∏± +2k views ‚∏± 01h 08m 52s \"Empower your Spring Applications with Python Features on GraalVM by Johannes Link @ Spring I/O 2023\" ‚∏± +2k views ‚∏± 00h 53m 00s \"Marysia Winkels - Data Storytelling through Visualization\" ‚∏± +2k views ‚∏± 00h 30m 57s \"What polars does for you ‚Äî Ritchie Vink\" ‚∏± +2k views ‚∏± 00h 27m 45s \"Beyond Toy Datasets: Timeseries Forecasting for Real Business Problems - Robert Haase\" ‚∏± +2k views ‚∏± 00h 33m 55s \"PyData Online - An AI assistant for football analytics - Petar Veliƒçkoviƒá (Google DeepMind)\" ‚∏± +2k views ‚∏± 01h 02m 40s \"Talks - Mark Shannon: How we are making CPython faster. Past, present and future.\" ‚∏± +2k views ‚∏± 00h 29m 11s \"JIm Dowling - Build a production ML system with only Python on free serverless services\" ‚∏± +2k views ‚∏± 01h 22m 35s \"Writing a Python interpreter from scratch, in half an hour ‚Äî Tushar Sadhwani\" ‚∏± +2k views ‚∏± 00h 43m 38s \"Dominika Basaj & Barbara Rychalska - Creating behavioral profiles of your customer\" ‚∏± +2k views ‚∏± 00h 26m 55s \"Stephan Sahm: Accelerate Python with Julia\" ‚∏± +2k views ‚∏± 01h 27m 14s \"Why Delta Lake is the Best Storage Format for Pandas Analyses\" ‚∏± +2k views ‚∏± 00h 28m 25s \"Tutorials - Geir Arne Hjelle: Introduction to Decorators: Power Up Your Python Code\" ‚∏± +2k views ‚∏± 02h 21m 20s \"Jeroen Overschie - How to create a Devcontainer for your Python project üê≥\" ‚∏± +2k views ‚∏± 00h 36m 01s \"Wesley Boelrijk - Lowering the barrier for ML monitoring\" ‚∏± +2k views ‚∏± 00h 36m 53s \"Talks - Dan Craig: Testing Spacecraft with Pytest\" ‚∏± +2k views ‚∏± 00h 30m 12s \"Keynote Speaker - Margaret Mitchell\" ‚∏± +2k views ‚∏± 00h 42m 29s \"Talks - Moshe Zadka: pyproject.toml, packaging, and you\" ‚∏± +2k views ‚∏± 00h 30m 06s \"Tutorials - Juhi, Dana: Intro to Hugging Face: Fine-tuning BERT for NLP tasks\" ‚∏± +2k views ‚∏± 02h 09m 52s \"Tutorials - Patrick Arminio: Build a production ready GraphQL API using Python\" ‚∏± +2k views ‚∏± 02h 23m 14s \"Ryan Curtin - Lightweight, low-overhead, high-performance: machine learning directly in C++\" ‚∏± +2k views ‚∏± 00h 43m 10s \"Python with Spark Connect\" ‚∏± +2k views ‚∏± 00h 33m 27s \"David Qiu - Jupyter AI ‚Äî Bringing Generative AI to Jupyter | PyData Seattle 2023\" ‚∏± +1k views ‚∏± 00h 39m 29s \"Polars: A highly optimized dataframe library | Matt Harrison | Conf42 Machine Learning 2023\" ‚∏± +1k views ‚∏± 00h 20m 31s \"Nico Kreiling: Raised by Pandas, striving for more: An opinionated introduction to Polars\" ‚∏± +1k views ‚∏± 00h 29m 47s \"Jiang et al. - Automated Machine Learning & Tuning with FLAML | PyData Seattle 2023\" ‚∏± +1k views ‚∏± 01h 21m 54s \"Alexander CS Hendorf: 5 Things about fastAPI I wish we had known beforehand\" ‚∏± +1k views ‚∏± 00h 32m 31s \"Duarte Carmo - MLOps for the rest of us- A poor man's guide to putting models in production\" ‚∏± +1k views ‚∏± 00h 26m 04s [**\"Ines Montani (spaCy) - Large Language Models from Prototype to Production PyData S√ºdwest]\"** ‚∏± +1k views ‚∏± 00h 39m 55s \"Robin Raymond: Rusty Python - A Case Study\" ‚∏± +1k views ‚∏± 00h 28m 47s \"Tutorials - Leah Berg, Ray: Feature Engineering is for Everyone!\" ‚∏± +1k views ‚∏± 02h 16m 35s \"Giles Weaver & Ian Ozsvald - Pandas 2, Dask or Polars? Tackling larger data on a single machine\" ‚∏± +1k views ‚∏± 00h 16m 22s \"Talks - Glyph: How To Keep A Secret\" ‚∏± +1k views ‚∏± 00h 26m 19s \"Maximilian M. - SHAPtivating Insights: unravelling blackbox AI models\" ‚∏± +1k views ‚∏± 00h 35m 02s \"Vahan Huroyan - Recent Developments in Self-Supervised Learning for Computer Vision\" ‚∏± +1k views ‚∏± 00h 36m 25s \"Lucas Durand - Building an Interactive Network Graph to Understand Communities | PyData Seattle 2023\" ‚∏± +1k views ‚∏± 01h 26m 14s \"Big PyData BBQ #5: LLMs feat. Ines Montani (spaCy) & Alejandro Saucedo (Zalando)\" ‚∏± +1k views ‚∏± 03h 48m 21s \"Stop using print! Understanding and using the \"logging\" module ‚Äî Reuven M. Lerner\" ‚∏± +1k views ‚∏± 00h 29m 32s \"Joris Van den Bossche & Patrick Hoefler: Pandas 2.0 and beyond\" ‚∏± +1k views ‚∏± 00h 37m 07s \"Structured output with large language models / Uri Goren (Argmax)\" ‚∏± +1k views ‚∏± 00h 25m 09s \"PyData Chicago April 2023 Meetup | Design and Analysis of (Backtest) Experiments with R and Python\" ‚∏± +1k views ‚∏± 00h 50m 19s \"Sponsor Presentation - Python & Bloomberg: An Open Source Duo\" ‚∏± +1k views ‚∏± 00h 57m 08s \"Soumith Chintala - Keynote: AI & the stuff built for AI - are they actually useful for data science?\" ‚∏± +1k views ‚∏± 00h 31m 54s \"The Future of Microprocessors ‚Äî Sophie Wilson\" ‚∏± +1k views ‚∏± 00h 56m 45s \"Jia Yu - How Apache Sedona is Revolutionizing Geospatial Data Analysis | PyData Seattle 2023\" ‚∏± +1k views ‚∏± 00h 46m 12s \"Lightning Talks - April 21, 5pm\" ‚∏± +1k views ‚∏± 01h 04m 16s \"How to Leverage the Full Potential of LLMs for Your Business with Langchain - Leon Ruddat\" ‚∏± +1k views ‚∏± 00h 34m 40s \"Jay Chia & Sammy Sidhu: Daft - The Distributed Python Dataframe for Complex Data\" ‚∏± +1k views ‚∏± 00h 23m 01s \"Talks - A. Jesse Jiryu Davis: Consistency and isolation for Python programmers\" ‚∏± +1k views ‚∏± 00h 24m 23s \"\"Python deployment with Docker and Poetry\" - Cristian Heredia (PyBay 2023)\" ‚∏± +1k views ‚∏± 00h 12m 24s \"Peterson & Qin - Contextual Multi-Arm Bandit and its applications to digital experiments | PyData\" ‚∏± +1k views ‚∏± 00h 44m 54s \"Tutorials - Ron Nathaniel: How To Troubleshoot and Monitor Applications using OpenTelemetry\" ‚∏± +1k views ‚∏± 02h 01m 57s \"Keynote Speaker - Python Steering Council\" ‚∏± +1k views ‚∏± 00h 20m 42s \"An unbiased evaluation of environment management and packaging tools ‚Äî Anna-Lena Popkes\" ‚∏± +1k views ‚∏± 00h 43m 14s \"Vincent Gosselin - Turning your Data/AI algorithms into full web apps in no time with Taipy\" ‚∏± +1k views ‚∏± 00h 38m 18s \"Malte Tichy - Knowing what you don‚Äôt know matters- Uncertainty-aware model rating\" ‚∏± +1k views ‚∏± 00h 30m 05s \"Talks - Andrew Godwin: Reconciling Everything\" ‚∏± +1k views ‚∏± 00h 30m 16s \"Vincent Warmerdam - Bulk Labelling Techniques\" ‚∏± +1k views ‚∏± 00h 32m 55s \"Hugo Bowne-Anderson - Full-stack Machine Learning and Generative AI for Data Scientists\" ‚∏± +1k views ‚∏± 01h 28m 30s \"J.J. Allaire - Keynote: Dashboards with Jupyter and Quarto | PyData NYC 2023\" ‚∏± +1k views ‚∏± 00h 40m 15s \"Alejandro Saucedo - Industrial Strength DALLE-E:Scaling Complex Large Text & Image Models\" ‚∏± +1k views ‚∏± 00h 28m 07s \"Subclassing, Composition, Python, and You ‚Äî Hynek Schlawack\" ‚∏± +1k views ‚∏± 00h 44m 59s \"Tutorials -Zac Hatfield-Dodds, Ryan Soklaski: Introduction to Property-Based Testing\" ‚∏± +1k views ‚∏± 01h 31m 20s [**\"Furkan M. Torun - Become a Data Storyteller with Streamlit! PyData Prague 2023-11-20]\"** ‚∏± +1k views ‚∏± 00h 23m 26s \"Anna-Lena Popkes: An unbiased evaluation of environment management and packaging tools\" ‚∏± +1k views ‚∏± 00h 43m 53s \"PyData Chicago March 2023 Meetup | Monte Carlo with QMCPy for Vector Functions of Integrals\" ‚∏± +1k views ‚∏± 00h 46m 35s \"James Powell - Simple Simulators with pandas and Generator Coroutines | PyData NYC 2023\" ‚∏± +1k views ‚∏± 01h 21m 43s \"Polars is the Pandas killer / Igor Mintz (Viz.ai)\" ‚∏± +1k views ‚∏± 00h 21m 46s \"Diving into Event-Driven Architectures with Python ‚Äî Marc-Andr√© Lemburg\" ‚∏± +1k views ‚∏± 00h 30m 59s \"Using Embeddings and Deep Neural Networks as a technique for AutoML Demand Forecasting PyData SW\" ‚∏± +1k views ‚∏± 00h 36m 06s \"Leland McInnes - Data Mapping for Data Exploration | PyData Seattle 2023\" ‚∏± +1k views ‚∏± 00h 38m 16s \"Eduardo Blancas - Using embedded SQL engines for plotting massive datasets on a laptop\" ‚∏± +1k views ‚∏± 00h 27m 00s \"Large Language Models: From Prototype to Production ‚Äî Ines Montani\" ‚∏± +1k views ‚∏± 00h 40m 20s \"Tutorials - Pavithra Eswaramoorthy, Dharhas Pothina: Data of Unusual Size: Interactive Visualization\" ‚∏± +1k views ‚∏± 03h 06m 45s \"Moritz Meister - Data Validation for Feature pipelines: Using Great Expectations and Hopsworks\" ‚∏± +1k views ‚∏± 00h 27m 40s \"Inge van den Ende-Leveraging conformal prediction for calibrated probabilistic time series forecasts\" ‚∏± +1k views ‚∏± 00h 31m 33s \"Nick Sorros - A Tour of Large Language Models\" ‚∏± +1k views ‚∏± 00h 46m 38s \"Thomas Frauholz: From notebook to pipeline in no time with LineaPy\" ‚∏± +1k views ‚∏± 00h 43m 17s \"EuroPython 2023 Opening Session\" ‚∏± +1k views ‚∏± 00h 20m 58s \"Optimizing Ad Conversions with DS / Yael Kiselman (DigitalTurbine)\" ‚∏± +1k views ‚∏± 00h 51m 44s \"Harizo Rajaona - A Tour of the Many DataFrame Frameworks\" ‚∏± +1k views ‚∏± 00h 34m 59s \"Andrei Alekseev - Why does everyone need to develop a machine learning package?\" ‚∏± +1k views ‚∏± 00h 28m 07s \"Ties de Kok - Going beyond ChatGPT: introduction to prompt engineering & LLMs | PyData Seattle 2023\" ‚∏± +1k views ‚∏± 00h 59m 56s \"PyData Boston Sept session 1: Mike Woodward - Data sci done wrong: how/why scientists make mistakes.\" ‚∏± +1k views ‚∏± 00h 27m 14s \"Joe Cheng - Shiny: Data-centric web applications in Python | PyData Seattle 2023\" ‚∏± +1k views ‚∏± 00h 43m 56s \"Tutorials - Cheuk Ting Ho: Power up your work with compiling and profiling\" ‚∏± +1k views ‚∏± 01h 33m 02s \"Talks -Algorithmic ideas, engineering tricks, and trivia behind CPython's new sorting algorithm\" ‚∏± +1k views ‚∏± 00h 29m 39s \"Talks - Dawn Wages: Supercharge your Python Development Environment with VS Code + Dev Container\" ‚∏± +1k views ‚∏± 00h 31m 01s \"Egor Romanov - Performance of Vector Databases\" ‚∏± +1k views ‚∏± 00h 43m 09s \"Alexander CS Hendorf - Ten Years of Community Organizer | PyData NYC 2023\" ‚∏± +1k views ‚∏± 00h 42m 41s \"Talks - Nicholas H.Tollervey, Paul Everitt: Build Yourself a PyScript\" ‚∏± +1k views ‚∏± 00h 39m 28s \"Emil Rijcken - FuzzyTM: a Python package for fuzzy topic models\" ‚∏± +1k views ‚∏± 00h 28m 12s \"Nikolas Markou - Artificial Intelligence for Vision: A walkthrough of recent breakthroughs\" ‚∏± +1k views ‚∏± 00h 42m 10s \"Would Rust make you a better Pythonista? ‚Äî Alexys Jacob\" ‚∏± +1k views ‚∏± 00h 47m 23s \"Simon Pressler: Getting started with JAX\" ‚∏± +1k views ‚∏± 00h 29m 49s \"Sponsor Presentation - How to build stunning Data Science Web applications in Python\" ‚∏± +1k views ‚∏± 00h 54m 10s \"Pavel Pleskov - All about ML competitions!\" ‚∏± +1k views ‚∏± 01h 03m 13s \"Laszlo Sragner - Code Smells in Data Science: What can we do about them? | PyData London 2023\" ‚∏± +1k views ‚∏± 00h 39m 19s \"Talks - Jodie Burchell: Vectorize using linear algebra and NumPy to make your Python code fast\" ‚∏± +1k views ‚∏± 00h 28m 32s \"Comparing Elixir and Python when working with Simple Neural Networks - A. Neto & L. C. Tavano\" ‚∏± +1k views ‚∏± 00h 38m 11s \"Talks - Valerio Maggio: Pythonic functional ( iter )tools for your data challenges\" ‚∏± +1k views ‚∏± 00h 32m 38s \"Topaz Gilad - Classification Through Regression:Unlock the True Potential of Your Labels\" ‚∏± +1k views ‚∏± 00h 28m 13s \"Running Python packages in the browser with Pyodide ‚Äî Roman Yurchak\" ‚∏± +1k views ‚∏± 00h 27m 49s \"J.J. Allaire - Publishing Jupyter Notebooks with Quarto | PyData Seattle 2023\" ‚∏± +1k views ‚∏± 00h 40m 47s \"ipyvizzu-story- a open-source tool to build create+share animated data stories w/ python in jupyter\" ‚∏± +1k views ‚∏± 01h 25m 54s Due to Reddit post length limit, see the remaining talks in this post or google sheet ."},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Python 3.12.3 - The third maintenance release of Python 3.12", "Author": "u/Neustradamus", "Content": "https://www.python.org/downloads/release/python-3123/"},
{"Title": "Adnexus DI/IoC Framework", "Author": "u/Nictec", "Content": "Repo: https://github.com/Nictec/Adnexus What My Project Does: Adnexus is a declarative and fully typed DI/IoC framework for python. It is inspired by python-dependency-injector because this project is no longer maintained. Target Audience: Currently Adnexus is in heavy development. So expect heavy errors and breaking changes. Eventually if we reach 1.0.0 the project will be ready for production. Comparison: The main difference to existing (and maintained) projects is the \"declarative\" part of the framework. Injectables are declared direclty in the container class using Providers. This takes away the \"magic\" of IoC because it is clear where your dependencies come from. Furthermore all dependencies can be directly configured using the integrated config system (for details see the repo) Please try it and give me Feedback! Contributions are always welcome just open a pull request or an issue."},
{"Title": "PGCacheWatch - Supercharge Your Caching Strategy", "Author": "u/GabelSnabel", "Content": "Hey Python enthusiasts! üëã I'm thrilled to introduce PGCacheWatch , my recent project designed to enhance your PostgreSQL databases by bringing real-time event notifications right into your applications. Vastly improved cache invalidation efficiency, without the need for adding any new services or infrastructure. What My Project Does PGCacheWatch capitalizes on PostgreSQL's built-in NOTIFY/LISTEN features to deliver instant, real-time notifications about database events, ensuring your application's cache remains fresh and synchronized with your database. Target Audience This project is tailored for developers and architects striving to maintain high-performance and scalable Python applications with real-time data demands. Whether you're developing web applications using FastAPI, orchestrating data processing pipelines, or any application dependent on PostgreSQL for data storage, PGCacheWatch simplifies cache invalidation management. Comparison PGCacheWatch differentiates itself from other caching and notification mechanisms by: Zero Additional Infrastructure: Utilizing PostgreSQL's existing functionalities, it circumvents the necessity for external tools or services. Key Features Instant Database Change Notifications: Ensures your application is immediately informed about database events as they occur, keeping data consistently updated. Adaptable Cache Invalidation Strategies: Offers a range of strategies to precisely control the cache invalidation process, striking a balance between performance and data fidelity. Optimized for Async Python: Conceived for the asynchronous Python ecosystem, making it a superb fit for applications utilizing asyncpg and FastAPI. Join Me on GitHub I'm keen to see how PGCacheWatch can bolster your projects. Your feedback and contributions are greatly appreciated as I aspire to continually enhance and broaden the library's functionality. Explore PGCacheWatch on GitHub"},
{"Title": "Advanced Macro & Calorie Calculator", "Author": "u/MCTrovato", "Content": "What My Project Does A macro-nutrient and calorie calculator to help me achieve body composition goals. This calculator is compatible with metric and imperial units. It supports weight changes based on percentage and fixed values (lbs, kg). This is useful when losing weight in a fat loss phase as a percentage, and for gaining at a fixed value during a muscle growth phase. The recommendations are built using scientific research based on lean body mass. If the user's body fat percentage is unknown, then it will be calculated using BMI and the Deurenberg formula. General guidance is also available on the project page . Key Features: Compatible with metric and imperial units Weight change based on percentage or fixed values Recommendations based on scientific research Body Fat Percentage Calculator Choose where to place remaining calories (after minimum fat & protein targets reached) Safety Limits (Max Change & Caloric Intake Below BMR) Target Audience This calculator is ready for production. Ideally, any end user should be able to use this program, not just developers. I hope that this calculator can help the general public. Comparison There are many other macro calculators available. However, I feel that this one is a step beyond other calculators. This calculator will always make sure that the end user reaches their minimum protein and fat amounts. It also also allows the user to select where to place any remaining calories. Some may prefer to place remaining calories in carbs, while others may wish to stay in ketosis and place remaining calories in fat, or even a mix of all macro-nutrients. It also has built in safety mechanism to keep calories and macros adequate for health status. Project Page: Advanced Macro Calculator on GitHub"},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Using Jupyter Notebooks with Flox", "Author": "u/z_mitchell", "Content": "https://flox.dev/blog/jupyter-remote-env Hey everyone, we released Flox 1.0 a few weeks ago and one of the cool things you can do is set up environments with a bunch of dependencies already set up for running a Jupyter notebook. Take a look to see how it all works!"},
{"Title": "automathon: A Python library for simulating and visualizing finite automata", "Author": "u/fexx3l", "Content": "What my project does automathon is a Python library for simulating and visualizing finite automata, is easy to use and have a docs website to read more about the functions that are implemented for DFAs and NFAs. During my CS degree, I learned about automata and this became one of my favorite topics, that's why I started to work on this project, I wanted to create something simple and easy to use. Target audience Students, professors and any developer that could need to use automata. Comparison I don't know if there's any python library that let you simulate finite automata, but I'm sure you will love this one, there's a blog talking about automata and have examples using automathon, you can read it, it's written in French."},
{"Title": "pwdgen - a simple password generator", "Author": "u/RRTheGuy", "Content": "What my project does My project generate simple, strong, memorable and easy-to-type passwords. Target audience For anyone who need to get passwords easily. Comparison Most passwords manager generate completely passwords with completely random characters that aren't very easy to memorize or tape. Examples include Dashlane , Norton , Avast . Or other like Bitwarden generate passwords that are not really fast-to-type. The mine generate sth like 6Nixe#Becokace 0Qubyby+Pomafy , or 7Zuxogu:Lebuwo . I wanted to make a password generator that combines simplicity, security, memorability, and ease of type. Usage You can install it with pip install pwd-generator and use the cli version: pwdgen To use it in a python code import pwdgen print(pwdgen.generate()) Source code The source code is on github"},
{"Title": "Paid libraries may kill Python", "Author": "u/Glxblt76", "Content": "It appeared too easy to me that we could create entire GUI from Python. I waited for the other shoe to drop, and it dropped. Now Anaconda and PySimpleGUI require payment if ever you want to use the libraries professionally. I wonder whether this will be the end of Python. It seems enshittification is finally reaching Python, where people previously used libraries for free and when they update them find out that they have to pay or stop using the libraries."},
{"Title": "async_api_caller - simplifies asynchronous web API calling", "Author": "u/teamamentum", "Content": "What My Project Does Making asynchronous web API calls with asyncio and aiohttp can be complicated. This async_api_caller package abstracts away that complexity for a common case of needing to make multiple web API calls while varying query parameters. Installation git clone git@github.com:amentumspace/async_api_caller.git cd async_api_caller/ pip install . Usage import async_api_caller url = \"https://ocean.amentum.io/gebco\" headers = {'API-Key': API_KEY} param_list = [ { \"latitude\": 42, \"longitude\": 42 },{ \"latitude\": 43, \"longitude\": 43 } ] responses_json = async_api_caller.run( url, headers, param_list ) Target Audience Python developers or data scientists or scientists or any Pythonista wanting a simple way to make asynchronous web API calls. Comparison Differs from existing alternatives in its simplicity. Simple like grequests without needing gevents. It really just abstracts the usual boilerplate needed to use asyncio with aiohttp ."},
{"Title": "magic-di: Dependency Injector with minimal boilerplate code, built-in support for FastAPI and Celery", "Author": "u/R1ngoB", "Content": "I‚Äôm excited to share something we‚Äôve been working on at Wolt: a new dependency injection library for Python ‚Äú magic-di ‚Äù. Born from our experience with a large service that has many components such as: API, event consumers, background workers, and cron jobs. magic-di aims to simplify dependency management without the hassle. from magic_di.fastapi import inject_app, Provide app = inject_app(FastAPI()) @app.get(path=\"/hello-reddit\") def hello_reddit(service: Provide[Service]) -> dict: return { \"is_connected\": service.is_connected(), } What Project Does magic-di cuts through the complexity of dependency management by adhering to a ‚Äúzero-config‚Äù philosophy. It was developed to address the challenges we faced in managing code dependencies across various deployments, aiming for a straightforward solution. Core Concepts Zero Config : Setup is a breeze with magic-di. There‚Äôs no need to wrangle with injector configurations. All parameters in dependencies should either be injected or fetched from the environment, file, or any other source Connectable Dependencies : Dependencies should implement a Connectable interface, which mandates __connect__ and __disconnect__ methods. This neat feature ensures that your dependencies are properly initialized and shut down. Dependency Order Resolution : magic-di ensures dependencies are injected in the correct order and ready to use right away once it‚Äôs injected Target Audience magic-di is ideal for Python developers looking for a straightforward dependency injection solution that helps to think only about business logic and not about boilerplate code. It‚Äôs designed to keep things simple, focusing on getting the job done without unnecessary configuration. Comparison magic-di differs from existing solutions by focusing on simplicity and practicality: Compared to FastAPI‚Äôs built-in DI: FastAPI couples your logic to its ecosystem. magic-di offers a framework-agnostic approach, making it easier to integrate without tying your code to a specific framework. Compared to python-dependency-injector : This library can be heavy on boilerplate. magic-di aims to reduce boilerplate to the bare minimum, advocating for a design where dependencies are self-configuring. Goals Our main goal with magic-di is to provide a DI tool that‚Äôs easy to integrate and simplifies the developer‚Äôs workflow. We wanted to address the common pain points in dependency management without introducing new complexities. In essence, magic-di is about making dependency injection straightforward and hassle-free for Python developers. It‚Äôs a tool born from practical needs, and while it may not revolutionize the field, it certainly aims to improve how we handle dependencies in our projects. We believe magic-di can make a positive impact on your projects, and we‚Äôre eager for you to try it out and share your thoughts. Your feedback is crucial as we continue to refine and improve this tool Read more about magic-di: https://github.com/woltapp/magic-di"},
{"Title": "3-dimensional/ heatmap peak picking?", "Author": "u/robbie_s1", "Content": "Hello python community. I‚Äôm pretty new to scripting and come from a pure chemistry background. I‚Äôm working with 3-D MALDI-trapped ion mobility mass spectrometry data and I‚Äôve been looking for a solid algorithm that can peak pick from two ‚Äòtime-resolved‚Äô dimensions (mass and mobility), and the intensity arrays associated with those two dimensions. I need to peak pick from a heatmap essentially. Does anyone know of any good 3-D peak picking algorithms, or otherwise know how I can approach this? Thank you all!"},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Do folks ever use Pandas when they should use SQL?", "Author": "u/ChristianPacifist", "Content": "I think I see this a lot. I believe SQL, including like dataframe SQL packages, should always be used over Pandas when possible unless one is doing something Pandas is more suited to or unless someone requires broader Python control structures or features. I am a big believer in sticking to SQL as a baseline data manipulation framework and only using other frameworks if SQL is lacking. One of the biggest ways to accumulate technical debt is solving problems in multiple ways that are not standardized that not all people know."},
{"Title": "Automatically extract phone call insights with LLMs - quick tutorial", "Author": "u/SleekEagle", "Content": "Hey everyone! I wrote this quick Python script showing how you can automatically extract information from phone calls with LLMS. The call I use is a phone call received by a home building company where the caller is inquiring about an estimate for the cost to build a home on land he may purchase. Here's what the LLM extracted: SUMMARY: - The caller is interested in getting an estimate for building a house on a property he is looking to purchase in Westchester. ACTION ITEMS: - Have someone call the customer back today to discuss building estimate. - Set up time for builder to assess potential property site prior to purchase. CONTACT INFORMATION: Name: Lindstrom, Kenny Phone number: 610-265-1715 If you skip to around 4:25 in this video you can see that all the information is extracted accurately! You can check out the code in this GitHub repository . It's really straightforward and only requires a handful of lines of code - feel free to drop any questions if you have any!"},
{"Title": "With the demise of open Pysimplegui is anyone interested in helping productionise this alternative?", "Author": "u/Black-DVD-Archiver", "Content": "I wrote my own Pyside6 alternative to Pysimple gui that I use in my projects starting some years back after some unsatisfactory interactions with the author of Pysimple gui. The GIT source for my GUI wrapper is in https://github.com/David-Worboys/Black-DVD-Archiver/blob/main/qtgui.py And the open source project it is used in is https://github.com/David-Worboys/Black-DVD-Archiver Just look for the layout methods in the .py files to see how it is used. https://github.com/David-Worboys/Black-DVD-Archiver/blob/main/video_cutter.py#L2391 is a good place to start, line 953,. Note size measurements are generally in chars, but pixels can be used Make no mistake there is a lot of work to produuctionise this and make a Python library, including, most importantly, doco of which there is none. If anyone is interested I will set up a new Git project for it.. I am living on borrowed time and have to focus more on my other projectsWith the demise of open Pysimplegui is anyone interested in helping productionise this alternative? Help I wrote my own Pyside6 alternative to Pysimple gui that  I use in my projects starting some years back after some unsatisfactory interactions with the author of Pysimple gui. The GIT source for my GUI wrapper is in https://github.com/David-Worboys/Black-DVD-Archiver/blob/main/qtgui.py And the open source project it is used in is https://github.com/David-Worboys/Black-DVD-Archiver Just look for the layout methods in the .py files to see how it is used. https://github.com/David-Worboys/Black-DVD-Archiver/blob/main/video_cutter.py#L2391 is a good place to start, line 953,. Note size measurements are generally in chars, but pixels can be used Make no mistake there is a lot of work to produuctionise this and make a Python library, including, most importantly, doco of which there is none. If anyone is interested I will set up a new Git project for it.. I am living on borrowed time and have to focus more on my other projects"},
{"Title": "Automated system testing on GUI app (Nodezator - Public domain generalist Python node editor)", "Author": "u/KennedyRichard", "Content": "Hello, everyone. I'm Kennedy Guerra, 33, open-source software maintainer. Just wanted to share a new feature I'm working on for the next version of Nodezator ( GitHub | website ), my public domain generalist Python node editor. In case you don't know Nodezator, the next few sections present it briefly. If you want to know about the new feature I'm working on, you can skip straight to the last subsection, which contains a detailed explanation and links to videos and extra info about the feature, called Automated System Testing. What my project does Nodezator is a generalist Python node editor (a desktop app) that allows developers to build and execute visual graphs whose nodes represent Python callables (your own and/or callables from Python standard or third-party libraries). Target audience Python developers in all areas. Intermediate Python knowledge is advised. You can also use it as a no-code interface, since people editing/executing the graph only have to deal with widgets and basic node editing operations like connecting/disconnecting sockets, creating or deleting nodes, etc. Comparison Nodezator has actually many features that are either absent in other similar apps or are very rarely found: it is dedicated to the public domain with much love devs only need to define Python functions and the app automatically turns them into nodes (yes, no need to subclass anything) instead of writing your own functions, you can also feed existing functions from third-party libraries and the app will also turn them into nodes, virtually making Nodezator an \"everything nodes\" solution the graphs created can be converted back into Python code with the click of a button exporting to SVG, HTML+SVG and PNG is also available in the same way functions can receive other functions, nodes can receive other nodes as arguments, allowing the creation of powerful operations (functions are first-class citizens after all, so the nodes that represent them are as well) a comprehensive user manual available both inside the app and online . New feature on development branch: Automated System Testing What I wanted to highlight today, though, is a feature I'm currently working on, called automated system testing , which is available in the development branch. Here's the link to a GitHub devlog post that contains a video demonstrating the feature in action: https://github.com/IndiePython/nodezator/discussions/72#discussioncomment-8997584 On the post you can also read more about the feature and how to reproduce the behaviour inside the app. Here's a brief explanation: once the user picks the tests to be performed in a dedicated form and clicks \"Start system testing session\", the app takes over and performs all the tests automatically by controlling the GUI. At the end of each test case, the app checks the state for expected values. After all test cases are performed, the app shows a report. I'm very relieved this feature is finally implemented and should soon be released (it needs a lot of polishing and is only available in the development branch, but is already functional). The reason for this is that as the project grows and becomes more and more complex, I needed to automated this kind of tests to ensure a more healthy/feasible development and more stable releases. That is, there are so many different tasks performed in the app that testing it manually takes a lot of time. As we all know, development requires a lot of iteration, so the enormous amount of time required to test everything manually would be spent many times over the implementation of even the simplest changes. Once this feature is released, we'll be able to automatically test everything and thus get feedback much earlier regarding the impact of our changes throughout the plethora of possible tasks that comprise Nodezator's workflow. Of course we also have unit tests, and they are important, but they are useful for testing units of behaviour, not their synergy when fully integrated in the working app as the user operates it. Once released, this feature will help me and other contributors check much sooner and with more precision whether our work is breaking any existing tasks/workflows/features. Sooner because the automated tests can be executed as fast as possible, as shown in the video, not only in normal time. With precision because the exact mouse and keyboard interactions are reproduced in the exact same way every time each test runs. This app is part of the Indie Python project, a personal project of mine to create and maintain completely free and open-source software (apps and games) and also release instructional/educational content related to them. Such software/content is created and maintained with much love, so please, consider supporting us if you have the means: https://patreon.com/KennedyRichard , https://github.com/sponsors/KennedyRichard , https://indiepython.com/donate . Thank you for your attention, let me know if you have any questions."},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Gmail Filter Squasher project", "Author": "u/unexpected_possum", "Content": "In Gmail I realized that I had way too many filters that performed exactly the same action, but they simply had different \"from\" addresses. To solve this I created this little project that merges similar filters into a single one. It does require a lot of setup due to having to authenticate into Gmail, but if someone is on the same situation as me, it could be helpful. Link to the project: github What My Project Does It connects into a Gmail account, reads their filters and squashes them into a single one. The program requires a manual setup of creating an OAuth credentials screen and downloading relevant credentials, which is cumbersome. But it's a one time effort as you can keep on using the application as long as you keep access to the GCP project. Note that it only merges filters if their criteria is a single \"from\" statement and they perform exactly the same action. Target Audience It's just a toy project of mine that I did for my sole benefit, but thought that it could be of use to anyone else in my same situation. Comparison I tried to look up for an already-existing solution for this issue before implementing it myself, but I couldn't find it. Relevant Gmail support post . Please do let me know if you have any suggestion or advice. Link to the project: github"},
{"Title": "I made my very first python library! It converts reddit posts to text format for feeding to LLM's!", "Author": "u/NFeruch", "Content": "Hello everyone, I've been programming for about 4 years now and this is my first ever library that I created! What My Project Does It's called Reddit2Text, and it converts a reddit post (and all its comments) into a single, clean, easy to copy/paste string. I often like to ask ChatGPT about reddit posts, but copying all the relevant information among a large amount of comments is difficult/impossible. I searched for a tool or library that would help me do this and was astonished to find no such thing! I took it into my own hands and decided to make it myself. Target Audience This project is useable in its current state, and always looking for more feedback/features from the community! Comparison There are no other similar alternatives AFAIK Here is the GitHub repo: https://github.com/NFeruch/reddit2text It's also available to download through pip/pypi :D Some basic features: Gathers the authors, upvotes, and text for the OP and every single comment Specify the max depth for how many comments you want Change the delimiter for the comment nesting Here is an example truncated output: https://pastebin.com/mmHFJtcc Under the hood, I relied heavily on the PRAW library (python reddit api wrapper) to do the actual interfacing with the Reddit API. I took it a step further though, by combining all these moving parts and raw outputs into something that's easily useable and very simple.Could you see yourself using something like this?"},
{"Title": "Would there be general interest for a library that ensure external application API call safety?", "Author": "u/SpeakerOk1974", "Content": "Python has become a popular inclusion as a scripting language for many professional software packages. However, the APIs presented by these applications may be unsafe and/or do not include pythonic error handling. As someone working in the energy sector at my day job, I often find myself wrestling issues external to my python code in our power system simulator (usually memory related) that due to the implementation of the API can error at runtime below the exception handling mechanism of the interpreter. I will have have to implement a library for this purpose either internally or as an open-source side project. Traditionally, we handle these errors by isolating all potentially unsafe API calls in their own function and running that function in a separate process with multiprocessing and a handler function orchestrating the process and handling the exit codes. Although effective, this basic solution doesn't have the level of functionality needed (no logging, no stdout/stderr redirection, no exceptions, etc) and brings a decent amount of boilerplate to simple scripts. I wanted to post here and see if this is an issue others in the community face interacting with external code written in lower level languages they cannot freely modify. This isn't a detailed proposal more so trying to generate discussion around the idea, but essentially the module would provide decorators to wrap the external calls safely, with extra features like logging, custom exception classes and more to allow pythonic handling of errors. TLDR: Would there be interest in a library centered around pain-free handling of runtime errors that happen below the exception handling mechanism with extra convenience features?"},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "PyPDFCompare PDF Comparison CLI Tool", "Author": "u/Motox2019", "Content": "First time poster here, Ill preface this post by saying I am not a software engineer by any metric. I am a hobbyist programmer and I mainly just create programs I find particularly useful in my day job as a designer. Figured I would share my handy PDF comparison tool, see if anyone else can find it useful as well. This tool is mostly designed around comparing CAD drawings. It probably isn't great for comparing text documents at the moment though I plan to add this functionality. It works by comparing the pdf's page by page in a raster image format. It creates several output files of different styles: Markup Page Differences Page Overlay Page The markup page is the main page (File 1) with outlines around any changes and some boxes to highlight larger areas. The differences page is essentially the main page - the secondary page. Any changes that were \"removed\" since the previous version end up in red and anything that was \"added\" is in blue. The overlay page is the secondary page in red with the main page slapped on top showing differences quite nicely in red. Don't judge too harshly :) https://github.com/TMan9654/PyPDFCompare"},
{"Title": "I created a way to connect your Python to LLM to provide it with debugger-level context (OpenSource)", "Author": "u/Financial_Muffin396", "Content": "I recently developed an OpenSource solution that can auto-fix production exceptions for Python servers. Think of it as Datadog, but with the added capability to refactor your code. Currently, it's suitable for experimental use, and yes based on OpenAI API. The source code is available at https://github.com/CaptureFlow/captureflow-py . What My Project Does: It incorporates a tracer client-side Python library and backend that accumulates such traces and is capable of proposing code improvements for your repository. It traverses the execution graph, extracts relevant parts of it, enriches it with implementation data from the GitHub API, and then generates suggestions using the OpenAI API. Target Audience: Python users who are interested in exploring code generation powered by production data (shoutout to DevinAI console.logging itself). Comparison: While there are not many direct comparisons, the POC use case for exception fixing is somewhat similar to Sentry's AI auto-fix feature that was released couple weeks ago. However, this library is not a replacement; it provides verbose tracing logs and will degrade your app's performance (for now). Example: For a bugfix POC, check out this toy MR and explore the source code. Feedback and stars are welcome."},
{"Title": "An efficient modeling interface for mathematical optimization: PyOptInterface", "Author": "u/c0decs", "Content": "I recently develop an efficient modeling interface for optimization problems in Python called PyOptInterface. Source code is at https://github.com/metab0t/PyOptInterface Documentation is at https://metab0t.github.io/PyOptInterface/ What My Project Does It is designed as a very thin wrapper of native C API of optimizers and attempts to provide common abstractions of an algebraic modelling environment including model, variable, constraint and expression with the least overhead of performance. Target Audience Python users that want to build and solve an optimization model Comparison The benchmark comparing PyOptInterface with some other modeling interfaces can be found here . Example This link shows an introductory example to use PyOptInterface and HiGHS to solve N-queens problem. Feedbacks and stars are welcome."},
{"Title": "The Python on Microcontrollers (and Raspberry Pi) Newsletter. Weekly news, subscribe for free", "Author": "u/HP7933", "Content": "With the Python on Microcontrollers newsletter, you get all the latest information on Python running on hardware in one place! MicroPython, CircuitPython and Python on single Board Computers like Raspberry Pi & many more. The Python on Microcontrollers newsletter is the place for the latest news. It arrives Monday morning with all the week‚Äôs happenings. No advertising, no spam, easy to unsubscribe. 10,921 subscribers - the largest Python on hardware newsletter out there. Catch all the weekly news on Python for Microcontrollers with adafruitdaily.com. This ad-free, spam-free weekly email is filled with CircuitPython, MicroPython, and Python information that you may have missed, all in one place! Ensure you catch the weekly Python on Hardware roundup‚Äì you can cancel anytime ‚Äì try our spam-free newsletter today! https://www.adafruitdaily.com/"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Python open source Projects", "Author": "u/Rare-Lion1261", "Content": "I'm seeking for python open source project where I can add things , colaborate with a community on building valuable stuff , Any good suggestions please ?"},
{"Title": "Where are you getting your Python news? (besides right here, of course)", "Author": "u/wanderingmonster", "Content": "Hello all.  I just saw this post about Pandas , which lead to a discussion of Polars being much faster.  Polars, which I had never heard about before this morning. This leads me to wonder:  where are you all getting your Python news and other information? r/Python obviously, but I fear there are some pipelines or subscriptions for Python news, information about must-have new packages, and the like that I'm currently missing."},
{"Title": "Do you make display-tables in Python? - \"The Design Philosophy of Great Tables\"", "Author": "u/economicurtis", "Content": "The maintainer of `great_tables` released an article about the design philosophy of his package https://posit-dev.github.io/great-tables/blog/design-philosophy/ Curious about your thoughts. What tools do you prefer to use when you need to share a data table based on work you've done in python?"},
{"Title": "A solver of Le Compte Est Bon and Le Mot Le Plus Long", "Author": "u/RRTheGuy", "Content": "Hi, I made a new project called dcdljeu (written in French), a solver for the games Le Compte Est Bon and Le Mot Le Plus Long from the TV Show Des Chiffres Et Des Lettres."},
{"Title": "pipxu - a faster alternative to pipx", "Author": "u/bulletmark", "Content": "What my project does pipxu installs Python applications, i.e. Python packages which have one or more executable programs, into independent isolated virtual environments on your system. Each package and it's dependencies are thus insulated from all other applications, and from the system Python. pipxu creates links to application executables in a common directory, which you have in your PATH . Packages are typically sourced from PyPI , the Python Package Index. Target Audience Python users and developers. Comparison pipxu is a re-implementation of most of the functionality of the popular pipx tool but is much faster because it uses uv to create and install application virtual environments instead of venv and pip as used by pipx . Read more at https://github.com/bulletmark/pipxu"},
{"Title": "UXsim 1.1.0 released: Network traffic flow simulator gets Multilane support", "Author": "u/Balance-", "Content": "Version 1.1.0 of UXsim is released, which now supports modeling multilane ways. Main Changes Add support for multilane links. More technically, it is a multilane, single-pipe model where vehicles cannot overtake others. This allows us to set traffic capacity significantly larger while keeping consistency to KW theory. Separate Analyzer class from uxsim.py. This means that uxsim.py now contains only the essential codes for the simulation. It makes it easier for users to understand the simulation logic. UXsim UXsim is a free, open-source macroscopic and mesoscopic network traffic flow simulator written in Python. It simulates the movements of car travelers and traffic congestion in road networks. It is suitable for simulating large-scale (e.g., city-scale) traffic phenomena. UXsim is especially useful for scientific and educational purposes because of its simple, lightweight, and customizable features, but users are free to use UXsim for any purpose."},
{"Title": "Does it really matter what order you import modules in python?", "Author": "u/lizziemoon89", "Content": "I am trying to understand why anyone would care whether you import math or numpy first. I am told it is good practise but it just seem pointless to care."},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "XZ Vulnerability Anaconda", "Author": "u/jimtoberfest", "Content": "Is the xz library compromised along with the xz Linux utility? Anaconda seems to auto load this library upon creation of new venvs."},
{"Title": "Nava: Python Light-Weight Sound-Player Library", "Author": "u/sepandhaghighi", "Content": "GitHub Repo: https://github.com/openscilab/nava What My Project Does: Nava is a Python library that allows users to play sound in Python without any dependencies or platform restrictions. It is a cross-platform solution that runs on any operating system, including Windows, macOS, and Linux. Its lightweight and easy-to-use design makes Nava an ideal choice for developers looking to add sound functionality to their Python programs. import time from nava import play, stop sound_id = play(\"alarm.wav\", async_mode=True, loop=False) time.sleep(4) stop(sound_id) Target Audience: Developers who are looking to add sound functionality to their Python programs Comparison: Light-weight and zero-dependency against pygame library. It can be inconvenient to install the pygame library just for playing sounds! playsound is deprecated and it's impossible to use it in modern environments. It also has some bugs on macOS and doesn't support the stop function."},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Docx2Python 2.10.0 will now extract comments from Word files", "Author": "u/Shay-Hill", "Content": "access comments with Docx2Python You can access docx comments with the comments attribute of the output DocxContent object. with docx2python('path/to/file.docx') as docx_content: print(docx_content.comments) For each comment, this will return a tuple: (reference_text, author, date, comment_text) https://github.com/ShayHill/docx2python"},
{"Title": "Who's going to PyCon US 2024?", "Author": "u/frocketgaming", "Content": "It's just around the corner. I'll be going on my own so it would be nice to meet up with some people potentially. Who plans to go and what speakers / sessions are you looking forward to?"},
{"Title": "Serverless Python Support in Cloudflare Workers", "Author": "u/TheMadFratter", "Content": "Today Cloudflare has announced support for Python in their Clouldflare Workers serverless environment. This allows Python to be run at the edge on their global network using a combination of Pyodine and the V8 runtime. Announcement can be found on the Cloudflare Blog: https://blog.cloudflare.com/python-workers"},
{"Title": "Security Code Challenge for Developers & Ethical Hackers - Damn Vulnerable RESTaurant API", "Author": "u/theowni", "Content": "Security Code Challenge for Developers & Ethical Hackers ‚Äì Damn Vulnerable RESTaurant A FastAPI based intentionally vulnerable web application teaching the most common security vulnerabilities in API through a dedicated game. Damn Vulnerable RESTaurant is operated by a mysterious Chef who has discovered that threat actors were able to compromise his restaurant‚Äôs API and the underlying system. He suspects that a competing restaurant located across the street might be involved in this attack! The goal of the challenge is to identify and fix vulnerabilities based on provided hints. During this adventure, you will have the opportunity to investigate how the attack was carried out and fix security issues to safeguard the application. Moreover, you can uncover the identity of the person behind the attack by the end of this adventure."},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "pointers.py being added to the standard library!", "Author": "u/ZeroIntensity", "Content": "As of PEP 4124 being accepted, the infamous pointers.py will be added to Python's standard library in 3.13! To quote Guido van Rossum's take on adding this, \"Why the hell not?\" This will also introduce pointer literals , the sizeof operator , and memory errors! from pointers import malloc ptr = &\"spam\"  # Pointer literal print(*ptr) mem = malloc(?\"hello\")  # New sizeof operator print(*mem)  # MemoryError: junk 13118820 6422376 4200155 at 0x7649f65a9670 # MemoryWarning: leak at 0x7649f65a9670 However, it was decided in this discussion that segfaults would be added to the language for \"extra flavor\": spam = *None # Segmentation fault, core dumped. Good luck, kiddo."},
{"Title": "Python isn't dramatic enough", "Author": "u/treyhunner", "Content": "Ever wished your Python interpreter had the dramatic feeling of a 300 baud modem connection? Today there's a solution: pip install dramatic dramatic on PyPI dramatic on GitHub What My Project Does All text output by Python will print character-by-character. It works as a context manager, as a decorator, or as a simple function call. Other features include a dramatic REPL, ability to run specific Python modules/scripts dramatically, and a --max-drama argument to make all Python programs dramatic all the time. Target Audience Those seeking amusement. Comparison Just like Python usually runs, but with the feeling that you're inside a text-based adventure game."},
{"Title": "geojson-shave: a command-line tool for reducing the file size of GeoJSON files", "Author": "u/Ok-Frosting7364", "Content": "Source code : https://github.com/ben-n93/geojson-shave What my Project Does Reduces the file size of GeoJSON files by truncating coordinates to the specified decimal place, eliminating whitespace and (optionally) replacing the property key with an empty dictionary. Target Audience For anyone that works with geospatial data (and specifically GeoJSON files) - so data analysts, data engineers, data journalists, GIS professionals, etc. Comparison There is a website that reduces the size of GeoJSON files, however it didn't work for me when I tested it. The advantage of my tool is that you can run it from the command-line and don't need an Internet connection."},
{"Title": "Drawpyo: create Draw.io diagrams with Python", "Author": "u/MerrimanIndustries", "Content": "This is a project that I initially created in a weekend for my own use after I was surprised to find out there was no easy way to generate Draw.io diagrams in Python. I'm a huge fan of Draw.io for documentation since it's free, lightweight, and the files are plaintext. After building basic initial functionality I decided to use this as a project to make a more mature Python library out of something I initially wrote for myself quick n dirty. I refactored (a lot), wrote docs, tests, build logic, etc. There's still a lot to do to make it as robust as it could be but I think it's good enough to share! GitHub Page Docs What My Project Does: There's some basic functionality that allows you to manually create Draw.io objects and edges, pre-format them from libraries, and lay them out. I've also started to implement more automated diagram types that extend that basic functionality for specific use cases. The ony currently released diagram type is a TreeDiagram, that allows you to define a tree structure with parent and children nodes, then auto generate a nice looking layout. For example, this code for a tree diagram generates this diagram ! Or for something a little more manual, this code for a flowchart generates this diagram . Target Audience: I use Draw.io for everything so your imagination is the limit. But I think this would be the best fit for auto generating documentation, specifically that which needs to be read by non-technical or non-programmers. Comparison: I couldn't find any other Python libraries for creating Draw.io graphs. There are other graphing options, most based on Graphviz. The advantage of Draw.io here is the ability to have finer control over formatting and have a great desktop UI for working with the diagrams. edit: Drawpyo is now on PIP ! Thanks for the push everyone."},
{"Title": "The Best* Python Cheat Sheet", "Author": "u/kmhnz", "Content": "A dense Python cheat sheet with just what you need. Design principles: ‚Ä¢ Focus on Python core ‚Ä¢ Comprehensive but selective (Just what you need) ‚Ä¢ Densely packed ‚Ä¢ Well-linked ‚Ä¢ Linkable ‚Ä¢ Responsive ‚Ä¢ Printable Issues and feedback are tracked at the best-python-cheat-sheet repository . *It may not be the best Python cheat sheet, but it aspires to be."},
{"Title": "I created a way to connect your Python to LLM to provide it with debugger-level context (OpenSource)", "Author": "u/Financial_Muffin396", "Content": "I recently developed an OpenSource solution that can auto-fix production exceptions for Python servers. Think of it as Datadog, but with the added capability to refactor your code. Currently, it's suitable for experimental use, and yes based on OpenAI API. The source code is available at https://github.com/CaptureFlow/captureflow-py . What My Project Does: It incorporates a tracer client-side Python library and backend that accumulates such traces and is capable of proposing code improvements for your repository. It traverses the execution graph, extracts relevant parts of it, enriches it with implementation data from the GitHub API, and then generates suggestions using the OpenAI API. Target Audience: Python users who are interested in exploring code generation powered by production data (shoutout to DevinAI console.logging itself). Comparison: While there are not many direct comparisons, the POC use case for exception fixing is somewhat similar to Sentry's AI auto-fix feature that was released couple weeks ago. However, this library is not a replacement; it provides verbose tracing logs and will degrade your app's performance (for now). Example: For a bugfix POC, check out this toy MR and explore the source code. Feedback and stars are welcome."},
{"Title": "An efficient modeling interface for mathematical optimization: PyOptInterface", "Author": "u/c0decs", "Content": "I recently develop an efficient modeling interface for optimization problems in Python called PyOptInterface. Source code is at https://github.com/metab0t/PyOptInterface Documentation is at https://metab0t.github.io/PyOptInterface/ What My Project Does It is designed as a very thin wrapper of native C API of optimizers and attempts to provide common abstractions of an algebraic modelling environment including model, variable, constraint and expression with the least overhead of performance. Target Audience Python users that want to build and solve an optimization model Comparison The benchmark comparing PyOptInterface with some other modeling interfaces can be found here . Example This link shows an introductory example to use PyOptInterface and HiGHS to solve N-queens problem. Feedbacks and stars are welcome."},
{"Title": "The Python on Microcontrollers (and Raspberry Pi) Newsletter. Weekly news, subscribe for free", "Author": "u/HP7933", "Content": "With the Python on Microcontrollers newsletter, you get all the latest information on Python running on hardware in one place! MicroPython, CircuitPython and Python on single Board Computers like Raspberry Pi & many more. The Python on Microcontrollers newsletter is the place for the latest news. It arrives Monday morning with all the week‚Äôs happenings. No advertising, no spam, easy to unsubscribe. 10,921 subscribers - the largest Python on hardware newsletter out there. Catch all the weekly news on Python for Microcontrollers with adafruitdaily.com. This ad-free, spam-free weekly email is filled with CircuitPython, MicroPython, and Python information that you may have missed, all in one place! Ensure you catch the weekly Python on Hardware roundup‚Äì you can cancel anytime ‚Äì try our spam-free newsletter today! https://www.adafruitdaily.com/"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Python open source Projects", "Author": "u/Rare-Lion1261", "Content": "I'm seeking for python open source project where I can add things , colaborate with a community on building valuable stuff , Any good suggestions please ?"},
{"Title": "Where are you getting your Python news? (besides right here, of course)", "Author": "u/wanderingmonster", "Content": "Hello all.  I just saw this post about Pandas , which lead to a discussion of Polars being much faster.  Polars, which I had never heard about before this morning. This leads me to wonder:  where are you all getting your Python news and other information? r/Python obviously, but I fear there are some pipelines or subscriptions for Python news, information about must-have new packages, and the like that I'm currently missing."},
{"Title": "Do you make display-tables in Python? - \"The Design Philosophy of Great Tables\"", "Author": "u/economicurtis", "Content": "The maintainer of `great_tables` released an article about the design philosophy of his package https://posit-dev.github.io/great-tables/blog/design-philosophy/ Curious about your thoughts. What tools do you prefer to use when you need to share a data table based on work you've done in python?"},
{"Title": "A solver of Le Compte Est Bon and Le Mot Le Plus Long", "Author": "u/RRTheGuy", "Content": "Hi, I made a new project called dcdljeu (written in French), a solver for the games Le Compte Est Bon and Le Mot Le Plus Long from the TV Show Des Chiffres Et Des Lettres."},
{"Title": "pipxu - a faster alternative to pipx", "Author": "u/bulletmark", "Content": "What my project does pipxu installs Python applications, i.e. Python packages which have one or more executable programs, into independent isolated virtual environments on your system. Each package and it's dependencies are thus insulated from all other applications, and from the system Python. pipxu creates links to application executables in a common directory, which you have in your PATH . Packages are typically sourced from PyPI , the Python Package Index. Target Audience Python users and developers. Comparison pipxu is a re-implementation of most of the functionality of the popular pipx tool but is much faster because it uses uv to create and install application virtual environments instead of venv and pip as used by pipx . Read more at https://github.com/bulletmark/pipxu"},
{"Title": "UXsim 1.1.0 released: Network traffic flow simulator gets Multilane support", "Author": "u/Balance-", "Content": "Version 1.1.0 of UXsim is released, which now supports modeling multilane ways. Main Changes Add support for multilane links. More technically, it is a multilane, single-pipe model where vehicles cannot overtake others. This allows us to set traffic capacity significantly larger while keeping consistency to KW theory. Separate Analyzer class from uxsim.py. This means that uxsim.py now contains only the essential codes for the simulation. It makes it easier for users to understand the simulation logic. UXsim UXsim is a free, open-source macroscopic and mesoscopic network traffic flow simulator written in Python. It simulates the movements of car travelers and traffic congestion in road networks. It is suitable for simulating large-scale (e.g., city-scale) traffic phenomena. UXsim is especially useful for scientific and educational purposes because of its simple, lightweight, and customizable features, but users are free to use UXsim for any purpose."},
{"Title": "Does it really matter what order you import modules in python?", "Author": "u/lizziemoon89", "Content": "I am trying to understand why anyone would care whether you import math or numpy first. I am told it is good practise but it just seem pointless to care."},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "XZ Vulnerability Anaconda", "Author": "u/jimtoberfest", "Content": "Is the xz library compromised along with the xz Linux utility? Anaconda seems to auto load this library upon creation of new venvs."},
{"Title": "Nava: Python Light-Weight Sound-Player Library", "Author": "u/sepandhaghighi", "Content": "GitHub Repo: https://github.com/openscilab/nava What My Project Does: Nava is a Python library that allows users to play sound in Python without any dependencies or platform restrictions. It is a cross-platform solution that runs on any operating system, including Windows, macOS, and Linux. Its lightweight and easy-to-use design makes Nava an ideal choice for developers looking to add sound functionality to their Python programs. import time from nava import play, stop sound_id = play(\"alarm.wav\", async_mode=True, loop=False) time.sleep(4) stop(sound_id) Target Audience: Developers who are looking to add sound functionality to their Python programs Comparison: Light-weight and zero-dependency against pygame library. It can be inconvenient to install the pygame library just for playing sounds! playsound is deprecated and it's impossible to use it in modern environments. It also has some bugs on macOS and doesn't support the stop function."},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Docx2Python 2.10.0 will now extract comments from Word files", "Author": "u/Shay-Hill", "Content": "access comments with Docx2Python You can access docx comments with the comments attribute of the output DocxContent object. with docx2python('path/to/file.docx') as docx_content: print(docx_content.comments) For each comment, this will return a tuple: (reference_text, author, date, comment_text) https://github.com/ShayHill/docx2python"},
{"Title": "Who's going to PyCon US 2024?", "Author": "u/frocketgaming", "Content": "It's just around the corner. I'll be going on my own so it would be nice to meet up with some people potentially. Who plans to go and what speakers / sessions are you looking forward to?"},
{"Title": "Serverless Python Support in Cloudflare Workers", "Author": "u/TheMadFratter", "Content": "Today Cloudflare has announced support for Python in their Clouldflare Workers serverless environment. This allows Python to be run at the edge on their global network using a combination of Pyodine and the V8 runtime. Announcement can be found on the Cloudflare Blog: https://blog.cloudflare.com/python-workers"},
{"Title": "Security Code Challenge for Developers & Ethical Hackers - Damn Vulnerable RESTaurant API", "Author": "u/theowni", "Content": "Security Code Challenge for Developers & Ethical Hackers ‚Äì Damn Vulnerable RESTaurant A FastAPI based intentionally vulnerable web application teaching the most common security vulnerabilities in API through a dedicated game. Damn Vulnerable RESTaurant is operated by a mysterious Chef who has discovered that threat actors were able to compromise his restaurant‚Äôs API and the underlying system. He suspects that a competing restaurant located across the street might be involved in this attack! The goal of the challenge is to identify and fix vulnerabilities based on provided hints. During this adventure, you will have the opportunity to investigate how the attack was carried out and fix security issues to safeguard the application. Moreover, you can uncover the identity of the person behind the attack by the end of this adventure."},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "pointers.py being added to the standard library!", "Author": "u/ZeroIntensity", "Content": "As of PEP 4124 being accepted, the infamous pointers.py will be added to Python's standard library in 3.13! To quote Guido van Rossum's take on adding this, \"Why the hell not?\" This will also introduce pointer literals , the sizeof operator , and memory errors! from pointers import malloc ptr = &\"spam\"  # Pointer literal print(*ptr) mem = malloc(?\"hello\")  # New sizeof operator print(*mem)  # MemoryError: junk 13118820 6422376 4200155 at 0x7649f65a9670 # MemoryWarning: leak at 0x7649f65a9670 However, it was decided in this discussion that segfaults would be added to the language for \"extra flavor\": spam = *None # Segmentation fault, core dumped. Good luck, kiddo."},
{"Title": "Python isn't dramatic enough", "Author": "u/treyhunner", "Content": "Ever wished your Python interpreter had the dramatic feeling of a 300 baud modem connection? Today there's a solution: pip install dramatic dramatic on PyPI dramatic on GitHub What My Project Does All text output by Python will print character-by-character. It works as a context manager, as a decorator, or as a simple function call. Other features include a dramatic REPL, ability to run specific Python modules/scripts dramatically, and a --max-drama argument to make all Python programs dramatic all the time. Target Audience Those seeking amusement. Comparison Just like Python usually runs, but with the feeling that you're inside a text-based adventure game."},
{"Title": "geojson-shave: a command-line tool for reducing the file size of GeoJSON files", "Author": "u/Ok-Frosting7364", "Content": "Source code : https://github.com/ben-n93/geojson-shave What my Project Does Reduces the file size of GeoJSON files by truncating coordinates to the specified decimal place, eliminating whitespace and (optionally) replacing the property key with an empty dictionary. Target Audience For anyone that works with geospatial data (and specifically GeoJSON files) - so data analysts, data engineers, data journalists, GIS professionals, etc. Comparison There is a website that reduces the size of GeoJSON files, however it didn't work for me when I tested it. The advantage of my tool is that you can run it from the command-line and don't need an Internet connection."},
{"Title": "Drawpyo: create Draw.io diagrams with Python", "Author": "u/MerrimanIndustries", "Content": "This is a project that I initially created in a weekend for my own use after I was surprised to find out there was no easy way to generate Draw.io diagrams in Python. I'm a huge fan of Draw.io for documentation since it's free, lightweight, and the files are plaintext. After building basic initial functionality I decided to use this as a project to make a more mature Python library out of something I initially wrote for myself quick n dirty. I refactored (a lot), wrote docs, tests, build logic, etc. There's still a lot to do to make it as robust as it could be but I think it's good enough to share! GitHub Page Docs What My Project Does: There's some basic functionality that allows you to manually create Draw.io objects and edges, pre-format them from libraries, and lay them out. I've also started to implement more automated diagram types that extend that basic functionality for specific use cases. The ony currently released diagram type is a TreeDiagram, that allows you to define a tree structure with parent and children nodes, then auto generate a nice looking layout. For example, this code for a tree diagram generates this diagram ! Or for something a little more manual, this code for a flowchart generates this diagram . Target Audience: I use Draw.io for everything so your imagination is the limit. But I think this would be the best fit for auto generating documentation, specifically that which needs to be read by non-technical or non-programmers. Comparison: I couldn't find any other Python libraries for creating Draw.io graphs. There are other graphing options, most based on Graphviz. The advantage of Draw.io here is the ability to have finer control over formatting and have a great desktop UI for working with the diagrams. edit: Drawpyo is now on PIP ! Thanks for the push everyone."},
{"Title": "The Best* Python Cheat Sheet", "Author": "u/kmhnz", "Content": "A dense Python cheat sheet with just what you need. Design principles: ‚Ä¢ Focus on Python core ‚Ä¢ Comprehensive but selective (Just what you need) ‚Ä¢ Densely packed ‚Ä¢ Well-linked ‚Ä¢ Linkable ‚Ä¢ Responsive ‚Ä¢ Printable Issues and feedback are tracked at the best-python-cheat-sheet repository . *It may not be the best Python cheat sheet, but it aspires to be."},
{"Title": "Python Inner class feature", "Author": "u/koldakov", "Content": "test.py import sys sys.setrecursionlimit(4) class Foo: class Foo: foo = 1 python test.py Please be honest, write your guess in comments about the output of this code before opening the spoiler =) The answer: RecursionError: maximum recursion depth exceeded Did you know?)"},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "AnyPathLib - A unified PathLib-like API for cloud storages", "Author": "u/kfirgold99", "Content": "Hi everybody, I'm happy to share AnyPathLib üõ£Ô∏è, a pip package I created to simplify access to different storage resources - S3, Azure, and local storage. What My Project Does: Basically, instead of writing custom code to handle the different storage resources using the SDK (boto3, azure-sdk), you can now just use AnyPathLib: from anypathlib import AnyPath # Create an AnyPath instance for a local file local_file = AnyPath(\"/path/to/local/file.txt\") # Create an AnyPath instance for an S3 object s3_file = AnyPath(\"s3://bucket/path/to/object.txt\") # Copy a file from local to S3 local_file.copy(s3_file) # Copy a directory from S3 to Azure s3_dir = AnyPath(\"s3://bucket/path/to/dir\") azure_dir = AnyPath(\"https://account_name.blob.core.windows.net/container_name/path\") s3_dir.copy(azure_dir) There are other cool features to save you some time, so check out this X thread for some more details or visit the Github project page . Target Audience: Anybody which works with both a local environment and Azure or AWS, and could use a simplified API to access their storage Comparison: I didn't find any similar library (that's why I wrote this one) - there are dedicated packages for S3 (boto3) and the Azure SDK, but they come with different, and somewhat unintuitive APIs. I'd love to hear your feedback, issues, and of course - PRs :)"},
{"Title": "Portr: Open source self hosted ngrok alternative designed for teams", "Author": "u/ShiftDefeat", "Content": "https://github.com/amalshaji/portr What my project does? Portr is a tunnelling solution that let's you expose local http/tcp connections to the public internet. Target audience Developers that work with webhooks or callbacks. Or anyone who want to quickly share their dev server with others. Comparison Monitor connections, and manage teams and members via the admin dashboard. Watch video Inspect and replay HTTP requests using portr inspector. Watch video . The admin backend is built using Python. I would appreciate a feedback."},
{"Title": "Better PDF Layout Detection for LLM's + table support", "Author": "u/serjester4", "Content": "What's the first step to building 90% of AI applications? Chunking your documents! Unfortunately there's very few libraries for this and most rely on naively splitting text into N characters. You lose so much valuable information hidden in the layout of the document when you do this. If you give an LLM bad inputs, you're going to get bad results. https://github.com/Filimoa/open-parse/ What it does: The library can detect the layout of a PDF, convert to markdown and extract tables into html in one line of code. You can then feed this directly into your vector database or something like LlamaIndex.  It does all this while preserving bounding boxes so you can display citations directly to the user. Target Audience: Anyone building a RAG AI app.  I run a talk-to-your-file website that's seen millions of documents and this library represents a giant chunk of what I know about feeding documents to LLM's. Comparison: Open source libraries like LlamaIndex chunk your documents into strings of fixed length.  There's a couple commercial solutions that do this but they're expensive (typically 100 pages / dollar) and honestly have less than ideal results (AWS Texttract, Adobe, Google). Facebook has nougat for converting files to markdown but it's incredibly slow on anything without a massive GPU."},
{"Title": "Genuine question: Where should I share my opinion about Python language?", "Author": "u/HalfAByteIsWord", "Content": "I recently posted my opinions on the import system in Python and how it can be made more intuitive and straightforward. I got some comments, some of them are just snappy as usual and some of them had some suggestions and workarounds. Either way the post got removed, citing that it is not suitable for this sub-reddit. I mean, come on, I cannot talk about Python features in Python sub-reddit? Update: To mods: Thanks for letting me have this conversation. To others who made helpful comments, thank you. Original post: I don't want to recreate the post verbatim here, I don't remember it exactly too. It was basically something like this, *requiring* or *loading* code, which is more or less importing can be much more straight forward if we can directly load files. Instead of the concept of packages, each file is a self contained module which can import and export classes, variables and functions. For example, src/constants.py ADMIN_ROLE_NAME = 'admin' src/user.py from constants import ADMIN_ROLE_NAME tests/user_test.py from ../../src/constants.py import ADMIN_ROLE_NAME Here there would be no __init__.py files, just straightforward imports directly from files. I would refrain from giving example from Ruby or JS, like I did in the original post, because people perceive it as a fan boy post, while my intention is not that. I'll give a real example, Here is my repo, which https://github.com/rajaravivarma-r/python-robyn-react I tried to put the contents of the `constants.py` file into `src/__init__.py` but there is no way of me importing them from within the child directory package, namely `src/api/__init__.py`. Backward compatibility: Lot of you have mentioned about how this would break existing code, but C++ standard had a recommendation to introduce module systems to new code which goes like this. Add a pragma like #pragma C++14_Modules at the top of the file, or something like that, but you get the idea. Ruby has a # frozen_string_literal: true So Python could introduce something like that for new code, while treating the rest of the old code as it is. I believe this is constructive enough. I know this is not a PEP, but I want to show the community that my intention was genuine and not a rant. Outro: Assume ignorance rather than malevolence. Your snarky comments or aggressive down voting doesn't improve anyone's life. Last time when I posted about some surprising behaviour of Python in this community (from my old account), I got a comment about my daughter and wife. It wasn't even deleted despite reporting."},
{"Title": "Wrote a Python script for downloading images/GIFs from subreddits in bulk.", "Author": "u/MonsieurKebab", "Content": "What my Project Does It's a program that searches through posts in a specified subreddit and downloads images, gifs etc. Target Audience It's a toy project at this point, but I might extend its functionality if I can find good resources. Maybe even add support for different websites (imgur et al.). Comparison I found a few websites for downloading images from reddit but they claimed they didn't work because of the Reddit API changes, I was sceptical and wrote my own script. Anyhow, For the last few days I was trying to find good wallpapers for my system. I wrote a Python script for getting images and gifs from subreddits, here is the project repo. Please read the instructions before using it. It's pretty basic but gets the job done. I am still working on the project. I also wrote a flake.nix for my NixOS bros. Didn't test the project on Windows but I don't see why it wouldn't work. Lastly, here is my NixOS config in case  you are interested. Enjoy"},
{"Title": "How to parallelize Pandas with Modin", "Author": "u/linchpiner", "Content": "Discover the power of Modin in overcoming the performance bottlenecks of Pandas. We'll walk you through how a simple switch can reduce your waiting times and make your workflow smooth and speedy, all without leaving the comfort of Pandas behind. Modin is an active  open source project. Take a look to this blog post to see if it is applicable for your code: https://dchigarev.github.io/modin_perf_examples/"},
{"Title": "Does anyone actually use PyPy or Cython?", "Author": "u/Sc0urge_", "Content": "I have been reading a lot about these tools recently and wondered if they are used in production, especially when building a backend with Python. As far as I know, PyPy is not ideal because of its lack of package support, but Cython seems pretty neat."},
{"Title": "Futurama API written in async Python", "Author": "u/koldakov", "Content": "I've created a simple API service https://github.com/koldakov/futuramaapi to access Futurama units with async Python + FastAPI + SQLAlchemy (PostgreSQL). I've chosen Hypercorn for a web server to achieve HTTP/2 support - considering I didn't find yet cheap hosting that supports HTTP/2. Heroku doesn't support, render doesn't support . Also it has some code related to GraphQL (I used strawberry). + I didn't find a lot of discussions about Heroku + FastAPI - the problem will be with redirect to https. As default fastapi (starlette) middleware doesn't work with proxies there will be an infinite loop if you try to use fastapi.middleware.httpsredirect.HTTPSRedirectMiddleware . Chain will be: Client https-> Proxy(Hypercorn, for example) http-> fastapi, in that case as you can see the request is secure, but fastapi (starlette) won't understand this and will redirect to https. Here is a simple workaround to support https under proxy: https://github.com/koldakov/futuramaapi/blob/main/app/middlewares/secure.py Constructive feedback is greatly appreciated. Source code: https://github.com/koldakov/futuramaapi Working example: https://futuramaapi.com"},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "remove background from image using AI in just 5 lines of python code", "Author": "u/Maleficent_Yak_993", "Content": "I created a python library \"dis-bg-remover\" based off the \"Highly Accurate Dichotomous Image Segmentation ( https://arxiv.org/pdf/2203.03041.pdf ), whose results are comparable, if not better, to the premium offerings in the market. Explainer video here https://www.youtube.com/watch?v=js7AYKkZvFI"},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "SQLModel vs native SQL Alchemy ORM for a web backend?", "Author": "u/Raz_Crimson", "Content": "Hi there I was tasked with migrating from mongoDb to PostgreSQL due to the performance issues. The backend is FastAPI-based web server for a mobile APP with a decent amount if CRUD. We also heavily use Pydantic for data validation and the primary carried of data across the service layers Would like to hear opinions from SQLModel users as to how easy it is to use or what kind of pitfalls am i setting myself up by using it.  Last post that I could find was years old and even that was not really conclusive. Now SQLAlchemy 2.0 is released and even SQLModel has support for Pydantic V2, so would like some newer thoughts in this topic. Our datamodel includes quite a bit of top level nestings, which I would prefer to be stored as `JSON` columns ( Similar SO Question ). SQLModel doesn't seem to have any documentation for such cases. If possible I would prefer avoiding to write and maintain our own internal library with SQLAlchemy ORM to perform these conversions. But maybe just using plain SQLAlchemy and Pydantic might be a good solution too. Seeking inputs from more experienced users. Thank you. Edit: In the end I gave up on using orm to query data and just used plain SQLAlchemy queries. The reason was that the ORM layer already abstract lot of stuff from us and SQLModel makes it more abstracted. This is fine if you know all the details of the abstractions. But at the point I am in, where I am new to sqlalchemy, it just adds lots more complexity and gotchas which can just avoid with plain queries. Maybe better documentation might help down the line but as of now, I am leaning towards just plain sqlqlchemy  core. Also using the ORM layer to just auto generate revisions is a nice feature."},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Automating Python with Google Cloud", "Author": "u/neb2357", "Content": "I just published a tutorial series on how to automate a Python script in Google Cloud using Cloud Functions and/or Cloud Run. Feedback would be great. Thanks! Automating Python with Google Cloud Automating Python with Google Cloud Functions Automating Python with Google Cloud Run"},
{"Title": "Python state management", "Author": "u/Extreme-Acid", "Content": "Hey all, I love what Django has with django-fsm. I require something but without Django, as there is no user interaction with the workflow. All inputs and outputs are either rabbitmq or another api. This is to be a workflow management system. I am looking for state management backed up by database so we can not suffer if a k8s pod dies with all the states in memory. Some of our workflows could take weeks. Is it still best to make this in Django or is there a database backed state management module available? I see pytransitions but I would have to add database logging to it."},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Creating a GeoTIFF raster XYZ tile service in python with caching capability", "Author": "u/iamgeoknight", "Content": "Creating a GeoTIFF raster XYZ tile service in python with caching capability"},
{"Title": "Makefile Parser for Python", "Author": "u/Cybasura", "Content": "Hello everyone! I am proud to introduce a Makefile Parser for Python that I think will be useful! the link is Thanatisia/makefile-parser-python As an introduction, I have been using python and been following this subreddit for quite awhile now, but this is my first post Recently, I've been planning a side-project involving the use of Makefiles in Python, and I required the use of a Makefile parser ala json or pyyaml - whereby you would import a Makefile into python as objects/dictionary/lists. Hence, I started searching for a Makefile Parser. The only parser I've found is PyMake(2) which is cool but it hasnt been updated since 2017 from what I recall and that it is more of a CLI utility, so with that in mind, I went about to make it I hope that this will be as useful as it is for me, currently I am using this in a side project and i'm able to format it such that its printing out a typical Makefile structure right off the bat, which is pretty nice. Additional information to the project What My Project Does This is a Makefile Parser, made in Python. The operational workflow is basically Start --> Import File into dictionary --> Manipulation and Usage --> Processing --> Output --> Export Target Audience (e.g., Is it meant for production, just a toy project, etc.) This is a library/package/module by design, much like json or pyyaml as mentioned but a smaller scale at the moment as its just started. I'm not sure if it applies for you but its a parser/importer Comparison (A brief comparison explaining how it differs from existing alternatives.) I'm not sure if there are any other Makefile Parsers other than Pymake2, but the idea is similar to the aforementioned ideas"},
{"Title": "Load Apple's .numbers Files into Pandas", "Author": "u/nez_har", "Content": "I recently ran into some challenges while trying to work with Apple's .numbers files on Linux. After a bit of experimentation, I figured out a workflow that simplifies the process. If you're planning to use .numbers files and need to load them into pandas, I've created a tutorial that covers the required dependencies and the steps to follow: https://nezhar.com/blog/load-apple-numbers-files-python-pandas-using-containers/ . Has anyone else here worked with .numbers files in Python? I‚Äôd love to hear about your experiences or any tips you might have."},
{"Title": "No More Cutting in Line: Crafting Fairness in Queues using RQ in Python", "Author": "u/soap94", "Content": "I was trying to tackle the problem of queue fairness and here's how I solved it."},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Type-Level Programming: a POC", "Author": "u/Kiuhnm", "Content": "While Python doesn't explicitly support Type-Level Programming , I've had some success with it. I think this may be of interest to the Python community, so I'm sharing a POC (Proof Of Concept) I've written. This POC is a statically typed list that encodes its length in its type so that, for instance, when you concatenate two lists, the result has the correct length, all done at type-checking time. Read more on GitHub"},
{"Title": "Open Jupyter Notebooks in Kaggle? (from GitHub)", "Author": "u/pbeens", "Content": "I have a repo in GitHub with Jupyter Notebooks for students. On each notebook I have a link where the student can open the notebook in Colab (by changing the URL from https://github.com/ to https://githubtocolab.com/ ). The problem is that some school boards have Colab blocked. Is there a similar technique I can use to open the notebooks in Kaggle? Are there other sites I should be considering? Thanks!"},
{"Title": "Python Inner class feature", "Author": "u/koldakov", "Content": "test.py import sys sys.setrecursionlimit(4) class Foo: class Foo: foo = 1 python test.py Please be honest, write your guess in comments about the output of this code before opening the spoiler =) The answer: RecursionError: maximum recursion depth exceeded Did you know?)"},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "AnyPathLib - A unified PathLib-like API for cloud storages", "Author": "u/kfirgold99", "Content": "Hi everybody, I'm happy to share AnyPathLib üõ£Ô∏è, a pip package I created to simplify access to different storage resources - S3, Azure, and local storage. What My Project Does: Basically, instead of writing custom code to handle the different storage resources using the SDK (boto3, azure-sdk), you can now just use AnyPathLib: from anypathlib import AnyPath # Create an AnyPath instance for a local file local_file = AnyPath(\"/path/to/local/file.txt\") # Create an AnyPath instance for an S3 object s3_file = AnyPath(\"s3://bucket/path/to/object.txt\") # Copy a file from local to S3 local_file.copy(s3_file) # Copy a directory from S3 to Azure s3_dir = AnyPath(\"s3://bucket/path/to/dir\") azure_dir = AnyPath(\"https://account_name.blob.core.windows.net/container_name/path\") s3_dir.copy(azure_dir) There are other cool features to save you some time, so check out this X thread for some more details or visit the Github project page . Target Audience: Anybody which works with both a local environment and Azure or AWS, and could use a simplified API to access their storage Comparison: I didn't find any similar library (that's why I wrote this one) - there are dedicated packages for S3 (boto3) and the Azure SDK, but they come with different, and somewhat unintuitive APIs. I'd love to hear your feedback, issues, and of course - PRs :)"},
{"Title": "Portr: Open source self hosted ngrok alternative designed for teams", "Author": "u/ShiftDefeat", "Content": "https://github.com/amalshaji/portr What my project does? Portr is a tunnelling solution that let's you expose local http/tcp connections to the public internet. Target audience Developers that work with webhooks or callbacks. Or anyone who want to quickly share their dev server with others. Comparison Monitor connections, and manage teams and members via the admin dashboard. Watch video Inspect and replay HTTP requests using portr inspector. Watch video . The admin backend is built using Python. I would appreciate a feedback."},
{"Title": "Better PDF Layout Detection for LLM's + table support", "Author": "u/serjester4", "Content": "What's the first step to building 90% of AI applications? Chunking your documents! Unfortunately there's very few libraries for this and most rely on naively splitting text into N characters. You lose so much valuable information hidden in the layout of the document when you do this. If you give an LLM bad inputs, you're going to get bad results. https://github.com/Filimoa/open-parse/ What it does: The library can detect the layout of a PDF, convert to markdown and extract tables into html in one line of code. You can then feed this directly into your vector database or something like LlamaIndex.  It does all this while preserving bounding boxes so you can display citations directly to the user. Target Audience: Anyone building a RAG AI app.  I run a talk-to-your-file website that's seen millions of documents and this library represents a giant chunk of what I know about feeding documents to LLM's. Comparison: Open source libraries like LlamaIndex chunk your documents into strings of fixed length.  There's a couple commercial solutions that do this but they're expensive (typically 100 pages / dollar) and honestly have less than ideal results (AWS Texttract, Adobe, Google). Facebook has nougat for converting files to markdown but it's incredibly slow on anything without a massive GPU."},
{"Title": "Genuine question: Where should I share my opinion about Python language?", "Author": "u/HalfAByteIsWord", "Content": "I recently posted my opinions on the import system in Python and how it can be made more intuitive and straightforward. I got some comments, some of them are just snappy as usual and some of them had some suggestions and workarounds. Either way the post got removed, citing that it is not suitable for this sub-reddit. I mean, come on, I cannot talk about Python features in Python sub-reddit? Update: To mods: Thanks for letting me have this conversation. To others who made helpful comments, thank you. Original post: I don't want to recreate the post verbatim here, I don't remember it exactly too. It was basically something like this, *requiring* or *loading* code, which is more or less importing can be much more straight forward if we can directly load files. Instead of the concept of packages, each file is a self contained module which can import and export classes, variables and functions. For example, src/constants.py ADMIN_ROLE_NAME = 'admin' src/user.py from constants import ADMIN_ROLE_NAME tests/user_test.py from ../../src/constants.py import ADMIN_ROLE_NAME Here there would be no __init__.py files, just straightforward imports directly from files. I would refrain from giving example from Ruby or JS, like I did in the original post, because people perceive it as a fan boy post, while my intention is not that. I'll give a real example, Here is my repo, which https://github.com/rajaravivarma-r/python-robyn-react I tried to put the contents of the `constants.py` file into `src/__init__.py` but there is no way of me importing them from within the child directory package, namely `src/api/__init__.py`. Backward compatibility: Lot of you have mentioned about how this would break existing code, but C++ standard had a recommendation to introduce module systems to new code which goes like this. Add a pragma like #pragma C++14_Modules at the top of the file, or something like that, but you get the idea. Ruby has a # frozen_string_literal: true So Python could introduce something like that for new code, while treating the rest of the old code as it is. I believe this is constructive enough. I know this is not a PEP, but I want to show the community that my intention was genuine and not a rant. Outro: Assume ignorance rather than malevolence. Your snarky comments or aggressive down voting doesn't improve anyone's life. Last time when I posted about some surprising behaviour of Python in this community (from my old account), I got a comment about my daughter and wife. It wasn't even deleted despite reporting."},
{"Title": "Wrote a Python script for downloading images/GIFs from subreddits in bulk.", "Author": "u/MonsieurKebab", "Content": "What my Project Does It's a program that searches through posts in a specified subreddit and downloads images, gifs etc. Target Audience It's a toy project at this point, but I might extend its functionality if I can find good resources. Maybe even add support for different websites (imgur et al.). Comparison I found a few websites for downloading images from reddit but they claimed they didn't work because of the Reddit API changes, I was sceptical and wrote my own script. Anyhow, For the last few days I was trying to find good wallpapers for my system. I wrote a Python script for getting images and gifs from subreddits, here is the project repo. Please read the instructions before using it. It's pretty basic but gets the job done. I am still working on the project. I also wrote a flake.nix for my NixOS bros. Didn't test the project on Windows but I don't see why it wouldn't work. Lastly, here is my NixOS config in case  you are interested. Enjoy"},
{"Title": "How to parallelize Pandas with Modin", "Author": "u/linchpiner", "Content": "Discover the power of Modin in overcoming the performance bottlenecks of Pandas. We'll walk you through how a simple switch can reduce your waiting times and make your workflow smooth and speedy, all without leaving the comfort of Pandas behind. Modin is an active  open source project. Take a look to this blog post to see if it is applicable for your code: https://dchigarev.github.io/modin_perf_examples/"},
{"Title": "Does anyone actually use PyPy or Cython?", "Author": "u/Sc0urge_", "Content": "I have been reading a lot about these tools recently and wondered if they are used in production, especially when building a backend with Python. As far as I know, PyPy is not ideal because of its lack of package support, but Cython seems pretty neat."},
{"Title": "Futurama API written in async Python", "Author": "u/koldakov", "Content": "I've created a simple API service https://github.com/koldakov/futuramaapi to access Futurama units with async Python + FastAPI + SQLAlchemy (PostgreSQL). I've chosen Hypercorn for a web server to achieve HTTP/2 support - considering I didn't find yet cheap hosting that supports HTTP/2. Heroku doesn't support, render doesn't support . Also it has some code related to GraphQL (I used strawberry). + I didn't find a lot of discussions about Heroku + FastAPI - the problem will be with redirect to https. As default fastapi (starlette) middleware doesn't work with proxies there will be an infinite loop if you try to use fastapi.middleware.httpsredirect.HTTPSRedirectMiddleware . Chain will be: Client https-> Proxy(Hypercorn, for example) http-> fastapi, in that case as you can see the request is secure, but fastapi (starlette) won't understand this and will redirect to https. Here is a simple workaround to support https under proxy: https://github.com/koldakov/futuramaapi/blob/main/app/middlewares/secure.py Constructive feedback is greatly appreciated. Source code: https://github.com/koldakov/futuramaapi Working example: https://futuramaapi.com"},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "remove background from image using AI in just 5 lines of python code", "Author": "u/Maleficent_Yak_993", "Content": "I created a python library \"dis-bg-remover\" based off the \"Highly Accurate Dichotomous Image Segmentation ( https://arxiv.org/pdf/2203.03041.pdf ), whose results are comparable, if not better, to the premium offerings in the market. Explainer video here https://www.youtube.com/watch?v=js7AYKkZvFI"},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "SQLModel vs native SQL Alchemy ORM for a web backend?", "Author": "u/Raz_Crimson", "Content": "Hi there I was tasked with migrating from mongoDb to PostgreSQL due to the performance issues. The backend is FastAPI-based web server for a mobile APP with a decent amount if CRUD. We also heavily use Pydantic for data validation and the primary carried of data across the service layers Would like to hear opinions from SQLModel users as to how easy it is to use or what kind of pitfalls am i setting myself up by using it.  Last post that I could find was years old and even that was not really conclusive. Now SQLAlchemy 2.0 is released and even SQLModel has support for Pydantic V2, so would like some newer thoughts in this topic. Our datamodel includes quite a bit of top level nestings, which I would prefer to be stored as `JSON` columns ( Similar SO Question ). SQLModel doesn't seem to have any documentation for such cases. If possible I would prefer avoiding to write and maintain our own internal library with SQLAlchemy ORM to perform these conversions. But maybe just using plain SQLAlchemy and Pydantic might be a good solution too. Seeking inputs from more experienced users. Thank you. Edit: In the end I gave up on using orm to query data and just used plain SQLAlchemy queries. The reason was that the ORM layer already abstract lot of stuff from us and SQLModel makes it more abstracted. This is fine if you know all the details of the abstractions. But at the point I am in, where I am new to sqlalchemy, it just adds lots more complexity and gotchas which can just avoid with plain queries. Maybe better documentation might help down the line but as of now, I am leaning towards just plain sqlqlchemy  core. Also using the ORM layer to just auto generate revisions is a nice feature."},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Automating Python with Google Cloud", "Author": "u/neb2357", "Content": "I just published a tutorial series on how to automate a Python script in Google Cloud using Cloud Functions and/or Cloud Run. Feedback would be great. Thanks! Automating Python with Google Cloud Automating Python with Google Cloud Functions Automating Python with Google Cloud Run"},
{"Title": "Python state management", "Author": "u/Extreme-Acid", "Content": "Hey all, I love what Django has with django-fsm. I require something but without Django, as there is no user interaction with the workflow. All inputs and outputs are either rabbitmq or another api. This is to be a workflow management system. I am looking for state management backed up by database so we can not suffer if a k8s pod dies with all the states in memory. Some of our workflows could take weeks. Is it still best to make this in Django or is there a database backed state management module available? I see pytransitions but I would have to add database logging to it."},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Creating a GeoTIFF raster XYZ tile service in python with caching capability", "Author": "u/iamgeoknight", "Content": "Creating a GeoTIFF raster XYZ tile service in python with caching capability"},
{"Title": "Makefile Parser for Python", "Author": "u/Cybasura", "Content": "Hello everyone! I am proud to introduce a Makefile Parser for Python that I think will be useful! the link is Thanatisia/makefile-parser-python As an introduction, I have been using python and been following this subreddit for quite awhile now, but this is my first post Recently, I've been planning a side-project involving the use of Makefiles in Python, and I required the use of a Makefile parser ala json or pyyaml - whereby you would import a Makefile into python as objects/dictionary/lists. Hence, I started searching for a Makefile Parser. The only parser I've found is PyMake(2) which is cool but it hasnt been updated since 2017 from what I recall and that it is more of a CLI utility, so with that in mind, I went about to make it I hope that this will be as useful as it is for me, currently I am using this in a side project and i'm able to format it such that its printing out a typical Makefile structure right off the bat, which is pretty nice. Additional information to the project What My Project Does This is a Makefile Parser, made in Python. The operational workflow is basically Start --> Import File into dictionary --> Manipulation and Usage --> Processing --> Output --> Export Target Audience (e.g., Is it meant for production, just a toy project, etc.) This is a library/package/module by design, much like json or pyyaml as mentioned but a smaller scale at the moment as its just started. I'm not sure if it applies for you but its a parser/importer Comparison (A brief comparison explaining how it differs from existing alternatives.) I'm not sure if there are any other Makefile Parsers other than Pymake2, but the idea is similar to the aforementioned ideas"},
{"Title": "Load Apple's .numbers Files into Pandas", "Author": "u/nez_har", "Content": "I recently ran into some challenges while trying to work with Apple's .numbers files on Linux. After a bit of experimentation, I figured out a workflow that simplifies the process. If you're planning to use .numbers files and need to load them into pandas, I've created a tutorial that covers the required dependencies and the steps to follow: https://nezhar.com/blog/load-apple-numbers-files-python-pandas-using-containers/ . Has anyone else here worked with .numbers files in Python? I‚Äôd love to hear about your experiences or any tips you might have."},
{"Title": "No More Cutting in Line: Crafting Fairness in Queues using RQ in Python", "Author": "u/soap94", "Content": "I was trying to tackle the problem of queue fairness and here's how I solved it."},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Type-Level Programming: a POC", "Author": "u/Kiuhnm", "Content": "While Python doesn't explicitly support Type-Level Programming , I've had some success with it. I think this may be of interest to the Python community, so I'm sharing a POC (Proof Of Concept) I've written. This POC is a statically typed list that encodes its length in its type so that, for instance, when you concatenate two lists, the result has the correct length, all done at type-checking time. Read more on GitHub"},
{"Title": "Open Jupyter Notebooks in Kaggle? (from GitHub)", "Author": "u/pbeens", "Content": "I have a repo in GitHub with Jupyter Notebooks for students. On each notebook I have a link where the student can open the notebook in Colab (by changing the URL from https://github.com/ to https://githubtocolab.com/ ). The problem is that some school boards have Colab blocked. Is there a similar technique I can use to open the notebooks in Kaggle? Are there other sites I should be considering? Thanks!"},
{"Title": "does anyone know about an alternative to aeneas that works on python 3.12.1?", "Author": "u/jonnisaesipylsur", "Content": "i need some python library that can automatically generate a synchronization map between a list of text fragments and an audio file containing the narration of the text. aeneas does exactly that except it doesnt work on higher than 2.7 with windows."},
{"Title": "PyOhio CFP open through May 20", "Author": "u/catherinedevlin", "Content": "Call for Proposals open through May 20 PyOhio is a fun, friendly, free general Python conference now in its Nth year (N is large).  This year it will be Sat & Sun Jul 27-28, 2024 at The Westin Cleveland Downtown in Cleveland , OH (its first year outside Columbus)! Hope to see you there!"},
{"Title": "\"Mark as read\" plugin for mkdocs-material", "Author": "u/br2krl", "Content": "Hi everyone, I developed a simple plugin for mkdocs-material. What My Project Does: In simple terms, this plugin allows users to mark pages as read and it shows a checkmark icon in navigation bar for the pages that was marked as read. The plugin adds a button under the page content and when users clicks, it stores read date in localStorage. It shows a checkmark icon in the navigation panels for the pages that marked as read. It also shows a \"document updated\" icon if the pages that marked as read got an update. Target Audience: Anyone who has a website built with Material for MkDocs (a.k.a. mkdocs-material). This plugin could be useful for the pages that user read by an order and wants to continue from where it left. I guess the Learn page of FastAPI documentation could be a great example to that. Comparison: No alternative plugin exist for Material for MkDocs afaik Project repo: github.com/berk-karaal/mkdocs-material-mark-as-read You can try this plugin on the documentation website . Let me know what you think about this plugin. Also please share if you have a feature request or an idea to improve this plugin."},
{"Title": "An Automated Bash Script for Python Virtual Environment Management", "Author": "u/SAV_NC", "Content": "Hello r/Python , What My Project Does: This script is designed to automate the management of Python virtual environments and dependencies, streamlining the setup process for Python projects. It facilitates a more efficient workflow for managing project-specific environments and package installations, especially in a professional development setting. Target Audience: This tool is particularly beneficial for developers and IT professionals looking for a systematic approach to environment management. It is designed to integrate into existing workflows, providing a reliable and consistent method for managing Python environments and dependencies. Features include: Automated creation of Python virtual environments. Batch installation of packages from a predefined array or a requirements.txt file. Can import , update , upgrade , and remove packages within the virtual environment. Functionalities for listing all installed packages for transparency and audit purposes. Benefits: Efficiency: Reduces manual setup and management of virtual environments. Consistency: Ensures uniform environments across development stages and projects. Flexibility: Supports custom package lists and requirements, adaptable to project-specific needs. Comparison : Fast setup , updates, and removal. Set custom paths to store your files Everyone is invited to download, implement, and provide feedback on this script to further refine its capabilities to meet professional standards and requirements (AKA just be really useful). For access and further details, please visit: GitHub"},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Analyzing Python Malware found in an open-source project", "Author": "u/42-is-the-number", "Content": "Hi all, I've recently found a Python Malware in a FOSS tool that is currently available on GitHub. I've written about how I found it, what it does and who the author is. The whole malware analysis is available in form of an article . I would appreciate any and all feedback."},
{"Title": "ipython-sql has been forked", "Author": "u/databot_", "Content": "Hi r/Python , In case you didn't know, ipython-sql has been forked. The new project has fixed some long-standing issues and added a bunch of new capapabilities: Splitting long SQL queries in multiple cells Plotting large-scale datasets More flexibility to open database connections The API remains the same, you can replace projects that depend on ipython-sql with jupysql: pip install jupysql You can read more about the project here. There's also a post in DuckDB's blog that you might want to check out, too."},
{"Title": "üöÄ Goprox: Simplify Google searches with automatic proxy handling and user-agent selection.", "Author": "u/SwiftGloss", "Content": "What My Project Does: Goprox is a Python module that revolutionizes Google searches by automatically checking and using proxies, eliminating the need for user input. With Goprox, users can enjoy seamless searching without worrying about proxy configuration or getting blocked. Target Audience: Goprox is perfect for developers seeking to automate Google searches for web scraping, automation, or data collection tasks. It's also ideal for anyone who desires a hassle-free search experience without the hassle of manual proxy management. Comparison: Compared to existing alternatives, Goprox stands out with its focus on automatic proxy handling. Unlike other solutions that require manual proxy input, Goprox streamlines the process by autonomously managing proxies for each search query. Experience the power of Goprox on GitHub today! Your feedback and contributions are greatly appreciated."},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Astrologers have announced the month of Python in Centrifugal Labs: real-time libraries and tutorial", "Author": "u/FZambia", "Content": "Hello Python community. My name is Alexander ‚Äì I am the author of Centrifugo project. It's a self-hosted real-time messaging server (alternative to Ably, Pusher, Pubnub services). It was written in Python originally, but then migrated to Go. But it's fully language-agnostic and helps many projects written in Python (and in Django in particular) to add real-time updates to the application. Centrifugo is quite fast, scales well, has super-efficient integration with Redis (~million of publications per second, and more with Redis sharding/Redis Cluster). What My Project Does Any kind of real-time messaging apps may be built with the help of Centrifugo. Chat/messenger apps, real-time multiplayer games, turn-based games in particular. Streaming metrics. The best thing is that Centrifugo is a separate language-agnostic service which provides API for client connections (WebSocket, EventSource, HTTP-streaming, experimental Webtransport, GRPC) and for backend communication (over HTTP or GRPC). So it may be used as a universal real-time component throughout different tech stacks. Including Python. Centrifugo is used in many projects, for example, our core WebSocket library is part of Grafana. Target Audience Software engineers, startups and mature projects that require real-time updates in the application. Centrifugo gives answers to some problems developers may come across when building real-time app in scale. See our blog post: Scaling WebSocket in Go and beyond . Comparison There is no direct analogue, but many projects exist in the area. Some of them cloud-based - like pusher.com , ably.com , pubnub.com . Some are self-hosted - like Mercure . We have comparison with similar technologies on Centrifugo site. I'd say Protobuf protocol, transport selection, both bidirectional and unidirectional approaches, super-efficient built-in Redis integration for scalability are some selling points of Centrifugo when comparing to other self-hosted solutions. The actual update During last month Centrifugal ecosystem got several Python updates, and I'd like to share this with you: We've released Python real-time SDK for Centrifugo. See centrifuge-python . This is a WebSocket client, uses JSON or Protobuf for communicating with Centrifugo. Real-time SDKs usually used on client-side of app - it's possible to subscribe/unsubscribe on channels, receive online presence data, communication with the backend over RPC calls through WebSocket. Next library we just released is pycent v5, HTTP SDK for Centrifugo server API. Most of the time you publish real-time data to Centrifugo channels you are using server API, and this is a small lib that simplifies integration with Centrifugo. It has both sync and async clients, uses Pydantic for DTO. Finally, not exactly generic Python related, but I'd like to mention it also because we've put a lot of effort into it. We've released a Grand Tutorial for Centrifugo which shows how to build scalable chat/messenger application on top of Django and Centrifugo . From scratch. It covers some aspects of application building other tutorials never mention - delivery guarantees, approaches for reliable delivery and idempotent processing, shows some numbers. Hope this may be useful to someone in the community. Since Centrifugo has roots in Python a good integration with the ecosystem is very important for us. If you have any questions about a project ‚Äì will be happy to answer."},
{"Title": "I forked Newspaper3k, fixed bugs and improved its article parsing performance - Newspaper4k package", "Author": "u/gringo6969", "Content": "Hi all! The Newspaper3k is abandoned (latest release in 2018) without any upgrades and bugfixing. I forked it, and imported all open Issues into my repo. The first two releases (0.9.0 and 0.9.1) were mainly bugfixes and bringing the project more up to date and compatible with python > 3.6  (I started from version 0.9.0 üòÅ). In the latest version, 0.9.3 I not only almost reworked the whole News article parsing process, but also added a lot of new supported languages (around 40 new languages) Repository : https://github.com/AndyTheFactory/newspaper4k Documentation : https://newspaper4k.readthedocs.io/ What My Project Does Newspaper4k helps you in extracting and curating articles from news websites. Leveraging automatic parsers and natural language processing (NLP) techniques, it aims to extract significant details such as: Title, Authors, Article Content, Images, Keywords, Summaries, and other relevant information and metadata from newspaper articles and web pages. The primary goal is to efficiently extract the main textual content of articles while eliminating any unnecessary elements or \"boilerplate\" text that doesn't contribute to the core information. Target Audience Newspaper4k is built for developers, researchers, and content creators who need to process and analyze news content at scale, providing them with powerful tools to automate the extraction and evaluation of news articles. Comparisons As of the 0.9.3 version, the library can also parse the Google News results based on keyword search, topic, country, etc The documentation is expanded and I added a series of usage examples. The integration with Playwright  is possible (for websites that generate the content with javascript), and since 0.9.3 I integrated cloudscraper that attempts to circumvent Cloudflair protections. Also, compared with the latest release of newspaper3k (0.2.8), the results on the Scraperhub Article Extraction Benchmark are much improved and the multithreaded news retrieval is now stable. Please don't hesitate to provide your feedback and make use of it! I highly value your input and encourage you to play around with the  project."},
{"Title": "We're building a Large Action Model (LAM) project that can do any task for you using Python!", "Author": "u/nobilis_rex_", "Content": "Hey guys! My friend and I are building Nelima. It's basically a Large Language Model designed to take actions on your behalf with natural language prompts and theoretically automate anything. For example, it can schedule appointments, send emails, check the weather, and even connect to IoT devices to let you command it ‚Äì you can ask it to publish a website or call an Uber for you! You can integrate your own custom actions, written in Python, to suit your specific needs, and layer multiple actions to perform more complex tasks. When you create these actions or functions, it contributes to the overall capabilities of Nelima, and everyone can now invoke the same action. Right now, it's a quite limited in terms of the # of actions it can do but we're having fun building a few :) Nelima can see the outcomes of each micro-action undertaken to achieve the overarching goal. The potential for reasoning is very much possible and doesn't shy away from taking measures ‚Äì for example, if it sees your grocery list from a sub-action on fulfilling an action and realizes that a certain item has allergens which might be harmful to the user, it puts a warning label, even though the user didn't ask for this. I thought the community here might find it useful. It uses Python 3 (Version 3.11), and the environment includes the following packages: BeautifulSoup, urllib3, requests, pyyaml. We‚Äôll try to include more if people need those. Give it a try and let me know what you think! :)"},
{"Title": "Is this project worth the time and effort: Camera to Keyboard?", "Author": "u/mnvoh", "Content": "EDIT: Since there are a lot of upvotes on that comment, yes, this is about seeking strangers' validations. But we're all validation seekers. For instance, we all need someone's validation to make a living (boss, customers, etc.) Your boss ain't gonna tell you \"god damn boy, you suck at your job, here take my money\". But in this case, validation seeking is kind of a broad term, let's use something more specialized like Market Research üòâ. Also, thanks for all your comments and inputs. Hello everyone I've been working on this project for the past 3 weeks, and I want people's opinions to determine how much time I should invest in it. Project Link: https://github.com/mnvoh/cameratokeyboard What My Project Does It's a computer vision project (python, YOLOv8) which essentially allows you to use your camera as a keyboard by detecting your fingers and analyzing their movements. The only requirement (besides having a camera) is that you have to print the keyboard (on an A4 paper, for example). The keyboard is for you and there are 4 markers identifying the boundaries of the keyboard which are for the program. Target Audience Currently, it's a PoC, but the goal is to develop it into a fully functioning virtual keyboard with a desirable accuracy. The initial idea came to me while thinking that it would be actually cool if you could print your keyboard design on your desk mat, and then just use that as a keyboard, but it could also have substantial applications in cell phones. Comparison I have searched quite a bit, but haven't found any similar solutions. EDIT: Thanks to u/avaqueue for finding these articles: https://www.academia.edu/105250798/Paper_Keyboard_Using_Image_Processing?uc-sb-sw=5982163 https://ieeexplore.ieee.org/document/6377072 Also, u/HobblingCobbler has mentioned that they've had a phone with this feature. waiting on more info The Main Question Now I have had an enormously gigantic amount of fun working on this and will continue for sure, but how much time depends on its potential. That's why I'm asking for your opinions: Is it actually worth it? Or am I imagining its potentials?"},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "What‚Äôs a script that you‚Äôve written that you still use frequently?", "Author": "u/HiT3Kvoyivoda", "Content": "Mine is a web scraper. It‚Äôs only like 50 lines of code. It takes in a link, pulls all the hyperlinks and then does some basic regex to pull out the info I want. Then it spits out a file with all the links. Took me like 20 minutes to code, but I feel like I use it every other week to pull a bunch of links for files I might want to download quickly or to pull data from sites to model."},
{"Title": "Alternative Queries: Typed and Reusable Handcrafted SQL", "Author": "u/baluyotraf", "Content": "Hello! It's still early in development, but I just want to know if people are interested. What My Project Does I created a library to integrate Pydantic type checking for handwritten SQL queries. It also allows you to test the types, and create nested queries easily. Alternative Queries Target Audience We had a project recently were we had to use handwritten SQL and managing the parameters and reusing queries was quite a hassle. So I'm planning to use this on my production projects moving forward. Comparison I would say in terms of usage, it's like Pydantic + SqlParams, but only with the python default formatting. SqlParams"},
{"Title": "Finite Element Analysis for Continuum Mechanics of Solid Bodies (FElupe)", "Author": "u/adtzlr", "Content": "Hi, I'd like to introduce to you my open-source FEA package FElupe , available on GitHub . Its target audience is typically an engineer, e.g. in the field of mechanical or biomedical engineering. It is designed to be both flexible for scientifc research as well as easy-to-use for industry-related problems. Packages with similar scopes are e.g. scikit-fem and Fenics(x). FElupe is a Python 3.8+ üêç finite element analysis package üì¶ focussing on the formulation and numerical solution of nonlinear problems in continuum mechanics üîß of solid bodies üöÇ. Its name is a combination of FE (finite element) and the german word Lupe üîç (magnifying glass) as a synonym for getting an insight üìñ how a finite element analysis code üßÆ looks like under the hood üï≥Ô∏è. FElupe has minimal requirements, all available at PyPI supporting all platforms. pip install felupe[all] It's pure Python but the assembly performance is well suited for mid-sized problems. It is possible to assemble up to 130000 degrees of freedom for linear elasticity on a modern notebook in one second runtime (results may vary). import felupe as fem mesh = fem.Cube(n=6) region = fem.RegionHexahedron(mesh) field = fem.FieldContainer([fem.Field(region, dim=3)]) boundaries, loadcase = fem.dof.uniaxial(field, clamped=True) umat = fem.OgdenRoxburgh(material=fem.NeoHooke(mu=1), r=3, m=1, beta=0) solid = fem.SolidBodyNearlyIncompressible(umat, field, bulk=5000) move = fem.math.linsteps([0, 1, 0, 1, 2, 1], num=5) step = fem.Step(items=[solid], ramp={boundaries[\"move\"]: move}, boundaries=boundaries) job = fem.CharacteristicCurve(steps=[step], boundary=boundaries[\"move\"]) job.evaluate(filename=\"result.xdmf\") fig, ax = job.plot( xlabel=\"Displacement $u$ in mm $\\longrightarrow$\", ylabel=\"Normal Force $F$ in N $\\longrightarrow$\", ) solid.plot(\"Principal Values of Cauchy Stress\").show() The above minmal code-block covers the essential high-level parts of creating and solving problems with FElupe. There is also a Gallery of Examples . Please let me know what you think about it. If you like it - I'd be glad if you share it with your friends, star it on GitHub, whatever you like. Thanks! https://github.com/adtzlr/felupe"},
{"Title": "Text extraction lib", "Author": "u/TraditionalAlps4337", "Content": "I created  a simple tool for extracting text from PDF, EPUB, TXT, and DOCX files.It is mainly for personal use, but I would really appreciate a feedback https://github.com/KirillAn/extractText/tree/main"},
{"Title": "PyBackport: Use instances from newer python releases", "Author": "u/tan_tata_chan", "Content": "What My Project Does PyBackport holds serves three purposes: Use instances from newer python releases in older python versions. For example, using StrEnum (new in python 3.11) in python 3.9 Enable experimental unreleased classes. For example, define new enums where members are tuples with TupleEnum Provide support to commonly used constants. For example, PyBackport allows to import and use colors.RGB to avoid re-defining constant color codes in every new project. Target Audience The audience is developers that must create scripts for older python versions, such as new packages which must support python 3.8+. With PyBackport these instances from newer python versions can be used. Comparison I've not encountered any similar released package with good documentation. I also choose not to set a release V1.0 for my package, as I backported only some enums functionality. Anyone feel free to open a new issue, contribute with new pull requests or to notify me any other useful instances to backport. I intend to keep expanding the package as much as possible. Links GitHub repo PyPI release Documentation"},
{"Title": "Distributing simple handy scripts for Windows - mouse-startable AND editable (the tricky one-liner)", "Author": "u/galkinvv", "Content": "In short: here is the hacky 2-line header that makes any python script a clickable batch file (assuming bundled python in a subfolder) @classmethod # 2>nul & (if not exist \"%~dp0\\python64-win\\python.exe\" (echo Fatal python64-win\\python.exe not found & pause) else (title %~f0 & \"%~dp0\\python64-win\\python.exe\" \"%~f0\" %*)) & exit /B & # noqa: E501 def __unused(): \"fake function to help writing header that allows executing same file as python and batch\" # Normal script starts here Its usage is either double-click to run without args or \"drag a file on it\" if the script processes a file (passed as an argument). Its installation is as easy as .zip unpacking More detailed: From time to time I get in touch with small handy one-file python scripts useful for some simple but very handy automations for a some niche use case on the windows platform. This post is not about tasks themselves - there may be really anything, but about distributing them. Such scripts are typically has no any repo/site and just attached to a message or similar, since the author has no any plans for further development, but still wants to share. 90% of target users are not developers, and just can use the \"double-clickable\" application. The other 10% are developers who may want to perform some script enhancements. Non-developers needs \"look at max 1-2 sentence documentation + several clicks to install + several to launch\". Developers need the ability to enhance the script. How to get both satisfied? Often such scripts are packed & bundled with a python into a single .exe - this is not editable, the other variant is distributing .py with the too large doc \"download python installer from official site, install it, then click the .py file\" and without an easy way to distribute dependency packages. As an author of some such scripts after several improve iterations I found a solution that satisfy both needs: distribute .zip containing bundled portable python interpreter with preinstalled dependencies in a subdirectory and a clickable file being simultaneously a .bat file and a python script - the extension is .py.bat or just .bat Getting portable python is a bit tricky, but embeddable official image is mostly ok."},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Chat with your friends within your (Mac & Linux) Terminal | Textual | Simple & Easy to use", "Author": "u/Plastic-Payment-934", "Content": "Toschat !What My Project Does? a lightweight python chat app within your terminal with simple UI and it's built on top of Textual framework. What you can do: - Create a new account - Add your friends to your contact - and chat! Target Audience It's my hobby project to improve my python knowledge but everyone can install and use it. All data store in a live server with Django framework. Comparison Instead of texting with your colleagues or your friends on your phone or a web browser, .. why not your terminal ? üòÅ I'm happy for advices on code improvements, features recommendations and feedbacks. üòÅüòÅ Github Link: https://github.com/MuongKimhong/toschat If you find it interesting and useful, give it a star I would appreciate it very much.Have a good weekend programmers!"},
{"Title": "Designing a Pure Python Web Framework", "Author": "u/Pleasant-Cow-3898", "Content": "From the Article: This provides a good overview of how Reflex works under the hood. TLDR : Under the hood, Reflex apps compile down to a React frontend app and a FastAPI backend app. Only the UI is compiled to Javascript; all the app logic and state management stays in Python and is run on the server. Reflex uses WebSockets to send events from the frontend to the backend, and to send state updates from the backend to the frontend. Full post: https://reflex.dev/blog/2024-03-21-reflex-architecture/#designing-a-pure-python-web-framework"},
{"Title": "UFO Tools- functional programming in python", "Author": "u/houseofleft", "Content": "I shared a \"monads in python library\" a little while back on here- since then I've been toying with and expanding things out into a more general project for helping functional programming patterns in python. [I had (am having) a bunch of fun making this - thought I'd share here!]( https://github.com/benrutter/ufo-tools ) What my project does: A simple library with utilities for functional programming in Python. Target Audience: It's meant for use in actual production contexts, but it's still in a very early and experimental phase. It's probably most helpful right now as a learning resource. Comparison: Most similar project I know of is the Returns library, which is great, but a lot more heavy weight. It has more of an expectation of being used for everything, rather than a drop in as-and-when tool. I'd love any feedback in general (as well as any healthy flamewars around the herecy or one-true-way of functional programming)"},
{"Title": "Code type checks but then `TypeError`s", "Author": "u/Kiuhnm", "Content": "Have you noticed that Python's static type system is unsound? Look at this code: from typing import overload @overload def f(x: int) -> str: ... @overload def f(x: str) -> int: ... def f(x: int | str) -> int | str: match x: case int(): return 1 case str(): return 'a' print('a' + f(1)) print(1 + f('a')) The code above type checks, but raises TypeError when executed. With proper overloading this would never happen. Can you think of other cases? You're not allowed to use cast , Any , to omit type hints, to use stubs, or to cheat in similar ways."},
{"Title": "does anyone know about an alternative to aeneas that works on python 3.12.1?", "Author": "u/jonnisaesipylsur", "Content": "i need some python library that can automatically generate a synchronization map between a list of text fragments and an audio file containing the narration of the text. aeneas does exactly that except it doesnt work on higher than 2.7 with windows."},
{"Title": "PyOhio CFP open through May 20", "Author": "u/catherinedevlin", "Content": "Call for Proposals open through May 20 PyOhio is a fun, friendly, free general Python conference now in its Nth year (N is large).  This year it will be Sat & Sun Jul 27-28, 2024 at The Westin Cleveland Downtown in Cleveland , OH (its first year outside Columbus)! Hope to see you there!"},
{"Title": "\"Mark as read\" plugin for mkdocs-material", "Author": "u/br2krl", "Content": "Hi everyone, I developed a simple plugin for mkdocs-material. What My Project Does: In simple terms, this plugin allows users to mark pages as read and it shows a checkmark icon in navigation bar for the pages that was marked as read. The plugin adds a button under the page content and when users clicks, it stores read date in localStorage. It shows a checkmark icon in the navigation panels for the pages that marked as read. It also shows a \"document updated\" icon if the pages that marked as read got an update. Target Audience: Anyone who has a website built with Material for MkDocs (a.k.a. mkdocs-material). This plugin could be useful for the pages that user read by an order and wants to continue from where it left. I guess the Learn page of FastAPI documentation could be a great example to that. Comparison: No alternative plugin exist for Material for MkDocs afaik Project repo: github.com/berk-karaal/mkdocs-material-mark-as-read You can try this plugin on the documentation website . Let me know what you think about this plugin. Also please share if you have a feature request or an idea to improve this plugin."},
{"Title": "An Automated Bash Script for Python Virtual Environment Management", "Author": "u/SAV_NC", "Content": "Hello r/Python , What My Project Does: This script is designed to automate the management of Python virtual environments and dependencies, streamlining the setup process for Python projects. It facilitates a more efficient workflow for managing project-specific environments and package installations, especially in a professional development setting. Target Audience: This tool is particularly beneficial for developers and IT professionals looking for a systematic approach to environment management. It is designed to integrate into existing workflows, providing a reliable and consistent method for managing Python environments and dependencies. Features include: Automated creation of Python virtual environments. Batch installation of packages from a predefined array or a requirements.txt file. Can import , update , upgrade , and remove packages within the virtual environment. Functionalities for listing all installed packages for transparency and audit purposes. Benefits: Efficiency: Reduces manual setup and management of virtual environments. Consistency: Ensures uniform environments across development stages and projects. Flexibility: Supports custom package lists and requirements, adaptable to project-specific needs. Comparison : Fast setup , updates, and removal. Set custom paths to store your files Everyone is invited to download, implement, and provide feedback on this script to further refine its capabilities to meet professional standards and requirements (AKA just be really useful). For access and further details, please visit: GitHub"},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Analyzing Python Malware found in an open-source project", "Author": "u/42-is-the-number", "Content": "Hi all, I've recently found a Python Malware in a FOSS tool that is currently available on GitHub. I've written about how I found it, what it does and who the author is. The whole malware analysis is available in form of an article . I would appreciate any and all feedback."},
{"Title": "ipython-sql has been forked", "Author": "u/databot_", "Content": "Hi r/Python , In case you didn't know, ipython-sql has been forked. The new project has fixed some long-standing issues and added a bunch of new capapabilities: Splitting long SQL queries in multiple cells Plotting large-scale datasets More flexibility to open database connections The API remains the same, you can replace projects that depend on ipython-sql with jupysql: pip install jupysql You can read more about the project here. There's also a post in DuckDB's blog that you might want to check out, too."},
{"Title": "üöÄ Goprox: Simplify Google searches with automatic proxy handling and user-agent selection.", "Author": "u/SwiftGloss", "Content": "What My Project Does: Goprox is a Python module that revolutionizes Google searches by automatically checking and using proxies, eliminating the need for user input. With Goprox, users can enjoy seamless searching without worrying about proxy configuration or getting blocked. Target Audience: Goprox is perfect for developers seeking to automate Google searches for web scraping, automation, or data collection tasks. It's also ideal for anyone who desires a hassle-free search experience without the hassle of manual proxy management. Comparison: Compared to existing alternatives, Goprox stands out with its focus on automatic proxy handling. Unlike other solutions that require manual proxy input, Goprox streamlines the process by autonomously managing proxies for each search query. Experience the power of Goprox on GitHub today! Your feedback and contributions are greatly appreciated."},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Astrologers have announced the month of Python in Centrifugal Labs: real-time libraries and tutorial", "Author": "u/FZambia", "Content": "Hello Python community. My name is Alexander ‚Äì I am the author of Centrifugo project. It's a self-hosted real-time messaging server (alternative to Ably, Pusher, Pubnub services). It was written in Python originally, but then migrated to Go. But it's fully language-agnostic and helps many projects written in Python (and in Django in particular) to add real-time updates to the application. Centrifugo is quite fast, scales well, has super-efficient integration with Redis (~million of publications per second, and more with Redis sharding/Redis Cluster). What My Project Does Any kind of real-time messaging apps may be built with the help of Centrifugo. Chat/messenger apps, real-time multiplayer games, turn-based games in particular. Streaming metrics. The best thing is that Centrifugo is a separate language-agnostic service which provides API for client connections (WebSocket, EventSource, HTTP-streaming, experimental Webtransport, GRPC) and for backend communication (over HTTP or GRPC). So it may be used as a universal real-time component throughout different tech stacks. Including Python. Centrifugo is used in many projects, for example, our core WebSocket library is part of Grafana. Target Audience Software engineers, startups and mature projects that require real-time updates in the application. Centrifugo gives answers to some problems developers may come across when building real-time app in scale. See our blog post: Scaling WebSocket in Go and beyond . Comparison There is no direct analogue, but many projects exist in the area. Some of them cloud-based - like pusher.com , ably.com , pubnub.com . Some are self-hosted - like Mercure . We have comparison with similar technologies on Centrifugo site. I'd say Protobuf protocol, transport selection, both bidirectional and unidirectional approaches, super-efficient built-in Redis integration for scalability are some selling points of Centrifugo when comparing to other self-hosted solutions. The actual update During last month Centrifugal ecosystem got several Python updates, and I'd like to share this with you: We've released Python real-time SDK for Centrifugo. See centrifuge-python . This is a WebSocket client, uses JSON or Protobuf for communicating with Centrifugo. Real-time SDKs usually used on client-side of app - it's possible to subscribe/unsubscribe on channels, receive online presence data, communication with the backend over RPC calls through WebSocket. Next library we just released is pycent v5, HTTP SDK for Centrifugo server API. Most of the time you publish real-time data to Centrifugo channels you are using server API, and this is a small lib that simplifies integration with Centrifugo. It has both sync and async clients, uses Pydantic for DTO. Finally, not exactly generic Python related, but I'd like to mention it also because we've put a lot of effort into it. We've released a Grand Tutorial for Centrifugo which shows how to build scalable chat/messenger application on top of Django and Centrifugo . From scratch. It covers some aspects of application building other tutorials never mention - delivery guarantees, approaches for reliable delivery and idempotent processing, shows some numbers. Hope this may be useful to someone in the community. Since Centrifugo has roots in Python a good integration with the ecosystem is very important for us. If you have any questions about a project ‚Äì will be happy to answer."},
{"Title": "I forked Newspaper3k, fixed bugs and improved its article parsing performance - Newspaper4k package", "Author": "u/gringo6969", "Content": "Hi all! The Newspaper3k is abandoned (latest release in 2018) without any upgrades and bugfixing. I forked it, and imported all open Issues into my repo. The first two releases (0.9.0 and 0.9.1) were mainly bugfixes and bringing the project more up to date and compatible with python > 3.6  (I started from version 0.9.0 üòÅ). In the latest version, 0.9.3 I not only almost reworked the whole News article parsing process, but also added a lot of new supported languages (around 40 new languages) Repository : https://github.com/AndyTheFactory/newspaper4k Documentation : https://newspaper4k.readthedocs.io/ What My Project Does Newspaper4k helps you in extracting and curating articles from news websites. Leveraging automatic parsers and natural language processing (NLP) techniques, it aims to extract significant details such as: Title, Authors, Article Content, Images, Keywords, Summaries, and other relevant information and metadata from newspaper articles and web pages. The primary goal is to efficiently extract the main textual content of articles while eliminating any unnecessary elements or \"boilerplate\" text that doesn't contribute to the core information. Target Audience Newspaper4k is built for developers, researchers, and content creators who need to process and analyze news content at scale, providing them with powerful tools to automate the extraction and evaluation of news articles. Comparisons As of the 0.9.3 version, the library can also parse the Google News results based on keyword search, topic, country, etc The documentation is expanded and I added a series of usage examples. The integration with Playwright  is possible (for websites that generate the content with javascript), and since 0.9.3 I integrated cloudscraper that attempts to circumvent Cloudflair protections. Also, compared with the latest release of newspaper3k (0.2.8), the results on the Scraperhub Article Extraction Benchmark are much improved and the multithreaded news retrieval is now stable. Please don't hesitate to provide your feedback and make use of it! I highly value your input and encourage you to play around with the  project."},
{"Title": "We're building a Large Action Model (LAM) project that can do any task for you using Python!", "Author": "u/nobilis_rex_", "Content": "Hey guys! My friend and I are building Nelima. It's basically a Large Language Model designed to take actions on your behalf with natural language prompts and theoretically automate anything. For example, it can schedule appointments, send emails, check the weather, and even connect to IoT devices to let you command it ‚Äì you can ask it to publish a website or call an Uber for you! You can integrate your own custom actions, written in Python, to suit your specific needs, and layer multiple actions to perform more complex tasks. When you create these actions or functions, it contributes to the overall capabilities of Nelima, and everyone can now invoke the same action. Right now, it's a quite limited in terms of the # of actions it can do but we're having fun building a few :) Nelima can see the outcomes of each micro-action undertaken to achieve the overarching goal. The potential for reasoning is very much possible and doesn't shy away from taking measures ‚Äì for example, if it sees your grocery list from a sub-action on fulfilling an action and realizes that a certain item has allergens which might be harmful to the user, it puts a warning label, even though the user didn't ask for this. I thought the community here might find it useful. It uses Python 3 (Version 3.11), and the environment includes the following packages: BeautifulSoup, urllib3, requests, pyyaml. We‚Äôll try to include more if people need those. Give it a try and let me know what you think! :)"},
{"Title": "Is this project worth the time and effort: Camera to Keyboard?", "Author": "u/mnvoh", "Content": "EDIT: Since there are a lot of upvotes on that comment, yes, this is about seeking strangers' validations. But we're all validation seekers. For instance, we all need someone's validation to make a living (boss, customers, etc.) Your boss ain't gonna tell you \"god damn boy, you suck at your job, here take my money\". But in this case, validation seeking is kind of a broad term, let's use something more specialized like Market Research üòâ. Also, thanks for all your comments and inputs. Hello everyone I've been working on this project for the past 3 weeks, and I want people's opinions to determine how much time I should invest in it. Project Link: https://github.com/mnvoh/cameratokeyboard What My Project Does It's a computer vision project (python, YOLOv8) which essentially allows you to use your camera as a keyboard by detecting your fingers and analyzing their movements. The only requirement (besides having a camera) is that you have to print the keyboard (on an A4 paper, for example). The keyboard is for you and there are 4 markers identifying the boundaries of the keyboard which are for the program. Target Audience Currently, it's a PoC, but the goal is to develop it into a fully functioning virtual keyboard with a desirable accuracy. The initial idea came to me while thinking that it would be actually cool if you could print your keyboard design on your desk mat, and then just use that as a keyboard, but it could also have substantial applications in cell phones. Comparison I have searched quite a bit, but haven't found any similar solutions. EDIT: Thanks to u/avaqueue for finding these articles: https://www.academia.edu/105250798/Paper_Keyboard_Using_Image_Processing?uc-sb-sw=5982163 https://ieeexplore.ieee.org/document/6377072 Also, u/HobblingCobbler has mentioned that they've had a phone with this feature. waiting on more info The Main Question Now I have had an enormously gigantic amount of fun working on this and will continue for sure, but how much time depends on its potential. That's why I'm asking for your opinions: Is it actually worth it? Or am I imagining its potentials?"},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "What‚Äôs a script that you‚Äôve written that you still use frequently?", "Author": "u/HiT3Kvoyivoda", "Content": "Mine is a web scraper. It‚Äôs only like 50 lines of code. It takes in a link, pulls all the hyperlinks and then does some basic regex to pull out the info I want. Then it spits out a file with all the links. Took me like 20 minutes to code, but I feel like I use it every other week to pull a bunch of links for files I might want to download quickly or to pull data from sites to model."},
{"Title": "Alternative Queries: Typed and Reusable Handcrafted SQL", "Author": "u/baluyotraf", "Content": "Hello! It's still early in development, but I just want to know if people are interested. What My Project Does I created a library to integrate Pydantic type checking for handwritten SQL queries. It also allows you to test the types, and create nested queries easily. Alternative Queries Target Audience We had a project recently were we had to use handwritten SQL and managing the parameters and reusing queries was quite a hassle. So I'm planning to use this on my production projects moving forward. Comparison I would say in terms of usage, it's like Pydantic + SqlParams, but only with the python default formatting. SqlParams"},
{"Title": "Finite Element Analysis for Continuum Mechanics of Solid Bodies (FElupe)", "Author": "u/adtzlr", "Content": "Hi, I'd like to introduce to you my open-source FEA package FElupe , available on GitHub . Its target audience is typically an engineer, e.g. in the field of mechanical or biomedical engineering. It is designed to be both flexible for scientifc research as well as easy-to-use for industry-related problems. Packages with similar scopes are e.g. scikit-fem and Fenics(x). FElupe is a Python 3.8+ üêç finite element analysis package üì¶ focussing on the formulation and numerical solution of nonlinear problems in continuum mechanics üîß of solid bodies üöÇ. Its name is a combination of FE (finite element) and the german word Lupe üîç (magnifying glass) as a synonym for getting an insight üìñ how a finite element analysis code üßÆ looks like under the hood üï≥Ô∏è. FElupe has minimal requirements, all available at PyPI supporting all platforms. pip install felupe[all] It's pure Python but the assembly performance is well suited for mid-sized problems. It is possible to assemble up to 130000 degrees of freedom for linear elasticity on a modern notebook in one second runtime (results may vary). import felupe as fem mesh = fem.Cube(n=6) region = fem.RegionHexahedron(mesh) field = fem.FieldContainer([fem.Field(region, dim=3)]) boundaries, loadcase = fem.dof.uniaxial(field, clamped=True) umat = fem.OgdenRoxburgh(material=fem.NeoHooke(mu=1), r=3, m=1, beta=0) solid = fem.SolidBodyNearlyIncompressible(umat, field, bulk=5000) move = fem.math.linsteps([0, 1, 0, 1, 2, 1], num=5) step = fem.Step(items=[solid], ramp={boundaries[\"move\"]: move}, boundaries=boundaries) job = fem.CharacteristicCurve(steps=[step], boundary=boundaries[\"move\"]) job.evaluate(filename=\"result.xdmf\") fig, ax = job.plot( xlabel=\"Displacement $u$ in mm $\\longrightarrow$\", ylabel=\"Normal Force $F$ in N $\\longrightarrow$\", ) solid.plot(\"Principal Values of Cauchy Stress\").show() The above minmal code-block covers the essential high-level parts of creating and solving problems with FElupe. There is also a Gallery of Examples . Please let me know what you think about it. If you like it - I'd be glad if you share it with your friends, star it on GitHub, whatever you like. Thanks! https://github.com/adtzlr/felupe"},
{"Title": "Text extraction lib", "Author": "u/TraditionalAlps4337", "Content": "I created  a simple tool for extracting text from PDF, EPUB, TXT, and DOCX files.It is mainly for personal use, but I would really appreciate a feedback https://github.com/KirillAn/extractText/tree/main"},
{"Title": "PyBackport: Use instances from newer python releases", "Author": "u/tan_tata_chan", "Content": "What My Project Does PyBackport holds serves three purposes: Use instances from newer python releases in older python versions. For example, using StrEnum (new in python 3.11) in python 3.9 Enable experimental unreleased classes. For example, define new enums where members are tuples with TupleEnum Provide support to commonly used constants. For example, PyBackport allows to import and use colors.RGB to avoid re-defining constant color codes in every new project. Target Audience The audience is developers that must create scripts for older python versions, such as new packages which must support python 3.8+. With PyBackport these instances from newer python versions can be used. Comparison I've not encountered any similar released package with good documentation. I also choose not to set a release V1.0 for my package, as I backported only some enums functionality. Anyone feel free to open a new issue, contribute with new pull requests or to notify me any other useful instances to backport. I intend to keep expanding the package as much as possible. Links GitHub repo PyPI release Documentation"},
{"Title": "Distributing simple handy scripts for Windows - mouse-startable AND editable (the tricky one-liner)", "Author": "u/galkinvv", "Content": "In short: here is the hacky 2-line header that makes any python script a clickable batch file (assuming bundled python in a subfolder) @classmethod # 2>nul & (if not exist \"%~dp0\\python64-win\\python.exe\" (echo Fatal python64-win\\python.exe not found & pause) else (title %~f0 & \"%~dp0\\python64-win\\python.exe\" \"%~f0\" %*)) & exit /B & # noqa: E501 def __unused(): \"fake function to help writing header that allows executing same file as python and batch\" # Normal script starts here Its usage is either double-click to run without args or \"drag a file on it\" if the script processes a file (passed as an argument). Its installation is as easy as .zip unpacking More detailed: From time to time I get in touch with small handy one-file python scripts useful for some simple but very handy automations for a some niche use case on the windows platform. This post is not about tasks themselves - there may be really anything, but about distributing them. Such scripts are typically has no any repo/site and just attached to a message or similar, since the author has no any plans for further development, but still wants to share. 90% of target users are not developers, and just can use the \"double-clickable\" application. The other 10% are developers who may want to perform some script enhancements. Non-developers needs \"look at max 1-2 sentence documentation + several clicks to install + several to launch\". Developers need the ability to enhance the script. How to get both satisfied? Often such scripts are packed & bundled with a python into a single .exe - this is not editable, the other variant is distributing .py with the too large doc \"download python installer from official site, install it, then click the .py file\" and without an easy way to distribute dependency packages. As an author of some such scripts after several improve iterations I found a solution that satisfy both needs: distribute .zip containing bundled portable python interpreter with preinstalled dependencies in a subdirectory and a clickable file being simultaneously a .bat file and a python script - the extension is .py.bat or just .bat Getting portable python is a bit tricky, but embeddable official image is mostly ok."},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Chat with your friends within your (Mac & Linux) Terminal | Textual | Simple & Easy to use", "Author": "u/Plastic-Payment-934", "Content": "Toschat !What My Project Does? a lightweight python chat app within your terminal with simple UI and it's built on top of Textual framework. What you can do: - Create a new account - Add your friends to your contact - and chat! Target Audience It's my hobby project to improve my python knowledge but everyone can install and use it. All data store in a live server with Django framework. Comparison Instead of texting with your colleagues or your friends on your phone or a web browser, .. why not your terminal ? üòÅ I'm happy for advices on code improvements, features recommendations and feedbacks. üòÅüòÅ Github Link: https://github.com/MuongKimhong/toschat If you find it interesting and useful, give it a star I would appreciate it very much.Have a good weekend programmers!"},
{"Title": "Designing a Pure Python Web Framework", "Author": "u/Pleasant-Cow-3898", "Content": "From the Article: This provides a good overview of how Reflex works under the hood. TLDR : Under the hood, Reflex apps compile down to a React frontend app and a FastAPI backend app. Only the UI is compiled to Javascript; all the app logic and state management stays in Python and is run on the server. Reflex uses WebSockets to send events from the frontend to the backend, and to send state updates from the backend to the frontend. Full post: https://reflex.dev/blog/2024-03-21-reflex-architecture/#designing-a-pure-python-web-framework"},
{"Title": "UFO Tools- functional programming in python", "Author": "u/houseofleft", "Content": "I shared a \"monads in python library\" a little while back on here- since then I've been toying with and expanding things out into a more general project for helping functional programming patterns in python. [I had (am having) a bunch of fun making this - thought I'd share here!]( https://github.com/benrutter/ufo-tools ) What my project does: A simple library with utilities for functional programming in Python. Target Audience: It's meant for use in actual production contexts, but it's still in a very early and experimental phase. It's probably most helpful right now as a learning resource. Comparison: Most similar project I know of is the Returns library, which is great, but a lot more heavy weight. It has more of an expectation of being used for everything, rather than a drop in as-and-when tool. I'd love any feedback in general (as well as any healthy flamewars around the herecy or one-true-way of functional programming)"},
{"Title": "Code type checks but then `TypeError`s", "Author": "u/Kiuhnm", "Content": "Have you noticed that Python's static type system is unsound? Look at this code: from typing import overload @overload def f(x: int) -> str: ... @overload def f(x: str) -> int: ... def f(x: int | str) -> int | str: match x: case int(): return 1 case str(): return 'a' print('a' + f(1)) print(1 + f('a')) The code above type checks, but raises TypeError when executed. With proper overloading this would never happen. Can you think of other cases? You're not allowed to use cast , Any , to omit type hints, to use stubs, or to cheat in similar ways."},
{"Title": "I made a free easy-to-use toast notification library for PyQt and PySide", "Author": "u/niklashnng", "Content": "Hey guys, since I couldn't find any good libraries for showing modern-looking toast notifications in PyQt, I made one myself. What My Project Does: It supports showing multiple toasts at the same time, queueing of toasts, 6 different positions, multiple screens and much more. Since it's developed with QtPy, an abstraction layer for multiple versions of PyQt and PySide, you can use it with PyQt5, PyQt6, PySide2, and PySide6. Also, basically anything can be completely customized and it's extremely easy to use. Target Audience: This is useful for any Python developer who is working with PyQt or PySide and wants to display clean and modern-looking toast nofifications easily. Comparison: Since I wasn't able to find any real libraries for PyQt toasts, I can't really give a comparison. Preview: https://github.com/niklashenning/pyqt-toast/assets/58544929/f4d7f4a4-6d69-4087-ae19-da54b6da499d Github: https://github.com/niklashenning/pyqt-toast Hope this helps :)"},
{"Title": "Who's coming to PyCon Italia this year?", "Author": "u/patrick91it", "Content": "Hey folks! Is anyone of you attending this year PyCon Italia ? I'm one of the organisers and I'd love to connect to new people this year üòä See you there! üêç‚ú®"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "python notebook+ insightface + inswapper_128", "Author": "u/Maleficent_Yak_993", "Content": "I moved from C to python and absolutely loving it. While looking into code of available face swappers I realized they are mostly wrapper on insightface and inswapper. So i thought I'll strip it all down to bare minimum, and the results were better than expected. I've captured my learnings in this video, if you're interested. https://www.youtube.com/watch?v=Ju_reA3zQso"},
{"Title": "Leaping: Fix tests instantly with an omniscient debugger", "Author": "u/kpetkar", "Content": "Link to Repo: https://github.com/leapingio/leaping What My Project Does We‚Äôve all been in hell when you have no idea why a test might be failing. You set a breakpoint, add print statements, and re-run the code, all to realize that you added them in the wrong spot or need to step backward. Leaping is a simple, fast and lightweight omniscient debugger for Python tests. Leaping traces the execution of your code and allows you to retroactively inspect the state of your program at any time, using an LLM-based debugger with natural language. Using Leaping, you can quickly get the answer to questions like: What was the value of variable x at this point? What was variable y set to this value? Why am I not hitting function x? What changes can I make to this test/code to make it pass? Here‚Äôs a brief demo of it in action: https://www.loom.com/share/94ebe34097a343c39876d7109f2a1428 Target Audience This is useful for any Python developer who is struggling with a test or even just an issue from production Comparison There‚Äôs just one alternative https://github.com/gleb-sevruk/pycrunch-trace . It‚Äôs different because it requires you to meticulously instrument your code, is quite difficult to set up (we built this because it took us almost a day to do so!) and requires uploading your trace data to a somewhat janky web portal. Here‚Äôs a link to the repo and we‚Äôd love it if you played around with it. We‚Äôre committed to being open-source and welcome all issues, feature requests or even contributions!"},
{"Title": "Do you like `def call() -> None: ...`", "Author": "u/silently--here", "Content": "So, I wanted to get a general idea about how people feel about giving return type hint of None for a function that doesn't return anything. With the introduction of PEP 484, type hints were introduced and we all rejoiced. Lot of my coworkers just don't get the importance of type hints and I worked way too hard to get everyone onboarded so they can see how incredibly useful it is! After some time I met a coworker who is a fan of typing and use it well... except they write -> None everywhere! Now this might be my personal opinion, but I hate this because it's redundant and not to mention ugly (at least to me). It is implicit and by default, functions return None in python, and I just don't see why -> None should be used. We have been arguing a lot over this since we are building a style guide for the team and I wanted to understand what the general consensus is about this. Even in PEP 484, they have mentioned that -> None should be used for __init__ functions and I just find that crazy. Am I in the wrong here? Is this fight pointless? What are your opinions on the matter?"},
{"Title": "EyeGestures: Opensource eyetracking python library", "Author": "u/TraditionalDistrict9", "Content": "Hey all! Wanted to share opensource project for all interested in building interfaces based on eyetracking. What My Project Does: EyeGestures is python library offering e2e eyetracking with blinking detection, and gaze fixation. It provides eye-motion-controlled cursor. Comparison: I have not found other e2e eye tracking library with focus on building interfaces. We offer eye-motion following cursor, with built-in switchable configuration, gaze detection and blinking. Furthermore webAPI provides activation zones, and eye-magnetic buttons to ease user control. Target Audience: GameDev, OS, App and Frontend Engineers wanting utilize eyetracking capabilities in their apps. BROADER DESCRIPTION: EyeGestures is python library, desktop app, and paid web API (only webAPI is paid if you want to use our servers, nothing stops you from adding library to your backend and have it for free :) ). It is not perfect yet, but we work towards making it smoother and easier experience for both users and developers. The idea of the project is to bring inexpensive eyetracking to everyone having device armed with native webcam or phone cam. Most of OSs support eye-tracking but only when having additional quite pricey hardware, we want to change it with opensource! Bare with us, it is still early stage project and team (basically me, but having some help from time to time) is working on bringing more documentation and making tracker better. For now we have cursor following motion of eye, calibration for it, blinking detection and gaze fixation detection. Feel free to check our [repo]( https://github.com/NativeSensors/EyeGestures/tree/main ), and [main_demo1]( https://eyegestures.com/ ) [demo2]( https://eyegestures.com/game ) [demo3]( https://eyegestures.com/cinema ): You can contact us/me under: contact@eyegestures.com"},
{"Title": "Free Review Copies of \"Python Real-World Projects\"", "Author": "u/entreluvkash", "Content": "Packt has published \"Python Real-World Projects\" As part of our marketing activities, we are offering free digital copies of the book in return for unbiased feedback in the form of a reader review. Here is what you will learn from the book: Explore core deliverables for an application including documentation and test cases Discover approaches to data acquisition such as file processing, RESTful APIs, and SQL queries Create a data inspection notebook to establish properties of source data Write applications to validate, clean, convert, and normalize source data Use foundational graphical analysis techniques to visualize data Build basic univariate and multivariate statistical analysis tools Create reports from raw data using JupyterLab publication tools If you feel you might be interested in this opportunity please comment below on or before 31st March 2024 Amazon Link"},
{"Title": "The wrong way to speed up your code with Numba", "Author": "u/itamarst", "Content": "If your NumPy-based code is too slow, you can sometimes use Numba to speed it up. Numba is a compiled language that uses the same syntax as Python, and it compiles at runtime, so it‚Äôs very easy to write. And because it re-implements a large part of the NumPy APIs, it can also easily be used with existing NumPy-based code. However, Numba‚Äôs NumPy support can be a trap: it can lead you to missing huge optimization opportunities by sticking to NumPy-style code. In this article I show examples of: The wrong way to use Numba, writing NumPy-style full array transforms. The right way to use Numba, namely for loops."},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "UXsim: Network traffic flow simulator in pure Python", "Author": "u/Balance-", "Content": "Disclaimer: Not my project but a very interesting one I encountered recently UXsim is a free, open-source traffic flow simulator designed for large-scale vehicular transportation simulations, developed purely in Python. Whether you're a researcher, student, or enthusiast in transportation, UXsim offers a comprehensive toolkit for macroscopic and mesoscopic traffic flow simulations. What My Project Does UXsim stands out by providing an easy-to-use Python implementation of standard models for dynamic network traffic flow. It's capable of simulating traffic dynamics in large networks, incorporating elements such as traffic signals, inflow control, route guidance, and congestion pricing. The simulator supports dynamic traffic assignments and comes with features for analyzing and visualizing simulation results, making it highly useful for scientific and educational purposes. Target Audience This project is aimed at both scientific and educational communities. While UXsim's primary goal is to support academic research and teaching in transportation and traffic flow modeling, its open-source nature and Python implementation make it accessible for anyone interested in traffic simulation. It is not just a toy project but a robust tool that can be used for serious research and educational purposes, potentially even in production scenarios with further customization. Comparison Compared to existing alternatives, UXsim differentiates itself in several key ways: Pure Python Implementation: This makes UXsim easily integratable with other Python-based frameworks, such as PyTorch for deep reinforcement learning in traffic control. It offers a high degree of customization and flexibility not always available in other simulators. Comprehensive Modeling and Visualization: UXsim combines Newell's simplified car-following model, Lagrangian Incremental Node Model, and a Dynamic User Optimum-type Route Choice Model. This comprehensive approach, coupled with its visualization capabilities, provides a deep understanding of traffic flow dynamics. Educational Value: Thanks to its simplicity and extensive documentation, UXsim serves as an excellent educational tool, helping students and newcomers understand the complexities of traffic flow and management. UXsim has already shown its capabilities through examples like large-scale city simulations and deep reinforcement learning for traffic signal control. For anyone looking to dive into the details of traffic flow simulation or seeking a platform for transportation research, UXsim is a versatile and powerful tool. Links GitHub: UXsim Repository Docs: https://toruseo.jp/UXsim/docs/ Images https://github.com/toruseo/UXsim/blob/images/gridnetwork_macro.gif https://github.com/toruseo/UXsim/blob/images/gridnetwork_fancy.gif https://github.com/toruseo/UXsim/blob/images/tsd_traj_links_grid.png https://github.com/toruseo/UXsim/blob/images/anim_network1_0.22_nocontrol.gif"},
{"Title": "üåà StreamJoy turns your stream of images into animations, in parallel", "Author": "u/zethiroth", "Content": "Docs here: https://ahuang11.github.io/streamjoy/ Repo here: https://github.com/ahuang11/streamjoy/tree/main What My Project Does Streamjoy turns your images into animations using sensible defaults for fun, hassle-free creation. It cuts down the boilerplate and time to work on animations, and it's simple to start with just a few lines of code. Target Audience For scientists or hobbyists that want to see something in motion and analyze it! It's still in beta, but would love feedback! Comparison Unlike other libraries, this runs in parallel, and I really like how it can add intros and pauses by specifying keywords! I also like how simple it is to use with pandas and xarray--just invoke the streamjoy method after import. Example import xarray as xr import streamjoy.xarray ds = xr.tutorial.open_dataset(\"air_temperature\") ds.streamjoy(\"air_temperature.mp4\") StreamJoy also supports custom renderers. import matplotlib.pyplot as plt import numpy as np from streamjoy import stream, wrap_matplotlib @wrap_matplotlib() def plot_frame(i): x = np.linspace(0, 2, 1000) y = np.sin(2 * np.pi * (x - 0.01 * i)) fig, ax = plt.subplots() ax.plot(x, y) return fig if __name__ == \"__main__\": stream(list(range(10)), uri=\"sine_wave.gif\", renderer=plot_frame) Install it with just pip to start: pip install streamjoy"},
{"Title": "Pyconf from Philly (Carpool/Hotel)", "Author": "u/the_ultimate_stoic", "Content": "Anyone going to pyconf from Philly? Looking to carpool and possibly get a Airbnb near the spot! lmk if anyone wants to join."},
{"Title": "PyPI Support Specialist", "Author": "u/monorepo", "Content": "PSF Official"},
{"Title": "quickapiclient: Create fully typed declarative API clients quickly and easily", "Author": "u/martinn404", "Content": "Hi all! I've had an idea for a while around creating a library that would allow you to quickly build new API clients that are fully typed. Last week I finally got around to building it and am keen to see what everyone thinks. What My Project Does A library for creating fully typed declarative API clients quickly and easily. So you could build an API integration with other services very quickly, and in a declarative way. And it would ensure that your request/query params are valid/correct before sending them (avoiding potential errors). As well as making the API response easy to work with, matching your own models rather than trying to work with JSON blobs. Target Audience It's still in early development but should work fine for a lot of use cases. I'm keen to hear if people find something like this useful and whether it works for your use cases or not so that I can continue to expand it. Comparison There's a few libraries that allow you to create API clients (and even some that auto generate API clients based on an OpenAPI spec). But their interfaces are (IMO) a bit more clunky and most don't seem to support serialization/deserialization or typing very well. What would an API client with this library look like? Glad you asked. Currently, it would look something like this (For a single API endpoint over HTTP GET): import attrs import quickapi # An example type that will be part of the API response @attrs.define class Fact: fact: str length: int # What the API response should look like @attrs.define class ResponseBody(quickapi.BaseResponseBody): current_page: int data: list[Fact] # Now we can define our API class MyApi(quickapi.BaseApi[ResponseBody]): url = \"https://catfact.ninja/facts\" response_body = ResponseBody And you would use it like this: api_client = MyApi() response = api_client.execute() # That's it! Now response is fully typed and conforms to our ResponseBody definition assert isinstance(response.body, ResponseBody) assert isinstance(response.body.data[0], Fact) But my goal is to eventually be able to define it all in a single class and allow using different serialization/deserialization libraries. So it could end up looking something closer to this (For a single endpoint over HTTP POST): import quickapi @quickapi.api class MyApi: url = \"https://catfact.ninja/facts\" method = quickapi.ApiMethod.POST class RequestBody: required_input: str optional_input: str | None = None class ResponseBody: current_page: int data: list[Fact] Github: https://github.com/martinn/quickapiclient What do you guys think?"},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Every dunder method in Python", "Author": "u/treyhunner", "Content": "For years my training students have been asking me for a list of all the dunder methods. The Python docs don't have such a list, so I compiled my own... after having on my to-do list for years. Every dunder method in Python I realized why it took me so long during when I finally finished compiling the table of all of them... there are over 100 dunder methods in Python! üíØ Edit: I should have said \"the Python docs don't have such a list in a single row-by-row table\". The Data Model page does indeed include a giant \"Special Names\" section and a \"Coroutines\" section which document nearly every special method, but it's quite challenging to skim and not *quite* complete."},
{"Title": "Grocery delivery with Python (Company Engineering blog post)", "Author": "u/svena33", "Content": "Hope that sharing this is allowed here, let me know otherwise. I just published a new post on our engineering blog on why and how we‚Äôre using Python at Picnic (an online supermarket in the EU). I think it might be interesting for this sub as well. https://picnic.app/careers/python-picnic"},
{"Title": "EOmaps v8.0 is here!", "Author": "u/se_pp", "Content": "Hey all, I'm excited to announce that EOmaps v8.0 is now released and that it has been accepted into the PyOpenSci ecosystem ! What is it? EOmaps is a pyhton package to visualize and analyze geographical datasets. It's 100% free and open-source! You can find all information you need to install (or contribute üöÄ) in the documentation and on GitHub ! Check it out and let me know what you think! EOmaps v8.0 brings a lot of updates, fixes and improvements like: A switch to BSD-3-Clause License A much improved documentation (new API docs, contribution guide etc.) Better management of optional dependencies with pip install Pre-configured Jupyter Widgets for Jupyter Notebooks ... and much more!"},
{"Title": "Stockstir - Instantly and easily gather real time stock data from any Python script at no cost", "Author": "u/PatzEdi", "Content": "Hello! It has been a while since I have made a post about Stockstir, the tool to gather stock information from any script at no cost, written in Python. After a couple months from my last post, I have updated and refined bugs and other issues, as well as added new functions for enhanced usability. The latest version is now 2.1.4. What My Project Does Stockstir is an easy way to instantly gather stock data from any of your Python scripts. Not only that, but it includes other features, such as multi data gathering, anti ban, a fail-safe mechanism, random user agents, and much more. Target Audience Stockstir is for everyone that needs to gather realtime company stock info from any of their scripts. It mostly differs from any other stock related project in the way that it is simple, and doesn't rely on apis. Comparison Stockstir differs from other methods of gathering stock data in that it is has a very simple concept behind it. It is largely a GET wrapper in the Tools class, but initial API support such as Alpha Vantage, as well as gathering much more data of a Company stock through cnbc's JSON api, under the API class. It is mostly a quick way to gather stock data through simple use. You can find installation instructions and other information under the project link provided below: Link: Stockstir Project Link To those of you that are new to Stockstir, V2 added a plethora of new features including a fail-safe mechanism (more on that further down), and V2.1.0 reconstructed the entire codebase to further match PEP guidelines upon the request and suggestion from many of you. To see the latest Changelog information, visit the CHANGELOG.md file located in the project files hosted on Github. Here are a few examples of the different usages of Stockstir: Quick Usage: To easily gather a single price of a company's stock, you can do it in one line. from stockstir import Stockstir price = Stockstir().tools.get_single_price(\"ticker/stockSymbol\") print(price) The above Stockstir method get_single_price is one of the most basic of the functions provided. Stockstir Object Instantiation Since update V2, you can instantiate Stockstir as an object, and customize certain parameters: from stockstir import Stockstir s = Stockstir() # Instantiate the Stockstir object, like so. # We can also create a new Stockstir object, if for example you need certain options toggled: s2 = Stockstir(print_output=True, random_user_agent=True, provider='cnbc') Stockstir Functionality, the Fail-Safe mechanism, and Providers: I am not going to cover the entirety of Stockstir functionality here, which is why Stockstir has a readthedocs.io documentation: Stockstir Documentation However, basic Stockstir functionality can be described as a GET wrapper. It has providers , or, in other words, a website, and a regex pattern to find the price based the request made. Providers are a large part of Stockstir. The fail-safe mechanism chooses a new provider that works, in case it fails. A provider fails if the provider has changed their code base and the regex can't find anything, or, if the website the request is made to has changed or is down. To counter provider failure in case a prpovider fails, automated checks on my side run five times a day. If one of them doesn't work, I get notified within a short period of time. You can choose between 'cnbc', 'insiders', or 'zacks' for the providers. 'cnbc' is the default. To view working providers, you can do so like this: from stockstir import Stockstir s = Stockstir(provider='cnbc') #You can set the provider via the provider option in the Stockstir instantiation. Default will always be cnbc. s.providers.list_available_providers() # list the available providers. Many Thanks Thank you for trying out Stockstir, or even just looking into trying it!"},
{"Title": "Dask Demo Day: Dask on Databricks, scale embedding pipelines, and Prefect on the cloud", "Author": "u/dask-jeeves", "Content": "I wanted to share the talks from last month‚Äôs Dask Demo Day, where folks from the Dask community give short demos to show off ongoing work. Hopefully this helps elevate some of the great work people are doing. Last month‚Äôs talks: - One trillion row challenge - Deploy Dask on Databricks with dask-databricks - Deploy Prefect workflows on the cloud with Coiled - Scale embedding pipelines (LlamaIndex + Dask) - Use AWS Cost Explorer to see the cost of public IPv4 addresses Recording on YouTube: https://www.youtube.com/watch?v=07e1JL83ur8 Join the next one this Thursday, March 21st, 11am ET https://github.com/dask/community/issues/307"},
{"Title": "slack-progress-bar: A progress bar for your Slack Bots", "Author": "u/UnemployedPython", "Content": "I built this after having conceptually designed it in an interview but failed to get the job. Figured I might as well build it. Any thoughts or advice welcome! How My Project Works This progress bar connects with your Slack Bot account via tokens to send you a message as something is loading. It updates the progress bar as called by the user to edit the message on Slack, always keeping you up to date on how progress is going (ex. Used in an application that's loading something, running tests, etc.) Target Audience This can be used in a professional setting where you want to notify employees of certain progress. Also can be used for personal projects and / or design teams in universities that use Slack. Comparison slack-progress : This project exists but has not been supported for a long time, as it's an old version of Python and the slacker api has been deprecated. Github: https://github.com/mlizzi/slack-progress-bar pypi: https://pypi.org/project/slack-progress-bar/"},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "How To Add Multiple Pages to NiceGUI", "Author": "u/bitdoze", "Content": "Created an article and a video that will help beginners to NiceGUI add multiple pages to NiceGUI to better structure the app. https://www.bitdoze.com/nicegui-pages/"},
{"Title": "btstrm: Torrent Streaming Program for Linux", "Author": "u/hurleymjohnston", "Content": "I'll disclaim: I'm not a programmer but a litterateur, my code may be quite bad and in some ( some!) places written with LLMs - but it works as should be and its simplicity makes me admire Python. // The beginning of lyric digression For the last 7 years I've been writing scripts in bash. I've accumulated quite a lot of them, I would say that it became a kind of escapism for me, in anxious moments of my life, when I was too lazy to do something on my own, I prefer to spend half a night writing a script, so that I could press one button and - bang - everything would work by itself. At one point, Bash became not enough. Despite the fact that it became convenient and fast to write in it, in fact, it works slowly enough to create some serious projects. And at one point, my friend encouraged me to use Python, luckily he knows Python and I had an idea that came to life. So now I use my Python program literally every day. // End of lyric digression I watch movies and shows often (don't blame me for piracy, of course it's Big Buck Bunny every time) and since I don't like to click a lot of buttons, I wrote a 500 line script (hopefully someone will recognize themselves in this :D). https://github.com/asakura42/btstrm // https://pypi.org/project/btstrm/ What My Project Does This program can: Search for movies using TMDB in a given language and show the list of found movies along with posters (without API key!) Search torrents by name using Jackett (local torrent aggregator). Stream them with your favorite player (personally I use mpv). Stream any torrent or magnet that contain video files Screenshots: Searching movie: https://files.catbox.moe/3ine82.png List of torrents: https://files.catbox.moe/u3i9va.png Torrent selected, 10 seconds passed: https://files.catbox.moe/eyr3uz.png And finally: https://files.catbox.moe/7x3sss.png Target Audience Literally every movie/show enjoyer. Comparison You may ask - \"why? There are peerflix and webtorrent\". These programs may be enough for some people. But, first of all, there is no search for torrents in them, which is quite important and saves a lot of time. Secondly, btstrm uses btfs to work with bittorrent network. Unlike webtorrent ( javascript, meh ), btfs is written in C and uses libtorrent , and it \"mounts\" the torrent as a file system, which makes it easier to work with the files themselves if you need to, and it doesn't clog up your /tmp with multi-gigabyte files. Other programs it uses are chafa for showing movie posters, fzf for selecting a movie or torrent, and a few Python libraries. Also, if you are learning languages, and listen to condensed audio with impd , the program has a flag to easily add audio to your collection. At first time this program was just a product of simple merging of btfsplay script and some functions to search in Jackett. But now it is program that shows progress, uses many arguments and... just does its job."},
{"Title": "I made a free easy-to-use toast notification library for PyQt and PySide", "Author": "u/niklashnng", "Content": "Hey guys, since I couldn't find any good libraries for showing modern-looking toast notifications in PyQt, I made one myself. What My Project Does: It supports showing multiple toasts at the same time, queueing of toasts, 6 different positions, multiple screens and much more. Since it's developed with QtPy, an abstraction layer for multiple versions of PyQt and PySide, you can use it with PyQt5, PyQt6, PySide2, and PySide6. Also, basically anything can be completely customized and it's extremely easy to use. Target Audience: This is useful for any Python developer who is working with PyQt or PySide and wants to display clean and modern-looking toast nofifications easily. Comparison: Since I wasn't able to find any real libraries for PyQt toasts, I can't really give a comparison. Preview: https://github.com/niklashenning/pyqt-toast/assets/58544929/f4d7f4a4-6d69-4087-ae19-da54b6da499d Github: https://github.com/niklashenning/pyqt-toast Hope this helps :)"},
{"Title": "Who's coming to PyCon Italia this year?", "Author": "u/patrick91it", "Content": "Hey folks! Is anyone of you attending this year PyCon Italia ? I'm one of the organisers and I'd love to connect to new people this year üòä See you there! üêç‚ú®"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "python notebook+ insightface + inswapper_128", "Author": "u/Maleficent_Yak_993", "Content": "I moved from C to python and absolutely loving it. While looking into code of available face swappers I realized they are mostly wrapper on insightface and inswapper. So i thought I'll strip it all down to bare minimum, and the results were better than expected. I've captured my learnings in this video, if you're interested. https://www.youtube.com/watch?v=Ju_reA3zQso"},
{"Title": "Leaping: Fix tests instantly with an omniscient debugger", "Author": "u/kpetkar", "Content": "Link to Repo: https://github.com/leapingio/leaping What My Project Does We‚Äôve all been in hell when you have no idea why a test might be failing. You set a breakpoint, add print statements, and re-run the code, all to realize that you added them in the wrong spot or need to step backward. Leaping is a simple, fast and lightweight omniscient debugger for Python tests. Leaping traces the execution of your code and allows you to retroactively inspect the state of your program at any time, using an LLM-based debugger with natural language. Using Leaping, you can quickly get the answer to questions like: What was the value of variable x at this point? What was variable y set to this value? Why am I not hitting function x? What changes can I make to this test/code to make it pass? Here‚Äôs a brief demo of it in action: https://www.loom.com/share/94ebe34097a343c39876d7109f2a1428 Target Audience This is useful for any Python developer who is struggling with a test or even just an issue from production Comparison There‚Äôs just one alternative https://github.com/gleb-sevruk/pycrunch-trace . It‚Äôs different because it requires you to meticulously instrument your code, is quite difficult to set up (we built this because it took us almost a day to do so!) and requires uploading your trace data to a somewhat janky web portal. Here‚Äôs a link to the repo and we‚Äôd love it if you played around with it. We‚Äôre committed to being open-source and welcome all issues, feature requests or even contributions!"},
{"Title": "Do you like `def call() -> None: ...`", "Author": "u/silently--here", "Content": "So, I wanted to get a general idea about how people feel about giving return type hint of None for a function that doesn't return anything. With the introduction of PEP 484, type hints were introduced and we all rejoiced. Lot of my coworkers just don't get the importance of type hints and I worked way too hard to get everyone onboarded so they can see how incredibly useful it is! After some time I met a coworker who is a fan of typing and use it well... except they write -> None everywhere! Now this might be my personal opinion, but I hate this because it's redundant and not to mention ugly (at least to me). It is implicit and by default, functions return None in python, and I just don't see why -> None should be used. We have been arguing a lot over this since we are building a style guide for the team and I wanted to understand what the general consensus is about this. Even in PEP 484, they have mentioned that -> None should be used for __init__ functions and I just find that crazy. Am I in the wrong here? Is this fight pointless? What are your opinions on the matter?"},
{"Title": "EyeGestures: Opensource eyetracking python library", "Author": "u/TraditionalDistrict9", "Content": "Hey all! Wanted to share opensource project for all interested in building interfaces based on eyetracking. What My Project Does: EyeGestures is python library offering e2e eyetracking with blinking detection, and gaze fixation. It provides eye-motion-controlled cursor. Comparison: I have not found other e2e eye tracking library with focus on building interfaces. We offer eye-motion following cursor, with built-in switchable configuration, gaze detection and blinking. Furthermore webAPI provides activation zones, and eye-magnetic buttons to ease user control. Target Audience: GameDev, OS, App and Frontend Engineers wanting utilize eyetracking capabilities in their apps. BROADER DESCRIPTION: EyeGestures is python library, desktop app, and paid web API (only webAPI is paid if you want to use our servers, nothing stops you from adding library to your backend and have it for free :) ). It is not perfect yet, but we work towards making it smoother and easier experience for both users and developers. The idea of the project is to bring inexpensive eyetracking to everyone having device armed with native webcam or phone cam. Most of OSs support eye-tracking but only when having additional quite pricey hardware, we want to change it with opensource! Bare with us, it is still early stage project and team (basically me, but having some help from time to time) is working on bringing more documentation and making tracker better. For now we have cursor following motion of eye, calibration for it, blinking detection and gaze fixation detection. Feel free to check our [repo]( https://github.com/NativeSensors/EyeGestures/tree/main ), and [main_demo1]( https://eyegestures.com/ ) [demo2]( https://eyegestures.com/game ) [demo3]( https://eyegestures.com/cinema ): You can contact us/me under: contact@eyegestures.com"},
{"Title": "Free Review Copies of \"Python Real-World Projects\"", "Author": "u/entreluvkash", "Content": "Packt has published \"Python Real-World Projects\" As part of our marketing activities, we are offering free digital copies of the book in return for unbiased feedback in the form of a reader review. Here is what you will learn from the book: Explore core deliverables for an application including documentation and test cases Discover approaches to data acquisition such as file processing, RESTful APIs, and SQL queries Create a data inspection notebook to establish properties of source data Write applications to validate, clean, convert, and normalize source data Use foundational graphical analysis techniques to visualize data Build basic univariate and multivariate statistical analysis tools Create reports from raw data using JupyterLab publication tools If you feel you might be interested in this opportunity please comment below on or before 31st March 2024 Amazon Link"},
{"Title": "The wrong way to speed up your code with Numba", "Author": "u/itamarst", "Content": "If your NumPy-based code is too slow, you can sometimes use Numba to speed it up. Numba is a compiled language that uses the same syntax as Python, and it compiles at runtime, so it‚Äôs very easy to write. And because it re-implements a large part of the NumPy APIs, it can also easily be used with existing NumPy-based code. However, Numba‚Äôs NumPy support can be a trap: it can lead you to missing huge optimization opportunities by sticking to NumPy-style code. In this article I show examples of: The wrong way to use Numba, writing NumPy-style full array transforms. The right way to use Numba, namely for loops."},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "UXsim: Network traffic flow simulator in pure Python", "Author": "u/Balance-", "Content": "Disclaimer: Not my project but a very interesting one I encountered recently UXsim is a free, open-source traffic flow simulator designed for large-scale vehicular transportation simulations, developed purely in Python. Whether you're a researcher, student, or enthusiast in transportation, UXsim offers a comprehensive toolkit for macroscopic and mesoscopic traffic flow simulations. What My Project Does UXsim stands out by providing an easy-to-use Python implementation of standard models for dynamic network traffic flow. It's capable of simulating traffic dynamics in large networks, incorporating elements such as traffic signals, inflow control, route guidance, and congestion pricing. The simulator supports dynamic traffic assignments and comes with features for analyzing and visualizing simulation results, making it highly useful for scientific and educational purposes. Target Audience This project is aimed at both scientific and educational communities. While UXsim's primary goal is to support academic research and teaching in transportation and traffic flow modeling, its open-source nature and Python implementation make it accessible for anyone interested in traffic simulation. It is not just a toy project but a robust tool that can be used for serious research and educational purposes, potentially even in production scenarios with further customization. Comparison Compared to existing alternatives, UXsim differentiates itself in several key ways: Pure Python Implementation: This makes UXsim easily integratable with other Python-based frameworks, such as PyTorch for deep reinforcement learning in traffic control. It offers a high degree of customization and flexibility not always available in other simulators. Comprehensive Modeling and Visualization: UXsim combines Newell's simplified car-following model, Lagrangian Incremental Node Model, and a Dynamic User Optimum-type Route Choice Model. This comprehensive approach, coupled with its visualization capabilities, provides a deep understanding of traffic flow dynamics. Educational Value: Thanks to its simplicity and extensive documentation, UXsim serves as an excellent educational tool, helping students and newcomers understand the complexities of traffic flow and management. UXsim has already shown its capabilities through examples like large-scale city simulations and deep reinforcement learning for traffic signal control. For anyone looking to dive into the details of traffic flow simulation or seeking a platform for transportation research, UXsim is a versatile and powerful tool. Links GitHub: UXsim Repository Docs: https://toruseo.jp/UXsim/docs/ Images https://github.com/toruseo/UXsim/blob/images/gridnetwork_macro.gif https://github.com/toruseo/UXsim/blob/images/gridnetwork_fancy.gif https://github.com/toruseo/UXsim/blob/images/tsd_traj_links_grid.png https://github.com/toruseo/UXsim/blob/images/anim_network1_0.22_nocontrol.gif"},
{"Title": "üåà StreamJoy turns your stream of images into animations, in parallel", "Author": "u/zethiroth", "Content": "Docs here: https://ahuang11.github.io/streamjoy/ Repo here: https://github.com/ahuang11/streamjoy/tree/main What My Project Does Streamjoy turns your images into animations using sensible defaults for fun, hassle-free creation. It cuts down the boilerplate and time to work on animations, and it's simple to start with just a few lines of code. Target Audience For scientists or hobbyists that want to see something in motion and analyze it! It's still in beta, but would love feedback! Comparison Unlike other libraries, this runs in parallel, and I really like how it can add intros and pauses by specifying keywords! I also like how simple it is to use with pandas and xarray--just invoke the streamjoy method after import. Example import xarray as xr import streamjoy.xarray ds = xr.tutorial.open_dataset(\"air_temperature\") ds.streamjoy(\"air_temperature.mp4\") StreamJoy also supports custom renderers. import matplotlib.pyplot as plt import numpy as np from streamjoy import stream, wrap_matplotlib @wrap_matplotlib() def plot_frame(i): x = np.linspace(0, 2, 1000) y = np.sin(2 * np.pi * (x - 0.01 * i)) fig, ax = plt.subplots() ax.plot(x, y) return fig if __name__ == \"__main__\": stream(list(range(10)), uri=\"sine_wave.gif\", renderer=plot_frame) Install it with just pip to start: pip install streamjoy"},
{"Title": "Pyconf from Philly (Carpool/Hotel)", "Author": "u/the_ultimate_stoic", "Content": "Anyone going to pyconf from Philly? Looking to carpool and possibly get a Airbnb near the spot! lmk if anyone wants to join."},
{"Title": "PyPI Support Specialist", "Author": "u/monorepo", "Content": "PSF Official"},
{"Title": "quickapiclient: Create fully typed declarative API clients quickly and easily", "Author": "u/martinn404", "Content": "Hi all! I've had an idea for a while around creating a library that would allow you to quickly build new API clients that are fully typed. Last week I finally got around to building it and am keen to see what everyone thinks. What My Project Does A library for creating fully typed declarative API clients quickly and easily. So you could build an API integration with other services very quickly, and in a declarative way. And it would ensure that your request/query params are valid/correct before sending them (avoiding potential errors). As well as making the API response easy to work with, matching your own models rather than trying to work with JSON blobs. Target Audience It's still in early development but should work fine for a lot of use cases. I'm keen to hear if people find something like this useful and whether it works for your use cases or not so that I can continue to expand it. Comparison There's a few libraries that allow you to create API clients (and even some that auto generate API clients based on an OpenAPI spec). But their interfaces are (IMO) a bit more clunky and most don't seem to support serialization/deserialization or typing very well. What would an API client with this library look like? Glad you asked. Currently, it would look something like this (For a single API endpoint over HTTP GET): import attrs import quickapi # An example type that will be part of the API response @attrs.define class Fact: fact: str length: int # What the API response should look like @attrs.define class ResponseBody(quickapi.BaseResponseBody): current_page: int data: list[Fact] # Now we can define our API class MyApi(quickapi.BaseApi[ResponseBody]): url = \"https://catfact.ninja/facts\" response_body = ResponseBody And you would use it like this: api_client = MyApi() response = api_client.execute() # That's it! Now response is fully typed and conforms to our ResponseBody definition assert isinstance(response.body, ResponseBody) assert isinstance(response.body.data[0], Fact) But my goal is to eventually be able to define it all in a single class and allow using different serialization/deserialization libraries. So it could end up looking something closer to this (For a single endpoint over HTTP POST): import quickapi @quickapi.api class MyApi: url = \"https://catfact.ninja/facts\" method = quickapi.ApiMethod.POST class RequestBody: required_input: str optional_input: str | None = None class ResponseBody: current_page: int data: list[Fact] Github: https://github.com/martinn/quickapiclient What do you guys think?"},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Every dunder method in Python", "Author": "u/treyhunner", "Content": "For years my training students have been asking me for a list of all the dunder methods. The Python docs don't have such a list, so I compiled my own... after having on my to-do list for years. Every dunder method in Python I realized why it took me so long during when I finally finished compiling the table of all of them... there are over 100 dunder methods in Python! üíØ Edit: I should have said \"the Python docs don't have such a list in a single row-by-row table\". The Data Model page does indeed include a giant \"Special Names\" section and a \"Coroutines\" section which document nearly every special method, but it's quite challenging to skim and not *quite* complete."},
{"Title": "Grocery delivery with Python (Company Engineering blog post)", "Author": "u/svena33", "Content": "Hope that sharing this is allowed here, let me know otherwise. I just published a new post on our engineering blog on why and how we‚Äôre using Python at Picnic (an online supermarket in the EU). I think it might be interesting for this sub as well. https://picnic.app/careers/python-picnic"},
{"Title": "EOmaps v8.0 is here!", "Author": "u/se_pp", "Content": "Hey all, I'm excited to announce that EOmaps v8.0 is now released and that it has been accepted into the PyOpenSci ecosystem ! What is it? EOmaps is a pyhton package to visualize and analyze geographical datasets. It's 100% free and open-source! You can find all information you need to install (or contribute üöÄ) in the documentation and on GitHub ! Check it out and let me know what you think! EOmaps v8.0 brings a lot of updates, fixes and improvements like: A switch to BSD-3-Clause License A much improved documentation (new API docs, contribution guide etc.) Better management of optional dependencies with pip install Pre-configured Jupyter Widgets for Jupyter Notebooks ... and much more!"},
{"Title": "Stockstir - Instantly and easily gather real time stock data from any Python script at no cost", "Author": "u/PatzEdi", "Content": "Hello! It has been a while since I have made a post about Stockstir, the tool to gather stock information from any script at no cost, written in Python. After a couple months from my last post, I have updated and refined bugs and other issues, as well as added new functions for enhanced usability. The latest version is now 2.1.4. What My Project Does Stockstir is an easy way to instantly gather stock data from any of your Python scripts. Not only that, but it includes other features, such as multi data gathering, anti ban, a fail-safe mechanism, random user agents, and much more. Target Audience Stockstir is for everyone that needs to gather realtime company stock info from any of their scripts. It mostly differs from any other stock related project in the way that it is simple, and doesn't rely on apis. Comparison Stockstir differs from other methods of gathering stock data in that it is has a very simple concept behind it. It is largely a GET wrapper in the Tools class, but initial API support such as Alpha Vantage, as well as gathering much more data of a Company stock through cnbc's JSON api, under the API class. It is mostly a quick way to gather stock data through simple use. You can find installation instructions and other information under the project link provided below: Link: Stockstir Project Link To those of you that are new to Stockstir, V2 added a plethora of new features including a fail-safe mechanism (more on that further down), and V2.1.0 reconstructed the entire codebase to further match PEP guidelines upon the request and suggestion from many of you. To see the latest Changelog information, visit the CHANGELOG.md file located in the project files hosted on Github. Here are a few examples of the different usages of Stockstir: Quick Usage: To easily gather a single price of a company's stock, you can do it in one line. from stockstir import Stockstir price = Stockstir().tools.get_single_price(\"ticker/stockSymbol\") print(price) The above Stockstir method get_single_price is one of the most basic of the functions provided. Stockstir Object Instantiation Since update V2, you can instantiate Stockstir as an object, and customize certain parameters: from stockstir import Stockstir s = Stockstir() # Instantiate the Stockstir object, like so. # We can also create a new Stockstir object, if for example you need certain options toggled: s2 = Stockstir(print_output=True, random_user_agent=True, provider='cnbc') Stockstir Functionality, the Fail-Safe mechanism, and Providers: I am not going to cover the entirety of Stockstir functionality here, which is why Stockstir has a readthedocs.io documentation: Stockstir Documentation However, basic Stockstir functionality can be described as a GET wrapper. It has providers , or, in other words, a website, and a regex pattern to find the price based the request made. Providers are a large part of Stockstir. The fail-safe mechanism chooses a new provider that works, in case it fails. A provider fails if the provider has changed their code base and the regex can't find anything, or, if the website the request is made to has changed or is down. To counter provider failure in case a prpovider fails, automated checks on my side run five times a day. If one of them doesn't work, I get notified within a short period of time. You can choose between 'cnbc', 'insiders', or 'zacks' for the providers. 'cnbc' is the default. To view working providers, you can do so like this: from stockstir import Stockstir s = Stockstir(provider='cnbc') #You can set the provider via the provider option in the Stockstir instantiation. Default will always be cnbc. s.providers.list_available_providers() # list the available providers. Many Thanks Thank you for trying out Stockstir, or even just looking into trying it!"},
{"Title": "Dask Demo Day: Dask on Databricks, scale embedding pipelines, and Prefect on the cloud", "Author": "u/dask-jeeves", "Content": "I wanted to share the talks from last month‚Äôs Dask Demo Day, where folks from the Dask community give short demos to show off ongoing work. Hopefully this helps elevate some of the great work people are doing. Last month‚Äôs talks: - One trillion row challenge - Deploy Dask on Databricks with dask-databricks - Deploy Prefect workflows on the cloud with Coiled - Scale embedding pipelines (LlamaIndex + Dask) - Use AWS Cost Explorer to see the cost of public IPv4 addresses Recording on YouTube: https://www.youtube.com/watch?v=07e1JL83ur8 Join the next one this Thursday, March 21st, 11am ET https://github.com/dask/community/issues/307"},
{"Title": "slack-progress-bar: A progress bar for your Slack Bots", "Author": "u/UnemployedPython", "Content": "I built this after having conceptually designed it in an interview but failed to get the job. Figured I might as well build it. Any thoughts or advice welcome! How My Project Works This progress bar connects with your Slack Bot account via tokens to send you a message as something is loading. It updates the progress bar as called by the user to edit the message on Slack, always keeping you up to date on how progress is going (ex. Used in an application that's loading something, running tests, etc.) Target Audience This can be used in a professional setting where you want to notify employees of certain progress. Also can be used for personal projects and / or design teams in universities that use Slack. Comparison slack-progress : This project exists but has not been supported for a long time, as it's an old version of Python and the slacker api has been deprecated. Github: https://github.com/mlizzi/slack-progress-bar pypi: https://pypi.org/project/slack-progress-bar/"},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "How To Add Multiple Pages to NiceGUI", "Author": "u/bitdoze", "Content": "Created an article and a video that will help beginners to NiceGUI add multiple pages to NiceGUI to better structure the app. https://www.bitdoze.com/nicegui-pages/"},
{"Title": "btstrm: Torrent Streaming Program for Linux", "Author": "u/hurleymjohnston", "Content": "I'll disclaim: I'm not a programmer but a litterateur, my code may be quite bad and in some ( some!) places written with LLMs - but it works as should be and its simplicity makes me admire Python. // The beginning of lyric digression For the last 7 years I've been writing scripts in bash. I've accumulated quite a lot of them, I would say that it became a kind of escapism for me, in anxious moments of my life, when I was too lazy to do something on my own, I prefer to spend half a night writing a script, so that I could press one button and - bang - everything would work by itself. At one point, Bash became not enough. Despite the fact that it became convenient and fast to write in it, in fact, it works slowly enough to create some serious projects. And at one point, my friend encouraged me to use Python, luckily he knows Python and I had an idea that came to life. So now I use my Python program literally every day. // End of lyric digression I watch movies and shows often (don't blame me for piracy, of course it's Big Buck Bunny every time) and since I don't like to click a lot of buttons, I wrote a 500 line script (hopefully someone will recognize themselves in this :D). https://github.com/asakura42/btstrm // https://pypi.org/project/btstrm/ What My Project Does This program can: Search for movies using TMDB in a given language and show the list of found movies along with posters (without API key!) Search torrents by name using Jackett (local torrent aggregator). Stream them with your favorite player (personally I use mpv). Stream any torrent or magnet that contain video files Screenshots: Searching movie: https://files.catbox.moe/3ine82.png List of torrents: https://files.catbox.moe/u3i9va.png Torrent selected, 10 seconds passed: https://files.catbox.moe/eyr3uz.png And finally: https://files.catbox.moe/7x3sss.png Target Audience Literally every movie/show enjoyer. Comparison You may ask - \"why? There are peerflix and webtorrent\". These programs may be enough for some people. But, first of all, there is no search for torrents in them, which is quite important and saves a lot of time. Secondly, btstrm uses btfs to work with bittorrent network. Unlike webtorrent ( javascript, meh ), btfs is written in C and uses libtorrent , and it \"mounts\" the torrent as a file system, which makes it easier to work with the files themselves if you need to, and it doesn't clog up your /tmp with multi-gigabyte files. Other programs it uses are chafa for showing movie posters, fzf for selecting a movie or torrent, and a few Python libraries. Also, if you are learning languages, and listen to condensed audio with impd , the program has a flag to easily add audio to your collection. At first time this program was just a product of simple merging of btfsplay script and some functions to search in Jackett. But now it is program that shows progress, uses many arguments and... just does its job."},
{"Title": "Type-safe & fast embedded vector database for RAG | OasysDB v0.3.0", "Author": "u/edwinkys", "Content": "Hey all! In this past few months, I've been working on OasysDB, an embedded vector database inspired by SQLite. OasysDB is written in Rust with interoperability to Python which offers high memory and type safety with high performance. What My Project Does OasysDB helps perform RAG with local AI models or any other small to medium size vector search operation . One of the perfect use case to use OasysDB is if you're building a LLM-based desktop app that doesn't require network connection. Why use OasysDB Requires almost zero setup with pip install oasysdb . Easy to use and straightforward API. Embedded database with optional persistence and high performance. Type and memory safe; less prone to unexpected errors. Here's what comes with v0.3.0 Storing vectors with a custom and flexible metadata in a Collection . You can modify, delete, or insert new vector records without the collection rebuilding the entire index. Optional persistence mode with Database class. By default, Collection runs in memory to make sure the high performance. But if you need to persist it for later use, simply call the Database.save_collection method. Target Audience Currently, OasysDB can be considered as in-beta which means there might be things that we change before we reach v1.0.0. Comparison OasysDB is fully-embedded inside the application instead of running separately. If most of the current vector database such as Qdrant is like Postgres, OasysDB is like SQLite. Benchmarking I'm still working on adding more benchmarks but we do have search performance benchmark which I will list below. The result below is taken using 16GB memory and Apple M2 CPU machine/my personal computer. Note that depending on different machine, the benchmark might differs. So take it with a grain of salt. Collection of 10,000 vectors with 128 dimensions: 0.15 ms Collection of 1,000,000 vectors with 128 dimensions: 1.5 ms Anyway, thank you so much for coming by. Please let me know if you decide to give it a go. I'll be happy to hear any feedback or question. I'll be happy to help too if you need any assistance. https://github.com/oasysai/oasysdb"},
{"Title": "The Biggest Hurdle in Learning Python", "Author": "u/shankarj68", "Content": "What is your biggest hurdle in learning the Python programming language? What specific area is hard for you to understand? Edit: Thank you to all the people who commented and discussed various challenges. Here are the obvious ones: Installation on various OS, along with which packages to use for installation (Pip, conda). Bootcamp tutorials seem to be boring and repetitive. There is hardly a resource available that mimics real-world scenarios. Type hinting can be challenging at first. Module and file structure - Navigate through the various sundirectory"},
{"Title": "Introducing ocrtoolkit: Your Go-To OCR Package", "Author": "u/ajkdrag_", "Content": "Hey Reddit community! I'm excited to present ocrtoolkit , a powerful OCR package designed to simplify your workflow and elevate your OCR tasks! What My Project Does If you've ever found yourself grappling with complex boilerplate code while tackling OCR-related challenges, you're in luck. ocrtoolkit streamlines the entire OCR process, offering intuitive wrappers for tasks such as image file handling, model execution, result parsing, and much more. Let's delve into the core features: Datasets Module : Need to effortlessly load image files or directories? Look no further than the ocrtoolkit.datasets module. Models Module : Seamlessly integrate with popular OCR frameworks like paddleOCR , ultralytics , and doctr through the ocrtoolkit.models module. Leverage sophisticated object detection models from ultralytics to pinpoint regions of interest before running OCR. Wrappers Module : Utilize wrappers for object detection, word detection, and recognition results with ease, courtesy of the ocrtoolkit.wrappers module. This standalone module ensures quick installation via pip install ocrtoolkit . Utilities Module : Access a plethora of utilities for tasks like word-to-line merging, geometry operations, file I/O, and beyond with the ocrtoolkit.utilities module. Target Audience Whether you're a researcher, developer, or data scientist embarking on OCR-related projects, ocrtoolkit caters to your needs. This package is your go-to solution for streamlining workflows, experimenting with different models and frameworks, and simplifying the inference process. Comparison Let's discuss how ocrtoolkit sets itself apart from existing alternatives: Comprehensive Support : Unlike packages solely focused on inference, ocrtoolkit offers comprehensive support for a myriad of OCR-related tasks, from parsing and processing ocr results, saving/loading and easy visualizations. Seamless Integration : Experience seamless integration with popular OCR and object detection frameworks, facilitating effortless experimentation within a unified environment. User-Friendly Design : Designed with ease of use in mind, ocrtoolkit ensures swift setup and configuration, enabling users to dive into OCR tasks without hassle. What ocrtoolkit is NOT for Training Models : ocrtoolkit is not designed for training new OCR models. Instead, its primary focus lies in utilizing pretrained or fine-tuned models for inference. High-Performance Applications : While ocrtoolkit boasts successful usage in production environments, it may not be the ideal choice for applications requiring maximum performance optimization. Additional Resources Explore the comprehensive documentation and discover more about ocrtoolkit on its PyPi page . Dive into the notebooks folder within the repository for insightful examples, and don't hesitate to share your feedback and suggestions! Thank you for your time, and I eagerly await your valuable insights! ^_^"},
{"Title": "What's the state of gradual typechecking in Python?", "Author": "u/messedupwindows123", "Content": "I've been using Ruby with Sorbet for a long time.  There are some pain points, and some errors that it misses, but in generally it's a really nice development tool.  I can immediately look up the types of different variables, or method signatures.  I also get (nearly) immediate feedback in Vim, if I write a function with an obvious type error.  Even though typing is \"gradual\", it catches a lot more problems than you would expect.  It also is pretty easy to migrate a file to be type-checked, as long as your code isn't too magical.  I like it because, tbh, if you can't encode your idea into the type system, you're probably doing something that another person will struggle to understand. Anyway I have been seeing that Python has type hints, and some libraries for runtime validation.  I was wondering if there are any static analysis tools that are widely used."},
{"Title": "Introducing, Taipy-Chess, A chess visualization tool, based on 20,000 games", "Author": "u/KorieDrakeChaney", "Content": "Taipy-Chess What my project does Taipy-Chess, is a chess visualization tool, based on 20,000 games.You can see all the games, the openings they played, opponents, top played openings and most successful openings. You can see heatmaps and charts on the data. This app was built using the taipy framework for easy data analysis and visualization. Target Audience The target audience is set for people in chess data visualization. I created this a part of a competition Comparison To compare to other data analysis chess tools, I would say, this is just a fun and easy to use chess visualization app. Compared to the projects posted on this competition, I would say, chess :). More I hope you guys enjoy. You should try creator quests . A fun way to challenge yourself. Star/Upvote if you like :). Demo https://github.com/KorieDrakeChaney/taipy-chess/assets/92071726/c25fb773-124c-4836-bf0a-8bd80b2d5d14 Links Github quine.sh"},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Introducing pixi's multiple environments implementation", "Author": "u/Accomplished-Treat85", "Content": "Link to Blog post: https://prefix.dev/blog/introducing_multi_env_pixi Pixi is a package manager that allows development using the conda & pypi ecosystem. Effortlessly switch between CUDA, CPU and other specialized software environments for maximum native performance and test against multiple versions of Pythonüêç."},
{"Title": "List of Python Libraries and Resources", "Author": "u/Basajaun-Eidean", "Content": "Hello r/python ! I've been compiling a list of Python libraries and resources that I've found useful across various projects. I thought it would be beneficial to share this collection with the community, so I've put together a repository for easy access and reference. You're welcome to check it out here: https://github.com/geru-scotland/pylib-atlas . If you find it useful, please consider starring it for future reference! Your feedback and contributions to enhance this collection are also highly appreciated. Thank you!"},
{"Title": "retry-later: Retry your functions in the background without stopping the flow of execution", "Author": "u/Free_Let_8315", "Content": "I was working on a project that needed to send an email for confirmation. I didn't want to wait for the function to complete and retry if it failed, since I could verify from the database later and retry again. To solve this issue, I created a decorator for my personal use but then I decided to create a small library out of it in case it can help others. The decorator @retry_later() when used with your function, will retry your function in the background without stopping the flow of execution. Only use it with functions for which you don't need an immediate return value. In case your function stores the state in the database or somewhere else, this is for you! The library is called retry-later . It's already on PyPI. If you want to see some features that are not implemented, please leave a comment - I will implement it! Since it's at a fairly early stage, the library is quite rudimentary, and I am willing to add some features as needed. Target Audience: Developers . If you have a long-running/infinite event loop and you have error-prone functions (like sending an email or calling a callback URL), and you want to retry the operations without interrupting your current flow of execution. This library supports both synchronous (without async keyword) and async functions Comparison : It's easy to use! Simply add @retry_later() to your function :) I looked at other libraries like retry but it retries synchronously, and your flow has to wait for it to complete. For some operations, I just don't wanna wait! To use, simply add the `@retry_later()` decorator to your function. @retry_later() async def send_email(email: str, body: str): await send_email_to_friend(email, body) This will retry your function asynchronously without stopping the flow of execution. Git Repo: https://github.com/krishnasism/retry-later/ Take a look inside the `examples` folder inside the repo for usage examples. Please leave a‚≠êon the repo if you think it can help you! And again, open to suggestions!"},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Prime Number Visualization", "Author": "u/Aaris_Kazi", "Content": "I tried to visualize the prime numbers in polar coordinates. This project was inspired by 3Blue1Brown channel to understand the relations. https://github.com/Aaris-Kazi/prime-visualization/blob/main/Snapshots/whole.gif To check more try to hit the link source code"},
{"Title": "Today I‚Äôm releasing PyBoy v2.0.0! A Game Boy emulator written in Python, focused on AI", "Author": "u/baekalfen", "Content": "Almost 5 years ago, I released PyBoy v1.0.0 to the public, and it's been incredible to see what people have used it for. Things have changed a lot since then, and you might even have seen the viral video by Peter Whidden, who created an AI to play Pok√©mon using PyBoy: https://www.youtube.com/watch?v=DcYLT37ImBY I've kept the API stable for all this time, and piled up improvements that I wanted to make. So the time has come for PyBoy v2.0.0 with a new and improved API. The hope is that the new API will be much more ergonomic to use, and still feel familiar to existing users. What is PyBoy In short, it‚Äôs a Game Boy emulator written from scratch in pure Python, with additional support for scripting through an API. PyBoy is as fast as comparable emulators written in C and C++ (or even better?). Benchmarked performance can get as high as 400 times faster than real-time. Meaning you can run 400 hours of game time in 1 hour. You can find the code and how to get started on the GitHub repo: https://github.com/Baekalfen/PyBoy The new API The changelog is long, but of the major features, I want to highlight: * Significant improvements to documentation * `pyboy.tick()` now supports frame-skipping which dramatically improves AI/RL performance * `pyboy.button('left')` a simplified way to send input, which automatically releases buttons * `pyboy.memory[bank, address]` an intuitive way to read/write memory of the emulator * `pyboy.hook_register(bank, address, callback, context)` register callbacks when the emulators hits certain parts of the ROM * `pyboy.memory_scanner` a tool to isolate addresses of interest Feedback, Contribute, Learn We‚Äôd love to hear your feedback, and see the projects you wish to use PyBoy for! We will do our best to make it happen. Please give any feedback in the comments below, on our Discord server or create issues/pull-requests on GitHub if you wish. And a special thanks to the people on Discord, who were a great motivational factor in all of this (in no specific order): krs013, thatguy, NicoleFaye, pdubs, jan0809, Lyfe, capnspacehook, kr1tzy, Nico, MLGxPwnentz, mr_seeker, Sky, Travis Scott Burger, Whippersnatch Pumpkinpatch, AutoMathis"},
{"Title": "Brain Tumor Classification using Deep learning [project]", "Author": "u/Feitgemel", "Content": "Welcome to Brain tumor beginner tutorial, where we delve into world of CNNs (Convolutional Neural Networks) and their groundbreaking applications in image classification and brain tumor detection. This is a simple tutorial convolutional neural network tutorial that demonstrates how to brain tumor in a dataset of images. We will build and train a model using CNN and see the model accuracy & loss, and then we will test and predict a tumor using new images. Here is a link to the Github Repo: https://youtu.be/-147KGbGI3g Enjoy Eran #cnnforimageclassification #cnnmachinelearningmodel #cnnml #deeplearningbraintumorclassification #aidetectbraintumor"},
{"Title": "Match statement: wish list", "Author": "u/Kiuhnm", "Content": "Without a doubt, the match statement is very powerful, but there are a few annoyances or missing features, IMO. Type Aliases Type aliases are not supported, so this won't work: NewType1: TypeAlias = ... type NewType2 = ... match x: case NewType1(): ... case NewType2(): ... Fallthrough The following is not possible: match x: case A() if cond1(x): fallthrough case B() if cond2(x): code1 The alternatives are match x: case A() if cond1(x): code1 case B() if cond2(x): code1 which usually forces one to define a function for code1, or match x: case _ if ((isinstance(x, A) and cond1(x)) or (isinstance(x, B) and cond2(x))): code1 which is much more verbose. Chained Member Accesses I'd like to be able to write match x: case A(b.c.d.x = 4): ... instead of match x: case A(b = object(c = object(d = object(x = 4)))): ... Double Indentation The match statement wastes horizontal space. Why not just match x: case ...: ... case ...: ... I've been doing a lot of parsing lately and I find myself going for the if isinstance pattern in place of a second, nested match. Most of the time, I like to have two code views one next to the other, so all my code has a maximum row length of 81 (79 + 2 extra chars for some leeway)."},
{"Title": "flect - Pure Python full-stack framework", "Author": "u/UnhappyActivity6133", "Content": "What is flect? flect is a Python framework designed for building full-stack web applications. By leveraging Pydantic models in the backend to correspond with React components in the frontend, flect enables developers to quickly craft interactive and visually appealing user interfaces using Python. Key Features Fast Development : Empowers writing your entire application in Python, offering a seamless integration between backend logic and frontend UI. Easy Form Validation : Utilizes a single Pydantic model for consistent form validation throughout your application, speeding up development and minimizing errors. Folder-Based Routing : Simplifies route management with an intuitive folder structure. Client-Side Routing : Ensures smooth and rapid page transitions without reloads. SEO Friendly : Supports server-side rendering to enhance search engine visibility. Custom Components : Allows the use of custom-built React components within flect. View the documentation website , built entirely with flect and deployed on Vercel, for more insights. Source code is available here . What My Project Does flect bridges the gap between Python backend development and React frontend design, enabling the creation of full-stack web applications without the need for JavaScript. This framework is perfect for rapidly developing feature-rich applications, offering out-of-the-box support for form validation, SEO, and custom components. Target Audience Python Developers : For those looking to build responsive web applications leveraging their Python skills, without the necessity to write JavaScript or interact with npm. Frontend Developers : For developers focused on crafting unique, reusable components without duplicating effort across views. General Audience : Anyone interested in a framework that offers a true separation of concerns, with backend logic defining the application and the frontend focusing solely on UI implementation. Comparison flect draws inspiration from FastUI but differentiates itself in several key aspects: Frontend Foundation: flect leverages react-router, ShadcnUI, and TailwindCSS, creating a more dynamic and customizable user interface than FastUI. Routing Capabilities: It introduces Folder-Based Routing and Client-Side Routing, making navigation management both intuitive and efficient. SEO Optimization: With support for server-side rendering, flect enhances the search engine visibility of web applications, an area where traditional Python frameworks might lag. This streamlined approach, combining modern frontend technologies with Python's backend, sets flect apart as a more versatile and user-friendly framework for full-stack development. Learn More Documentation Thanks for Reading"},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "JSON Serialization", "Author": "u/Realistic-Sea-666", "Content": "I have been using more and more of Python over the past few weeks because I got sick of JS. When I asked a programmer friend of mine who uses JS religiously what he thought about using Python for web backends, he told me that it would be slow because of JSON serialization. I spent some time researching this, but couldn‚Äôt find anything decisive that explained why JSON serialization would be faster in JS. The answers I found said a few things: V8 is optimized for this because it‚Äôs part of the JS standard it‚Äôs slow in Python because of an implementation of how the JSON to Python representation (JSON dump) function is represented in Python. Does anyone else have any further insight on this topic? I am trying to understand this at a fundamental level and cut through any noise / wrong understanding. Thanks so much!"},
{"Title": "Is backend development the most common career path", "Author": "u/Grandtosh", "Content": "Maybe i worded my title wrong. Taking a look at job listings involving python, backend development seems to be the only listing I come across. Why is that the case ? Are they no other career paths outside backend dev for python developers ?"},
{"Title": "Ralf Gommers, leading maintainer of NumPy, on the release of NumPy 2.0", "Author": "u/rubiesordiamonds", "Content": "We spoke with Ralf about the upcoming 2.0 release, the first major release in 16 years. Ralf has a great story as to how he got into the open source community in general, and NumPy and SciPy in particular. You can check it out here: https://open.substack.com/pub/onceamaintainer/p/once-a-maintainer-ralf-gommers?r=2773u5&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "FlowQ: Your Friendly Neighbourhood Distributed Computing System!", "Author": "u/StoneSteel_1", "Content": "FlowQ is a Distributed Computing system API, Which aims to make Distributed Computing Free and Simple to use! Features of FlowQ: Effortless Setup : Ditch the complicated configurations! FlowQ runs right out of the box, no ssh headaches or pre-installation required. Simple and Secure Connection : Leverages the Hack.Chat  and FileBin platforms to establish secure, base-64 encrypted and anonymous connections with your computing cluster. Parallel task execution : FlowQ maximizes your network by executing tasks in parallel across your machines(with multi-threading), significantly boosting your processing speed. You can add computers to your cluster just running by: pip install FlowQ python -m FlowQ.cluster -c <your-channel-name> Do not worry, if you dont have any extra computers!, you can just run the above code in a Google Colab Instances, to add computers in your cluster! What my project does: Helps to complete tasks which are time and memory intensive, faster by splitting the work between computers. It can speedup your program task by X times( X is number of computers in cluster) Target Audience: Anyone, who wants to speedup thier program tasks. Eg: Data Extraction, Data Transformation, or any kind of repetative task :) Github: FlowQ GitHub Repo (Has more informations!) PyPi page: https://pypi.org/project/FlowQ/ Some Points I wanted to say: The project is in very early development stage . So, if you get any issues , Please report it. It would be a great help! Your Contributions are Happily Welcomed! I would like to see new ideas from people! Leave feedbacks on anything you would wanna see it Fixed or Improved! I would love to read your opinions! Thanks for Reading :)"},
{"Title": "PyYAML documentation and source code offline?", "Author": "u/Veyron180", "Content": "Hey everyone, Today, I wanted to search for some documentation about PyYAML for a personal project. However, I saw that their documentation and GitHub pages all give 404's. The package is still in PyPi. Does anyone know why? Did they go private? https://pyyaml.org https://github.com/yaml/pyyaml https://pypi.org/project/PyYAML/"},
{"Title": "FuzzTypes: Pydantic Library for Auto-correcting Annotation Types", "Author": "u/imaurer", "Content": "Repo: https://github.com/genomoncology/FuzzTypes FuzzTypes is a library for defining custom annotation types that auto-correct data. This can be used to clean the structured data generated or extracted using OpenAI Function Calling. Since it is built on Pydantic it will work with FastAPI Custom GPT Actions, Instructor, Marvin, DSPy Typed Predictors and any other library currently using Pydantic to define and validate function calling tools. FuzzTypes comes with a variety of \"usable types\" that can be used immediately such as Fuzzy Dates/Times, Email, Zipcode, Integer Words, Emojis. It's also easy to create your own custom annotation types. Simply: - Provide a function that converts an \"bad\" input value to a \"good\" output value. - Provide a regex for matching desired patterns (e.g. zipcode, email) - Provide a list of \"named entities\" (name + aliases) that can be matched to either exactly, case insensitively, fuzzily or semantically (e.g. country, currency) For more information, check out the README: https://github.com/genomoncology/FuzzTypes"},
{"Title": "SCHNOZ: Advanced packet sniffer/analyzer and IDS written in Python", "Author": "u/OkDevelopment4423", "Content": "What it Does: Schnoz is an advanced packet sniffer/analyzer and IDS with several options to conduct network monitoring and threat analysis. My script supports both Windows and Linux and is written in Python. Features: Actively sniffs local traffic on specified interfaces Passively sniffs traffic from pcap files Can either actively sniff an interface or file for suspicious activity, alerting on potential malicious traffic. Malicious terms can come from a user-inputted wordlist or use the Schnozlist , which are terms that I've encountered through CTFs Analyzes HTTP requests and responses on either an active interface or file Windows and Linux support Very, very user friendly Customizable results with the use of arguments Target Audience : Anyone working in a blue team environment and anyone working on a network, defensive CTF. This tool may also be useful for someone trying to get a better understanding of network traffic Comparisons: My script is comparable to Wireshark and tcpdump. These tools, though, can be harder to use for someone who's just getting into packet analysis. I've included the option of using arguments for more advanced analysis. I just published the code today, but I'm going to try and improve it consistently for the community. I'm very open to feedback about its functionality and source code. Thanks! Repo: https://github.com/abelreqma/schnoz Source code: https://github.com/abelreqma/schnoz/blob/main/schnoz.py"},
{"Title": "Python devs, whats the best complimentary language for your area and why?", "Author": "u/Brilliant-Donkey-320", "Content": "Hey Everybody, I have seen Python used for many things and I am just wondering, for those who work with Python and another language, what is the best complimentary language for your area (or just in general in your opinion) and why? Is the language used to make faster libraries (like making a C/C++ library for a CPU intensive task)? Maybe you use a higher level language like C# or Java for an application and Python for some DS, AI/ML section? I am curious which languages work well with Python and why? Thanks! Edit: Thanks everyone for all of this info about languages that are useful with Python. It has been very informative and I will definitely be checking out some of these suggested companion languages. Thanks!"},
{"Title": "Type-safe & fast embedded vector database for RAG | OasysDB v0.3.0", "Author": "u/edwinkys", "Content": "Hey all! In this past few months, I've been working on OasysDB, an embedded vector database inspired by SQLite. OasysDB is written in Rust with interoperability to Python which offers high memory and type safety with high performance. What My Project Does OasysDB helps perform RAG with local AI models or any other small to medium size vector search operation . One of the perfect use case to use OasysDB is if you're building a LLM-based desktop app that doesn't require network connection. Why use OasysDB Requires almost zero setup with pip install oasysdb . Easy to use and straightforward API. Embedded database with optional persistence and high performance. Type and memory safe; less prone to unexpected errors. Here's what comes with v0.3.0 Storing vectors with a custom and flexible metadata in a Collection . You can modify, delete, or insert new vector records without the collection rebuilding the entire index. Optional persistence mode with Database class. By default, Collection runs in memory to make sure the high performance. But if you need to persist it for later use, simply call the Database.save_collection method. Target Audience Currently, OasysDB can be considered as in-beta which means there might be things that we change before we reach v1.0.0. Comparison OasysDB is fully-embedded inside the application instead of running separately. If most of the current vector database such as Qdrant is like Postgres, OasysDB is like SQLite. Benchmarking I'm still working on adding more benchmarks but we do have search performance benchmark which I will list below. The result below is taken using 16GB memory and Apple M2 CPU machine/my personal computer. Note that depending on different machine, the benchmark might differs. So take it with a grain of salt. Collection of 10,000 vectors with 128 dimensions: 0.15 ms Collection of 1,000,000 vectors with 128 dimensions: 1.5 ms Anyway, thank you so much for coming by. Please let me know if you decide to give it a go. I'll be happy to hear any feedback or question. I'll be happy to help too if you need any assistance. https://github.com/oasysai/oasysdb"},
{"Title": "The Biggest Hurdle in Learning Python", "Author": "u/shankarj68", "Content": "What is your biggest hurdle in learning the Python programming language? What specific area is hard for you to understand? Edit: Thank you to all the people who commented and discussed various challenges. Here are the obvious ones: Installation on various OS, along with which packages to use for installation (Pip, conda). Bootcamp tutorials seem to be boring and repetitive. There is hardly a resource available that mimics real-world scenarios. Type hinting can be challenging at first. Module and file structure - Navigate through the various sundirectory"},
{"Title": "Introducing ocrtoolkit: Your Go-To OCR Package", "Author": "u/ajkdrag_", "Content": "Hey Reddit community! I'm excited to present ocrtoolkit , a powerful OCR package designed to simplify your workflow and elevate your OCR tasks! What My Project Does If you've ever found yourself grappling with complex boilerplate code while tackling OCR-related challenges, you're in luck. ocrtoolkit streamlines the entire OCR process, offering intuitive wrappers for tasks such as image file handling, model execution, result parsing, and much more. Let's delve into the core features: Datasets Module : Need to effortlessly load image files or directories? Look no further than the ocrtoolkit.datasets module. Models Module : Seamlessly integrate with popular OCR frameworks like paddleOCR , ultralytics , and doctr through the ocrtoolkit.models module. Leverage sophisticated object detection models from ultralytics to pinpoint regions of interest before running OCR. Wrappers Module : Utilize wrappers for object detection, word detection, and recognition results with ease, courtesy of the ocrtoolkit.wrappers module. This standalone module ensures quick installation via pip install ocrtoolkit . Utilities Module : Access a plethora of utilities for tasks like word-to-line merging, geometry operations, file I/O, and beyond with the ocrtoolkit.utilities module. Target Audience Whether you're a researcher, developer, or data scientist embarking on OCR-related projects, ocrtoolkit caters to your needs. This package is your go-to solution for streamlining workflows, experimenting with different models and frameworks, and simplifying the inference process. Comparison Let's discuss how ocrtoolkit sets itself apart from existing alternatives: Comprehensive Support : Unlike packages solely focused on inference, ocrtoolkit offers comprehensive support for a myriad of OCR-related tasks, from parsing and processing ocr results, saving/loading and easy visualizations. Seamless Integration : Experience seamless integration with popular OCR and object detection frameworks, facilitating effortless experimentation within a unified environment. User-Friendly Design : Designed with ease of use in mind, ocrtoolkit ensures swift setup and configuration, enabling users to dive into OCR tasks without hassle. What ocrtoolkit is NOT for Training Models : ocrtoolkit is not designed for training new OCR models. Instead, its primary focus lies in utilizing pretrained or fine-tuned models for inference. High-Performance Applications : While ocrtoolkit boasts successful usage in production environments, it may not be the ideal choice for applications requiring maximum performance optimization. Additional Resources Explore the comprehensive documentation and discover more about ocrtoolkit on its PyPi page . Dive into the notebooks folder within the repository for insightful examples, and don't hesitate to share your feedback and suggestions! Thank you for your time, and I eagerly await your valuable insights! ^_^"},
{"Title": "What's the state of gradual typechecking in Python?", "Author": "u/messedupwindows123", "Content": "I've been using Ruby with Sorbet for a long time.  There are some pain points, and some errors that it misses, but in generally it's a really nice development tool.  I can immediately look up the types of different variables, or method signatures.  I also get (nearly) immediate feedback in Vim, if I write a function with an obvious type error.  Even though typing is \"gradual\", it catches a lot more problems than you would expect.  It also is pretty easy to migrate a file to be type-checked, as long as your code isn't too magical.  I like it because, tbh, if you can't encode your idea into the type system, you're probably doing something that another person will struggle to understand. Anyway I have been seeing that Python has type hints, and some libraries for runtime validation.  I was wondering if there are any static analysis tools that are widely used."},
{"Title": "Introducing, Taipy-Chess, A chess visualization tool, based on 20,000 games", "Author": "u/KorieDrakeChaney", "Content": "Taipy-Chess What my project does Taipy-Chess, is a chess visualization tool, based on 20,000 games.You can see all the games, the openings they played, opponents, top played openings and most successful openings. You can see heatmaps and charts on the data. This app was built using the taipy framework for easy data analysis and visualization. Target Audience The target audience is set for people in chess data visualization. I created this a part of a competition Comparison To compare to other data analysis chess tools, I would say, this is just a fun and easy to use chess visualization app. Compared to the projects posted on this competition, I would say, chess :). More I hope you guys enjoy. You should try creator quests . A fun way to challenge yourself. Star/Upvote if you like :). Demo https://github.com/KorieDrakeChaney/taipy-chess/assets/92071726/c25fb773-124c-4836-bf0a-8bd80b2d5d14 Links Github quine.sh"},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Introducing pixi's multiple environments implementation", "Author": "u/Accomplished-Treat85", "Content": "Link to Blog post: https://prefix.dev/blog/introducing_multi_env_pixi Pixi is a package manager that allows development using the conda & pypi ecosystem. Effortlessly switch between CUDA, CPU and other specialized software environments for maximum native performance and test against multiple versions of Pythonüêç."},
{"Title": "List of Python Libraries and Resources", "Author": "u/Basajaun-Eidean", "Content": "Hello r/python ! I've been compiling a list of Python libraries and resources that I've found useful across various projects. I thought it would be beneficial to share this collection with the community, so I've put together a repository for easy access and reference. You're welcome to check it out here: https://github.com/geru-scotland/pylib-atlas . If you find it useful, please consider starring it for future reference! Your feedback and contributions to enhance this collection are also highly appreciated. Thank you!"},
{"Title": "retry-later: Retry your functions in the background without stopping the flow of execution", "Author": "u/Free_Let_8315", "Content": "I was working on a project that needed to send an email for confirmation. I didn't want to wait for the function to complete and retry if it failed, since I could verify from the database later and retry again. To solve this issue, I created a decorator for my personal use but then I decided to create a small library out of it in case it can help others. The decorator @retry_later() when used with your function, will retry your function in the background without stopping the flow of execution. Only use it with functions for which you don't need an immediate return value. In case your function stores the state in the database or somewhere else, this is for you! The library is called retry-later . It's already on PyPI. If you want to see some features that are not implemented, please leave a comment - I will implement it! Since it's at a fairly early stage, the library is quite rudimentary, and I am willing to add some features as needed. Target Audience: Developers . If you have a long-running/infinite event loop and you have error-prone functions (like sending an email or calling a callback URL), and you want to retry the operations without interrupting your current flow of execution. This library supports both synchronous (without async keyword) and async functions Comparison : It's easy to use! Simply add @retry_later() to your function :) I looked at other libraries like retry but it retries synchronously, and your flow has to wait for it to complete. For some operations, I just don't wanna wait! To use, simply add the `@retry_later()` decorator to your function. @retry_later() async def send_email(email: str, body: str): await send_email_to_friend(email, body) This will retry your function asynchronously without stopping the flow of execution. Git Repo: https://github.com/krishnasism/retry-later/ Take a look inside the `examples` folder inside the repo for usage examples. Please leave a‚≠êon the repo if you think it can help you! And again, open to suggestions!"},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Prime Number Visualization", "Author": "u/Aaris_Kazi", "Content": "I tried to visualize the prime numbers in polar coordinates. This project was inspired by 3Blue1Brown channel to understand the relations. https://github.com/Aaris-Kazi/prime-visualization/blob/main/Snapshots/whole.gif To check more try to hit the link source code"},
{"Title": "Today I‚Äôm releasing PyBoy v2.0.0! A Game Boy emulator written in Python, focused on AI", "Author": "u/baekalfen", "Content": "Almost 5 years ago, I released PyBoy v1.0.0 to the public, and it's been incredible to see what people have used it for. Things have changed a lot since then, and you might even have seen the viral video by Peter Whidden, who created an AI to play Pok√©mon using PyBoy: https://www.youtube.com/watch?v=DcYLT37ImBY I've kept the API stable for all this time, and piled up improvements that I wanted to make. So the time has come for PyBoy v2.0.0 with a new and improved API. The hope is that the new API will be much more ergonomic to use, and still feel familiar to existing users. What is PyBoy In short, it‚Äôs a Game Boy emulator written from scratch in pure Python, with additional support for scripting through an API. PyBoy is as fast as comparable emulators written in C and C++ (or even better?). Benchmarked performance can get as high as 400 times faster than real-time. Meaning you can run 400 hours of game time in 1 hour. You can find the code and how to get started on the GitHub repo: https://github.com/Baekalfen/PyBoy The new API The changelog is long, but of the major features, I want to highlight: * Significant improvements to documentation * `pyboy.tick()` now supports frame-skipping which dramatically improves AI/RL performance * `pyboy.button('left')` a simplified way to send input, which automatically releases buttons * `pyboy.memory[bank, address]` an intuitive way to read/write memory of the emulator * `pyboy.hook_register(bank, address, callback, context)` register callbacks when the emulators hits certain parts of the ROM * `pyboy.memory_scanner` a tool to isolate addresses of interest Feedback, Contribute, Learn We‚Äôd love to hear your feedback, and see the projects you wish to use PyBoy for! We will do our best to make it happen. Please give any feedback in the comments below, on our Discord server or create issues/pull-requests on GitHub if you wish. And a special thanks to the people on Discord, who were a great motivational factor in all of this (in no specific order): krs013, thatguy, NicoleFaye, pdubs, jan0809, Lyfe, capnspacehook, kr1tzy, Nico, MLGxPwnentz, mr_seeker, Sky, Travis Scott Burger, Whippersnatch Pumpkinpatch, AutoMathis"},
{"Title": "Brain Tumor Classification using Deep learning [project]", "Author": "u/Feitgemel", "Content": "Welcome to Brain tumor beginner tutorial, where we delve into world of CNNs (Convolutional Neural Networks) and their groundbreaking applications in image classification and brain tumor detection. This is a simple tutorial convolutional neural network tutorial that demonstrates how to brain tumor in a dataset of images. We will build and train a model using CNN and see the model accuracy & loss, and then we will test and predict a tumor using new images. Here is a link to the Github Repo: https://youtu.be/-147KGbGI3g Enjoy Eran #cnnforimageclassification #cnnmachinelearningmodel #cnnml #deeplearningbraintumorclassification #aidetectbraintumor"},
{"Title": "Match statement: wish list", "Author": "u/Kiuhnm", "Content": "Without a doubt, the match statement is very powerful, but there are a few annoyances or missing features, IMO. Type Aliases Type aliases are not supported, so this won't work: NewType1: TypeAlias = ... type NewType2 = ... match x: case NewType1(): ... case NewType2(): ... Fallthrough The following is not possible: match x: case A() if cond1(x): fallthrough case B() if cond2(x): code1 The alternatives are match x: case A() if cond1(x): code1 case B() if cond2(x): code1 which usually forces one to define a function for code1, or match x: case _ if ((isinstance(x, A) and cond1(x)) or (isinstance(x, B) and cond2(x))): code1 which is much more verbose. Chained Member Accesses I'd like to be able to write match x: case A(b.c.d.x = 4): ... instead of match x: case A(b = object(c = object(d = object(x = 4)))): ... Double Indentation The match statement wastes horizontal space. Why not just match x: case ...: ... case ...: ... I've been doing a lot of parsing lately and I find myself going for the if isinstance pattern in place of a second, nested match. Most of the time, I like to have two code views one next to the other, so all my code has a maximum row length of 81 (79 + 2 extra chars for some leeway)."},
{"Title": "flect - Pure Python full-stack framework", "Author": "u/UnhappyActivity6133", "Content": "What is flect? flect is a Python framework designed for building full-stack web applications. By leveraging Pydantic models in the backend to correspond with React components in the frontend, flect enables developers to quickly craft interactive and visually appealing user interfaces using Python. Key Features Fast Development : Empowers writing your entire application in Python, offering a seamless integration between backend logic and frontend UI. Easy Form Validation : Utilizes a single Pydantic model for consistent form validation throughout your application, speeding up development and minimizing errors. Folder-Based Routing : Simplifies route management with an intuitive folder structure. Client-Side Routing : Ensures smooth and rapid page transitions without reloads. SEO Friendly : Supports server-side rendering to enhance search engine visibility. Custom Components : Allows the use of custom-built React components within flect. View the documentation website , built entirely with flect and deployed on Vercel, for more insights. Source code is available here . What My Project Does flect bridges the gap between Python backend development and React frontend design, enabling the creation of full-stack web applications without the need for JavaScript. This framework is perfect for rapidly developing feature-rich applications, offering out-of-the-box support for form validation, SEO, and custom components. Target Audience Python Developers : For those looking to build responsive web applications leveraging their Python skills, without the necessity to write JavaScript or interact with npm. Frontend Developers : For developers focused on crafting unique, reusable components without duplicating effort across views. General Audience : Anyone interested in a framework that offers a true separation of concerns, with backend logic defining the application and the frontend focusing solely on UI implementation. Comparison flect draws inspiration from FastUI but differentiates itself in several key aspects: Frontend Foundation: flect leverages react-router, ShadcnUI, and TailwindCSS, creating a more dynamic and customizable user interface than FastUI. Routing Capabilities: It introduces Folder-Based Routing and Client-Side Routing, making navigation management both intuitive and efficient. SEO Optimization: With support for server-side rendering, flect enhances the search engine visibility of web applications, an area where traditional Python frameworks might lag. This streamlined approach, combining modern frontend technologies with Python's backend, sets flect apart as a more versatile and user-friendly framework for full-stack development. Learn More Documentation Thanks for Reading"},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "JSON Serialization", "Author": "u/Realistic-Sea-666", "Content": "I have been using more and more of Python over the past few weeks because I got sick of JS. When I asked a programmer friend of mine who uses JS religiously what he thought about using Python for web backends, he told me that it would be slow because of JSON serialization. I spent some time researching this, but couldn‚Äôt find anything decisive that explained why JSON serialization would be faster in JS. The answers I found said a few things: V8 is optimized for this because it‚Äôs part of the JS standard it‚Äôs slow in Python because of an implementation of how the JSON to Python representation (JSON dump) function is represented in Python. Does anyone else have any further insight on this topic? I am trying to understand this at a fundamental level and cut through any noise / wrong understanding. Thanks so much!"},
{"Title": "Is backend development the most common career path", "Author": "u/Grandtosh", "Content": "Maybe i worded my title wrong. Taking a look at job listings involving python, backend development seems to be the only listing I come across. Why is that the case ? Are they no other career paths outside backend dev for python developers ?"},
{"Title": "Ralf Gommers, leading maintainer of NumPy, on the release of NumPy 2.0", "Author": "u/rubiesordiamonds", "Content": "We spoke with Ralf about the upcoming 2.0 release, the first major release in 16 years. Ralf has a great story as to how he got into the open source community in general, and NumPy and SciPy in particular. You can check it out here: https://open.substack.com/pub/onceamaintainer/p/once-a-maintainer-ralf-gommers?r=2773u5&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "FlowQ: Your Friendly Neighbourhood Distributed Computing System!", "Author": "u/StoneSteel_1", "Content": "FlowQ is a Distributed Computing system API, Which aims to make Distributed Computing Free and Simple to use! Features of FlowQ: Effortless Setup : Ditch the complicated configurations! FlowQ runs right out of the box, no ssh headaches or pre-installation required. Simple and Secure Connection : Leverages the Hack.Chat  and FileBin platforms to establish secure, base-64 encrypted and anonymous connections with your computing cluster. Parallel task execution : FlowQ maximizes your network by executing tasks in parallel across your machines(with multi-threading), significantly boosting your processing speed. You can add computers to your cluster just running by: pip install FlowQ python -m FlowQ.cluster -c <your-channel-name> Do not worry, if you dont have any extra computers!, you can just run the above code in a Google Colab Instances, to add computers in your cluster! What my project does: Helps to complete tasks which are time and memory intensive, faster by splitting the work between computers. It can speedup your program task by X times( X is number of computers in cluster) Target Audience: Anyone, who wants to speedup thier program tasks. Eg: Data Extraction, Data Transformation, or any kind of repetative task :) Github: FlowQ GitHub Repo (Has more informations!) PyPi page: https://pypi.org/project/FlowQ/ Some Points I wanted to say: The project is in very early development stage . So, if you get any issues , Please report it. It would be a great help! Your Contributions are Happily Welcomed! I would like to see new ideas from people! Leave feedbacks on anything you would wanna see it Fixed or Improved! I would love to read your opinions! Thanks for Reading :)"},
{"Title": "PyYAML documentation and source code offline?", "Author": "u/Veyron180", "Content": "Hey everyone, Today, I wanted to search for some documentation about PyYAML for a personal project. However, I saw that their documentation and GitHub pages all give 404's. The package is still in PyPi. Does anyone know why? Did they go private? https://pyyaml.org https://github.com/yaml/pyyaml https://pypi.org/project/PyYAML/"},
{"Title": "FuzzTypes: Pydantic Library for Auto-correcting Annotation Types", "Author": "u/imaurer", "Content": "Repo: https://github.com/genomoncology/FuzzTypes FuzzTypes is a library for defining custom annotation types that auto-correct data. This can be used to clean the structured data generated or extracted using OpenAI Function Calling. Since it is built on Pydantic it will work with FastAPI Custom GPT Actions, Instructor, Marvin, DSPy Typed Predictors and any other library currently using Pydantic to define and validate function calling tools. FuzzTypes comes with a variety of \"usable types\" that can be used immediately such as Fuzzy Dates/Times, Email, Zipcode, Integer Words, Emojis. It's also easy to create your own custom annotation types. Simply: - Provide a function that converts an \"bad\" input value to a \"good\" output value. - Provide a regex for matching desired patterns (e.g. zipcode, email) - Provide a list of \"named entities\" (name + aliases) that can be matched to either exactly, case insensitively, fuzzily or semantically (e.g. country, currency) For more information, check out the README: https://github.com/genomoncology/FuzzTypes"},
{"Title": "SCHNOZ: Advanced packet sniffer/analyzer and IDS written in Python", "Author": "u/OkDevelopment4423", "Content": "What it Does: Schnoz is an advanced packet sniffer/analyzer and IDS with several options to conduct network monitoring and threat analysis. My script supports both Windows and Linux and is written in Python. Features: Actively sniffs local traffic on specified interfaces Passively sniffs traffic from pcap files Can either actively sniff an interface or file for suspicious activity, alerting on potential malicious traffic. Malicious terms can come from a user-inputted wordlist or use the Schnozlist , which are terms that I've encountered through CTFs Analyzes HTTP requests and responses on either an active interface or file Windows and Linux support Very, very user friendly Customizable results with the use of arguments Target Audience : Anyone working in a blue team environment and anyone working on a network, defensive CTF. This tool may also be useful for someone trying to get a better understanding of network traffic Comparisons: My script is comparable to Wireshark and tcpdump. These tools, though, can be harder to use for someone who's just getting into packet analysis. I've included the option of using arguments for more advanced analysis. I just published the code today, but I'm going to try and improve it consistently for the community. I'm very open to feedback about its functionality and source code. Thanks! Repo: https://github.com/abelreqma/schnoz Source code: https://github.com/abelreqma/schnoz/blob/main/schnoz.py"},
{"Title": "Python devs, whats the best complimentary language for your area and why?", "Author": "u/Brilliant-Donkey-320", "Content": "Hey Everybody, I have seen Python used for many things and I am just wondering, for those who work with Python and another language, what is the best complimentary language for your area (or just in general in your opinion) and why? Is the language used to make faster libraries (like making a C/C++ library for a CPU intensive task)? Maybe you use a higher level language like C# or Java for an application and Python for some DS, AI/ML section? I am curious which languages work well with Python and why? Thanks! Edit: Thanks everyone for all of this info about languages that are useful with Python. It has been very informative and I will definitely be checking out some of these suggested companion languages. Thanks!"},
{"Title": "deptry 0.14.0: Now up to 10 times faster by leveraging Rust!", "Author": "u/fpgmaas", "Content": "deptry 0.14.0 was just released, bringing significant speed improvements: It is now up to 10 times faster than the previous release! üöÄ For those unfamiliar with deptry; deptry is a command line tool to check for issues with dependencies in a Python project, such as unused or missing dependencies. [ GitHub ] For some benchmarks of the new release, see the release notes here . The performance improvement was achieved by leveraging Rust to parse the AST and extract the import statements from .py files, rather than using Python's ast module. The addition of Rust to the project also opens up doors for more optimizations in the future, so stay tuned!"},
{"Title": "DSPy: Programming‚Äînot prompting‚ÄîFoundation Models", "Author": "u/EnoughProject7477", "Content": "DSPy is a framework that aims to solve the fragility problem in language model (LM)-based applications by prioritizing programming over prompting. It allows you to recompile the entire pipeline to optimize it to your specific task ‚Äî instead of repeating manual rounds of prompt engineering ‚Äî whenever you change a component. https://github.com/stanfordnlp/dspy"},
{"Title": "I forked pyppeteer and revamped it into a new browser automation package: mokr.", "Author": "u/lamerlink", "Content": "Hello! Been working on this for awhile and excited to finally drop it. I forked Pyppeteer (no longer maintained) and revamped it, doing a good amount of refactoring and adding features along the way. Project link: https://github.com/michaeleveringham/mokr Documentation link: https://mokr.readthedocs.io/en/latest/index.html What My Project Does Offers automated web browsing for both Chrome and Firefox. Target Audience Currently enthusiasts seeking to automate tests or web scrape. Eventually I‚Äôd like to offer this as production-ready but my test suite isn‚Äôt finished yet. Comparisons I won‚Äôt regurgitate the entire readme but some notable changes from Pyppeteer or Playwright include an overhauled network manager, Firefox (partial) support, an httpx session that shares browser state, and proxy support, including SOCKS proxies. There are loads of other small changes too, like making names more ‚Äúpythonic‚Äù and restructuring the project. Feel free to give feedback and use it! I noted in the documentation some potential advantages over other solutions like Playwright (though I‚Äôm not deluded, Playwright is far superior still)."},
{"Title": "API Health Checks", "Author": "u/BigHeed87", "Content": "Healthy-API What it does Provide an easy interface for defining health checks for web applications written in Flask and FastAPI Target Audience For Python developers who are using Flask and FastAPI Similar Packages https://pypi.org/project/fastapi-healthchecks/ - This looks more Object/class based, but appears to only handles FastAPI. Also allows you to put up a maintenance page https://pypi.org/project/Flask-Meter/ - My older package which only handles Flask Background When I was a lesser experienced developer, I ended up writing my own solutions for health checks as a  tour of the language ecosystem. It also gave me insight on how to write a package which could be shared & maintained. I recently updated an older package of mine which only worked for Flask apps. These days I've been writing more FastAPI. Enjoy!"},
{"Title": "Declarative GUI for Python", "Author": "u/madnirua", "Content": "Slint is a declarative GUI toolkit to build native user interfaces for desktop and embedded applications. Sometime ago, we had announced that Slint will be supporting Python https://www.reddit.com/r/Python/comments/18mdpig/declarative_gui_for_python/ Today we are excited to share that we released Slint v1.5 introducing Pythonic Slint. Check out all the details at https://slint.dev/blog/slint-1.5-released.html üöÄ Check out the repo at https://github.com/slint-ui/slint/tree/master/api/python#slint-python-alpha"},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "BrowserForge: Intelligent browser header and fingerprint generator", "Author": "u/daijro", "Content": "What it does: BrowserForge is a smart browser header and fingerprint generator that mimics the frequency if different browsers, operating systems, and devices found in the wild. Features Uses a Bayesian generative network to mimic actual web traffic Extremely fast runtime (0.1-0.2 miliseconds) Easy and simple for humans to use Extensive customization options for browsers, operating systems, devices, locales, and HTTP version Injectors for Playwright and Pyppeteer Written with type safety Target audience : Anyone interested in webscraping Comparison : Other popular libraries such as fake-headers do not consider the frequencies of header values in the real world, and are often flagged by bot detectors for unusual traffic. See it here: https://github.com/daijro/browserforge Credit to Apify's nodejs fingerprint-suite for the original logic! Hope you guys find it useful!"},
{"Title": "Fast API code to create/manage Open AI assistants", "Author": "u/HenryObj", "Content": "Hello everyone, I built a V1 for a SaaS platform to create and manage OpenAI assistants. It is made to be white-labeled. The back-end is in Python (FastAPI). I decided to work on other projects. If the code can be helpful to you, please feel free to use it the way you want ü§ù Link to the repo here üëâ github[dot]com/HenryObj/assistants"},
{"Title": "Virtual Workshop: Real-time streaming + analytics + visualization (Mar, 19)", "Author": "u/oli_k", "Content": "Hello, I'm organizing a workshop next Tuesday, March 19, 2024, at 4 PM Pacific Time. During this workshop, instructors will build a real-time dashboard for analyzing pizza shop orders using Bytewax, Pinot, and Streamlit. For more details and to RSVP (attendance is free), please visit: https://bytewax.io/events/real-time-pizza-analytics I believe this workshop is especially noteworthy for those interested, as I've personally enjoyed migrating the previous solution from Kafka Streams to Bytewax. The solution is now more Python-friendly, and the complexity of the codebase is significantly reduced. For me, it felt truly refreshing to rewrite it :D I am still in the process of updating the repository and will share it later, but here is the previous version: https://dev.startree.ai/docs/pinot/demo-apps/pizza-shop"},
{"Title": "Collecting Dask user quotes", "Author": "u/mrocklin", "Content": "So I'm updating the dask.org homepage and want to include user quotes of people who Dask has had some positive impact.  If you have ... Like Dask or found Dask it easy or helpful in some way don't mind having your name and face on the dask.org website ... then can you leave a comment here with a quote that you wouldn't mind me putting online?  Bonus points if you can link to something like a personal homepage or linkedin page where I can scrape a tiny headshot.  For long quotes I may tighten up language a bit (let me know if you mind). And hey, if you want to say negative things here that's ok too!  (although I probably won't put them on the homepage üôÇ) Thanks all, -matt!"},
{"Title": "Comparing hosts / providers for serverless cloud functions (FaaS) for Python", "Author": "u/hxmartin", "Content": "I was going crazy trying to compare Python cloud function hosts and started taking notes ... hope this helps someone, feedback welcome! https://github.com/hbmartin/comparison-hosts-serverless-cloud-function-faas-for-python"},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "NiceGUI For Beginners", "Author": "u/bitdoze", "Content": "Created  a beginner guide to NiceGUI and how you can get started, NiceGUI can help you build UIs to Python apps and offers performance and flexibility, in case someone is interested there is an article and a video: https://www.bitdoze.com/nicegui-get-started/"},
{"Title": "PyCon US grants free booth space and conference passes to early-stage startups. Apply by Sunday 3/17", "Author": "u/jrowley", "Content": "Official Event"},
{"Title": "Email Testing with Python smtpd module", "Author": "u/mraza007", "Content": "Python is packed with numerous command-line modules, one of them being smtpd . It allows you to run your own local SMTP server for testing emails. You can read more about it here in my blog post: Email Testing with Python's smtpd Module"},
{"Title": "No More Docstring Despair: Highlight Them in VSCode!", "Author": "u/erunosaurus", "Content": "Hey everyone! I'm excited to share a simple yet effective tool I've developed that's all about enhancing your Python coding experience in VSCode . We all know how Docstrings are crucial for understanding and documenting our Python code, but they often blend into the background, treated as standard comments in VSCode . This can make them harder to read and differentiate from the rest of your code. That's why I created a Python Docstring Highlighter for VSCode that recognizes the main styles (Google, NumPy, and Sphinx). This extension not only makes your code more readable but also allows you to customize the highlighting to fit your theme, making your coding environment as comfortable and productive as possible. Whether you're documenting your own project or navigating through others', this extension is designed to make your life easier and your code more accessible. Give it a try and see the difference for yourself! Source code Install extension (VSCode) Looking forward to your feedback and suggestions. Happy coding!"},
{"Title": "I created a tool that scrapes learncpp.com and make it into a pdf book using asyncio+multiprocessing", "Author": "u/AstronomerTerrible49", "Content": "Link Github - learncpp_pdf What My Project Does scrape htmls from learncpp.com and make it a pdf book Target Audience people who wants to learn cpp and would like to have a pdf copy of learncpp.com as well as people who wants to learn how to use asyncio and multiprocessing Comparison amalrajan/learncpp-download Too slow, since it does not make use of asyncio and multiprocessing LearnCpp converter Difficult to use, you have to run different scripts manually Introduction For thoese who might not familiar with learncpp.com, it is one of the most recommended and highly rated free online source for anyone who wants to learn cpp, but since it is hosted online and requires internet to access, it might not be as convenient as a pdf book for reading. the author specifically said that a PDF version of the site is not planned, and people should create the PDF version on their own, so I created this tool to help you easily make the PDF version on your own. Quote from the author Q: Is there a PDF version of this site available for offline viewing? Unfortunately, there is not. The site is able to stay free for everyone because we‚Äôre ad-sponsored -- that model simply doesn‚Äôt work in PDF format. You are welcome to convert pages from this website into PDF (or any other) format for your own private use, so long as you do not distribute them. Features Ultra fast, utilize concurrency for scraping and parallel for making PDF, the whole process is expected to finish within a few minutes. Rich cli interface showing realtime progress of the application Cached on fail, you can just re-run the application without worrying about redundant IO or calcualtion. This project is also meant to be a demonstration on usage of libs like pixi, multiprocessing and asyncio. Please share your thoughts on this, I'm all ears!"},
{"Title": "Understanding the Python memory footprint provides pointers to improve your code", "Author": "u/squareape", "Content": "While it is easy to use Python to turn an idea into a program, one will quickly run into bottlenecks that make their code less performant than they might want it to be. One such bottleneck is memory, of which Python consumes a lot compared to statically typed languages. Indeed, someone asking for advice on how to optimize their Python application online will likely receive the following advice: \"Rewrite it in Rust\". For obvious reasons, this is not very practical advice most of the time. Thus, we must make do with what we have: Python, and libraries written for Python. What follows is an exhibition of the memory model behind your Python application: How objects are allocated, where they are stored, and how they are eventually cleaned up. https://codebeez.nl/blogs/the-memory-footprint-of-your-python-application/"},
{"Title": "Disabling the GIL option has been merged into Python.", "Author": "u/germandiago", "Content": "Exciting to see, after many years, serious work in enabling multithreading that takes advantage of multiple CPUs in a more effective way in Python. One step at a time: https://github.com/python/cpython/pull/116338"},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Sudoku game in python - In terminal (TUI)", "Author": "u/python_game", "Content": "Hello, I made a Sudoku game in python (not really original). The difficult part for me was the user interface with curses library. A diagram is available to explain the algorithm used. The fame is compatible Linux, Mac and Windows. Link to the project here How to install and play? $ pip install play-sudoku $ sudoku What My Project Does Sudoku game, made with python & curses library. Play in the terminal. For Linux, Mac OS and Windows. Target Audience Geek and people who don't want to work. Linux, Mac and Windows. Doesn't work on virtual terminal. Comparison I just did it for fun. I try to make several games in python in terminal. Don't hesitate to let me know if you come across any errors or bugs . Thanks."},
{"Title": "What are the limitations for PSFL and BSD licences within a commercial used software?", "Author": "u/earthspaceman", "Content": "I'm currently building a program for internal use only to perform some calculations after receiving input from a Tkinter interface, loading and saving data locally. Matplotlib is used to create a graph for better visualisation of the output. libraries used: tkinter, json, os, math, matplotlib From what I understand based on the PSFL licence for tkinter, os, json, math there should be no problem. I think matplotlib BSD licence should also be ok. Libraries haven't been changed at all but only used inside the program by calling functions. Software is for internal use only and not for sell. What are the limitations and what do I have to indicate other and the PSFL licence and Matplotlib BSD?"},
{"Title": "Haystack 2.0 launch", "Author": "u/tuanacelik", "Content": "Haystack 2.0 stable is live! Try it out for yourself: https://haystack.deepset.ai/blog/haystack-2-release/ Haystack is an open-source AI framework in Python for creating production-ready apps using LLMs and other language models. It's nearly 4 years old - we've been doing NLP and LLM engineering since before it was cool. üòé and is model and database agnostic ‚Äî you can use whatever tools make the most sense for your use case. These tools are combined into a full AI app with the use of Haystack's components and pipelines. The aim of the framework is to provide the structure for you to compose your own custom app. üöÄ The pipeline architecture is a true graph, and I'll be presenting just that in this years PyCon USA too: Everything is a graph, including LLM Applications (and that's handy) Context about the 2.0 release: Haystack was first officially released in 2020, when the forefront of NLP was mostly semantic search, retrieval, and extractive question-answering. Haystack 2.0 is a complete rewrite, but the underlying principle of composing components into flexible pipelines remains the same.The release has quite a few model providers, tracing and monitoring capabilities, and databases supported out of the box:For models (generative and embedding): OpenAI, Mistral, Cohere, Jina AI, Google AI, Vertex AI, Optimum (by hugging face), sentence transformers, Amazon Bedrock, Azure, Fast Embed, OllamaFor databases: Weaviate, Pinecone, Qdrant, Mongo DB, Astra DB, Neo4j, pgvector, Chroma, Elastic Search, OpenSearch... Hope you give it a try and let us know what your think! We have a quick start guide to get going: https://haystack.deepset.ai/overview/quick-start"},
{"Title": "A Project I Am Working On!", "Author": "u/sushantshah-dev", "Content": "I am trying to build my own CMS in Python. It is supposed to be simple and useful for developers who like to tinker with the inner workings of their websites. Most mainstream content management systems like WordPress seem too complicated to play around with to me. https://pypi.org/project/sapphirecms/ https://github.com/SapphireCMS/SapphireCMS/ I am asking for help in the sense that I need to know the pain points and issues other developers face with the CMSs they use. This is also an open invitation for collaborators on the project. Your efforts will be credited."},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "deptry 0.14.0: Now up to 10 times faster by leveraging Rust!", "Author": "u/fpgmaas", "Content": "deptry 0.14.0 was just released, bringing significant speed improvements: It is now up to 10 times faster than the previous release! üöÄ For those unfamiliar with deptry; deptry is a command line tool to check for issues with dependencies in a Python project, such as unused or missing dependencies. [ GitHub ] For some benchmarks of the new release, see the release notes here . The performance improvement was achieved by leveraging Rust to parse the AST and extract the import statements from .py files, rather than using Python's ast module. The addition of Rust to the project also opens up doors for more optimizations in the future, so stay tuned!"},
{"Title": "DSPy: Programming‚Äînot prompting‚ÄîFoundation Models", "Author": "u/EnoughProject7477", "Content": "DSPy is a framework that aims to solve the fragility problem in language model (LM)-based applications by prioritizing programming over prompting. It allows you to recompile the entire pipeline to optimize it to your specific task ‚Äî instead of repeating manual rounds of prompt engineering ‚Äî whenever you change a component. https://github.com/stanfordnlp/dspy"},
{"Title": "I forked pyppeteer and revamped it into a new browser automation package: mokr.", "Author": "u/lamerlink", "Content": "Hello! Been working on this for awhile and excited to finally drop it. I forked Pyppeteer (no longer maintained) and revamped it, doing a good amount of refactoring and adding features along the way. Project link: https://github.com/michaeleveringham/mokr Documentation link: https://mokr.readthedocs.io/en/latest/index.html What My Project Does Offers automated web browsing for both Chrome and Firefox. Target Audience Currently enthusiasts seeking to automate tests or web scrape. Eventually I‚Äôd like to offer this as production-ready but my test suite isn‚Äôt finished yet. Comparisons I won‚Äôt regurgitate the entire readme but some notable changes from Pyppeteer or Playwright include an overhauled network manager, Firefox (partial) support, an httpx session that shares browser state, and proxy support, including SOCKS proxies. There are loads of other small changes too, like making names more ‚Äúpythonic‚Äù and restructuring the project. Feel free to give feedback and use it! I noted in the documentation some potential advantages over other solutions like Playwright (though I‚Äôm not deluded, Playwright is far superior still)."},
{"Title": "API Health Checks", "Author": "u/BigHeed87", "Content": "Healthy-API What it does Provide an easy interface for defining health checks for web applications written in Flask and FastAPI Target Audience For Python developers who are using Flask and FastAPI Similar Packages https://pypi.org/project/fastapi-healthchecks/ - This looks more Object/class based, but appears to only handles FastAPI. Also allows you to put up a maintenance page https://pypi.org/project/Flask-Meter/ - My older package which only handles Flask Background When I was a lesser experienced developer, I ended up writing my own solutions for health checks as a  tour of the language ecosystem. It also gave me insight on how to write a package which could be shared & maintained. I recently updated an older package of mine which only worked for Flask apps. These days I've been writing more FastAPI. Enjoy!"},
{"Title": "Declarative GUI for Python", "Author": "u/madnirua", "Content": "Slint is a declarative GUI toolkit to build native user interfaces for desktop and embedded applications. Sometime ago, we had announced that Slint will be supporting Python https://www.reddit.com/r/Python/comments/18mdpig/declarative_gui_for_python/ Today we are excited to share that we released Slint v1.5 introducing Pythonic Slint. Check out all the details at https://slint.dev/blog/slint-1.5-released.html üöÄ Check out the repo at https://github.com/slint-ui/slint/tree/master/api/python#slint-python-alpha"},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "BrowserForge: Intelligent browser header and fingerprint generator", "Author": "u/daijro", "Content": "What it does: BrowserForge is a smart browser header and fingerprint generator that mimics the frequency if different browsers, operating systems, and devices found in the wild. Features Uses a Bayesian generative network to mimic actual web traffic Extremely fast runtime (0.1-0.2 miliseconds) Easy and simple for humans to use Extensive customization options for browsers, operating systems, devices, locales, and HTTP version Injectors for Playwright and Pyppeteer Written with type safety Target audience : Anyone interested in webscraping Comparison : Other popular libraries such as fake-headers do not consider the frequencies of header values in the real world, and are often flagged by bot detectors for unusual traffic. See it here: https://github.com/daijro/browserforge Credit to Apify's nodejs fingerprint-suite for the original logic! Hope you guys find it useful!"},
{"Title": "Fast API code to create/manage Open AI assistants", "Author": "u/HenryObj", "Content": "Hello everyone, I built a V1 for a SaaS platform to create and manage OpenAI assistants. It is made to be white-labeled. The back-end is in Python (FastAPI). I decided to work on other projects. If the code can be helpful to you, please feel free to use it the way you want ü§ù Link to the repo here üëâ github[dot]com/HenryObj/assistants"},
{"Title": "Virtual Workshop: Real-time streaming + analytics + visualization (Mar, 19)", "Author": "u/oli_k", "Content": "Hello, I'm organizing a workshop next Tuesday, March 19, 2024, at 4 PM Pacific Time. During this workshop, instructors will build a real-time dashboard for analyzing pizza shop orders using Bytewax, Pinot, and Streamlit. For more details and to RSVP (attendance is free), please visit: https://bytewax.io/events/real-time-pizza-analytics I believe this workshop is especially noteworthy for those interested, as I've personally enjoyed migrating the previous solution from Kafka Streams to Bytewax. The solution is now more Python-friendly, and the complexity of the codebase is significantly reduced. For me, it felt truly refreshing to rewrite it :D I am still in the process of updating the repository and will share it later, but here is the previous version: https://dev.startree.ai/docs/pinot/demo-apps/pizza-shop"},
{"Title": "Collecting Dask user quotes", "Author": "u/mrocklin", "Content": "So I'm updating the dask.org homepage and want to include user quotes of people who Dask has had some positive impact.  If you have ... Like Dask or found Dask it easy or helpful in some way don't mind having your name and face on the dask.org website ... then can you leave a comment here with a quote that you wouldn't mind me putting online?  Bonus points if you can link to something like a personal homepage or linkedin page where I can scrape a tiny headshot.  For long quotes I may tighten up language a bit (let me know if you mind). And hey, if you want to say negative things here that's ok too!  (although I probably won't put them on the homepage üôÇ) Thanks all, -matt!"},
{"Title": "Comparing hosts / providers for serverless cloud functions (FaaS) for Python", "Author": "u/hxmartin", "Content": "I was going crazy trying to compare Python cloud function hosts and started taking notes ... hope this helps someone, feedback welcome! https://github.com/hbmartin/comparison-hosts-serverless-cloud-function-faas-for-python"},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "NiceGUI For Beginners", "Author": "u/bitdoze", "Content": "Created  a beginner guide to NiceGUI and how you can get started, NiceGUI can help you build UIs to Python apps and offers performance and flexibility, in case someone is interested there is an article and a video: https://www.bitdoze.com/nicegui-get-started/"},
{"Title": "PyCon US grants free booth space and conference passes to early-stage startups. Apply by Sunday 3/17", "Author": "u/jrowley", "Content": "Official Event"},
{"Title": "Email Testing with Python smtpd module", "Author": "u/mraza007", "Content": "Python is packed with numerous command-line modules, one of them being smtpd . It allows you to run your own local SMTP server for testing emails. You can read more about it here in my blog post: Email Testing with Python's smtpd Module"},
{"Title": "No More Docstring Despair: Highlight Them in VSCode!", "Author": "u/erunosaurus", "Content": "Hey everyone! I'm excited to share a simple yet effective tool I've developed that's all about enhancing your Python coding experience in VSCode . We all know how Docstrings are crucial for understanding and documenting our Python code, but they often blend into the background, treated as standard comments in VSCode . This can make them harder to read and differentiate from the rest of your code. That's why I created a Python Docstring Highlighter for VSCode that recognizes the main styles (Google, NumPy, and Sphinx). This extension not only makes your code more readable but also allows you to customize the highlighting to fit your theme, making your coding environment as comfortable and productive as possible. Whether you're documenting your own project or navigating through others', this extension is designed to make your life easier and your code more accessible. Give it a try and see the difference for yourself! Source code Install extension (VSCode) Looking forward to your feedback and suggestions. Happy coding!"},
{"Title": "I created a tool that scrapes learncpp.com and make it into a pdf book using asyncio+multiprocessing", "Author": "u/AstronomerTerrible49", "Content": "Link Github - learncpp_pdf What My Project Does scrape htmls from learncpp.com and make it a pdf book Target Audience people who wants to learn cpp and would like to have a pdf copy of learncpp.com as well as people who wants to learn how to use asyncio and multiprocessing Comparison amalrajan/learncpp-download Too slow, since it does not make use of asyncio and multiprocessing LearnCpp converter Difficult to use, you have to run different scripts manually Introduction For thoese who might not familiar with learncpp.com, it is one of the most recommended and highly rated free online source for anyone who wants to learn cpp, but since it is hosted online and requires internet to access, it might not be as convenient as a pdf book for reading. the author specifically said that a PDF version of the site is not planned, and people should create the PDF version on their own, so I created this tool to help you easily make the PDF version on your own. Quote from the author Q: Is there a PDF version of this site available for offline viewing? Unfortunately, there is not. The site is able to stay free for everyone because we‚Äôre ad-sponsored -- that model simply doesn‚Äôt work in PDF format. You are welcome to convert pages from this website into PDF (or any other) format for your own private use, so long as you do not distribute them. Features Ultra fast, utilize concurrency for scraping and parallel for making PDF, the whole process is expected to finish within a few minutes. Rich cli interface showing realtime progress of the application Cached on fail, you can just re-run the application without worrying about redundant IO or calcualtion. This project is also meant to be a demonstration on usage of libs like pixi, multiprocessing and asyncio. Please share your thoughts on this, I'm all ears!"},
{"Title": "Understanding the Python memory footprint provides pointers to improve your code", "Author": "u/squareape", "Content": "While it is easy to use Python to turn an idea into a program, one will quickly run into bottlenecks that make their code less performant than they might want it to be. One such bottleneck is memory, of which Python consumes a lot compared to statically typed languages. Indeed, someone asking for advice on how to optimize their Python application online will likely receive the following advice: \"Rewrite it in Rust\". For obvious reasons, this is not very practical advice most of the time. Thus, we must make do with what we have: Python, and libraries written for Python. What follows is an exhibition of the memory model behind your Python application: How objects are allocated, where they are stored, and how they are eventually cleaned up. https://codebeez.nl/blogs/the-memory-footprint-of-your-python-application/"},
{"Title": "Disabling the GIL option has been merged into Python.", "Author": "u/germandiago", "Content": "Exciting to see, after many years, serious work in enabling multithreading that takes advantage of multiple CPUs in a more effective way in Python. One step at a time: https://github.com/python/cpython/pull/116338"},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Sudoku game in python - In terminal (TUI)", "Author": "u/python_game", "Content": "Hello, I made a Sudoku game in python (not really original). The difficult part for me was the user interface with curses library. A diagram is available to explain the algorithm used. The fame is compatible Linux, Mac and Windows. Link to the project here How to install and play? $ pip install play-sudoku $ sudoku What My Project Does Sudoku game, made with python & curses library. Play in the terminal. For Linux, Mac OS and Windows. Target Audience Geek and people who don't want to work. Linux, Mac and Windows. Doesn't work on virtual terminal. Comparison I just did it for fun. I try to make several games in python in terminal. Don't hesitate to let me know if you come across any errors or bugs . Thanks."},
{"Title": "What are the limitations for PSFL and BSD licences within a commercial used software?", "Author": "u/earthspaceman", "Content": "I'm currently building a program for internal use only to perform some calculations after receiving input from a Tkinter interface, loading and saving data locally. Matplotlib is used to create a graph for better visualisation of the output. libraries used: tkinter, json, os, math, matplotlib From what I understand based on the PSFL licence for tkinter, os, json, math there should be no problem. I think matplotlib BSD licence should also be ok. Libraries haven't been changed at all but only used inside the program by calling functions. Software is for internal use only and not for sell. What are the limitations and what do I have to indicate other and the PSFL licence and Matplotlib BSD?"},
{"Title": "Haystack 2.0 launch", "Author": "u/tuanacelik", "Content": "Haystack 2.0 stable is live! Try it out for yourself: https://haystack.deepset.ai/blog/haystack-2-release/ Haystack is an open-source AI framework in Python for creating production-ready apps using LLMs and other language models. It's nearly 4 years old - we've been doing NLP and LLM engineering since before it was cool. üòé and is model and database agnostic ‚Äî you can use whatever tools make the most sense for your use case. These tools are combined into a full AI app with the use of Haystack's components and pipelines. The aim of the framework is to provide the structure for you to compose your own custom app. üöÄ The pipeline architecture is a true graph, and I'll be presenting just that in this years PyCon USA too: Everything is a graph, including LLM Applications (and that's handy) Context about the 2.0 release: Haystack was first officially released in 2020, when the forefront of NLP was mostly semantic search, retrieval, and extractive question-answering. Haystack 2.0 is a complete rewrite, but the underlying principle of composing components into flexible pipelines remains the same.The release has quite a few model providers, tracing and monitoring capabilities, and databases supported out of the box:For models (generative and embedding): OpenAI, Mistral, Cohere, Jina AI, Google AI, Vertex AI, Optimum (by hugging face), sentence transformers, Amazon Bedrock, Azure, Fast Embed, OllamaFor databases: Weaviate, Pinecone, Qdrant, Mongo DB, Astra DB, Neo4j, pgvector, Chroma, Elastic Search, OpenSearch... Hope you give it a try and let us know what your think! We have a quick start guide to get going: https://haystack.deepset.ai/overview/quick-start"},
{"Title": "A Project I Am Working On!", "Author": "u/sushantshah-dev", "Content": "I am trying to build my own CMS in Python. It is supposed to be simple and useful for developers who like to tinker with the inner workings of their websites. Most mainstream content management systems like WordPress seem too complicated to play around with to me. https://pypi.org/project/sapphirecms/ https://github.com/SapphireCMS/SapphireCMS/ I am asking for help in the sense that I need to know the pain points and issues other developers face with the CMSs they use. This is also an open invitation for collaborators on the project. Your efforts will be credited."},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "CTF-style Python Challenge", "Author": "u/seunghan_shin", "Content": "Hello everyone, I enjoy solving CTFs, and I often have thought it would be interesting to have CTF events in other domain besides security. Last year, I organized a small Python-themed CTF event for learners at a Korean conference. The response from participants was positive, so I decided to open it for everyone. https://pyctf.seung.de Anyone can read the challenges, but registration is required to submit flags. Since there's no email verification, feel free to try it without concerns. I made it for beginners and learners, so the challenges are generally easy, but there are also challenges for experienced people. I hope you all have fun. There may be mistakes in the moving to English ver., so I would greatly appreciate bug report or any feedback to help improve, and I will be more happier if I get to see more Python CTF events in the future."},
{"Title": "I Built a Squaredle Bot", "Author": "u/2bytesgoat", "Content": "https://www.youtube.com/watch?v=VtHtAbOSp8E Hey everyone üëã I‚Äôm trying to make a habit out of finishing projects and posting the progress on YouTube Any feedback is more than welcome üôè What my project does reads on screen mocks mouse input finds all words in a squaredle based on a word dictionary Target audience it‚Äôs s just a toy project Link to the bot written in Python https://github.com/2BytesGoat/squaredle-solver"},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Would you use Go / Rust style error handling in Python?", "Author": "u/ejstembler", "Content": "GitLab snippet \"\"\"v2 Exception instance is a part of the Result\"\"\" from typing import Any, Optional, Tuple from os import path Result = Tuple[Optional[Any], Optional[Exception]] def try_result(result: Result) -> Optional[Any]: # Pre-condition if result is None: raise ValueError('result is None') value, error = result if error is not None: raise error return value def summarize_file_ex(file_path: str) -> Result: # Pre-conditions if file_path is None: return None, ValueError('file_path is None') if file_path.strip() == '': return None, ValueError('file path is blank') if not path.exists(file_path): return None, FileExistsError(f\"file_path \\\"{file_path}\\\" not found\") total = 0.0 with open(file_path, 'r', encoding='utf-8') as file: for line in file: try: stripped_line: str = line.strip() num = float(stripped_line) except ValueError: return None, RuntimeError(f\"error converting line {line} value \\\"{stripped_line}\\\" to float in file \\\"{file_path}\\\".\") total += num return total, None if __name__ == '__main__': file_path: str = path.join('data', 'file_001.txt') total: float = try_result(summarize_file_ex(file_path=file_path)) print(f\"total = {total}\") ChatGPT4 had a mixed response: This approach can make your code more verbose, and it's somewhat unconventional in the Python community because it goes against the typical idiomatic way of using exceptions for error handling. Python's exceptions are powerful and are the standard way of handling errors, so using Go-style error handling may not leverage the full capabilities and syntactical sugar of Python, like the try-except blocks. However, if you prefer this explicit style of error handling for certain cases, or if you're working in an environment where the explicitness and clarity of error handling are prioritized, this pattern can be quite useful. Just keep in mind that it can lead to more boilerplate code and might be unfamiliar to other Python developers."},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Introducing thread: Extending the built-in threading module", "Author": "u/caffeine-addictt", "Content": "Good day everyone! I'm here to share a project I have been working on: thread. What is thread? Thread is a lightweight and performant library which extends the built-in threading module, enabling you to fetch return values, decorate functions and run functions concurrently. Comparison 9 times out of 10, what you are looking for are other built-in modules like multiprocessing , asyncio or concurrent.futures . Thread is not a replacement for these, but is for improving your experience for that 1 time. Target audience Thread is targeted at developers who want low-level control over threads without giving up features of other similar libraries. Thanks for your time! If you'd like to contribute or check out the project, you can do so at these links: Source code , Documentation , PyPI"},
{"Title": "How often do you find yourself ditching wrapper libraries?", "Author": "u/BarryTownCouncil", "Content": "I started writing some code to interact with Salesforce and pagerduty. So I found simple-salesforce and pypdras and set to work. After what felt like both too long and not long at all I started to get annoyed by both libraries. They (reasonably enough) worked totally different to each other and documentation would be poor. Alternatively they both wrap an HTTP REST API using the requests module, which has excellent documentation as do their official APIs. Well, relatively speaking of course. Whilst I head to dog a little into he authentication mechanisms and such it was very quickly massively easier to do the API calls myself and be able to program against them in very similar ways. I even decided to use requests_cache to add implicit caching to these backends and a couple of others I also started to use. Left me me wondering why anyone would bother making some of these sorts of wrapper libraries in the first place. So often they seem to be nothing but a hindrance. I'm sure I've had similar experiences with other non API libraries too, a library might make something 5 lines simpler but at the cost of another import, another pip installation on deployment... Often they seem to be more hassle than they're worth!"},
{"Title": "What is the combined size of your Python codebase?", "Author": "u/serge_databricks", "Content": "What is the combined size of your Python codebase? 1k-50k lines (Small) 50k-300k lines (Medium) 301k-1M lines (Large) 1M+ lines (Extra Large) For the reference: - Requests ( https://github.com/psf/requests ) is 17k lines - Flask ( https://github.com/pallets/flask ) is 34k lines - urllib3 ( https://github.com/urllib3/urllib3 ) is 35k lines - boto3 ( https://github.com/boto/boto3 ) is 70k lines - black ( https://github.com/psf/black ) is 131k lines - pip ( https://github.com/pypa/pip ) is 240k lines - Ansible ( https://github.com/ansible/ansible ) is 366k lines - SciKit Learn ( https://github.com/scikit-learn/scikit-learn ) is 471k lines - Django ( https://github.com/django/django ) is 668k lines - NumPy ( https://github.com/numpy/numpy ) is 757k lines - Airflow ( https://github.com/apache/airflow ) is 1.2M lines Should the bucket sizes be adjusted?"},
{"Title": "Github Copilot Lies to me", "Author": "u/codeguru42", "Content": "My first attempt to get some help from Github Copilot: Me: write a match case statement in python to match the first character of a string is a semicolon Copilot: Python doesn't have a built-in match or case statement like some other languages (e.g., JavaScript, Swift, Rust). However, you can achieve similar functionality using... Strangely, it finally gave me an answer when I made a small change to my prompt: Me: write a match statement in python to match the first character of a string is a semicolon Copilot: Python 3.10 introduces the structural pattern matching in the form of the match statement. Here's how you can use it to check if the first character of a string is a semicolon:... I almost gave up after the first response that was blatantly false. Fortunately I something that looks like reasonable syntax...but then I haven't tried it yet, so that might be a lie, too."},
{"Title": "Stockdex: Python Package to Extract Financial Insights From Multiple Sources", "Author": "u/nginx26", "Content": "Happy weekend every one! A while ago I showcased stockdex in this community. Since then, I've improved data retrieval performance by integrating yahoo finance API besides web scarping from sources. These tweaks have made more detailed data available in returned pandas dataframes while making the queries 5 times faster in average. What My Project Does A lightweight python package designed for the efficient retrieval of financial data from various sources like Yahoo Finance, Nasdaq, and Digrin. It is similar to yfinance python package. Comparison with yfinance It allows data retrieval from more sources, not just yahoo finance API. It also provides more data, for example yfinance returns dividends data for the last 5 years at max, but stockdex returns the entire dividends history. Target Audience The package is targeted at people who are interested in financial analysis using python. For more details or to contribute, feel free to visit the links below: Github Repo Link Pypi link"},
{"Title": "PyCon US 2024 Approaches", "Author": "u/monorepo", "Content": "Official Event"},
{"Title": "sdv: MIT created Python library for generating Synthetic Data", "Author": "u/semicausal", "Content": "MIT's Data-to-AI lab created sdv (synthetic data vault) to generate _realistic_ fake data from real, production data. - CTGAN paper: https://dspace.mit.edu/handle/1721.1/128349 - Vine Copula Models paper: https://dai.lids.mit.edu/wp-content/uploads/2019/01/1812.01226.pdf Code example: from sdv.lite import SingleTablePreset from sdv.datasets import demo import pandas as pd real_data, metadata = download_demo('single_table', 'adult') synthesizer = SingleTablePreset(metadata, name='FAST_ML') synthetic_data = synthesizer.sample(num_rows=500) # Once the model is trained, you can generate as many # look-alike rows of data as you'd like even if original data is smaller synthetic_data_more = synthesizer.sample(num_rows=10_000) Link to library: https://github.com/sdv-dev/SDV"},
{"Title": "Introducing Ludic: web development with HTMX, async, components, templating with f-strings", "Author": "u/pdcz", "Content": "A while ago I started experimenting with f-strings a bit, how to make them safe to use for HTML rendering without safety issues, basically I tried to make p(f\"Hello {b(\"World\")}! work. I also experimented with HTMX along the way. It ended as an open-source library. So today I'm releasing Ludic 0.1.0: https://github.com/paveldedik/ludic I'm looking for any feedback, is it something you would find useful? I'm still not sure how flexible this approach is. I know there are quite a lot of similar libraries, like reflex (I started researching when I already had quite a bit of code written as it was quite fun to write it). Nevertheless, I'm not done experimenting with it, I'm now trying to implement examples from htmx.org to write them in pure Python using the library and see how it goes. TL;DR looking for feedback for my library ."},
{"Title": "typedtemplate: A Pydantic approach to text templates", "Author": "u/Shakakai", "Content": "URL: https://github.com/Shakakai/typedtemplate I'm in the process of releasing a few of my internal libraries for building LLM-based apps. The first release is typedtemplate , a library that allows you to use Pydantic syntax to describe the data required by a template to render. What does it currently do? Documentation: Provides a class that you can easily generate documentation for. This means all your normal python documentation generators and tooling can provide useful docs for your templates that are easily consumable by your team. IDE/Copilot Support: Pydantic classes provide an great developer experience in the IDE with helpful autocomplete and a structure that Copilot understands well. Validation: Runtime validation that the data conforms to the type definitions. Why did I create this? I'm working with LLMs everyday now. I have hundreds of LLM prompts (and countless variants) in my project. If you are like me, it doesn't take long after creating a template to forget the shape of the data required to make it render properly. I'm lazy. This library lets me forget more things and lean on my tooling. Target Audience I'm currently using this in production and my company is supporting it. Comparison I actually don't know of a library that provides a type interface around string templates. I'm sure one exists. Notes Currently works with Jinja2 and Django templating engines. If you need a different engine, just ask (quick to implement). Was built to play nice with any framework. Next library coming is a Non-OpenAI interface to LLMs that feels good. Uses LiteLLM under the hood to support almost all available LLMs."},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "I made a really obfuscated way of doing FizzBuzz in python", "Author": "u/1cubealot", "Content": "Today I felt bored, so I made a program to do FizzBuzz in the most obfuscated way possible: def d(i,m):return lambda x:m if x%i==0else\"\" def f(n,a):return str(n)if len((m:=(''.join([l(n)for l in a]))))==0else m def r(n):return f(n,[d(3,'\\x46\\x69\\x7a\\x7a'),d(5,'\\x42\\x75\\x7a\\x7a')]) def p(l):print(r(l[0]));return None if len(l)==1else p(l[1:]) p([n for n in range(1, 101)]) # Runs it And here is the deobfuscated version: def divisibility_lambda_creator(divisor, message): return lambda num: message if num % divisor == 0 else \"\" def fizzbuzzify(num, lambdas): out = ''.join([l(num)for l in lambdas]) if len(out) == 0: return str(num) else: return out def run(num): return fizzbuzzify(num,[divisibility_lambda_creator(3, \"Fizz\"),divisibility_lambda_creator(5,'Buzz')]) def printer(nums): print(run(nums[0])) if len(nums) != 1: printer(nums[1:]) printer([n for n in range(1, 101)]) It is also really extendible: You can add  7, \"Bazz\" by doing: - def r(n):return f(n,[d(3,'\\x46\\x69\\x7a\\x7a'),d(5,'\\x42\\x75\\x7a\\x7a')]) + def r(n):return f(n,[d(3,'\\x46\\x69\\x7a\\x7a'),d(5,'\\x42\\x75\\x7a\\x7a'),d(7,'\\x42\\x61\\x7a\\x7a')]) If anyone has any way to obfuscate it further that'd be cool!"},
{"Title": "Website creation using UiWizard", "Author": "u/TheBoiDec", "Content": "What my project does UiWizard allows any python developer to create a website using python. I wanted to have a tool that I could use for my own personal projects with minimal exposure to the underlying technologies like JavaScript, CSS and HTML. This however does not mean that it is not possible to use these tools. UiWizard is made using FastAPI, Tailwind CSS, DaisyUI, HTMX as the major libraries. Target audience Anyone interested in creating a website. I currently would not recommend it for a production use-case in the current state but as it is with anything I'm not going to stop you. There are still some minor inconsistencies in the library API. Comparison This project mostly resembles NiceGUI as it was inspired by it. The feature set provided by NiceGUI is a lot bigger but the main difference is the approach to handling interactivity. NiceGUI uses websockets and client side to render its interface. UiWizard uses server side rendering and HTMX for interactivity instead. One difference that I think is important is NiceGUI uses websockets and any change or interactive element on the site triggers the user defined function in the code using events. This means there is only one request associated with the initial websocket connection. When a user interacts with UiWizard it triggers a new request each time, which means each function is an endpoint with access to the new request if needed. Source code This repository contains the source and some examples https://github.com/Declow/uiwiz Docs Some docs can be found here https://ui-wizard.com This website is made using UiWizard I started this project because I wanted to learn more about web development and have a tool I liked to work with."},
{"Title": "DukPy lightweight JS interpreter released for Python 3.12", "Author": "u/_amol_", "Content": "DukPy is a lightweight and easy to use JavaScript interpreter for Python. It comes with builtin support for various transpilers like SCSS, Less and React JSX thus allowing to build web apps in pure python without the need for NodeJS Release 0.3.1 finally added support for Python 3.12 and provides prebuilt wheels."},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Introducing PyPixelStream: An Open-Source Streaming Tool for Low-Res LED Displays üöÄ", "Author": "u/drboom9", "Content": "Hello, Python community! I'm excited to share a project I've been working on called PyPixelStream. This is an open-source live streaming software, specifically designed for low-resolution LED panels. Inspired by OBS (Open Broadcaster Software), PyPixelStream aims to simplify the management and broadcasting of content to LED screens, addressing the unique challenges presented by low-resolution visual media. What Does PyPixelStream Do? PyPixelStream enables live streaming to low-resolution LED screens, providing a cross-platform (Windows, Linux, macOS) environment to manage and broadcast visual content in a way that's optimized for these displays. It offers a variety of media sources (images, videos, webcam, screen capture, GIFs) and customizable text sources (clocks, timers, dates, cryptocurrency prices), along with dynamic effects and extendable filters to adjust brightness, contrast, apply chroma key, and more. Target Audience This project is aimed at technology enthusiasts, developers, visual artists, and anyone interested in exploring new ways to interact with low-resolution visual media. PyPixelStream is perfect for use in personal projects, artistic experimentation, or even in professional applications where LED displays play a central role. Comparison With Existing Alternatives Unlike other streaming solutions, PyPixelStream is specifically optimized for low-resolution LED panels, offering detailed customization and control over the broadcast content. While OBS is a powerful tool for live streaming in general, PyPixelStream focuses on the efficiency and adaptability needed for low-resolution media, ensuring that performance and quality are not compromised, even on limited-resource devices like the Raspberry Pi. This is an open-source project, and I'm eager to make it as accessible and versatile as possible! Whether you're interested in contributing code, suggesting features, or just providing feedback, your input would be greatly appreciated. Check out the project on GitHub here: https://github.com/offerrall/PyPixelStream"},
{"Title": "Tutorial: Multi-tenant todo list with FastAPI and Postgres", "Author": "u/gwen_from_nile", "Content": "I'm active in a SaaS community, so there are a lot of conversations about building multi-tenant apps. And FastAPI is super popular.  I didn't see a lot of examples though... So I created one: Example: https://github.com/niledatabase/niledatabase/tree/main/examples/quickstart/python Video tutorial: https://youtu.be/Axl63TUf2bc Written tutorial: https://www.thenile.dev/docs/getting-started/languages/python (Disclaimer: The example uses Nile, a serverless Postgres that I'm working on. But it can be modified to work on any Postgres). Feedback is welcome!"},
{"Title": "Tests should have 100% coverage", "Author": "u/Kiuhnm", "Content": "No, I'm not saying what you probably think I am. Keep reading: I'll be brief. Here's what I found today in one of the tests: for bad_fp in _bad_fps: with pytest.raises(BadArgumentError): follow_path(bad_fp, '.') follow_path('.', bad_fp) The correct code should've been for bad_fp in _bad_fps: with pytest.raises(BadArgumentError): follow_path(bad_fp, '.') with pytest.raises(BadArgumentError): follow_path('.', bad_fp) Is that a stupid mistake? Yes, it is, but stupid mistakes do happen, unfortunately. How did I catch it? The coverage of the testing file wasn't 100% like it usually is! I got suspicious and I saw that there was a glaring red line (vscode, coverage gutter) on the second call to follow_path . P.S. In case you're wondering, the code uses a custom hierarchy of exceptions instead of reusing the standard ones. The code is internal and we like it that way :)"},
{"Title": "Meet XposedOrNot: The Python Project Redefining How We Tackle Data Breaches", "Author": "u/DevaOnBreaches", "Content": "Hello r/Python community! I'm excited to share with you XposedOrNot, an open-source project built using Python to improve how we monitor data breaches. This project isn't just about enhancing security; it's about utilizing the power and flexibility of Python to address one of the most pressing issues in the digital world today. What My Project Does: XposedOrNot is an open-source tool built using Python, designed to enhance data breach monitoring. By leveraging the simplicity and versatility of Python, it offers a set of comprehensive features aimed at improving security practices and data breach awareness. The tool includes a dashboard with a risk score for emails, data breach impact analysis and analytics, visualization tools, and categorized data for better navigation and understanding of breach data. Target Audience: This project is geared towards developers, security professionals, and anyone interested in contributing to or utilizing security tools. It's suitable for both educational purposes and as a practical tool for monitoring data breaches in a production environment. Comparison: Unlike many commercial solutions that may be closed source and require payment for full features, XposedOrNot stands out because: Built with Python : It leverages Python for its development, making it easily accessible for contributions and customization by developers familiar with Python. 100% Open Source : The project invites open collaboration and contribution, emphasizing transparency and community-driven development. Free API Access & API Playground : It provides open API access and a playground for users to experiment and integrate the tool's capabilities into their projects. Comprehensive Features : Offers in-depth insights with its dashboard, analytics, and visualization tools specifically designed to navigate and understand data breaches effectively. Community-Driven Development : Encourages contributions and feedback with an open roadmap, recognizing contributors to foster a collaborative development environment. What‚Äôs Next & How to Support: I'm inviting the community to help shape the future of data breach monitoring by contributing ideas, code, or feedback. Your support is crucial. Explore the project on GitHub , where your stars help increase visibility and encourage more contributions. Let's use the power of Python to improve data breach monitoring together!"},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "LLM Based NLP Library", "Author": "u/FareedKhan557", "Content": "What My Project Does My friend and I recently developed an open-source NLP Library based on LLMs in Python, accessible via the pip install command. This library is capable of solving a wide array of NLP tasks for developers with minimal involvement. It contains over 30 features, covering everything from pattern extraction to OCR text extraction and beyond. Currently, it operates with the Gemini 1.0 Pro Model, allowing it to function without the need to run an LLM on your local environment. All that's required is a Gemini API key, which is available for free. Target audience Over the past few years, developers have encountered significant challenges when dealing with text data, despite the availability of rich resources such as SpaCy and other NLP libraries. These challenges persist due to the constantly increasing volume of data, which demands considerable human involvement, thought, and effort. Our target audience includes anyone who works with text data using Python. Comparison Consider one of the most stressful scenarios where you use regex to extract patterns. You define pattern formats and still remain unsure whether it completely yields accurate results. However, within our library, in one of our features, we attempt to solve this issue. You just need to pass the pattern name in a comma-separated string, such as \"email, person name, phone number\" , and the backend prompt template will handle the effort for you, returning the extracted patterns in a list. The output format can be a list, dictionary, string, or more, depending on the feature you utilize. The same applies to cleaning, intent detection, and many other features that you will explore in our library. GitHub Code, documentation, and example can all be found on GitHub: https://github.com/FareedKhan-dev/basiclingua-LLM-Based-NLP"},
{"Title": "CTF-style Python Challenge", "Author": "u/seunghan_shin", "Content": "Hello everyone, I enjoy solving CTFs, and I often have thought it would be interesting to have CTF events in other domain besides security. Last year, I organized a small Python-themed CTF event for learners at a Korean conference. The response from participants was positive, so I decided to open it for everyone. https://pyctf.seung.de Anyone can read the challenges, but registration is required to submit flags. Since there's no email verification, feel free to try it without concerns. I made it for beginners and learners, so the challenges are generally easy, but there are also challenges for experienced people. I hope you all have fun. There may be mistakes in the moving to English ver., so I would greatly appreciate bug report or any feedback to help improve, and I will be more happier if I get to see more Python CTF events in the future."},
{"Title": "I Built a Squaredle Bot", "Author": "u/2bytesgoat", "Content": "https://www.youtube.com/watch?v=VtHtAbOSp8E Hey everyone üëã I‚Äôm trying to make a habit out of finishing projects and posting the progress on YouTube Any feedback is more than welcome üôè What my project does reads on screen mocks mouse input finds all words in a squaredle based on a word dictionary Target audience it‚Äôs s just a toy project Link to the bot written in Python https://github.com/2BytesGoat/squaredle-solver"},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Would you use Go / Rust style error handling in Python?", "Author": "u/ejstembler", "Content": "GitLab snippet \"\"\"v2 Exception instance is a part of the Result\"\"\" from typing import Any, Optional, Tuple from os import path Result = Tuple[Optional[Any], Optional[Exception]] def try_result(result: Result) -> Optional[Any]: # Pre-condition if result is None: raise ValueError('result is None') value, error = result if error is not None: raise error return value def summarize_file_ex(file_path: str) -> Result: # Pre-conditions if file_path is None: return None, ValueError('file_path is None') if file_path.strip() == '': return None, ValueError('file path is blank') if not path.exists(file_path): return None, FileExistsError(f\"file_path \\\"{file_path}\\\" not found\") total = 0.0 with open(file_path, 'r', encoding='utf-8') as file: for line in file: try: stripped_line: str = line.strip() num = float(stripped_line) except ValueError: return None, RuntimeError(f\"error converting line {line} value \\\"{stripped_line}\\\" to float in file \\\"{file_path}\\\".\") total += num return total, None if __name__ == '__main__': file_path: str = path.join('data', 'file_001.txt') total: float = try_result(summarize_file_ex(file_path=file_path)) print(f\"total = {total}\") ChatGPT4 had a mixed response: This approach can make your code more verbose, and it's somewhat unconventional in the Python community because it goes against the typical idiomatic way of using exceptions for error handling. Python's exceptions are powerful and are the standard way of handling errors, so using Go-style error handling may not leverage the full capabilities and syntactical sugar of Python, like the try-except blocks. However, if you prefer this explicit style of error handling for certain cases, or if you're working in an environment where the explicitness and clarity of error handling are prioritized, this pattern can be quite useful. Just keep in mind that it can lead to more boilerplate code and might be unfamiliar to other Python developers."},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Introducing thread: Extending the built-in threading module", "Author": "u/caffeine-addictt", "Content": "Good day everyone! I'm here to share a project I have been working on: thread. What is thread? Thread is a lightweight and performant library which extends the built-in threading module, enabling you to fetch return values, decorate functions and run functions concurrently. Comparison 9 times out of 10, what you are looking for are other built-in modules like multiprocessing , asyncio or concurrent.futures . Thread is not a replacement for these, but is for improving your experience for that 1 time. Target audience Thread is targeted at developers who want low-level control over threads without giving up features of other similar libraries. Thanks for your time! If you'd like to contribute or check out the project, you can do so at these links: Source code , Documentation , PyPI"},
{"Title": "How often do you find yourself ditching wrapper libraries?", "Author": "u/BarryTownCouncil", "Content": "I started writing some code to interact with Salesforce and pagerduty. So I found simple-salesforce and pypdras and set to work. After what felt like both too long and not long at all I started to get annoyed by both libraries. They (reasonably enough) worked totally different to each other and documentation would be poor. Alternatively they both wrap an HTTP REST API using the requests module, which has excellent documentation as do their official APIs. Well, relatively speaking of course. Whilst I head to dog a little into he authentication mechanisms and such it was very quickly massively easier to do the API calls myself and be able to program against them in very similar ways. I even decided to use requests_cache to add implicit caching to these backends and a couple of others I also started to use. Left me me wondering why anyone would bother making some of these sorts of wrapper libraries in the first place. So often they seem to be nothing but a hindrance. I'm sure I've had similar experiences with other non API libraries too, a library might make something 5 lines simpler but at the cost of another import, another pip installation on deployment... Often they seem to be more hassle than they're worth!"},
{"Title": "What is the combined size of your Python codebase?", "Author": "u/serge_databricks", "Content": "What is the combined size of your Python codebase? 1k-50k lines (Small) 50k-300k lines (Medium) 301k-1M lines (Large) 1M+ lines (Extra Large) For the reference: - Requests ( https://github.com/psf/requests ) is 17k lines - Flask ( https://github.com/pallets/flask ) is 34k lines - urllib3 ( https://github.com/urllib3/urllib3 ) is 35k lines - boto3 ( https://github.com/boto/boto3 ) is 70k lines - black ( https://github.com/psf/black ) is 131k lines - pip ( https://github.com/pypa/pip ) is 240k lines - Ansible ( https://github.com/ansible/ansible ) is 366k lines - SciKit Learn ( https://github.com/scikit-learn/scikit-learn ) is 471k lines - Django ( https://github.com/django/django ) is 668k lines - NumPy ( https://github.com/numpy/numpy ) is 757k lines - Airflow ( https://github.com/apache/airflow ) is 1.2M lines Should the bucket sizes be adjusted?"},
{"Title": "Github Copilot Lies to me", "Author": "u/codeguru42", "Content": "My first attempt to get some help from Github Copilot: Me: write a match case statement in python to match the first character of a string is a semicolon Copilot: Python doesn't have a built-in match or case statement like some other languages (e.g., JavaScript, Swift, Rust). However, you can achieve similar functionality using... Strangely, it finally gave me an answer when I made a small change to my prompt: Me: write a match statement in python to match the first character of a string is a semicolon Copilot: Python 3.10 introduces the structural pattern matching in the form of the match statement. Here's how you can use it to check if the first character of a string is a semicolon:... I almost gave up after the first response that was blatantly false. Fortunately I something that looks like reasonable syntax...but then I haven't tried it yet, so that might be a lie, too."},
{"Title": "Stockdex: Python Package to Extract Financial Insights From Multiple Sources", "Author": "u/nginx26", "Content": "Happy weekend every one! A while ago I showcased stockdex in this community. Since then, I've improved data retrieval performance by integrating yahoo finance API besides web scarping from sources. These tweaks have made more detailed data available in returned pandas dataframes while making the queries 5 times faster in average. What My Project Does A lightweight python package designed for the efficient retrieval of financial data from various sources like Yahoo Finance, Nasdaq, and Digrin. It is similar to yfinance python package. Comparison with yfinance It allows data retrieval from more sources, not just yahoo finance API. It also provides more data, for example yfinance returns dividends data for the last 5 years at max, but stockdex returns the entire dividends history. Target Audience The package is targeted at people who are interested in financial analysis using python. For more details or to contribute, feel free to visit the links below: Github Repo Link Pypi link"},
{"Title": "PyCon US 2024 Approaches", "Author": "u/monorepo", "Content": "Official Event"},
{"Title": "sdv: MIT created Python library for generating Synthetic Data", "Author": "u/semicausal", "Content": "MIT's Data-to-AI lab created sdv (synthetic data vault) to generate _realistic_ fake data from real, production data. - CTGAN paper: https://dspace.mit.edu/handle/1721.1/128349 - Vine Copula Models paper: https://dai.lids.mit.edu/wp-content/uploads/2019/01/1812.01226.pdf Code example: from sdv.lite import SingleTablePreset from sdv.datasets import demo import pandas as pd real_data, metadata = download_demo('single_table', 'adult') synthesizer = SingleTablePreset(metadata, name='FAST_ML') synthetic_data = synthesizer.sample(num_rows=500) # Once the model is trained, you can generate as many # look-alike rows of data as you'd like even if original data is smaller synthetic_data_more = synthesizer.sample(num_rows=10_000) Link to library: https://github.com/sdv-dev/SDV"},
{"Title": "Introducing Ludic: web development with HTMX, async, components, templating with f-strings", "Author": "u/pdcz", "Content": "A while ago I started experimenting with f-strings a bit, how to make them safe to use for HTML rendering without safety issues, basically I tried to make p(f\"Hello {b(\"World\")}! work. I also experimented with HTMX along the way. It ended as an open-source library. So today I'm releasing Ludic 0.1.0: https://github.com/paveldedik/ludic I'm looking for any feedback, is it something you would find useful? I'm still not sure how flexible this approach is. I know there are quite a lot of similar libraries, like reflex (I started researching when I already had quite a bit of code written as it was quite fun to write it). Nevertheless, I'm not done experimenting with it, I'm now trying to implement examples from htmx.org to write them in pure Python using the library and see how it goes. TL;DR looking for feedback for my library ."},
{"Title": "typedtemplate: A Pydantic approach to text templates", "Author": "u/Shakakai", "Content": "URL: https://github.com/Shakakai/typedtemplate I'm in the process of releasing a few of my internal libraries for building LLM-based apps. The first release is typedtemplate , a library that allows you to use Pydantic syntax to describe the data required by a template to render. What does it currently do? Documentation: Provides a class that you can easily generate documentation for. This means all your normal python documentation generators and tooling can provide useful docs for your templates that are easily consumable by your team. IDE/Copilot Support: Pydantic classes provide an great developer experience in the IDE with helpful autocomplete and a structure that Copilot understands well. Validation: Runtime validation that the data conforms to the type definitions. Why did I create this? I'm working with LLMs everyday now. I have hundreds of LLM prompts (and countless variants) in my project. If you are like me, it doesn't take long after creating a template to forget the shape of the data required to make it render properly. I'm lazy. This library lets me forget more things and lean on my tooling. Target Audience I'm currently using this in production and my company is supporting it. Comparison I actually don't know of a library that provides a type interface around string templates. I'm sure one exists. Notes Currently works with Jinja2 and Django templating engines. If you need a different engine, just ask (quick to implement). Was built to play nice with any framework. Next library coming is a Non-OpenAI interface to LLMs that feels good. Uses LiteLLM under the hood to support almost all available LLMs."},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "I made a really obfuscated way of doing FizzBuzz in python", "Author": "u/1cubealot", "Content": "Today I felt bored, so I made a program to do FizzBuzz in the most obfuscated way possible: def d(i,m):return lambda x:m if x%i==0else\"\" def f(n,a):return str(n)if len((m:=(''.join([l(n)for l in a]))))==0else m def r(n):return f(n,[d(3,'\\x46\\x69\\x7a\\x7a'),d(5,'\\x42\\x75\\x7a\\x7a')]) def p(l):print(r(l[0]));return None if len(l)==1else p(l[1:]) p([n for n in range(1, 101)]) # Runs it And here is the deobfuscated version: def divisibility_lambda_creator(divisor, message): return lambda num: message if num % divisor == 0 else \"\" def fizzbuzzify(num, lambdas): out = ''.join([l(num)for l in lambdas]) if len(out) == 0: return str(num) else: return out def run(num): return fizzbuzzify(num,[divisibility_lambda_creator(3, \"Fizz\"),divisibility_lambda_creator(5,'Buzz')]) def printer(nums): print(run(nums[0])) if len(nums) != 1: printer(nums[1:]) printer([n for n in range(1, 101)]) It is also really extendible: You can add  7, \"Bazz\" by doing: - def r(n):return f(n,[d(3,'\\x46\\x69\\x7a\\x7a'),d(5,'\\x42\\x75\\x7a\\x7a')]) + def r(n):return f(n,[d(3,'\\x46\\x69\\x7a\\x7a'),d(5,'\\x42\\x75\\x7a\\x7a'),d(7,'\\x42\\x61\\x7a\\x7a')]) If anyone has any way to obfuscate it further that'd be cool!"},
{"Title": "Website creation using UiWizard", "Author": "u/TheBoiDec", "Content": "What my project does UiWizard allows any python developer to create a website using python. I wanted to have a tool that I could use for my own personal projects with minimal exposure to the underlying technologies like JavaScript, CSS and HTML. This however does not mean that it is not possible to use these tools. UiWizard is made using FastAPI, Tailwind CSS, DaisyUI, HTMX as the major libraries. Target audience Anyone interested in creating a website. I currently would not recommend it for a production use-case in the current state but as it is with anything I'm not going to stop you. There are still some minor inconsistencies in the library API. Comparison This project mostly resembles NiceGUI as it was inspired by it. The feature set provided by NiceGUI is a lot bigger but the main difference is the approach to handling interactivity. NiceGUI uses websockets and client side to render its interface. UiWizard uses server side rendering and HTMX for interactivity instead. One difference that I think is important is NiceGUI uses websockets and any change or interactive element on the site triggers the user defined function in the code using events. This means there is only one request associated with the initial websocket connection. When a user interacts with UiWizard it triggers a new request each time, which means each function is an endpoint with access to the new request if needed. Source code This repository contains the source and some examples https://github.com/Declow/uiwiz Docs Some docs can be found here https://ui-wizard.com This website is made using UiWizard I started this project because I wanted to learn more about web development and have a tool I liked to work with."},
{"Title": "DukPy lightweight JS interpreter released for Python 3.12", "Author": "u/_amol_", "Content": "DukPy is a lightweight and easy to use JavaScript interpreter for Python. It comes with builtin support for various transpilers like SCSS, Less and React JSX thus allowing to build web apps in pure python without the need for NodeJS Release 0.3.1 finally added support for Python 3.12 and provides prebuilt wheels."},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Introducing PyPixelStream: An Open-Source Streaming Tool for Low-Res LED Displays üöÄ", "Author": "u/drboom9", "Content": "Hello, Python community! I'm excited to share a project I've been working on called PyPixelStream. This is an open-source live streaming software, specifically designed for low-resolution LED panels. Inspired by OBS (Open Broadcaster Software), PyPixelStream aims to simplify the management and broadcasting of content to LED screens, addressing the unique challenges presented by low-resolution visual media. What Does PyPixelStream Do? PyPixelStream enables live streaming to low-resolution LED screens, providing a cross-platform (Windows, Linux, macOS) environment to manage and broadcast visual content in a way that's optimized for these displays. It offers a variety of media sources (images, videos, webcam, screen capture, GIFs) and customizable text sources (clocks, timers, dates, cryptocurrency prices), along with dynamic effects and extendable filters to adjust brightness, contrast, apply chroma key, and more. Target Audience This project is aimed at technology enthusiasts, developers, visual artists, and anyone interested in exploring new ways to interact with low-resolution visual media. PyPixelStream is perfect for use in personal projects, artistic experimentation, or even in professional applications where LED displays play a central role. Comparison With Existing Alternatives Unlike other streaming solutions, PyPixelStream is specifically optimized for low-resolution LED panels, offering detailed customization and control over the broadcast content. While OBS is a powerful tool for live streaming in general, PyPixelStream focuses on the efficiency and adaptability needed for low-resolution media, ensuring that performance and quality are not compromised, even on limited-resource devices like the Raspberry Pi. This is an open-source project, and I'm eager to make it as accessible and versatile as possible! Whether you're interested in contributing code, suggesting features, or just providing feedback, your input would be greatly appreciated. Check out the project on GitHub here: https://github.com/offerrall/PyPixelStream"},
{"Title": "Tutorial: Multi-tenant todo list with FastAPI and Postgres", "Author": "u/gwen_from_nile", "Content": "I'm active in a SaaS community, so there are a lot of conversations about building multi-tenant apps. And FastAPI is super popular.  I didn't see a lot of examples though... So I created one: Example: https://github.com/niledatabase/niledatabase/tree/main/examples/quickstart/python Video tutorial: https://youtu.be/Axl63TUf2bc Written tutorial: https://www.thenile.dev/docs/getting-started/languages/python (Disclaimer: The example uses Nile, a serverless Postgres that I'm working on. But it can be modified to work on any Postgres). Feedback is welcome!"},
{"Title": "Tests should have 100% coverage", "Author": "u/Kiuhnm", "Content": "No, I'm not saying what you probably think I am. Keep reading: I'll be brief. Here's what I found today in one of the tests: for bad_fp in _bad_fps: with pytest.raises(BadArgumentError): follow_path(bad_fp, '.') follow_path('.', bad_fp) The correct code should've been for bad_fp in _bad_fps: with pytest.raises(BadArgumentError): follow_path(bad_fp, '.') with pytest.raises(BadArgumentError): follow_path('.', bad_fp) Is that a stupid mistake? Yes, it is, but stupid mistakes do happen, unfortunately. How did I catch it? The coverage of the testing file wasn't 100% like it usually is! I got suspicious and I saw that there was a glaring red line (vscode, coverage gutter) on the second call to follow_path . P.S. In case you're wondering, the code uses a custom hierarchy of exceptions instead of reusing the standard ones. The code is internal and we like it that way :)"},
{"Title": "Meet XposedOrNot: The Python Project Redefining How We Tackle Data Breaches", "Author": "u/DevaOnBreaches", "Content": "Hello r/Python community! I'm excited to share with you XposedOrNot, an open-source project built using Python to improve how we monitor data breaches. This project isn't just about enhancing security; it's about utilizing the power and flexibility of Python to address one of the most pressing issues in the digital world today. What My Project Does: XposedOrNot is an open-source tool built using Python, designed to enhance data breach monitoring. By leveraging the simplicity and versatility of Python, it offers a set of comprehensive features aimed at improving security practices and data breach awareness. The tool includes a dashboard with a risk score for emails, data breach impact analysis and analytics, visualization tools, and categorized data for better navigation and understanding of breach data. Target Audience: This project is geared towards developers, security professionals, and anyone interested in contributing to or utilizing security tools. It's suitable for both educational purposes and as a practical tool for monitoring data breaches in a production environment. Comparison: Unlike many commercial solutions that may be closed source and require payment for full features, XposedOrNot stands out because: Built with Python : It leverages Python for its development, making it easily accessible for contributions and customization by developers familiar with Python. 100% Open Source : The project invites open collaboration and contribution, emphasizing transparency and community-driven development. Free API Access & API Playground : It provides open API access and a playground for users to experiment and integrate the tool's capabilities into their projects. Comprehensive Features : Offers in-depth insights with its dashboard, analytics, and visualization tools specifically designed to navigate and understand data breaches effectively. Community-Driven Development : Encourages contributions and feedback with an open roadmap, recognizing contributors to foster a collaborative development environment. What‚Äôs Next & How to Support: I'm inviting the community to help shape the future of data breach monitoring by contributing ideas, code, or feedback. Your support is crucial. Explore the project on GitHub , where your stars help increase visibility and encourage more contributions. Let's use the power of Python to improve data breach monitoring together!"},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "LLM Based NLP Library", "Author": "u/FareedKhan557", "Content": "What My Project Does My friend and I recently developed an open-source NLP Library based on LLMs in Python, accessible via the pip install command. This library is capable of solving a wide array of NLP tasks for developers with minimal involvement. It contains over 30 features, covering everything from pattern extraction to OCR text extraction and beyond. Currently, it operates with the Gemini 1.0 Pro Model, allowing it to function without the need to run an LLM on your local environment. All that's required is a Gemini API key, which is available for free. Target audience Over the past few years, developers have encountered significant challenges when dealing with text data, despite the availability of rich resources such as SpaCy and other NLP libraries. These challenges persist due to the constantly increasing volume of data, which demands considerable human involvement, thought, and effort. Our target audience includes anyone who works with text data using Python. Comparison Consider one of the most stressful scenarios where you use regex to extract patterns. You define pattern formats and still remain unsure whether it completely yields accurate results. However, within our library, in one of our features, we attempt to solve this issue. You just need to pass the pattern name in a comma-separated string, such as \"email, person name, phone number\" , and the backend prompt template will handle the effort for you, returning the extracted patterns in a list. The output format can be a list, dictionary, string, or more, depending on the feature you utilize. The same applies to cleaning, intent detection, and many other features that you will explore in our library. GitHub Code, documentation, and example can all be found on GitHub: https://github.com/FareedKhan-dev/basiclingua-LLM-Based-NLP"},
{"Title": "Sieve of Eratosthenes (Haskell VS Python)", "Author": "u/Kiuhnm", "Content": "I've just come across an article where a Haskell, a Python, and a JS implementation of the Sieve of Eratosthenes are shown. Here's the Haskell one: primes = sieve [ 2.. ] where sieve (p:x) = p : sieve [ n | n <- x, n `mod` p > 0 ] And here's the Python one: def eratosthenes(n): sieve = [ True for i in range(n+1) ] def markOff(pv): for i in range(pv+pv, n+1, pv): sieve[i] = False markOff(2) for i in range(3, n+1): if sieve[i]: markOff(i) return [ i for i in range(1, n+1) if sieve[i] ] I don't understand why markOff(2) is singled out and not handled in the loop. Why compare a clever Haskell implementation with a stupid Python one? Here's my version: def eratosthenes(n): sieve = [True] * (n + 1) for i, x in enumerate(sieve): if i < 2 or not x: continue for j in range(2 * i, n + 1, i): sieve[j] = False return [i for i, x in enumerate(sieve) if x][2:] We can also start from 2: def eratosthenes2(n): sieve_from2 = [True] * (n - 1)      # for ints 2, 3, ..., n for i, x in enumerate(sieve_from2): if not x: continue for j in range(i + (i + 2), n - 1, i + 2): sieve_from2[j] = False return [i + 2 for i, x in enumerate(sieve_from2) if x] But next time someone shows you the Haskell code above, please show them this: def sieve(xs): yield (p := next(it := iter(xs))) yield from sieve(x for x in it if x % p > 0) primes = sieve(count(2)) We're just some syntactic sugar away from the Haskell implementation! This proves that we can if we want to. We just don't see the point in being that clever all the time :) EDIT: Here's the recursive version without using any division: def sieve2(xs): yield (p := next(it := dropwhile(lambda x: not x[1], xs))[0]) yield from sieve2((x, b and r != p - 1) for (x, b), r in zip(it, cycle(range(p)))) primes2 = sieve2(zip(count(2), repeat(True))) Not so elegant anymore..."},
{"Title": "The First Annual Meeting of the Extraordinary League of Runtime Typers", "Author": "u/monorepo", "Content": "Original Posts GitHub Discussion Python.org Discussion PEP 649 As a user of strong typing in Python, and maintainer of libraries that utilize them greatly with plans of further work on a runtime type-using library, I found this quite an interesting read. Basically seeks to bring up implications of Python 3.13's  updates on runtime type checkers via PEP 649... This comes from the maintainer of Beartype - if you aren't familiar with beartype 's documentation then it's worth a read even if you never use the library - a runtime-static type-checker. It brings in some highly thought after individuals in the Python typing community including the Anyio, Pydantic, Msgspec maintainers... certainly worth a read through if this (typing) interests you."},
{"Title": "Reflex 0.4.0 - Web Apps in Pure Python", "Author": "u/Boordman", "Content": "Hey everyone, we just released a new version of reflex and wanted to share some updates. For those who don‚Äôt know about Reflex (we used to be called Pynecone ), it‚Äôs a framework to build web apps in pure Python. We wanted to make it easy for Python developers to share their ideas without having to use Javascript and traditional frontend tools, while still being as flexible enough to create any type of web app. Since our last post, we‚Äôve made many improvements including: We‚Äôve released our hosting service . Just type reflex deploy and we will set up your app, and give you a URL back to share with others. During our alpha we‚Äôre giving free hosting for all apps (and always plan to have a free tier). A tutorial on building a ChatGPT clone using Reflex. See the final app https://chat.reflex.run New core components based on Radix UI, with a unified theming system. More guides on how to wrap custom React components. We‚Äôre working now on building out our 3rd party component ecosystem. Our key focuses going forward are on making the framework stable, speed improvements, and growing out the ecosystem of 3rd party components. We‚Äôve published our roadmap here . Let us know what you think - we‚Äôre fully open source and welcome contributions! We also have a Reddit where we post updates: https://www.reddit.com/r/reflex/"},
{"Title": "Cadwyn: the most sophisticated Python API Versioning framework", "Author": "u/Varabe", "Content": "What My Project Does Cadwyn allows you to support a single version of your code while auto-generating the schemas and routes for older versions. You keep REST API versioning encapsulated in small and independent \"version change\" modules while your business logic stays simple and knows nothing about versioning. It is heavily inspired by Stripe 's approach to API versioning but it is much more sophisticated and allows you to reach the true \"zero-duplication\" versioning. We have recently discussed it on TalkPython podcast. Target audience Cadwyn is made for FastAPI so any FastAPI developer could benefit from it immediately if they need versioning. However, Cadwyn is also a huge case study: its documentation contains a lot of guides and research on how to build any kind of API Versioning and Cadwyn in itself can serve as a guide for making it in another web framework or language. Comparison There does not exist any tool (especially open-source and especially in python) for API versioning of this quality and scalability. GitHub https://github.com/zmievsa/cadwyn"},
{"Title": "Anaconda 2024-02 Release Notes", "Author": "u/PhilipYip", "Content": "Anaconda 2024-02 is released: python 3.11.7 conda 24.1.2 numpy 1.26.3 pandas 2.1.4 matplotlib 3.8.0 jupyterlab 4.0.11 spyder 5.4.3 The JupyterLab Variable Inspector extension is also preinstalled. Anaconda Release Notes Miniconda 24.1.2-0 (February 27, 2024) is released: Python 3.12.1 conda 24.1.2 Miniconda Release Notes The new version of conda uses the libmamba solver by default which has a number of performance improvements over the classic solver. The conda team is pleased to announce the availability of ‚Äòlibmamba‚Äô as a new, much faster dependency solver for conda! Three different companies worked to make this release possible: QuantStack, developing mamba and libmamba; Quansight, integrating libmamba into conda; and Anaconda, developing conda and managing the overall effort. A faster conda We are changing the default solver of conda to conda-libmamba-solverFirst, complex solves will run noticeably faster. Benchmarking predicts a 50 to 80% improvement in run times. Second, you will notice improved error messages when conda encounters problems. libmamba's error messages give you more insight into what is triggering the problem. Third, conda-libmamba-solver outputs more details about the channels in use and target platform at the beginning of the process. Mind these differences if you are parsing stdout (although we definitely recommend enabling the --json mode for programmatic usage!). conda 23.10.0+ uses libmamba by default Originally libmamba was supposed to be rolled out for the 2023-09 release but this came with conda=23.7.4 which included the libmamba solver but wasn't enabled by default: conda libmamba rollout"},
{"Title": "Sitcom Simulator: a tool for generating weird AI meme videos", "Author": "u/ricekrispysawdust", "Content": "What My Project Does Sitcom Simulator is a python/CLI tool that takes any text prompt (for example, \"Joe Biden vs. Donald Trump: ultimate weightlifting competition\") and turns it into a bizarre meme video with realistic images and voices. The tool is essentially duct tape that combines various AI tools into one unholy abomination: ChatGPT generates the video script. FakeYou generates voices for the characters. Stable Diffusion generates images of the characters. Freepd provides the background music. FFmpeg connects the images and voices into a movie. Target Audience People who like making memes, funny videos, or weird AI \"art\" but are too lazy to do it manually. The code is fairly customizable and extendable; it could probably be used as a base for many types of AI video generation projects even if your use case is somewhat different. Comparison There are many AI video editing tools out there (e.g., Kapwing), almost all of which are complicated commercial products with a vague notion of improving \"productivity\" or whatever. In contrast, Sitcom Simulator is simple, open source, and the only AI video tool focused on humor, memes, and wasting your time with terrible brain rot. GitHub Code, documentation, and example videos can all be found on GitHub: https://github.com/joshmoody24/sitcom-simulator"},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Framework-agnostic RPC API with auto-generated TypeScript client", "Author": "u/westandskif", "Content": "Idea The below must be enough to define an API: class UserParams(pydantic.BaseModel): uid: str class GetUsers(AbstractProcedure): def call(self, in_: UserParams, context) -> List[UserDetails]: ... to then do the following somewhere in your TypeScript code: import { callGetUsers } from \"./src/out\"; expect(callGetUsers(userParams).$promise).resolves.toEqual(listOfUserDetails); Desired benefits Automated typescript client generation Of course, it's possible to annotate your API, export an OpenAPI schema and generate a typescript client from it. I just want it out of the box. Browser Dates done right Javascript doesn't have a separate date type, so it uses Date for both python's date and datetime. Hence when you pass 2000-01-01 to a browser in New York, the browser will read it as UTC datetime and then convert it to the local timezone, so it will give you Dec 31, 1991 7:00PM, which is fine if you wanted to work with a particular moment in time, but what if you wanted to display someone's date of birth? That's why lacking the date type is a problem. The library should see that you want to pass python's date to the browser and automatically prepare it in the browser, so that  Jan 1st is still Jan 1st. Browser friendly types only synclane raises an exception if you use types, which browser won't be able to understand. No need to define URLs A procedure name should be enough. e.g. you name it AddUser, you should be able to just run callAddUser function in the typescript client. I don't want to define any other identifier like path per API endpoint. Enums If your procedure in/out types include enums, they should become available in the typescript client. What My Project Does It does all the above: checks types, handles RPC requests, generates TypeScript client code, including all the above benefits. Target Audience It is in currently in beta. If it doesn't require breaking changes within next 6-12 months, I'll make it stable. Comparison It is like FastAPI + Pydantic, but with: out of the box TypeScript client generation benefits described above should be easily embeddable to any framework, because it only requires a single API endpoint (all the routing happens inside of RPC) Links docs & examples: https://synclane.readthedocs.io/en/latest/ github: https://github.com/westandskif/synclane pypi: https://pypi.org/project/synclane/ What do you think? Would you use it? Any feedback is much appreciated!"},
{"Title": "üßë‚Äçüíª Open Interface - Operate your Computer Using LLMs - Built completely in Python", "Author": "u/reasonableWiseguy", "Content": "What My Project Does Open Interface is a desktop app (MacOS/Windows/Linux) that can control your mouse and keyboard to perform any human-computer task you ask of it by using LLMs as a backend to break down the steps required. Target Audience Since most LLMs are not production reliable, this is still a toy project but anyone can use it. Comparison I haven't much looked at alternatives since this is an application not a library. Libraries used - Tkinter - PyAutoGUI - Pyinstaller - The official OpenAI module Github : https://github.com/AmberSahdev/Open-Interface/ Demo : https://github.com/AmberSahdev/Open-Interface/?tab=readme-ov-file#demo Install : https://github.com/AmberSahdev/Open-Interface/?tab=readme-ov-file#install"},
{"Title": "Contribute to Python cybersecurity projects in GSoC 2024!", "Author": "u/stratospherelab", "Content": "Hello everyone, We're happy to announce that our Stratosphere Laboratory has been accepted as a mentor organization in the Google Summer of Code 2024! Projects you can contribute to are our Intrusion Prevention System and our AI VPN: https://github.com/stratosphereips/StratosphereLinuxIPS https://github.com/stratosphereips/AIVPN Check out our ideas list here: https://github.com/stratosphereips/Google-Summer-of-Code/blob/main/list_of_ideas.md If you think you have the required skills to contribute, you can check the contributor guide here: https://github.com/stratosphereips/Google-Summer-of-Code/blob/main/contributor_guide.md You can join our discord if you have more questions https://discord.gg/zu5HwMFy5C Let's make this summer a memorable one!  üöÄ üöÄ"},
{"Title": "I made a YouTube downloader with Modern UI | PyQt6 | PyTube | Fluent Design", "Author": "u/Specialist-Arachnid6", "Content": "What my Project Does? Youtility helps you to download YouTube content locally. With Youtility, you can download: Single videos with captions file Playlists (also as audio-only files) Video to Mp3 Target Audience People who want to save YouTube playlists/videos locally who don't wanna use command line tools like PyTube. Comparison Unlike existing alternatives, Youtility helps you to download even an entire playlist as audio files. It can also download XML captions for you. Plus, it also has a great UI. GitHub GitHub Link: https://github.com/rohankishore/Youtility"},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "I hate typing out every 'self.x = x' line in an __init__ method. Is this alternative acceptable?", "Author": "u/MomICantPauseReddit", "Content": "class Movable: def __init__(self, x, y, dx, dy, worldwidth, worldheight): \"\"\"automatically sets the given arguments. Can be reused with any class that has an order of named args.\"\"\" nonmembers = [] #populate with names that should not become members and will be used later. In many simple classes, this can be left empty. for key, value in list(locals().items())[1:]: #exclude 'self', which is the first entry. if not key in nonmembers: setattr(self, key, value) #handle all nonmembers and assign other members: return I always hate how redundant and bothersome it is to type \"self.member = member\" 10+ times, and this code does work the way I want it to. It's pretty readable in my opinion, especially with the documentation. That aside, is it considered acceptable practice in python? Will other developers get annoyed if I use it? Edit:  Thanks for the very fast replies. Data classes it is! I meant for this to be a discussion of code conventions, but since I learned about a completely new feature to me, I guess this post belongs in r/learpython ."},
{"Title": "shavis - Visualize SHA256 and SHA1", "Author": "u/perfectminimalism", "Content": "https://github.com/kernel137/shavis Install with pip install shavis shavis is (secure hash algorithm visualization), is a CLI tool that can hash any file (Only through SHA256) and create an 8x8 pixel image displaying the hash through themed colours. What My Project Does : Main functionality of this CLI tool is taking the 64 digit hex number (SHA256 hash) and, through 16 hex colors in ascending order, turning it into a 8x8 pixel image that can then be scaled to powers of two. Shavis also supports piping: echo -n \"Hello World!\" | shavis This also works for git hashes, since git hashes are SHA1, images for this hash are 8x5, I think with some clever integration, its possible to integrate a small res picture of a git commit hash  to quickly visualize the hash within vscode or some other platform. A quick way to get a visual of the last git commit while within a local git repository is: git rev-parse HEAD | shavis -g What do you guys think? Is it a worthwhile investment to implement other hashes? Is it an interesting project? Do you guys have any ideas about what to add to it?"},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "An extremely modern and configurable Python project template", "Author": "u/tedivm", "Content": "What My Project Does Rob's Awesome Python Template is a cookiecutter template meant to bootstrap python projects using modern best practices. At the very basic level it includes: Modern pyproject.toml without any legacy files (no setup.py or setup.cfg). Development Management using Makefiles . Configuration Management with Pydantic . PyPI Publishing from Git Tags using setuptools-scm . Formatting and Linting with Ruff . Typing with mypy . Lockfiles (requirements.txt, requirements-dev.txt) with uv . Testing with pytest . CI/CD using Github Actions . Precommit Hooks using the precommit framework . It also has a ton of optional features: Github Actions for CI Cross Platform (arm, arm64, amd64) Docker containers using the Multi-Py project. Optionally use any combination of FastAPI, Click/Typer, Celery, and Sqlalchemy. I've used this template for a number of projects- QuasiQueue , Paracelsus , and Fedimapper being some nice examples. If you want to see exactly what the project would generate today you can review the examples repository which builds a few projects using different options to give people a feel for what things can look like. Target Audience Any developer looking to bootstrap their projects can use this! What makes it really helpful is that you get a modern, high quality project with tools already configured before you even write a single line of code. Even if all you're doing is creating a POC this template can help you do it with style. Comparison Although there are other templates out there, this project is unique in a few ways: It is extremely modern, using tools such as Ruff and UV while also removing legacy fluff such as setup.py . It isn't targeted towards a specific type of software (library, cli, app) but can be used for a variety of project types, or even for projects with multiple entry points. It uses Cookiecutter, rather than direct git forking, to fill in fields and customize the project so you can get started right away after running it. Try it now! If you haven't installed Cookiecutter yet it's pretty easy on most platforms. With homebrew, for instance, just run brew install cookiecutter . From there- cookiecutter gh:tedivm/robs_awesome_python_template"},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "sew: SQLite Extensions and Wrappers", "Author": "u/Ok-Matter9741", "Content": "https://github.com/icyveins7/sew Motivation This project started because at work I realised I was copying / editing code to create tables / insert rows etc in many different scripts. Oftentimes the structure of the tables, and the way I inserted the data, was the same; just inserting all columns. So at first I started auto-generating my CREATE TABLE and INSERT statements, but then it eventually grew into this. What This Project Does The goal of this package is essentially to reduce the code typed for simple CRUD operations. I pretty much only use this now because I can always dip down to make custom statements if I want to anyway (which still happens for more complex things like inner joins). Here's an example: Before: import sqlite3 as sq con = sq.connect(\"path/to/db\") cur = con.cursor() cur.execute(\"select * from mytable where ...\") results = cur.fetchall() After: import sew db = sew.Database(\"path/to/db\") db['mytable'].select(\"*\", ...) results = db.fetchall() Target Audience This is really for anyone who uses SQLite from python‚Äôs in built library. Hopefully someone else will find this useful; it‚Äôs my first ever package uploaded to PyPI so things may not be as crisp as they should be, but constructive feedback is welcome. Otherwise, ask me anything here! Comparisons I think there are other libraries out there that attempt to optimize database interaction speed but none that I found that do what I described above. Might be wrong though."},
{"Title": "vmn:  git tag based versioning system", "Author": "u/ArmGroundbreaking809", "Content": "https://github.com/final-israel/vmn Hello r/Python community! I‚Äôm excited to share a project I‚Äôve developed called vmn. It‚Äôs a git tag based versioning system that we‚Äôve been using extensively at my company to manage our application versions. Here‚Äôs a quick overview: What vmn Does: vmn is a tool that simplifies version management by leveraging git tags. It‚Äôs designed to automate and streamline the process of versioning your applications, making it easier to maintain a clear and consistent version history. Target Audience: This project is aimed at developers and teams who use git for version control and are looking for a straightforward way to manage their application versions. It‚Äôs suitable for both production use and development environments. Comparison: Unlike other versioning systems that might rely on manual updates or complex configurations, vmn keeps things simple and git-centric. It‚Äôs a lightweight and easy-to-use solution that integrates seamlessly with your existing git workflow. I would LOVE to hear your feedback and suggestions! If you have any ideas for features or improvements, I‚Äôm open to adding them. Check it out and give it a star ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è ThanksüòÅ"},
{"Title": "Valid uses of eval()?", "Author": "u/not_dmr", "Content": "I‚Äôm wondering if anyone has ever seen a case of code using eval() and thought to themselves ‚Äúyeah actually that‚Äôs probably the right way to do it‚Äù? My understanding has always been that it‚Äôs a huge security risk and generally a recipe for disaster. But I was just working on a task where I couldn‚Äôt really figure out any other way to achieve the dynamic functionality I was looking for, so I wrote code that assembles a string to do what I need, and then runs eval() on that string. Pretty sure this is the first time I‚Äôve ever used eval() at all. It‚Äôs a low-stakes proof of concept for a totally internal tool, so I‚Äôm not hugely worried about security at the moment, but it just feels so icky to do something like that. I‚Äôm curious if in others‚Äô experience there‚Äôs always a better way than using eval() , or if sometimes it‚Äôs legit."},
{"Title": "Text-to-speech with python", "Author": "u/Crafty-Wheel2068", "Content": "Greetings pythonistas, I'm thrilled to share with you my latest article which delves into the amazing python package, pyttsx3. Explore the capabilities and how interesting the Text-to-Speech(TTS) package is Read the full blog post here: [ https://jeffmint.hashnode.dev/code-that-speaks-a-beginners-guide-to-pyttsx3-text-to-speech-tts-in-python ] If you find the content intriguing and would like to actively contribute to its development, I'm excited to announce that it's an open-source initiative! Feel free to explore the repository, and let's join forces to enhance and elevate the usage of the package together. Your collaboration and ideas are invaluable in making it even more remarkable. [ https://github.com/Minty-cyber/CodeThatSpeaks] Cheers to building something great¬†together!¬†üöÄ"},
{"Title": "The python accounting library that everyone has been waiting for", "Author": "u/afroblut", "Content": "Hello all, After having published a double entry accounting package in php a couple of years ago, I hereby present the python version to the community. Please take a look here , all and any feedback (and criticism) is most welcome. What My Project Does The library provides a fully featued double entry accounting backend with support for postgres and mysql, allowing you to develop python accounting applications that can produce finacial reports compatible with the IFRS and GAAP standards. Target Audience May be used for production/commercial use or just for writing a private bookkeeping system for your business Comparison Most other libraries either focus on personal finance or keeping a simple ledger with which one can keep track of expenses and incomes. This library represents a fully featured accounting software kernel that can handle the accounting needs of any size of business."},
{"Title": "üêÇ Oxen.ai 0.11.1 - Added python support for data diffs on csvs, parquet, jsonl etc", "Author": "u/FallMindless3563", "Content": "Hey Pythonistas, We are working on extending the Oxen.ai toolchain to support diffs of different data types. For example, quickly finding changes in large csv or parquet files. Oxen identifies the data schema and performs hashing under the hood to find changes in the rows and columns. Docs üëâ https://docs.oxen.ai/concepts/diffs Eventually we want to extend support to diffs of many data types (directories, images, videos, audio, etc). If you aren‚Äôt familiar with Oxen.ai it is an Open Source CLI, Server and Python Library for versioning and iterating on large machine learning datasets that would be too large for git. Source code üë®‚Äçüíª https://github.com/Oxen-AI/oxen-release The first two diff types we are supporting are a TextDiff and a TabularDiff . The TabularDiff is useful for finding changes in rows and columns of tabular datasets of csv, parquet, jsonl etc. The TextDiff is like your standard git diff that finds added and removed lines in a text file. We also rolled out a Web UI to help visualize the diffs beyond a simple text diff like GitHub. Check it out here: https://oxen.ai Let us know if you find these features helpful or if there are any other data types you think would be helpful. The next one on the roadmap is DirDiff that will summarize changes in large directories and sub directories of data. Contributors welcome! Best & Moo, The Oxen.ai Herd"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Ruff 0.3.0 - first stable version of ruff formatter", "Author": "u/WaterFromPotato", "Content": "Blog - https://astral.sh/blog/ruff-v0.3.0 Changes: - The Ruff 2024.2 style guide - Range Formatting - f-string placeholder formatting - Lint for invalid formatter suppression comments - Multiple new rules - both stable and in preview"},
{"Title": "Sieve of Eratosthenes (Haskell VS Python)", "Author": "u/Kiuhnm", "Content": "I've just come across an article where a Haskell, a Python, and a JS implementation of the Sieve of Eratosthenes are shown. Here's the Haskell one: primes = sieve [ 2.. ] where sieve (p:x) = p : sieve [ n | n <- x, n `mod` p > 0 ] And here's the Python one: def eratosthenes(n): sieve = [ True for i in range(n+1) ] def markOff(pv): for i in range(pv+pv, n+1, pv): sieve[i] = False markOff(2) for i in range(3, n+1): if sieve[i]: markOff(i) return [ i for i in range(1, n+1) if sieve[i] ] I don't understand why markOff(2) is singled out and not handled in the loop. Why compare a clever Haskell implementation with a stupid Python one? Here's my version: def eratosthenes(n): sieve = [True] * (n + 1) for i, x in enumerate(sieve): if i < 2 or not x: continue for j in range(2 * i, n + 1, i): sieve[j] = False return [i for i, x in enumerate(sieve) if x][2:] We can also start from 2: def eratosthenes2(n): sieve_from2 = [True] * (n - 1)      # for ints 2, 3, ..., n for i, x in enumerate(sieve_from2): if not x: continue for j in range(i + (i + 2), n - 1, i + 2): sieve_from2[j] = False return [i + 2 for i, x in enumerate(sieve_from2) if x] But next time someone shows you the Haskell code above, please show them this: def sieve(xs): yield (p := next(it := iter(xs))) yield from sieve(x for x in it if x % p > 0) primes = sieve(count(2)) We're just some syntactic sugar away from the Haskell implementation! This proves that we can if we want to. We just don't see the point in being that clever all the time :) EDIT: Here's the recursive version without using any division: def sieve2(xs): yield (p := next(it := dropwhile(lambda x: not x[1], xs))[0]) yield from sieve2((x, b and r != p - 1) for (x, b), r in zip(it, cycle(range(p)))) primes2 = sieve2(zip(count(2), repeat(True))) Not so elegant anymore..."},
{"Title": "The First Annual Meeting of the Extraordinary League of Runtime Typers", "Author": "u/monorepo", "Content": "Original Posts GitHub Discussion Python.org Discussion PEP 649 As a user of strong typing in Python, and maintainer of libraries that utilize them greatly with plans of further work on a runtime type-using library, I found this quite an interesting read. Basically seeks to bring up implications of Python 3.13's  updates on runtime type checkers via PEP 649... This comes from the maintainer of Beartype - if you aren't familiar with beartype 's documentation then it's worth a read even if you never use the library - a runtime-static type-checker. It brings in some highly thought after individuals in the Python typing community including the Anyio, Pydantic, Msgspec maintainers... certainly worth a read through if this (typing) interests you."},
{"Title": "Reflex 0.4.0 - Web Apps in Pure Python", "Author": "u/Boordman", "Content": "Hey everyone, we just released a new version of reflex and wanted to share some updates. For those who don‚Äôt know about Reflex (we used to be called Pynecone ), it‚Äôs a framework to build web apps in pure Python. We wanted to make it easy for Python developers to share their ideas without having to use Javascript and traditional frontend tools, while still being as flexible enough to create any type of web app. Since our last post, we‚Äôve made many improvements including: We‚Äôve released our hosting service . Just type reflex deploy and we will set up your app, and give you a URL back to share with others. During our alpha we‚Äôre giving free hosting for all apps (and always plan to have a free tier). A tutorial on building a ChatGPT clone using Reflex. See the final app https://chat.reflex.run New core components based on Radix UI, with a unified theming system. More guides on how to wrap custom React components. We‚Äôre working now on building out our 3rd party component ecosystem. Our key focuses going forward are on making the framework stable, speed improvements, and growing out the ecosystem of 3rd party components. We‚Äôve published our roadmap here . Let us know what you think - we‚Äôre fully open source and welcome contributions! We also have a Reddit where we post updates: https://www.reddit.com/r/reflex/"},
{"Title": "Cadwyn: the most sophisticated Python API Versioning framework", "Author": "u/Varabe", "Content": "What My Project Does Cadwyn allows you to support a single version of your code while auto-generating the schemas and routes for older versions. You keep REST API versioning encapsulated in small and independent \"version change\" modules while your business logic stays simple and knows nothing about versioning. It is heavily inspired by Stripe 's approach to API versioning but it is much more sophisticated and allows you to reach the true \"zero-duplication\" versioning. We have recently discussed it on TalkPython podcast. Target audience Cadwyn is made for FastAPI so any FastAPI developer could benefit from it immediately if they need versioning. However, Cadwyn is also a huge case study: its documentation contains a lot of guides and research on how to build any kind of API Versioning and Cadwyn in itself can serve as a guide for making it in another web framework or language. Comparison There does not exist any tool (especially open-source and especially in python) for API versioning of this quality and scalability. GitHub https://github.com/zmievsa/cadwyn"},
{"Title": "Anaconda 2024-02 Release Notes", "Author": "u/PhilipYip", "Content": "Anaconda 2024-02 is released: python 3.11.7 conda 24.1.2 numpy 1.26.3 pandas 2.1.4 matplotlib 3.8.0 jupyterlab 4.0.11 spyder 5.4.3 The JupyterLab Variable Inspector extension is also preinstalled. Anaconda Release Notes Miniconda 24.1.2-0 (February 27, 2024) is released: Python 3.12.1 conda 24.1.2 Miniconda Release Notes The new version of conda uses the libmamba solver by default which has a number of performance improvements over the classic solver. The conda team is pleased to announce the availability of ‚Äòlibmamba‚Äô as a new, much faster dependency solver for conda! Three different companies worked to make this release possible: QuantStack, developing mamba and libmamba; Quansight, integrating libmamba into conda; and Anaconda, developing conda and managing the overall effort. A faster conda We are changing the default solver of conda to conda-libmamba-solverFirst, complex solves will run noticeably faster. Benchmarking predicts a 50 to 80% improvement in run times. Second, you will notice improved error messages when conda encounters problems. libmamba's error messages give you more insight into what is triggering the problem. Third, conda-libmamba-solver outputs more details about the channels in use and target platform at the beginning of the process. Mind these differences if you are parsing stdout (although we definitely recommend enabling the --json mode for programmatic usage!). conda 23.10.0+ uses libmamba by default Originally libmamba was supposed to be rolled out for the 2023-09 release but this came with conda=23.7.4 which included the libmamba solver but wasn't enabled by default: conda libmamba rollout"},
{"Title": "Sitcom Simulator: a tool for generating weird AI meme videos", "Author": "u/ricekrispysawdust", "Content": "What My Project Does Sitcom Simulator is a python/CLI tool that takes any text prompt (for example, \"Joe Biden vs. Donald Trump: ultimate weightlifting competition\") and turns it into a bizarre meme video with realistic images and voices. The tool is essentially duct tape that combines various AI tools into one unholy abomination: ChatGPT generates the video script. FakeYou generates voices for the characters. Stable Diffusion generates images of the characters. Freepd provides the background music. FFmpeg connects the images and voices into a movie. Target Audience People who like making memes, funny videos, or weird AI \"art\" but are too lazy to do it manually. The code is fairly customizable and extendable; it could probably be used as a base for many types of AI video generation projects even if your use case is somewhat different. Comparison There are many AI video editing tools out there (e.g., Kapwing), almost all of which are complicated commercial products with a vague notion of improving \"productivity\" or whatever. In contrast, Sitcom Simulator is simple, open source, and the only AI video tool focused on humor, memes, and wasting your time with terrible brain rot. GitHub Code, documentation, and example videos can all be found on GitHub: https://github.com/joshmoody24/sitcom-simulator"},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Framework-agnostic RPC API with auto-generated TypeScript client", "Author": "u/westandskif", "Content": "Idea The below must be enough to define an API: class UserParams(pydantic.BaseModel): uid: str class GetUsers(AbstractProcedure): def call(self, in_: UserParams, context) -> List[UserDetails]: ... to then do the following somewhere in your TypeScript code: import { callGetUsers } from \"./src/out\"; expect(callGetUsers(userParams).$promise).resolves.toEqual(listOfUserDetails); Desired benefits Automated typescript client generation Of course, it's possible to annotate your API, export an OpenAPI schema and generate a typescript client from it. I just want it out of the box. Browser Dates done right Javascript doesn't have a separate date type, so it uses Date for both python's date and datetime. Hence when you pass 2000-01-01 to a browser in New York, the browser will read it as UTC datetime and then convert it to the local timezone, so it will give you Dec 31, 1991 7:00PM, which is fine if you wanted to work with a particular moment in time, but what if you wanted to display someone's date of birth? That's why lacking the date type is a problem. The library should see that you want to pass python's date to the browser and automatically prepare it in the browser, so that  Jan 1st is still Jan 1st. Browser friendly types only synclane raises an exception if you use types, which browser won't be able to understand. No need to define URLs A procedure name should be enough. e.g. you name it AddUser, you should be able to just run callAddUser function in the typescript client. I don't want to define any other identifier like path per API endpoint. Enums If your procedure in/out types include enums, they should become available in the typescript client. What My Project Does It does all the above: checks types, handles RPC requests, generates TypeScript client code, including all the above benefits. Target Audience It is in currently in beta. If it doesn't require breaking changes within next 6-12 months, I'll make it stable. Comparison It is like FastAPI + Pydantic, but with: out of the box TypeScript client generation benefits described above should be easily embeddable to any framework, because it only requires a single API endpoint (all the routing happens inside of RPC) Links docs & examples: https://synclane.readthedocs.io/en/latest/ github: https://github.com/westandskif/synclane pypi: https://pypi.org/project/synclane/ What do you think? Would you use it? Any feedback is much appreciated!"},
{"Title": "üßë‚Äçüíª Open Interface - Operate your Computer Using LLMs - Built completely in Python", "Author": "u/reasonableWiseguy", "Content": "What My Project Does Open Interface is a desktop app (MacOS/Windows/Linux) that can control your mouse and keyboard to perform any human-computer task you ask of it by using LLMs as a backend to break down the steps required. Target Audience Since most LLMs are not production reliable, this is still a toy project but anyone can use it. Comparison I haven't much looked at alternatives since this is an application not a library. Libraries used - Tkinter - PyAutoGUI - Pyinstaller - The official OpenAI module Github : https://github.com/AmberSahdev/Open-Interface/ Demo : https://github.com/AmberSahdev/Open-Interface/?tab=readme-ov-file#demo Install : https://github.com/AmberSahdev/Open-Interface/?tab=readme-ov-file#install"},
{"Title": "Contribute to Python cybersecurity projects in GSoC 2024!", "Author": "u/stratospherelab", "Content": "Hello everyone, We're happy to announce that our Stratosphere Laboratory has been accepted as a mentor organization in the Google Summer of Code 2024! Projects you can contribute to are our Intrusion Prevention System and our AI VPN: https://github.com/stratosphereips/StratosphereLinuxIPS https://github.com/stratosphereips/AIVPN Check out our ideas list here: https://github.com/stratosphereips/Google-Summer-of-Code/blob/main/list_of_ideas.md If you think you have the required skills to contribute, you can check the contributor guide here: https://github.com/stratosphereips/Google-Summer-of-Code/blob/main/contributor_guide.md You can join our discord if you have more questions https://discord.gg/zu5HwMFy5C Let's make this summer a memorable one!  üöÄ üöÄ"},
{"Title": "I made a YouTube downloader with Modern UI | PyQt6 | PyTube | Fluent Design", "Author": "u/Specialist-Arachnid6", "Content": "What my Project Does? Youtility helps you to download YouTube content locally. With Youtility, you can download: Single videos with captions file Playlists (also as audio-only files) Video to Mp3 Target Audience People who want to save YouTube playlists/videos locally who don't wanna use command line tools like PyTube. Comparison Unlike existing alternatives, Youtility helps you to download even an entire playlist as audio files. It can also download XML captions for you. Plus, it also has a great UI. GitHub GitHub Link: https://github.com/rohankishore/Youtility"},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "I hate typing out every 'self.x = x' line in an __init__ method. Is this alternative acceptable?", "Author": "u/MomICantPauseReddit", "Content": "class Movable: def __init__(self, x, y, dx, dy, worldwidth, worldheight): \"\"\"automatically sets the given arguments. Can be reused with any class that has an order of named args.\"\"\" nonmembers = [] #populate with names that should not become members and will be used later. In many simple classes, this can be left empty. for key, value in list(locals().items())[1:]: #exclude 'self', which is the first entry. if not key in nonmembers: setattr(self, key, value) #handle all nonmembers and assign other members: return I always hate how redundant and bothersome it is to type \"self.member = member\" 10+ times, and this code does work the way I want it to. It's pretty readable in my opinion, especially with the documentation. That aside, is it considered acceptable practice in python? Will other developers get annoyed if I use it? Edit:  Thanks for the very fast replies. Data classes it is! I meant for this to be a discussion of code conventions, but since I learned about a completely new feature to me, I guess this post belongs in r/learpython ."},
{"Title": "shavis - Visualize SHA256 and SHA1", "Author": "u/perfectminimalism", "Content": "https://github.com/kernel137/shavis Install with pip install shavis shavis is (secure hash algorithm visualization), is a CLI tool that can hash any file (Only through SHA256) and create an 8x8 pixel image displaying the hash through themed colours. What My Project Does : Main functionality of this CLI tool is taking the 64 digit hex number (SHA256 hash) and, through 16 hex colors in ascending order, turning it into a 8x8 pixel image that can then be scaled to powers of two. Shavis also supports piping: echo -n \"Hello World!\" | shavis This also works for git hashes, since git hashes are SHA1, images for this hash are 8x5, I think with some clever integration, its possible to integrate a small res picture of a git commit hash  to quickly visualize the hash within vscode or some other platform. A quick way to get a visual of the last git commit while within a local git repository is: git rev-parse HEAD | shavis -g What do you guys think? Is it a worthwhile investment to implement other hashes? Is it an interesting project? Do you guys have any ideas about what to add to it?"},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "An extremely modern and configurable Python project template", "Author": "u/tedivm", "Content": "What My Project Does Rob's Awesome Python Template is a cookiecutter template meant to bootstrap python projects using modern best practices. At the very basic level it includes: Modern pyproject.toml without any legacy files (no setup.py or setup.cfg). Development Management using Makefiles . Configuration Management with Pydantic . PyPI Publishing from Git Tags using setuptools-scm . Formatting and Linting with Ruff . Typing with mypy . Lockfiles (requirements.txt, requirements-dev.txt) with uv . Testing with pytest . CI/CD using Github Actions . Precommit Hooks using the precommit framework . It also has a ton of optional features: Github Actions for CI Cross Platform (arm, arm64, amd64) Docker containers using the Multi-Py project. Optionally use any combination of FastAPI, Click/Typer, Celery, and Sqlalchemy. I've used this template for a number of projects- QuasiQueue , Paracelsus , and Fedimapper being some nice examples. If you want to see exactly what the project would generate today you can review the examples repository which builds a few projects using different options to give people a feel for what things can look like. Target Audience Any developer looking to bootstrap their projects can use this! What makes it really helpful is that you get a modern, high quality project with tools already configured before you even write a single line of code. Even if all you're doing is creating a POC this template can help you do it with style. Comparison Although there are other templates out there, this project is unique in a few ways: It is extremely modern, using tools such as Ruff and UV while also removing legacy fluff such as setup.py . It isn't targeted towards a specific type of software (library, cli, app) but can be used for a variety of project types, or even for projects with multiple entry points. It uses Cookiecutter, rather than direct git forking, to fill in fields and customize the project so you can get started right away after running it. Try it now! If you haven't installed Cookiecutter yet it's pretty easy on most platforms. With homebrew, for instance, just run brew install cookiecutter . From there- cookiecutter gh:tedivm/robs_awesome_python_template"},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "sew: SQLite Extensions and Wrappers", "Author": "u/Ok-Matter9741", "Content": "https://github.com/icyveins7/sew Motivation This project started because at work I realised I was copying / editing code to create tables / insert rows etc in many different scripts. Oftentimes the structure of the tables, and the way I inserted the data, was the same; just inserting all columns. So at first I started auto-generating my CREATE TABLE and INSERT statements, but then it eventually grew into this. What This Project Does The goal of this package is essentially to reduce the code typed for simple CRUD operations. I pretty much only use this now because I can always dip down to make custom statements if I want to anyway (which still happens for more complex things like inner joins). Here's an example: Before: import sqlite3 as sq con = sq.connect(\"path/to/db\") cur = con.cursor() cur.execute(\"select * from mytable where ...\") results = cur.fetchall() After: import sew db = sew.Database(\"path/to/db\") db['mytable'].select(\"*\", ...) results = db.fetchall() Target Audience This is really for anyone who uses SQLite from python‚Äôs in built library. Hopefully someone else will find this useful; it‚Äôs my first ever package uploaded to PyPI so things may not be as crisp as they should be, but constructive feedback is welcome. Otherwise, ask me anything here! Comparisons I think there are other libraries out there that attempt to optimize database interaction speed but none that I found that do what I described above. Might be wrong though."},
{"Title": "vmn:  git tag based versioning system", "Author": "u/ArmGroundbreaking809", "Content": "https://github.com/final-israel/vmn Hello r/Python community! I‚Äôm excited to share a project I‚Äôve developed called vmn. It‚Äôs a git tag based versioning system that we‚Äôve been using extensively at my company to manage our application versions. Here‚Äôs a quick overview: What vmn Does: vmn is a tool that simplifies version management by leveraging git tags. It‚Äôs designed to automate and streamline the process of versioning your applications, making it easier to maintain a clear and consistent version history. Target Audience: This project is aimed at developers and teams who use git for version control and are looking for a straightforward way to manage their application versions. It‚Äôs suitable for both production use and development environments. Comparison: Unlike other versioning systems that might rely on manual updates or complex configurations, vmn keeps things simple and git-centric. It‚Äôs a lightweight and easy-to-use solution that integrates seamlessly with your existing git workflow. I would LOVE to hear your feedback and suggestions! If you have any ideas for features or improvements, I‚Äôm open to adding them. Check it out and give it a star ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è ThanksüòÅ"},
{"Title": "Valid uses of eval()?", "Author": "u/not_dmr", "Content": "I‚Äôm wondering if anyone has ever seen a case of code using eval() and thought to themselves ‚Äúyeah actually that‚Äôs probably the right way to do it‚Äù? My understanding has always been that it‚Äôs a huge security risk and generally a recipe for disaster. But I was just working on a task where I couldn‚Äôt really figure out any other way to achieve the dynamic functionality I was looking for, so I wrote code that assembles a string to do what I need, and then runs eval() on that string. Pretty sure this is the first time I‚Äôve ever used eval() at all. It‚Äôs a low-stakes proof of concept for a totally internal tool, so I‚Äôm not hugely worried about security at the moment, but it just feels so icky to do something like that. I‚Äôm curious if in others‚Äô experience there‚Äôs always a better way than using eval() , or if sometimes it‚Äôs legit."},
{"Title": "Text-to-speech with python", "Author": "u/Crafty-Wheel2068", "Content": "Greetings pythonistas, I'm thrilled to share with you my latest article which delves into the amazing python package, pyttsx3. Explore the capabilities and how interesting the Text-to-Speech(TTS) package is Read the full blog post here: [ https://jeffmint.hashnode.dev/code-that-speaks-a-beginners-guide-to-pyttsx3-text-to-speech-tts-in-python ] If you find the content intriguing and would like to actively contribute to its development, I'm excited to announce that it's an open-source initiative! Feel free to explore the repository, and let's join forces to enhance and elevate the usage of the package together. Your collaboration and ideas are invaluable in making it even more remarkable. [ https://github.com/Minty-cyber/CodeThatSpeaks] Cheers to building something great¬†together!¬†üöÄ"},
{"Title": "The python accounting library that everyone has been waiting for", "Author": "u/afroblut", "Content": "Hello all, After having published a double entry accounting package in php a couple of years ago, I hereby present the python version to the community. Please take a look here , all and any feedback (and criticism) is most welcome. What My Project Does The library provides a fully featued double entry accounting backend with support for postgres and mysql, allowing you to develop python accounting applications that can produce finacial reports compatible with the IFRS and GAAP standards. Target Audience May be used for production/commercial use or just for writing a private bookkeeping system for your business Comparison Most other libraries either focus on personal finance or keeping a simple ledger with which one can keep track of expenses and incomes. This library represents a fully featured accounting software kernel that can handle the accounting needs of any size of business."},
{"Title": "üêÇ Oxen.ai 0.11.1 - Added python support for data diffs on csvs, parquet, jsonl etc", "Author": "u/FallMindless3563", "Content": "Hey Pythonistas, We are working on extending the Oxen.ai toolchain to support diffs of different data types. For example, quickly finding changes in large csv or parquet files. Oxen identifies the data schema and performs hashing under the hood to find changes in the rows and columns. Docs üëâ https://docs.oxen.ai/concepts/diffs Eventually we want to extend support to diffs of many data types (directories, images, videos, audio, etc). If you aren‚Äôt familiar with Oxen.ai it is an Open Source CLI, Server and Python Library for versioning and iterating on large machine learning datasets that would be too large for git. Source code üë®‚Äçüíª https://github.com/Oxen-AI/oxen-release The first two diff types we are supporting are a TextDiff and a TabularDiff . The TabularDiff is useful for finding changes in rows and columns of tabular datasets of csv, parquet, jsonl etc. The TextDiff is like your standard git diff that finds added and removed lines in a text file. We also rolled out a Web UI to help visualize the diffs beyond a simple text diff like GitHub. Check it out here: https://oxen.ai Let us know if you find these features helpful or if there are any other data types you think would be helpful. The next one on the roadmap is DirDiff that will summarize changes in large directories and sub directories of data. Contributors welcome! Best & Moo, The Oxen.ai Herd"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Ruff 0.3.0 - first stable version of ruff formatter", "Author": "u/WaterFromPotato", "Content": "Blog - https://astral.sh/blog/ruff-v0.3.0 Changes: - The Ruff 2024.2 style guide - Range Formatting - f-string placeholder formatting - Lint for invalid formatter suppression comments - Multiple new rules - both stable and in preview"},
{"Title": "Some of my thoughts about virtual environment tools", "Author": "u/KingsmanVince", "Content": "This document provides some fundamental and comparisons about virtual environment tools. The readers are expected to use one of similar tools once. If not, the readers should read 12. Virtual Environments and Packages ‚Äî Python 3.12.2 documentation . Some concepts might be oversimplified. The fundamental Every beginner tutorial about creating virtual environment starts with venv or virtualenv . venv is a standard module for creating lightweight environment. virtualenv is a tool to create isolated environments. Since Python 3.3 , a subset of it has been integrated into the standard library under the venv module. Eventually, the developers build management tools. virtualenvwrapper is a useful set of scripts for creating and deleting virtual environments. pyenv is a tool to install, isolate, manage , Python versions per user and per project. pyenv-virtualenv is a plugin of pyenv to manage virtualenv . pipenv is a Python virtualenv management tool that supports a multitude of systems and nicely bridges the gaps between pip , python (using system python, pyenv or asdf) and virtualenv . The combinations Black: virtual environment creation tools Red: virtual environment management tools Green: Python versions management tools Here is the image Here are some of the tool sets that people commonly use. The comparisons virtualenvwrapper pipenv pyenv pyenv pyenv-virtualenv Manage multiple Python versions No Yes Yes Manage per project No, you can have many venv per project and many projects per venv Yes No, like virtualenvwrapper. The holdup The readers may ask what conda / mamaba and poetry are all about. Conda-related conda is a system-level, binary package and environment manager running on all major operating systems and platforms. mamba is conda implemented in c++. Many people might be more familiar with miniconda , micromamba which are minimal CLI versions of aforementioned tools. Conda is similar to pyenv pyenv-virtualenv but not limited to Python. One can install C++ packages such as Nvidia drivers. Important use case: people often use this for machine learning projects. Poetry poetry is a tool for dependency management and packaging in Python. It allows you to declare the libraries your project depends on and it will manage (install/update) them for you. Poetry offers a lockfile to ensure repeatable installs, and can build your project for distribution. Poetry either uses your configured virtualenvs or creates its own to always be isolated from your system. Reading list python - What is the difference between venv, pyvenv, pyenv, virtualenv, virtualenvwrapper, pipenv, etc? - Stack Overflow Python Virtual Environments: A Primer ‚Äì Real Python References venv ‚Äî Creation of virtual environments ‚Äî Python 3.12.2 documentation virtualenv (pypa.io) virtualenvwrapper 6.0.1.dev12+gb1aa495 ‚Äî virtualenvwrapper 6.0.1.dev12+gb1aa495 documentation https://github.com/pyenv/pyenv https://github.com/pyenv/pyenv-virtualenv https://github.com/pypa/pipenv Miniconda ‚Äî Anaconda documentation Micromamba Installation ‚Äî documentation Introduction | Documentation | Poetry - Python dependency management and packaging made easy (python-poetry.org)"},
{"Title": "Hatchet - a Celery replacement focused on scale and observability", "Author": "u/hatchet-dev", "Content": "Hey everyone - really excited to showcase Hatchet, an OSS project I've been working on for the past few months: https://github.com/hatchet-dev/hatchet What My Project Does Hatchet is a Celery alternative built for scale and observability. Specifically, it supports: Low Latency and High Throughput Scheduling: Hatchet is built on a low-latency queue (25ms average start) so it can support real-time user interaction even while running from an async worker. Concurrency, Fairness, and Rate Limiting: implements FIFO, LIFO, Round Robin, and Priority Queues with built-in strategies for limiting concurrency. DAG workflows: Hatchet lets you declare tasks which are dependent on the execution of other tasks for full DAG-style execution. Durability and error handling: all events and executions are persisted to Postgres. When a worker fails, tasks automatically get reassigned to new workers, and the workflow will pick up where it left off. Web UI and API for observability: visualize events, logs and workflows within the dashboard. Workflow replay: replay tasks, workflows, and events right from the UI or via the API. Target Audience This is being actively used in production at 5 companies, the largest of which is executing 50k tasks per day. It's ideal for companies looking to scale their async tasks or want more visibility into workflow progression. Comparison Celery is a clear alternative - and while it's a great framework, there are a few reasons to favor Hatchet: - Better observability - I've spent a lot of time in the Celery Flower UI or building Grafana views for exported prom metrics. We wanted to build a modern, dev-friendly dashboard. There's still a long way to go, but a core focus of the platform is dev experience after deployment to production. - Postgres-backed - when I started to build this, I wanted a transactional database that's easy to horizontally scale and can handle high volumes of writes. We are working on batching execution data and forwarding it to Clickhouse, so Postgres is a natural choice. - Networking - when deploying with Celery, each worker manages its own connection to the underlying broker using redis or amqp . With Hatchet, each worker connects via a long-lived gRPC connection, which makes it easier to distribute workers across different networks or clusters, as there is widespread support for proxying HTTP 2.0 rather than forwarding TLS which can get tricky. FAQs How can I get started? Here's a quickstart repository for Python. This is written in Go, are you in the wrong place? After the original engine was deployed, we spent the next month building our Python SDK. I made the decision to use a lower-level language for more control over the underlying engine runtime which makes it easier to optimize latency. What's next on the roadmap? Support for logging from a task execution and giving a great logs API + view on our dashboard. This also can be difficult to integrate well with Celery AFAIK. Feedback Would love to hear what you think - also feel free to join our Discord and share your thoughts there."},
{"Title": "Use Firebase with Tkinter (Free Database & Authentication)", "Author": "u/TurbineThree", "Content": "Hey, heres a tutorial I made that shows how to use Firebase with Tkinter to create a simple application that has authentication and which uses the Firestore database: https://youtu.be/hxuiRPUJMwU?si=QF0zevMvOmq6BN1K Using Firebase is much easier than setting up your own backend and you can start using it for free!"},
{"Title": "Real-time ingestion pipeline for RAG app in Python", "Author": "u/oli_k", "Content": "Hi, I am working on a stream processing framework called Bytewax and I recently gave a talk where I read live data from HackerNews API, cleaned it with unstructured.io, created embeddings with a hugging face model, and stored everything in Milvus. I find a lot of devs are struggling with real-time streaming. The steps are easy to follow, and it is an end-to-end Python: https://github.com/bytewax/real-time-milvus I also added diagrams and summarized it in a blog https://bytewax.io/blog/stream-process-embed-repeat"},
{"Title": "BlendSQL: Unified dialect for orchestrating SQLite logic and LLM reasoning", "Author": "u/parkervg5", "Content": "https://github.com/parkervg/blendsql What my Project Does BlendSQL is a superset of SQLite for blending together complex reasoning between vanilla SQL and LLM calls. It's got a bunch of optimizations implemented using the amazing pure Python parser sqlglot to minimize the number of external LLM calls made. It allows you to efficiently perform tasks ranging from simple table question-answering: {{ LLMQA( 'Give me a short summary of my spending.', context=(SELECT * FROM spending) ) }} ...to complex question-answering with unstructured data using constrained decoding: (BlendSQL corresponding to 'List the term lengths of all U.S. presidents from the 3rd most populous city in Ohio') SELECT {{LLMMap('How long did they serve?', 'presidents::term')}} FROM presidents WHERE birthplace = {{ LLMQA( 'Which city is the 3rd most populous city in Ohio?', (SELECT * FROM documents WHERE documents MATCH 'ohio'), options='presidents::birthplace' ) }} Target Audience BlendSQL can be used in a couple ways - by directly writing the queries yourself as you would normal SQL to operate over a database, or as an intermediate representation that is taught to a LLM via few-shot learning . In our research, we show that using BlendSQL as an intermediate representation for complex question answering tasks boosts performance over the traditional end-to-end approach while using substantially fewer tokens. Comparison While other existing tools enable text-to-SQL using LLMs to write queries, this tool enables the user to oversee all calls (LLM + SQL) within a unified query language. Appreciate any feedback folks have! Thanks."},
{"Title": "Python code to fit data to a sum of exponentials with arbitrary number of terms", "Author": "u/juangburgos", "Content": "I used jupyterlite, pyodide and the PyArma module to implement the method described in this stackexchange question . The jupyterlite site with the notebooks is: http://juangburgos.github.io/FitSumExponentials/"},
{"Title": "A Discord Bot to converse with documentation using GPT-4 + RAG", "Author": "u/import-username-as-u", "Content": "What my project does This project is a Discord bot that lets you talk with document collections stored in the Qdrant vector database. The project comes with a Discord bot written using discord.py, and a backend written in FastAPI. I work for Hasura as a Community Engineer and we are in the Developer tooling space and so we get lots of support requests from developers. I built this bot to help answer questions that should be covered in our technical documentation. There is a blog post written in the README, and setup instructions in SETUP.md. This bot lives in our Discord server and automatically responds to requests in our help forum. Target Audience We are running this in production, and you probably could too. I won‚Äôt say it‚Äôs perfect, but it would be a really great project for someone wanting to get started with event-driven programming, interested in building a Discord bot, or who was curious about Hasura. I would definitely say this is something you want to be relatively comfortable with Python for it to be approachable. It uses a decent handful of some of my favorite python libraries like FastAPI, selenium, beautifulsoup4, and others. Comparison There are lots of chatbots coming out right now, and some might be better than this. This isn‚Äôt anything more than a simple case of RAG. The specific use case we wanted was a bot that would automatically try to answer help-forum questions and provide links to documentation. I‚Äôm in a handful of programming related Discord servers, and but just because I haven‚Äôt seen a bot like this in the ones I am in doesn‚Äôt mean there aren‚Äôt others. I think what‚Äôs maybe most useful is how easy it is to run this yourself because of the Docker-compose."},
{"Title": "pyCage: A VSCode extension to search and install popular Python packages from the command palette.", "Author": "u/KitN_X", "Content": "What Does My Project Do? pyCage is a VSCode extension powered by astral.sh's uv package manager and pip. It enables users to search for and download Python packages directly from the command palette, similar to Dart's package manager. Target Audience This extension is designed for individuals who frequently use venvs and struggle with remembering package names. Comparison The installation speed is notably fast due to the option to download using uv.It might be faster to download packages using pyCage than using the terminal in some cases. GitHub GitHub Link: https://github.com/qKitNp/pyCage"},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "MuAlphaZeroLibrary: simple usage and training of MuZero and AlphaZero type models", "Author": "u/Skirlaxx", "Content": "A few days ago I published the first version of my python library for simple usage and training of DeepMind's AlphaZero and MuZero algorithms. The library is designed for easy usage and extensibility, allowing you to define custom environments, memory buffers, networks, etc. while also providing default implementations for all of these. What my project does Allows for simple training and usage of the MuZero and AlphaZero models with focus on extensibility. Target audience Human-like Inhabitants of the planet earth. Comparison There are a couple of projects implementing one of the algorithms, but my project offers both algorithms in one easy to use library. Github link : https://github.com/Skirlax/MuAlphaZeroLibrary"},
{"Title": "I made a CV Generator using PyQt6 and Python | Modern Design", "Author": "u/Specialist-Arachnid6", "Content": "What my Project Does? cvGen is a CV generator made using PyQt6 and Python. It collects user data via inputs given by the user and then crafts a beautiful CV using built-in HTML and CSS templates. Target Audience People who want to make good looking CVs without using any command line tools / YAML formatting manually. Comparison For comparison, I don't know what to compare it to since there are no other UI-based CV generators. If you know any,  please let me know. GitHub GitHub Link: https://github.com/rohankishore/cvGen Known Issues Only one template as of now (looking forward to expanding it) Only support input via the UI (coding support for JSON/YAML)"},
{"Title": "Automate creating a virtual environment and upgrading pip [tutorial]", "Author": "u/Responsible_Rip_4365", "Content": "I've been building a lot of projects lately and got tired of upgrading pip each time I create a new venv (virtual environment), so I decided to look up solutions and found that I can use a script to run the steps I usually do manually. This is obviously not something new or that necessary but thought it would be fun and save a few seconds especially when am building new projects everyday. I created a short tutorial on how to do this: https://youtu.be/xMDh4TYoIB4?si=iEziqP5YQFj2wIHW"},
{"Title": "Starter template for Cython", "Author": "u/joegeezer", "Content": "Hi guys I made a starter template repo for creating Python bindings from a C library.. maybe someone else might find this useful - https://github.com/joegasewicz/cython-starter-template thanks for looking"},
{"Title": "Pandopt, basicaly pandas, but optimized", "Author": "u/Fun-Coffee-6390", "Content": "Pandas is Great, But... Pandas excels when your dataset is manageable in size, and you adhere strictly to its built-in functions. However, when either of these conditions is not met, performance drastically declines. Relying solely on custom, albeit user-friendly, built-in functions for decent performance seems suboptimal. Despite this, for many operations, learning an array of built-ins isn't necessary; you can accomplish a great deal using just a few key methods like apply , aggregate , groupby , and rolling (not addressing I/O here). However, the apply is known to be one of the worst in terms of performance, that's why I wanted to make it the best..! Nonetheless, pandas shines for several reasons, including its user-friendly API and widespread adoption. Why I'm Skeptical of Alternatives In the quest for better performance, you might consider options like Modin or Polars. However, my experience with them has been mixed: they either introduce a steep learning curve or fail to function in 90% of the environments I've tested. The Essence of Pandopt Pandopt aims to enhance basic pandas operations, such as apply and rolling , by implementing dynamic code transformation and JIT compilation via Numba. This approach significantly boosts performance without altering other aspects of pandas functionality. Target Audience This message is intended for data science enthusiasts, optimization fans, and those who appreciate elegant coding. Performance Comparison Pandopt dramatically outperforms pandas in executing apply functions on large datasets (over 1 million rows), offering speed improvements of approximately 10,000 to 20,000 times post-initial compilation (which is cached). Even for standard operations like sum , it outpaces numpy or Polars by roughly 8 times. While gains in rolling functions are less pronounced, Pandopt still enhances flexibility, allowing the application of custom functions across multiple columns and functions natively. Project is public on git and available on Pypi even if unstable: https://github.com/remigenet/pandopt"},
{"Title": "hotpdf: Fast PDF Text Search and Extraction Library", "Author": "u/Free_Let_8315", "Content": "While working at my current company, I developed a PDF text search and extraction library based on pdfminer.six. We open-sourced the library, and would love to know if this could help you! :) What My Project Does: hotpdf was developed as an alternative to libraries like pdfquery with a focus on text search and extraction. The project was built to be memory efficient and have fast text searching and extraction on PDF documents. Target Audience: If you work with PDFs, especially performing text operations search and extraction by bounding box, then this library is for you. In my current company, we use it for this exact use case. For example: We want to extract the value associated with IBAN: in a document. We search for the \"IBAN\" and extract text around the surrounding coordinates. Comparison : For search, it implements a Trie, and for coordinate-based extraction, it implements a Sparse Matrix. These 2 data structures are created on each page. This helps the loading of PDFs into the memory fast. And since every page has its data structure implementation, it's easy to parallelize the whole process and merge the objects into one. Against pdfquery , it's around 5-8x times faster and uses 5-10x less memory. All benchmarks were performed on test files that are found on the repo itself :) (plus some confidential internal files to test our whole flow) Git Repo: https://github.com/weareprestatech/hotpdf"},
{"Title": "SpeechBrain 1.0 - Open Source Toolkit for Conversational AI and More!", "Author": "u/FrancooMan", "Content": "Hello All, SpeechBrain 1.0 has just come out, and I wanted to post a project showcase to give the project some more exposure. There is so much you can do with SpeechBrain and it's completely free and open source! What The Project Does SpeechBrain is a completely free and open-source PyTorch toolkit for Conversational AI development, the technology behind things like speech assistants, chatbots, and large language models. With the toolkit you can perform tasks like speech recognition, speaker recognition, speech enhancement, speech separation, language modeling, dialogue, and much much more! There are over 200+ pre-made recipes you can use to train your own models for all kinds of tasks! Sometimes training models is extremely expensive though, and isn't possible for the average person. That's why the project also features 100+ pretrained models uploaded to hugging face that are free to download and use! By just creating an instance of a class and calling a function, you can use these models yourself. An example is transcribing speech in languages like English, French, Italian, and Mandarin to text! This is just a small taste of what you can do with SpeechBrain! Target Audience The project is intended for anyone interested in speech processing, natural language processing, and machine learning. Users of SpeechBrain range from those looking to build a product, researchers looking to discover something new, users looking to build their own tools/scripts and anything else you can think of! Comparison There are some alternative like NeMO and ESPNET. They are all good toolkits. SpeechBrain stands for its simplicity and flexibility that make it suitable for things like research and fast development of Conversational AI technologies. Thank you to the community for all the support! Useful Links: Website | Tutorials | Twitter | HuggingFace | Contributing"},
{"Title": "What all IDEs do you use? And why?", "Author": "u/Curious_ansh", "Content": "I have been using python to code for almost 2 years and wanted to know what all IDEs people use ? So I can make a wise choice. TIA"},
{"Title": "Streaming Python ETL with Airbyte and Pathway", "Author": "u/dxtros", "Content": "Now you can use Airbyte source connectors to process data in memory with Python. https://pathway.com/developers/showcases/etl-python-airbyte What My Project Does We integrated Airbyte connectors with Pathway, a Python stream processing framework, using the airbyte-serverless project. ETL pipelines are coming back with many use cases in AI (RAG pipelines), ETL for unstructured data and pipelines that deal with PII data. In this article, we show how to stream data from Github using Airbyte and remove PII data with Pathway. Target Audience This is a production-ready approach, to be used as a template for streaming ETL production settings. Comparison The setup is meant as an alternative to ELT setups (like Fivetran/Airbyte + dbt + warehouse), applying transform-before-load with Python. We are curious on your feedback on the implementation and other use cases you may think of from decoupling the extract and load steps. For the brave who want to see how it's done: https://github.com/pathwaycom/pathway/blob/main/python/pathway/io/airbyte/ + https://github.com/unytics/airbyte_serverless"},
{"Title": "clifunction - A minimal/simple way of building cli tools in python!", "Author": "u/derper-man", "Content": "clifunction I just published the first production quality version of my library clifunction! Code: https://github.com/Isaak-Malers/clifunction PyPi: https://pypi.org/project/clifunction/ Documentation: https://isaak-malers.github.io/clifunction/ What my project does Builds a fully featured CLI from your python functions.  All you need to do is annotate your functions fully, put the @cli_function decorator on the function, and then call your new CLI from the terminal! Target audience Anyone making small audience deployments of python code.  Are you writing scripts to distribute to 10's of collaborators?  This is a really fast way to reach MVP for shipping something out that can be run by technical people without an IDE/Editor.  Its also great for transitioning from running on a local machine, to running on a cloud process.  Academics, Data Scientists, Infastructure/Devops will find this is a great way to quickly share projects with their peers. Comparison This fills a similar need to the ubiquitous \"argparse\", but this is a lot easier to throw on existing code.  When developing it I asked myself \"How can this make it as easy as possible to go from a script running in my IDE, to a script running in a docker container somewhere\".  It has fewer features than argparse, but it leverages pythons built in declarative annotations to maximize how quickly you can get something useable."},
{"Title": "Automatically Generate Python Docstrings with LLMs", "Author": "u/snorkell_", "Content": "Thank you for the all the great feedbacks on my last post. Context: What My Project Does? Penify is a GitHub App that auto-generates and dynamically updates documentation for your repo. It supports various styles like Google, Numpy, reST, and Epydoc, ensuring your documentation stays current with every change. Check it out: [Penify GitHub App]( https://github.com/apps/penify-dev ). Here's how it works in action: [Sample PR]( https://github.com/NVIDIA/trt-llm-rag-windows/pull/44/files ). Updates: Generates consistent style docstrings: Google, reST, Numpy, Epydoc. Reduces hallucination through improved prompts and more context. Offers Full Repo documentation generation - previously it used to generate docs for only code changes. Working on ignore-private ignore-semiprivate ignore-init-method ignore-setters-getters Target Audience: Ideal for freelance developers requiring high-quality project documentation. Perfect for project managers desiring updated codebase docs for smooth onboarding of new developers. Alternatives: Copilot & Tabnine Cons: In consistent style docstring style - sometimes it generates Google,Numpy.... Context selection is very limited in Co-pilot Need to manually do doc updates for each modified code; Penify does it automatically in the background(it detects code changes and generates docs) Pros: You can update the modified docstring before pushing it. I know the community believes that Docstring generation shouldn't be done by LLMs - but I am seeing some good results. Hence, I am giving my effort. Please try and let me know your feedback."},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "üêô complexipy: An extremely fast Python library to calculate the cognitive complexity of python", "Author": "u/fexx3l", "Content": "Hey! I've recently started to work on this new project, once I met the Ann Campbell's work just feel in love, really liked this way to measure understandability, cognitive complexity is a measurement of how difficult it is to understand a specific piece of code. Unlike cyclomatic complexity, which primarily focuses on control flow based on branches and decisions, cognitive complexity takes into account the cognitive load required to comprehend the code. complexipy is extremely fast, and you can use it in your local files and directories or in your git repos to calculate the cognitive complexity, checkout the docs . As you can see, I was inspired also by the astral-sh way to describe their projects, really admire their work. What my project does I've created complexipy an extremely fast Python library to calculate the cognitive complexity of python files, written in Rust. This metric is measured by Sonar and I find it really useful when working on huge projects. Target audience Any developer that cares about understandability. Comparison I don't know if there's any python library that let you calculate the cognitive complexity, but this one is powered by Rust."},
{"Title": "Advanced Python Web Scraping Techniques | Jacob Padilla", "Author": "u/jpjacobpadilla", "Content": "Check out my article on advanced web scraping techniques: Get and manage cookies and custom headers Avoid TLS fingerprinting Recognize important HTTP headers to send in requests Implement exponential-backoff HTTP request retrying. https://jacobpadilla.com/articles/advanced-web-scraping-techniques"},
{"Title": "Dive Into Key Generation: RSA and AES Performance", "Author": "Unknown author", "Content": "Hey r/Python enthusiasts! I've recently wrapped up a fascinating project where I benchmark the performance of RSA and AES key generation using Python's cryptography library. The goal was to understand the time dynamics and security implications behind these two cryptographic heavyweights. I did it to understand why we need both Here's what went down: RSA (4089 bits) vs. AES (256 bits): I generated keys for both, focusing on RSA's asymmetric capabilities for secure messaging and AES's symmetric speed for bulk data encryption. Performance Measurement: I recorded the time taken to generate each key type, providing real-time insights into their operational efficiency. Output Documentation: Generated keys were saved in PEM format, alongside a detailed log of the time metrics in a 'result.txt' file. Load RSA Keys: You've already generated RSA keys in your project. Let's assume you have them ready. 5. Create a Signature: Use your private RSA key to create a signature for your message. The signature is created by encrypting a hash of the message with your private key. 6. Verify Signature: Anyone with your public key can verify the signature by decrypting it with the public key and comparing the hash value with their own hash of the original message. This project not only offered a hands-on experience with cryptographic key generation but also shed light on the practical aspects of implementing RSA and AES in Python. Would love to hear your thoughts, experiences, or any insights on improving the benchmarks or understanding the nuances better. Let's decrypt the complexities of cryptography together! GitHub Link: https://github.com/BDR-Pro/Comparing_RSA_AES Happy Encrypting! #python #cryptography #RSA #AES #security What My Project Does The project benchmarks the performance of RSA and AES key generation using Python's cryptography library. It focuses on analyzing the time required to generate cryptographic keys, evaluating the efficiency and practicality of RSA's asymmetric encryption against AES's symmetric encryption. The project extends beyond key generation, incorporating a demonstration of digital signature creation and verification with RSA, showcasing a real-world application of these cryptographic principles. Target Audience This project is designed for cybersecurity enthusiasts, cryptography students, and developers interested in understanding and implementing encryption in their applications. It serves as an educational tool for those new to cryptography and as a benchmarking utility for more experienced users evaluating the performance and applicability of RSA and AES in their projects. While it can be used as a reference for production environments, its primary intent is educational. Comparison Unlike many existing alternatives that focus solely on theoretical aspects or provide isolated examples of cryptographic functions, this project delivers a hands-on, comparative analysis between two pivotal cryptographic standards. It uniquely bridges the gap between theory and practice by providing a real-world application context, complete with performance metrics and operational insights. Furthermore, the inclusion of signature generation and verification with RSA provides a comprehensive overview that many tutorials or benchmarks lack, offering users a broader understanding of cryptographic applications beyond mere key generation."},
{"Title": "Flask application for transcribing phone calls in real-time - full tutorial", "Author": "u/SleekEagle", "Content": "Hey everyone! I wrote a Flask application that transcribes phone calls in real-time, feel free to check out if you're looking to do something similar: GitHub repo YouTube video tutorial Blog tutorial"},
{"Title": "Some of my thoughts about virtual environment tools", "Author": "u/KingsmanVince", "Content": "This document provides some fundamental and comparisons about virtual environment tools. The readers are expected to use one of similar tools once. If not, the readers should read 12. Virtual Environments and Packages ‚Äî Python 3.12.2 documentation . Some concepts might be oversimplified. The fundamental Every beginner tutorial about creating virtual environment starts with venv or virtualenv . venv is a standard module for creating lightweight environment. virtualenv is a tool to create isolated environments. Since Python 3.3 , a subset of it has been integrated into the standard library under the venv module. Eventually, the developers build management tools. virtualenvwrapper is a useful set of scripts for creating and deleting virtual environments. pyenv is a tool to install, isolate, manage , Python versions per user and per project. pyenv-virtualenv is a plugin of pyenv to manage virtualenv . pipenv is a Python virtualenv management tool that supports a multitude of systems and nicely bridges the gaps between pip , python (using system python, pyenv or asdf) and virtualenv . The combinations Black: virtual environment creation tools Red: virtual environment management tools Green: Python versions management tools Here is the image Here are some of the tool sets that people commonly use. The comparisons virtualenvwrapper pipenv pyenv pyenv pyenv-virtualenv Manage multiple Python versions No Yes Yes Manage per project No, you can have many venv per project and many projects per venv Yes No, like virtualenvwrapper. The holdup The readers may ask what conda / mamaba and poetry are all about. Conda-related conda is a system-level, binary package and environment manager running on all major operating systems and platforms. mamba is conda implemented in c++. Many people might be more familiar with miniconda , micromamba which are minimal CLI versions of aforementioned tools. Conda is similar to pyenv pyenv-virtualenv but not limited to Python. One can install C++ packages such as Nvidia drivers. Important use case: people often use this for machine learning projects. Poetry poetry is a tool for dependency management and packaging in Python. It allows you to declare the libraries your project depends on and it will manage (install/update) them for you. Poetry offers a lockfile to ensure repeatable installs, and can build your project for distribution. Poetry either uses your configured virtualenvs or creates its own to always be isolated from your system. Reading list python - What is the difference between venv, pyvenv, pyenv, virtualenv, virtualenvwrapper, pipenv, etc? - Stack Overflow Python Virtual Environments: A Primer ‚Äì Real Python References venv ‚Äî Creation of virtual environments ‚Äî Python 3.12.2 documentation virtualenv (pypa.io) virtualenvwrapper 6.0.1.dev12+gb1aa495 ‚Äî virtualenvwrapper 6.0.1.dev12+gb1aa495 documentation https://github.com/pyenv/pyenv https://github.com/pyenv/pyenv-virtualenv https://github.com/pypa/pipenv Miniconda ‚Äî Anaconda documentation Micromamba Installation ‚Äî documentation Introduction | Documentation | Poetry - Python dependency management and packaging made easy (python-poetry.org)"},
{"Title": "Hatchet - a Celery replacement focused on scale and observability", "Author": "u/hatchet-dev", "Content": "Hey everyone - really excited to showcase Hatchet, an OSS project I've been working on for the past few months: https://github.com/hatchet-dev/hatchet What My Project Does Hatchet is a Celery alternative built for scale and observability. Specifically, it supports: Low Latency and High Throughput Scheduling: Hatchet is built on a low-latency queue (25ms average start) so it can support real-time user interaction even while running from an async worker. Concurrency, Fairness, and Rate Limiting: implements FIFO, LIFO, Round Robin, and Priority Queues with built-in strategies for limiting concurrency. DAG workflows: Hatchet lets you declare tasks which are dependent on the execution of other tasks for full DAG-style execution. Durability and error handling: all events and executions are persisted to Postgres. When a worker fails, tasks automatically get reassigned to new workers, and the workflow will pick up where it left off. Web UI and API for observability: visualize events, logs and workflows within the dashboard. Workflow replay: replay tasks, workflows, and events right from the UI or via the API. Target Audience This is being actively used in production at 5 companies, the largest of which is executing 50k tasks per day. It's ideal for companies looking to scale their async tasks or want more visibility into workflow progression. Comparison Celery is a clear alternative - and while it's a great framework, there are a few reasons to favor Hatchet: - Better observability - I've spent a lot of time in the Celery Flower UI or building Grafana views for exported prom metrics. We wanted to build a modern, dev-friendly dashboard. There's still a long way to go, but a core focus of the platform is dev experience after deployment to production. - Postgres-backed - when I started to build this, I wanted a transactional database that's easy to horizontally scale and can handle high volumes of writes. We are working on batching execution data and forwarding it to Clickhouse, so Postgres is a natural choice. - Networking - when deploying with Celery, each worker manages its own connection to the underlying broker using redis or amqp . With Hatchet, each worker connects via a long-lived gRPC connection, which makes it easier to distribute workers across different networks or clusters, as there is widespread support for proxying HTTP 2.0 rather than forwarding TLS which can get tricky. FAQs How can I get started? Here's a quickstart repository for Python. This is written in Go, are you in the wrong place? After the original engine was deployed, we spent the next month building our Python SDK. I made the decision to use a lower-level language for more control over the underlying engine runtime which makes it easier to optimize latency. What's next on the roadmap? Support for logging from a task execution and giving a great logs API + view on our dashboard. This also can be difficult to integrate well with Celery AFAIK. Feedback Would love to hear what you think - also feel free to join our Discord and share your thoughts there."},
{"Title": "Use Firebase with Tkinter (Free Database & Authentication)", "Author": "u/TurbineThree", "Content": "Hey, heres a tutorial I made that shows how to use Firebase with Tkinter to create a simple application that has authentication and which uses the Firestore database: https://youtu.be/hxuiRPUJMwU?si=QF0zevMvOmq6BN1K Using Firebase is much easier than setting up your own backend and you can start using it for free!"},
{"Title": "Real-time ingestion pipeline for RAG app in Python", "Author": "u/oli_k", "Content": "Hi, I am working on a stream processing framework called Bytewax and I recently gave a talk where I read live data from HackerNews API, cleaned it with unstructured.io, created embeddings with a hugging face model, and stored everything in Milvus. I find a lot of devs are struggling with real-time streaming. The steps are easy to follow, and it is an end-to-end Python: https://github.com/bytewax/real-time-milvus I also added diagrams and summarized it in a blog https://bytewax.io/blog/stream-process-embed-repeat"},
{"Title": "BlendSQL: Unified dialect for orchestrating SQLite logic and LLM reasoning", "Author": "u/parkervg5", "Content": "https://github.com/parkervg/blendsql What my Project Does BlendSQL is a superset of SQLite for blending together complex reasoning between vanilla SQL and LLM calls. It's got a bunch of optimizations implemented using the amazing pure Python parser sqlglot to minimize the number of external LLM calls made. It allows you to efficiently perform tasks ranging from simple table question-answering: {{ LLMQA( 'Give me a short summary of my spending.', context=(SELECT * FROM spending) ) }} ...to complex question-answering with unstructured data using constrained decoding: (BlendSQL corresponding to 'List the term lengths of all U.S. presidents from the 3rd most populous city in Ohio') SELECT {{LLMMap('How long did they serve?', 'presidents::term')}} FROM presidents WHERE birthplace = {{ LLMQA( 'Which city is the 3rd most populous city in Ohio?', (SELECT * FROM documents WHERE documents MATCH 'ohio'), options='presidents::birthplace' ) }} Target Audience BlendSQL can be used in a couple ways - by directly writing the queries yourself as you would normal SQL to operate over a database, or as an intermediate representation that is taught to a LLM via few-shot learning . In our research, we show that using BlendSQL as an intermediate representation for complex question answering tasks boosts performance over the traditional end-to-end approach while using substantially fewer tokens. Comparison While other existing tools enable text-to-SQL using LLMs to write queries, this tool enables the user to oversee all calls (LLM + SQL) within a unified query language. Appreciate any feedback folks have! Thanks."},
{"Title": "Python code to fit data to a sum of exponentials with arbitrary number of terms", "Author": "u/juangburgos", "Content": "I used jupyterlite, pyodide and the PyArma module to implement the method described in this stackexchange question . The jupyterlite site with the notebooks is: http://juangburgos.github.io/FitSumExponentials/"},
{"Title": "A Discord Bot to converse with documentation using GPT-4 + RAG", "Author": "u/import-username-as-u", "Content": "What my project does This project is a Discord bot that lets you talk with document collections stored in the Qdrant vector database. The project comes with a Discord bot written using discord.py, and a backend written in FastAPI. I work for Hasura as a Community Engineer and we are in the Developer tooling space and so we get lots of support requests from developers. I built this bot to help answer questions that should be covered in our technical documentation. There is a blog post written in the README, and setup instructions in SETUP.md. This bot lives in our Discord server and automatically responds to requests in our help forum. Target Audience We are running this in production, and you probably could too. I won‚Äôt say it‚Äôs perfect, but it would be a really great project for someone wanting to get started with event-driven programming, interested in building a Discord bot, or who was curious about Hasura. I would definitely say this is something you want to be relatively comfortable with Python for it to be approachable. It uses a decent handful of some of my favorite python libraries like FastAPI, selenium, beautifulsoup4, and others. Comparison There are lots of chatbots coming out right now, and some might be better than this. This isn‚Äôt anything more than a simple case of RAG. The specific use case we wanted was a bot that would automatically try to answer help-forum questions and provide links to documentation. I‚Äôm in a handful of programming related Discord servers, and but just because I haven‚Äôt seen a bot like this in the ones I am in doesn‚Äôt mean there aren‚Äôt others. I think what‚Äôs maybe most useful is how easy it is to run this yourself because of the Docker-compose."},
{"Title": "pyCage: A VSCode extension to search and install popular Python packages from the command palette.", "Author": "u/KitN_X", "Content": "What Does My Project Do? pyCage is a VSCode extension powered by astral.sh's uv package manager and pip. It enables users to search for and download Python packages directly from the command palette, similar to Dart's package manager. Target Audience This extension is designed for individuals who frequently use venvs and struggle with remembering package names. Comparison The installation speed is notably fast due to the option to download using uv.It might be faster to download packages using pyCage than using the terminal in some cases. GitHub GitHub Link: https://github.com/qKitNp/pyCage"},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "MuAlphaZeroLibrary: simple usage and training of MuZero and AlphaZero type models", "Author": "u/Skirlaxx", "Content": "A few days ago I published the first version of my python library for simple usage and training of DeepMind's AlphaZero and MuZero algorithms. The library is designed for easy usage and extensibility, allowing you to define custom environments, memory buffers, networks, etc. while also providing default implementations for all of these. What my project does Allows for simple training and usage of the MuZero and AlphaZero models with focus on extensibility. Target audience Human-like Inhabitants of the planet earth. Comparison There are a couple of projects implementing one of the algorithms, but my project offers both algorithms in one easy to use library. Github link : https://github.com/Skirlax/MuAlphaZeroLibrary"},
{"Title": "I made a CV Generator using PyQt6 and Python | Modern Design", "Author": "u/Specialist-Arachnid6", "Content": "What my Project Does? cvGen is a CV generator made using PyQt6 and Python. It collects user data via inputs given by the user and then crafts a beautiful CV using built-in HTML and CSS templates. Target Audience People who want to make good looking CVs without using any command line tools / YAML formatting manually. Comparison For comparison, I don't know what to compare it to since there are no other UI-based CV generators. If you know any,  please let me know. GitHub GitHub Link: https://github.com/rohankishore/cvGen Known Issues Only one template as of now (looking forward to expanding it) Only support input via the UI (coding support for JSON/YAML)"},
{"Title": "Automate creating a virtual environment and upgrading pip [tutorial]", "Author": "u/Responsible_Rip_4365", "Content": "I've been building a lot of projects lately and got tired of upgrading pip each time I create a new venv (virtual environment), so I decided to look up solutions and found that I can use a script to run the steps I usually do manually. This is obviously not something new or that necessary but thought it would be fun and save a few seconds especially when am building new projects everyday. I created a short tutorial on how to do this: https://youtu.be/xMDh4TYoIB4?si=iEziqP5YQFj2wIHW"},
{"Title": "Starter template for Cython", "Author": "u/joegeezer", "Content": "Hi guys I made a starter template repo for creating Python bindings from a C library.. maybe someone else might find this useful - https://github.com/joegasewicz/cython-starter-template thanks for looking"},
{"Title": "Pandopt, basicaly pandas, but optimized", "Author": "u/Fun-Coffee-6390", "Content": "Pandas is Great, But... Pandas excels when your dataset is manageable in size, and you adhere strictly to its built-in functions. However, when either of these conditions is not met, performance drastically declines. Relying solely on custom, albeit user-friendly, built-in functions for decent performance seems suboptimal. Despite this, for many operations, learning an array of built-ins isn't necessary; you can accomplish a great deal using just a few key methods like apply , aggregate , groupby , and rolling (not addressing I/O here). However, the apply is known to be one of the worst in terms of performance, that's why I wanted to make it the best..! Nonetheless, pandas shines for several reasons, including its user-friendly API and widespread adoption. Why I'm Skeptical of Alternatives In the quest for better performance, you might consider options like Modin or Polars. However, my experience with them has been mixed: they either introduce a steep learning curve or fail to function in 90% of the environments I've tested. The Essence of Pandopt Pandopt aims to enhance basic pandas operations, such as apply and rolling , by implementing dynamic code transformation and JIT compilation via Numba. This approach significantly boosts performance without altering other aspects of pandas functionality. Target Audience This message is intended for data science enthusiasts, optimization fans, and those who appreciate elegant coding. Performance Comparison Pandopt dramatically outperforms pandas in executing apply functions on large datasets (over 1 million rows), offering speed improvements of approximately 10,000 to 20,000 times post-initial compilation (which is cached). Even for standard operations like sum , it outpaces numpy or Polars by roughly 8 times. While gains in rolling functions are less pronounced, Pandopt still enhances flexibility, allowing the application of custom functions across multiple columns and functions natively. Project is public on git and available on Pypi even if unstable: https://github.com/remigenet/pandopt"},
{"Title": "hotpdf: Fast PDF Text Search and Extraction Library", "Author": "u/Free_Let_8315", "Content": "While working at my current company, I developed a PDF text search and extraction library based on pdfminer.six. We open-sourced the library, and would love to know if this could help you! :) What My Project Does: hotpdf was developed as an alternative to libraries like pdfquery with a focus on text search and extraction. The project was built to be memory efficient and have fast text searching and extraction on PDF documents. Target Audience: If you work with PDFs, especially performing text operations search and extraction by bounding box, then this library is for you. In my current company, we use it for this exact use case. For example: We want to extract the value associated with IBAN: in a document. We search for the \"IBAN\" and extract text around the surrounding coordinates. Comparison : For search, it implements a Trie, and for coordinate-based extraction, it implements a Sparse Matrix. These 2 data structures are created on each page. This helps the loading of PDFs into the memory fast. And since every page has its data structure implementation, it's easy to parallelize the whole process and merge the objects into one. Against pdfquery , it's around 5-8x times faster and uses 5-10x less memory. All benchmarks were performed on test files that are found on the repo itself :) (plus some confidential internal files to test our whole flow) Git Repo: https://github.com/weareprestatech/hotpdf"},
{"Title": "SpeechBrain 1.0 - Open Source Toolkit for Conversational AI and More!", "Author": "u/FrancooMan", "Content": "Hello All, SpeechBrain 1.0 has just come out, and I wanted to post a project showcase to give the project some more exposure. There is so much you can do with SpeechBrain and it's completely free and open source! What The Project Does SpeechBrain is a completely free and open-source PyTorch toolkit for Conversational AI development, the technology behind things like speech assistants, chatbots, and large language models. With the toolkit you can perform tasks like speech recognition, speaker recognition, speech enhancement, speech separation, language modeling, dialogue, and much much more! There are over 200+ pre-made recipes you can use to train your own models for all kinds of tasks! Sometimes training models is extremely expensive though, and isn't possible for the average person. That's why the project also features 100+ pretrained models uploaded to hugging face that are free to download and use! By just creating an instance of a class and calling a function, you can use these models yourself. An example is transcribing speech in languages like English, French, Italian, and Mandarin to text! This is just a small taste of what you can do with SpeechBrain! Target Audience The project is intended for anyone interested in speech processing, natural language processing, and machine learning. Users of SpeechBrain range from those looking to build a product, researchers looking to discover something new, users looking to build their own tools/scripts and anything else you can think of! Comparison There are some alternative like NeMO and ESPNET. They are all good toolkits. SpeechBrain stands for its simplicity and flexibility that make it suitable for things like research and fast development of Conversational AI technologies. Thank you to the community for all the support! Useful Links: Website | Tutorials | Twitter | HuggingFace | Contributing"},
{"Title": "What all IDEs do you use? And why?", "Author": "u/Curious_ansh", "Content": "I have been using python to code for almost 2 years and wanted to know what all IDEs people use ? So I can make a wise choice. TIA"},
{"Title": "Streaming Python ETL with Airbyte and Pathway", "Author": "u/dxtros", "Content": "Now you can use Airbyte source connectors to process data in memory with Python. https://pathway.com/developers/showcases/etl-python-airbyte What My Project Does We integrated Airbyte connectors with Pathway, a Python stream processing framework, using the airbyte-serverless project. ETL pipelines are coming back with many use cases in AI (RAG pipelines), ETL for unstructured data and pipelines that deal with PII data. In this article, we show how to stream data from Github using Airbyte and remove PII data with Pathway. Target Audience This is a production-ready approach, to be used as a template for streaming ETL production settings. Comparison The setup is meant as an alternative to ELT setups (like Fivetran/Airbyte + dbt + warehouse), applying transform-before-load with Python. We are curious on your feedback on the implementation and other use cases you may think of from decoupling the extract and load steps. For the brave who want to see how it's done: https://github.com/pathwaycom/pathway/blob/main/python/pathway/io/airbyte/ + https://github.com/unytics/airbyte_serverless"},
{"Title": "clifunction - A minimal/simple way of building cli tools in python!", "Author": "u/derper-man", "Content": "clifunction I just published the first production quality version of my library clifunction! Code: https://github.com/Isaak-Malers/clifunction PyPi: https://pypi.org/project/clifunction/ Documentation: https://isaak-malers.github.io/clifunction/ What my project does Builds a fully featured CLI from your python functions.  All you need to do is annotate your functions fully, put the @cli_function decorator on the function, and then call your new CLI from the terminal! Target audience Anyone making small audience deployments of python code.  Are you writing scripts to distribute to 10's of collaborators?  This is a really fast way to reach MVP for shipping something out that can be run by technical people without an IDE/Editor.  Its also great for transitioning from running on a local machine, to running on a cloud process.  Academics, Data Scientists, Infastructure/Devops will find this is a great way to quickly share projects with their peers. Comparison This fills a similar need to the ubiquitous \"argparse\", but this is a lot easier to throw on existing code.  When developing it I asked myself \"How can this make it as easy as possible to go from a script running in my IDE, to a script running in a docker container somewhere\".  It has fewer features than argparse, but it leverages pythons built in declarative annotations to maximize how quickly you can get something useable."},
{"Title": "Automatically Generate Python Docstrings with LLMs", "Author": "u/snorkell_", "Content": "Thank you for the all the great feedbacks on my last post. Context: What My Project Does? Penify is a GitHub App that auto-generates and dynamically updates documentation for your repo. It supports various styles like Google, Numpy, reST, and Epydoc, ensuring your documentation stays current with every change. Check it out: [Penify GitHub App]( https://github.com/apps/penify-dev ). Here's how it works in action: [Sample PR]( https://github.com/NVIDIA/trt-llm-rag-windows/pull/44/files ). Updates: Generates consistent style docstrings: Google, reST, Numpy, Epydoc. Reduces hallucination through improved prompts and more context. Offers Full Repo documentation generation - previously it used to generate docs for only code changes. Working on ignore-private ignore-semiprivate ignore-init-method ignore-setters-getters Target Audience: Ideal for freelance developers requiring high-quality project documentation. Perfect for project managers desiring updated codebase docs for smooth onboarding of new developers. Alternatives: Copilot & Tabnine Cons: In consistent style docstring style - sometimes it generates Google,Numpy.... Context selection is very limited in Co-pilot Need to manually do doc updates for each modified code; Penify does it automatically in the background(it detects code changes and generates docs) Pros: You can update the modified docstring before pushing it. I know the community believes that Docstring generation shouldn't be done by LLMs - but I am seeing some good results. Hence, I am giving my effort. Please try and let me know your feedback."},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "üêô complexipy: An extremely fast Python library to calculate the cognitive complexity of python", "Author": "u/fexx3l", "Content": "Hey! I've recently started to work on this new project, once I met the Ann Campbell's work just feel in love, really liked this way to measure understandability, cognitive complexity is a measurement of how difficult it is to understand a specific piece of code. Unlike cyclomatic complexity, which primarily focuses on control flow based on branches and decisions, cognitive complexity takes into account the cognitive load required to comprehend the code. complexipy is extremely fast, and you can use it in your local files and directories or in your git repos to calculate the cognitive complexity, checkout the docs . As you can see, I was inspired also by the astral-sh way to describe their projects, really admire their work. What my project does I've created complexipy an extremely fast Python library to calculate the cognitive complexity of python files, written in Rust. This metric is measured by Sonar and I find it really useful when working on huge projects. Target audience Any developer that cares about understandability. Comparison I don't know if there's any python library that let you calculate the cognitive complexity, but this one is powered by Rust."},
{"Title": "Advanced Python Web Scraping Techniques | Jacob Padilla", "Author": "u/jpjacobpadilla", "Content": "Check out my article on advanced web scraping techniques: Get and manage cookies and custom headers Avoid TLS fingerprinting Recognize important HTTP headers to send in requests Implement exponential-backoff HTTP request retrying. https://jacobpadilla.com/articles/advanced-web-scraping-techniques"},
{"Title": "Dive Into Key Generation: RSA and AES Performance", "Author": "Unknown author", "Content": "Hey r/Python enthusiasts! I've recently wrapped up a fascinating project where I benchmark the performance of RSA and AES key generation using Python's cryptography library. The goal was to understand the time dynamics and security implications behind these two cryptographic heavyweights. I did it to understand why we need both Here's what went down: RSA (4089 bits) vs. AES (256 bits): I generated keys for both, focusing on RSA's asymmetric capabilities for secure messaging and AES's symmetric speed for bulk data encryption. Performance Measurement: I recorded the time taken to generate each key type, providing real-time insights into their operational efficiency. Output Documentation: Generated keys were saved in PEM format, alongside a detailed log of the time metrics in a 'result.txt' file. Load RSA Keys: You've already generated RSA keys in your project. Let's assume you have them ready. 5. Create a Signature: Use your private RSA key to create a signature for your message. The signature is created by encrypting a hash of the message with your private key. 6. Verify Signature: Anyone with your public key can verify the signature by decrypting it with the public key and comparing the hash value with their own hash of the original message. This project not only offered a hands-on experience with cryptographic key generation but also shed light on the practical aspects of implementing RSA and AES in Python. Would love to hear your thoughts, experiences, or any insights on improving the benchmarks or understanding the nuances better. Let's decrypt the complexities of cryptography together! GitHub Link: https://github.com/BDR-Pro/Comparing_RSA_AES Happy Encrypting! #python #cryptography #RSA #AES #security What My Project Does The project benchmarks the performance of RSA and AES key generation using Python's cryptography library. It focuses on analyzing the time required to generate cryptographic keys, evaluating the efficiency and practicality of RSA's asymmetric encryption against AES's symmetric encryption. The project extends beyond key generation, incorporating a demonstration of digital signature creation and verification with RSA, showcasing a real-world application of these cryptographic principles. Target Audience This project is designed for cybersecurity enthusiasts, cryptography students, and developers interested in understanding and implementing encryption in their applications. It serves as an educational tool for those new to cryptography and as a benchmarking utility for more experienced users evaluating the performance and applicability of RSA and AES in their projects. While it can be used as a reference for production environments, its primary intent is educational. Comparison Unlike many existing alternatives that focus solely on theoretical aspects or provide isolated examples of cryptographic functions, this project delivers a hands-on, comparative analysis between two pivotal cryptographic standards. It uniquely bridges the gap between theory and practice by providing a real-world application context, complete with performance metrics and operational insights. Furthermore, the inclusion of signature generation and verification with RSA provides a comprehensive overview that many tutorials or benchmarks lack, offering users a broader understanding of cryptographic applications beyond mere key generation."},
{"Title": "Flask application for transcribing phone calls in real-time - full tutorial", "Author": "u/SleekEagle", "Content": "Hey everyone! I wrote a Flask application that transcribes phone calls in real-time, feel free to check out if you're looking to do something similar: GitHub repo YouTube video tutorial Blog tutorial"},
{"Title": "Codeflash - Optimize your code's performance automatically", "Author": "u/ml_guy1", "Content": "Hi! I am Saurabh. I love writing fast programs and I've always hated how slow Python code can sometimes be. To solve this problem, I have created Codeflash. What My Project Does codeflash is a Python package that uses AI to figure out the most performant way to rewrite a Python code. It not only optimizes the performance but also verifies the correctness of the new code, i.e. makes sure that the new code follows exactly the same behavior as your original code. This automates the manual optimization process. It can improve algorithms, data structures, fix logic, use better optimized libraries etc to speed up your code. Website - https://www.codeflash.ai/ , get started here. PyPi - https://pypi.org/project/codeflash/ If you have a Python project, it should take you less than 5 minutes to setup codeflash - pip install codeflash and codeflash init Codeflash can also optimize your entire project! Run codeflash --all after setting up codeflash, and codeflash will optimize your project, function by function, and create PRs on GitHub when it finds an optimization. This is super powerful. You can also install codeflash as a GitHub actions check that runs on every new PR you create, to ensure that all new code is performant. If codeflash finds that a code can be made more performant, it will create a PR comment with the new optimized code. This ensures that your project stays at peak performance everytime. How it works Codeflash works by optimizing the code path under a function. So if there is a function foo(a, b): , codeflash finds the fastest implementation of the function foo and all the other functions it calls. The optimization procedure preserves the signature of the function foo and then figures out a new optimized implementation that results in exactly the same return values as the original foo. The behavior of the new function is verified to be correct by running your unit tests and generating a bunch of new regression tests. The runtime of the new code is measured and the fastest one is recommended. Target Audience Codeflash is currently the best at optimizing pure-functions without side effects. You can use codeflash to improve performance of any custom algorithm, numpy code, pandas code, data processing code etc. It is very general purpose. You should be able to optimize anything that can be unit-tested. You can also try to optimize non-pure functions but you should review the new code. We are improving support to more types of functions. Would love to know about your use case and how we can support it! Comparison I am currently unaware of any direct comparison with codeflash on optimizing performance of user level code. Codeflash is still early but has gotten great results already by optimizing open source projects like Langchain and many others. I would love you to try codeflash to optimize your code and let me know how you use it and how we can improve it for you! Thank you."},
{"Title": "Pycharm SQLAlchemy Plugin adds typing support for Mapped columns", "Author": "u/riksi", "Content": "This has been a long standing issue on Pycharm and nobody on the company seems interested in it. Looks like somebody went ahead and created a plugin that from a quick check looks like it works fine: I did not make this. https://github.com/souperk/pycharm-sqlalchemy-plugin"},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "PySTAC Decoded: A Step-by-Step Tutorial", "Author": "u/m_razali", "Content": "If you work with spatio temporal data like satellite imagery, SAR, Point Clouds, etc. Try to use Spatio Temporal Data Catalog (STAC). Take a look this tutorial that will explain step by step approach how to use STAC in Python from searching data, display it and do analysis the data in a row. https://www.geodose.com/2024/02/pystac-decoded-step-by-step-tutorial.html"},
{"Title": "I Remade Infinite Craft But In PySide6 And You Can Create Your Own Versions With it", "Author": "u/FUS3N", "Content": "Hello, I'd like to share a recent project of mine. infinite Sides! Screenshots Project Repo: https://github.com/Fus3n/infinite-sides What my project does I recently played the game called Infinite Craft and it was quite fun to see how you can craft anything from any word you can check out the original game here . This game uses LLM to generate words based on a combination of other words or two words, you get a few words that you start with and you can go with it and come up with infinite combinations (well as far as an LLM can go in theory, its not perfect). So I decided to make it in Python and PySide6 right after discovering Ollama , which is a way to run local LLMs pretty fast. So you can just boot up Ollama and my app will use its local API  to communicate with LLMs to generate texts in your computer, so no need for the internet , but its main feature is that you can just create your OWN infinitely crafting game, for example, a Minecraft themed one, Example Here So you can basically create any kind of game! You can also forward the port Ollama is hosted on by default using ngrok and share the link with other people which they can use to play the game in infinite-sides, all they have to do is change the base URL in settings, so you can give access to your model to your friends if they don't have a beefy computer to run LLMs. Target audience If you enjoy games like this you also might enjoy this one! Comparison I don't know If there are already any apps/games like this either way this is my version just a project I did and hope people can enjoy it! All instructions to run and use it are in the README, there is currently no binaries but will be in the future. Also, I am not good with \"prompt engineering\" so the current default system prompt is pretty bad and might not give the best experience, So I suggest you edit it in the settings and try different variations if you want to give the game a try."},
{"Title": "Python Certification Guide- What should we include in it to make it super useful?", "Author": "u/entreluvkash", "Content": "Our company is currently developing a book/guide for the PCEP certification. While there are numerous resources available, we aim to create a comprehensive book covering essential aspects such as tips and tricks, example questions, brain teasers, and case studies. I'd love to hear your suggestions for additional content that would be particularly valuable and not commonly found in existing resources on this certification."},
{"Title": "Balancing chemical equations with Python", "Author": "u/chicolucio", "Content": "Recently, I wrote a blog post on ChemPy, a Python package, for balancing chemical equations. I hope you find it useful. https://chemistryprogramming.xyz/2024/02/chempy-balancing-equations-python/"},
{"Title": "Resource management rant: Current concepts not sufficient", "Author": "u/Wild_Meeting1428", "Content": "Why do, garbage collected languages like python, not implement resource management concepts in a sane way? C++ and Rust have RAII, which is awesome. Python has __enter__ , __exit__ , with ... as ... , try: ... finally: . But that is extremely limited: Returning a resource prevents the user to use the safe with statement, the resource is closed. And even if it would work, how should the caller know? Since python never cared, this would be an improvement. On top writing with for all resources, especially when working with resources, creating resources, is very cumbersome. The code is blown up by with or try ... finally/catch blocks. And it is still unsafe because you will forget to use these wherever they are required. The best solution to this would be, to clean up all uncatched resources automatically on scope exit. Python already knows that they are resources, since they implement __enter__ and __exit__ . Python just has to prevent the creation of strong references from resources. Weak references should still creatable with weakref . In the end this would be only syntactic sugar, but it would make the resource management way safer and on top more convenient: f = open(__file__, \"r\") some_code_may_throw(f) return f would compile to f = open(__file__, \"r\") try: some_code_may_throw(f) return f except: f.close() raise"},
{"Title": "Online tool for visualizing regex matches in Python", "Author": "u/yogeshkd", "Content": "It highlights all matches including groups so you can quickly test regex patterns: https://python-fiddle.com/tools/regex"},
{"Title": "SuiteSpotAuth: A simple authentication wrapper for SuiteSpot", "Author": "u/yhavin", "Content": "What my project does suitespotauth provides a simple wrapper for the authentication flow required by SuiteSpot construction management software. It exposes a secure access_token attribute which you need (for Bearer Authorization) to make API calls to get your construction data from SuiteSpot. This cuts out all the boilerplate flow required of creating Basic Authorization to generate an API token and exchange it for the required access token. GitHub: https://github.com/yhavin/suitespotauth Homepage: https://pypi.org/project/suitespotauth Target audience Available for use in production. The target audience is for developers who want to make API calls to SuiteSpot analytics to retrieve their construction data, but do not want to write 100 lines of code just to get the access token. Comparison There is no known other package providing a wrapper to the authentication flow. The official authentication docs are available at https://auth.suitespot.io/api#/ . Development This package is being actively developed, and all replies/issued will be read. Thank you!"},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Tyche: Supercharge Your Hypothesis Testing", "Author": "u/Different_Hippo7511", "Content": "Note , Tyche is an active research project and we're looking for users to help us do a formal evaluation study. If you're interested in helping out and giving feedback on the tool, please fill out this Google form . First of all, if you haven't heard of Hypothesis , check it out. It's a \"library for creating unit tests which are simpler to write and more powerful when run, finding edge cases in your code you wouldn‚Äôt have thought to look for\" (from the docs) and it's my favorite way to test Python code. But if you're a long-time user of Hypothesis, you've likely run into a situation where a Hypothesis test wasn't actually doing what you thought it was doing. Maybe you thought you were testing on positive integers, but you were actually wasting time generating lots of negative ones too. Or maybe a from_type generator just didn't produce the values you thought it would. Sometimes Hypothesis tells you about these mistakes, but often you don't notice until a bug made it into production. What My Project Does Tyche is an open source ( source ) Visual Studio Code extension that helps you analyze your Hypothesis tests and make sure they're exactly as thorough as you expect. The interface shows you charts that summarize the distribution of data used to test your code and helps to warn you if things seem off. The extension has already helped the Hypothesis team to fix a couple of bugs in the Hypothesis framework itself, and it may be able to help you improve your own test suites too. Target Audience Users of Hypothesis. Even if you're pretty confident that your tests are already as effective as they can be, it's worth using Tyche to double check. Comparison As far as I know, there aren't other tools like Tyche out there yet. This project is part of cutting-edge research at a major university."},
{"Title": "remoteio - A remote GPIO library for the Raspberry Pi", "Author": "u/Saltibarciai", "Content": "Hi guys! I created my very first python lib. It's a server/client solution that allows to control  GPIO pins of a remote Raspberry Pi. GitHub: https://github.com/schech1/remoteio Homepage: https://pypi.org/project/remoteio/ What My Project Does remoteio creates a socket and can run as a deamon at startup. The remoteio python lib provides gpiozero-like commands to control outputs on the remote Raspberry Pi. Target Audience Makers who want to control the GPIO outputs of one or more remote Raspberry Pis. Comparison The previously known remote GPIO option of Raspberry Pi OS is provided by pigpiod , which does not work on the Raspberry Pi 5 anymore. The project is still small and currently only offers to toggle outputs. I plan to add also inputs and I2c/SPI in future updates. I appreciate your support!"},
{"Title": "RenderCV v1 is released! Create an elegant CV/resume from YAML.", "Author": "u/egehancry", "Content": "I released RenderCV a while ago with this post . Today, I released v1 of RenderCV, and it's much more capable now. I hope it will help people to automate their CV generation process and version-control their CVs. What My Project Does RenderCV is a LaTeX CV/resume generator from a JSON/YAML input file. The primary motivation behind the RenderCV is to allow the separation between the content and design of a CV . It takes a YAML file that looks like this: cv: name: John Doe location: Your Location email: youremail@yourdomain.com phone: tel:+90-541-999-99-99 website: https://yourwebsite.com/ social_networks: - network: LinkedIn username: yourusername - network: GitHub username: yourusername sections: summary: - This is an example resume to showcase the capabilities of the open-source LaTeX CV generator, [RenderCV](https://github.com/sinaatalay/rendercv). A substantial part of the content is taken from [here](https://www.careercup.com/resume), where a *clean and tidy CV* pattern is proposed by **Gayle L. McDowell**. education: ... And then produces these PDFs and their LaTeX code: classic theme sb2nov theme moderncv theme engineeringresumes theme Example PDF , Example PDF Example PDF Example PDF Corresponding YAML Corresponding YAML Corresponding YAML Corresponding YAML It also generates an HTML file so that the content can be pasted into Grammarly for spell-checking. See README.md of the repository. RenderCV also validates the input file, and if there are any problems, it tells users where the issues are and how they can fix them. I recorded a short video to introduce RenderCV and its capabilities: https://youtu.be/0aXEArrN-_c Target Audience Anyone who would like to generate an elegant CV from a YAML input. Comparison I don't know of any other LaTeX CV generator tools implemented with Python."},
{"Title": "Pyright plugin for PyCharm", "Author": "u/InSyncWithFoo", "Content": "I just wrote a plugin that runs Pyright on-the-fly as you code in PyCharm. This is just an MVP and hasn't been battle-tested, so basically I'm asking for help. Please try it out and let me know what can be improved. Version 0.1.0 will be released on the Marketplace once PyCharm 2024.1 is released. Until then I'll try to make it as good as possible. Target audience PyCharm users who also use Pyright. Comparison Instead of arguing with your colleagues about whether you should switching to VSCode for better type checking, now you can enjoy the power of Pyright directly in PyCharm. What it does More or less the same thing as running pyright path/to/your-file.py in the terminal, but automatically."},
{"Title": "sqlcipher3-wheels: Wheels for sqlcipher3, 100% less pain during install (Windows, MacOS, Linux)", "Author": "u/laggykiller", "Content": "PyPI: https://pypi.org/project/sqlcipher3-wheels/ GitHub: https://github.com/laggykiller/sqlcipher3 What My Project Does sqlcipher3 is a python3 binding for SQLCipher maintained by coleifer. Officially it only provides manylinux wheels for x86_64 and bundles with SQLCipher version 3 (From sqlcipher3-binary ). sdist does not work if sqlcipher is not installed in system, as it does not contain source of sqlcipher. It is quite a pain to compile, especially on Windows due to dependency to openssl . I decided to create my own fork that build sqlcipher3 wheels with cibuildwheel. The wheels are compiled with sqlcipher version 4. Wheels are available for all platforms (Windows, MacOS, manylinux, musllinux) and all supported architectures (x86_64, i686, aarch64, ppc64le, s390x). openssl is installed with conan, and sdist now bundles with source of sqlcipher so it could be built even if the system does not have openssl and sqlcipher installed. You can download the wheels by pip install sqlcipher3-wheels . Hope this reduces your pain and suffering! Target Audience Those who want to use sqlcipher3 without compiling by themselves Comparison sqlcipher3 is where I have forked from, which only provides manylinux x86_64 wheels built with sqlcipher version 3. The sdist does not contain source of sqlcipher and cannot compile if openssl is absent."},
{"Title": "Financial-Analyzer (Python CLI money analyzer / Mint alternative)", "Author": "u/manutoe", "Content": "Hey all, I am making this post to highlight a video I just published on YouTube about a Python CLI (command-line interface) financial analyzer application I have developed. What my project does The project in question is meant to analyze spending, record transactions, and view balances/investments. I think it's a great alternative to something like Mint (especially because Mint is shutting down!) Yes, you will likely need programming experience to use it cleanly but hey that's why I'm posting in r/Python :) Target Audience Anyone who wants to track their spending would find value in this project. If you like applications like Mint, you will like this project. Comparison One similar project is LedgerCLI . However this project is more for the pure accounting side and doesn't involve the budgeting / investment aspect that I think of when I think about examining personal finance. References Please checkout this recent post and this post on the GUI version And checkout the YouTube video here ! GitHub Repo Github Link"},
{"Title": "Introducing LIVA: Your Local Intelligent Voice Assistant", "Author": "u/Automatic-Net-757", "Content": "Hey Redditors, What My Project Does I'm excited to introduce you to LIVA (Local Intelligent Voice Assistant), a side project I've been working on that brings the power of voice assistants right to your terminal! It uses LLMs, Text to Speech and Speech to Text to create a voice assistant Target Audience The project is intended for developers, tech enthusiasts who are working with Large Language Models would quickly like to run a voice chat with their favorite Large Language Model Comparison So far I have not found anything that does speech to text in almost real time combined with LLM and a Speech Generator. These were available as separate projects but did not quite found a project that combines all three (I did find voice assistants which use LLMs and Speech Generators but they were lacking near real-time speech to text capabilities) Key Features: Here's what you can expect from LIVA: üé§ Speech Recognition : LIVA accurately transcribes your spoken words into text, making interaction seamless and intuitive. By default whisper-base.en is being used for speech recognition üí° Powered by LLM : Leveraging advanced Large Language Models, LIVA understands context and provides intelligent responses to your queries. By default the parameters are set to Mistral:Instruct with Ollama Endpoint, but the model can be easily changed, you can use any OpenAI compatible endpoint üîä Text-to-Speech Synthesis : LIVA doesn't just understand ‚Äì it speaks back to you! With natural-sounding text-to-speech synthesis, LIVA's responses are clear and human-like. For the TTS, I'm going with the SpeechT5 üõ†Ô∏è Customizable and User-Friendly : With customizable settings and an intuitive interface, LIVA adapts to your preferences and needs, making it easy to use for everyone. Right now, you can customize the LLM and the TTS model (It accepts the variants of Whisper) Let's say you want to use the openhermes from ollama with whisper-small.en. Then you just simply run python --model-id openhermes --stt-model openai/whisper-small.enmain.py Running the python main.py will look for the whisper-base.en and will download if it isn't present. And coming to the model, by default it looks for the Mistral:Instruct on the Ollama Endpoint Contributions But here's where you come in ‚Äì I want your input! Your feedback, suggestions, and ideas are invaluable in making LIVA even better. Whether you're a developer, a tech enthusiast, or simply curious to try it out, your voice matters. Here's how you can get involved: Try It Out: Head over to GitHub to check out the project code. Install it, give it a try, and let me know what you think. Feedback and Suggestions: Have ideas for new features or improvements? Found a bug? Share your thoughts by submitting feedback on GitHub. Your input helps shape the future of LIVA. Spread the Word: Know someone who might benefit from LIVA? Share it with them! The more people who use and contribute to LIVA, the stronger the community becomes. Collaborate: Interested in contributing code, documentation, or ideas? Fork the repository, make your changes, and submit a pull request. Let's collaborate and make LIVA the best it can be. I'm excited about the potential of LIVA, and I can't wait to see where this journey takes us. Together, let's create a voice assistant that's intelligent, accessible, and tailored to our needs. Got questions, ideas, or just want to chat about LIVA? Drop a comment below or reach out to me directly. Your input is what makes LIVA great! (P.S. If you're interested in joining the LIVA project and contributing, check out the suggestions above!)"},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "A simple comic strip API wrapper", "Author": "u/dumblechode", "Content": "I'd like to present my simple GoComics API wrapper . What My Project Does This PyPI package allows for easy searching, downloading, and displaying of comic strips directly from GoComics. Target Audience This package is intended for developers and comic enthusiasts looking to integrate comic strips into their websites or projects. Comparison The only other comparison I can find is a repository to download Marvel comics, last updated 8 years ago. Key Features: Simple Installation: Get started with pip install comics. Versatile Usage: Find specific comics by date or randomly explore the large collection from GoComics, including classics like Calvin and Hobbes . Easy Integration: Add comic strips to your personal website, create a curated collection, or enjoy them in Jupyter notebooks. This wrapper supports Python 3.7+, is open for contributions on GitHub, and is licensed under MIT. For more details, check out the GitHub repository !"},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Rybak 0.3 - library for generating directory trees", "Author": "u/walkie-talkie24", "Content": "https://github.com/python-lapidary/rybak What my project does Rybak is a directory-tree generator library. It accepts directory-tree structure, where file content as well as file and directory names can be templates (Jinja or Mako), applies client provided data, and generates a directory structure. Target audience It's intended for use in any project that needs to generate complex directory structures from templates. I keep it as a helper library for my OpenAPI client generator. Comparison It's inspired by Cookiecutter and Copier. Compared to them: focuses on delivering a clean public API and not a CLI or interacting with user can generate more complex directory structures, where a single template file can be used to generate multiple output files Reads templates from importlib.resources works with Jinja as well as Mako templates (only one engine for a template directory)"},
{"Title": "A python hacky script to scan rdp port 3389", "Author": "u/Technical_Shelter621", "Content": "Wannabe Security Researcher!?!? Is the title of my very first blog post of my very first blog, I hope it to be informative for who is interested in Security and more specifically about an home assignment I received for a position of Sr. Security Researcher and how I approached it. Thanks for reading https://cyberroute.github.io/post/2024-02-23-rdpscan/"},
{"Title": "DataLoader - Dynamically load all files.", "Author": "u/yousefabuz", "Content": "GitHub: DataLoader What My Project Does DataLoader is a comprehensive utility that facilitates the efficient loading and processing of data from a specified path (directory) or all data from multiple directories. This project is designed to be user-friendly and easy to integrate into your projects. Please note this project may not be suited for everyone as most users do not prefer dynamically loading their files/data but rather traditionally. Along with users not prefer having fixed loading methods for their files (which can be changed using a specified parameter for user preference). Target Audience This module is tailored for users who seek for loading all files in a given path/directories on the go. Reducing the use for context managers when wanting to simply retrieve some data from the file. I mainly built this project because I tend to load files all the time mainly just for retrieval purposes rather writing or manipulating the data itself. I definitely know I am not the only one so thought I would publish it here. It was definitely fun and also fairly challenging as I had to take in account for handling many use cases that may occur all while giving the user an easy experience. I understand many might not be a fan of this considering no project related to this has been made for obvious reasons but is very handy and has many benefits to it towards efficiency. Comparison This module will dynamically obtain all the files in a given path or directories (while automatically filtering out hidden files) and load them with their respective loading method. You can also specify exactly what extensions you'd like to load and skip all other files. If any files fails to load, it will be returned as a TextWrapperIO instance. For the reason that directories will contain many or long files, I tried my best to efficiently write my code so that everything will be fast, smooth and simple. Therefore, I purposely implemented a parameter whether you'd like it as a generator or if set to False , as a dictionary (Custom made dictionary for displaying the data purposes). The ext_loaders parameter allows you to customize the loading methods for your file extensions. You can also pass in keyword arguments for the specified loaders. # Structure: {extension: {<loading method>: {<loading method kwargs>}}} ext_loaders = {\"csv\": {pd.read_csv: {\"header\": 10}}} Please note, if you prefer all files to not be mapped with any loading method but rather just returned as a TextWrapperIO object, you can pass in the no_method parameter. The class was built primarily focused on memory. The speed results are promising depending on file size being loaded as well as computer specifications of course. Use Cases For example, if the specified path or directories contains the specified file extensions of (csv, pdf, txt), the class will return a generator (or a dictionary if set to false). The csv files will be loaded with pandas.read_csv , the pdf files will be a generator from pdf.high_level.extract_pages , and the txt files will be opened and read using the tradition open method. If the object is being returned as a dictionary (when generator is set to False ), all the keys will be the full posix path by default and the values will be masked by its objects loaded type. This was implemented to prevent overflow in the terminal as all the values would contain all loaded data. - Load all files with a specified path (directory) as a Generator dl_gen = DataLoader(path=Path(__file__).parent) dl_files = dl_gen.files print(dl_files) # Output: <generator object DataLoader.files.<key-value> at 0x1163f4ba0> - Load all files with a specified path (directory) as a Dictionary  (Custom-Repr) # Disabling 'generator' and 'full_posix'(Otherwise will return the keys as the full posix path) for displaying purposes. dl_dict = DataLoader(path=Path(__file__).parent, generator=False, full_posix=False) dl_files = dl_dict.files print(dl_files) # Output: DataLoader((LICENSE.md, <TextIOWrapper>), (requirements.txt, <Str>), (Makefile, <Str>), (pyblack.toml, <Str>), (README.md, <TextIOWrapper>), (setup.py, <Str>), (MANIFEST.ini, <TextIOWrapper>), (tox.ini, <ConfigParser>), (setup.cfg, <ConfigParser>)) - Load all files from multiple directories # Disabling 'generator' and 'full_posix'(Otherwise will return the keys as the full posix path) for displaying purposes. dl = DataLoader(directories=Path(<directory_with_directories>).glob(\"*\"), generator=False, full_posix=False ) dl_dir_files = dl.dir_fiiles print(dl_fir_files) # Output: DataLoader((technologie_3.txt, <Str>), (technologie_2.txt, <Str>), (technologie_1.txt, <Str>), (technologie_5.txt, <Str>), (technologie_4.txt, <Str>), (sport_1.txt, <Str>), (sport_2.txt, <Str>), (sport_3.txt, <Str>), (8_00083.csv, <DataFrame>), (8_00082.csv, <DataFrame>), (8_00085.csv, <DataFrame>), (descriptor.json, <Dict>), (vehicles.csv, <DataFrame>), (bart-43253423.pdf, <TextIOWrapper>), (caltrain-425345423423.pdf, <TextIOWrapper>), (bart_20180908_007.pdf, <TextIOWrapper>), (space_5.txt, <Str>), (space_4.txt, <Str>), - Load all files with default extensions dl_dict = DataLoader(path=Path(__file__).parent, default_extensions=(\"csv\"), generator=False, full_posix=False) dl_files = dl_dict.files print(dl_files) Output: DataLoader((8_00083.csv, <DataFrame>), (8_00082.csv, <DataFrame>), (8_00085.csv, <DataFrame>), (vehicles.csv, <DataFrame>)) - Retrieve the files data dl_vehicles = dl_files[\"vehicles.csv\"] # Output: <DataFrame> CNR Manufacturer    Model      Fuel  Power (kW) Transmision  Weight (kg)0    2        Skoda    Fabia  gasoline          81      Manual         1312 1    3         Seat    Ibiza  gasoline         110      Manual         1376 2    4        Skoda  Octavia  gasoline         140   Automatic         1728 3    5        Skoda  Octavia  gasoline         110      Manual         1376 4    6        Skoda    Karoq  gasoline         110      Manual         1632 - Using DataMetrics Class This class was mainly built for fun and can seamlessly be used with DataLoader. It takes in an iterable of valid file paths and returns a dictionary of all the OS stats results for each file. The dictionary will also have 2 custom made keys called st_vsize and st_fsize . st_vsize : The value will contain a dictionary of the disk volume size ( shutil.disk_usage ) st_fsize : The value will be a NamedTuple instance called Stats which will contain 3 values. Stats(symbolic='0.26 KB', calculated_size=0.2607421875, bytes_size=267) Please note, if passing in a DataLoader.files/dir_files instance, full_posix must be enabled to ensure all the paths are absolute and no errors are raised. full_posix is enabled by default, but can be disabled with DataMetrics for displaying purposes (disabled by default). - Load All File Stats dl_gen = DataLoader(path=Path(__file__).parent) dl_files = dl_gen.files dm = DataMetrics(files=dl_files, full_posix=False) dm_stats = dm.all_stats print(dm_stats) # Output: DataMetrics((8_00083.csv, <Dict>), (8_00082.csv, <Dict>), (8_00085.csv, <Dict>), (vehicles.csv, <Dict>)) dm_vehicles = dm_stats[\"vehicles.csv\"] print(dm_vehicles) <dictionary of all stats. For representation purposes, going to pass the custom dictionary to it.> print(DataLoader.special_repr(dm_vehicles, module=\"DataMetrics\")) # Output: DataMetrics((st_atime, <Stats>), (st_ctime, <Stats>), (st_dev, <Stats>), (st_gid, <Stats>), (st_ino, <Stats>), (st_mode, <Stats>), (st_mtime, <Stats>), (st_nlink, <Stats>), (st_size, <Stats>), (st_uid, <Stats>), (st_fsize, <Stats>), (st_vsize, <Dict>)) print(dm_vehicles[\"st_fsize\"]) # Output: NamedTuple Instance Stats(symbolic='0.26 KB', calculated_size=0.2607421875, bytes_size=267) - Export All Files Stats dm_stats.export_stats() # A JSON file containing the stats for each file. - Retrieve the Calculated Total Size # This method calculates the total size by summing the st_size bytes size for all specified files print(dm_stats.total_stats) # Output: Stats(symbolic='165.93 KB (Kilobytes)', calculated_size=165.9345703125, bytes_size=169917) Project Limitations and Downsides While the project offers valuable features, it's essential to highlight a few limitations for a comprehensive understanding. 1. Path and Directories Redundancy - The inclusion of both ` path ` and ` directories ` parameters may seem redundant and could be streamlined into a single parameter. Presently, users need to use the ` files ` property method when specifying the path or the ` dir_files ` property method when directories are specified. 2. Dynamic File Mapping -  The current implementation does not allow users to explicitly specify which files to map the loaders with. The mapping is dynamic, meaning that the loader specified for a particular extension will be applied universally. This project is meant to be quick and easy for data retrieval keep in mind. 3. Limited Handling of Duplicate Filenames - When full_posix is disabled, and multiple files share the same name, only one of the files will be retrieved. Users should be aware of this behavior and consider passing in full_posix or simply load the file using the classes load_file method. If expectations are not meant, consider loading the file(s) using the traditional way for now."},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Codeflash - Optimize your code's performance automatically", "Author": "u/ml_guy1", "Content": "Hi! I am Saurabh. I love writing fast programs and I've always hated how slow Python code can sometimes be. To solve this problem, I have created Codeflash. What My Project Does codeflash is a Python package that uses AI to figure out the most performant way to rewrite a Python code. It not only optimizes the performance but also verifies the correctness of the new code, i.e. makes sure that the new code follows exactly the same behavior as your original code. This automates the manual optimization process. It can improve algorithms, data structures, fix logic, use better optimized libraries etc to speed up your code. Website - https://www.codeflash.ai/ , get started here. PyPi - https://pypi.org/project/codeflash/ If you have a Python project, it should take you less than 5 minutes to setup codeflash - pip install codeflash and codeflash init Codeflash can also optimize your entire project! Run codeflash --all after setting up codeflash, and codeflash will optimize your project, function by function, and create PRs on GitHub when it finds an optimization. This is super powerful. You can also install codeflash as a GitHub actions check that runs on every new PR you create, to ensure that all new code is performant. If codeflash finds that a code can be made more performant, it will create a PR comment with the new optimized code. This ensures that your project stays at peak performance everytime. How it works Codeflash works by optimizing the code path under a function. So if there is a function foo(a, b): , codeflash finds the fastest implementation of the function foo and all the other functions it calls. The optimization procedure preserves the signature of the function foo and then figures out a new optimized implementation that results in exactly the same return values as the original foo. The behavior of the new function is verified to be correct by running your unit tests and generating a bunch of new regression tests. The runtime of the new code is measured and the fastest one is recommended. Target Audience Codeflash is currently the best at optimizing pure-functions without side effects. You can use codeflash to improve performance of any custom algorithm, numpy code, pandas code, data processing code etc. It is very general purpose. You should be able to optimize anything that can be unit-tested. You can also try to optimize non-pure functions but you should review the new code. We are improving support to more types of functions. Would love to know about your use case and how we can support it! Comparison I am currently unaware of any direct comparison with codeflash on optimizing performance of user level code. Codeflash is still early but has gotten great results already by optimizing open source projects like Langchain and many others. I would love you to try codeflash to optimize your code and let me know how you use it and how we can improve it for you! Thank you."},
{"Title": "Pycharm SQLAlchemy Plugin adds typing support for Mapped columns", "Author": "u/riksi", "Content": "This has been a long standing issue on Pycharm and nobody on the company seems interested in it. Looks like somebody went ahead and created a plugin that from a quick check looks like it works fine: I did not make this. https://github.com/souperk/pycharm-sqlalchemy-plugin"},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "PySTAC Decoded: A Step-by-Step Tutorial", "Author": "u/m_razali", "Content": "If you work with spatio temporal data like satellite imagery, SAR, Point Clouds, etc. Try to use Spatio Temporal Data Catalog (STAC). Take a look this tutorial that will explain step by step approach how to use STAC in Python from searching data, display it and do analysis the data in a row. https://www.geodose.com/2024/02/pystac-decoded-step-by-step-tutorial.html"},
{"Title": "I Remade Infinite Craft But In PySide6 And You Can Create Your Own Versions With it", "Author": "u/FUS3N", "Content": "Hello, I'd like to share a recent project of mine. infinite Sides! Screenshots Project Repo: https://github.com/Fus3n/infinite-sides What my project does I recently played the game called Infinite Craft and it was quite fun to see how you can craft anything from any word you can check out the original game here . This game uses LLM to generate words based on a combination of other words or two words, you get a few words that you start with and you can go with it and come up with infinite combinations (well as far as an LLM can go in theory, its not perfect). So I decided to make it in Python and PySide6 right after discovering Ollama , which is a way to run local LLMs pretty fast. So you can just boot up Ollama and my app will use its local API  to communicate with LLMs to generate texts in your computer, so no need for the internet , but its main feature is that you can just create your OWN infinitely crafting game, for example, a Minecraft themed one, Example Here So you can basically create any kind of game! You can also forward the port Ollama is hosted on by default using ngrok and share the link with other people which they can use to play the game in infinite-sides, all they have to do is change the base URL in settings, so you can give access to your model to your friends if they don't have a beefy computer to run LLMs. Target audience If you enjoy games like this you also might enjoy this one! Comparison I don't know If there are already any apps/games like this either way this is my version just a project I did and hope people can enjoy it! All instructions to run and use it are in the README, there is currently no binaries but will be in the future. Also, I am not good with \"prompt engineering\" so the current default system prompt is pretty bad and might not give the best experience, So I suggest you edit it in the settings and try different variations if you want to give the game a try."},
{"Title": "Python Certification Guide- What should we include in it to make it super useful?", "Author": "u/entreluvkash", "Content": "Our company is currently developing a book/guide for the PCEP certification. While there are numerous resources available, we aim to create a comprehensive book covering essential aspects such as tips and tricks, example questions, brain teasers, and case studies. I'd love to hear your suggestions for additional content that would be particularly valuable and not commonly found in existing resources on this certification."},
{"Title": "Balancing chemical equations with Python", "Author": "u/chicolucio", "Content": "Recently, I wrote a blog post on ChemPy, a Python package, for balancing chemical equations. I hope you find it useful. https://chemistryprogramming.xyz/2024/02/chempy-balancing-equations-python/"},
{"Title": "Resource management rant: Current concepts not sufficient", "Author": "u/Wild_Meeting1428", "Content": "Why do, garbage collected languages like python, not implement resource management concepts in a sane way? C++ and Rust have RAII, which is awesome. Python has __enter__ , __exit__ , with ... as ... , try: ... finally: . But that is extremely limited: Returning a resource prevents the user to use the safe with statement, the resource is closed. And even if it would work, how should the caller know? Since python never cared, this would be an improvement. On top writing with for all resources, especially when working with resources, creating resources, is very cumbersome. The code is blown up by with or try ... finally/catch blocks. And it is still unsafe because you will forget to use these wherever they are required. The best solution to this would be, to clean up all uncatched resources automatically on scope exit. Python already knows that they are resources, since they implement __enter__ and __exit__ . Python just has to prevent the creation of strong references from resources. Weak references should still creatable with weakref . In the end this would be only syntactic sugar, but it would make the resource management way safer and on top more convenient: f = open(__file__, \"r\") some_code_may_throw(f) return f would compile to f = open(__file__, \"r\") try: some_code_may_throw(f) return f except: f.close() raise"},
{"Title": "Online tool for visualizing regex matches in Python", "Author": "u/yogeshkd", "Content": "It highlights all matches including groups so you can quickly test regex patterns: https://python-fiddle.com/tools/regex"},
{"Title": "SuiteSpotAuth: A simple authentication wrapper for SuiteSpot", "Author": "u/yhavin", "Content": "What my project does suitespotauth provides a simple wrapper for the authentication flow required by SuiteSpot construction management software. It exposes a secure access_token attribute which you need (for Bearer Authorization) to make API calls to get your construction data from SuiteSpot. This cuts out all the boilerplate flow required of creating Basic Authorization to generate an API token and exchange it for the required access token. GitHub: https://github.com/yhavin/suitespotauth Homepage: https://pypi.org/project/suitespotauth Target audience Available for use in production. The target audience is for developers who want to make API calls to SuiteSpot analytics to retrieve their construction data, but do not want to write 100 lines of code just to get the access token. Comparison There is no known other package providing a wrapper to the authentication flow. The official authentication docs are available at https://auth.suitespot.io/api#/ . Development This package is being actively developed, and all replies/issued will be read. Thank you!"},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Tyche: Supercharge Your Hypothesis Testing", "Author": "u/Different_Hippo7511", "Content": "Note , Tyche is an active research project and we're looking for users to help us do a formal evaluation study. If you're interested in helping out and giving feedback on the tool, please fill out this Google form . First of all, if you haven't heard of Hypothesis , check it out. It's a \"library for creating unit tests which are simpler to write and more powerful when run, finding edge cases in your code you wouldn‚Äôt have thought to look for\" (from the docs) and it's my favorite way to test Python code. But if you're a long-time user of Hypothesis, you've likely run into a situation where a Hypothesis test wasn't actually doing what you thought it was doing. Maybe you thought you were testing on positive integers, but you were actually wasting time generating lots of negative ones too. Or maybe a from_type generator just didn't produce the values you thought it would. Sometimes Hypothesis tells you about these mistakes, but often you don't notice until a bug made it into production. What My Project Does Tyche is an open source ( source ) Visual Studio Code extension that helps you analyze your Hypothesis tests and make sure they're exactly as thorough as you expect. The interface shows you charts that summarize the distribution of data used to test your code and helps to warn you if things seem off. The extension has already helped the Hypothesis team to fix a couple of bugs in the Hypothesis framework itself, and it may be able to help you improve your own test suites too. Target Audience Users of Hypothesis. Even if you're pretty confident that your tests are already as effective as they can be, it's worth using Tyche to double check. Comparison As far as I know, there aren't other tools like Tyche out there yet. This project is part of cutting-edge research at a major university."},
{"Title": "remoteio - A remote GPIO library for the Raspberry Pi", "Author": "u/Saltibarciai", "Content": "Hi guys! I created my very first python lib. It's a server/client solution that allows to control  GPIO pins of a remote Raspberry Pi. GitHub: https://github.com/schech1/remoteio Homepage: https://pypi.org/project/remoteio/ What My Project Does remoteio creates a socket and can run as a deamon at startup. The remoteio python lib provides gpiozero-like commands to control outputs on the remote Raspberry Pi. Target Audience Makers who want to control the GPIO outputs of one or more remote Raspberry Pis. Comparison The previously known remote GPIO option of Raspberry Pi OS is provided by pigpiod , which does not work on the Raspberry Pi 5 anymore. The project is still small and currently only offers to toggle outputs. I plan to add also inputs and I2c/SPI in future updates. I appreciate your support!"},
{"Title": "RenderCV v1 is released! Create an elegant CV/resume from YAML.", "Author": "u/egehancry", "Content": "I released RenderCV a while ago with this post . Today, I released v1 of RenderCV, and it's much more capable now. I hope it will help people to automate their CV generation process and version-control their CVs. What My Project Does RenderCV is a LaTeX CV/resume generator from a JSON/YAML input file. The primary motivation behind the RenderCV is to allow the separation between the content and design of a CV . It takes a YAML file that looks like this: cv: name: John Doe location: Your Location email: youremail@yourdomain.com phone: tel:+90-541-999-99-99 website: https://yourwebsite.com/ social_networks: - network: LinkedIn username: yourusername - network: GitHub username: yourusername sections: summary: - This is an example resume to showcase the capabilities of the open-source LaTeX CV generator, [RenderCV](https://github.com/sinaatalay/rendercv). A substantial part of the content is taken from [here](https://www.careercup.com/resume), where a *clean and tidy CV* pattern is proposed by **Gayle L. McDowell**. education: ... And then produces these PDFs and their LaTeX code: classic theme sb2nov theme moderncv theme engineeringresumes theme Example PDF , Example PDF Example PDF Example PDF Corresponding YAML Corresponding YAML Corresponding YAML Corresponding YAML It also generates an HTML file so that the content can be pasted into Grammarly for spell-checking. See README.md of the repository. RenderCV also validates the input file, and if there are any problems, it tells users where the issues are and how they can fix them. I recorded a short video to introduce RenderCV and its capabilities: https://youtu.be/0aXEArrN-_c Target Audience Anyone who would like to generate an elegant CV from a YAML input. Comparison I don't know of any other LaTeX CV generator tools implemented with Python."},
{"Title": "Pyright plugin for PyCharm", "Author": "u/InSyncWithFoo", "Content": "I just wrote a plugin that runs Pyright on-the-fly as you code in PyCharm. This is just an MVP and hasn't been battle-tested, so basically I'm asking for help. Please try it out and let me know what can be improved. Version 0.1.0 will be released on the Marketplace once PyCharm 2024.1 is released. Until then I'll try to make it as good as possible. Target audience PyCharm users who also use Pyright. Comparison Instead of arguing with your colleagues about whether you should switching to VSCode for better type checking, now you can enjoy the power of Pyright directly in PyCharm. What it does More or less the same thing as running pyright path/to/your-file.py in the terminal, but automatically."},
{"Title": "sqlcipher3-wheels: Wheels for sqlcipher3, 100% less pain during install (Windows, MacOS, Linux)", "Author": "u/laggykiller", "Content": "PyPI: https://pypi.org/project/sqlcipher3-wheels/ GitHub: https://github.com/laggykiller/sqlcipher3 What My Project Does sqlcipher3 is a python3 binding for SQLCipher maintained by coleifer. Officially it only provides manylinux wheels for x86_64 and bundles with SQLCipher version 3 (From sqlcipher3-binary ). sdist does not work if sqlcipher is not installed in system, as it does not contain source of sqlcipher. It is quite a pain to compile, especially on Windows due to dependency to openssl . I decided to create my own fork that build sqlcipher3 wheels with cibuildwheel. The wheels are compiled with sqlcipher version 4. Wheels are available for all platforms (Windows, MacOS, manylinux, musllinux) and all supported architectures (x86_64, i686, aarch64, ppc64le, s390x). openssl is installed with conan, and sdist now bundles with source of sqlcipher so it could be built even if the system does not have openssl and sqlcipher installed. You can download the wheels by pip install sqlcipher3-wheels . Hope this reduces your pain and suffering! Target Audience Those who want to use sqlcipher3 without compiling by themselves Comparison sqlcipher3 is where I have forked from, which only provides manylinux x86_64 wheels built with sqlcipher version 3. The sdist does not contain source of sqlcipher and cannot compile if openssl is absent."},
{"Title": "Financial-Analyzer (Python CLI money analyzer / Mint alternative)", "Author": "u/manutoe", "Content": "Hey all, I am making this post to highlight a video I just published on YouTube about a Python CLI (command-line interface) financial analyzer application I have developed. What my project does The project in question is meant to analyze spending, record transactions, and view balances/investments. I think it's a great alternative to something like Mint (especially because Mint is shutting down!) Yes, you will likely need programming experience to use it cleanly but hey that's why I'm posting in r/Python :) Target Audience Anyone who wants to track their spending would find value in this project. If you like applications like Mint, you will like this project. Comparison One similar project is LedgerCLI . However this project is more for the pure accounting side and doesn't involve the budgeting / investment aspect that I think of when I think about examining personal finance. References Please checkout this recent post and this post on the GUI version And checkout the YouTube video here ! GitHub Repo Github Link"},
{"Title": "Introducing LIVA: Your Local Intelligent Voice Assistant", "Author": "u/Automatic-Net-757", "Content": "Hey Redditors, What My Project Does I'm excited to introduce you to LIVA (Local Intelligent Voice Assistant), a side project I've been working on that brings the power of voice assistants right to your terminal! It uses LLMs, Text to Speech and Speech to Text to create a voice assistant Target Audience The project is intended for developers, tech enthusiasts who are working with Large Language Models would quickly like to run a voice chat with their favorite Large Language Model Comparison So far I have not found anything that does speech to text in almost real time combined with LLM and a Speech Generator. These were available as separate projects but did not quite found a project that combines all three (I did find voice assistants which use LLMs and Speech Generators but they were lacking near real-time speech to text capabilities) Key Features: Here's what you can expect from LIVA: üé§ Speech Recognition : LIVA accurately transcribes your spoken words into text, making interaction seamless and intuitive. By default whisper-base.en is being used for speech recognition üí° Powered by LLM : Leveraging advanced Large Language Models, LIVA understands context and provides intelligent responses to your queries. By default the parameters are set to Mistral:Instruct with Ollama Endpoint, but the model can be easily changed, you can use any OpenAI compatible endpoint üîä Text-to-Speech Synthesis : LIVA doesn't just understand ‚Äì it speaks back to you! With natural-sounding text-to-speech synthesis, LIVA's responses are clear and human-like. For the TTS, I'm going with the SpeechT5 üõ†Ô∏è Customizable and User-Friendly : With customizable settings and an intuitive interface, LIVA adapts to your preferences and needs, making it easy to use for everyone. Right now, you can customize the LLM and the TTS model (It accepts the variants of Whisper) Let's say you want to use the openhermes from ollama with whisper-small.en. Then you just simply run python --model-id openhermes --stt-model openai/whisper-small.enmain.py Running the python main.py will look for the whisper-base.en and will download if it isn't present. And coming to the model, by default it looks for the Mistral:Instruct on the Ollama Endpoint Contributions But here's where you come in ‚Äì I want your input! Your feedback, suggestions, and ideas are invaluable in making LIVA even better. Whether you're a developer, a tech enthusiast, or simply curious to try it out, your voice matters. Here's how you can get involved: Try It Out: Head over to GitHub to check out the project code. Install it, give it a try, and let me know what you think. Feedback and Suggestions: Have ideas for new features or improvements? Found a bug? Share your thoughts by submitting feedback on GitHub. Your input helps shape the future of LIVA. Spread the Word: Know someone who might benefit from LIVA? Share it with them! The more people who use and contribute to LIVA, the stronger the community becomes. Collaborate: Interested in contributing code, documentation, or ideas? Fork the repository, make your changes, and submit a pull request. Let's collaborate and make LIVA the best it can be. I'm excited about the potential of LIVA, and I can't wait to see where this journey takes us. Together, let's create a voice assistant that's intelligent, accessible, and tailored to our needs. Got questions, ideas, or just want to chat about LIVA? Drop a comment below or reach out to me directly. Your input is what makes LIVA great! (P.S. If you're interested in joining the LIVA project and contributing, check out the suggestions above!)"},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "A simple comic strip API wrapper", "Author": "u/dumblechode", "Content": "I'd like to present my simple GoComics API wrapper . What My Project Does This PyPI package allows for easy searching, downloading, and displaying of comic strips directly from GoComics. Target Audience This package is intended for developers and comic enthusiasts looking to integrate comic strips into their websites or projects. Comparison The only other comparison I can find is a repository to download Marvel comics, last updated 8 years ago. Key Features: Simple Installation: Get started with pip install comics. Versatile Usage: Find specific comics by date or randomly explore the large collection from GoComics, including classics like Calvin and Hobbes . Easy Integration: Add comic strips to your personal website, create a curated collection, or enjoy them in Jupyter notebooks. This wrapper supports Python 3.7+, is open for contributions on GitHub, and is licensed under MIT. For more details, check out the GitHub repository !"},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Rybak 0.3 - library for generating directory trees", "Author": "u/walkie-talkie24", "Content": "https://github.com/python-lapidary/rybak What my project does Rybak is a directory-tree generator library. It accepts directory-tree structure, where file content as well as file and directory names can be templates (Jinja or Mako), applies client provided data, and generates a directory structure. Target audience It's intended for use in any project that needs to generate complex directory structures from templates. I keep it as a helper library for my OpenAPI client generator. Comparison It's inspired by Cookiecutter and Copier. Compared to them: focuses on delivering a clean public API and not a CLI or interacting with user can generate more complex directory structures, where a single template file can be used to generate multiple output files Reads templates from importlib.resources works with Jinja as well as Mako templates (only one engine for a template directory)"},
{"Title": "A python hacky script to scan rdp port 3389", "Author": "u/Technical_Shelter621", "Content": "Wannabe Security Researcher!?!? Is the title of my very first blog post of my very first blog, I hope it to be informative for who is interested in Security and more specifically about an home assignment I received for a position of Sr. Security Researcher and how I approached it. Thanks for reading https://cyberroute.github.io/post/2024-02-23-rdpscan/"},
{"Title": "DataLoader - Dynamically load all files.", "Author": "u/yousefabuz", "Content": "GitHub: DataLoader What My Project Does DataLoader is a comprehensive utility that facilitates the efficient loading and processing of data from a specified path (directory) or all data from multiple directories. This project is designed to be user-friendly and easy to integrate into your projects. Please note this project may not be suited for everyone as most users do not prefer dynamically loading their files/data but rather traditionally. Along with users not prefer having fixed loading methods for their files (which can be changed using a specified parameter for user preference). Target Audience This module is tailored for users who seek for loading all files in a given path/directories on the go. Reducing the use for context managers when wanting to simply retrieve some data from the file. I mainly built this project because I tend to load files all the time mainly just for retrieval purposes rather writing or manipulating the data itself. I definitely know I am not the only one so thought I would publish it here. It was definitely fun and also fairly challenging as I had to take in account for handling many use cases that may occur all while giving the user an easy experience. I understand many might not be a fan of this considering no project related to this has been made for obvious reasons but is very handy and has many benefits to it towards efficiency. Comparison This module will dynamically obtain all the files in a given path or directories (while automatically filtering out hidden files) and load them with their respective loading method. You can also specify exactly what extensions you'd like to load and skip all other files. If any files fails to load, it will be returned as a TextWrapperIO instance. For the reason that directories will contain many or long files, I tried my best to efficiently write my code so that everything will be fast, smooth and simple. Therefore, I purposely implemented a parameter whether you'd like it as a generator or if set to False , as a dictionary (Custom made dictionary for displaying the data purposes). The ext_loaders parameter allows you to customize the loading methods for your file extensions. You can also pass in keyword arguments for the specified loaders. # Structure: {extension: {<loading method>: {<loading method kwargs>}}} ext_loaders = {\"csv\": {pd.read_csv: {\"header\": 10}}} Please note, if you prefer all files to not be mapped with any loading method but rather just returned as a TextWrapperIO object, you can pass in the no_method parameter. The class was built primarily focused on memory. The speed results are promising depending on file size being loaded as well as computer specifications of course. Use Cases For example, if the specified path or directories contains the specified file extensions of (csv, pdf, txt), the class will return a generator (or a dictionary if set to false). The csv files will be loaded with pandas.read_csv , the pdf files will be a generator from pdf.high_level.extract_pages , and the txt files will be opened and read using the tradition open method. If the object is being returned as a dictionary (when generator is set to False ), all the keys will be the full posix path by default and the values will be masked by its objects loaded type. This was implemented to prevent overflow in the terminal as all the values would contain all loaded data. - Load all files with a specified path (directory) as a Generator dl_gen = DataLoader(path=Path(__file__).parent) dl_files = dl_gen.files print(dl_files) # Output: <generator object DataLoader.files.<key-value> at 0x1163f4ba0> - Load all files with a specified path (directory) as a Dictionary  (Custom-Repr) # Disabling 'generator' and 'full_posix'(Otherwise will return the keys as the full posix path) for displaying purposes. dl_dict = DataLoader(path=Path(__file__).parent, generator=False, full_posix=False) dl_files = dl_dict.files print(dl_files) # Output: DataLoader((LICENSE.md, <TextIOWrapper>), (requirements.txt, <Str>), (Makefile, <Str>), (pyblack.toml, <Str>), (README.md, <TextIOWrapper>), (setup.py, <Str>), (MANIFEST.ini, <TextIOWrapper>), (tox.ini, <ConfigParser>), (setup.cfg, <ConfigParser>)) - Load all files from multiple directories # Disabling 'generator' and 'full_posix'(Otherwise will return the keys as the full posix path) for displaying purposes. dl = DataLoader(directories=Path(<directory_with_directories>).glob(\"*\"), generator=False, full_posix=False ) dl_dir_files = dl.dir_fiiles print(dl_fir_files) # Output: DataLoader((technologie_3.txt, <Str>), (technologie_2.txt, <Str>), (technologie_1.txt, <Str>), (technologie_5.txt, <Str>), (technologie_4.txt, <Str>), (sport_1.txt, <Str>), (sport_2.txt, <Str>), (sport_3.txt, <Str>), (8_00083.csv, <DataFrame>), (8_00082.csv, <DataFrame>), (8_00085.csv, <DataFrame>), (descriptor.json, <Dict>), (vehicles.csv, <DataFrame>), (bart-43253423.pdf, <TextIOWrapper>), (caltrain-425345423423.pdf, <TextIOWrapper>), (bart_20180908_007.pdf, <TextIOWrapper>), (space_5.txt, <Str>), (space_4.txt, <Str>), - Load all files with default extensions dl_dict = DataLoader(path=Path(__file__).parent, default_extensions=(\"csv\"), generator=False, full_posix=False) dl_files = dl_dict.files print(dl_files) Output: DataLoader((8_00083.csv, <DataFrame>), (8_00082.csv, <DataFrame>), (8_00085.csv, <DataFrame>), (vehicles.csv, <DataFrame>)) - Retrieve the files data dl_vehicles = dl_files[\"vehicles.csv\"] # Output: <DataFrame> CNR Manufacturer    Model      Fuel  Power (kW) Transmision  Weight (kg)0    2        Skoda    Fabia  gasoline          81      Manual         1312 1    3         Seat    Ibiza  gasoline         110      Manual         1376 2    4        Skoda  Octavia  gasoline         140   Automatic         1728 3    5        Skoda  Octavia  gasoline         110      Manual         1376 4    6        Skoda    Karoq  gasoline         110      Manual         1632 - Using DataMetrics Class This class was mainly built for fun and can seamlessly be used with DataLoader. It takes in an iterable of valid file paths and returns a dictionary of all the OS stats results for each file. The dictionary will also have 2 custom made keys called st_vsize and st_fsize . st_vsize : The value will contain a dictionary of the disk volume size ( shutil.disk_usage ) st_fsize : The value will be a NamedTuple instance called Stats which will contain 3 values. Stats(symbolic='0.26 KB', calculated_size=0.2607421875, bytes_size=267) Please note, if passing in a DataLoader.files/dir_files instance, full_posix must be enabled to ensure all the paths are absolute and no errors are raised. full_posix is enabled by default, but can be disabled with DataMetrics for displaying purposes (disabled by default). - Load All File Stats dl_gen = DataLoader(path=Path(__file__).parent) dl_files = dl_gen.files dm = DataMetrics(files=dl_files, full_posix=False) dm_stats = dm.all_stats print(dm_stats) # Output: DataMetrics((8_00083.csv, <Dict>), (8_00082.csv, <Dict>), (8_00085.csv, <Dict>), (vehicles.csv, <Dict>)) dm_vehicles = dm_stats[\"vehicles.csv\"] print(dm_vehicles) <dictionary of all stats. For representation purposes, going to pass the custom dictionary to it.> print(DataLoader.special_repr(dm_vehicles, module=\"DataMetrics\")) # Output: DataMetrics((st_atime, <Stats>), (st_ctime, <Stats>), (st_dev, <Stats>), (st_gid, <Stats>), (st_ino, <Stats>), (st_mode, <Stats>), (st_mtime, <Stats>), (st_nlink, <Stats>), (st_size, <Stats>), (st_uid, <Stats>), (st_fsize, <Stats>), (st_vsize, <Dict>)) print(dm_vehicles[\"st_fsize\"]) # Output: NamedTuple Instance Stats(symbolic='0.26 KB', calculated_size=0.2607421875, bytes_size=267) - Export All Files Stats dm_stats.export_stats() # A JSON file containing the stats for each file. - Retrieve the Calculated Total Size # This method calculates the total size by summing the st_size bytes size for all specified files print(dm_stats.total_stats) # Output: Stats(symbolic='165.93 KB (Kilobytes)', calculated_size=165.9345703125, bytes_size=169917) Project Limitations and Downsides While the project offers valuable features, it's essential to highlight a few limitations for a comprehensive understanding. 1. Path and Directories Redundancy - The inclusion of both ` path ` and ` directories ` parameters may seem redundant and could be streamlined into a single parameter. Presently, users need to use the ` files ` property method when specifying the path or the ` dir_files ` property method when directories are specified. 2. Dynamic File Mapping -  The current implementation does not allow users to explicitly specify which files to map the loaders with. The mapping is dynamic, meaning that the loader specified for a particular extension will be applied universally. This project is meant to be quick and easy for data retrieval keep in mind. 3. Limited Handling of Duplicate Filenames - When full_posix is disabled, and multiple files share the same name, only one of the files will be retrieved. Users should be aware of this behavior and consider passing in full_posix or simply load the file using the classes load_file method. If expectations are not meant, consider loading the file(s) using the traditional way for now."},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "A Visual Basic for Applications precompiler written in python.", "Author": "u/TheRealBeakerboy", "Content": "I‚Äôve just created an official release of a VBA precompiler written in python. What my Project Does For those who don‚Äôt know, in VBA you can have precompiler blocks to change things for different OSs or VBA versions. For example, to change the function signature for a new windows version: #if Win64 Then Function foo(Bar, Baz) #Else Function Foo(Bar) #Endif The problem with this is, if you want to scan raw source code, you can‚Äôt just ignore the precompiler lines, because now the code looks like you‚Äôve tried to define the same function twice, and one was never ended. This tool takes environment variables that the user provides, and will comment out any lines that need to be skipped, creating 100% valid precompiled code. It even performs the comparisons, and arithmetic operations specified in the VBA specification. Target Audience People interested in malware prevention, static analysis, and Linting may find it helpful. Also, useful if you are interested in learning about compilers, ANTLR, and code parsing. Limitations It is currently missing the standard library functions, like Cbool(), Abs(), etc. I‚Äôm guessing these are never called by users in the precompiler phase."},
{"Title": "PyQuest: Python everything Cheatsheet and a Journey to the land of Python programming", "Author": "u/veny_xd", "Content": "Hi all! I've made another Python cheat sheet tutorial. Yeah yeah, nothing new, I know. But here's the thing: What My Project Does The main idea was not just to write a wall of text telling about everything, but to make it interactive. So that everything would have its example code snippet, which you could change, run, and see how it worked. And not somewhere in a web version, but on your own computer, in your own environment. Fortunately, Python has the perfect tool for this - the Jupyter Notebook. That's why all chapters are written as separate notebooks and there is an example for each point (well, almost). Target Audience I originally aimed at complete beginners and tried to go from simple to complex, but I think I overdid it at some point. So it might be just as suitable to refresh knowledge and fill in the gaps for anyone. What else It also has some useful information for Python-developer interview preparation and a link to the absolutely wonderful Tech Interview Handbook for Coding Interview preparation in case anyone missed it. I would very welcome any constructive criticism, any advice, and of course contributions and GitHub stars :) A little less, but still I will be glad to unreasonable hatred and comments that nobody needs it and that there are hundreds of similar projects on the Internet. Check out the project on GitHub: PyQuest"},
{"Title": "Hyperdiv: Reactive web UI framework for Python", "Author": "u/Hyperdiv-io", "Content": "Hi guys! I'd like to share a reactive web UI framework I've been working on for a while that I made public a couple of days ago. GitHub: https://github.com/hyperdiv/hyperdiv Homepage: https://hyperdiv.io There is a short coding demo video and intro article on the website. What My Project Does Hyperdiv is a way to build reactive UIs in pure Python quickly, with a built-in UI component system based on Shoelace ( https://shoelace.style ), markdown, and charts based on Chart.js ( https://chartjs.org ). It uses immediate-mode syntax which enables seamlessly blending declarative UI code with Python logic and event handling. Target Audience The aim of Hyperdiv is to reduce tool and language complexity when building full stack apps, and enable people to get to a working UI very quickly. I think it is a good fit for adding browser UIs to CLI tools, prototyping UIs, and internal tools. You can also put it behind Nginx and deploy it on the internet. Comparison Hyperdiv adds to a niche currently occupied by Streamlit, Reflex.dev , PyWebIO, PyJS, etc. -- frameworks that let you build web apps in pure Python. Hyperdiv stands apart with a unique blend of immediate-mode UI + reactive state, and letting you build fairly unrestricted, arbitrarily nested UI layouts with terse syntax. I appreciate your support!"},
{"Title": "Dispatch: Stateful Functions for Python", "Author": "u/achille-roussel", "Content": "Hello everyone! We (4 people startup) open-sourced an SDK to create Stateful Functions in Python! Github: https://github.com/stealthrocket/dispatch-sdk-python Docs: https://docs.stealthrocket.cloud Package: https://pypi.org/project/dispatch-functions What My Project Does We're developing Dispatch, a cloud service that helps developers create reliable systems. It can be used to build real-time and durable workflows, data pipelines, highly parallel jobs, and much more. Say you have a background job that fails and needs to be retried; getting the retry logic right is difficult, and it gets even harder when the retries need to happen across a fleet of instances and in a way that doesn‚Äôt take down the application. However, the biggest pain point is managing the state of the system, which often requires adding queues, databases, or workflow engines. With the SDK we released, we are giving Python developers a simple solution to these problems: Stateful Functions. Audience This SDK is useful to Python developers who build production services and need solutions for state management of their background jobs, data streams, workflows, etc... Comparison Dispatch addresses technical problems that are sometimes solved with cloud products like AWS Step Functions, Azure Durable Functions, or Temporal. Dispatch focuses on being lightweight and simple : there is no heavy framework to bring into the application, no cloud component to deploy, no workflow language to learn. The code can run in a serverless deployment, a container, or really anywhere. Python developers can add it to an existing application without having to change anything about the rest of the code. Technical Details At the core of the SDK is an event loop integrating with native Python coroutines (async/await) that supports pausing execution and serializing the state. On resumption, the coroutine state is recreated to continue execution, which could happen in a different instance of the application! Capturing stack frames was the most difficult part: even if Python has native support for pickling values, some objects like generators or coroutines cannot be serialized by default. To work around this limitation, we developed a native extension for CPython that allows us to save and reload the internal interpreter state. Feedback The project is young, but we're really excited to share it. Check it out and let us know what you think!"},
{"Title": "New Package for Continued Fractions", "Author": "u/LeftAcanthocephala23", "Content": "I've published a new package for people interested in working with mathematical objects called continued fractions in an object-oriented way with Python. https://pypi.org/project/continuedfractions/ What My Project Does? The package has been designed to take advantage of an existing Python standard library for rational numbers ( `fractions` ), and provides a way of computing continued fraction representations for any real number that can be defined in Python, and inspecting their properties (elements, convergents, segments, remainders), and also performing operations with them as normal rational numbers . Continued fractions are not part of the `fractions` standard library. Background I know it's a rather specialised topic, but I guess the first question is \"What is a continued fraction?\", and the next might be, \"What can you do with them?\". A continued fraction is a way of representing real numbers as the sum of an integer part + a nested fraction, where the denominators contain sums of integers and fractions, and those fractions then contain sums of integers and fractions, and so on. Since integers and fractions add up to rational numbers, continued fractions are ways of representing real numbers as the sum of (possibly, infinitely many) rational numbers. They are nested, hierarchical structures, which can be finite or infinite depending on the type of number being represented. For irrational numbers like pi the continued fraction representation will always be infinite, but for rational numbers (ratios of integers) it will always be finite. I don't want to go into the full details, but to answer the next question: continued fractions are really quite beautiful, interesting and useful in many ways. They are very important for the approximation of irrational numbers, which have infinite decimal expansions, with rational numbers, which have a finite decimal expansion. They are also connected to the study of special sequences and orderings of rational numbers - see the Stern-Brocot tree . Target Audience I guess people interested in continued fractions and, more generally, perhaps, number theory. Comparison I don't know of any other Python package that deals with continued fractions in the same way - I did come across one called `contfrac` , but that is not object-oriented, and when I compared the performance of its main function for computing continued fraction representations with mine it was much slower. There are other non-Python packages that can do continued fractions, obviously, like Mathematica, and who knows what else. But this has been designed for Python users in mind. Limitations Obviously the package cannot give you a complete continued fraction representation for pi or any irrational number - it gives you a finite representation that is the best possible, given the limitations of binary floating point arithmetic , as implemented in Python. For irrational numbers the representation is approximate, not exact - this will be improved in the future to allow the representations to be as exact as possible. Documentation At the moment the documentation is confined to the GitHub project README , but it is quite detailed, with lots of examples. I do plan to publish the docs separately, e.g. GitHub Pages or something like that, in later releases. Feedback All/any feedback is welcome, and so are contributions from any one interested in contributing to adding more cool features for studying continued fractions with Python."},
{"Title": "KeyCraftsman Project", "Author": "u/yousefabuz", "Content": "GitHub: KeyCraftsman What My Project Does KeyCraftsman is an innovative Python class designed to generate passcodes to your own liking. Offering an array of features such as key length specification, character exclusion, inclusion of all characters, URL-safe encoding, and exportation of the generated passkey(s). Target Audience This module is tailored for users who seek for flexibility in generating custom randomized password key(s). Comparison Many existing modules lack the flexibility needed in terms of features and often include deprecated elements. In contrast, this module stands out by offering a rich set of features compared to traditional Python password -generating modules. While the demand for such features may not be exceptionally high, I embarked on creating a modernized, medium/heavyweight version of key generation for the sheer enjoyment of exploring new possibilities. Features - Exclude Characters : Tailor your keys by excluding specific characters. - Include All Characters: Embrace diversity by including all ASCII letters, digits, and punctuation. - Unique Characters: Ensure uniqueness in generated keys or words. If words is specified, it will generate words with only unique number of letters in them. - Custom Text Wrapping: Wrap your keys with a custom separator and width for a personalized touch. - Multiple Key Generation: Efficiently generate multiple keys with a single instance. - Word Generation: Explore creative possibilities with word generation using random.SystemRandom(). - Exclusion Chart: Simplify character exclusion with the provided exclusion chart, available for printing and export. For a comprehensive overview of all features and methods, please refer to the documentation. We invite you to explore the capabilities of KeyCraftsman and hope you find joy in utilizing this modernized approach to key generation."},
{"Title": "marimo-wasm: a reactive Python notebook in the browser", "Author": "u/mmmmmmyles", "Content": "Showcasing marimo-wasm Hey everyone! We (2 developers) made marimo compatible with WebAssembly (WASM), so you can run it entirely in the browser thanks to Pyodide https://marimo.app What My Project Does Background: marimo is an open-source reactive notebook for Python, that aims to solve well-known problems with traditional Python notebooks[1]. This showcase is for marimo-wasm : a WASM build of marimo allowing the notebook to run entirely in the browser. This is done through WebAssembly and standing on the shoulder of Pyodide (h/t!) You can try out the playground: https://marimo.app/ This tool has been great for educating others through interactive examples. You can share snippets of code with just a URL. For example, here is a notebook on Bayes' Theorem . While the installable marimo library is hardened and used in production, the WASM build may have some rough edges. Still, the playground is great for sharing notebooks, data applications[2], or explorations with the community without the cost of servers to you or your audience. [1] https://docs.marimo.io/faq.html#faq-problems [2] You can hide the code and turn your notebook into an app Target Audience Data Engineers, Data Scientists, Story Tellers, Educators, and Researchers. Comparison JupyterLite - a WASM powered Jupyter running in the browser. However, it is not reactive like marimo. IPyflow - a reactive notebook for Python implemented as a Jupyter kernel. However, it is not WASM compatible. Jupyter - marimo is a reinvention of the Python notebook as a reproducible, interactive, and shareable Python program that can be executed as scripts or deployed as interactive web apps - without the need of extensions or additional infrastructure. More here ."},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "copykitten: the missing clipboard library for Python", "Author": "u/Jediroman", "Content": "What My Project Does copykitten is a clipboard library with support for text and images and a simple API. It's built around Rust arboard library. Thanks to this, it's multiplatform and doesn't require any dependencies. Target Audience Developers building CLI/GUI/TUI applications. The library has beta status on PyPI, but the underlying Rust library is pretty stable, being used in commercial projects like Bitwarden and 1Password. Comparison There are lots of other clipboard libraries for Python: pyperclip , jaraco.clipboard , pyclip , just to name a few. However, most of them are not maintained for years and require the presence of additional libraries or tools in the operating system. copykitten doesn't suffer from these shortcomings. A bit of history Throughout my years with Python there were several times when I needed to use the clipboard in my applications and every time I had to fall back to some shaky methods like asking the end user to install xclip and calling subprocess.run . This never felt great. Right now I'm making a multiplayer TUI game (maybe I‚Äôll showcase it later too :) ) , where users can copy join game codes into the clipboard to easily share it (much like Among Us). This is how I came to the idea of making such a library. I also wanted to try Rust for a long time, and so this all just clicked in my head instantly. I had fun building it and definitely had some pain too and learned a bit of nitty-gritty details about how clipboards work in different operating systems. Now I hate Windows. With this post I hope to gain some attention to the project so that I can receive feedback about the issues and maybe feature requests and spread the word that there's a modern, convenient alternative to the existing packages. Feel free to try it out: https://github.com/Klavionik/copykitten"},
{"Title": "Squirrels v0.2.0 Now Released: Create Dynamic Data Analytics APIs!", "Author": "u/squirrels-api", "Content": "Hi all! Introducing version 0.2.0 of the Squirrels Python library! Squirrels is a dbt-like framework that allows users to create REST API endpoints for dynamic data analytics. Feel free to read more about it on our website ! Following our version 0.1.0 release in our previous post here , we've recreated the framework to bring you an improved experience! Although this introduces many breaking changes, we intend to prevent breaking changes as much as possible for subsequent minor version increments (i.e. until 1.0.0). Compared to 0.1.0, the new 0.2.0 version has incorporated the following features: Flexible Lineage for Data Models Create data models (in SQL or Python) as database views or federates. Federate models provide a \"ref\" function/macro to refer to other models just like dbt! Specify a target data model for your dataset API, and Squirrels takes care of the model dependencies and execution order for you. You can query different database systems with different database views and join the results together in a federate model! Consolidated Parameters Multiple dataset APIs can share common parameters. You can specify a pool of parameter properties in one place (in Python or YAML) and specify the list of parameters that each dataset uses. Authentication You can specify a custom User model and an authentication method that integrates with your authentication source in a Python file. The user attributes can be used the affect the widget parameters or models in real time! We now support three different levels of access for datasets, with public datasets open to all, protected datasets open to both internal and external users, and private datasets only for private users. Feel free to give Squirrels a whirl by following our simple tutorial listed on our tutorial page ! And here's the link to our GitHub repo: https://github.com/squirrels-nest/squirrels Below are the sections specified in Rule 10: What My Project Does: Squirrels is a dbt-like framework that allows users to create REST API endpoints for dynamic data analytics. Again, feel free to read more about it on our website here ! Target Audience: Data Engineers, Analytics Engineers, Data Architects, Data Analysts, Data Scientists, Business Analysts, API Engineers, etc. Comparisons: We've come across several questions regarding \"What's the difference between Squirrels and <some other data tool>?\". Below are a few of the points on some commonly asked about comparisons. dbt: Used for data transformations only (Squirrels is largely inspired by dbt, but made for the real-time analytics or semantic layer use case instead) Does not support widget parameters (and does not need to for non-customer-facing data transformations) cube.js: Data models are created in YAML (for Squirrels, they're created in SQL or Python) Cannot create a lineage of multiple data models under one API endpoint No support for specifying widget parameters properties to dynamically change dataset behavior VulcanSQL No support for Python models and cannot create a lineage of multiple data models under one API endpoint Can specify parameters, but it's very \"free-form\" with limited control over the typing of parameters for the client (\"typing\" such as dropdown parameter, date parameter, etc.) Similar to Squirrels, it can have authenticated users affect the model behavior, but cannot integrate with your authentication source. Thus, we are not aware of existing tools that have the same capabilities as Squirrel does. We have tried to use a dbt-like project structure to make it easier for people to pick up."},
{"Title": "Cry Baby: A Tool to Detect Baby Cries", "Author": "u/technologicalBridges", "Content": "Hi all, long-time reader and first-time poster. I recently had my 1st kid, have some time off, and built Cry Baby What My Project Does Cry Baby provides a probability that your baby is crying by continuously recording audio, chunking it into 4-second clips, and feeding these clips into a Convolutional Neural Network (CNN). Cry Baby is currently compatible with MAC and Linux, and you can find the setup instructions in the README. Target Audience People with babies with too much time on their hands. I envisioned this tool as a high-tech baby monitor that could send notifications and allow live audio streaming. However, my partner opted for a traditional baby monitor instead. üòÖ Comparison I know baby monitors exist that claim to notify you when a baby is crying, but the ones I've seen are only based on decibels. Then Amazon's Alexa seems to work based on crying...but I REALLY don't like the idea of having that in my house. I couldn't find an open source model that detected baby crying so I decided to make one myself. The model alone may be useful for someone, I'm happy to clean up the training code and publish that if anyone is interested. I'm taking a break from the project, but I'm eager to hear your thoughts, especially if you see potential uses or improvements. If there's interest, I'd love to collaborate further‚ÄîI still have four weeks of paternity leave to dive back in! Update: I've noticed his poops are loud, which is one predictor of his crying. Have any other parents experienced this of 1 week-olds? I assume it's going to end once he starts eating solids. But it would be funny to try and train another model on the sound of babies pooping so I change his diaper before he starts crying."},
{"Title": "Extracting information (Text, Tables, Layouts) from PDFs using OCR.", "Author": "u/FlyingRaijinEX", "Content": "I've received an assignment whereby I am required to extract texts, tables, layouts, headers, titles, etc from PDFs (Multi-page). These PDFs have actual text on them and not images. So far I've tried using Camelot, PyMuPDF, and Nougat. Unfortunately, none of these modules are able to meet my client's expectations. Due to this, I've tried AWS Textract. I've showed a sample result of Textract and they immediately loved it. However, only then they mentioned that the PDFs have sensitive data and cannot be exposed via the internet. Now, they are looking to find an on-prem solution to get similar results as AWS Textract. Anyone know any kind of software/tool/python module that can be self-hosted and able to get similar results as AWS Textract? Thanks in advance."},
{"Title": "Typed FFmpeg: Type-Hinted Python Wrapper for Enhanced FFmpeg Integration", "Author": "u/lucemia51", "Content": "I'm excited to share my latest open-source project: Typed FFmpeg, a Python wrapper for FFmpeg enhanced with type hints for better code predictability and IDE support. https://github.com/livingbio/typed-ffmpeg Key Features: Type-hinted for improved developer experience. Simplified FFmpeg operations in Python. Detailed docs with examples for easy start. What My Project Does: Typed FFmpeg is an open-source Python wrapper for FFmpeg, designed to integrate multimedia processing capabilities into Python applications more seamlessly. It offers type hints throughout, improving code reliability and developer experience by providing better IDE support and error checking before runtime. Whether you're resizing videos, converting file formats, or extracting metadata, Typed FFmpeg simplifies these tasks with an intuitive, Pythonic interface. Target Audience: This project is aimed at developers working on multimedia applications, educational content, or data analysis involving video and audio processing. It's suitable for both production and experimental projects, offering a balance between ease of use and robust functionality. Comparison: Unlike existing high level FFmpeg wrappers for Python, Typed FFmpeg places a strong emphasis on type hints, which sets it apart by enhancing code quality and development speed. While other tools may provide similar functionalities, Typed FFmpeg aims to reduce the learning curve and improve code safety, making multimedia processing accessible to a broader range of Python developers, from beginners to seasoned professionals. PyAV provides detailed control over media through direct FFmpeg library bindings, catering to advanced users for in-depth manipulation. It demands a greater understanding of FFmpeg‚Äôs architecture, presenting a steeper learning curve compared to Typed FFmpeg, which focuses on ease of use and type safety. How to Help: Check out the project and star it if you like it. Try it out, report issues, or contribute. Share any feedback or suggestions for features. Appreciate your support and feedback!"},
{"Title": "python-compile - compile your python app for linux execution in Win/Mac/Linux using docker", "Author": "u/ZachVorhies", "Content": "What my project does This project will use docker to compile your one file python app into a binary using nuitka that's runnable on many linux systems. You'll need to have docker desktop installed. https://pypi.org/project/python-compile/ Target Audience Developers trying to target production services with optimized python binaries. To use this you'll need a requirements.txt file and a python file entry point that runs your app. For example: pip install python-compile python-compile --os debian --input demo\\http\\server.py --requirements requirements.txt I use nuitka under the hood. Github link: https://github.com/zackees/python-compile"},
{"Title": "Comparison of hosting platforms for Python app deployment", "Author": "u/Plus_Ad7909", "Content": "Hey all! I recently wrote a review comparing different hosting platforms for Python applications. In it, I take a look at Koyeb, Render, pythonanywhere, Fly.io, DigitalOcean App Platform, and Heroku: https://git-push-to-production.hashnode.dev/5-top-free-hosting-platforms-for-python-apps-2024 I'd love to hear about your experiences with these platforms. Which ones have you used and would recommend? Also, please feel free to share any managed hosting solutions not featured in the article. While I focused on platforms that do a lot of the heavy lifting for you, I'm open to hearing about more self-managed solutions too."},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Did you know a Python \"project\" mean you have to package it?", "Author": "u/kaydzed", "Content": "Having almost 10 YOE in Python, it took me this long to understand \"project\" in pyproject.toml means package (in JS, package.json name is more explicit): https://packaging.python.org/en/latest/glossary/#term-Project I've always thought `pyproject.toml` was intended to manage any sort of Python project especially those I don't intended to package and distribute (e.g., some script/notebook and requirements file)."},
{"Title": "Grid World Application", "Author": "u/bwe587", "Content": "What my project does: Agent reinforcement learning by interacting with a grid-base environment. This is achieved by implementing the bellman's equation. Target Audience: Initially, the intended audience for this application consisted of my fellow classmates from the    mathematical modeling course at my university. Toy or sandbox project. Comparison: User-friendly interface that allows you to create and manipulate grid-based environments effortlessly. The power to define and modify most parameters of your grid worlds at the click of a button! https://github.com/bwe587/GridWorld/"},
{"Title": "3D engine in Turtle", "Author": "u/the_nebulo", "Content": "Hi everyone! I wanted to share a side project of mine that grew into something that, to my knowledge, doesn't exist yet. My high school does basic python, and some of it involves Turtle. My friends were messing around with how to do 3D rendering in turtle, which inspired me to try the same but go full out. Over the course of 5 days, I went from rendering wireframe cubes to what eventually became a 3D engine in Turtle, with way too many features for it's own good. Project Overview 3D engine module built exclusively with turtle and mathematical imports such as math or numpy. Features include: A scene system that is similar to modern game engines (a lot simpler of course) A physical camera system, with gimbal lock prevention features A robust object system, with full transformation support and vertex management Lighting/Material framework allowing for physical light objects and materials applied to objects (experimental) Loading OBJ files directly Realtime positioning in 3D space using vectors Missing Features However, I never considered the scale of this project, so I started getting burnt out quickly, and there are several features that I would love to do, but honestly I don't have the motivation to, so I decided to share the project in it's current state. Some of the things I wish to have added are: A raycasted lighting system with shadows Proper object clipping management Parent/child system for objects alongside object groups Improved turtle management/optimisation Verifying function parameters and proper documentation (beyond docstrings) Vector3 classes instead of lists A lot more flexible vertex management/manipulation. Target Audience This project is a complete gimmick, just seeing how far the Turtle module can be taken despite it's primitive nature and extremely poor performance. Regardless, I'm proud of how far I managed to push Turtle (even if it runs at 0.3 FPS), and I would love to see anyone interested to push this gimmick as far as possible. One of my friends is even working on implementing a physics system using Turtle3D as a renderer. Comparison If this module were to somehow be taken seriously, the only partially fair comparison would be Panda3D or PyOpenGL, and in that regard Turtle3D is nothing. While Turtle3D does allow for a good range of flexibility, it's absolutely nothing like Panda3D or PyOpenGL as they are capable of so much more at a much better rate of performance and code quality. Closing Thoughts Of course, it isn't anywhere near high level or complex, let alone incredibly impressive, and I'm sure there are people in this subreddit who are far smarter than me that will see an incredible amount of miscalculations or atrocities in my code than I have already noticed. All my code is open source and on GitHub here . Feel free to improve it as you will!"},
{"Title": "Geospatial mapping : package to import layer style from qgis file to matplotlib", "Author": "u/Necessary-Rough2295", "Content": "Hello ! Long time reader, first time poster! I began coding in 2023 and I tend to work on GIS related topics. I have to edit a lot of maps for work, and I automated the process by rendering them directly from a python script. One issue I have is that I sometimes have a lot of style information to 'translate' from my normal QGis environment to matplotlib properties, and that is a unpractical process. I started to write a little package to help myself translating QGis layer style files (.qml) into a list of dict of kwargs for matplotlib. It is very basic for now, but it is already working for my use cases, and I would be interested in making it more useful for the community. Here is the git repo : https://github.com/PiouPiou974/qgis_style_files_extractor NB that GIS softwares like qgis are able to render more complex styles thant matplotlib, by example a polygon with 2 different hatches + markers, by drawing as many times as needed the same polygon with different parameters. I do the same thing in matplotlib, therefore I use lists of style kwargs dict instead of a unique dict for each style. If you are interested in this package, I would love to discuss about it with you !"},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "pySSV - Leverage the power of shader in Jupyter Notebooks", "Author": "u/space928", "Content": "I'm excited to share my Bachelor's degree project: pySSV. It allows you to write and interact with shaders in a Jupyter Notebook. Source code: https://github.com/space928/Shaders-For-Scientific-Visualisation Documentation: https://pyssv.readthedocs.io/en/latest/ If you're interested in the project I'd greatly appreciate if you help with the evaluation of this project by filling out my survey: https://forms.gle/rX7uPxaxQ1xcNVQB8 Project overview This is a library which allows you to write shaders in your Jupyter notebooks to create interactive data visualisations. It provides the following features: Custom Jupyter Widget Shader templating and preprocessor ShaderToy shader template, allowing ShaderToy shaders to be very easily used in pySSV Vertex/Pixel/Geometry shader support Vertex buffer input from NumPy arrays Texture input from NumPy arrays and Pillow Images Simple to use IMGUI library Multipass rendering Renderdoc support And more! A simple SDF shader example in pySSV , utilising the sdf shader template: import pySSV as ssv canvas = ssv.canvas() canvas.shader(\"\"\" #pragma SSV sdf sdf_main --camera_distance 2. --rotate_speed 1.5 --render_mode SOLID // SDF taken from: https://iquilezles.org/articles/distfunctions/ float sdCappedTorus(vec3 p, vec2 sc, float ra, float rb) { p.x = abs(p.x); float k = (sc.y*p.x>sc.x*p.y) ? dot(p.xy,sc) : length(p.xy); return sqrt( dot(p,p) + ra*ra - 2.0*ra*k ) - rb; } float sdf_main(vec3 p) { float t = 2.*(sin(uTime)*0.5+0.5)+0.2; return sdCappedTorus(p, vec2(sin(t), cos(t)), 0.5, 0.2); } \"\"\") canvas.run() Target audience pySSV is targeted towards shader/computer graphics enthusiasts and data scientists. A basic understanding of shaders is recommended, but through the use of it's templating system, pySSV aims to be very approachable for people new to shaders and graphics programming. Comparison Compared to dedicated visualisation libraries such as pyVista or VTK, pySSV aims to give users more control over how their data is rendered and can provide more flexibility for creative visualisation by allowing direct access to shaders. Compared to other graphics libraries such as pythreejs or Glumpy, pySSV places the focus on shaders, and attempts to streamline the API needed to write highly flexible shaders."},
{"Title": "import-embargo: package for limiting imports across your app", "Author": "u/khaarkoo", "Content": "https://github.com/KlemenS189/import-embargo What the project does In bigger codebases with multiple people/teams it is usually a good architecture pattern to expose certain packages as public and others as private. Python does not enforce this by default. This is why I have created this tool, to add these constraints if needed. You can check more docs in github repo. Code is pure python without any dependencies apart from dev deps. It uses AST under the hood. Target audience Usage in production at your own discretion for now. Comparison Simpler alternative to https://github.com/seddonym/import-linter/ NOTE: This is only a 0.0.1 version so bugs are of course possible. Collaborators welcome :)"},
{"Title": "DKP - tool for creating self-executable encrypted backups from docker compose", "Author": "u/leshiy-urban", "Content": "Hi there! What My Project Does This is a simple, yet powerful, script in Python (3.8 - 3.12+ without python deps) for which packs existent docker-compose project in self-executable encrypted archive. To restore, you don't need to install DKP or remember the details: just run archive as a script (help included!). It can backup images, mounted files, directories, volumes, as well as project env files. Can be used as CLI or as library. Target Audience Created to help other people, since I believe it's not only me who setups once backup routine and after a years forgot how to recover. Primarily for sysadmins, for self-hosters, for SMB (for large corporation it's probably not very useful). Comparison The closest possible solution: restic - is not compose-aware Installation : pip install dkp Source : https://github.com/reddec/dkp Usage: usage: dkp [-h] [--output OUTPUT] [--skip-images] [--passphrase PASSPHRASE] [project] Docker Compose packer - pack compose project with all batteries included positional arguments: project               Compose project name options: -h, --help            show this help message and exit --output OUTPUT, -o OUTPUT Output file --skip-images, -S     Do not archive images --passphrase PASSPHRASE, -p PASSPHRASE Passphrase to encrypt backup. Can be set via env PASSPHRASE"},
{"Title": "quarto-pyodide: Quarto extension to embed interactive Python code cells in HTML documents", "Author": "u/coatless", "Content": "üëã Greetings and Salutations all! I wanted to share a new way of embedding interactive Python code cells into websites, slides, and books through the {quarto-pyodide} extension for Quarto See the documentation: https://quarto.thecoatlessprofessor.com/pyodide Check it out on GitHub: https://github.com/coatless-quarto/pyodide Project Overview This project introduces a new custom code cell engine for Quarto that allows for interactive Python cells into HTML documents. Leveraging the capabilities of Pyodide's WebAssembly distribution for Python , the Python code is run directly in the user's web browser. Moreover, the custom engine removes the need for any expertise in HTML or JavaScript on the author's side. Target Audience The primary audience for this project includes instructors teaching Python who wish to simplify the setup process for their students, making it ideal for introductory Computer Science and Data Science courses. Additionally, it offers a swift and reproducible way for data scientists and others interested in Python scripts to analyze data interactively. Note: While this provides a user-friendly approach, it does not eliminate the eventual need for exposure to more comprehensive IDEs like VS Code, PyCharm, or even Posit Workbench. Comparison Compared to existing approaches like JupyterLite , PyScript , and Thebe , the {quarto-pyodide} extension adopts a more user-friendly stance by abstracting away complex interfaces and setup for both visitors to the web page and authors. Demos README: Live: https://quarto.thecoatlessprofessor.com/pyodide/examples/readme/ Source: https://github.com/coatless-quarto/pyodide/tree/main/examples/readme RevealJS: Live: https://quarto.thecoatlessprofessor.com/pyodide/examples/revealjs/#/title-slide Source: https://github.com/coatless-quarto/pyodide/tree/main/examples/revealjs Blog: Live: https://quarto.thecoatlessprofessor.com/pyodide/examples/blog/ Source: https://github.com/coatless-quarto/pyodide/tree/main/examples/blog"},
{"Title": "A Visual Basic for Applications precompiler written in python.", "Author": "u/TheRealBeakerboy", "Content": "I‚Äôve just created an official release of a VBA precompiler written in python. What my Project Does For those who don‚Äôt know, in VBA you can have precompiler blocks to change things for different OSs or VBA versions. For example, to change the function signature for a new windows version: #if Win64 Then Function foo(Bar, Baz) #Else Function Foo(Bar) #Endif The problem with this is, if you want to scan raw source code, you can‚Äôt just ignore the precompiler lines, because now the code looks like you‚Äôve tried to define the same function twice, and one was never ended. This tool takes environment variables that the user provides, and will comment out any lines that need to be skipped, creating 100% valid precompiled code. It even performs the comparisons, and arithmetic operations specified in the VBA specification. Target Audience People interested in malware prevention, static analysis, and Linting may find it helpful. Also, useful if you are interested in learning about compilers, ANTLR, and code parsing. Limitations It is currently missing the standard library functions, like Cbool(), Abs(), etc. I‚Äôm guessing these are never called by users in the precompiler phase."},
{"Title": "PyQuest: Python everything Cheatsheet and a Journey to the land of Python programming", "Author": "u/veny_xd", "Content": "Hi all! I've made another Python cheat sheet tutorial. Yeah yeah, nothing new, I know. But here's the thing: What My Project Does The main idea was not just to write a wall of text telling about everything, but to make it interactive. So that everything would have its example code snippet, which you could change, run, and see how it worked. And not somewhere in a web version, but on your own computer, in your own environment. Fortunately, Python has the perfect tool for this - the Jupyter Notebook. That's why all chapters are written as separate notebooks and there is an example for each point (well, almost). Target Audience I originally aimed at complete beginners and tried to go from simple to complex, but I think I overdid it at some point. So it might be just as suitable to refresh knowledge and fill in the gaps for anyone. What else It also has some useful information for Python-developer interview preparation and a link to the absolutely wonderful Tech Interview Handbook for Coding Interview preparation in case anyone missed it. I would very welcome any constructive criticism, any advice, and of course contributions and GitHub stars :) A little less, but still I will be glad to unreasonable hatred and comments that nobody needs it and that there are hundreds of similar projects on the Internet. Check out the project on GitHub: PyQuest"},
{"Title": "Hyperdiv: Reactive web UI framework for Python", "Author": "u/Hyperdiv-io", "Content": "Hi guys! I'd like to share a reactive web UI framework I've been working on for a while that I made public a couple of days ago. GitHub: https://github.com/hyperdiv/hyperdiv Homepage: https://hyperdiv.io There is a short coding demo video and intro article on the website. What My Project Does Hyperdiv is a way to build reactive UIs in pure Python quickly, with a built-in UI component system based on Shoelace ( https://shoelace.style ), markdown, and charts based on Chart.js ( https://chartjs.org ). It uses immediate-mode syntax which enables seamlessly blending declarative UI code with Python logic and event handling. Target Audience The aim of Hyperdiv is to reduce tool and language complexity when building full stack apps, and enable people to get to a working UI very quickly. I think it is a good fit for adding browser UIs to CLI tools, prototyping UIs, and internal tools. You can also put it behind Nginx and deploy it on the internet. Comparison Hyperdiv adds to a niche currently occupied by Streamlit, Reflex.dev , PyWebIO, PyJS, etc. -- frameworks that let you build web apps in pure Python. Hyperdiv stands apart with a unique blend of immediate-mode UI + reactive state, and letting you build fairly unrestricted, arbitrarily nested UI layouts with terse syntax. I appreciate your support!"},
{"Title": "Dispatch: Stateful Functions for Python", "Author": "u/achille-roussel", "Content": "Hello everyone! We (4 people startup) open-sourced an SDK to create Stateful Functions in Python! Github: https://github.com/stealthrocket/dispatch-sdk-python Docs: https://docs.stealthrocket.cloud Package: https://pypi.org/project/dispatch-functions What My Project Does We're developing Dispatch, a cloud service that helps developers create reliable systems. It can be used to build real-time and durable workflows, data pipelines, highly parallel jobs, and much more. Say you have a background job that fails and needs to be retried; getting the retry logic right is difficult, and it gets even harder when the retries need to happen across a fleet of instances and in a way that doesn‚Äôt take down the application. However, the biggest pain point is managing the state of the system, which often requires adding queues, databases, or workflow engines. With the SDK we released, we are giving Python developers a simple solution to these problems: Stateful Functions. Audience This SDK is useful to Python developers who build production services and need solutions for state management of their background jobs, data streams, workflows, etc... Comparison Dispatch addresses technical problems that are sometimes solved with cloud products like AWS Step Functions, Azure Durable Functions, or Temporal. Dispatch focuses on being lightweight and simple : there is no heavy framework to bring into the application, no cloud component to deploy, no workflow language to learn. The code can run in a serverless deployment, a container, or really anywhere. Python developers can add it to an existing application without having to change anything about the rest of the code. Technical Details At the core of the SDK is an event loop integrating with native Python coroutines (async/await) that supports pausing execution and serializing the state. On resumption, the coroutine state is recreated to continue execution, which could happen in a different instance of the application! Capturing stack frames was the most difficult part: even if Python has native support for pickling values, some objects like generators or coroutines cannot be serialized by default. To work around this limitation, we developed a native extension for CPython that allows us to save and reload the internal interpreter state. Feedback The project is young, but we're really excited to share it. Check it out and let us know what you think!"},
{"Title": "New Package for Continued Fractions", "Author": "u/LeftAcanthocephala23", "Content": "I've published a new package for people interested in working with mathematical objects called continued fractions in an object-oriented way with Python. https://pypi.org/project/continuedfractions/ What My Project Does? The package has been designed to take advantage of an existing Python standard library for rational numbers ( `fractions` ), and provides a way of computing continued fraction representations for any real number that can be defined in Python, and inspecting their properties (elements, convergents, segments, remainders), and also performing operations with them as normal rational numbers . Continued fractions are not part of the `fractions` standard library. Background I know it's a rather specialised topic, but I guess the first question is \"What is a continued fraction?\", and the next might be, \"What can you do with them?\". A continued fraction is a way of representing real numbers as the sum of an integer part + a nested fraction, where the denominators contain sums of integers and fractions, and those fractions then contain sums of integers and fractions, and so on. Since integers and fractions add up to rational numbers, continued fractions are ways of representing real numbers as the sum of (possibly, infinitely many) rational numbers. They are nested, hierarchical structures, which can be finite or infinite depending on the type of number being represented. For irrational numbers like pi the continued fraction representation will always be infinite, but for rational numbers (ratios of integers) it will always be finite. I don't want to go into the full details, but to answer the next question: continued fractions are really quite beautiful, interesting and useful in many ways. They are very important for the approximation of irrational numbers, which have infinite decimal expansions, with rational numbers, which have a finite decimal expansion. They are also connected to the study of special sequences and orderings of rational numbers - see the Stern-Brocot tree . Target Audience I guess people interested in continued fractions and, more generally, perhaps, number theory. Comparison I don't know of any other Python package that deals with continued fractions in the same way - I did come across one called `contfrac` , but that is not object-oriented, and when I compared the performance of its main function for computing continued fraction representations with mine it was much slower. There are other non-Python packages that can do continued fractions, obviously, like Mathematica, and who knows what else. But this has been designed for Python users in mind. Limitations Obviously the package cannot give you a complete continued fraction representation for pi or any irrational number - it gives you a finite representation that is the best possible, given the limitations of binary floating point arithmetic , as implemented in Python. For irrational numbers the representation is approximate, not exact - this will be improved in the future to allow the representations to be as exact as possible. Documentation At the moment the documentation is confined to the GitHub project README , but it is quite detailed, with lots of examples. I do plan to publish the docs separately, e.g. GitHub Pages or something like that, in later releases. Feedback All/any feedback is welcome, and so are contributions from any one interested in contributing to adding more cool features for studying continued fractions with Python."},
{"Title": "KeyCraftsman Project", "Author": "u/yousefabuz", "Content": "GitHub: KeyCraftsman What My Project Does KeyCraftsman is an innovative Python class designed to generate passcodes to your own liking. Offering an array of features such as key length specification, character exclusion, inclusion of all characters, URL-safe encoding, and exportation of the generated passkey(s). Target Audience This module is tailored for users who seek for flexibility in generating custom randomized password key(s). Comparison Many existing modules lack the flexibility needed in terms of features and often include deprecated elements. In contrast, this module stands out by offering a rich set of features compared to traditional Python password -generating modules. While the demand for such features may not be exceptionally high, I embarked on creating a modernized, medium/heavyweight version of key generation for the sheer enjoyment of exploring new possibilities. Features - Exclude Characters : Tailor your keys by excluding specific characters. - Include All Characters: Embrace diversity by including all ASCII letters, digits, and punctuation. - Unique Characters: Ensure uniqueness in generated keys or words. If words is specified, it will generate words with only unique number of letters in them. - Custom Text Wrapping: Wrap your keys with a custom separator and width for a personalized touch. - Multiple Key Generation: Efficiently generate multiple keys with a single instance. - Word Generation: Explore creative possibilities with word generation using random.SystemRandom(). - Exclusion Chart: Simplify character exclusion with the provided exclusion chart, available for printing and export. For a comprehensive overview of all features and methods, please refer to the documentation. We invite you to explore the capabilities of KeyCraftsman and hope you find joy in utilizing this modernized approach to key generation."},
{"Title": "marimo-wasm: a reactive Python notebook in the browser", "Author": "u/mmmmmmyles", "Content": "Showcasing marimo-wasm Hey everyone! We (2 developers) made marimo compatible with WebAssembly (WASM), so you can run it entirely in the browser thanks to Pyodide https://marimo.app What My Project Does Background: marimo is an open-source reactive notebook for Python, that aims to solve well-known problems with traditional Python notebooks[1]. This showcase is for marimo-wasm : a WASM build of marimo allowing the notebook to run entirely in the browser. This is done through WebAssembly and standing on the shoulder of Pyodide (h/t!) You can try out the playground: https://marimo.app/ This tool has been great for educating others through interactive examples. You can share snippets of code with just a URL. For example, here is a notebook on Bayes' Theorem . While the installable marimo library is hardened and used in production, the WASM build may have some rough edges. Still, the playground is great for sharing notebooks, data applications[2], or explorations with the community without the cost of servers to you or your audience. [1] https://docs.marimo.io/faq.html#faq-problems [2] You can hide the code and turn your notebook into an app Target Audience Data Engineers, Data Scientists, Story Tellers, Educators, and Researchers. Comparison JupyterLite - a WASM powered Jupyter running in the browser. However, it is not reactive like marimo. IPyflow - a reactive notebook for Python implemented as a Jupyter kernel. However, it is not WASM compatible. Jupyter - marimo is a reinvention of the Python notebook as a reproducible, interactive, and shareable Python program that can be executed as scripts or deployed as interactive web apps - without the need of extensions or additional infrastructure. More here ."},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "copykitten: the missing clipboard library for Python", "Author": "u/Jediroman", "Content": "What My Project Does copykitten is a clipboard library with support for text and images and a simple API. It's built around Rust arboard library. Thanks to this, it's multiplatform and doesn't require any dependencies. Target Audience Developers building CLI/GUI/TUI applications. The library has beta status on PyPI, but the underlying Rust library is pretty stable, being used in commercial projects like Bitwarden and 1Password. Comparison There are lots of other clipboard libraries for Python: pyperclip , jaraco.clipboard , pyclip , just to name a few. However, most of them are not maintained for years and require the presence of additional libraries or tools in the operating system. copykitten doesn't suffer from these shortcomings. A bit of history Throughout my years with Python there were several times when I needed to use the clipboard in my applications and every time I had to fall back to some shaky methods like asking the end user to install xclip and calling subprocess.run . This never felt great. Right now I'm making a multiplayer TUI game (maybe I‚Äôll showcase it later too :) ) , where users can copy join game codes into the clipboard to easily share it (much like Among Us). This is how I came to the idea of making such a library. I also wanted to try Rust for a long time, and so this all just clicked in my head instantly. I had fun building it and definitely had some pain too and learned a bit of nitty-gritty details about how clipboards work in different operating systems. Now I hate Windows. With this post I hope to gain some attention to the project so that I can receive feedback about the issues and maybe feature requests and spread the word that there's a modern, convenient alternative to the existing packages. Feel free to try it out: https://github.com/Klavionik/copykitten"},
{"Title": "Squirrels v0.2.0 Now Released: Create Dynamic Data Analytics APIs!", "Author": "u/squirrels-api", "Content": "Hi all! Introducing version 0.2.0 of the Squirrels Python library! Squirrels is a dbt-like framework that allows users to create REST API endpoints for dynamic data analytics. Feel free to read more about it on our website ! Following our version 0.1.0 release in our previous post here , we've recreated the framework to bring you an improved experience! Although this introduces many breaking changes, we intend to prevent breaking changes as much as possible for subsequent minor version increments (i.e. until 1.0.0). Compared to 0.1.0, the new 0.2.0 version has incorporated the following features: Flexible Lineage for Data Models Create data models (in SQL or Python) as database views or federates. Federate models provide a \"ref\" function/macro to refer to other models just like dbt! Specify a target data model for your dataset API, and Squirrels takes care of the model dependencies and execution order for you. You can query different database systems with different database views and join the results together in a federate model! Consolidated Parameters Multiple dataset APIs can share common parameters. You can specify a pool of parameter properties in one place (in Python or YAML) and specify the list of parameters that each dataset uses. Authentication You can specify a custom User model and an authentication method that integrates with your authentication source in a Python file. The user attributes can be used the affect the widget parameters or models in real time! We now support three different levels of access for datasets, with public datasets open to all, protected datasets open to both internal and external users, and private datasets only for private users. Feel free to give Squirrels a whirl by following our simple tutorial listed on our tutorial page ! And here's the link to our GitHub repo: https://github.com/squirrels-nest/squirrels Below are the sections specified in Rule 10: What My Project Does: Squirrels is a dbt-like framework that allows users to create REST API endpoints for dynamic data analytics. Again, feel free to read more about it on our website here ! Target Audience: Data Engineers, Analytics Engineers, Data Architects, Data Analysts, Data Scientists, Business Analysts, API Engineers, etc. Comparisons: We've come across several questions regarding \"What's the difference between Squirrels and <some other data tool>?\". Below are a few of the points on some commonly asked about comparisons. dbt: Used for data transformations only (Squirrels is largely inspired by dbt, but made for the real-time analytics or semantic layer use case instead) Does not support widget parameters (and does not need to for non-customer-facing data transformations) cube.js: Data models are created in YAML (for Squirrels, they're created in SQL or Python) Cannot create a lineage of multiple data models under one API endpoint No support for specifying widget parameters properties to dynamically change dataset behavior VulcanSQL No support for Python models and cannot create a lineage of multiple data models under one API endpoint Can specify parameters, but it's very \"free-form\" with limited control over the typing of parameters for the client (\"typing\" such as dropdown parameter, date parameter, etc.) Similar to Squirrels, it can have authenticated users affect the model behavior, but cannot integrate with your authentication source. Thus, we are not aware of existing tools that have the same capabilities as Squirrel does. We have tried to use a dbt-like project structure to make it easier for people to pick up."},
{"Title": "Cry Baby: A Tool to Detect Baby Cries", "Author": "u/technologicalBridges", "Content": "Hi all, long-time reader and first-time poster. I recently had my 1st kid, have some time off, and built Cry Baby What My Project Does Cry Baby provides a probability that your baby is crying by continuously recording audio, chunking it into 4-second clips, and feeding these clips into a Convolutional Neural Network (CNN). Cry Baby is currently compatible with MAC and Linux, and you can find the setup instructions in the README. Target Audience People with babies with too much time on their hands. I envisioned this tool as a high-tech baby monitor that could send notifications and allow live audio streaming. However, my partner opted for a traditional baby monitor instead. üòÖ Comparison I know baby monitors exist that claim to notify you when a baby is crying, but the ones I've seen are only based on decibels. Then Amazon's Alexa seems to work based on crying...but I REALLY don't like the idea of having that in my house. I couldn't find an open source model that detected baby crying so I decided to make one myself. The model alone may be useful for someone, I'm happy to clean up the training code and publish that if anyone is interested. I'm taking a break from the project, but I'm eager to hear your thoughts, especially if you see potential uses or improvements. If there's interest, I'd love to collaborate further‚ÄîI still have four weeks of paternity leave to dive back in! Update: I've noticed his poops are loud, which is one predictor of his crying. Have any other parents experienced this of 1 week-olds? I assume it's going to end once he starts eating solids. But it would be funny to try and train another model on the sound of babies pooping so I change his diaper before he starts crying."},
{"Title": "Extracting information (Text, Tables, Layouts) from PDFs using OCR.", "Author": "u/FlyingRaijinEX", "Content": "I've received an assignment whereby I am required to extract texts, tables, layouts, headers, titles, etc from PDFs (Multi-page). These PDFs have actual text on them and not images. So far I've tried using Camelot, PyMuPDF, and Nougat. Unfortunately, none of these modules are able to meet my client's expectations. Due to this, I've tried AWS Textract. I've showed a sample result of Textract and they immediately loved it. However, only then they mentioned that the PDFs have sensitive data and cannot be exposed via the internet. Now, they are looking to find an on-prem solution to get similar results as AWS Textract. Anyone know any kind of software/tool/python module that can be self-hosted and able to get similar results as AWS Textract? Thanks in advance."},
{"Title": "Typed FFmpeg: Type-Hinted Python Wrapper for Enhanced FFmpeg Integration", "Author": "u/lucemia51", "Content": "I'm excited to share my latest open-source project: Typed FFmpeg, a Python wrapper for FFmpeg enhanced with type hints for better code predictability and IDE support. https://github.com/livingbio/typed-ffmpeg Key Features: Type-hinted for improved developer experience. Simplified FFmpeg operations in Python. Detailed docs with examples for easy start. What My Project Does: Typed FFmpeg is an open-source Python wrapper for FFmpeg, designed to integrate multimedia processing capabilities into Python applications more seamlessly. It offers type hints throughout, improving code reliability and developer experience by providing better IDE support and error checking before runtime. Whether you're resizing videos, converting file formats, or extracting metadata, Typed FFmpeg simplifies these tasks with an intuitive, Pythonic interface. Target Audience: This project is aimed at developers working on multimedia applications, educational content, or data analysis involving video and audio processing. It's suitable for both production and experimental projects, offering a balance between ease of use and robust functionality. Comparison: Unlike existing high level FFmpeg wrappers for Python, Typed FFmpeg places a strong emphasis on type hints, which sets it apart by enhancing code quality and development speed. While other tools may provide similar functionalities, Typed FFmpeg aims to reduce the learning curve and improve code safety, making multimedia processing accessible to a broader range of Python developers, from beginners to seasoned professionals. PyAV provides detailed control over media through direct FFmpeg library bindings, catering to advanced users for in-depth manipulation. It demands a greater understanding of FFmpeg‚Äôs architecture, presenting a steeper learning curve compared to Typed FFmpeg, which focuses on ease of use and type safety. How to Help: Check out the project and star it if you like it. Try it out, report issues, or contribute. Share any feedback or suggestions for features. Appreciate your support and feedback!"},
{"Title": "python-compile - compile your python app for linux execution in Win/Mac/Linux using docker", "Author": "u/ZachVorhies", "Content": "What my project does This project will use docker to compile your one file python app into a binary using nuitka that's runnable on many linux systems. You'll need to have docker desktop installed. https://pypi.org/project/python-compile/ Target Audience Developers trying to target production services with optimized python binaries. To use this you'll need a requirements.txt file and a python file entry point that runs your app. For example: pip install python-compile python-compile --os debian --input demo\\http\\server.py --requirements requirements.txt I use nuitka under the hood. Github link: https://github.com/zackees/python-compile"},
{"Title": "Comparison of hosting platforms for Python app deployment", "Author": "u/Plus_Ad7909", "Content": "Hey all! I recently wrote a review comparing different hosting platforms for Python applications. In it, I take a look at Koyeb, Render, pythonanywhere, Fly.io, DigitalOcean App Platform, and Heroku: https://git-push-to-production.hashnode.dev/5-top-free-hosting-platforms-for-python-apps-2024 I'd love to hear about your experiences with these platforms. Which ones have you used and would recommend? Also, please feel free to share any managed hosting solutions not featured in the article. While I focused on platforms that do a lot of the heavy lifting for you, I'm open to hearing about more self-managed solutions too."},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Did you know a Python \"project\" mean you have to package it?", "Author": "u/kaydzed", "Content": "Having almost 10 YOE in Python, it took me this long to understand \"project\" in pyproject.toml means package (in JS, package.json name is more explicit): https://packaging.python.org/en/latest/glossary/#term-Project I've always thought `pyproject.toml` was intended to manage any sort of Python project especially those I don't intended to package and distribute (e.g., some script/notebook and requirements file)."},
{"Title": "Grid World Application", "Author": "u/bwe587", "Content": "What my project does: Agent reinforcement learning by interacting with a grid-base environment. This is achieved by implementing the bellman's equation. Target Audience: Initially, the intended audience for this application consisted of my fellow classmates from the    mathematical modeling course at my university. Toy or sandbox project. Comparison: User-friendly interface that allows you to create and manipulate grid-based environments effortlessly. The power to define and modify most parameters of your grid worlds at the click of a button! https://github.com/bwe587/GridWorld/"},
{"Title": "3D engine in Turtle", "Author": "u/the_nebulo", "Content": "Hi everyone! I wanted to share a side project of mine that grew into something that, to my knowledge, doesn't exist yet. My high school does basic python, and some of it involves Turtle. My friends were messing around with how to do 3D rendering in turtle, which inspired me to try the same but go full out. Over the course of 5 days, I went from rendering wireframe cubes to what eventually became a 3D engine in Turtle, with way too many features for it's own good. Project Overview 3D engine module built exclusively with turtle and mathematical imports such as math or numpy. Features include: A scene system that is similar to modern game engines (a lot simpler of course) A physical camera system, with gimbal lock prevention features A robust object system, with full transformation support and vertex management Lighting/Material framework allowing for physical light objects and materials applied to objects (experimental) Loading OBJ files directly Realtime positioning in 3D space using vectors Missing Features However, I never considered the scale of this project, so I started getting burnt out quickly, and there are several features that I would love to do, but honestly I don't have the motivation to, so I decided to share the project in it's current state. Some of the things I wish to have added are: A raycasted lighting system with shadows Proper object clipping management Parent/child system for objects alongside object groups Improved turtle management/optimisation Verifying function parameters and proper documentation (beyond docstrings) Vector3 classes instead of lists A lot more flexible vertex management/manipulation. Target Audience This project is a complete gimmick, just seeing how far the Turtle module can be taken despite it's primitive nature and extremely poor performance. Regardless, I'm proud of how far I managed to push Turtle (even if it runs at 0.3 FPS), and I would love to see anyone interested to push this gimmick as far as possible. One of my friends is even working on implementing a physics system using Turtle3D as a renderer. Comparison If this module were to somehow be taken seriously, the only partially fair comparison would be Panda3D or PyOpenGL, and in that regard Turtle3D is nothing. While Turtle3D does allow for a good range of flexibility, it's absolutely nothing like Panda3D or PyOpenGL as they are capable of so much more at a much better rate of performance and code quality. Closing Thoughts Of course, it isn't anywhere near high level or complex, let alone incredibly impressive, and I'm sure there are people in this subreddit who are far smarter than me that will see an incredible amount of miscalculations or atrocities in my code than I have already noticed. All my code is open source and on GitHub here . Feel free to improve it as you will!"},
{"Title": "Geospatial mapping : package to import layer style from qgis file to matplotlib", "Author": "u/Necessary-Rough2295", "Content": "Hello ! Long time reader, first time poster! I began coding in 2023 and I tend to work on GIS related topics. I have to edit a lot of maps for work, and I automated the process by rendering them directly from a python script. One issue I have is that I sometimes have a lot of style information to 'translate' from my normal QGis environment to matplotlib properties, and that is a unpractical process. I started to write a little package to help myself translating QGis layer style files (.qml) into a list of dict of kwargs for matplotlib. It is very basic for now, but it is already working for my use cases, and I would be interested in making it more useful for the community. Here is the git repo : https://github.com/PiouPiou974/qgis_style_files_extractor NB that GIS softwares like qgis are able to render more complex styles thant matplotlib, by example a polygon with 2 different hatches + markers, by drawing as many times as needed the same polygon with different parameters. I do the same thing in matplotlib, therefore I use lists of style kwargs dict instead of a unique dict for each style. If you are interested in this package, I would love to discuss about it with you !"},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "pySSV - Leverage the power of shader in Jupyter Notebooks", "Author": "u/space928", "Content": "I'm excited to share my Bachelor's degree project: pySSV. It allows you to write and interact with shaders in a Jupyter Notebook. Source code: https://github.com/space928/Shaders-For-Scientific-Visualisation Documentation: https://pyssv.readthedocs.io/en/latest/ If you're interested in the project I'd greatly appreciate if you help with the evaluation of this project by filling out my survey: https://forms.gle/rX7uPxaxQ1xcNVQB8 Project overview This is a library which allows you to write shaders in your Jupyter notebooks to create interactive data visualisations. It provides the following features: Custom Jupyter Widget Shader templating and preprocessor ShaderToy shader template, allowing ShaderToy shaders to be very easily used in pySSV Vertex/Pixel/Geometry shader support Vertex buffer input from NumPy arrays Texture input from NumPy arrays and Pillow Images Simple to use IMGUI library Multipass rendering Renderdoc support And more! A simple SDF shader example in pySSV , utilising the sdf shader template: import pySSV as ssv canvas = ssv.canvas() canvas.shader(\"\"\" #pragma SSV sdf sdf_main --camera_distance 2. --rotate_speed 1.5 --render_mode SOLID // SDF taken from: https://iquilezles.org/articles/distfunctions/ float sdCappedTorus(vec3 p, vec2 sc, float ra, float rb) { p.x = abs(p.x); float k = (sc.y*p.x>sc.x*p.y) ? dot(p.xy,sc) : length(p.xy); return sqrt( dot(p,p) + ra*ra - 2.0*ra*k ) - rb; } float sdf_main(vec3 p) { float t = 2.*(sin(uTime)*0.5+0.5)+0.2; return sdCappedTorus(p, vec2(sin(t), cos(t)), 0.5, 0.2); } \"\"\") canvas.run() Target audience pySSV is targeted towards shader/computer graphics enthusiasts and data scientists. A basic understanding of shaders is recommended, but through the use of it's templating system, pySSV aims to be very approachable for people new to shaders and graphics programming. Comparison Compared to dedicated visualisation libraries such as pyVista or VTK, pySSV aims to give users more control over how their data is rendered and can provide more flexibility for creative visualisation by allowing direct access to shaders. Compared to other graphics libraries such as pythreejs or Glumpy, pySSV places the focus on shaders, and attempts to streamline the API needed to write highly flexible shaders."},
{"Title": "import-embargo: package for limiting imports across your app", "Author": "u/khaarkoo", "Content": "https://github.com/KlemenS189/import-embargo What the project does In bigger codebases with multiple people/teams it is usually a good architecture pattern to expose certain packages as public and others as private. Python does not enforce this by default. This is why I have created this tool, to add these constraints if needed. You can check more docs in github repo. Code is pure python without any dependencies apart from dev deps. It uses AST under the hood. Target audience Usage in production at your own discretion for now. Comparison Simpler alternative to https://github.com/seddonym/import-linter/ NOTE: This is only a 0.0.1 version so bugs are of course possible. Collaborators welcome :)"},
{"Title": "DKP - tool for creating self-executable encrypted backups from docker compose", "Author": "u/leshiy-urban", "Content": "Hi there! What My Project Does This is a simple, yet powerful, script in Python (3.8 - 3.12+ without python deps) for which packs existent docker-compose project in self-executable encrypted archive. To restore, you don't need to install DKP or remember the details: just run archive as a script (help included!). It can backup images, mounted files, directories, volumes, as well as project env files. Can be used as CLI or as library. Target Audience Created to help other people, since I believe it's not only me who setups once backup routine and after a years forgot how to recover. Primarily for sysadmins, for self-hosters, for SMB (for large corporation it's probably not very useful). Comparison The closest possible solution: restic - is not compose-aware Installation : pip install dkp Source : https://github.com/reddec/dkp Usage: usage: dkp [-h] [--output OUTPUT] [--skip-images] [--passphrase PASSPHRASE] [project] Docker Compose packer - pack compose project with all batteries included positional arguments: project               Compose project name options: -h, --help            show this help message and exit --output OUTPUT, -o OUTPUT Output file --skip-images, -S     Do not archive images --passphrase PASSPHRASE, -p PASSPHRASE Passphrase to encrypt backup. Can be set via env PASSPHRASE"},
{"Title": "quarto-pyodide: Quarto extension to embed interactive Python code cells in HTML documents", "Author": "u/coatless", "Content": "üëã Greetings and Salutations all! I wanted to share a new way of embedding interactive Python code cells into websites, slides, and books through the {quarto-pyodide} extension for Quarto See the documentation: https://quarto.thecoatlessprofessor.com/pyodide Check it out on GitHub: https://github.com/coatless-quarto/pyodide Project Overview This project introduces a new custom code cell engine for Quarto that allows for interactive Python cells into HTML documents. Leveraging the capabilities of Pyodide's WebAssembly distribution for Python , the Python code is run directly in the user's web browser. Moreover, the custom engine removes the need for any expertise in HTML or JavaScript on the author's side. Target Audience The primary audience for this project includes instructors teaching Python who wish to simplify the setup process for their students, making it ideal for introductory Computer Science and Data Science courses. Additionally, it offers a swift and reproducible way for data scientists and others interested in Python scripts to analyze data interactively. Note: While this provides a user-friendly approach, it does not eliminate the eventual need for exposure to more comprehensive IDEs like VS Code, PyCharm, or even Posit Workbench. Comparison Compared to existing approaches like JupyterLite , PyScript , and Thebe , the {quarto-pyodide} extension adopts a more user-friendly stance by abstracting away complex interfaces and setup for both visitors to the web page and authors. Demos README: Live: https://quarto.thecoatlessprofessor.com/pyodide/examples/readme/ Source: https://github.com/coatless-quarto/pyodide/tree/main/examples/readme RevealJS: Live: https://quarto.thecoatlessprofessor.com/pyodide/examples/revealjs/#/title-slide Source: https://github.com/coatless-quarto/pyodide/tree/main/examples/revealjs Blog: Live: https://quarto.thecoatlessprofessor.com/pyodide/examples/blog/ Source: https://github.com/coatless-quarto/pyodide/tree/main/examples/blog"},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "A rich-text editor with PyQt6", "Author": "u/OvidPerl", "Content": "For those who work with Qt, I'm sure some of you have seen Megasolid Idiom , a simple, rich-text editor. However, it's written with PyQt5. If you're learning graphics programming in Python, that's disappointing since PyQt6 has been around since January of 2021 and appears to be stable. I wanted that editor in PyQt6, so I updated it and shared it on github . It has an MIT license, like the original."},
{"Title": "Alto: CLI tool to run functions, scripts, and Jupyter notebooks in the cloud", "Author": "u/runprism", "Content": "Hey everyone! I want to share with everyone the project I have been working on called Alto. It's a Python CLI tool to run functions, scripts, and Jupyter notebooks in the cloud. Github: https://github.com/runprism/alto Documentation: https://alto.mintlify.app/welcome/v0.0.8/welcome What My Project Does Alto allows data practitioners to effortlessly run functions, scripts, and Jupyter notebooks in the cloud. Currently, Alto supports running code in EC2 instances. However, we're working on adding more infrastructure types (e.g., EMR clusters) and cloud providers (GCP, Azure, etc.) Target Audience For data teams, building and maintaining infrastructure is difficult. It's also not exactly within their wheelhouse. I built Alto for data analysts / data scientists (like myself) who wish to run data workloads in the cloud without having to deal with provisioning the infrastructure to do so. Alto is an early-stage project, so use in production at your own risk. Comparison Alto allows users to perform serverless data execution, similar to AWS Lambda. There are a few features that make Alto distinct: Code format. Alto supports running functions, scripts, and Jupyter notebooks, whereas Lambda is primarily used for executing functions. Local development. Alto allows users to run their code in the cloud as is. In other words, users can develop and test their code on their local machine and then deploy to the cloud when they're ready. They could also choose to develop in the cloud itself."},
{"Title": "CLI tools hidden in the Python standard library", "Author": "u/LinearArray", "Content": "Found a cool resource which explains the CLI tools hidden in the Python Standard Library. Link : https://til.simonwillison.net/python/stdlib-cli-tools"},
{"Title": "ADIX: Data Exploration Made Easy & Colorful with themes you can customize (EDA)", "Author": "u/xmooger", "Content": "Hi everyone, I have released a beta version of ADIX - a new Python package for Data Scientists and anyone interested in data analysis. What My Project Does ADIX works in Jupyter notebooks or Google colab and is specifically focused on streamlining the exploratory data analysis process, making it incredibly easy and fast to gain insights from your dataset with just one command: ix.eda(). This allows you to quickly understand your data and be more productive in your research. Target Audience Data scientists, ML engineers and anyone interested in data analysis / exploration. PyPi : https://pypi.org/project/adix/ I'd greatly appreciate any feedback you may have. And if you like the project please consider giving a star on GitHub . You can find the documentation here . Main features üé® Customizable Themes: Personalize your analysis environment with your own unique style using customizable color schemes! ‚ö° Efficient Cache Utilization: Say goodbye to long loading times! ADIX leverages optimized caching mechanisms for lightning-fast performance. üîç Rapid Data Insight: ADIX prioritizes speed, delivering crucial insights at your fingertips for quick decision-making. üî¢ Automatic Type Detection: Let ADIX do the heavy lifting! It automatically detects numerical, categorical, and text features, with the option for manual overrides when needed. üìä Statistically Rich Summary Information: Unveil the intricate details of your data with comprehensive summaries, including type identification, unique values, missing values, and more. Dive deep into numerical data with properties like min-max range, quartiles, and skewness. üìà Univariate and Bivariate Statistics Unveiled: Explore univariate and bivariate insights with ADIX's versatile visualization options. From bar charts to matrices, and box plots, uncover a multitude of ways to interpret and analyze your data effectively. Comparison Automated data exploration is a popular topic and there are some alternatives that work similarly, for example sweetviz or lux ."},
{"Title": "periodic-table-cli: An interactive Periodic Table of Elements App for the Console! Now for Python!", "Author": "u/spirometaxas", "Content": "What My Project Does : It's a lightweight and fun Periodic Table of Elements app for the console. Supports Browse and Search for all 118 elements, and 20 Display Modes.  Interact with the periodic table like never before as you command the elements with just a few keystrokes the cool way, in the console.  A few months ago, I built this app for NodeJS. After receiving lots of positive reviews and feedback, I decided to rebuild it again for Python! PYPI : periodic-table-cli The controls are the same. Use Arrows for navigation. Use Slash/BackSlash to toggle 20 display modes. Use Letters/Numbers to search and Enter to select. Use ESC or CTRL+C to quit. Just make sure to have Python 3.8+ installed and use a terminal that supports 256 colors. Tested on MacOS (Terminal + iTerm2), Ubuntu (Terminal + XTerm), and Windows Command Prompt (requires windows-curses). Target Audience : Scientists, chemists, developers, terminal enthusiasts, and anyone who just likes cool tools. Comparison : It's not the first time this idea has been done, but this one has more browse features, display modes, and an overall cool minimalist design. More directions available in the links below. Enjoy! Homepage : https://spirometaxas.com/projects/periodic-table-cli PYPI Page : https://pypi.org/project/periodic-table-cli/ Source Code : https://github.com/spirometaxas/periodic-table-cli-py"},
{"Title": "slow-learner ‚Äî type generation from data", "Author": "u/nj_vs_valhalla", "Content": "What My Project Does : A library and CLI that reads a stream of data and generates Python types describing it. See below for a detailed description and an example. Target Audience : Primarily web developers, but also anyone who works with structured data. Stability-wise, it is in beta, but fairly well-tested and I haven't encountered major problems using it regularly for the past year. Comparison : There are libraries that do the same, but output JSON Schema (e.g. genson). To the best of my knowledge, no other library generates Python types directly from data. A while ago I found myself working on a large distributed backend codebase with almost no types / schemas available for inter-service RPCs. Everything was just pure dicts! I hated it, so I decided to make a \"type learner\" that would consume a stream of data and generate Python types. Initially I built it for myself and it may reflect my specific use-case, but I hope that I was able to make it quite generic so it can be useful for others. For example, let's generate types for GitHub REST API # fetch GitHub releases list gh api \\ -H \"Accept: application/vnd.github+json\" \\ -H \"X-GitHub-Api-Version: 2022-11-28\" \\ /repos/facebook/react/releases --paginate |> data.json slow-learner learn --spread --type-name Release data.json The result is \"\"\" This file contains Python 3.8+ type definitions generated by TypeLearner from 99 observed value(s) Source JSON files: - /Users/njvh/Documents/Personal/slow-learner/data.json \"\"\" from typing import List from typing import Literal from typing_extensions import NotRequired from typing import Optional from typing import TypedDict from typing import Union class ReleaseAuthor(TypedDict): login: str id: int node_id: str avatar_url: str gravatar_id: Literal[\"\"] url: str html_url: str followers_url: str following_url: str gists_url: str starred_url: str subscriptions_url: str organizations_url: str repos_url: str events_url: str received_events_url: str type: Literal[\"User\"] site_admin: Literal[False] class ReleaseAssetsItemUploader(TypedDict): login: str id: int node_id: str avatar_url: str gravatar_id: Literal[\"\"] url: str html_url: str followers_url: str following_url: str gists_url: str starred_url: str subscriptions_url: str organizations_url: str repos_url: str events_url: str received_events_url: str type: Literal[\"User\"] site_admin: Literal[False] class ReleaseAssetsItem(TypedDict): url: str id: int node_id: str name: str label: Optional[str] uploader: ReleaseAssetsItemUploader content_type: Union[ Literal[\"text/javascript\"], Literal[\"application/javascript\"], Literal[\"application/x-javascript\"], Literal[\"application/zip\"], ] state: Literal[\"uploaded\"] size: int download_count: int created_at: str updated_at: str browser_download_url: str ReleaseReactions = TypedDict( \"ReleaseReactions\", { \"url\": str, \"total_count\": int, \"+1\": int, \"-1\": Literal[0], \"laugh\": int, \"hooray\": int, \"confused\": Literal[0], \"heart\": int, \"rocket\": int, \"eyes\": int, }, ) class Release(TypedDict): url: str assets_url: str upload_url: str html_url: str id: int author: ReleaseAuthor node_id: str tag_name: str target_commitish: str name: str draft: Literal[False] prerelease: bool created_at: str published_at: str assets: List[ReleaseAssetsItem] tarball_url: str zipball_url: str body: str reactions: NotRequired[ReleaseReactions] Github | PyPI | Read more"},
{"Title": "file_unittest: a Python package to run unit tests based on text file results", "Author": "u/teamamentum", "Content": "What My Project Does Ever wanted to have a unittest compare results to reference data from a text file, and fail if they don't match? So did we! We have just opened sourced our Python package that extends Python unittest to enable comparison to data from a reference text file containing data. Target Audience Python developers, data scientists, looking to include file comparison in their unit testing. Comparison Not the first time this has been done. This just packages functionality into an easy to install and use Python package. Clone the project git clone https://github.com/amentumspace/file_unittest Installation The package can be installed into any active environment by running: cd file_unittest/ pip install . file_unittest.TestCase This class extends the unittest.TestCase class to allow the user to run unit tests by writing data to a file and verifying the output has not changed since the last time the unit test was run. Once the user is satisfied that the output data is correct, these unit test simply ensure that the data does not change with changes in the code base. Usage Rather than inherit from unittest.TestCase the user should derive a class from file_unittest.TestCase . Like the normal unit test, individual test functions are appended with test_ . Any desired output data is written to file by calling self.output Example unit test file: import unittest import file_unittest class MyTest(file_unittest.TestCase): def test_case(self): self.output(\"hello\") if __name__ == '__main__': unittest.main() Unlike the normal unittest.TestCase the user does not need to make any assertion calls eg. self.assertTrue , self.assertFalse etc. This class will take care of warning about any differences in output. Default output location The default location to output the test results will be: {derived_class_filepath}/test_results/ {derived_class_filename}.{derived_class_name}.{test_name}.txt Missing or different results If the expected output file is missing, or differences are detected, the output data will be written to the same file, but with .new postfix: {derived_class_filepath}/test_results/ {derived_class_filename}.{derived_class_name}.{test_name}.new On the first run, the user will need to inspect the .new file for expected results; Otherwise, if differences are detected, the user can diff the .txt and .new files to investigate and resolve any differences; In both cases once the user is satisfied with the results, the .new files can be renamed with .txt  extension and the .txt files can be checked into the git repo Committing test result files to source control Once the output .txt files have been satisfactorily generated as in the previous step, they should be checked into source control so that they can be used as the benchmark for future runs."},
{"Title": "First python project - Low latency offline voice recognition to remap your voice/words to keybinds.", "Author": "u/Captain-H2O", "Content": "What My Project Does I just dropped my first open source project for python on github! Its a speeech recognition program written 100% in python. I made it to help me while I play star citizens by rebinding the keys too speech recognition commands, that way I dont need to remember over 100 keybinds or accidently click the wrong one. Comparison (A brief comparison explaining how it differs from existing alternatives.) Some features that I have not seen in other similar programs. Such as A topmost/ Picture in picture Browser function so that users can simply call a voice command and it will open a Picture in picture browser over top of the game, that they can interact with and use so they do not have to switch to desktop open a browser type in the url, and the switch back. They simply can interact with a browser on top of their game. and close it without ever having to leave i think its pretty neat idea. Target Audience (e.g., Is it meant for production, just a toy project, etc. So far im pretty pleased, and would like anyone thats intersted in checking out other peoples side/hobby projects to check it out! Ive gone through tons of offline speech recognition librarys like vosk and sphinx but none of them can compete in the terms of speed, and accuracy like the windows speech recognition uses. Im using a python libary called Dragonfly2. So far it is working great on windows 10 and windows 11 computers. Its extremely fast from voice command to keypress. and im pretty happy so far with my side project and the results! I figured I would post it here and see what everyone here thinks about it so far? This is my first time releaseing a python script I've created, Ive made a few small things here and there with python but never released anything so I decided I might as well start https://github.com/techsavvy42/Captains_Voice_Commander"},
{"Title": "Here's my ideal python publishing process - would you use it? why or why not?", "Author": "u/burnfearless", "Content": "I've finally implemented my \"ideal state\" for Python release management on GitHub. Focused around these three tenets: Must be idiot proof , meaning: all checks are automated and operations work is automated. Must be runnable from a mobile browser. If your publish operation requires a laptop, you are probably doing it wrong. Must decouple marketing work from engineering work , while not duplicating efforts. And here's how I got there: Semantic pull request titles. Basically, take a conventional commits approach but apply them to your PR titles (which are trivial to edit+groom) instead of commit history (which is frustrating to edit+groom). No changelogs. If you use Semantic PRs, and if you default to those PR titles as your commit message when merging, then your main branch commit history is your changelog. No version bumps. Instead use dynamic versioning, such as provided by the poetry-dynamic-versioning plugin. Automatically-drafted release notes. If we've followed all the above, then a machine can create and continually edit a release notes draft (including calculating the correct version bump and the ability to generate and categorize changelog entries), which means at any moment we are ready to... One-click publish. Optionally groom before you do, but if you've followed the above four steps, it's just a single click to \"publish\" the release in GitHub. A GitHub Action can listen for the new release tag and respond by publishing to PyPi. My tools: Semantic PR checker: https://github.com/zeke/semantic-pull-requests Poetry dynamic versions plugin: https://github.com/mtkennerly/poetry-dynamic-versioning Release drafter: https://github.com/marketplace/actions/release-drafter Let me know what you think! Would you adopt this? What would you change?"},
{"Title": "Thread Local Data in Python", "Author": "u/Any-Tune-3880", "Content": "Since threads in Python share the memory space of their parent process, we might need to define thread-specific variables for specific use cases to avoid unintended side effects. In this post, we will: Explore Thread local storage: Python threading module's solution for thread-specific/thread-private values. See its real-world example from Open-Source ( usage in the Peewee ORM library). Look at the CPython source code to see how thread local storage is implemented under the hood. Background Threads share the memory space of their parent process. This will allow us to seamlessly access and share variables, data structures, etc., across threads. But this comes with its own challenges. There may be scenarios where we need to isolate variables and might need to store data specific to each thread. Thread local storage can be leveraged in this case. We can use local() found in the threading module to define thread-local variables. import threading import time # Create a thread-local storage object (1) thread_local = threading.local() def init_data(number): thread_local.number = number * 100 def show_data(): print(f\"Thread {threading.current_thread().name} has number {thread_local.number}\") def worker(number): init_data(number) for _ in range(3): time.sleep(1) show_data() thread1 = threading.Thread(target=worker, name=\"A\", kwargs={\"number\": 1}) thread2 = threading.Thread(target=worker, name=\"B\", kwargs={\"number\": 2}) thread1.start() thread2.start() thread1.join() thread2.join() The program output will be: Thread A has number 100 Thread B has number 200 Thread A has number 100 Thread B has number 200 Thread A has number 100 Thread B has number 200 As seen in comment #(1) , a thread-local storage object was created and assigned to the variable thread_local . Arbitrary attributes can be assigned to this variable, which are specific to the thread that performs the assignment and is isolated from others. In the example, each thread stores its own number attribute to the thread_local object and accesses the thread-specific value during their concurrent execution. Usage in the Wild Peewee, a Python ORM, utilizes thread-local data in its ThreadSafeDatabaseMetadata to support dynamic database switches at runtime in multithreaded applications. The source code can be found here. # File: peewee/playhouse/shortcuts.py class ThreadSafeDatabaseMetadata(Metadata): \"\"\" Metadata class to allow swapping database at run-time in a multi-threaded application. To use: class Base(Model): class Meta: model_metadata_class = ThreadSafeDatabaseMetadata \"\"\" def __init__(self, *args, **kwargs): # The database attribute is stored in a thread-local. self._database = None self._local = threading.local() super(ThreadSafeDatabaseMetadata, self).__init__(*args, **kwargs) def _get_db(self): return getattr(self._local, \"database\", self._database) def _set_db(self, db): if self._database is None: self._database = db self._local.database = db database = property(_get_db, _set_db) In multithreaded applications using peewee ORM, database switching at runtime without using ThreadSafeDatabaseMetada can lead to errors. If multiple threads work in parallel and one thread changes the connection parameters, this can lead to errors such as writing to a wrong DB, inconsistent writes (in case of non-atomic DB operations), etc. ThreadSafeDatabaseMetada solves this by keeping the database attributes in a thread-local object ( self._local ). In this way, dynamic changes to the database will only affect the thread that made the change. Other threads will keep working with their existing databases. Should I use it? Thread-local storage should be used when: You are writing a multi-threaded application (obviously) and: If some variables are used by the current thread only and are not relevant to the main thread/other threads. You find out that changes made by one thread can lead to unintended side effects in other concurrent threads. Generally, if you are working with multiple threads and there is shared mutable data: You should check if sharing these data across threads is actually needed. If sharing can be avoided, use thread-local storage to make the data specific to each thread. Otherwise, implement locks or other synchronization primitives to enforce thread safety. Note : Context variables from the contextvars standard library module can be used as an alternative to threading.local() . They work with multithreading as well as asyncio to store context-specific information. In the case of asyncio programs, context variables allow each coroutine task to have its own set of variables isolated from other (asyncio) tasks. Behind the scenes The Python implementation of threading.local can be found in the /Lib/_threading_local.py path in CPython source code. The _localimpl class is used to store thread-local values. class _localimpl: \"\"\"A class managing thread-local dicts\"\"\" __slots__ = 'key', 'dicts', 'localargs', 'locallock', '__weakref__' def __init__(self): # The key used in the Thread objects' attribute dicts. # We keep it a string for speed but make it unlikely to clash with # a \"real\" attribute. self.key = '_threading_local._localimpl.' + str(id(self)) # { id(Thread) -> (ref(Thread), thread-local dict) } self.dicts = {} The dicts attribute maps the id of a thread to a tuple. The tuple is two-membered, containing a reference to the thread and the actual dictionary storing thread local values ( # { id(Thread) -> (ref(Thread), thread-local dict) } ). Looking at a few other methods of the class: class _localimpl: \"\"\"A class managing thread-local dicts\"\"\" def get_dict(self): \"\"\"Return the dict for the current thread. Raises KeyError if none defined.\"\"\" thread = current_thread() return self.dicts[id(thread)][1] # (1) returning the local dict of current thread def create_dict(self): \"\"\"Create a new dict for the current thread, and return it.\"\"\" localdict = {} key = self.key thread = current_thread() idt = id(thread) ... wrthread = ref(thread, thread_deleted) thread.__dict__[key] = wrlocal self.dicts[idt] = wrthread, localdict  # (2) Populating the `dicts` with a new thread return localdict The comments #(1) & #(2) illustrate operations on the dicts attribute discussed previously. (1) : Returns the local data dictionary corresponding to the current thread accessing it. (2) : This part initializes the local dict for a new thread. Then, we have the actual local callable, which we call as threading.local() to initialize a thread-local object. @contextmanager def _patch(self): impl = object.__getattribute__(self, '_local__impl') try: dct = impl.get_dict()  # (3) this will return local dict of current thread -/ # see its definition in the above snippet except KeyError: ... # calls _localimpl`s create_dict to create & init a new dict with impl.locallock: object.__setattr__(self, '__dict__', dct) # (4) <The magic> Temporarily replaces -/ # the instance's __dict__ attribute with the thread-specific dictionary. yield class local: __slots__ = '_local__impl', '__dict__' def __new__(cls, /, *args, **kw): ... impl = _localimpl()  # (1) - wraps the _localimpl object in an attribute impl.localargs = (args, kw) impl.locallock = RLock()  # (2) - Lock for thread safety object.__setattr__(self, '_local__impl', impl) # We need to create the thread dict in anticipation of # __init__ being called, to make sure we don't call it # again ourselves. impl.create_dict() return self def __getattribute__(self, name): with _patch(self): return object.__getattribute__(self, name) def __setattr__(self, name, value): if name == '__dict__': raise AttributeError( \"%r object attribute '__dict__' is read-only\" % self.__class__.__name__) with _patch(self): return object.__setattr__(self, name, value) ... Let's look at various parts of the code one by one. # (1) :  An object of _localimpl class is created and later stored in _local__impl attribute # (2) : RLock() is used on dict operations - Concurrent writes by multiple threads may cause 'lost writes' since dict is not thread-safe. # (3) : In the try/except block, the current thread's local data is fetched and assigned to the variable named dct . # (4) : The magic happens here. Here is a quick refresher on class attributes before we delve further: A class has a namespace implemented by a dictionary object. Class attribute references are translated to lookups in this dictionary, e.g., C.x is translated to C. dict [\"x\"] (although there are a number of hooks which allow for other means of locating attributes). The _patch function we are currently in is a context manager. Using the line marked as # (4) , it patches the namespace dictionary of the local class with the dot variable (which stores the current-thread specific data). Since the __getattribute__ and __setattr__ dunders of the class local use the _patch context manager, attribute access performed inside the context will use the thread-local dictionary ( dct ) replacing the class's actual namespace dictionary. Note : The Lib/_threading_local.py starts with the below note: Note that this module provides a Python version of the threading.local class. Depending on the version of Python you're using, there may be a faster one available. You should always import the local class from threading . The code we looked at might not be the one running in our Python installations. I think newer Pythons versions are using C implementations of the thread local functionality for efficiency. ... I share interesting Python snippets üêç like this from open-source projects illustrating Python language features in my newsletter, \"Python in the Wild\". Subscribe to the newsletter on Substack or Linkedin to receive new Pythonic posts to your email üíåüöÄ."},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Stockdex - simple package for extracting stocks financial data from yahoo finance", "Author": "u/nginx26", "Content": "Happy weekend every one! I have recently created my first python package. What My Project Does The package helps with extracting financial data of companies from Yahoo into pandas dataframes using companies stocks symbol(like yfinance package) Target Audience The package is targeted at people who are interested in financial data of companies and want to use it for analysis or other purposes. Feel free to give you opinion or open issues on the Github project: Github Repo Link Pypi link"},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "ugropy: Group detection for thermodynamic UNIFAC/Joback models in Python", "Author": "u/SalvadorBrandolin", "Content": "Hi there! I'm working in ugropy , a Python library for molecule fragmentation to get the UNIFAC/Joback subgroups from the molecule's name or SMILES. GitHub PyPi What My Project Does By now ugropy can detect the subgroups for classic liquid-vapor UNIFAC, Predictive Soave-Redlich-Kwong (PSRK), and Joback (and estimate properties). Target Audience Chemical engineers or scientists that work in thermodynamics or process simulation. Example of use from ugropy import Groups hexane = Groups(\"hexane\") print(hexane.unifac_groups) print(hexane.psrk_groups) print(hexane.joback.groups) You will obtain: {'CH3': 2, 'CH2': 4} {'CH3': 2, 'CH2': 4} {'-CH3': 2, '-CH2-': 4} Any help and recommendations are welcome! I hope that the project may be helpful to someone :)"},
{"Title": "Recording and visualising the 20k system calls it takes to \"import seaborn\"", "Author": "u/sYnfo", "Content": "Last time I showed how to count how many CPU instructions it takes to print(\"Hello\") and import seaborn . Here's a new post on how to record and visualise system calls that your Python code makes . Spoiler: 1 for print(\"Hello\") , about 20k for import seaborn , including an execve for lscpu !"},
{"Title": "how to make photos look like paintings [project]", "Author": "u/Feitgemel", "Content": "Hi, üé® Discover how easy it is to transform your own phots into beautiful paintings üñºÔ∏è This is a cool effect based on Stylized Neural Painting library. Simple to use , and the outcome is impressive, You can find instructions here : https://github.com/feitgemel/Python-Code-Cool-Stuff/tree/master/How%20to%20make%20photos%20look%20like%20paintings The link for the tutorial video : https://youtu.be/m1QhxOWeeRc Enjoy Eran #convertphototodigitalart #makephotolooklikepainting #makephotoslooklikepaintings #makepicturelooklikepainting"},
{"Title": "Announcing uv: Python packaging in Rust", "Author": "u/monorepo", "Content": "From the makers of ruff comes uv TL;DR: uv is an extremely fast Python package installer and resolver, written in Rust, and designed as a drop-in replacement for pip and pip-tools workflows. It is also capable of replacing virtualenv . With this announcement, the rye project and package management solution created by u/mitsuhiko (creator of Flask, minijinja, and so much more) in Rust, will be maintained by the astral team. This \"merger\" and announcement is all working toward the goal of a Cargo -type project and package management experience, but for Python. For those of you who have big problems with the state of Python's package and project management, this is a great set of announcements... For everyone else, there is https://xkcd.com/927/ . Twitter Announcement PyPI GitHub Install it today: pip install uv # or pipx install uv # or curl -LsSf https://astral.sh/uv/install.sh | sh"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Solve doubts on Time Series Reporting with Pandas (tutorials+interactive streaming)", "Author": "u/datonsx", "Content": "Hi people, after teaching Python applied to Data Science more than 800 people (the majority 1:1) and producing many courses, I‚Äôve decided to put them on YouTube. They are mostly applied practical cases. For example, this tutorial to preprocess and visualize solar energy generation (Time Series). On top of that, I‚Äôm hosting a weekly live streaming to solve doubts on the tutorials I publish. If you are interested in stopping by and get some doubts solved, feel free to join here ."},
{"Title": "Anaconda Python Distribution Tutorials", "Author": "u/PhilipYip", "Content": "A GitHub repository of Python Tutorials in markdown and notebook format: https://github.com/PhilipYip1988/python-notebooks Covering: Python Installation using Anaconda/Miniconda Python Environments Markdown and TeX Syntax OOP Fundamentals and classes in the Builtins Module IPython Magics Formatters (AutoPEP8, ISort, Black and Ruff) Code Blocks and Comprehensions Collections Module Itertools Module Math Module Statistics Module Random Module Datetime Module File Formats (IO, CSV, Pickle and Shelve Modules) OS Module PathLib Module System Module NumPy Library Pandas Library Matplotlib Library Seaborn Library Plotly Library Pillow Library The tutorials are mainly focused on JupyterLab 4 but also mention how to setup VSCode. These are fairly detailed tutorials and it took me a long time to put together, hopefully it will help some beginners, particularly those looking to use Python for datascience."},
{"Title": "BlackMarblePy: Python Package to Retrieve NASA Black Marble Data", "Author": "u/g4brielvs", "Content": "I'm excited to announce the beta release of BlackMarblePy - a new Python package designed to retrieve NASA Black Marble data. For those unfamiliar, NASA Black Marble imagery provides stunning views of Earth at night, capturing the lights from cities and other human activity. This package aims to make accessing this data easier for researchers, developers, and anyone interested in exploring our planet's nighttime lights. Whether you're studying urbanization, monitoring light pollution, or simply fascinated by Earth's beauty after dark, this package is for you. Key Features: Simple Python interface for accessing NASA Black Marble data. Download daily, monthly, and yearly nighttime lights data for user-specified region of interest and time. Parallel downloading for faster data retrieval and automatic retry mechanism for handling network errors. Access NASA Black Marble as a Xarray Dataset Comprehensive documentation and examples to get you started quickly. How You Can Help: I'm reaching out to the community to gather feedback and suggestions for improvement. Whether you encounter any bugs, have ideas for additional features, or just want to share your experience using the package, your input is invaluable. Blog post: https://blogs.worldbank.org/opendata/illuminating-insights-harnessing-nasas-black-marble-r-and-python-packages Repository: https://github.com/worldbank/blackmarblepy"},
{"Title": "A small python package to scrape images from web", "Author": "u/JOHNJRAMBO2403", "Content": "Hi there, I hope you guys are doing well. I have some amazing news to share with you! I have created a new python library called better_bing_image_downloader . It is a powerful and convenient tool to download bulk images from Bing without any API keys. It is compatible with python 3.x and above and it is hosted on both pip and github. You can install it easily with pip install better-bing-image-downloader . The README.md file on github gives you a comprehensive idea of how to use this package to its fullest potential in any project that requires web image scraping. You can check it out here: Pypi Link or GitHub Repo . I hope this library will be helpful for your data and machine learning projects. I would love to hear your feedback and suggestions!"},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Why use Pycharm Pro in 2024?", "Author": "u/Adorable_Type_2861", "Content": "What‚Äôs the value proposition of Pycharm, compared with VS Vode + copilot suscription? Both will cost about the same yearly. Why would you keep your development in Pycharm? In the medium run, do you see Pycharm pro stay attractive? I‚Äôve been using Pycharm pro for years, and recently tried using VS Code because of copilot. VS Code seems to have better integration of LLM code assistance (and faster development here), and a more modular design which seems promising for future improvements. I am considering to totally shift to VS Code."},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "A rich-text editor with PyQt6", "Author": "u/OvidPerl", "Content": "For those who work with Qt, I'm sure some of you have seen Megasolid Idiom , a simple, rich-text editor. However, it's written with PyQt5. If you're learning graphics programming in Python, that's disappointing since PyQt6 has been around since January of 2021 and appears to be stable. I wanted that editor in PyQt6, so I updated it and shared it on github . It has an MIT license, like the original."},
{"Title": "Alto: CLI tool to run functions, scripts, and Jupyter notebooks in the cloud", "Author": "u/runprism", "Content": "Hey everyone! I want to share with everyone the project I have been working on called Alto. It's a Python CLI tool to run functions, scripts, and Jupyter notebooks in the cloud. Github: https://github.com/runprism/alto Documentation: https://alto.mintlify.app/welcome/v0.0.8/welcome What My Project Does Alto allows data practitioners to effortlessly run functions, scripts, and Jupyter notebooks in the cloud. Currently, Alto supports running code in EC2 instances. However, we're working on adding more infrastructure types (e.g., EMR clusters) and cloud providers (GCP, Azure, etc.) Target Audience For data teams, building and maintaining infrastructure is difficult. It's also not exactly within their wheelhouse. I built Alto for data analysts / data scientists (like myself) who wish to run data workloads in the cloud without having to deal with provisioning the infrastructure to do so. Alto is an early-stage project, so use in production at your own risk. Comparison Alto allows users to perform serverless data execution, similar to AWS Lambda. There are a few features that make Alto distinct: Code format. Alto supports running functions, scripts, and Jupyter notebooks, whereas Lambda is primarily used for executing functions. Local development. Alto allows users to run their code in the cloud as is. In other words, users can develop and test their code on their local machine and then deploy to the cloud when they're ready. They could also choose to develop in the cloud itself."},
{"Title": "CLI tools hidden in the Python standard library", "Author": "u/LinearArray", "Content": "Found a cool resource which explains the CLI tools hidden in the Python Standard Library. Link : https://til.simonwillison.net/python/stdlib-cli-tools"},
{"Title": "ADIX: Data Exploration Made Easy & Colorful with themes you can customize (EDA)", "Author": "u/xmooger", "Content": "Hi everyone, I have released a beta version of ADIX - a new Python package for Data Scientists and anyone interested in data analysis. What My Project Does ADIX works in Jupyter notebooks or Google colab and is specifically focused on streamlining the exploratory data analysis process, making it incredibly easy and fast to gain insights from your dataset with just one command: ix.eda(). This allows you to quickly understand your data and be more productive in your research. Target Audience Data scientists, ML engineers and anyone interested in data analysis / exploration. PyPi : https://pypi.org/project/adix/ I'd greatly appreciate any feedback you may have. And if you like the project please consider giving a star on GitHub . You can find the documentation here . Main features üé® Customizable Themes: Personalize your analysis environment with your own unique style using customizable color schemes! ‚ö° Efficient Cache Utilization: Say goodbye to long loading times! ADIX leverages optimized caching mechanisms for lightning-fast performance. üîç Rapid Data Insight: ADIX prioritizes speed, delivering crucial insights at your fingertips for quick decision-making. üî¢ Automatic Type Detection: Let ADIX do the heavy lifting! It automatically detects numerical, categorical, and text features, with the option for manual overrides when needed. üìä Statistically Rich Summary Information: Unveil the intricate details of your data with comprehensive summaries, including type identification, unique values, missing values, and more. Dive deep into numerical data with properties like min-max range, quartiles, and skewness. üìà Univariate and Bivariate Statistics Unveiled: Explore univariate and bivariate insights with ADIX's versatile visualization options. From bar charts to matrices, and box plots, uncover a multitude of ways to interpret and analyze your data effectively. Comparison Automated data exploration is a popular topic and there are some alternatives that work similarly, for example sweetviz or lux ."},
{"Title": "periodic-table-cli: An interactive Periodic Table of Elements App for the Console! Now for Python!", "Author": "u/spirometaxas", "Content": "What My Project Does : It's a lightweight and fun Periodic Table of Elements app for the console. Supports Browse and Search for all 118 elements, and 20 Display Modes.  Interact with the periodic table like never before as you command the elements with just a few keystrokes the cool way, in the console.  A few months ago, I built this app for NodeJS. After receiving lots of positive reviews and feedback, I decided to rebuild it again for Python! PYPI : periodic-table-cli The controls are the same. Use Arrows for navigation. Use Slash/BackSlash to toggle 20 display modes. Use Letters/Numbers to search and Enter to select. Use ESC or CTRL+C to quit. Just make sure to have Python 3.8+ installed and use a terminal that supports 256 colors. Tested on MacOS (Terminal + iTerm2), Ubuntu (Terminal + XTerm), and Windows Command Prompt (requires windows-curses). Target Audience : Scientists, chemists, developers, terminal enthusiasts, and anyone who just likes cool tools. Comparison : It's not the first time this idea has been done, but this one has more browse features, display modes, and an overall cool minimalist design. More directions available in the links below. Enjoy! Homepage : https://spirometaxas.com/projects/periodic-table-cli PYPI Page : https://pypi.org/project/periodic-table-cli/ Source Code : https://github.com/spirometaxas/periodic-table-cli-py"},
{"Title": "slow-learner ‚Äî type generation from data", "Author": "u/nj_vs_valhalla", "Content": "What My Project Does : A library and CLI that reads a stream of data and generates Python types describing it. See below for a detailed description and an example. Target Audience : Primarily web developers, but also anyone who works with structured data. Stability-wise, it is in beta, but fairly well-tested and I haven't encountered major problems using it regularly for the past year. Comparison : There are libraries that do the same, but output JSON Schema (e.g. genson). To the best of my knowledge, no other library generates Python types directly from data. A while ago I found myself working on a large distributed backend codebase with almost no types / schemas available for inter-service RPCs. Everything was just pure dicts! I hated it, so I decided to make a \"type learner\" that would consume a stream of data and generate Python types. Initially I built it for myself and it may reflect my specific use-case, but I hope that I was able to make it quite generic so it can be useful for others. For example, let's generate types for GitHub REST API # fetch GitHub releases list gh api \\ -H \"Accept: application/vnd.github+json\" \\ -H \"X-GitHub-Api-Version: 2022-11-28\" \\ /repos/facebook/react/releases --paginate |> data.json slow-learner learn --spread --type-name Release data.json The result is \"\"\" This file contains Python 3.8+ type definitions generated by TypeLearner from 99 observed value(s) Source JSON files: - /Users/njvh/Documents/Personal/slow-learner/data.json \"\"\" from typing import List from typing import Literal from typing_extensions import NotRequired from typing import Optional from typing import TypedDict from typing import Union class ReleaseAuthor(TypedDict): login: str id: int node_id: str avatar_url: str gravatar_id: Literal[\"\"] url: str html_url: str followers_url: str following_url: str gists_url: str starred_url: str subscriptions_url: str organizations_url: str repos_url: str events_url: str received_events_url: str type: Literal[\"User\"] site_admin: Literal[False] class ReleaseAssetsItemUploader(TypedDict): login: str id: int node_id: str avatar_url: str gravatar_id: Literal[\"\"] url: str html_url: str followers_url: str following_url: str gists_url: str starred_url: str subscriptions_url: str organizations_url: str repos_url: str events_url: str received_events_url: str type: Literal[\"User\"] site_admin: Literal[False] class ReleaseAssetsItem(TypedDict): url: str id: int node_id: str name: str label: Optional[str] uploader: ReleaseAssetsItemUploader content_type: Union[ Literal[\"text/javascript\"], Literal[\"application/javascript\"], Literal[\"application/x-javascript\"], Literal[\"application/zip\"], ] state: Literal[\"uploaded\"] size: int download_count: int created_at: str updated_at: str browser_download_url: str ReleaseReactions = TypedDict( \"ReleaseReactions\", { \"url\": str, \"total_count\": int, \"+1\": int, \"-1\": Literal[0], \"laugh\": int, \"hooray\": int, \"confused\": Literal[0], \"heart\": int, \"rocket\": int, \"eyes\": int, }, ) class Release(TypedDict): url: str assets_url: str upload_url: str html_url: str id: int author: ReleaseAuthor node_id: str tag_name: str target_commitish: str name: str draft: Literal[False] prerelease: bool created_at: str published_at: str assets: List[ReleaseAssetsItem] tarball_url: str zipball_url: str body: str reactions: NotRequired[ReleaseReactions] Github | PyPI | Read more"},
{"Title": "file_unittest: a Python package to run unit tests based on text file results", "Author": "u/teamamentum", "Content": "What My Project Does Ever wanted to have a unittest compare results to reference data from a text file, and fail if they don't match? So did we! We have just opened sourced our Python package that extends Python unittest to enable comparison to data from a reference text file containing data. Target Audience Python developers, data scientists, looking to include file comparison in their unit testing. Comparison Not the first time this has been done. This just packages functionality into an easy to install and use Python package. Clone the project git clone https://github.com/amentumspace/file_unittest Installation The package can be installed into any active environment by running: cd file_unittest/ pip install . file_unittest.TestCase This class extends the unittest.TestCase class to allow the user to run unit tests by writing data to a file and verifying the output has not changed since the last time the unit test was run. Once the user is satisfied that the output data is correct, these unit test simply ensure that the data does not change with changes in the code base. Usage Rather than inherit from unittest.TestCase the user should derive a class from file_unittest.TestCase . Like the normal unit test, individual test functions are appended with test_ . Any desired output data is written to file by calling self.output Example unit test file: import unittest import file_unittest class MyTest(file_unittest.TestCase): def test_case(self): self.output(\"hello\") if __name__ == '__main__': unittest.main() Unlike the normal unittest.TestCase the user does not need to make any assertion calls eg. self.assertTrue , self.assertFalse etc. This class will take care of warning about any differences in output. Default output location The default location to output the test results will be: {derived_class_filepath}/test_results/ {derived_class_filename}.{derived_class_name}.{test_name}.txt Missing or different results If the expected output file is missing, or differences are detected, the output data will be written to the same file, but with .new postfix: {derived_class_filepath}/test_results/ {derived_class_filename}.{derived_class_name}.{test_name}.new On the first run, the user will need to inspect the .new file for expected results; Otherwise, if differences are detected, the user can diff the .txt and .new files to investigate and resolve any differences; In both cases once the user is satisfied with the results, the .new files can be renamed with .txt  extension and the .txt files can be checked into the git repo Committing test result files to source control Once the output .txt files have been satisfactorily generated as in the previous step, they should be checked into source control so that they can be used as the benchmark for future runs."},
{"Title": "First python project - Low latency offline voice recognition to remap your voice/words to keybinds.", "Author": "u/Captain-H2O", "Content": "What My Project Does I just dropped my first open source project for python on github! Its a speeech recognition program written 100% in python. I made it to help me while I play star citizens by rebinding the keys too speech recognition commands, that way I dont need to remember over 100 keybinds or accidently click the wrong one. Comparison (A brief comparison explaining how it differs from existing alternatives.) Some features that I have not seen in other similar programs. Such as A topmost/ Picture in picture Browser function so that users can simply call a voice command and it will open a Picture in picture browser over top of the game, that they can interact with and use so they do not have to switch to desktop open a browser type in the url, and the switch back. They simply can interact with a browser on top of their game. and close it without ever having to leave i think its pretty neat idea. Target Audience (e.g., Is it meant for production, just a toy project, etc. So far im pretty pleased, and would like anyone thats intersted in checking out other peoples side/hobby projects to check it out! Ive gone through tons of offline speech recognition librarys like vosk and sphinx but none of them can compete in the terms of speed, and accuracy like the windows speech recognition uses. Im using a python libary called Dragonfly2. So far it is working great on windows 10 and windows 11 computers. Its extremely fast from voice command to keypress. and im pretty happy so far with my side project and the results! I figured I would post it here and see what everyone here thinks about it so far? This is my first time releaseing a python script I've created, Ive made a few small things here and there with python but never released anything so I decided I might as well start https://github.com/techsavvy42/Captains_Voice_Commander"},
{"Title": "Here's my ideal python publishing process - would you use it? why or why not?", "Author": "u/burnfearless", "Content": "I've finally implemented my \"ideal state\" for Python release management on GitHub. Focused around these three tenets: Must be idiot proof , meaning: all checks are automated and operations work is automated. Must be runnable from a mobile browser. If your publish operation requires a laptop, you are probably doing it wrong. Must decouple marketing work from engineering work , while not duplicating efforts. And here's how I got there: Semantic pull request titles. Basically, take a conventional commits approach but apply them to your PR titles (which are trivial to edit+groom) instead of commit history (which is frustrating to edit+groom). No changelogs. If you use Semantic PRs, and if you default to those PR titles as your commit message when merging, then your main branch commit history is your changelog. No version bumps. Instead use dynamic versioning, such as provided by the poetry-dynamic-versioning plugin. Automatically-drafted release notes. If we've followed all the above, then a machine can create and continually edit a release notes draft (including calculating the correct version bump and the ability to generate and categorize changelog entries), which means at any moment we are ready to... One-click publish. Optionally groom before you do, but if you've followed the above four steps, it's just a single click to \"publish\" the release in GitHub. A GitHub Action can listen for the new release tag and respond by publishing to PyPi. My tools: Semantic PR checker: https://github.com/zeke/semantic-pull-requests Poetry dynamic versions plugin: https://github.com/mtkennerly/poetry-dynamic-versioning Release drafter: https://github.com/marketplace/actions/release-drafter Let me know what you think! Would you adopt this? What would you change?"},
{"Title": "Thread Local Data in Python", "Author": "u/Any-Tune-3880", "Content": "Since threads in Python share the memory space of their parent process, we might need to define thread-specific variables for specific use cases to avoid unintended side effects. In this post, we will: Explore Thread local storage: Python threading module's solution for thread-specific/thread-private values. See its real-world example from Open-Source ( usage in the Peewee ORM library). Look at the CPython source code to see how thread local storage is implemented under the hood. Background Threads share the memory space of their parent process. This will allow us to seamlessly access and share variables, data structures, etc., across threads. But this comes with its own challenges. There may be scenarios where we need to isolate variables and might need to store data specific to each thread. Thread local storage can be leveraged in this case. We can use local() found in the threading module to define thread-local variables. import threading import time # Create a thread-local storage object (1) thread_local = threading.local() def init_data(number): thread_local.number = number * 100 def show_data(): print(f\"Thread {threading.current_thread().name} has number {thread_local.number}\") def worker(number): init_data(number) for _ in range(3): time.sleep(1) show_data() thread1 = threading.Thread(target=worker, name=\"A\", kwargs={\"number\": 1}) thread2 = threading.Thread(target=worker, name=\"B\", kwargs={\"number\": 2}) thread1.start() thread2.start() thread1.join() thread2.join() The program output will be: Thread A has number 100 Thread B has number 200 Thread A has number 100 Thread B has number 200 Thread A has number 100 Thread B has number 200 As seen in comment #(1) , a thread-local storage object was created and assigned to the variable thread_local . Arbitrary attributes can be assigned to this variable, which are specific to the thread that performs the assignment and is isolated from others. In the example, each thread stores its own number attribute to the thread_local object and accesses the thread-specific value during their concurrent execution. Usage in the Wild Peewee, a Python ORM, utilizes thread-local data in its ThreadSafeDatabaseMetadata to support dynamic database switches at runtime in multithreaded applications. The source code can be found here. # File: peewee/playhouse/shortcuts.py class ThreadSafeDatabaseMetadata(Metadata): \"\"\" Metadata class to allow swapping database at run-time in a multi-threaded application. To use: class Base(Model): class Meta: model_metadata_class = ThreadSafeDatabaseMetadata \"\"\" def __init__(self, *args, **kwargs): # The database attribute is stored in a thread-local. self._database = None self._local = threading.local() super(ThreadSafeDatabaseMetadata, self).__init__(*args, **kwargs) def _get_db(self): return getattr(self._local, \"database\", self._database) def _set_db(self, db): if self._database is None: self._database = db self._local.database = db database = property(_get_db, _set_db) In multithreaded applications using peewee ORM, database switching at runtime without using ThreadSafeDatabaseMetada can lead to errors. If multiple threads work in parallel and one thread changes the connection parameters, this can lead to errors such as writing to a wrong DB, inconsistent writes (in case of non-atomic DB operations), etc. ThreadSafeDatabaseMetada solves this by keeping the database attributes in a thread-local object ( self._local ). In this way, dynamic changes to the database will only affect the thread that made the change. Other threads will keep working with their existing databases. Should I use it? Thread-local storage should be used when: You are writing a multi-threaded application (obviously) and: If some variables are used by the current thread only and are not relevant to the main thread/other threads. You find out that changes made by one thread can lead to unintended side effects in other concurrent threads. Generally, if you are working with multiple threads and there is shared mutable data: You should check if sharing these data across threads is actually needed. If sharing can be avoided, use thread-local storage to make the data specific to each thread. Otherwise, implement locks or other synchronization primitives to enforce thread safety. Note : Context variables from the contextvars standard library module can be used as an alternative to threading.local() . They work with multithreading as well as asyncio to store context-specific information. In the case of asyncio programs, context variables allow each coroutine task to have its own set of variables isolated from other (asyncio) tasks. Behind the scenes The Python implementation of threading.local can be found in the /Lib/_threading_local.py path in CPython source code. The _localimpl class is used to store thread-local values. class _localimpl: \"\"\"A class managing thread-local dicts\"\"\" __slots__ = 'key', 'dicts', 'localargs', 'locallock', '__weakref__' def __init__(self): # The key used in the Thread objects' attribute dicts. # We keep it a string for speed but make it unlikely to clash with # a \"real\" attribute. self.key = '_threading_local._localimpl.' + str(id(self)) # { id(Thread) -> (ref(Thread), thread-local dict) } self.dicts = {} The dicts attribute maps the id of a thread to a tuple. The tuple is two-membered, containing a reference to the thread and the actual dictionary storing thread local values ( # { id(Thread) -> (ref(Thread), thread-local dict) } ). Looking at a few other methods of the class: class _localimpl: \"\"\"A class managing thread-local dicts\"\"\" def get_dict(self): \"\"\"Return the dict for the current thread. Raises KeyError if none defined.\"\"\" thread = current_thread() return self.dicts[id(thread)][1] # (1) returning the local dict of current thread def create_dict(self): \"\"\"Create a new dict for the current thread, and return it.\"\"\" localdict = {} key = self.key thread = current_thread() idt = id(thread) ... wrthread = ref(thread, thread_deleted) thread.__dict__[key] = wrlocal self.dicts[idt] = wrthread, localdict  # (2) Populating the `dicts` with a new thread return localdict The comments #(1) & #(2) illustrate operations on the dicts attribute discussed previously. (1) : Returns the local data dictionary corresponding to the current thread accessing it. (2) : This part initializes the local dict for a new thread. Then, we have the actual local callable, which we call as threading.local() to initialize a thread-local object. @contextmanager def _patch(self): impl = object.__getattribute__(self, '_local__impl') try: dct = impl.get_dict()  # (3) this will return local dict of current thread -/ # see its definition in the above snippet except KeyError: ... # calls _localimpl`s create_dict to create & init a new dict with impl.locallock: object.__setattr__(self, '__dict__', dct) # (4) <The magic> Temporarily replaces -/ # the instance's __dict__ attribute with the thread-specific dictionary. yield class local: __slots__ = '_local__impl', '__dict__' def __new__(cls, /, *args, **kw): ... impl = _localimpl()  # (1) - wraps the _localimpl object in an attribute impl.localargs = (args, kw) impl.locallock = RLock()  # (2) - Lock for thread safety object.__setattr__(self, '_local__impl', impl) # We need to create the thread dict in anticipation of # __init__ being called, to make sure we don't call it # again ourselves. impl.create_dict() return self def __getattribute__(self, name): with _patch(self): return object.__getattribute__(self, name) def __setattr__(self, name, value): if name == '__dict__': raise AttributeError( \"%r object attribute '__dict__' is read-only\" % self.__class__.__name__) with _patch(self): return object.__setattr__(self, name, value) ... Let's look at various parts of the code one by one. # (1) :  An object of _localimpl class is created and later stored in _local__impl attribute # (2) : RLock() is used on dict operations - Concurrent writes by multiple threads may cause 'lost writes' since dict is not thread-safe. # (3) : In the try/except block, the current thread's local data is fetched and assigned to the variable named dct . # (4) : The magic happens here. Here is a quick refresher on class attributes before we delve further: A class has a namespace implemented by a dictionary object. Class attribute references are translated to lookups in this dictionary, e.g., C.x is translated to C. dict [\"x\"] (although there are a number of hooks which allow for other means of locating attributes). The _patch function we are currently in is a context manager. Using the line marked as # (4) , it patches the namespace dictionary of the local class with the dot variable (which stores the current-thread specific data). Since the __getattribute__ and __setattr__ dunders of the class local use the _patch context manager, attribute access performed inside the context will use the thread-local dictionary ( dct ) replacing the class's actual namespace dictionary. Note : The Lib/_threading_local.py starts with the below note: Note that this module provides a Python version of the threading.local class. Depending on the version of Python you're using, there may be a faster one available. You should always import the local class from threading . The code we looked at might not be the one running in our Python installations. I think newer Pythons versions are using C implementations of the thread local functionality for efficiency. ... I share interesting Python snippets üêç like this from open-source projects illustrating Python language features in my newsletter, \"Python in the Wild\". Subscribe to the newsletter on Substack or Linkedin to receive new Pythonic posts to your email üíåüöÄ."},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Stockdex - simple package for extracting stocks financial data from yahoo finance", "Author": "u/nginx26", "Content": "Happy weekend every one! I have recently created my first python package. What My Project Does The package helps with extracting financial data of companies from Yahoo into pandas dataframes using companies stocks symbol(like yfinance package) Target Audience The package is targeted at people who are interested in financial data of companies and want to use it for analysis or other purposes. Feel free to give you opinion or open issues on the Github project: Github Repo Link Pypi link"},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "ugropy: Group detection for thermodynamic UNIFAC/Joback models in Python", "Author": "u/SalvadorBrandolin", "Content": "Hi there! I'm working in ugropy , a Python library for molecule fragmentation to get the UNIFAC/Joback subgroups from the molecule's name or SMILES. GitHub PyPi What My Project Does By now ugropy can detect the subgroups for classic liquid-vapor UNIFAC, Predictive Soave-Redlich-Kwong (PSRK), and Joback (and estimate properties). Target Audience Chemical engineers or scientists that work in thermodynamics or process simulation. Example of use from ugropy import Groups hexane = Groups(\"hexane\") print(hexane.unifac_groups) print(hexane.psrk_groups) print(hexane.joback.groups) You will obtain: {'CH3': 2, 'CH2': 4} {'CH3': 2, 'CH2': 4} {'-CH3': 2, '-CH2-': 4} Any help and recommendations are welcome! I hope that the project may be helpful to someone :)"},
{"Title": "Recording and visualising the 20k system calls it takes to \"import seaborn\"", "Author": "u/sYnfo", "Content": "Last time I showed how to count how many CPU instructions it takes to print(\"Hello\") and import seaborn . Here's a new post on how to record and visualise system calls that your Python code makes . Spoiler: 1 for print(\"Hello\") , about 20k for import seaborn , including an execve for lscpu !"},
{"Title": "how to make photos look like paintings [project]", "Author": "u/Feitgemel", "Content": "Hi, üé® Discover how easy it is to transform your own phots into beautiful paintings üñºÔ∏è This is a cool effect based on Stylized Neural Painting library. Simple to use , and the outcome is impressive, You can find instructions here : https://github.com/feitgemel/Python-Code-Cool-Stuff/tree/master/How%20to%20make%20photos%20look%20like%20paintings The link for the tutorial video : https://youtu.be/m1QhxOWeeRc Enjoy Eran #convertphototodigitalart #makephotolooklikepainting #makephotoslooklikepaintings #makepicturelooklikepainting"},
{"Title": "Announcing uv: Python packaging in Rust", "Author": "u/monorepo", "Content": "From the makers of ruff comes uv TL;DR: uv is an extremely fast Python package installer and resolver, written in Rust, and designed as a drop-in replacement for pip and pip-tools workflows. It is also capable of replacing virtualenv . With this announcement, the rye project and package management solution created by u/mitsuhiko (creator of Flask, minijinja, and so much more) in Rust, will be maintained by the astral team. This \"merger\" and announcement is all working toward the goal of a Cargo -type project and package management experience, but for Python. For those of you who have big problems with the state of Python's package and project management, this is a great set of announcements... For everyone else, there is https://xkcd.com/927/ . Twitter Announcement PyPI GitHub Install it today: pip install uv # or pipx install uv # or curl -LsSf https://astral.sh/uv/install.sh | sh"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Solve doubts on Time Series Reporting with Pandas (tutorials+interactive streaming)", "Author": "u/datonsx", "Content": "Hi people, after teaching Python applied to Data Science more than 800 people (the majority 1:1) and producing many courses, I‚Äôve decided to put them on YouTube. They are mostly applied practical cases. For example, this tutorial to preprocess and visualize solar energy generation (Time Series). On top of that, I‚Äôm hosting a weekly live streaming to solve doubts on the tutorials I publish. If you are interested in stopping by and get some doubts solved, feel free to join here ."},
{"Title": "Anaconda Python Distribution Tutorials", "Author": "u/PhilipYip", "Content": "A GitHub repository of Python Tutorials in markdown and notebook format: https://github.com/PhilipYip1988/python-notebooks Covering: Python Installation using Anaconda/Miniconda Python Environments Markdown and TeX Syntax OOP Fundamentals and classes in the Builtins Module IPython Magics Formatters (AutoPEP8, ISort, Black and Ruff) Code Blocks and Comprehensions Collections Module Itertools Module Math Module Statistics Module Random Module Datetime Module File Formats (IO, CSV, Pickle and Shelve Modules) OS Module PathLib Module System Module NumPy Library Pandas Library Matplotlib Library Seaborn Library Plotly Library Pillow Library The tutorials are mainly focused on JupyterLab 4 but also mention how to setup VSCode. These are fairly detailed tutorials and it took me a long time to put together, hopefully it will help some beginners, particularly those looking to use Python for datascience."},
{"Title": "BlackMarblePy: Python Package to Retrieve NASA Black Marble Data", "Author": "u/g4brielvs", "Content": "I'm excited to announce the beta release of BlackMarblePy - a new Python package designed to retrieve NASA Black Marble data. For those unfamiliar, NASA Black Marble imagery provides stunning views of Earth at night, capturing the lights from cities and other human activity. This package aims to make accessing this data easier for researchers, developers, and anyone interested in exploring our planet's nighttime lights. Whether you're studying urbanization, monitoring light pollution, or simply fascinated by Earth's beauty after dark, this package is for you. Key Features: Simple Python interface for accessing NASA Black Marble data. Download daily, monthly, and yearly nighttime lights data for user-specified region of interest and time. Parallel downloading for faster data retrieval and automatic retry mechanism for handling network errors. Access NASA Black Marble as a Xarray Dataset Comprehensive documentation and examples to get you started quickly. How You Can Help: I'm reaching out to the community to gather feedback and suggestions for improvement. Whether you encounter any bugs, have ideas for additional features, or just want to share your experience using the package, your input is invaluable. Blog post: https://blogs.worldbank.org/opendata/illuminating-insights-harnessing-nasas-black-marble-r-and-python-packages Repository: https://github.com/worldbank/blackmarblepy"},
{"Title": "A small python package to scrape images from web", "Author": "u/JOHNJRAMBO2403", "Content": "Hi there, I hope you guys are doing well. I have some amazing news to share with you! I have created a new python library called better_bing_image_downloader . It is a powerful and convenient tool to download bulk images from Bing without any API keys. It is compatible with python 3.x and above and it is hosted on both pip and github. You can install it easily with pip install better-bing-image-downloader . The README.md file on github gives you a comprehensive idea of how to use this package to its fullest potential in any project that requires web image scraping. You can check it out here: Pypi Link or GitHub Repo . I hope this library will be helpful for your data and machine learning projects. I would love to hear your feedback and suggestions!"},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Why use Pycharm Pro in 2024?", "Author": "u/Adorable_Type_2861", "Content": "What‚Äôs the value proposition of Pycharm, compared with VS Vode + copilot suscription? Both will cost about the same yearly. Why would you keep your development in Pycharm? In the medium run, do you see Pycharm pro stay attractive? I‚Äôve been using Pycharm pro for years, and recently tried using VS Code because of copilot. VS Code seems to have better integration of LLM code assistance (and faster development here), and a more modular design which seems promising for future improvements. I am considering to totally shift to VS Code."},
{"Title": "Modguard - a lightweight python tool for enforcing modular design", "Author": "u/the1024", "Content": "https://github.com/Never-Over/modguard We built modguard to solve a recurring problem that we've experienced on software teams -- code sprawl . Unintended cross-module imports would tightly couple together what used to be independent domains, and eventually create \"balls of mud\" . This made it harder to test, and harder to make changes. Mis-use of modules which were intended to be private would then degrade performance and even cause security incidents. This would happen for a variety of reasons: Junior developers had a limited understanding of the existing architecture and/or frameworks being used It's significantly easier to add to an existing service than to create a new one Python doesn't stop you from importing any code living anywhere When changes are in a 'gray area', social desire to not block others would let changes through code review External deadlines and management pressure would result in \"doing it properly\" getting punted and/or never done The attempts to fix this problem almost always came up short. Inevitably, standards guides would be written and stricter and stricter attempts would be made to enforce style guides, lead developer education efforts, and restrict code review. However, each of these approaches had their own flaws. The solution was to explicitly define a module's boundary and public interface in code, and enforce those domain boundaries through CI. This meant that no developer could introduce a new cross-module dependency without explicitly changing the public interface or the boundary itself. This was a significantly smaller and well-scoped set of changes that could be maintained and managed by those who understood the intended design of the system. With modguard set up, you can collaborate on your codebase with confidence that the intentional design of your modules will always be preserved. modguard is: fully open source able to be adopted incrementally implemented with no runtime footprint a standalone library with no external dependencies interoperable with your existing system (cli, generated config) We hope you give it a try! Would love any feedback."},
{"Title": "Hunting down n + 1 queries in your Django application code", "Author": "u/paulg1989", "Content": "Libraries such as django-silk are excellent for profiling the queries executed by your Django application. We have found, however, that they do not provide a completely straightforward way to identify the lines of your application code which are responsible for executing the most queries. django-queryhunter aims to fill that gap by providing a simple code-first approach to query profiling. This is achieved by providing a context manager and middleware which can provide a detailed report of the lines of your application code which are responsible for executing SQL queries, including data on: * The module name and the line number of the code which executed the query. * The executing code itself on that line. * The number of times that line was responsible for executing a query and the total time that line spent querying the database. * The last SQL statement executed by that line. * If installed as a middleware, the URL and HTTP method of the request which invoked the query. Here's some sample output: queryhunter/tests/my_module.py ==================================== Line no: 13 | Code: for post in posts: | Num. Queries: 1 | SQL: SELECT \"tests_post\".\"id\", \"tests_post\".\"content\", \"tests_post\".\"author_id\" FROM \"tests_post\" | Duration: 4.783299999999713e-05 Line no: 14 | Code: authors.append(post.author.name) | Num. Queries: 5 | SQL: SELECT \"tests_author\".\"id\", \"tests_author\".\"name\" FROM \"tests_author\" WHERE \"tests_author\".\"id\" = %s LIMIT 21 | Duration: 8.804199999801199e-05 One particularly useful feature of this view of profiling is quickly identifying n+1 queries caused by missing select_related or prefetch_related calls. Using queryhunter Let's suppose we have a Django application with two simple models Author and Post where Post has a foreign key author pointing to the Author model. Let's also suppose we have a simple view function get_authors as below, which examines all Posts and fetches the author names in list: # queryhunter/tests/views.py def get_authors(request): # url: /mysite.com/v1/get_authors/ authors = [] posts = Post.objects.all()  # suppose we have 5 posts for post in posts: authors.append(post.author.name) return JsonResponse({'authors': authors}) With the queryhunter middleware installed, requests to the get_authors view will be automatically profiled, with output printed like below. Notice that the code responsible for invoking the SQL query appears next to the SQL statement itself and to the URL of the request which invoked it: queryhunter/tests/my_module.py ==================================== Line no: 6 | Code: for post in posts: | Num. Queries: 1 | Duration: 0.04 | url: /mysite.com/v1/get_authors/ | method: GET | SQL: SELECT \"tests_post\".\"id\", \"tests_post\".\"content\", \"tests_post\".\"author_id\" FROM \"tests_post\" Line no: 7 | Code: authors.append(post.author.name) | Num. Queries: 5 | Duration: 0.05 | url: /mysite.com/v1/get_authors/ | method: GET | SQL: SELECT \"tests_author\".\"id\", \"tests_author\".\"name\" FROM \"tests_author\" WHERE \"tests_author\".\"id\" = %s LIMIT 21 What can we learn from this output? * The line authors.append(post.author.name) was responsible for executing 5 SQL queries, one for each post. * This is a quick way to identify that we are missing a select_related('author') call in our Post.objects.all() query. This may have been obvious in this contrived example, but in a large code base, flushing out these kinds of issues can be very handy. Sharing here in case others find the library useful."},
{"Title": "pytest-bisect-tests - find test leaking state and breaking other tests", "Author": "u/Askriz", "Content": "Hey all, Sometimes it happens that a test passes successfully when executed in isolation, but fails when the whole suite is run. Finding the offending test is annoying when you have 10000 tests to check. I have written a small utility script that helps with finding the offending test, available here: https://github.com/maciej-gol/pytest-bisect-tests . There is already a similar alternative, but the author wasn't keen to hear suggestions around improving the UX so I thought I would go ahead and simplify the UX for the users (+ a few more goodies). Happy to hear your feedback!"},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "So long, Django Migrations! You were good to us.", "Author": "u/rotemtam", "Content": "Hi All, My name is Rotem, I'm one of the creators of Atlas , an open-source schema migrations tool ( GitHub ). We have recently released a \"schema loader\" plugin for Django, which enables Atlas to seamlessly read Django schemas and manage the database schema for them. The provider's code on GitHub is available here . Effectively, this means you can stop using python manage.py migrate and python manage.py makemigration and replace them with Atlas. But why replace Django Migrations? Among the many ORMs available in our industry, Django's automatic migration is one of the most powerful and robust. It can handle a wide range of schema changes, including adding new tables and columns, basic indexes, and more. However, having been created in 2014, a very different era in software engineering, it naturally has some limitations. Some of the limitations of Django's migration system include: Database Features. Because it was created to provide interoperability across database engines, Django's migration system is centered around the \"lowest common denominator\" of database features. More advanced features such as Triggers, Views, and Stored Procedures have very limited support and require developers to jump through all kinds of hoops to use them. Ensuring Migration Safety. Migrations are a risky business. If you're not careful, you can easily cause data loss or a production outage. Django's migration system does not provide a native way to ensure that a migration is safe to apply. Modern Deployments. Django does not provide native integration with modern deployment practices such as GitOps or Infrastructure-as-Code . Atlas, on the other hand, lets you manage your Django applications using the Database Schema-as-Code paradigm. This means that you can use Atlas to automatically plan schema migrations for your Django project, and then apply them to your database. Using Atlas, you can enjoy automatic migration planning, automatic code review, and integrations with your favorite CI/CD tools. Read the Full Guide Here As always, looking to hear your feedback and thoughts R"},
{"Title": "League of Legends 'mod' where you lose an in-game spell if you answer a trivia question incorrectly!", "Author": "u/wrong_way_wonders_", "Content": "It is a quite simple app that runs while you play League of Legends. It was quite fun to create and use! üëâ You can find a repo. with the code in here if you want to check it out. FYI, I am not a super experienced python programmer. I have been using it for a few years and always try to improve little by little. Thanks and happy coding!"},
{"Title": "OnePyece : Python package which provides various data from OnePiece Universe", "Author": "u/IcePick444", "Content": "Hello, I just released a new version of my API Wrapper in Python called \"OnePyece\", to get various data from OnePiece ! It uses this source API : https://www.api-onepiece.com/en Which was only available in french but has an english version now ! Thats why I'm sharing with you my python package to communicate with this API very easily in Python : https://github.com/icepick4/onepyece/ If you want to try it just follow the README instructions ! Have fun with the OnePiece universe in available Python ! If you like the project do not forget to star it on GitHub ;)"},
{"Title": "Handling Tasks in Asyncio Like a Pro | Jacob Padilla", "Author": "u/jpjacobpadilla", "Content": "Python‚Äôs Asyncio module is a great way to handle IO-bound jobs and has seen many improvements in recent Python updates. However, there are so many ways to handle async tasks that figuring out which method to use for different scenarios can be a bit confusing. In my new article , I first go over the basics of what a task object is and then talk about all of the different ways to handle them and the pros and cons of each."},
{"Title": "Add Null Safety", "Author": "u/brand02", "Content": "It would hurt simplicity but it is beyond that point. Python feels like Scratch compared to other languages at the moment. Lacking this basic feature hurts productivity, I don't want to write 50 lines of \"if not product.name\" etc."},
{"Title": "Kanji Lookup: Search for Japanese Kanji characters by drawing them", "Author": "u/etrotta", "Content": "My first machine learning project I really put out there ; as far as the end user is concerned it should work in a way vaguely similar to Jisho Draw searching or Google Handwriting but is open source. The source code is available in https://github.com/etrotta/kanji_lookup , and you can access an online demo in https://huggingface.co/spaces/etrotta/kanji_lookup . The project steps are roughly: Generate synthetic Kanji images for multiple different fonts (using pillow) Encode the images into embeddings using a Neural Network, using an existing open source Vision Transformer model (using transformers and pytorch) Store the embeddings in a Vector Database (using qdrant) Encode an User submitted image and compare it with existing records Create an web app to make submitting images more convenient (using gradio) Note: The GitHub repository and the HuggingFace Space repositories are separate, the GitHub repository is responsible for creating the embeddings and can search files. The HF Space only contains the gradio app."},
{"Title": "Spotify Developer API", "Author": "u/mr_Pepper762", "Content": "I threw together a bunch of scripts that help visualize user data from the Spotify developer API, including hourly listening patterns, favorite genres, favorite tracks, and favorite artist popularity among Spotify users.  User data can be imported using the spotify_Data_CSV_Github.py script, which appends the data to a CSV file. Repository can be found here: https://github.com/zachzion762/spotify_Developer_API This is my first real python project that I've made on my own with a little bit (a lot) of help from ChatGPT."},
{"Title": "I made an app that adds subtitles to any video", "Author": "u/TimeConsideration336", "Content": "https://github.com/Dimitris02/subtitler/tree/main I made it in python 3.9. It uses the following libraries: speech_recognition, selenium, moviepy, pydub It uses google's speech recognition as well as a deepl.com scraper (deepl is the most accurate translator out there afaik). Usually it works pretty well with the default variables but you can tweak the SILENCE and THRESH variables in the subtitler.py file for better results. You can also change the LANG variable to pick another language. To launch the app run the launch.bat file and enter the filename/path of the video you want to add subtitles to. The default language is Russian because I was testing it on some Tarkovsky movie scenes. I hope you find it useful. Edit: I just added a feature where you can change the TRANSLATE_TO variable in the subtitler.py file to change the language of the resulting subtitles."},
{"Title": "I made a single and batch Universal File Converter for Audio and Video files. (With GUI)", "Author": "u/Gameguy39", "Content": "As stated, I created a (mostly) universal file converter. You can select to convert single files and batch files. Handles most video and audio formats. If I am forgetting anything, let me know and I'll see if I can add it! I needed this to batch convert raw MKV files to MP4 that I could use for editing in Premiere Pro. But I also wanted to make it more cause I know I like to convert files a lot. GitHub Link to Source Code and instructions on how to run the file. You will need FFMPEG installed, but I have instructions for that as well."},
{"Title": "Cookiecutter hooks: who's using them and what for?", "Author": "u/pismyfc", "Content": "Hi all, I'm currently working on a side project ( tplmgr.dev ) which is a software template management platform. Think backstage software templates as a standalone service and simpler! I'm aiming to primarily support Cookiecutter templates in the first instance, but I wanted to get some feedback on people's usage of Cookiecutter hooks , seeing as it's going to be quite complex to support this feature. So, do you use Cookiecutter hooks in your templates? If so, can you link to some examples of usage so that I can get a general idea as to whether supporting this should be prioritised. Thanks for reading!"},
{"Title": "Automate labels and filters setup for gmail", "Author": "u/NinjaSniperC", "Content": "https://github.com/RClayGates/gmail_auto_label I wanted to get the 17,000+ emails in my gmail sorted out. So I made this and I'm so happy with how my project is shaping up! It's my first time ever making a project with anything google api. It goes through the entirety of the gmail account, retrieves the From header for each email. Then creates a dictionary from all the emails, using everything after the @ in the email address. example { \"com\":  [ \"example\", \"info\"], \"org\" : [[ \"School\"], [ \"gov\", \"us\"]] } Then uses that to create nested labels and appropriate filters to add those labels. After those are created it will go back and update every email with the appropriate labels. Any feedback/pointers or even a git pull on making this better would be awesome!"},
{"Title": "Simple Text Extraction Server with Python, Tika, MinIO", "Author": "u/replicantrose", "Content": "Tried to put together a document text extraction server using Apache Tika (with ~30 lines of Python code). This can be used to get the text needed for retrieval-augmented generation or to create LLM training datasets (or frankly, anything else that is text-dependent). It worked out pretty well, so thought it would be cool to share: https://blog.min.io/minio-tika-text-extraction/ An aside on Tika: Apache Tika is time-tested and, by some, considered a legacy toolkit. However, with Tika running as a container and the use of Python bindings, it's possible to get a text extraction experience that is as easy to build with as newer frameworks like Unstructured, but also matches the extraction capability of dedicated extraction models like Nougat. Kind of surprising! Much credit to the tika-python project for making the Python bindings! A further aside on object storage: Furthermore, using a backing object store (i.e. MinIO) to hold the source documents is very useful (whether the extracted text is being used for RAG or an LLM training dataset)."},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Netflix Data Analysis with Python and Pandas", "Author": "u/Prior-Sink-4104", "Content": "https://buzzpy.hashnode.dev/analyzing-netflix-data-with-pandas-python Analysing Netflix Movie Data üëÜ This is better: https://buzzpy.hashnode.dev/beginners-guide-to-pandas"},
{"Title": "PySimpleGUI now closed-source", "Author": "u/rturnbull", "Content": "PySimpleGUI , a popular Python GUI library with 13k GitHub stars went closed source / commercial today. Previously it had been licensed under LGPL. I've got no issue with open source devs making money but to changing the license on a library many have contributed to seems to be pretty poor form. This had been a great cross-platform library for beginners."},
{"Title": "any thing like Word Mail Merge to mox text with dynamic placeholders for data", "Author": "u/siraj-samsudeen", "Content": "We have a large report in Word based on several Excel files containing Budget data. We have to generate a PDF report that contains a narrative that is based on the data and we use pandas to manipulate the data - when the data changes, we want the numbers to be updated - for example, I might have a text that says \"The department with the highest increase in budgeting for Staff costs is XYZ with 76.5% increase\". In addition, we will have several data tables presenting the budget data, summarized by different categories, etc. I am looking to replace Word as there is no easy way to insert dynamic data, except for mail merge, which is quite basic. Jupyter Notebook or Quarto would be great choices, but the end users are not programmers - I was wondering whether there are tools to support such a scenario. <<Update>>: Just got to know - https://docxtemplater.com/ and it looks quite close to what I wanted to have. Updating here for the benefit of others."},
{"Title": "Airbnb scraper made pure in Python", "Author": "u/JohnBalvin", "Content": "The project will get Airbnb's information including images, description, price, title ..etcIt also full search given coordinates https://github.com/johnbalvin/pybnb Install: $ pip install gobnb Usage: from gobnb import * data = Get_from_room_url(room_url,currency,\"\")"},
{"Title": "I made a GeoGebra clone using PyQt6, Matplotlib and NumPy!", "Author": "u/Specialist-Arachnid6", "Content": "Graphyte. is a Python-based graphing application built with PyQt6 and Matplotlib inspired by GeoGebra, designed to provide users with an intuitive interface for creating and visualizing mathematical functions and data sets. Since Graphyte. is a new Graphing software, the list of current features is pretty limited. But I'm focused on implementing more and more features to get it to GeoGebra's level, although that would require much more time. GitHub: https://github.com/rohankishore/Graphyte."},
{"Title": "Supabase Sponsoring OSS on supabase-py", "Author": "u/olirice", "Content": "Supabase is looking to sponsor a an experienced python dev tools OSS contributor to close the gap between our javascript client supabase-js and python client supabase-py Full details and an application link available here"},
{"Title": "I've just released logot - a log testing library", "Author": "u/etianen", "Content": "Hello! üëã I've just released logot , a log testing library. logot has a few unique things, such as being logging-framework-agnostic and having support for testing highly concurrent code using threads or async . But those things are relatively niche. What I'd like to show here are a few examples of how it can be a nice caplog replacement for pytest , even in \"normal\" synchronous code. As a caplog replacement Here's a really simple example testing that a piece of code logs as expected: from logot import Logot, logged def test_something(logot: Logot) -> None: do_something() logot.assert_logged(logged.info(\"Something was done\")) You'll see a couple of things here. A logot fixture, and a logged API. Use these together to make neat little log assertions. The equivalent code using caplog would be: def test_something(caplog: pytest.LogCaptureFixture) -> None: do_something() assert any( record.levelno == logging.INFO and record.message == \"Something was done\" for record in caplog.records ) I think the logot code is clearer, and hopefully you do too! ü§ó Log message matching One of logot s more useful features is the ability to match log messages using %-style placeholders rather than regex. This syntax was chosen to be as close as possible to the % placeholders used by the stdlib logging library. from logot import Logot, logged def test_something(logot: Logot) -> None: do_something() # Match a string placeholder with `%s`. logot.assert_logged(logged.info(\"Something %s done\")) The equivalent using caplog gets pretty verbose: import re def test_something(caplog: pytest.LogCaptureFixture) -> None: do_something() assert any( record.levelno == logging.INFO and re.fullmatch(\"Something .*? done\", record.message, re.DOTALL) for record in caplog.records ) If your message contains everyday punctuation like . , you have to start worrying about regex escaping too! I hope that % -style message matching gives a clearer, more loggy way of matching log messages. Log pattern matching This feature is generally aimed towards testing code using threads or async , where messages can arrive out-of-order. But it's also useful for testing synchronous code. from logot import Logot, logged def test_app(logot: Logot) -> None: do_something() logot.wait_for(logged.info(\"Something happened\") | logged.error(\"Something broke!\")) This example tests whether the INFO log \"Something happened\" or the ERROR \"Something broke!\" was emitted, and passes on either. The equivalent using caplog gets quite long: def test_something(caplog: pytest.LogCaptureFixture) -> None: do_something() assert any( (record.levelno == logging.INFO and record.message == \"Something happened\") or (record.levelno == logging.ERROR and record.message == \"Something broke!\") for record in caplog.records ) I hope you like it! ‚ù§Ô∏è This is only a v1 release, but it's building on a lot of ideas I've been developing in different projects for a while now. I hope you like it, and find it useful. The project documentation is there if you'd like to find out more. üôá"},
{"Title": "PrimeNote: superior sticky note-taking app for Windows and Linux", "Author": "u/crapaud_dindon", "Content": "Introducing PrimeNote , a full-fledged sticky note-taking application for Windows and Linux. Unleash your creativity and optimize your workflow with an intuitive interface packed with powerful features. PrimeNote empowers you to: Capture ideas effortlessly in various formats, from plain text and rich text to images and even full-fledged Vim or terminal environments. Organize your notes impeccably with a folder-based structure for instant retrieval. Safeguard confidential information with robust Fernet encryption. Navigate your notes with lightning speed using the built-in keyboard-driven search tool. Craft a personalized workspace by tailoring the appearance, menus, hotkeys, and mouse events to your preferences. Transmit content seamlessly in real-time by sharing your note repository on a virtual machine. Landing page (with screenshots) Gitlab repository AUR page"},
{"Title": "Modguard - a lightweight python tool for enforcing modular design", "Author": "u/the1024", "Content": "https://github.com/Never-Over/modguard We built modguard to solve a recurring problem that we've experienced on software teams -- code sprawl . Unintended cross-module imports would tightly couple together what used to be independent domains, and eventually create \"balls of mud\" . This made it harder to test, and harder to make changes. Mis-use of modules which were intended to be private would then degrade performance and even cause security incidents. This would happen for a variety of reasons: Junior developers had a limited understanding of the existing architecture and/or frameworks being used It's significantly easier to add to an existing service than to create a new one Python doesn't stop you from importing any code living anywhere When changes are in a 'gray area', social desire to not block others would let changes through code review External deadlines and management pressure would result in \"doing it properly\" getting punted and/or never done The attempts to fix this problem almost always came up short. Inevitably, standards guides would be written and stricter and stricter attempts would be made to enforce style guides, lead developer education efforts, and restrict code review. However, each of these approaches had their own flaws. The solution was to explicitly define a module's boundary and public interface in code, and enforce those domain boundaries through CI. This meant that no developer could introduce a new cross-module dependency without explicitly changing the public interface or the boundary itself. This was a significantly smaller and well-scoped set of changes that could be maintained and managed by those who understood the intended design of the system. With modguard set up, you can collaborate on your codebase with confidence that the intentional design of your modules will always be preserved. modguard is: fully open source able to be adopted incrementally implemented with no runtime footprint a standalone library with no external dependencies interoperable with your existing system (cli, generated config) We hope you give it a try! Would love any feedback."},
{"Title": "Hunting down n + 1 queries in your Django application code", "Author": "u/paulg1989", "Content": "Libraries such as django-silk are excellent for profiling the queries executed by your Django application. We have found, however, that they do not provide a completely straightforward way to identify the lines of your application code which are responsible for executing the most queries. django-queryhunter aims to fill that gap by providing a simple code-first approach to query profiling. This is achieved by providing a context manager and middleware which can provide a detailed report of the lines of your application code which are responsible for executing SQL queries, including data on: * The module name and the line number of the code which executed the query. * The executing code itself on that line. * The number of times that line was responsible for executing a query and the total time that line spent querying the database. * The last SQL statement executed by that line. * If installed as a middleware, the URL and HTTP method of the request which invoked the query. Here's some sample output: queryhunter/tests/my_module.py ==================================== Line no: 13 | Code: for post in posts: | Num. Queries: 1 | SQL: SELECT \"tests_post\".\"id\", \"tests_post\".\"content\", \"tests_post\".\"author_id\" FROM \"tests_post\" | Duration: 4.783299999999713e-05 Line no: 14 | Code: authors.append(post.author.name) | Num. Queries: 5 | SQL: SELECT \"tests_author\".\"id\", \"tests_author\".\"name\" FROM \"tests_author\" WHERE \"tests_author\".\"id\" = %s LIMIT 21 | Duration: 8.804199999801199e-05 One particularly useful feature of this view of profiling is quickly identifying n+1 queries caused by missing select_related or prefetch_related calls. Using queryhunter Let's suppose we have a Django application with two simple models Author and Post where Post has a foreign key author pointing to the Author model. Let's also suppose we have a simple view function get_authors as below, which examines all Posts and fetches the author names in list: # queryhunter/tests/views.py def get_authors(request): # url: /mysite.com/v1/get_authors/ authors = [] posts = Post.objects.all()  # suppose we have 5 posts for post in posts: authors.append(post.author.name) return JsonResponse({'authors': authors}) With the queryhunter middleware installed, requests to the get_authors view will be automatically profiled, with output printed like below. Notice that the code responsible for invoking the SQL query appears next to the SQL statement itself and to the URL of the request which invoked it: queryhunter/tests/my_module.py ==================================== Line no: 6 | Code: for post in posts: | Num. Queries: 1 | Duration: 0.04 | url: /mysite.com/v1/get_authors/ | method: GET | SQL: SELECT \"tests_post\".\"id\", \"tests_post\".\"content\", \"tests_post\".\"author_id\" FROM \"tests_post\" Line no: 7 | Code: authors.append(post.author.name) | Num. Queries: 5 | Duration: 0.05 | url: /mysite.com/v1/get_authors/ | method: GET | SQL: SELECT \"tests_author\".\"id\", \"tests_author\".\"name\" FROM \"tests_author\" WHERE \"tests_author\".\"id\" = %s LIMIT 21 What can we learn from this output? * The line authors.append(post.author.name) was responsible for executing 5 SQL queries, one for each post. * This is a quick way to identify that we are missing a select_related('author') call in our Post.objects.all() query. This may have been obvious in this contrived example, but in a large code base, flushing out these kinds of issues can be very handy. Sharing here in case others find the library useful."},
{"Title": "pytest-bisect-tests - find test leaking state and breaking other tests", "Author": "u/Askriz", "Content": "Hey all, Sometimes it happens that a test passes successfully when executed in isolation, but fails when the whole suite is run. Finding the offending test is annoying when you have 10000 tests to check. I have written a small utility script that helps with finding the offending test, available here: https://github.com/maciej-gol/pytest-bisect-tests . There is already a similar alternative, but the author wasn't keen to hear suggestions around improving the UX so I thought I would go ahead and simplify the UX for the users (+ a few more goodies). Happy to hear your feedback!"},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "So long, Django Migrations! You were good to us.", "Author": "u/rotemtam", "Content": "Hi All, My name is Rotem, I'm one of the creators of Atlas , an open-source schema migrations tool ( GitHub ). We have recently released a \"schema loader\" plugin for Django, which enables Atlas to seamlessly read Django schemas and manage the database schema for them. The provider's code on GitHub is available here . Effectively, this means you can stop using python manage.py migrate and python manage.py makemigration and replace them with Atlas. But why replace Django Migrations? Among the many ORMs available in our industry, Django's automatic migration is one of the most powerful and robust. It can handle a wide range of schema changes, including adding new tables and columns, basic indexes, and more. However, having been created in 2014, a very different era in software engineering, it naturally has some limitations. Some of the limitations of Django's migration system include: Database Features. Because it was created to provide interoperability across database engines, Django's migration system is centered around the \"lowest common denominator\" of database features. More advanced features such as Triggers, Views, and Stored Procedures have very limited support and require developers to jump through all kinds of hoops to use them. Ensuring Migration Safety. Migrations are a risky business. If you're not careful, you can easily cause data loss or a production outage. Django's migration system does not provide a native way to ensure that a migration is safe to apply. Modern Deployments. Django does not provide native integration with modern deployment practices such as GitOps or Infrastructure-as-Code . Atlas, on the other hand, lets you manage your Django applications using the Database Schema-as-Code paradigm. This means that you can use Atlas to automatically plan schema migrations for your Django project, and then apply them to your database. Using Atlas, you can enjoy automatic migration planning, automatic code review, and integrations with your favorite CI/CD tools. Read the Full Guide Here As always, looking to hear your feedback and thoughts R"},
{"Title": "League of Legends 'mod' where you lose an in-game spell if you answer a trivia question incorrectly!", "Author": "u/wrong_way_wonders_", "Content": "It is a quite simple app that runs while you play League of Legends. It was quite fun to create and use! üëâ You can find a repo. with the code in here if you want to check it out. FYI, I am not a super experienced python programmer. I have been using it for a few years and always try to improve little by little. Thanks and happy coding!"},
{"Title": "OnePyece : Python package which provides various data from OnePiece Universe", "Author": "u/IcePick444", "Content": "Hello, I just released a new version of my API Wrapper in Python called \"OnePyece\", to get various data from OnePiece ! It uses this source API : https://www.api-onepiece.com/en Which was only available in french but has an english version now ! Thats why I'm sharing with you my python package to communicate with this API very easily in Python : https://github.com/icepick4/onepyece/ If you want to try it just follow the README instructions ! Have fun with the OnePiece universe in available Python ! If you like the project do not forget to star it on GitHub ;)"},
{"Title": "Handling Tasks in Asyncio Like a Pro | Jacob Padilla", "Author": "u/jpjacobpadilla", "Content": "Python‚Äôs Asyncio module is a great way to handle IO-bound jobs and has seen many improvements in recent Python updates. However, there are so many ways to handle async tasks that figuring out which method to use for different scenarios can be a bit confusing. In my new article , I first go over the basics of what a task object is and then talk about all of the different ways to handle them and the pros and cons of each."},
{"Title": "Add Null Safety", "Author": "u/brand02", "Content": "It would hurt simplicity but it is beyond that point. Python feels like Scratch compared to other languages at the moment. Lacking this basic feature hurts productivity, I don't want to write 50 lines of \"if not product.name\" etc."},
{"Title": "Kanji Lookup: Search for Japanese Kanji characters by drawing them", "Author": "u/etrotta", "Content": "My first machine learning project I really put out there ; as far as the end user is concerned it should work in a way vaguely similar to Jisho Draw searching or Google Handwriting but is open source. The source code is available in https://github.com/etrotta/kanji_lookup , and you can access an online demo in https://huggingface.co/spaces/etrotta/kanji_lookup . The project steps are roughly: Generate synthetic Kanji images for multiple different fonts (using pillow) Encode the images into embeddings using a Neural Network, using an existing open source Vision Transformer model (using transformers and pytorch) Store the embeddings in a Vector Database (using qdrant) Encode an User submitted image and compare it with existing records Create an web app to make submitting images more convenient (using gradio) Note: The GitHub repository and the HuggingFace Space repositories are separate, the GitHub repository is responsible for creating the embeddings and can search files. The HF Space only contains the gradio app."},
{"Title": "Spotify Developer API", "Author": "u/mr_Pepper762", "Content": "I threw together a bunch of scripts that help visualize user data from the Spotify developer API, including hourly listening patterns, favorite genres, favorite tracks, and favorite artist popularity among Spotify users.  User data can be imported using the spotify_Data_CSV_Github.py script, which appends the data to a CSV file. Repository can be found here: https://github.com/zachzion762/spotify_Developer_API This is my first real python project that I've made on my own with a little bit (a lot) of help from ChatGPT."},
{"Title": "I made an app that adds subtitles to any video", "Author": "u/TimeConsideration336", "Content": "https://github.com/Dimitris02/subtitler/tree/main I made it in python 3.9. It uses the following libraries: speech_recognition, selenium, moviepy, pydub It uses google's speech recognition as well as a deepl.com scraper (deepl is the most accurate translator out there afaik). Usually it works pretty well with the default variables but you can tweak the SILENCE and THRESH variables in the subtitler.py file for better results. You can also change the LANG variable to pick another language. To launch the app run the launch.bat file and enter the filename/path of the video you want to add subtitles to. The default language is Russian because I was testing it on some Tarkovsky movie scenes. I hope you find it useful. Edit: I just added a feature where you can change the TRANSLATE_TO variable in the subtitler.py file to change the language of the resulting subtitles."},
{"Title": "I made a single and batch Universal File Converter for Audio and Video files. (With GUI)", "Author": "u/Gameguy39", "Content": "As stated, I created a (mostly) universal file converter. You can select to convert single files and batch files. Handles most video and audio formats. If I am forgetting anything, let me know and I'll see if I can add it! I needed this to batch convert raw MKV files to MP4 that I could use for editing in Premiere Pro. But I also wanted to make it more cause I know I like to convert files a lot. GitHub Link to Source Code and instructions on how to run the file. You will need FFMPEG installed, but I have instructions for that as well."},
{"Title": "Cookiecutter hooks: who's using them and what for?", "Author": "u/pismyfc", "Content": "Hi all, I'm currently working on a side project ( tplmgr.dev ) which is a software template management platform. Think backstage software templates as a standalone service and simpler! I'm aiming to primarily support Cookiecutter templates in the first instance, but I wanted to get some feedback on people's usage of Cookiecutter hooks , seeing as it's going to be quite complex to support this feature. So, do you use Cookiecutter hooks in your templates? If so, can you link to some examples of usage so that I can get a general idea as to whether supporting this should be prioritised. Thanks for reading!"},
{"Title": "Automate labels and filters setup for gmail", "Author": "u/NinjaSniperC", "Content": "https://github.com/RClayGates/gmail_auto_label I wanted to get the 17,000+ emails in my gmail sorted out. So I made this and I'm so happy with how my project is shaping up! It's my first time ever making a project with anything google api. It goes through the entirety of the gmail account, retrieves the From header for each email. Then creates a dictionary from all the emails, using everything after the @ in the email address. example { \"com\":  [ \"example\", \"info\"], \"org\" : [[ \"School\"], [ \"gov\", \"us\"]] } Then uses that to create nested labels and appropriate filters to add those labels. After those are created it will go back and update every email with the appropriate labels. Any feedback/pointers or even a git pull on making this better would be awesome!"},
{"Title": "Simple Text Extraction Server with Python, Tika, MinIO", "Author": "u/replicantrose", "Content": "Tried to put together a document text extraction server using Apache Tika (with ~30 lines of Python code). This can be used to get the text needed for retrieval-augmented generation or to create LLM training datasets (or frankly, anything else that is text-dependent). It worked out pretty well, so thought it would be cool to share: https://blog.min.io/minio-tika-text-extraction/ An aside on Tika: Apache Tika is time-tested and, by some, considered a legacy toolkit. However, with Tika running as a container and the use of Python bindings, it's possible to get a text extraction experience that is as easy to build with as newer frameworks like Unstructured, but also matches the extraction capability of dedicated extraction models like Nougat. Kind of surprising! Much credit to the tika-python project for making the Python bindings! A further aside on object storage: Furthermore, using a backing object store (i.e. MinIO) to hold the source documents is very useful (whether the extracted text is being used for RAG or an LLM training dataset)."},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Netflix Data Analysis with Python and Pandas", "Author": "u/Prior-Sink-4104", "Content": "https://buzzpy.hashnode.dev/analyzing-netflix-data-with-pandas-python Analysing Netflix Movie Data üëÜ This is better: https://buzzpy.hashnode.dev/beginners-guide-to-pandas"},
{"Title": "PySimpleGUI now closed-source", "Author": "u/rturnbull", "Content": "PySimpleGUI , a popular Python GUI library with 13k GitHub stars went closed source / commercial today. Previously it had been licensed under LGPL. I've got no issue with open source devs making money but to changing the license on a library many have contributed to seems to be pretty poor form. This had been a great cross-platform library for beginners."},
{"Title": "any thing like Word Mail Merge to mox text with dynamic placeholders for data", "Author": "u/siraj-samsudeen", "Content": "We have a large report in Word based on several Excel files containing Budget data. We have to generate a PDF report that contains a narrative that is based on the data and we use pandas to manipulate the data - when the data changes, we want the numbers to be updated - for example, I might have a text that says \"The department with the highest increase in budgeting for Staff costs is XYZ with 76.5% increase\". In addition, we will have several data tables presenting the budget data, summarized by different categories, etc. I am looking to replace Word as there is no easy way to insert dynamic data, except for mail merge, which is quite basic. Jupyter Notebook or Quarto would be great choices, but the end users are not programmers - I was wondering whether there are tools to support such a scenario. <<Update>>: Just got to know - https://docxtemplater.com/ and it looks quite close to what I wanted to have. Updating here for the benefit of others."},
{"Title": "Airbnb scraper made pure in Python", "Author": "u/JohnBalvin", "Content": "The project will get Airbnb's information including images, description, price, title ..etcIt also full search given coordinates https://github.com/johnbalvin/pybnb Install: $ pip install gobnb Usage: from gobnb import * data = Get_from_room_url(room_url,currency,\"\")"},
{"Title": "I made a GeoGebra clone using PyQt6, Matplotlib and NumPy!", "Author": "u/Specialist-Arachnid6", "Content": "Graphyte. is a Python-based graphing application built with PyQt6 and Matplotlib inspired by GeoGebra, designed to provide users with an intuitive interface for creating and visualizing mathematical functions and data sets. Since Graphyte. is a new Graphing software, the list of current features is pretty limited. But I'm focused on implementing more and more features to get it to GeoGebra's level, although that would require much more time. GitHub: https://github.com/rohankishore/Graphyte."},
{"Title": "Supabase Sponsoring OSS on supabase-py", "Author": "u/olirice", "Content": "Supabase is looking to sponsor a an experienced python dev tools OSS contributor to close the gap between our javascript client supabase-js and python client supabase-py Full details and an application link available here"},
{"Title": "I've just released logot - a log testing library", "Author": "u/etianen", "Content": "Hello! üëã I've just released logot , a log testing library. logot has a few unique things, such as being logging-framework-agnostic and having support for testing highly concurrent code using threads or async . But those things are relatively niche. What I'd like to show here are a few examples of how it can be a nice caplog replacement for pytest , even in \"normal\" synchronous code. As a caplog replacement Here's a really simple example testing that a piece of code logs as expected: from logot import Logot, logged def test_something(logot: Logot) -> None: do_something() logot.assert_logged(logged.info(\"Something was done\")) You'll see a couple of things here. A logot fixture, and a logged API. Use these together to make neat little log assertions. The equivalent code using caplog would be: def test_something(caplog: pytest.LogCaptureFixture) -> None: do_something() assert any( record.levelno == logging.INFO and record.message == \"Something was done\" for record in caplog.records ) I think the logot code is clearer, and hopefully you do too! ü§ó Log message matching One of logot s more useful features is the ability to match log messages using %-style placeholders rather than regex. This syntax was chosen to be as close as possible to the % placeholders used by the stdlib logging library. from logot import Logot, logged def test_something(logot: Logot) -> None: do_something() # Match a string placeholder with `%s`. logot.assert_logged(logged.info(\"Something %s done\")) The equivalent using caplog gets pretty verbose: import re def test_something(caplog: pytest.LogCaptureFixture) -> None: do_something() assert any( record.levelno == logging.INFO and re.fullmatch(\"Something .*? done\", record.message, re.DOTALL) for record in caplog.records ) If your message contains everyday punctuation like . , you have to start worrying about regex escaping too! I hope that % -style message matching gives a clearer, more loggy way of matching log messages. Log pattern matching This feature is generally aimed towards testing code using threads or async , where messages can arrive out-of-order. But it's also useful for testing synchronous code. from logot import Logot, logged def test_app(logot: Logot) -> None: do_something() logot.wait_for(logged.info(\"Something happened\") | logged.error(\"Something broke!\")) This example tests whether the INFO log \"Something happened\" or the ERROR \"Something broke!\" was emitted, and passes on either. The equivalent using caplog gets quite long: def test_something(caplog: pytest.LogCaptureFixture) -> None: do_something() assert any( (record.levelno == logging.INFO and record.message == \"Something happened\") or (record.levelno == logging.ERROR and record.message == \"Something broke!\") for record in caplog.records ) I hope you like it! ‚ù§Ô∏è This is only a v1 release, but it's building on a lot of ideas I've been developing in different projects for a while now. I hope you like it, and find it useful. The project documentation is there if you'd like to find out more. üôá"},
{"Title": "PrimeNote: superior sticky note-taking app for Windows and Linux", "Author": "u/crapaud_dindon", "Content": "Introducing PrimeNote , a full-fledged sticky note-taking application for Windows and Linux. Unleash your creativity and optimize your workflow with an intuitive interface packed with powerful features. PrimeNote empowers you to: Capture ideas effortlessly in various formats, from plain text and rich text to images and even full-fledged Vim or terminal environments. Organize your notes impeccably with a folder-based structure for instant retrieval. Safeguard confidential information with robust Fernet encryption. Navigate your notes with lightning speed using the built-in keyboard-driven search tool. Craft a personalized workspace by tailoring the appearance, menus, hotkeys, and mouse events to your preferences. Transmit content seamlessly in real-time by sharing your note repository on a virtual machine. Landing page (with screenshots) Gitlab repository AUR page"},
{"Title": "Genetic Algorithm (PyGAD) Paints Realistic Monalisa using 16 Brush Strokes + Source Code", "Author": "u/ahmed26gad", "Content": "Video: https://youtu.be/m65DbFWFqIs?si=Shelj2rhN9lvNg8j Source Code: https://github.com/ahmedfgad/GeneticArtist"},
{"Title": "Oregon Trail hunting mini-game with neural network AI", "Author": "u/MoTTs_", "Content": "Inspired from CrashCourse AI , I used Python and Pygame to re-create the hunting mini-game from the 1970's Oregon Trail. Then I made a neural network AI that learns to play the game on its own and gets better over successive generations. Oregon Trail hunting mini-game with neural network AI"},
{"Title": "Looking for Civil Engineering Group to Discuss Python Projects", "Author": "u/youssef3698", "Content": "Hello everyone! Hope you're having a great day. I am a civil engineer working as a quantity surveyor and office admin. I recently started learning python and set up multiple automations in my work. Some of them include automating document sorting into our file system, filtering out emails to be sent to subcontractors (could have been fully automated but I kept the last step to be controlled manually for quality control), helper to choose images for monthly report and so on... I know there are many more projects I can do but I am honestly losing motivation when working on them alone and not being able to discuss the ideas with someone. Is there any group of engineers which might enjoy brainstorming together and discussing issues?"},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "tunein-python: A highly experminetal Python TuneIn wrapper.", "Author": "u/MajliTech", "Content": "Hello world! Just recently, I made a (short, but might be useful) library for interfacing with TuneIn's (non-existent) API. You can check it out at PyPI or GitHub My motivation behind this is I wanted to make a radio receiver using a Pi. While slapping TuneIn's website would be sufficient, I wanted it to be \"locally\" (no heavy browser for playing mp3's or aac's) Check it out, and tell me if you have any other questions."},
{"Title": "Analyzing GPS files from sport watches", "Author": "u/iamgearshifter", "Content": "Hi! I analyzed GPS-based data from my Garmin sport watch using the library gpxpy . There is no documentation available, so I relied on docstrings to find the methods I needed. There is a very similar project working with files from Strava, but there most of the stats were calculated directly on the pandas dataframe. I used methods from gpxpy wherever possible. I wrote a blog post about my project and uploaded the code to github . Please let me know what you think!"},
{"Title": "Just a simple terminal based WebSocket client", "Author": "u/Slight_Smile654", "Content": "One brought me a project for review that uses Websocket. I wanted to play around with it, send some messages, and check if the result matches the expectations. I decided to use the popular app: Postman, but I found out that it now requires registration. So, I had two options: Spend a minute registering Spend one evening writing my own alternative From my point of view the choice is obvious, so I proudly present to you a simple console WebSocket client: https://github.com/Sets88/wscls Please star üåü repo if you liked what i created"},
{"Title": "Writing Python like it's Elixir (or Erlang)", "Author": "u/david-delassus", "Content": "Ever since I learned Erlang, and then Elixir, I suffered from ‚ÄúErlang envy‚Äù. A few years ago, I made this project to try to mimic the OTP framework using Python and the Trio async library (IMHO, better alternative to asyncio). Recently, for work, I had to write some Python project that needed multiple isolated components that interacted through message-passing. I reused the patterns I implemented in that triotp library I made. This weekend, I also wrote a blog article to detail a bit more about my reasoning with why those design patterns are nice and IMHO a must have. DISCLAIMER: Yes it's a Medium article, I promise it's not a shitty 5 lines article about how print() is a nice function. It's also a \"friend link\" with an auth token to bypass the paywall. NB: I didn‚Äôt use triotp directly in the code for work, I only extracted what I needed from it. This is because my process registry implementation is weak at best, and I didn‚Äôt need a process registry anyway. Link to the project: https://github.com/linkdd/triotp"},
{"Title": "My second article ever: \"The Guide to Making Your Django SaaS Business Worldwide (for free)\".", "Author": "u/catnotfoundnear", "Content": "I published my second ever tutorial (followed by the first one, a week ago)! \"The Guide to Making Your Django SaaS Business Worldwide (for free)\". Read now, for free, without ads, on my blog: https://catnotfoundnear.github.io/the-guide-to-making-your-django-saas-business-worldwide-for-free.html I will truly appreciate your suggestions or recommendations! Thank you! - Anna Willis (Catnotfoundnear)"},
{"Title": "Filesystem magic with the Python standard library", "Author": "u/willm", "Content": "Hi r/python I wrote about some of the techniques used in my Toolong project. Hope you find it interesting. https://textual.textualize.io/blog/2024/02/11/file-magic-with-the-python-standard-library/"},
{"Title": "Toolong: A terminal app to view, tail, and search log files", "Author": "u/willm", "Content": "Hi r/python I'm the founder of the Textual project. I recently built this app to view log files. It can open log files of any size, and tail in realtime. It has syntax highlighting and a versatile search feature. It can also merge a bunch of log files in to a single view. Here's the repo: https://github.com/Textualize/toolong Happy to answer any questions about Toolong or any of our projects..."},
{"Title": "How are numpy.linalg.* functions built so that they use multiple cores out-of-the-box?", "Author": "u/spherical_shell", "Content": "I have tried numpy.linalg.eigvals built with openblas and apple accelerate framework on apple silicon. Both run on multiple threads for very big matrices, by default. I thought the multi-threading is provided by the FORTRAN LAPACK library, and numpy.linalg is simply a wrapper for the FORTRAN library, and once I use LAPACK, I should have similar behaviour. However, when I call LAPACK in rust, although the performance is good ( maybe even better than numpy, still need to benchmark carefully), it does not use multiple threads. I have checked that I am also using openblas and accelerate in rust. This seems perplexing. Correction: I said my rust program just uses one thread. This is not entirely accurate. It has many threads, but CPU usage is still exactly one CPU core. NOTE: for both rust and numpy, I have made sure they are linked to the accelerate framework. I am aware different implementations of bias can give different performance, but here both should be linked to the same backend, which is why I find perplexing. Isn't numpy.linalg.eigvals nothing more than a wrapper over the FORTRAN library? So how does it manage to use more threads? What is going on here?"},
{"Title": "Graphite. : Math Graphing Tool Inspired from GeoGebra", "Author": "u/Specialist-Arachnid6", "Content": "Graphite is a Python-based graphing application built with PyQt6 and Matplotlib inspired by GeoGebra, designed to provide users with an intuitive interface for creating and visualizing mathematical functions and data sets. GitHub: https://github.com/rohankishore/Graphite"},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Is there a community organisation focussed on maintaining Python projects?", "Author": "u/nicholashairs", "Content": "After a few too many experiences with unmaintained Python projects recently I've been thinking about how one might use GitHub and PyPI organisations to help prevent projects ending up in an unmaintained state due to authors and maintainers eventually leaving the projects. Someone pointed me to The PHP League as an example of a similar group in the PHP world. I'll note a primary difference between what I'm looking for and The PHP League is that they appear to focus on authoring projects where I'd focus on maintaining/ administrating them. Rather than reinvent the wheel I'm wondering if there's such an organisation that already exists in the Python world? I'm aware that we have organisations like: the PSF, PyPA, PyPI, but they're more suited to large projects that are core to the language communities / organisations built around popular projects like Django I'm looking more at projects which are important but not popular or \"feature rich\" enough to build an active community over, but are important to keep maintained because of how much they are used."},
{"Title": "Python Package for Free ETF Data", "Author": "u/yikuang5", "Content": "Hi all, I created this package a while back and will like to share it since I recently started maintaining it once again. https://github.com/lvxhnat/pyetfdb-scraper It retrieves ETF data from https://etfdb.com in json format. Do give it a look and a star if it is helpful ! Here are some fields you can get, for example, top holdings to observe how much your ETF holdings overlap with each other: \"top_holdings\": [ { \"symbol\": \"AAPL\", \"holding\": \"Apple Inc.\", \"share\": \"7.18%\", \"url\": \"https://etfdb.com/stock/AAPL/\", }, { \"symbol\": \"MSFT\", \"holding\": \"Microsoft Corporation\", \"share\": \"6.50%\", \"url\": \"https://etfdb.com/stock/MSFT/\", }, { \"symbol\": \"AMZN\", \"holding\": \"Amazon.com, Inc.\", \"share\": \"3.32%\", \"url\": \"https://etfdb.com/stock/AMZN/\", }, ... even industries { \"Technology Services\": 21.08, \"Electronic Technology\": 18.45, \"Finance\": 12.34, \"Health Technology\": 9.51, \"Retail Trade\": 7.75, \"Consumer Non-Durables\": 4.48, \"Producer Manufacturing\": 3.61, \"Consumer Services\": 3.4, \"Energy Minerals\": 3.11, \"Commercial Services\": 2.92, \"Utilities\": 2.25, \"Health Services\": 2.21, \"Consumer Durables\": 1.91, \"Process Industries\": 1.78, \"Transportation\": 1.76, \"Communications\": 0.93, \"Distribution Services\": 0.92, \"Industrial Services\": 0.92, \"Non-Energy Minerals\": 0.54, \"CASH\": 0.2, }, ..."},
{"Title": "New feature proposal \"try for\" and \"try async for\"", "Author": "u/jembishop1", "Content": "I have been working with async iterators a lot recently and one of my bugbears is handling exceptions which are raised in the iteration. For context my use case (fairly common I imagine) is using async iterators over IO streams, eg MQTT or Kafka messages. These systems can potentially throw errors when you receive a message. I like to use the async for interface but it comes with an irritation, it is cumbersome to catch an error in the iteration, then continue iterating. I would like to propose a new feature try async for (and for consistency try for to address this) To illustrate my example imagine we have a mock IO function and I create a class to iterate over some fetched results: import asyncio import random class IOError(Exception): pass async def io(x): if random.random() > 0.5: raise IOError else: return x class MyIterator: def __init__(self): self.n = 0 async def __anext__(self): try: result = await io(self.n) finally: self.n += 1 if self.n > 10: raise StopAsyncIteration return result def __aiter__(self): return self So the issue is what if if the IOError happens, but I still want to continue the iteration? One option is to abandon the for loop and use a while loop: my_iter = MyIterator() while True: try: result = await my_iter.__anext__() print(result) except IOError: pass except StopAsyncIteration: break This works but it's not very pythonic, and removes the point of async for . Another option is to catch the exception externally then continue iterating. my_iter = MyIterator() while True: try: async for res in my_iter: print(res) else: break except IOError: pass This to me seems difficult to read and is nested excessively. To solve this we might consider changing the iterator to return a sentinel value instead of raising an exception: class MyIterator: def __init__(self): self.n = 0 async def __anext__(self): try: result = await io(self.n) except IOError: return None finally: self.n += 1 if self.n > 10: raise StopAsyncIteration return result def __aiter__(self): return self So we can iterate through like so: async for result in MyIterator(): if result is not None: print(result) However this is also not very pythonic, as we are meant to use exceptions, not sentinel values to show exceptional conditions. To solve this I would propose a simple syntax extension, allow try before async for or for loops. So going back to the first class which raises the exception in the __anext__ method my example would look like: try async for result in MyIterator(): print(result) except: pass The semantics here are if the __anext__ method raises an exception, it triggers the except branch, and continues with the iteration. This to me seems much cleaner, and it seems fairly intuitive to understand how this syntax works. finally would work in a similar fashion: try async for result in MyIterator(): print(result) except: pass finally: print(\"Always prints!\") What do you think about this new syntax addition? Am I missing something?"},
{"Title": "Why is pandas not compiled with -o3 by deafualt?", "Author": "u/rejectedlesbian", "Content": "Is it just me or does that seem simply bizzar. Like if you have gcc u can just get so much extra speed from doing that simple trick and as far as I can tell from their cython setup they don't seem to... Why is that?"},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "PReam-Team - a python TUI app for keeping track of open GH PRs for a set of people", "Author": "u/legjitter", "Content": "Hi! I'm part of a bigger team at work and we often touch multiple repositories. We often forget or simply don't add everyone from the team to reviewer list. And if we do add, sometimes that gets lost in the sea of GH notification emails. So I've made a small python app that helps me prioritise reviewing team PRs and gives me a nice overview on what's going on in the team across multiple repos. Maybe it can be of use for you too? You provide list of names and GitHub PAT and pream-team lists all open PRs for each person. Also signalising if the PR is in draft or ready for review state. I'm fairly new to python so any feedback on improving the codebase is more than welcome. Also any ideas on improving the app, issues and/or pull requests are welcome too. GitHub link: https://github.com/NikolaDucak/pream-team"},
{"Title": "What are your biggest challenges with Dependencies?", "Author": "u/mitsuhiko", "Content": "In the process of looking deeper into making Rye work I am trying to better capture problems people other than me are facing. In particular I'm trying to ensure that Rye can accommodate packages and their dependencies that are problematic today. As a point of reference tensorflow's ecosystem is notoriously hard to support. There are also packages like flash-attn which don't publish wheels but also incorrectly define the build dependencies so that they often cannot be built. Do you have some packages that you often need to use but where you run into problems depending on them? Regardless of which tool you use (Poetry, PDM, Rye, pip etc.)."},
{"Title": "PIP Manager + PyPI Search", "Author": "u/puddsszz", "Content": "i made a PIP Manager + PyPI Search im still new to all this but im interested to know what people think and how i can make it better, thanks. ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ PIP Manager + PyPI Search - https://github.com/pudszttiot/PIP-Manager-PyPI-Search a Python application designed to simplify package management and exploration for Python developers. With a user-friendly interface, it allows for effortless installation, uninstallation, and upgrading of packages via PIP Manager. Additionally, the PyPI Search feature enables quick searches for packages available on the Python Package Index (PyPI). Explore, manage, and stay updated with your Python packages effortlessly with this handy tool."},
{"Title": "Memory leak and Garbage collectors in python", "Author": "u/navjbans", "Content": "Memory management is a critical aspect of application lifecycle and developers put efforts to keep the application light.For most cases Memory management is built in and runs silently in the back end.Garbage collector are the heroes responsible for cleaning up runtime memory and ensuring enough Heap Space is available for application‚Äôs execution. Without garbage collectors They work routinely to ensure all dangling data is cleaned up and developers can work freely to create extensive applications.This newsletter article aims to discuss the importance and impact of Garbage collectors in an applications‚Äô memory management. https://www.linkedin.com/pulse/trash-talk-garbage-collection-navjot-bansal-pzxqc%3FtrackingId=i1TOo%252FFaR3%252Bk%252F39C3aJGUw%253D%253D/?trackingId=i1TOo%2FFaR3%2Bk%2F39C3aJGUw%3D%3D"},
{"Title": "Building a low-code agent automation open-source project", "Author": "u/sivasurendira", "Content": "We are building or attempting to build a multi-agent automation framework that is low-code and easy to use. We released the first stable version yesterday. Github - https://github.com/LyzrCore/lyzr-automata The idea is to bring in more flexibility for builders to add their own agents in addition to the 'prompt agents' one uses in these agent automation frameworks. Here is the demo video of how we managed to automate 'Newsletter Creation' using Perplexity, GPT4 and Lyzr Automata (the framework that we started building). https://www.loom.com/share/c5878b106f634b3d9079a9c9b86de93b?sid=c20c03b9-1c8c-4c45-8845-660328c9d846 What do you think? How can we improve this and make it more versatile, but sticking to the 'low-code' principles? Here is the demo video of how we managed to automate 'Newsletter Creation' using Perplexity, GPT4, and Lyzr Automata (the framework we started building)."},
{"Title": "Counting CPU Instructions in Python", "Author": "u/sYnfo", "Content": "Did you know it takes about 17,000 CPU instructions to print(\"Hello\") in Python? And that it takes ~2 billion of them to import seaborn? I wrote a little blog post on how you can measure this yourself ."},
{"Title": "Genetic Algorithm (PyGAD) Paints Realistic Monalisa using 16 Brush Strokes + Source Code", "Author": "u/ahmed26gad", "Content": "Video: https://youtu.be/m65DbFWFqIs?si=Shelj2rhN9lvNg8j Source Code: https://github.com/ahmedfgad/GeneticArtist"},
{"Title": "Oregon Trail hunting mini-game with neural network AI", "Author": "u/MoTTs_", "Content": "Inspired from CrashCourse AI , I used Python and Pygame to re-create the hunting mini-game from the 1970's Oregon Trail. Then I made a neural network AI that learns to play the game on its own and gets better over successive generations. Oregon Trail hunting mini-game with neural network AI"},
{"Title": "Looking for Civil Engineering Group to Discuss Python Projects", "Author": "u/youssef3698", "Content": "Hello everyone! Hope you're having a great day. I am a civil engineer working as a quantity surveyor and office admin. I recently started learning python and set up multiple automations in my work. Some of them include automating document sorting into our file system, filtering out emails to be sent to subcontractors (could have been fully automated but I kept the last step to be controlled manually for quality control), helper to choose images for monthly report and so on... I know there are many more projects I can do but I am honestly losing motivation when working on them alone and not being able to discuss the ideas with someone. Is there any group of engineers which might enjoy brainstorming together and discussing issues?"},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "tunein-python: A highly experminetal Python TuneIn wrapper.", "Author": "u/MajliTech", "Content": "Hello world! Just recently, I made a (short, but might be useful) library for interfacing with TuneIn's (non-existent) API. You can check it out at PyPI or GitHub My motivation behind this is I wanted to make a radio receiver using a Pi. While slapping TuneIn's website would be sufficient, I wanted it to be \"locally\" (no heavy browser for playing mp3's or aac's) Check it out, and tell me if you have any other questions."},
{"Title": "Analyzing GPS files from sport watches", "Author": "u/iamgearshifter", "Content": "Hi! I analyzed GPS-based data from my Garmin sport watch using the library gpxpy . There is no documentation available, so I relied on docstrings to find the methods I needed. There is a very similar project working with files from Strava, but there most of the stats were calculated directly on the pandas dataframe. I used methods from gpxpy wherever possible. I wrote a blog post about my project and uploaded the code to github . Please let me know what you think!"},
{"Title": "Just a simple terminal based WebSocket client", "Author": "u/Slight_Smile654", "Content": "One brought me a project for review that uses Websocket. I wanted to play around with it, send some messages, and check if the result matches the expectations. I decided to use the popular app: Postman, but I found out that it now requires registration. So, I had two options: Spend a minute registering Spend one evening writing my own alternative From my point of view the choice is obvious, so I proudly present to you a simple console WebSocket client: https://github.com/Sets88/wscls Please star üåü repo if you liked what i created"},
{"Title": "Writing Python like it's Elixir (or Erlang)", "Author": "u/david-delassus", "Content": "Ever since I learned Erlang, and then Elixir, I suffered from ‚ÄúErlang envy‚Äù. A few years ago, I made this project to try to mimic the OTP framework using Python and the Trio async library (IMHO, better alternative to asyncio). Recently, for work, I had to write some Python project that needed multiple isolated components that interacted through message-passing. I reused the patterns I implemented in that triotp library I made. This weekend, I also wrote a blog article to detail a bit more about my reasoning with why those design patterns are nice and IMHO a must have. DISCLAIMER: Yes it's a Medium article, I promise it's not a shitty 5 lines article about how print() is a nice function. It's also a \"friend link\" with an auth token to bypass the paywall. NB: I didn‚Äôt use triotp directly in the code for work, I only extracted what I needed from it. This is because my process registry implementation is weak at best, and I didn‚Äôt need a process registry anyway. Link to the project: https://github.com/linkdd/triotp"},
{"Title": "My second article ever: \"The Guide to Making Your Django SaaS Business Worldwide (for free)\".", "Author": "u/catnotfoundnear", "Content": "I published my second ever tutorial (followed by the first one, a week ago)! \"The Guide to Making Your Django SaaS Business Worldwide (for free)\". Read now, for free, without ads, on my blog: https://catnotfoundnear.github.io/the-guide-to-making-your-django-saas-business-worldwide-for-free.html I will truly appreciate your suggestions or recommendations! Thank you! - Anna Willis (Catnotfoundnear)"},
{"Title": "Filesystem magic with the Python standard library", "Author": "u/willm", "Content": "Hi r/python I wrote about some of the techniques used in my Toolong project. Hope you find it interesting. https://textual.textualize.io/blog/2024/02/11/file-magic-with-the-python-standard-library/"},
{"Title": "Toolong: A terminal app to view, tail, and search log files", "Author": "u/willm", "Content": "Hi r/python I'm the founder of the Textual project. I recently built this app to view log files. It can open log files of any size, and tail in realtime. It has syntax highlighting and a versatile search feature. It can also merge a bunch of log files in to a single view. Here's the repo: https://github.com/Textualize/toolong Happy to answer any questions about Toolong or any of our projects..."},
{"Title": "How are numpy.linalg.* functions built so that they use multiple cores out-of-the-box?", "Author": "u/spherical_shell", "Content": "I have tried numpy.linalg.eigvals built with openblas and apple accelerate framework on apple silicon. Both run on multiple threads for very big matrices, by default. I thought the multi-threading is provided by the FORTRAN LAPACK library, and numpy.linalg is simply a wrapper for the FORTRAN library, and once I use LAPACK, I should have similar behaviour. However, when I call LAPACK in rust, although the performance is good ( maybe even better than numpy, still need to benchmark carefully), it does not use multiple threads. I have checked that I am also using openblas and accelerate in rust. This seems perplexing. Correction: I said my rust program just uses one thread. This is not entirely accurate. It has many threads, but CPU usage is still exactly one CPU core. NOTE: for both rust and numpy, I have made sure they are linked to the accelerate framework. I am aware different implementations of bias can give different performance, but here both should be linked to the same backend, which is why I find perplexing. Isn't numpy.linalg.eigvals nothing more than a wrapper over the FORTRAN library? So how does it manage to use more threads? What is going on here?"},
{"Title": "Graphite. : Math Graphing Tool Inspired from GeoGebra", "Author": "u/Specialist-Arachnid6", "Content": "Graphite is a Python-based graphing application built with PyQt6 and Matplotlib inspired by GeoGebra, designed to provide users with an intuitive interface for creating and visualizing mathematical functions and data sets. GitHub: https://github.com/rohankishore/Graphite"},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Is there a community organisation focussed on maintaining Python projects?", "Author": "u/nicholashairs", "Content": "After a few too many experiences with unmaintained Python projects recently I've been thinking about how one might use GitHub and PyPI organisations to help prevent projects ending up in an unmaintained state due to authors and maintainers eventually leaving the projects. Someone pointed me to The PHP League as an example of a similar group in the PHP world. I'll note a primary difference between what I'm looking for and The PHP League is that they appear to focus on authoring projects where I'd focus on maintaining/ administrating them. Rather than reinvent the wheel I'm wondering if there's such an organisation that already exists in the Python world? I'm aware that we have organisations like: the PSF, PyPA, PyPI, but they're more suited to large projects that are core to the language communities / organisations built around popular projects like Django I'm looking more at projects which are important but not popular or \"feature rich\" enough to build an active community over, but are important to keep maintained because of how much they are used."},
{"Title": "Python Package for Free ETF Data", "Author": "u/yikuang5", "Content": "Hi all, I created this package a while back and will like to share it since I recently started maintaining it once again. https://github.com/lvxhnat/pyetfdb-scraper It retrieves ETF data from https://etfdb.com in json format. Do give it a look and a star if it is helpful ! Here are some fields you can get, for example, top holdings to observe how much your ETF holdings overlap with each other: \"top_holdings\": [ { \"symbol\": \"AAPL\", \"holding\": \"Apple Inc.\", \"share\": \"7.18%\", \"url\": \"https://etfdb.com/stock/AAPL/\", }, { \"symbol\": \"MSFT\", \"holding\": \"Microsoft Corporation\", \"share\": \"6.50%\", \"url\": \"https://etfdb.com/stock/MSFT/\", }, { \"symbol\": \"AMZN\", \"holding\": \"Amazon.com, Inc.\", \"share\": \"3.32%\", \"url\": \"https://etfdb.com/stock/AMZN/\", }, ... even industries { \"Technology Services\": 21.08, \"Electronic Technology\": 18.45, \"Finance\": 12.34, \"Health Technology\": 9.51, \"Retail Trade\": 7.75, \"Consumer Non-Durables\": 4.48, \"Producer Manufacturing\": 3.61, \"Consumer Services\": 3.4, \"Energy Minerals\": 3.11, \"Commercial Services\": 2.92, \"Utilities\": 2.25, \"Health Services\": 2.21, \"Consumer Durables\": 1.91, \"Process Industries\": 1.78, \"Transportation\": 1.76, \"Communications\": 0.93, \"Distribution Services\": 0.92, \"Industrial Services\": 0.92, \"Non-Energy Minerals\": 0.54, \"CASH\": 0.2, }, ..."},
{"Title": "New feature proposal \"try for\" and \"try async for\"", "Author": "u/jembishop1", "Content": "I have been working with async iterators a lot recently and one of my bugbears is handling exceptions which are raised in the iteration. For context my use case (fairly common I imagine) is using async iterators over IO streams, eg MQTT or Kafka messages. These systems can potentially throw errors when you receive a message. I like to use the async for interface but it comes with an irritation, it is cumbersome to catch an error in the iteration, then continue iterating. I would like to propose a new feature try async for (and for consistency try for to address this) To illustrate my example imagine we have a mock IO function and I create a class to iterate over some fetched results: import asyncio import random class IOError(Exception): pass async def io(x): if random.random() > 0.5: raise IOError else: return x class MyIterator: def __init__(self): self.n = 0 async def __anext__(self): try: result = await io(self.n) finally: self.n += 1 if self.n > 10: raise StopAsyncIteration return result def __aiter__(self): return self So the issue is what if if the IOError happens, but I still want to continue the iteration? One option is to abandon the for loop and use a while loop: my_iter = MyIterator() while True: try: result = await my_iter.__anext__() print(result) except IOError: pass except StopAsyncIteration: break This works but it's not very pythonic, and removes the point of async for . Another option is to catch the exception externally then continue iterating. my_iter = MyIterator() while True: try: async for res in my_iter: print(res) else: break except IOError: pass This to me seems difficult to read and is nested excessively. To solve this we might consider changing the iterator to return a sentinel value instead of raising an exception: class MyIterator: def __init__(self): self.n = 0 async def __anext__(self): try: result = await io(self.n) except IOError: return None finally: self.n += 1 if self.n > 10: raise StopAsyncIteration return result def __aiter__(self): return self So we can iterate through like so: async for result in MyIterator(): if result is not None: print(result) However this is also not very pythonic, as we are meant to use exceptions, not sentinel values to show exceptional conditions. To solve this I would propose a simple syntax extension, allow try before async for or for loops. So going back to the first class which raises the exception in the __anext__ method my example would look like: try async for result in MyIterator(): print(result) except: pass The semantics here are if the __anext__ method raises an exception, it triggers the except branch, and continues with the iteration. This to me seems much cleaner, and it seems fairly intuitive to understand how this syntax works. finally would work in a similar fashion: try async for result in MyIterator(): print(result) except: pass finally: print(\"Always prints!\") What do you think about this new syntax addition? Am I missing something?"},
{"Title": "Why is pandas not compiled with -o3 by deafualt?", "Author": "u/rejectedlesbian", "Content": "Is it just me or does that seem simply bizzar. Like if you have gcc u can just get so much extra speed from doing that simple trick and as far as I can tell from their cython setup they don't seem to... Why is that?"},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "PReam-Team - a python TUI app for keeping track of open GH PRs for a set of people", "Author": "u/legjitter", "Content": "Hi! I'm part of a bigger team at work and we often touch multiple repositories. We often forget or simply don't add everyone from the team to reviewer list. And if we do add, sometimes that gets lost in the sea of GH notification emails. So I've made a small python app that helps me prioritise reviewing team PRs and gives me a nice overview on what's going on in the team across multiple repos. Maybe it can be of use for you too? You provide list of names and GitHub PAT and pream-team lists all open PRs for each person. Also signalising if the PR is in draft or ready for review state. I'm fairly new to python so any feedback on improving the codebase is more than welcome. Also any ideas on improving the app, issues and/or pull requests are welcome too. GitHub link: https://github.com/NikolaDucak/pream-team"},
{"Title": "What are your biggest challenges with Dependencies?", "Author": "u/mitsuhiko", "Content": "In the process of looking deeper into making Rye work I am trying to better capture problems people other than me are facing. In particular I'm trying to ensure that Rye can accommodate packages and their dependencies that are problematic today. As a point of reference tensorflow's ecosystem is notoriously hard to support. There are also packages like flash-attn which don't publish wheels but also incorrectly define the build dependencies so that they often cannot be built. Do you have some packages that you often need to use but where you run into problems depending on them? Regardless of which tool you use (Poetry, PDM, Rye, pip etc.)."},
{"Title": "PIP Manager + PyPI Search", "Author": "u/puddsszz", "Content": "i made a PIP Manager + PyPI Search im still new to all this but im interested to know what people think and how i can make it better, thanks. ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ PIP Manager + PyPI Search - https://github.com/pudszttiot/PIP-Manager-PyPI-Search a Python application designed to simplify package management and exploration for Python developers. With a user-friendly interface, it allows for effortless installation, uninstallation, and upgrading of packages via PIP Manager. Additionally, the PyPI Search feature enables quick searches for packages available on the Python Package Index (PyPI). Explore, manage, and stay updated with your Python packages effortlessly with this handy tool."},
{"Title": "Memory leak and Garbage collectors in python", "Author": "u/navjbans", "Content": "Memory management is a critical aspect of application lifecycle and developers put efforts to keep the application light.For most cases Memory management is built in and runs silently in the back end.Garbage collector are the heroes responsible for cleaning up runtime memory and ensuring enough Heap Space is available for application‚Äôs execution. Without garbage collectors They work routinely to ensure all dangling data is cleaned up and developers can work freely to create extensive applications.This newsletter article aims to discuss the importance and impact of Garbage collectors in an applications‚Äô memory management. https://www.linkedin.com/pulse/trash-talk-garbage-collection-navjot-bansal-pzxqc%3FtrackingId=i1TOo%252FFaR3%252Bk%252F39C3aJGUw%253D%253D/?trackingId=i1TOo%2FFaR3%2Bk%2F39C3aJGUw%3D%3D"},
{"Title": "Building a low-code agent automation open-source project", "Author": "u/sivasurendira", "Content": "We are building or attempting to build a multi-agent automation framework that is low-code and easy to use. We released the first stable version yesterday. Github - https://github.com/LyzrCore/lyzr-automata The idea is to bring in more flexibility for builders to add their own agents in addition to the 'prompt agents' one uses in these agent automation frameworks. Here is the demo video of how we managed to automate 'Newsletter Creation' using Perplexity, GPT4 and Lyzr Automata (the framework that we started building). https://www.loom.com/share/c5878b106f634b3d9079a9c9b86de93b?sid=c20c03b9-1c8c-4c45-8845-660328c9d846 What do you think? How can we improve this and make it more versatile, but sticking to the 'low-code' principles? Here is the demo video of how we managed to automate 'Newsletter Creation' using Perplexity, GPT4, and Lyzr Automata (the framework we started building)."},
{"Title": "Counting CPU Instructions in Python", "Author": "u/sYnfo", "Content": "Did you know it takes about 17,000 CPU instructions to print(\"Hello\") in Python? And that it takes ~2 billion of them to import seaborn? I wrote a little blog post on how you can measure this yourself ."},
{"Title": "Lagom - dependency injector", "Author": "u/ZestycloseGate7928", "Content": "Question for ports and adapters architecture experts . Why this DI injector https://github.com/meadsteve/lagom Is not popular ?  Only 230 stars on Github. Uses Python typing to map dependencies, looks much more pythonic than injector or dependency-injector libs. Are there any known problems that solution has or it's only not enough marketing ? If not Lagom than what you recommend ?"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "IDE made with PyQt6 [UPDATE]", "Author": "u/Specialist-Arachnid6", "Content": "Aura Text is an excellent text/code editor that offers a wide array of essential tools, and is built with PyQt6 and Python. Key Features: Edit files (duh) Support up to 30 languages Autocompletion Split pane Markdown editor Terminal with history Python Console Plugin support Extensive theming including Material Theming support Extremely customisable GitHub: https://github.com/rohankishore/Aura-Text"},
{"Title": "CustomTkinter Snippets v3.0.0: Enhance Your Python GUI Workflow!", "Author": "u/Playful_Round_6530", "Content": "Excited to announce CustomTkinter Snippets v3.0.0! This Visual Studio Code extension turbocharges your Python GUI development with intuitive code snippets. With a wide range of components covered and customization options available, coding sleek GUIs has never been easier. What's New: Expanded snippet coverage Enhanced customization options Bug fixes for smoother development TODO: Addition of special events and binds as snippets Addition of object based snippets Give it a try and supercharge your GUI development today! Links: Repository Link: CustomTkinter Snippets Extension Link: CustomTkinter Snippets VSCode Extension Credit: Credits to Tom Schimansky for the awesome CustomTkinter library! Don't forget to star the repo! #Python #GUI #Development #VisualStudioCode"},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "One Trillion Row Challenge (1TRC)", "Author": "u/mrocklin", "Content": "I really liked the simplicity of the One Billion Row Challenge (1BRC) that took off last month. It was fun to see lots of people apply different tools to the same simple-yet-clear problem ‚ÄúHow do you parse, process, and aggregate a large CSV file as quickly as possible?‚Äù For fun, my colleagues and I made a One Trillion Row Challenge (1TRC) dataset üôÇ. Data lives on S3 in Parquet format (CSV made zero sense here) in a public bucket at s3://coiled-datasets-rp/1trc and is roughly 12 TiB uncompressed. We (the Dask team) were able to complete the TRC query in around six minutes for around $1.10.For more information see this blogpost and this repository (Edit: this was taken down originally for having a Medium link.  I've now included an open-access blog link instead)"},
{"Title": "What are you going to do with latest python release", "Author": "u/Arckman_", "Content": "I was eagerly waiting for the parallel programming or subinterpreters to do something with it. What were you all expecting and thinking to do with the latest version?"},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Python 3.12.2 and 3.11.8 released", "Author": "u/japaget", "Content": "Get Python 3.12.2 here: https://www.python.org/downloads/release/python-3122/ Get Python 3.11.8 here: https://www.python.org/downloads/release/python-3118/"},
{"Title": "I wrote a minimalistic search engine in Python", "Author": "u/AM_DS", "Content": "Hi * Some months ago I joined a new company as a search data scientist, and since then I've been working with Solr (a search engine written in Java). Since this wasn't my field of expertise I decided to implement a simple search engine in Python. It's not a production-ready project, but it shows how a search engine works under the hood. You can find the project here . I've also written a post explaining how I've implemented it here . Besides the search engine, the project also includes a FastAPI app that exposes a website allowing users to interact with the search engine. Let me know what you think!"},
{"Title": "2,000 free sign ups available for \"Automate the Boring Stuff with Python\" online course. (Feb 2024)", "Author": "u/AlSweigart", "Content": "If you want to learn to code, I've released 2,000 free sign ups for my course following my Automate the Boring Stuff with Python book (each has 1,000 sign ups, use the other one if one is sold out): *The sign ups are all used up, but you can still watch all the videos for free. Read below! https://udemy.com/course/automate/?couponCode=FEB2024FREE https://udemy.com/course/automate/?couponCode=FEB2024FREE2 If you are reading this after the sign ups are used up, you can always find the first 15 of the course's 50 videos are free on YouTube if you want to preview them. YOU CAN ALSO WATCH THE VIDEOS WITHOUT SIGNING UP FOR THE COURSE. All of the videos on the course webpage have \"preview\" turned on. Scroll down to find and click \"Expand All Sections\" and then click the preview link. You won't have access to the forums and other materials, but you can watch the videos. NOTE: Be sure to BUY the course for $0, and not sign up for Udemy's subscription plan. The subscription plan is free for the first seven days and then they charge you. It's selected by default. If you are on a laptop and can't click the BUY checkbox, try shrinking the browser window. Some have reported it works in mobile view. I'm also working on another Udemy course that follows my recent book \"Beyond the Basic Stuff with Python\". So far I have the first 15 of the planned 56 videos done. You can watch them for free on YouTube. Frequently Asked Questions: ( read this before posting questions ) This course is for beginners and assumes no previous programming experience, but the second half is useful for experienced programmers who want to learn about various third-party Python modules. If you don't have time to take the course now, that's fine. Signing up gives you lifetime access so you can work on it at your own pace. This Udemy course covers roughly the same content as the 1st edition book (the book has a little bit more, but all the basics are covered in the online course), which you can read for free online at https://inventwithpython.com The 2nd edition of Automate the Boring Stuff with Python is free online: https://automatetheboringstuff.com/2e/ I do plan on updating the Udemy course, but it'll take a while because I have other book projects I'm working on. If you sign up for this Udemy course, you'll get the updated content automatically once I finish it. It won't be a separate course. It's totally fine to start on the first edition and then read the second edition later. I'll be writing a blog post to guide first edition readers to the parts of the second edition they should read. You're not too old to learn to code. You don't need to be \"good at math\" to be good at coding. Signing up is the first step. Actually finishing the course is the next. :) There are several ways to get/stay motivated. I suggest getting a \"gym buddy\" to learn with. Check out r/ProgrammingBuddies"},
{"Title": "Video: AI Driven Automation for Microservices in Minutes", "Author": "u/ValBayArea", "Content": "Here's a video showing how you can create a database, an API and a Web App instantly, using ChatGPT and API Logic Server.  The abbreviated transcript is shown below.  At the end of this article, there's a link for the detailed instructions for running this on your own machine. API Logic Server is an open source Python project that provides Microservice Automation, based on the Flask and SQLAlchemy frameworks.  Create projects with 1 command, and customize them with Python and Rules in your IDE.  Deploy them as standard containers. 1. AI: Schema Automation You can start with an existing database, or create a new one with AI, using ChatGPT.  We enter our database description in Natural Language (shown below),  ChatGPT translates it to SQL; we then copy that to our database tool. Create a sqlite database for customers, orders, items and product Hints: use autonum keys, allow nulls, Decimal types, foreign keys, no check constraints. Include a notes field for orders. Create a few rows of only customer and product data. Enforce the Check Credit requirement: 1. Customer.Balance <= CreditLimit 2. Customer.Balance = Sum(Order.AmountTotal where date shipped is null) 3. Order.AmountTotal = Sum(Items.Amount) 4. Items.Amount = Quantity * UnitPrice 5. Store the Items.UnitPrice as a copy from Product.UnitPrice 2. Microservice Automation: ApiLogicServer create Given a new or existing database, API Logic Server provides Microservice Automation to create a project.  It‚Äôs 1 command‚Ä¶ ApiLogicServer create --project_name=sample_ai --db_url=sqlite:///sample_ai.sqlite This creates a project we can open and run in our IDE. Microservice Automation includes App Automation - a Multi-Page, Multi-Table Admin App. It‚Äôs a model - no complex UI framework code.  Customize by editing a simple yaml file. Microservice automation has also includes API Automation - a JSON:API.  The API supports related data access, pagination, optimistic locking, filtering, and sorting. Importantly, JSON:APIs are self-serve: API consumers can use Swagger to obtain the data they want  - no server coding is required. That means Custom App Dev is unblocked, day 1 . No more waiting on time-consuming, framework-based API development. 3. Customize in your IDE: Rules, and Python In minutes, we can begin collaborating with business users with the Admin App. They might uncover a requirement for Check Credit. Instead of 200 lines of code, it‚Äôs 5 spreadsheet-like rules that exactly reflect our logic design.  We declare rules using Python, with IDE code completion. \"\"\" Declarative multi-table derivations and constraints, extensible with Python. Use code completion (Rule.) to declare rules here Check Credit - Logic Design (note: translates directly into rules) 1. Customer.Balance <= CreditLimit 2. Customer.Balance = Sum(Order.AmountTotal where unshipped) 3. Order.AmountTotal = Sum(Items.Amount) 4. Items.Amount = Quantity * UnitPrice 5. Items.UnitPrice = copy from Product \"\"\" Rule.constraint(validate=models.Customer, as_condition=lambda row: row.Balance <= row.CreditLimit, error_msg=\"balance ({round(row.Balance, 2)}) exceeds credit ({round(row.CreditLimit, 2)})\") Rule.sum(derive=models.Customer.Balance,     # adjusts... as_sum_of=models.Order.AmountTotal,      # *not* a sql select sum... where=lambda row: row.ShipDate is None) Rule.sum(derive=models.Order.AmountTotal as_sum_of=models.Item.Amount) Rule.formula(derive=models.Item.Amount, as_expression=lambda row: row.UnitPrice * row.Quantity) Rule.copy(derive=models.Item.UnitPrice, from_parent=models.Product.UnitPrice) Logic is automatically reused across all relevant Use Cases (add order, reselect products, re-assign orders to different customers, etc), and optimized to minimize SQL. We can also activate security ( ApiLogicServer add-security db_url=auth ), and add a declarative Grant to filter out inactive customers for the sales role. The system also created a script for image creation, with deployment examples. Test it with the Admin App: to add an Order, and some Items.  Note the automatic Lookup, and Automatic Joins (the app shows Product Name, not ProductId). Debug the multi-table logic in your debugger.  The logic log depicts each rule firing, with multi-row chaining shown by indentation. 4. Iterate: Python and Standard Libraries Further collaboration sets the stage for a new iteration ‚Äì volume discounts for carbon neutral products.  Let‚Äôs implement that now. Volume Discounts We use our database tools to add a column. Then, we rebuild the project; our customizations are preserved. We update the logic - we change the amount derivation to test for carbon neutral products, using standard Python: def derive_amount(row: models.Item, old_row: models.Item, logic_row: LogicRow): amount = row.Quantity * row.UnitPrice if row.Product.CarbonNeutral == True and row.Quantity >= 10: amount = amount * Decimal(0.9)  # breakpoint here return amount Rule.formula(derive=models.Item.Amount, calling=derive_amount) We can verify it works by using the Admin app to add a new Item to our Order. Logic execution is automatically ordered, eliminating a major cause of friction in iteration. App Integration We might also want to integrate our microservice‚Ä¶ Provide basic read access for internal applications, An endpoint to accept orders from B2B partners, and Logic to send Kafka messages to internal systems. Internal application requirements are met with API Automation, as we discussed for custom app developers.  Basic internal application integration no longer requires complex framework-based development. We create a new B2B endpoint using standard Flask.  API Logic Server provides RowDictMapping services to transform incoming requests into SQLAlchemy rows. class ServicesEndPoint(safrs.JABase): @classmethod @jsonapi_rpc(http_methods=[\"POST\"]) def OrderB2B(self, *args, **kwargs): \"\"\" # yaml creates Swagger description (not shown) \"\"\" db = safrs.DB         # Use the safrs.DB, not db! session = db.session  # sqlalchemy.orm.scoping.scoped_session order_b2b_def = OrderB2B() request_dict_data = request.json[\"meta\"][\"args\"][\"order\"] sql_alchemy_row = order_b2b_def.dict_to_row(row_dict = request_dict_data, session = session) session.add(sql_alchemy_row) return {\"Thankyou For Your OrderB2B\"}  # automatic commit, which executes transaction logic Our logic is automatically re-used for all updates, whether from the Admin App or the new custom endpoint.  That is why our service implementation is so small. We extend our logic with an event that sends a Kafka message (not shown - about 10 lines of code).  Similar RowDictMapping transforms our rows to Kafka json payloads. Test the integration with Swagger.  The log shows the logic, and the Kafka payload. Summary And there you have it. Microservice Automation creates projects with one command, providing API and App Automation. Customize with Logic Automation , declaring rules to reduce the backend half of your system by 40X. Open source, your IDE, container-based deployment, and all the flexibility of a framework. You can run this on your own machine.  No database to install.  Here's the detailed Tutorial."},
{"Title": "r/Python Community Updates", "Author": "u/monorepo", "Content": "Hello, this is a meta-level update regarding the health of r/Python , and a candid call for action of sorts to see what the community at large considers pain points and enhancements they want addressed. I am a moderator here solely because this is one of the 2-3 subreddits I browse every day. I moderate in a way to reflects the train of thought: \"What do I want to see when I open Reddit today and scroll through my feed of cat memes and programming stuff?\" With that being said, personally I really dislike some things that come up each time I open or pass by an r/Python post: Poorly written Medium articles expanding to anywhere with paywalled articles Most things related to ChatGPT, ML/AI Everyone, including Bob's uncle, has made some sort of LLM or interface these days... Beginner Help Incorrectly flaired showcases Everyone thinks their single file, unlinted/untested/undocumented project is an intermediate showcase? Everyone thinks instead of showcase, their thing is a vital resource and flair it as such. ... and probably some more. I see these viewpoints reflected in the comments throughout the various posts here. I may not reply to everything, as my Reddit browsing is limited to bedtime, bathroom time, or 5 minutes on a meeting that I should've been emailed a summary of afterward.. so these thoughts and changes are just my own but shared by most of you (minus a few fanatics) With all of those things mentioned above, it makes r/Python a place I don't want to come to often.. so: The following changes are live and being tested to try and help improve the community health. Medium.com articles are blanket banned. Showcase flairs have been relegated to a single \"Showcase\" flair that users will pick. All other showcase flairs have been made mod-only, and 2 new ones have been added: Advanced Showcase, Invalid Showcase To be honest, hand flairing all showcase posts is nonviable.. but when we/I come across a good showcase we may take the liberty of properly marking it. Constraints placed on post title Minimum 15, Max 100 This stems from times people just have a post titled \"check it\", or conversely \"I built a thing whereby we did this cool ML/AI inferencing that did a thing because we are cool look here\" (proceeds to just post a link in the post body, and the title takes up 1/2 of the screen on your phone...) (some older changes, but noting them) Live feed of Python events from Python.org Added new rules #7, #8.. updated existing ones #4, #6 The follow changes have been live for a few months: Increased filtering for showcase posts (must include bitbucket/github/gitlab link) Greatly increased filtering for help-type questions. This might cause your posts to be in the modqueue for a little longer, as we get hit with literally tons of beginner questions even though there are clear rules and posting guidelines that pop up when you make a post that say \"Please ask your questions in r/LearnPython \" Some questions for the community: What would you like to see? How can we allow noteworthy ML/AI to be posted, as it relates to Python, but keep the not-so-fitting-of-a-whole-post type things from clogging our feeds? Should we have a megathread? The daily threads are pretty underutilized. I remove quite a bit of content that is not post-worthy that could go there but it still doesn't get the love it could. If we were to remove it, what should take its place? How can we improve it as is? Anything else you've been thinking about when browsing r/Python ."},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Create an installer for your app using TkInstaller", "Author": "u/Aareon", "Content": "A basic and buggy implementation of an installer built using Tkinter. Check it out on GitHub ! PRs welcome ‚ù§Ô∏è"},
{"Title": "To PyCharm users: How are you type checking your code?", "Author": "u/pycharm_user_5679", "Content": "There are five major type checkers for Python users: Mypy (PSF?), Pyright (Microsoft), Pyre (Meta), Pytype (Google) and the built-in type checker of PyCharm (JetBrains). According to pypistats.org, Mypy is the most downloaded last month and most popular overall: Mypy : 20.4M Pyright : 1.5M, albeit just a CLI wrapper. Pytype : 632K Pyre : 600, and this is not a typo. I guess it is just installed indirectly? These stats may not reflect the actual usage. I have no experience with Pyre and Pytype and have rarely, if ever, seen anyone using these two. For VSCode users, the go-to extension is Pylance , which ships with Pyright and has 84M installs thus far. Among the two Mypy competitors of it, one is made by Microsoft (127K installs) and one independent (172K installs). The Python Developer Survey 2022 by JetBrains shows that the number of users who use PyCharm as their primary IDE for Python programming is 29%, second only to VSCode (37%). The annual report of the same year says JetBrains have 15.9M users, but the number of PyCharm users or downloads are not mentioned. One popular and currently the only working Mypy plugin has mixed reviews . Personally, I think Pyright is the best type checker. It has support for latest feature, doesn't choke on WIP code and the maintainers are very responsive. Mypy is not as good, but is quite decent. On the contrary, PyCharm's type checker has many major problems. It is either too lenient or just fails to infer the right types most of the times. PyCharm users, how do you or your team type check your code? Do you use one or multiple of the first four type checkers? If so, is it via the CLI or a plugin? Do you just use whatever people around you use? Or do you don't care about type hinting at all?"},
{"Title": "[Polars] Why we have rewritten our string/binary type", "Author": "u/ritchie46", "Content": "I have been working on a rewrite of Polars' string/binary type for the last couple of weeks. This has been a huge refactor (the biggest I have ever done I think). This blog post I want to share the rationale behind the refactor. Pathological cases can now be 2 orders of magnitudes faster, so I think it was worth the eye-strain. :') https://pola.rs/posts/polars-string-type/"},
{"Title": "PyPDFForm - A Python PDF Form Library", "Author": "u/chinapandaman", "Content": "Hello folks! I have a project that I have been working on for three years that I‚Äôd love to show you today called PyPDForm ( https://github.com/chinapandaman/PyPDFForm ). It is a Python library that specializes in processing PDF forms, with the most outstanding feature being programmatically filling a PDF form by simply feeding a Python dictionary. I used to work at a startup company with Python as our backend stack. We were constantly given paper documents by our clients that we needed to generate into PDFs. We were doing it using reportlab scripts and I quickly found the process tedious and time consuming for more complex PDFs. This is where the idea of this project came from. Instead of writing lengthy and unmaintainable reportlab scripts to generate PDFs, you can just turn any paper document into a PDF form template and PyPDFForm can fill it easily. On top of the GitHub repo, here are some additional resources for this project: PyPi: https://pypi.org/project/PyPDFForm/ Docs: https://chinapandaman.github.io/PyPDFForm/ A public speak I did about this project: https://www.youtube.com/watch?v=8t1RdAKwr9w I hope you guys find the library helpful for your own PDF generation workflow. Feel free to try it, test it, leave comments or suggestions, and open issues. And of course if you are willing, kindly give me a star on GitHub."},
{"Title": "Do you know what is Durable Execution in Python?", "Author": "u/danzilberdan", "Content": "Hi r/Python ! Wanted to share something I have been really excited about for a long time and got the time to work on. TL;DR - Sharing with you the Serverless Durable Execution Project that I created - Durable When building stateful applications like chatbots, data workflows, etc,  we need to use various mechanisms to keep the state of our application. For example, a chatbot that receives a message needs to query past conversations and metadata from a database in order to decide how to respond. What if we could just write a simple loop that will send a message, wait for a response and keep all of the conversation's state in-memory using simple python data types? Of course the problem is that the state may be deleted occasionally - version upgrades, network failures and so on... We don't trust our code and it's state to survive more that a couple of milliseconds, maybe seconds. Durable execution is a way to execute functions in a way that can survive over months and years . You can read about it here . But in short, it keeps a history of events for each functions and replays the events when a function needs to be restored to a previous state. The service that I created, Durable , allows you to write simple python functions that execute on serverless compute and that are inherently Durable (executed with Durable Execution). I have a lot of ideas of how this can be used by developers - personally and professionally. Things like personal finance tracking, TODO list automations and even cloud infrustructure automation and so on.. If you are also interested in this tech please let me know! I would also be glad for feedback on the project. If you got all the way here, you have got to checkout Durable :) Dan"},
{"Title": "I made an online editor that runs matplotlib in your browser", "Author": "u/kuim1", "Content": "I work in academia, and plotting is one of the most painful things. Most of the time I end up with multiple virtual environments and files spread across my file system with plots for experiments, presentations, or papers. It is especially inconvenient when I want to reuse some old designs. A couple of weeks ago I found out about Pyodide , which is a Python interpreter compiled to Webassembly that allows to run Python code in the browser, and it supports matplotlib! So I had the idea to create a website similar to Overleaf but for plotting with matplotlib. The website is just a quick prototype, but it works. You can create multiple projects (which are stored in localStorage ). When you select a project, there is a code editor and the plot output. The first time you open a project it takes some time to download Python, but the next executions are really fast IMO (just press \"Ctrl/Cmd + S\" inside the editor to execute the code). The cool thing is that with Pyodide everything happens in your browser! No servers are involved in executing your code. You can check it out here: https://matplotlib.online Features already present: Two columns, one for the code, and one for the plot. Much better than google collab (when the code starts to grow) Import 1-dimensional variables from spreadsheets: We have all the experimental data in spreadsheets, and it's a pain to convert it to Python for plotting. So I implemented a feature where you directly paste one spreadsheet row/column and it converts the data into a Python list. Vim/emacs/sublime text mode Things I'm planning to add in the future: Render the figure in a different window, to allow an efficient use of dual-screen setups. Allow to optionally create an account to store the scripts in a server (now they reside in the browser storage). I would love to have a feature where I could paste 2-dimensional data from a spreadsheet and get a matplotlib code/plot, just like you do in Google Docs or Excel. Collaborative editing would be awesome but I don't think I will be able to implement it soon. Better design/UX. I'm not a web developer so this is not my strongest skill. Let me know what do you think about it :)"},
{"Title": "Redis Queue Dashboard | Fast API", "Author": "u/arne226", "Content": "Hey guys. I just launched a small open-source project, a FastAPI-based RQ-dashboard. Feel free to check it out at: https://github.com/Hannes221/rq-dashboard-fast Feedback is highly appreciated ‚≠ê The Goal is to make it easier to integrate an RQ Dashboard into FastAPI applications. *The Docker Image has just been launched today."},
{"Title": "Twitter API Wrapper for Python without API Keys", "Author": "u/06ddd", "Content": "Twikit https://github.com/d60/twikit You can create a twitter bot for free! I have created a Twitter API wrapper that works with just a username, email address, and password ‚Äî no API key required. With this library, you can post tweets , search tweets , get trending topics , etc. for free. In addition, it supports both asynchronous and synchronous use, so it can be used in a variety of situations. Please send me your comments and suggestions. Additionally, if you're willing, kindly give me a star on GitHub‚≠êÔ∏è."},
{"Title": "I made a CLI to remind users of the custom bash/zsh aliases and functions in their dotfiles", "Author": "u/grefft", "Content": "Maybe it's just me, but I always forget many of the time saving aliases and functions I've added to my dotfiles over the years.  So I built a CLI named Halp to be a TLDR/ManPage for your custom commands. Point it to your dotfiles and it will index them. Then run halp <command> to remind yourself what something does. You can categorize your commands, search them with regex, customize the help text, etc. If you have TLDR installed, it will query TLDR pages for built-in commands. Or, pass in a command with options halp \"sed -Eni\" to get the man pages filtered for the options you provided. This effectively turns Halp into a one-stop-shop for command line help. Github link: https://github.com/natelandau/halp Pypi link: https://pypi.org/project/halper/ Currently only tested on MacOS and Linux using Bash/ZSH."},
{"Title": "Custom Browser Testing and Capturing Framework", "Author": "u/ki3selerde", "Content": "At the moment, I create all demos for my web apps by hand, and because I am a very special kind of lazy I invested quite some time to automate this task, using a simple Python script. This script quickly spiraled out of control into a full-blown browser testing and capturing framework.The first prototype of my new tool is now able to generate video clips like this, fully automated \\o/ https://florianscherf.de/blog/2024/02/05/how-to-write-your-own-browser-testing-framework/ https://github.com/fscherf/milan"},
{"Title": "ienv: brutalise your venvs by symlinking them all together!", "Author": "u/binlargin", "Content": "https://github.com/bitplane/ienv Does exactly what it says in the disclaimer; reduce venv sizes by recklessly replacing all the files with symlinks. (I as in Roman numeral for 1, the other letters were taken) A simple and effective tool that might cause you more trouble than it saves you, but it might get you out of a tough disk space situation. If it breaks your environments then it's your fault, but if it saves you gigs of disk space then I'll take full credit up until the moment you realise it caused problems. works_on_my_machine.jpg Readme follows: ienv !!WARNING!! THIS IS A ONE WAY PROCESS !!WARNING!! Have you got 30GB of SciPy on your disk because every time someone wants to add two numbers together they install a whole lab on your machine? Are your fifty copies of PyTorch and TensorFlow weighing heavy on your SSD? Why not throw caution to the wind and replace everyhing in the site-packages dir with symlinks? It's not like you're going to need them anyway. And nobody will ever write to them and mess up every venv on your machine. Right? !!WARNING!! THIS IS RECKLESS AND STUPID !!WARNING!! Usage pip install ienv ienv .venv ienv some/other/venv Recovery Pull requests welcome! All the files are there, I've just not written anything to bring them back yet. Ever, probably. Credits Mostly written by ChatGPT just to see if it could do it. With a bit of guidance it actually could, but it can't learn like that so it's like a student that nods along and you think it's listening and it's really just playing along and tricking you into doing its homework. But to be honest it was either that or copilot anyway. License They say you get what you pay for, sometimes less. This is one of those times. As free software distributed under the WTFPL (with one additional clause); this is one of the times when you pay for what you get."},
{"Title": "Lagom - dependency injector", "Author": "u/ZestycloseGate7928", "Content": "Question for ports and adapters architecture experts . Why this DI injector https://github.com/meadsteve/lagom Is not popular ?  Only 230 stars on Github. Uses Python typing to map dependencies, looks much more pythonic than injector or dependency-injector libs. Are there any known problems that solution has or it's only not enough marketing ? If not Lagom than what you recommend ?"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "IDE made with PyQt6 [UPDATE]", "Author": "u/Specialist-Arachnid6", "Content": "Aura Text is an excellent text/code editor that offers a wide array of essential tools, and is built with PyQt6 and Python. Key Features: Edit files (duh) Support up to 30 languages Autocompletion Split pane Markdown editor Terminal with history Python Console Plugin support Extensive theming including Material Theming support Extremely customisable GitHub: https://github.com/rohankishore/Aura-Text"},
{"Title": "CustomTkinter Snippets v3.0.0: Enhance Your Python GUI Workflow!", "Author": "u/Playful_Round_6530", "Content": "Excited to announce CustomTkinter Snippets v3.0.0! This Visual Studio Code extension turbocharges your Python GUI development with intuitive code snippets. With a wide range of components covered and customization options available, coding sleek GUIs has never been easier. What's New: Expanded snippet coverage Enhanced customization options Bug fixes for smoother development TODO: Addition of special events and binds as snippets Addition of object based snippets Give it a try and supercharge your GUI development today! Links: Repository Link: CustomTkinter Snippets Extension Link: CustomTkinter Snippets VSCode Extension Credit: Credits to Tom Schimansky for the awesome CustomTkinter library! Don't forget to star the repo! #Python #GUI #Development #VisualStudioCode"},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "One Trillion Row Challenge (1TRC)", "Author": "u/mrocklin", "Content": "I really liked the simplicity of the One Billion Row Challenge (1BRC) that took off last month. It was fun to see lots of people apply different tools to the same simple-yet-clear problem ‚ÄúHow do you parse, process, and aggregate a large CSV file as quickly as possible?‚Äù For fun, my colleagues and I made a One Trillion Row Challenge (1TRC) dataset üôÇ. Data lives on S3 in Parquet format (CSV made zero sense here) in a public bucket at s3://coiled-datasets-rp/1trc and is roughly 12 TiB uncompressed. We (the Dask team) were able to complete the TRC query in around six minutes for around $1.10.For more information see this blogpost and this repository (Edit: this was taken down originally for having a Medium link.  I've now included an open-access blog link instead)"},
{"Title": "What are you going to do with latest python release", "Author": "u/Arckman_", "Content": "I was eagerly waiting for the parallel programming or subinterpreters to do something with it. What were you all expecting and thinking to do with the latest version?"},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Python 3.12.2 and 3.11.8 released", "Author": "u/japaget", "Content": "Get Python 3.12.2 here: https://www.python.org/downloads/release/python-3122/ Get Python 3.11.8 here: https://www.python.org/downloads/release/python-3118/"},
{"Title": "I wrote a minimalistic search engine in Python", "Author": "u/AM_DS", "Content": "Hi * Some months ago I joined a new company as a search data scientist, and since then I've been working with Solr (a search engine written in Java). Since this wasn't my field of expertise I decided to implement a simple search engine in Python. It's not a production-ready project, but it shows how a search engine works under the hood. You can find the project here . I've also written a post explaining how I've implemented it here . Besides the search engine, the project also includes a FastAPI app that exposes a website allowing users to interact with the search engine. Let me know what you think!"},
{"Title": "2,000 free sign ups available for \"Automate the Boring Stuff with Python\" online course. (Feb 2024)", "Author": "u/AlSweigart", "Content": "If you want to learn to code, I've released 2,000 free sign ups for my course following my Automate the Boring Stuff with Python book (each has 1,000 sign ups, use the other one if one is sold out): *The sign ups are all used up, but you can still watch all the videos for free. Read below! https://udemy.com/course/automate/?couponCode=FEB2024FREE https://udemy.com/course/automate/?couponCode=FEB2024FREE2 If you are reading this after the sign ups are used up, you can always find the first 15 of the course's 50 videos are free on YouTube if you want to preview them. YOU CAN ALSO WATCH THE VIDEOS WITHOUT SIGNING UP FOR THE COURSE. All of the videos on the course webpage have \"preview\" turned on. Scroll down to find and click \"Expand All Sections\" and then click the preview link. You won't have access to the forums and other materials, but you can watch the videos. NOTE: Be sure to BUY the course for $0, and not sign up for Udemy's subscription plan. The subscription plan is free for the first seven days and then they charge you. It's selected by default. If you are on a laptop and can't click the BUY checkbox, try shrinking the browser window. Some have reported it works in mobile view. I'm also working on another Udemy course that follows my recent book \"Beyond the Basic Stuff with Python\". So far I have the first 15 of the planned 56 videos done. You can watch them for free on YouTube. Frequently Asked Questions: ( read this before posting questions ) This course is for beginners and assumes no previous programming experience, but the second half is useful for experienced programmers who want to learn about various third-party Python modules. If you don't have time to take the course now, that's fine. Signing up gives you lifetime access so you can work on it at your own pace. This Udemy course covers roughly the same content as the 1st edition book (the book has a little bit more, but all the basics are covered in the online course), which you can read for free online at https://inventwithpython.com The 2nd edition of Automate the Boring Stuff with Python is free online: https://automatetheboringstuff.com/2e/ I do plan on updating the Udemy course, but it'll take a while because I have other book projects I'm working on. If you sign up for this Udemy course, you'll get the updated content automatically once I finish it. It won't be a separate course. It's totally fine to start on the first edition and then read the second edition later. I'll be writing a blog post to guide first edition readers to the parts of the second edition they should read. You're not too old to learn to code. You don't need to be \"good at math\" to be good at coding. Signing up is the first step. Actually finishing the course is the next. :) There are several ways to get/stay motivated. I suggest getting a \"gym buddy\" to learn with. Check out r/ProgrammingBuddies"},
{"Title": "Video: AI Driven Automation for Microservices in Minutes", "Author": "u/ValBayArea", "Content": "Here's a video showing how you can create a database, an API and a Web App instantly, using ChatGPT and API Logic Server.  The abbreviated transcript is shown below.  At the end of this article, there's a link for the detailed instructions for running this on your own machine. API Logic Server is an open source Python project that provides Microservice Automation, based on the Flask and SQLAlchemy frameworks.  Create projects with 1 command, and customize them with Python and Rules in your IDE.  Deploy them as standard containers. 1. AI: Schema Automation You can start with an existing database, or create a new one with AI, using ChatGPT.  We enter our database description in Natural Language (shown below),  ChatGPT translates it to SQL; we then copy that to our database tool. Create a sqlite database for customers, orders, items and product Hints: use autonum keys, allow nulls, Decimal types, foreign keys, no check constraints. Include a notes field for orders. Create a few rows of only customer and product data. Enforce the Check Credit requirement: 1. Customer.Balance <= CreditLimit 2. Customer.Balance = Sum(Order.AmountTotal where date shipped is null) 3. Order.AmountTotal = Sum(Items.Amount) 4. Items.Amount = Quantity * UnitPrice 5. Store the Items.UnitPrice as a copy from Product.UnitPrice 2. Microservice Automation: ApiLogicServer create Given a new or existing database, API Logic Server provides Microservice Automation to create a project.  It‚Äôs 1 command‚Ä¶ ApiLogicServer create --project_name=sample_ai --db_url=sqlite:///sample_ai.sqlite This creates a project we can open and run in our IDE. Microservice Automation includes App Automation - a Multi-Page, Multi-Table Admin App. It‚Äôs a model - no complex UI framework code.  Customize by editing a simple yaml file. Microservice automation has also includes API Automation - a JSON:API.  The API supports related data access, pagination, optimistic locking, filtering, and sorting. Importantly, JSON:APIs are self-serve: API consumers can use Swagger to obtain the data they want  - no server coding is required. That means Custom App Dev is unblocked, day 1 . No more waiting on time-consuming, framework-based API development. 3. Customize in your IDE: Rules, and Python In minutes, we can begin collaborating with business users with the Admin App. They might uncover a requirement for Check Credit. Instead of 200 lines of code, it‚Äôs 5 spreadsheet-like rules that exactly reflect our logic design.  We declare rules using Python, with IDE code completion. \"\"\" Declarative multi-table derivations and constraints, extensible with Python. Use code completion (Rule.) to declare rules here Check Credit - Logic Design (note: translates directly into rules) 1. Customer.Balance <= CreditLimit 2. Customer.Balance = Sum(Order.AmountTotal where unshipped) 3. Order.AmountTotal = Sum(Items.Amount) 4. Items.Amount = Quantity * UnitPrice 5. Items.UnitPrice = copy from Product \"\"\" Rule.constraint(validate=models.Customer, as_condition=lambda row: row.Balance <= row.CreditLimit, error_msg=\"balance ({round(row.Balance, 2)}) exceeds credit ({round(row.CreditLimit, 2)})\") Rule.sum(derive=models.Customer.Balance,     # adjusts... as_sum_of=models.Order.AmountTotal,      # *not* a sql select sum... where=lambda row: row.ShipDate is None) Rule.sum(derive=models.Order.AmountTotal as_sum_of=models.Item.Amount) Rule.formula(derive=models.Item.Amount, as_expression=lambda row: row.UnitPrice * row.Quantity) Rule.copy(derive=models.Item.UnitPrice, from_parent=models.Product.UnitPrice) Logic is automatically reused across all relevant Use Cases (add order, reselect products, re-assign orders to different customers, etc), and optimized to minimize SQL. We can also activate security ( ApiLogicServer add-security db_url=auth ), and add a declarative Grant to filter out inactive customers for the sales role. The system also created a script for image creation, with deployment examples. Test it with the Admin App: to add an Order, and some Items.  Note the automatic Lookup, and Automatic Joins (the app shows Product Name, not ProductId). Debug the multi-table logic in your debugger.  The logic log depicts each rule firing, with multi-row chaining shown by indentation. 4. Iterate: Python and Standard Libraries Further collaboration sets the stage for a new iteration ‚Äì volume discounts for carbon neutral products.  Let‚Äôs implement that now. Volume Discounts We use our database tools to add a column. Then, we rebuild the project; our customizations are preserved. We update the logic - we change the amount derivation to test for carbon neutral products, using standard Python: def derive_amount(row: models.Item, old_row: models.Item, logic_row: LogicRow): amount = row.Quantity * row.UnitPrice if row.Product.CarbonNeutral == True and row.Quantity >= 10: amount = amount * Decimal(0.9)  # breakpoint here return amount Rule.formula(derive=models.Item.Amount, calling=derive_amount) We can verify it works by using the Admin app to add a new Item to our Order. Logic execution is automatically ordered, eliminating a major cause of friction in iteration. App Integration We might also want to integrate our microservice‚Ä¶ Provide basic read access for internal applications, An endpoint to accept orders from B2B partners, and Logic to send Kafka messages to internal systems. Internal application requirements are met with API Automation, as we discussed for custom app developers.  Basic internal application integration no longer requires complex framework-based development. We create a new B2B endpoint using standard Flask.  API Logic Server provides RowDictMapping services to transform incoming requests into SQLAlchemy rows. class ServicesEndPoint(safrs.JABase): @classmethod @jsonapi_rpc(http_methods=[\"POST\"]) def OrderB2B(self, *args, **kwargs): \"\"\" # yaml creates Swagger description (not shown) \"\"\" db = safrs.DB         # Use the safrs.DB, not db! session = db.session  # sqlalchemy.orm.scoping.scoped_session order_b2b_def = OrderB2B() request_dict_data = request.json[\"meta\"][\"args\"][\"order\"] sql_alchemy_row = order_b2b_def.dict_to_row(row_dict = request_dict_data, session = session) session.add(sql_alchemy_row) return {\"Thankyou For Your OrderB2B\"}  # automatic commit, which executes transaction logic Our logic is automatically re-used for all updates, whether from the Admin App or the new custom endpoint.  That is why our service implementation is so small. We extend our logic with an event that sends a Kafka message (not shown - about 10 lines of code).  Similar RowDictMapping transforms our rows to Kafka json payloads. Test the integration with Swagger.  The log shows the logic, and the Kafka payload. Summary And there you have it. Microservice Automation creates projects with one command, providing API and App Automation. Customize with Logic Automation , declaring rules to reduce the backend half of your system by 40X. Open source, your IDE, container-based deployment, and all the flexibility of a framework. You can run this on your own machine.  No database to install.  Here's the detailed Tutorial."},
{"Title": "r/Python Community Updates", "Author": "u/monorepo", "Content": "Hello, this is a meta-level update regarding the health of r/Python , and a candid call for action of sorts to see what the community at large considers pain points and enhancements they want addressed. I am a moderator here solely because this is one of the 2-3 subreddits I browse every day. I moderate in a way to reflects the train of thought: \"What do I want to see when I open Reddit today and scroll through my feed of cat memes and programming stuff?\" With that being said, personally I really dislike some things that come up each time I open or pass by an r/Python post: Poorly written Medium articles expanding to anywhere with paywalled articles Most things related to ChatGPT, ML/AI Everyone, including Bob's uncle, has made some sort of LLM or interface these days... Beginner Help Incorrectly flaired showcases Everyone thinks their single file, unlinted/untested/undocumented project is an intermediate showcase? Everyone thinks instead of showcase, their thing is a vital resource and flair it as such. ... and probably some more. I see these viewpoints reflected in the comments throughout the various posts here. I may not reply to everything, as my Reddit browsing is limited to bedtime, bathroom time, or 5 minutes on a meeting that I should've been emailed a summary of afterward.. so these thoughts and changes are just my own but shared by most of you (minus a few fanatics) With all of those things mentioned above, it makes r/Python a place I don't want to come to often.. so: The following changes are live and being tested to try and help improve the community health. Medium.com articles are blanket banned. Showcase flairs have been relegated to a single \"Showcase\" flair that users will pick. All other showcase flairs have been made mod-only, and 2 new ones have been added: Advanced Showcase, Invalid Showcase To be honest, hand flairing all showcase posts is nonviable.. but when we/I come across a good showcase we may take the liberty of properly marking it. Constraints placed on post title Minimum 15, Max 100 This stems from times people just have a post titled \"check it\", or conversely \"I built a thing whereby we did this cool ML/AI inferencing that did a thing because we are cool look here\" (proceeds to just post a link in the post body, and the title takes up 1/2 of the screen on your phone...) (some older changes, but noting them) Live feed of Python events from Python.org Added new rules #7, #8.. updated existing ones #4, #6 The follow changes have been live for a few months: Increased filtering for showcase posts (must include bitbucket/github/gitlab link) Greatly increased filtering for help-type questions. This might cause your posts to be in the modqueue for a little longer, as we get hit with literally tons of beginner questions even though there are clear rules and posting guidelines that pop up when you make a post that say \"Please ask your questions in r/LearnPython \" Some questions for the community: What would you like to see? How can we allow noteworthy ML/AI to be posted, as it relates to Python, but keep the not-so-fitting-of-a-whole-post type things from clogging our feeds? Should we have a megathread? The daily threads are pretty underutilized. I remove quite a bit of content that is not post-worthy that could go there but it still doesn't get the love it could. If we were to remove it, what should take its place? How can we improve it as is? Anything else you've been thinking about when browsing r/Python ."},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Create an installer for your app using TkInstaller", "Author": "u/Aareon", "Content": "A basic and buggy implementation of an installer built using Tkinter. Check it out on GitHub ! PRs welcome ‚ù§Ô∏è"},
{"Title": "To PyCharm users: How are you type checking your code?", "Author": "u/pycharm_user_5679", "Content": "There are five major type checkers for Python users: Mypy (PSF?), Pyright (Microsoft), Pyre (Meta), Pytype (Google) and the built-in type checker of PyCharm (JetBrains). According to pypistats.org, Mypy is the most downloaded last month and most popular overall: Mypy : 20.4M Pyright : 1.5M, albeit just a CLI wrapper. Pytype : 632K Pyre : 600, and this is not a typo. I guess it is just installed indirectly? These stats may not reflect the actual usage. I have no experience with Pyre and Pytype and have rarely, if ever, seen anyone using these two. For VSCode users, the go-to extension is Pylance , which ships with Pyright and has 84M installs thus far. Among the two Mypy competitors of it, one is made by Microsoft (127K installs) and one independent (172K installs). The Python Developer Survey 2022 by JetBrains shows that the number of users who use PyCharm as their primary IDE for Python programming is 29%, second only to VSCode (37%). The annual report of the same year says JetBrains have 15.9M users, but the number of PyCharm users or downloads are not mentioned. One popular and currently the only working Mypy plugin has mixed reviews . Personally, I think Pyright is the best type checker. It has support for latest feature, doesn't choke on WIP code and the maintainers are very responsive. Mypy is not as good, but is quite decent. On the contrary, PyCharm's type checker has many major problems. It is either too lenient or just fails to infer the right types most of the times. PyCharm users, how do you or your team type check your code? Do you use one or multiple of the first four type checkers? If so, is it via the CLI or a plugin? Do you just use whatever people around you use? Or do you don't care about type hinting at all?"},
{"Title": "[Polars] Why we have rewritten our string/binary type", "Author": "u/ritchie46", "Content": "I have been working on a rewrite of Polars' string/binary type for the last couple of weeks. This has been a huge refactor (the biggest I have ever done I think). This blog post I want to share the rationale behind the refactor. Pathological cases can now be 2 orders of magnitudes faster, so I think it was worth the eye-strain. :') https://pola.rs/posts/polars-string-type/"},
{"Title": "PyPDFForm - A Python PDF Form Library", "Author": "u/chinapandaman", "Content": "Hello folks! I have a project that I have been working on for three years that I‚Äôd love to show you today called PyPDForm ( https://github.com/chinapandaman/PyPDFForm ). It is a Python library that specializes in processing PDF forms, with the most outstanding feature being programmatically filling a PDF form by simply feeding a Python dictionary. I used to work at a startup company with Python as our backend stack. We were constantly given paper documents by our clients that we needed to generate into PDFs. We were doing it using reportlab scripts and I quickly found the process tedious and time consuming for more complex PDFs. This is where the idea of this project came from. Instead of writing lengthy and unmaintainable reportlab scripts to generate PDFs, you can just turn any paper document into a PDF form template and PyPDFForm can fill it easily. On top of the GitHub repo, here are some additional resources for this project: PyPi: https://pypi.org/project/PyPDFForm/ Docs: https://chinapandaman.github.io/PyPDFForm/ A public speak I did about this project: https://www.youtube.com/watch?v=8t1RdAKwr9w I hope you guys find the library helpful for your own PDF generation workflow. Feel free to try it, test it, leave comments or suggestions, and open issues. And of course if you are willing, kindly give me a star on GitHub."},
{"Title": "Do you know what is Durable Execution in Python?", "Author": "u/danzilberdan", "Content": "Hi r/Python ! Wanted to share something I have been really excited about for a long time and got the time to work on. TL;DR - Sharing with you the Serverless Durable Execution Project that I created - Durable When building stateful applications like chatbots, data workflows, etc,  we need to use various mechanisms to keep the state of our application. For example, a chatbot that receives a message needs to query past conversations and metadata from a database in order to decide how to respond. What if we could just write a simple loop that will send a message, wait for a response and keep all of the conversation's state in-memory using simple python data types? Of course the problem is that the state may be deleted occasionally - version upgrades, network failures and so on... We don't trust our code and it's state to survive more that a couple of milliseconds, maybe seconds. Durable execution is a way to execute functions in a way that can survive over months and years . You can read about it here . But in short, it keeps a history of events for each functions and replays the events when a function needs to be restored to a previous state. The service that I created, Durable , allows you to write simple python functions that execute on serverless compute and that are inherently Durable (executed with Durable Execution). I have a lot of ideas of how this can be used by developers - personally and professionally. Things like personal finance tracking, TODO list automations and even cloud infrustructure automation and so on.. If you are also interested in this tech please let me know! I would also be glad for feedback on the project. If you got all the way here, you have got to checkout Durable :) Dan"},
{"Title": "I made an online editor that runs matplotlib in your browser", "Author": "u/kuim1", "Content": "I work in academia, and plotting is one of the most painful things. Most of the time I end up with multiple virtual environments and files spread across my file system with plots for experiments, presentations, or papers. It is especially inconvenient when I want to reuse some old designs. A couple of weeks ago I found out about Pyodide , which is a Python interpreter compiled to Webassembly that allows to run Python code in the browser, and it supports matplotlib! So I had the idea to create a website similar to Overleaf but for plotting with matplotlib. The website is just a quick prototype, but it works. You can create multiple projects (which are stored in localStorage ). When you select a project, there is a code editor and the plot output. The first time you open a project it takes some time to download Python, but the next executions are really fast IMO (just press \"Ctrl/Cmd + S\" inside the editor to execute the code). The cool thing is that with Pyodide everything happens in your browser! No servers are involved in executing your code. You can check it out here: https://matplotlib.online Features already present: Two columns, one for the code, and one for the plot. Much better than google collab (when the code starts to grow) Import 1-dimensional variables from spreadsheets: We have all the experimental data in spreadsheets, and it's a pain to convert it to Python for plotting. So I implemented a feature where you directly paste one spreadsheet row/column and it converts the data into a Python list. Vim/emacs/sublime text mode Things I'm planning to add in the future: Render the figure in a different window, to allow an efficient use of dual-screen setups. Allow to optionally create an account to store the scripts in a server (now they reside in the browser storage). I would love to have a feature where I could paste 2-dimensional data from a spreadsheet and get a matplotlib code/plot, just like you do in Google Docs or Excel. Collaborative editing would be awesome but I don't think I will be able to implement it soon. Better design/UX. I'm not a web developer so this is not my strongest skill. Let me know what do you think about it :)"},
{"Title": "Redis Queue Dashboard | Fast API", "Author": "u/arne226", "Content": "Hey guys. I just launched a small open-source project, a FastAPI-based RQ-dashboard. Feel free to check it out at: https://github.com/Hannes221/rq-dashboard-fast Feedback is highly appreciated ‚≠ê The Goal is to make it easier to integrate an RQ Dashboard into FastAPI applications. *The Docker Image has just been launched today."},
{"Title": "Twitter API Wrapper for Python without API Keys", "Author": "u/06ddd", "Content": "Twikit https://github.com/d60/twikit You can create a twitter bot for free! I have created a Twitter API wrapper that works with just a username, email address, and password ‚Äî no API key required. With this library, you can post tweets , search tweets , get trending topics , etc. for free. In addition, it supports both asynchronous and synchronous use, so it can be used in a variety of situations. Please send me your comments and suggestions. Additionally, if you're willing, kindly give me a star on GitHub‚≠êÔ∏è."},
{"Title": "I made a CLI to remind users of the custom bash/zsh aliases and functions in their dotfiles", "Author": "u/grefft", "Content": "Maybe it's just me, but I always forget many of the time saving aliases and functions I've added to my dotfiles over the years.  So I built a CLI named Halp to be a TLDR/ManPage for your custom commands. Point it to your dotfiles and it will index them. Then run halp <command> to remind yourself what something does. You can categorize your commands, search them with regex, customize the help text, etc. If you have TLDR installed, it will query TLDR pages for built-in commands. Or, pass in a command with options halp \"sed -Eni\" to get the man pages filtered for the options you provided. This effectively turns Halp into a one-stop-shop for command line help. Github link: https://github.com/natelandau/halp Pypi link: https://pypi.org/project/halper/ Currently only tested on MacOS and Linux using Bash/ZSH."},
{"Title": "Custom Browser Testing and Capturing Framework", "Author": "u/ki3selerde", "Content": "At the moment, I create all demos for my web apps by hand, and because I am a very special kind of lazy I invested quite some time to automate this task, using a simple Python script. This script quickly spiraled out of control into a full-blown browser testing and capturing framework.The first prototype of my new tool is now able to generate video clips like this, fully automated \\o/ https://florianscherf.de/blog/2024/02/05/how-to-write-your-own-browser-testing-framework/ https://github.com/fscherf/milan"},
{"Title": "ienv: brutalise your venvs by symlinking them all together!", "Author": "u/binlargin", "Content": "https://github.com/bitplane/ienv Does exactly what it says in the disclaimer; reduce venv sizes by recklessly replacing all the files with symlinks. (I as in Roman numeral for 1, the other letters were taken) A simple and effective tool that might cause you more trouble than it saves you, but it might get you out of a tough disk space situation. If it breaks your environments then it's your fault, but if it saves you gigs of disk space then I'll take full credit up until the moment you realise it caused problems. works_on_my_machine.jpg Readme follows: ienv !!WARNING!! THIS IS A ONE WAY PROCESS !!WARNING!! Have you got 30GB of SciPy on your disk because every time someone wants to add two numbers together they install a whole lab on your machine? Are your fifty copies of PyTorch and TensorFlow weighing heavy on your SSD? Why not throw caution to the wind and replace everyhing in the site-packages dir with symlinks? It's not like you're going to need them anyway. And nobody will ever write to them and mess up every venv on your machine. Right? !!WARNING!! THIS IS RECKLESS AND STUPID !!WARNING!! Usage pip install ienv ienv .venv ienv some/other/venv Recovery Pull requests welcome! All the files are there, I've just not written anything to bring them back yet. Ever, probably. Credits Mostly written by ChatGPT just to see if it could do it. With a bit of guidance it actually could, but it can't learn like that so it's like a student that nods along and you think it's listening and it's really just playing along and tricking you into doing its homework. But to be honest it was either that or copilot anyway. License They say you get what you pay for, sometimes less. This is one of those times. As free software distributed under the WTFPL (with one additional clause); this is one of the times when you pay for what you get."},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Surprising behavior of xml.etree", "Author": "u/danmickla", "Content": "If this is \"help\" and belongs in LearnPython, forgive me.  I think I know a fair bit about Python, but this behavior of a standard library module is puzzling to me. Test code: import xml.etree.ElementTree as ET XMLSTR=\"\"\" <r> root text1 <c1>c1 text</c1> root text 2 </r>\"\"\" print(f'XML:{XMLSTR}') root=ET.fromstring(XMLSTR) print(f'{root.text=} {root.tail=}') c1=root.find('c1') print(f'{c1.text=} {c1.tail=}') Results: XML: <r> root text1 <c1>c1 text</c1> root text 2 </r> root.text='\\n   root text1\\n   ' root.tail=None c1.text='c1 text' c1.tail='\\n   root text 2\\n' It's surprising to me that the 'root text 2' string is apparently not associated with the root node.  Does this surprise anyone else?  Am I expecting the wrong thing from xml.etree?"},
{"Title": "RD DICOM exportation", "Author": "u/One_Speech_5909", "Content": "Hi you all! Im working on a RD DICOM file, and I cannot exportexport a data, in DICOM format wich contains data likewise maximun dose slide, dose value, slide dimensiom, etc. My idea is to explort a single frame  and perform gamma 2D comparison. Some idea ?"},
{"Title": "Scheduled (cron) tasks in one line for Quart, with Quart-Tasks", "Author": "u/stetio", "Content": "Hello, I've recently released Quart-Tasks which is a Quart extension that provides scheduled background tasks, from quart import Quart from quart_tasks import QuartTasks app = Quart(__name__) tasks = QuartTasks(app) @tasks.cron(\"*/5 * * * *\")  # every 5 minutes async def infrequent_task(): ...  # Do something @tasks.cron( seconds=\"*1/0\",  # every 10 seconds minutes=\"*\", hours=\"*\", day_of_month=\"*\", month=\"*\", day_of_week=\"*\", ) async def frequent_task(): ...  # Do something @tasks.periodic(timedelta(seconds=10)) async def regular_task(): ...  # Do Something I've also recorded a tutorial showing how to use it here . Questions, thoughts, and feedback welcome!"},
{"Title": "I made a QR Code with Logo Generator using CTk", "Author": "u/Specialist-Arachnid6", "Content": "This app has a simple purpose, to create QR codes with logos/images inside them. It is then integrated into a UI made with CustomTkinter. You can customise the color of the QR code, image, and background color. GitHub: https://github.com/rohankishore/QrGen"},
{"Title": "Factory and Repository Pattern with SQLAlchemy and Pydantic", "Author": "u/LTMullineux", "Content": "Hey! I recently wanted to start sharing my knowledge of Python, and the struggles/joy I have with it. Check out this article showing how to implement the Factory and Repository patterns in Python with SQLAlchemy and Pydantic. It's a bit advanced, drawing from my freelancing experience and having to build the same applications over and over again, but I'm sure there's something for everyone. I hope your coding life easier! üé•üíª"},
{"Title": "Dev: The Ultimate AI-Powered Coding Assistant for Python Developers", "Author": "u/Quirky-Low-7500", "Content": "I created a toolkit for python developers I named it Dev, Dev is a Python package that integrates seamlessly with your code editor and provides you with a range of functionalities, such as: - AI-powered Debugging: Stuck on a coding bug? Dev can analyze your code, identify the error, and suggest corrections. - Automated README Generation: Forget manually writing READMEs! Dev can automatically generate a comprehensive README file for your project. - Creative Code Generation: Need a jumpstart on your code? Dev can generate code snippets based on your instructions, offering you creative and effective solutions. - In-depth Code Review: Unsure about your code's quality? Dev provides a detailed code review, highlighting potential improvements, optimizations, and areas for consideration. - Seamless Git Integration: Dev simplifies project management by automating Git commands. You can easily create and upload repositories, rename existing ones, and push your code with just a few function calls. - And much more! Dev is your comprehensive coding companion for python developers looking for a productivity boost or a beginner seeking guidance, Dev has something to offer. Embrace the power of AI and watch your coding workflow reach new heights of efficiency and effectiveness. To get started with Dev, visit the GitHub repository and follow the installation instructions: https://github.com/GitCoder052023/Dev.git"},
{"Title": "PyDev Debugger and sys.monitoring (PEP 669) -- i.e.: really fast debugging for Python 3.12!", "Author": "u/fabioz", "Content": "I've just released the newer version of pydevd (available in PyDev: https://www.pydev.org/ and LiClipse: https://www.liclipse.com/ ) which now makes use of the improvements from PEP 669. See: https://pydev.blogspot.com/2024/02/pydev-debugger-and-sysmonitoring-pep.html for more details."},
{"Title": "Python is weakly scoped", "Author": "u/Kiuhnm", "Content": "In the same way global variables pollute the global environment, local variables without any scoping pollute the local environment. To split or not to split That's less of a problem because functions are usually short, but \"usually\" is not \"always\". Splitting longer functions at all costs is a mistake. For instance, I have a longish function which generates a textual description of a certain object. The function has the following structure: def get_text(self, ...) -> str: text = TextBuilder() text.add_block(...) ... text.add_span(...) ... text.add_block(...) ... text.newline() ... return str(text) Splitting such a function wouldn't help because there are no particular abstractions to factor out. The rule  according to which a function should've maximum N LOCs is superficial. A more sensible rule is that a function should only do one thing. Often times you need less than 20 lines, but occasionally 50 or more. A function that does half or less of one thing is worse than a function that does more than one thing. That's my opinion, at least. As an auditor, I've noticed that splitting long functions at all costs may result in bad abstractions, messy interfaces (i.e. long complicated function signatures), and, ultimately, subtle bugs. Scopes are your friends I've recently come across some code with a scope-related mistake. Here's the pattern: def f(...): text = ... ... for ...: match ...: case ...: ... case ...: ... case text: ... ... Despite the nesting, that case text (or, equivalently, case _ as text ) inadvertently overwrites the outer text . There's no shadowing at all. Scoping offers encapsulation at the function level. When I write for (int i = 0; i < 56; ++i) { ... } I know that the i introduced by the for loop is internal to the loop. I don't have to know whether there's another i , outside of the loop, that I might overwrite. In C++ I can even create naked scopes like this: { int i; ... } Even in C++, people don't rely on scopes as much as I'd like. I claim that: Scopes localize reasoning, i.e. they reduce the amount of surrounding code one needs to keep in mind to understand one line of code. Workaround Create internal functions! The problem with functions, though, is that they're not just scopes, but have deeper meaning. As a consequence, I use them only when I can factor out some sub-task. Using them only to localize some variables doesn't seem worth it. I usually define them right at the start of the outer function: def f(...): def g1(...): ... def g2(...): ... ... g1(...) ... g2(...) As I said, this is not real scoping, but it does help simplifying unsplittable functions."},
{"Title": "EarQuiz Frequencies: My New OSS Application for Technical Ear Training on Equalization", "Author": "u/Background-Brother90", "Content": "Hi! After more than a year of working on my new (and first) Open Source application on technical ear training, it's finally out! Its goal is to help audio people (professionals & hobbyists), musicians, etc. develop and master the ability to recognize frequency bands by ear. Perfect for students and educators in music and sound industry. It works with Python 3.9, 3.10 on Windows and macOS. I have built the binaries with PyInstaller, which work/have been tested with Windows 10, 11 (x64) and macOS 11 or higher (Intel processors only). I used PyQt6 for GUI, Pedalboard (from Spotify) and numpy libraries for audio processing. See app description here GitHub Repository Download Page I will be grateful for your feedback!"},
{"Title": "TIL: `yield` inside a `try` followed by `finally` has some interesting behaviour.", "Author": "u/alexmojaki", "Content": "Quiz: what does the following program output? Why? def foo(x): try: yield print('hi') finally: print(x) def bar(x): f = foo(x) next(f) return f a = bar(1) bar(2) I was reading through the original PEP that introduced generators and yield way back in Python 2.2 just for fun: https://peps.python.org/pep-0255/ And came across this: Restriction: A yield statement is not allowed in the try clause of a try/finally construct. The difficulty is that there‚Äôs no guarantee the generator will ever be resumed, hence no guarantee that the finally block will ever get executed; that‚Äôs too much a violation of finally‚Äôs purpose to bear. So naturally I tried this out. In hindsight it feels obvious that this quote is outdated, because nowadays try/yield/finally is extremely common in functions decorated with @contextlib.contextmanager . But it's still interesting to see how the problem above was handled so that finally can continue to 'guarantee' that it gets executed even if the generator isn't resumed. I don't think I'd have been able to answer the quiz above correctly. The answer, in case you haven't checked for yourself, is that (in both 2.7 and 3.12 and presumably everything in between) the output is: 2 1 I haven't verified, but I think the reason this happens is: a = bar(1) runs. next(f) advances the generator foo(1) up to the yield . Without next(f) (or with yield before try ), the foo frame wouldn't pause inside the try block and the rest wouldn't happen. return f assigns foo(1) to a . Python still has hope that the generator will be resumed because maybe something will advance a again, so nothing happens. bar(2) runs. The process in step 1 happens again. After return f with bar(2) being assigned to nothing, the generator foo(2) has no references. Before garbage collection can completely deallocate it from memory, Python sees that it was paused inside a try with a finally . It makes sure that finally clause runs, skipping print('hi') but running print(x) with x being 2. The entire program ends. Python accepts that nothing will resume a (i.e. foo(1) ) and so print(x) with x being 1 happens, just as in step 4."},
{"Title": "Simple-TOML-Configurator - A Python library for management of configuration settings in TOML format", "Author": "u/gilbn", "Content": "Wanted to share a project I've been working on. https://github.com/GilbN/Simple-TOML-Configurator It's a library for managing configuration values in your python app. I needed to change config values on the fly in my Flask app, so I created this where I could use my api to update configuration values that my backend uses. Some features: TOML File Storage: Configuration settings are stored in TOML files, a popular human-readable format. This enables developers to review, modify, and track configuration changes easily. Attribute-Based Access: Accessing configuration values is straightforward, thanks to the attribute-based approach. Settings can be accessed and updated as attributes, making it convenient for both reading and modifying values. Environment Variable Support: Configuration values are automatically set as environment variables, making it easier to use the configuration values in your application. Environment variable are set as uppercase. e.g. APP_HOST and APP_PORT or PROJECT_APP_HOST and PROJECT_APP_PORT if env_prefix is set to \"project\". This also works for nested values. ex: TABLE_KEY_LEVEL1_KEY_LEVEL2_KEY . This works for any level of nesting. Environment variables set before the configuration is loaded will not be overwritten, but instead will overwrite the existing config value. Default Values: Define default values for various configuration sections and keys. The library automatically incorporates new values and manages the removal of outdated ones. Usage examples: https://gilbn.github.io/Simple-TOML-Configurator/latest/usage-examples/ Example using Flask: https://gilbn.github.io/Simple-TOML-Configurator/latest/flask-simple-example/ Here is a quick example: import os from simple_toml_configurator import Configuration # Define default configuration values default_config = { \"app\": { \"ip\": \"0.0.0.0\", \"host\": \"\", \"port\": 5000, \"upload_folder\": \"uploads\", }, \"mysql\": { \"user\": \"root\", \"password\": \"root\", \"databases\": { \"prod\": \"db1\", \"dev\": \"db2\", }, } } # Set environment variables os.environ[\"PROJECT_APP_UPLOAD_FOLDER\"] = \"overridden_uploads\" # Initialize the Simple TOML Configurator settings = Configuration(config_path=\"config\", defaults=default_config, config_file_name=\"app_config\", env_prefix=\"project\") # Creates an `app_config.toml` file in the `config` folder at the current working directory. # Access and update configuration values print(settings.app.ip)  # Output: '0.0.0.0' settings.app.ip = \"1.2.3.4\" settings.update() print(settings.app_ip)  # Output: '1.2.3.4' # Access nested configuration values print(settings.mysql.databases.prod)  # Output: 'db1' settings.mysql.databases.prod = 'new_value' settings.update() print(settings.mysql.databases.prod)  # Output: 'new_value' # Access and update configuration values print(settings.app_ip)  # Output: '1.2.3.4' settings.update_config({\"app_ip\": \"1.1.1.1\"}) print(settings.app_ip)  # Output: '1.1.1.1' # Access all settings as a dictionary all_settings = settings.get_settings() print(all_settings) # Output: {'app_ip': '1.1.1.1', 'app_host': '', 'app_port': 5000, 'app_upload_folder': 'overridden_uploads', 'mysql_user': 'root', 'mysql_password': 'root', 'mysql_databases': {'prod': 'new_value', 'dev': 'db2'}} # Modify values directly in the config dictionary settings.config[\"mysql\"][\"databases\"][\"prod\"] = \"db3\" settings.update() print(settings.mysql_databases[\"prod\"])  # Output: 'db3' # Access environment variables print(os.environ[\"PROJECT_MYSQL_DATABASES_PROD\"])  # Output: 'db3' print(os.environ[\"PROJECT_APP_UPLOAD_FOLDER\"])  # Output: 'overridden_uploads' app_config.toml contents [app] ip = \"1.1.1.1\" host = \"\" port = 5000 upload_folder = \"overridden_uploads\" [mysql] user = \"root\" password = \"root\" [mysql.databases] prod = \"db3\" dev = \"db2\""},
{"Title": "This poem encapsulates code, poetry, and cryptography.", "Author": "u/Slice_According", "Content": "The Cryptic Symphony: Poems and Keys Encrypted in Python (AES-256): def whisper_behind_veils(message): \"\"\"A message cloaked in secrecy, yearning to be heard.\"\"\" print(f\"Encoding: {message.encode('base64')}\") # Symbolic representation of encryption (replace with actual code if possible) print(\"Weaving whispers into —Ü–∏—Ñ—Ä–æ–≤—ã–µ –∫—Ä—É–∂–µ–≤–∞...\") print(\"Message encoded, locked with a –∫–ª—é—á of 256 bits.\") return \"‚Ä¶.\" secret_message = whisper_behind_veils(\"Alan Turing once said, 'The important thing is not to stop questioning.'\") print(secret_message) Encrypted in Ruby (RSA): def heartsong_in_ciphertext(soul_whisper): \"\"\"A yearning for connection, veiled in the language of math.\"\"\" song = soul_whisper.gsub(/[bcdfghjklmnpqrstvwxyz]/, \"*\") # Symbolic representation of encryption (replace with actual code if possible) print(\"Encrypting with the power of prime numbers...\") print(\"Heart's song transformed, a cryptic melody...\") return song hidden_verse = heartsong_in_ciphertext(\"I carry your heart with me (I carry it in my heart). - Pablo Neruda\") print(hidden_verse) The Decoder's Key: A hidden melody whispers through the code, Two poems veiled, their secrets to be bestowed. Key #1: Python's Cipher Within the whispers, —Ü–∏—Ñ—Ä—ã hold the clue, A number's strength, 256, rings true. Search the code, where shadows gently creep, For a —Ü–∏—Ñ—Ä–æ–≤–æ–µ —Å–ª–æ–≤–æ, slumbering deep. Key #2: Ruby's Enigma A heartsong transformed, its essence concealed, By symbols arcane, emotions revealed. Prime numbers dance, a cryptic ballet, Their whispers unfold, if you know the way. Seek patterns hidden, where stars align, Letters replaced, a message to find. Unveiling the Secrets: With keys in hand, the journey unfolds, Through Python's whispers and Ruby's untold. Python's Revelation: Beneath the —Ü–∏—Ñ—Ä–æ–≤–æ–π cloak, a quote takes flight, A Turing whisper, bathed in digital light. \"The important thing is not to stop questioning,\" it cries, A call to explore, with curious eyes. Ruby's Confession: The hidden verse emerges, a love untold, Neruda's words, in symbols bold. \"I carry your heart with me,\" the message sighs, A connection revealed, beneath starry skies. The Symphony's End: Two languages sing, their voices entwined, In code and emotion, secrets enshrined. A journey of discovery, a message conveyed, The beauty of connection, forever displayed."},
{"Title": "Renity: Pure Python Binary Protocol Buffer", "Author": "u/DmitrievichLevin", "Content": "As of late I have a ton of new Open Source Components & Systems that I've worked on and here's one that finally made it through üòÖ The best part of it all is I got to create my own term Object-Binary-Mapper(OBM). Any ways, Renity is a pure Python Binary Protocol Buffer with an Interface similar to popular ODM(s) and I hope to extend it End-to-End eventually with the help of the community, check out the release on Pypi! We encourage all contributors to reach out for work reference's. We're here to help and are available for any inquiries regarding our contributors! Links: Renity @ Github Renity @ Pypi"},
{"Title": "[OC] Smassh your Keyboard, TUI Edition!", "Author": "u/otaku_____", "Content": "Smassh is a TUI based typing test application inspired by MonkeyType -- A very popular online web-based typing application Smassh tries to be a full fledged typing test experience but not missing out on looks and feel! There is a lot of room for improvements/additions and I am open to contributions and suggestions! Github: https://github.com/kraanzu/smassh Thank you! <3"},
{"Title": "Key sounds when you press a keyboard button", "Author": "u/jabbalaci", "Content": "I made a little fun project: https://github.com/jabbalaci/keysound . When you press a button on the keyboard, an audio file (a click sound) is played. I also included two sample files in mp3 that you can easily check out."},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Tranformer-based Denoising AutoEncoder for Sentence Transformers Unsupervised pre-training", "Author": "u/louisbrulenaudet", "Content": "A new PyPI package for training sentence embedding models in just 2 lines. The acquisition of sentence embeddings often necessitates a substantial volume of labeled data. However, in many cases and fields, labeled data is rarely accessible, and the procurement of such data is costly. In this project, we employ an unsupervised process grounded in pre-trained Transformers-based Sequential Denoising Auto-Encoder (TSDAE), introduced by the Ubiquitous Knowledge Processing Lab of Darmstadt, which can realize a performance level reaching 93.1% of in-domain supervised methodologies. The TSDAE schema comprises two components: an encoder and a decoder. Throughout the training process, TSDAE translates tainted sentences into uniform-sized vectors, necessitating the decoder to reconstruct the original sentences utilizing this sentence embedding. For good reconstruction quality, the semantics must be captured well in the sentence embeddings from the encoder. Subsequently, during inference, the encoder is solely utilized to form sentence embeddings. PyPI url : https://pypi.org/project/tsdae GitHub : https://github.com/louisbrulenaudet/tsdae Installation : pip3 install tsdae nltk datasets sentence-transformers torch Python code : from tsdae import TSDAE # Initialize an instance of TSDAE instance = TSDAE() # Load a dataset train_dataset = instance.load_dataset_from_hf( dataset=\"louisbrulenaudet/cgi\" ) # Train the model with the dataset model = instance.train( train_dataset=train_dataset, model_name=\"bert-base-multilingual-uncased\", column=\"output\", output_path=\"output/tsdae-lemon-mbert-base\" )"},
{"Title": "PySapScript: Automate SAP Processes with Python", "Author": "u/roztopasnik", "Content": "Hello everyone, I'd like to share with you my first library, designed to automate SAP tasks. It's meant to streamline the repetitive tasks, it can navigate, click, write, select and more. Example use would be a robot that launches SAP, open a transaction and fills in data in a table, fills other text fields, unchecks a checkbox, saves it and read the generated number from a status bar. Initially I have created this library for personal use -- employing it as robot that runs on a schedule. Now I want to open it up for others. To get the SAP paths -- elements, I use a Scripting tracker ( https://tracker.stschnell.de/ ) or I just record a Sap script and use elements from a file created. Please feel free to give it a try and don't hesitate to provide feedback after testing it. Github : https://github.com/kamildemocko/PySapScript Pypi : https://pypi.org/project/pysapscript/ pip install pysapscript Thank you!"},
{"Title": "\"En-Rich\" your Python Testing - Using Rich in pytest output", "Author": "u/frankwiles", "Content": "I had some thorny math to debug today for a client and realized I could probably use a Rich Table to better visualize my values. I wrote up a short blog post how using tabular output and some fun colors helped the debugging process both in real terms but also in terms of fun and motivation."},
{"Title": "MakrellPy, a programming language that is embedded in Python", "Author": "u/ZyF69", "Content": "I just released MakrellPy , a programming language that compiles to Python AST. It's part of the Makrell language family. Blurb from the home page: Makrell is a family of programming languages implemented in Python. It consists currently of these languages: MakrellPy , a general-purpose, functional programming language with two-way Python interoperability, metaprogramming support and simple syntax. MRON (Makrell Object Notation), a lightweight alternative to JSON. MRML (Makrell Markup Language), a lightweight alternative to XML and HTML. Makrell Base Format (MBF) , a simple data format that forms the basis for both MakrellPy, MRON and MRML. The project is in an early stage of development and is not yet ready for production use. GitHub page: https://github.com/hcholm/makrell-py Visual Studio Code extension with syntax highlighting and basic diagnostics using the Language Server Protocol: https://marketplace.visualstudio.com/items?itemName=hcholm.vscode-makrell Comments are welcome!"},
{"Title": "TIL that `for x in 1, 2, 3:` is valid", "Author": "u/alexmojaki", "Content": "I consider myself a Python expert. I don't know everything about it, but I've delved very, very deep . So I was surprised when reading this recent post by u/nicholashairs to discover that 3.11 introduced this syntax: for x in *a, *b: print(x) And I was even more surprised that just for x in a, b without the * s was also valid and has been since at least 2.7. I know that 'commas make the tuple', e.g. x = 1, is the same as x = (1,) . I can't believe I missed this implication or that I don't remember ever seeing this. It is used in library code, I can see it when I search for it, but I don't know if I've ever come across it without noticing. Anyone else feel this way?"},
{"Title": "rexi: A Terminal UI for Regex Testing Built in Python", "Author": "u/R3zn1kk", "Content": "Hey everyone, I'm excited to share a project I've been working on called rexi . It's a CLI tool designed to make regex testing more interactive and accessible directly from your terminal. Built with Python and utilizing the textual library, rexi offers a sleek terminal UI where you can effortlessly test your regex patterns. Key Features: Interactive terminal UI for an enhanced user experience. Supports match and finditer modes for regex evaluation. Real-time feedback on regex patterns with marked outputs. Why rexi? I created rexi to streamline the process of testing regex patterns. It's perfect for those who prefer working within the terminal or need a quick way to validate and learn regex through immediate feedback. Getting Started: Getting started with rexi is simple. After installation, just pipe your input text into rexi and start testing your regex patterns in an interactive UI. Here's a quick peek at how you can use it: echo \"your sample text\" | rexi Check it out and let me know what you think! I'm open to any suggestions, bug reports, or contributions to make rexi even better. The repo: https://github.com/royreznik/rexi"},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "SSH Telegram Bot", "Author": "u/SnooCookies1145", "Content": "Hello everyone! A couple of months ago, I shared details about my open-source project, which received a warm reception from the wonderful Reddit community, including you. Thanks to your recommendations, I've significantly improved and updated the project. Today, I got a very interesting idea. Since I regularly work with remote servers, I thought, just out of curiosity, what if I create a Telegram wrapper that allows executing commands on a server remotely? And you know what - that's exactly what I did. I implemented functionality that enables sending basic commands through a Telegram bot, which are then directly executed on the server where the bot is located. The project is implemented in a super raw and basic form, in the Proof Of Concept format and nothing more. Ideally, I would like to get feedback from other developers on how interesting this project could be. Also, I'm curious to learn how to execute remote commands without using subprocess, within a loop, to preserve, let's say, the \"progress.\" For example, if I execute 'cd ..', write 'ls' and want to see the current directory. How to solve this is not entirely clear at the moment. Nevertheless, I hope you can appreciate my idea and try out the prototype I've assembled: https://github.com/dinosaurtirex/telegram_ssh_bot"},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Surprising behavior of xml.etree", "Author": "u/danmickla", "Content": "If this is \"help\" and belongs in LearnPython, forgive me.  I think I know a fair bit about Python, but this behavior of a standard library module is puzzling to me. Test code: import xml.etree.ElementTree as ET XMLSTR=\"\"\" <r> root text1 <c1>c1 text</c1> root text 2 </r>\"\"\" print(f'XML:{XMLSTR}') root=ET.fromstring(XMLSTR) print(f'{root.text=} {root.tail=}') c1=root.find('c1') print(f'{c1.text=} {c1.tail=}') Results: XML: <r> root text1 <c1>c1 text</c1> root text 2 </r> root.text='\\n   root text1\\n   ' root.tail=None c1.text='c1 text' c1.tail='\\n   root text 2\\n' It's surprising to me that the 'root text 2' string is apparently not associated with the root node.  Does this surprise anyone else?  Am I expecting the wrong thing from xml.etree?"},
{"Title": "RD DICOM exportation", "Author": "u/One_Speech_5909", "Content": "Hi you all! Im working on a RD DICOM file, and I cannot exportexport a data, in DICOM format wich contains data likewise maximun dose slide, dose value, slide dimensiom, etc. My idea is to explort a single frame  and perform gamma 2D comparison. Some idea ?"},
{"Title": "Scheduled (cron) tasks in one line for Quart, with Quart-Tasks", "Author": "u/stetio", "Content": "Hello, I've recently released Quart-Tasks which is a Quart extension that provides scheduled background tasks, from quart import Quart from quart_tasks import QuartTasks app = Quart(__name__) tasks = QuartTasks(app) @tasks.cron(\"*/5 * * * *\")  # every 5 minutes async def infrequent_task(): ...  # Do something @tasks.cron( seconds=\"*1/0\",  # every 10 seconds minutes=\"*\", hours=\"*\", day_of_month=\"*\", month=\"*\", day_of_week=\"*\", ) async def frequent_task(): ...  # Do something @tasks.periodic(timedelta(seconds=10)) async def regular_task(): ...  # Do Something I've also recorded a tutorial showing how to use it here . Questions, thoughts, and feedback welcome!"},
{"Title": "I made a QR Code with Logo Generator using CTk", "Author": "u/Specialist-Arachnid6", "Content": "This app has a simple purpose, to create QR codes with logos/images inside them. It is then integrated into a UI made with CustomTkinter. You can customise the color of the QR code, image, and background color. GitHub: https://github.com/rohankishore/QrGen"},
{"Title": "Factory and Repository Pattern with SQLAlchemy and Pydantic", "Author": "u/LTMullineux", "Content": "Hey! I recently wanted to start sharing my knowledge of Python, and the struggles/joy I have with it. Check out this article showing how to implement the Factory and Repository patterns in Python with SQLAlchemy and Pydantic. It's a bit advanced, drawing from my freelancing experience and having to build the same applications over and over again, but I'm sure there's something for everyone. I hope your coding life easier! üé•üíª"},
{"Title": "Dev: The Ultimate AI-Powered Coding Assistant for Python Developers", "Author": "u/Quirky-Low-7500", "Content": "I created a toolkit for python developers I named it Dev, Dev is a Python package that integrates seamlessly with your code editor and provides you with a range of functionalities, such as: - AI-powered Debugging: Stuck on a coding bug? Dev can analyze your code, identify the error, and suggest corrections. - Automated README Generation: Forget manually writing READMEs! Dev can automatically generate a comprehensive README file for your project. - Creative Code Generation: Need a jumpstart on your code? Dev can generate code snippets based on your instructions, offering you creative and effective solutions. - In-depth Code Review: Unsure about your code's quality? Dev provides a detailed code review, highlighting potential improvements, optimizations, and areas for consideration. - Seamless Git Integration: Dev simplifies project management by automating Git commands. You can easily create and upload repositories, rename existing ones, and push your code with just a few function calls. - And much more! Dev is your comprehensive coding companion for python developers looking for a productivity boost or a beginner seeking guidance, Dev has something to offer. Embrace the power of AI and watch your coding workflow reach new heights of efficiency and effectiveness. To get started with Dev, visit the GitHub repository and follow the installation instructions: https://github.com/GitCoder052023/Dev.git"},
{"Title": "PyDev Debugger and sys.monitoring (PEP 669) -- i.e.: really fast debugging for Python 3.12!", "Author": "u/fabioz", "Content": "I've just released the newer version of pydevd (available in PyDev: https://www.pydev.org/ and LiClipse: https://www.liclipse.com/ ) which now makes use of the improvements from PEP 669. See: https://pydev.blogspot.com/2024/02/pydev-debugger-and-sysmonitoring-pep.html for more details."},
{"Title": "Python is weakly scoped", "Author": "u/Kiuhnm", "Content": "In the same way global variables pollute the global environment, local variables without any scoping pollute the local environment. To split or not to split That's less of a problem because functions are usually short, but \"usually\" is not \"always\". Splitting longer functions at all costs is a mistake. For instance, I have a longish function which generates a textual description of a certain object. The function has the following structure: def get_text(self, ...) -> str: text = TextBuilder() text.add_block(...) ... text.add_span(...) ... text.add_block(...) ... text.newline() ... return str(text) Splitting such a function wouldn't help because there are no particular abstractions to factor out. The rule  according to which a function should've maximum N LOCs is superficial. A more sensible rule is that a function should only do one thing. Often times you need less than 20 lines, but occasionally 50 or more. A function that does half or less of one thing is worse than a function that does more than one thing. That's my opinion, at least. As an auditor, I've noticed that splitting long functions at all costs may result in bad abstractions, messy interfaces (i.e. long complicated function signatures), and, ultimately, subtle bugs. Scopes are your friends I've recently come across some code with a scope-related mistake. Here's the pattern: def f(...): text = ... ... for ...: match ...: case ...: ... case ...: ... case text: ... ... Despite the nesting, that case text (or, equivalently, case _ as text ) inadvertently overwrites the outer text . There's no shadowing at all. Scoping offers encapsulation at the function level. When I write for (int i = 0; i < 56; ++i) { ... } I know that the i introduced by the for loop is internal to the loop. I don't have to know whether there's another i , outside of the loop, that I might overwrite. In C++ I can even create naked scopes like this: { int i; ... } Even in C++, people don't rely on scopes as much as I'd like. I claim that: Scopes localize reasoning, i.e. they reduce the amount of surrounding code one needs to keep in mind to understand one line of code. Workaround Create internal functions! The problem with functions, though, is that they're not just scopes, but have deeper meaning. As a consequence, I use them only when I can factor out some sub-task. Using them only to localize some variables doesn't seem worth it. I usually define them right at the start of the outer function: def f(...): def g1(...): ... def g2(...): ... ... g1(...) ... g2(...) As I said, this is not real scoping, but it does help simplifying unsplittable functions."},
{"Title": "EarQuiz Frequencies: My New OSS Application for Technical Ear Training on Equalization", "Author": "u/Background-Brother90", "Content": "Hi! After more than a year of working on my new (and first) Open Source application on technical ear training, it's finally out! Its goal is to help audio people (professionals & hobbyists), musicians, etc. develop and master the ability to recognize frequency bands by ear. Perfect for students and educators in music and sound industry. It works with Python 3.9, 3.10 on Windows and macOS. I have built the binaries with PyInstaller, which work/have been tested with Windows 10, 11 (x64) and macOS 11 or higher (Intel processors only). I used PyQt6 for GUI, Pedalboard (from Spotify) and numpy libraries for audio processing. See app description here GitHub Repository Download Page I will be grateful for your feedback!"},
{"Title": "TIL: `yield` inside a `try` followed by `finally` has some interesting behaviour.", "Author": "u/alexmojaki", "Content": "Quiz: what does the following program output? Why? def foo(x): try: yield print('hi') finally: print(x) def bar(x): f = foo(x) next(f) return f a = bar(1) bar(2) I was reading through the original PEP that introduced generators and yield way back in Python 2.2 just for fun: https://peps.python.org/pep-0255/ And came across this: Restriction: A yield statement is not allowed in the try clause of a try/finally construct. The difficulty is that there‚Äôs no guarantee the generator will ever be resumed, hence no guarantee that the finally block will ever get executed; that‚Äôs too much a violation of finally‚Äôs purpose to bear. So naturally I tried this out. In hindsight it feels obvious that this quote is outdated, because nowadays try/yield/finally is extremely common in functions decorated with @contextlib.contextmanager . But it's still interesting to see how the problem above was handled so that finally can continue to 'guarantee' that it gets executed even if the generator isn't resumed. I don't think I'd have been able to answer the quiz above correctly. The answer, in case you haven't checked for yourself, is that (in both 2.7 and 3.12 and presumably everything in between) the output is: 2 1 I haven't verified, but I think the reason this happens is: a = bar(1) runs. next(f) advances the generator foo(1) up to the yield . Without next(f) (or with yield before try ), the foo frame wouldn't pause inside the try block and the rest wouldn't happen. return f assigns foo(1) to a . Python still has hope that the generator will be resumed because maybe something will advance a again, so nothing happens. bar(2) runs. The process in step 1 happens again. After return f with bar(2) being assigned to nothing, the generator foo(2) has no references. Before garbage collection can completely deallocate it from memory, Python sees that it was paused inside a try with a finally . It makes sure that finally clause runs, skipping print('hi') but running print(x) with x being 2. The entire program ends. Python accepts that nothing will resume a (i.e. foo(1) ) and so print(x) with x being 1 happens, just as in step 4."},
{"Title": "Simple-TOML-Configurator - A Python library for management of configuration settings in TOML format", "Author": "u/gilbn", "Content": "Wanted to share a project I've been working on. https://github.com/GilbN/Simple-TOML-Configurator It's a library for managing configuration values in your python app. I needed to change config values on the fly in my Flask app, so I created this where I could use my api to update configuration values that my backend uses. Some features: TOML File Storage: Configuration settings are stored in TOML files, a popular human-readable format. This enables developers to review, modify, and track configuration changes easily. Attribute-Based Access: Accessing configuration values is straightforward, thanks to the attribute-based approach. Settings can be accessed and updated as attributes, making it convenient for both reading and modifying values. Environment Variable Support: Configuration values are automatically set as environment variables, making it easier to use the configuration values in your application. Environment variable are set as uppercase. e.g. APP_HOST and APP_PORT or PROJECT_APP_HOST and PROJECT_APP_PORT if env_prefix is set to \"project\". This also works for nested values. ex: TABLE_KEY_LEVEL1_KEY_LEVEL2_KEY . This works for any level of nesting. Environment variables set before the configuration is loaded will not be overwritten, but instead will overwrite the existing config value. Default Values: Define default values for various configuration sections and keys. The library automatically incorporates new values and manages the removal of outdated ones. Usage examples: https://gilbn.github.io/Simple-TOML-Configurator/latest/usage-examples/ Example using Flask: https://gilbn.github.io/Simple-TOML-Configurator/latest/flask-simple-example/ Here is a quick example: import os from simple_toml_configurator import Configuration # Define default configuration values default_config = { \"app\": { \"ip\": \"0.0.0.0\", \"host\": \"\", \"port\": 5000, \"upload_folder\": \"uploads\", }, \"mysql\": { \"user\": \"root\", \"password\": \"root\", \"databases\": { \"prod\": \"db1\", \"dev\": \"db2\", }, } } # Set environment variables os.environ[\"PROJECT_APP_UPLOAD_FOLDER\"] = \"overridden_uploads\" # Initialize the Simple TOML Configurator settings = Configuration(config_path=\"config\", defaults=default_config, config_file_name=\"app_config\", env_prefix=\"project\") # Creates an `app_config.toml` file in the `config` folder at the current working directory. # Access and update configuration values print(settings.app.ip)  # Output: '0.0.0.0' settings.app.ip = \"1.2.3.4\" settings.update() print(settings.app_ip)  # Output: '1.2.3.4' # Access nested configuration values print(settings.mysql.databases.prod)  # Output: 'db1' settings.mysql.databases.prod = 'new_value' settings.update() print(settings.mysql.databases.prod)  # Output: 'new_value' # Access and update configuration values print(settings.app_ip)  # Output: '1.2.3.4' settings.update_config({\"app_ip\": \"1.1.1.1\"}) print(settings.app_ip)  # Output: '1.1.1.1' # Access all settings as a dictionary all_settings = settings.get_settings() print(all_settings) # Output: {'app_ip': '1.1.1.1', 'app_host': '', 'app_port': 5000, 'app_upload_folder': 'overridden_uploads', 'mysql_user': 'root', 'mysql_password': 'root', 'mysql_databases': {'prod': 'new_value', 'dev': 'db2'}} # Modify values directly in the config dictionary settings.config[\"mysql\"][\"databases\"][\"prod\"] = \"db3\" settings.update() print(settings.mysql_databases[\"prod\"])  # Output: 'db3' # Access environment variables print(os.environ[\"PROJECT_MYSQL_DATABASES_PROD\"])  # Output: 'db3' print(os.environ[\"PROJECT_APP_UPLOAD_FOLDER\"])  # Output: 'overridden_uploads' app_config.toml contents [app] ip = \"1.1.1.1\" host = \"\" port = 5000 upload_folder = \"overridden_uploads\" [mysql] user = \"root\" password = \"root\" [mysql.databases] prod = \"db3\" dev = \"db2\""},
{"Title": "This poem encapsulates code, poetry, and cryptography.", "Author": "u/Slice_According", "Content": "The Cryptic Symphony: Poems and Keys Encrypted in Python (AES-256): def whisper_behind_veils(message): \"\"\"A message cloaked in secrecy, yearning to be heard.\"\"\" print(f\"Encoding: {message.encode('base64')}\") # Symbolic representation of encryption (replace with actual code if possible) print(\"Weaving whispers into —Ü–∏—Ñ—Ä–æ–≤—ã–µ –∫—Ä—É–∂–µ–≤–∞...\") print(\"Message encoded, locked with a –∫–ª—é—á of 256 bits.\") return \"‚Ä¶.\" secret_message = whisper_behind_veils(\"Alan Turing once said, 'The important thing is not to stop questioning.'\") print(secret_message) Encrypted in Ruby (RSA): def heartsong_in_ciphertext(soul_whisper): \"\"\"A yearning for connection, veiled in the language of math.\"\"\" song = soul_whisper.gsub(/[bcdfghjklmnpqrstvwxyz]/, \"*\") # Symbolic representation of encryption (replace with actual code if possible) print(\"Encrypting with the power of prime numbers...\") print(\"Heart's song transformed, a cryptic melody...\") return song hidden_verse = heartsong_in_ciphertext(\"I carry your heart with me (I carry it in my heart). - Pablo Neruda\") print(hidden_verse) The Decoder's Key: A hidden melody whispers through the code, Two poems veiled, their secrets to be bestowed. Key #1: Python's Cipher Within the whispers, —Ü–∏—Ñ—Ä—ã hold the clue, A number's strength, 256, rings true. Search the code, where shadows gently creep, For a —Ü–∏—Ñ—Ä–æ–≤–æ–µ —Å–ª–æ–≤–æ, slumbering deep. Key #2: Ruby's Enigma A heartsong transformed, its essence concealed, By symbols arcane, emotions revealed. Prime numbers dance, a cryptic ballet, Their whispers unfold, if you know the way. Seek patterns hidden, where stars align, Letters replaced, a message to find. Unveiling the Secrets: With keys in hand, the journey unfolds, Through Python's whispers and Ruby's untold. Python's Revelation: Beneath the —Ü–∏—Ñ—Ä–æ–≤–æ–π cloak, a quote takes flight, A Turing whisper, bathed in digital light. \"The important thing is not to stop questioning,\" it cries, A call to explore, with curious eyes. Ruby's Confession: The hidden verse emerges, a love untold, Neruda's words, in symbols bold. \"I carry your heart with me,\" the message sighs, A connection revealed, beneath starry skies. The Symphony's End: Two languages sing, their voices entwined, In code and emotion, secrets enshrined. A journey of discovery, a message conveyed, The beauty of connection, forever displayed."},
{"Title": "Renity: Pure Python Binary Protocol Buffer", "Author": "u/DmitrievichLevin", "Content": "As of late I have a ton of new Open Source Components & Systems that I've worked on and here's one that finally made it through üòÖ The best part of it all is I got to create my own term Object-Binary-Mapper(OBM). Any ways, Renity is a pure Python Binary Protocol Buffer with an Interface similar to popular ODM(s) and I hope to extend it End-to-End eventually with the help of the community, check out the release on Pypi! We encourage all contributors to reach out for work reference's. We're here to help and are available for any inquiries regarding our contributors! Links: Renity @ Github Renity @ Pypi"},
{"Title": "[OC] Smassh your Keyboard, TUI Edition!", "Author": "u/otaku_____", "Content": "Smassh is a TUI based typing test application inspired by MonkeyType -- A very popular online web-based typing application Smassh tries to be a full fledged typing test experience but not missing out on looks and feel! There is a lot of room for improvements/additions and I am open to contributions and suggestions! Github: https://github.com/kraanzu/smassh Thank you! <3"},
{"Title": "Key sounds when you press a keyboard button", "Author": "u/jabbalaci", "Content": "I made a little fun project: https://github.com/jabbalaci/keysound . When you press a button on the keyboard, an audio file (a click sound) is played. I also included two sample files in mp3 that you can easily check out."},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Tranformer-based Denoising AutoEncoder for Sentence Transformers Unsupervised pre-training", "Author": "u/louisbrulenaudet", "Content": "A new PyPI package for training sentence embedding models in just 2 lines. The acquisition of sentence embeddings often necessitates a substantial volume of labeled data. However, in many cases and fields, labeled data is rarely accessible, and the procurement of such data is costly. In this project, we employ an unsupervised process grounded in pre-trained Transformers-based Sequential Denoising Auto-Encoder (TSDAE), introduced by the Ubiquitous Knowledge Processing Lab of Darmstadt, which can realize a performance level reaching 93.1% of in-domain supervised methodologies. The TSDAE schema comprises two components: an encoder and a decoder. Throughout the training process, TSDAE translates tainted sentences into uniform-sized vectors, necessitating the decoder to reconstruct the original sentences utilizing this sentence embedding. For good reconstruction quality, the semantics must be captured well in the sentence embeddings from the encoder. Subsequently, during inference, the encoder is solely utilized to form sentence embeddings. PyPI url : https://pypi.org/project/tsdae GitHub : https://github.com/louisbrulenaudet/tsdae Installation : pip3 install tsdae nltk datasets sentence-transformers torch Python code : from tsdae import TSDAE # Initialize an instance of TSDAE instance = TSDAE() # Load a dataset train_dataset = instance.load_dataset_from_hf( dataset=\"louisbrulenaudet/cgi\" ) # Train the model with the dataset model = instance.train( train_dataset=train_dataset, model_name=\"bert-base-multilingual-uncased\", column=\"output\", output_path=\"output/tsdae-lemon-mbert-base\" )"},
{"Title": "PySapScript: Automate SAP Processes with Python", "Author": "u/roztopasnik", "Content": "Hello everyone, I'd like to share with you my first library, designed to automate SAP tasks. It's meant to streamline the repetitive tasks, it can navigate, click, write, select and more. Example use would be a robot that launches SAP, open a transaction and fills in data in a table, fills other text fields, unchecks a checkbox, saves it and read the generated number from a status bar. Initially I have created this library for personal use -- employing it as robot that runs on a schedule. Now I want to open it up for others. To get the SAP paths -- elements, I use a Scripting tracker ( https://tracker.stschnell.de/ ) or I just record a Sap script and use elements from a file created. Please feel free to give it a try and don't hesitate to provide feedback after testing it. Github : https://github.com/kamildemocko/PySapScript Pypi : https://pypi.org/project/pysapscript/ pip install pysapscript Thank you!"},
{"Title": "\"En-Rich\" your Python Testing - Using Rich in pytest output", "Author": "u/frankwiles", "Content": "I had some thorny math to debug today for a client and realized I could probably use a Rich Table to better visualize my values. I wrote up a short blog post how using tabular output and some fun colors helped the debugging process both in real terms but also in terms of fun and motivation."},
{"Title": "MakrellPy, a programming language that is embedded in Python", "Author": "u/ZyF69", "Content": "I just released MakrellPy , a programming language that compiles to Python AST. It's part of the Makrell language family. Blurb from the home page: Makrell is a family of programming languages implemented in Python. It consists currently of these languages: MakrellPy , a general-purpose, functional programming language with two-way Python interoperability, metaprogramming support and simple syntax. MRON (Makrell Object Notation), a lightweight alternative to JSON. MRML (Makrell Markup Language), a lightweight alternative to XML and HTML. Makrell Base Format (MBF) , a simple data format that forms the basis for both MakrellPy, MRON and MRML. The project is in an early stage of development and is not yet ready for production use. GitHub page: https://github.com/hcholm/makrell-py Visual Studio Code extension with syntax highlighting and basic diagnostics using the Language Server Protocol: https://marketplace.visualstudio.com/items?itemName=hcholm.vscode-makrell Comments are welcome!"},
{"Title": "TIL that `for x in 1, 2, 3:` is valid", "Author": "u/alexmojaki", "Content": "I consider myself a Python expert. I don't know everything about it, but I've delved very, very deep . So I was surprised when reading this recent post by u/nicholashairs to discover that 3.11 introduced this syntax: for x in *a, *b: print(x) And I was even more surprised that just for x in a, b without the * s was also valid and has been since at least 2.7. I know that 'commas make the tuple', e.g. x = 1, is the same as x = (1,) . I can't believe I missed this implication or that I don't remember ever seeing this. It is used in library code, I can see it when I search for it, but I don't know if I've ever come across it without noticing. Anyone else feel this way?"},
{"Title": "rexi: A Terminal UI for Regex Testing Built in Python", "Author": "u/R3zn1kk", "Content": "Hey everyone, I'm excited to share a project I've been working on called rexi . It's a CLI tool designed to make regex testing more interactive and accessible directly from your terminal. Built with Python and utilizing the textual library, rexi offers a sleek terminal UI where you can effortlessly test your regex patterns. Key Features: Interactive terminal UI for an enhanced user experience. Supports match and finditer modes for regex evaluation. Real-time feedback on regex patterns with marked outputs. Why rexi? I created rexi to streamline the process of testing regex patterns. It's perfect for those who prefer working within the terminal or need a quick way to validate and learn regex through immediate feedback. Getting Started: Getting started with rexi is simple. After installation, just pipe your input text into rexi and start testing your regex patterns in an interactive UI. Here's a quick peek at how you can use it: echo \"your sample text\" | rexi Check it out and let me know what you think! I'm open to any suggestions, bug reports, or contributions to make rexi even better. The repo: https://github.com/royreznik/rexi"},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "SSH Telegram Bot", "Author": "u/SnooCookies1145", "Content": "Hello everyone! A couple of months ago, I shared details about my open-source project, which received a warm reception from the wonderful Reddit community, including you. Thanks to your recommendations, I've significantly improved and updated the project. Today, I got a very interesting idea. Since I regularly work with remote servers, I thought, just out of curiosity, what if I create a Telegram wrapper that allows executing commands on a server remotely? And you know what - that's exactly what I did. I implemented functionality that enables sending basic commands through a Telegram bot, which are then directly executed on the server where the bot is located. The project is implemented in a super raw and basic form, in the Proof Of Concept format and nothing more. Ideally, I would like to get feedback from other developers on how interesting this project could be. Also, I'm curious to learn how to execute remote commands without using subprocess, within a loop, to preserve, let's say, the \"progress.\" For example, if I execute 'cd ..', write 'ls' and want to see the current directory. How to solve this is not entirely clear at the moment. Nevertheless, I hope you can appreciate my idea and try out the prototype I've assembled: https://github.com/dinosaurtirex/telegram_ssh_bot"},
{"Title": "Complex Discord Bot with Economy Game", "Author": "u/DanielJ-mal", "Content": "Hi I made a discord bot called Echo that has 1500+ lines of code and 50+ commands along with a very complex economy game and a large amount of commands for fun and moderation. Its still in early deveopemnt but I am constantly updating the project. Any suggestions will be appreciated! Source Code: https://github.com/DanielJones02/Echo"},
{"Title": "How many lines are there in your code?", "Author": "u/Mooncake911-_-", "Content": "I saw such a question appear here. I was bored today, so I implemented a program that recognizes the number of lines in (.py) files of your GitHub and draws a couple of simple but informative graphs. You will only need to insert your Token from GitHub Developer Settings. https://github.com/Mooncake911/GitHub-Statistics"},
{"Title": "Dragonfly - Lightweight CPython Debugger", "Author": "u/P403n1x87", "Content": "If you rely on pdb for debugging your Python code and find it slow, then consider giving Dragonfly a try https://github.com/P403n1x87/dragonfly . Dragonfly is a lightweight CPython debugger designed with speed in mind. Contrary to more traditional debuggers, like pdb, Dragonfly does not rely heavily on tracing, allowing the target application to run at full speed in most cases. Occasionally, tracing might be required, so that the slowdown would be similar to that of pdb in the worst case."},
{"Title": "Summary of major Python changes between versions", "Author": "u/nicholashairs", "Content": "TLDR: I've thrown together a one \"page\" reference documenting the major changes to between Python versions . I've spent a fair amount of time recently upgrading some old code-bases and would have found it helpful to have a one page summary of changes between versions. I couldn't find one via Google so decided to create one for myself. It might be useful for others so sharing it ‚ò∫Ô∏è"},
{"Title": "My first ever article: \"Finding the fastest Python JSON library on all Python versions (8 compared)\"", "Author": "u/catnotfoundnear", "Content": "Just published my first ever article! \"Finding the fastest Python JSON library on all Python versions (8 compared)\". Read now, for free, without ads, on my blog: https://catnotfoundnear.github.io/finding-the-fastest-python-json-library-on-all-python-versions-8-compared.html I will truly appreciate your suggestions or recommendations! Thank you! EDIT: I have just uploaded my second tutorial on extending your SaaS Django website to reach a global audience and boost sales with a free method. I'd love to hear your thoughts: https://catnotfoundnear.github.io/the-guide-to-making-your-django-saas-business-worldwide-for-free.html - Anna Willis (Catnotfoundnear)"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "RasterioXYZ: A Python package for tiling georeferenced raster images according to the XYZ tiles standard", "Author": "u/duncanmartyn", "Content": "Hi all! First post here (engagement of any kind, actually) so wanted to present my first Python package. RasterioXYZ is used to tile georeferenced raster images (in the form of Rasterio DatasetReader objects) according to the XYZ tiles standard. Designed to be straightforward, flexible, and memory efficient, RasterioXYZ was created in response to the comparatively manual process used in creating such tiles in a previous role. Right now, it's much slower than equivalent functionality in QGIS, a popular open-source desktop GIS software. However, as noted in the benchmarks and roadmap sections of the README, there's plenty for me to look at to address this, as well as features to add. GitHub: https://github.com/duncanmartyn/rasterioxyz Any feedback is welcome, cheers!"},
{"Title": "Hosting a Flask web app on pythonanywhere : extremely simple but some points to note..", "Author": "u/whoeverdidnt", "Content": "The coding experience you gain is really all the coding dead ends you heavily invested in. So I developed ,on my Linux laptop ,a Flask app with Nginx,Gunicorn,Celery, and Redis Set-ups.I looked at a few hosting providers and the whole process seemed overwhelming ,at least to me. I then stumbled upon pythonanywhere (PA) and hosting became a breeze : I ditched Nginx ,Gunicorn and Celery as pythonanywhere provides it own flavors in the area. Set up a remote ,free, redis db on Redis Labs as PA has no native support for redis. I got an automated SSL Certificate as a courtesy of PA. I simply declared one of my script as a scheduled task. Subscribed to a yearly plan for a very affordable price (down from their standard 5 eur a month). This was all there was to get me hosted using a domain name from namecheap. I then found out that PA using natively a multi workers environment (yes..I am a bit slow...) my web app behaved very differently from the one worker environment provided by my local machine. To get it to work ,as it should ,I had to resort to two extra series of changes in my app design: Replace some heavily used global variables with flask session variables. Add some javascript to a few html selects to disable/enable them for some milliseconds I thought I would share the experience and may be get some comments on this multiple/single worker context ,of which I have a very limited knowledge."},
{"Title": "I made this app in customtkinter", "Author": "u/ekkivox", "Content": "Hey, i made this app called ezres using customtkinter, it's an app that allows you to quickly change your Fortnite in-game resolution and fps lock, also allows to enable exclusive fullscreen for any game. ezres github"},
{"Title": "The Python on Microcontrollers (and Raspberry Pi) Newsletter, a weekly news and project resource, subscribe for free", "Author": "u/HP7933", "Content": "With the Python on Microcontrollers newsletter , you get all the latest information on Python running on hardware in one place! MicroPython, CircuitPython and Python on single Board Computers like Raspberry Pi & many more. The Python on Microcontrollers newsletter is the place for the latest news. It arrives Monday morning with all the week‚Äôs happenings. No advertising, no spam, easy to unsubscribe. 10,817 subscribers - the largest Python on hardware newsletter out there. Catch all the weekly news on Python for Microcontrollers with adafruitdaily.com . This ad-free, spam-free weekly email is filled with CircuitPython, MicroPython, and Python information that you may have missed, all in one place! Ensure you catch the weekly Python on Hardware roundup‚Äì you can cancel anytime ‚Äì try our spam-free newsletter today! https://www.adafruitdaily.com/"},
{"Title": "PyPhotoCollage", "Author": "u/twilsonco", "Content": "Here's a tool I put together that makes nice photo collages. Hope you enjoy! v1.2 released, with image rotation option github link: https://github.com/twilsonco/PyPhotoCollage It's available as vanilla Python (only uses pillow), and comes in a Pythonista (iOS app) version with a nice UI, and as a native iOS/macOS Siri Shortcut ."},
{"Title": "GPTAuthor: open-source CLI tool for writing long form, multi-chapter stories given a story outline", "Author": "u/infocruncher", "Content": "My wife wrote a children's book (8-12yo) a while back, and I took on the challenge of writing a sequel using ChatGPT. It was a fun project and I built a handy tool to automate the book writing given a story outline. It makes iterative API calls so the token count doesn't blow out. Source in case it's of interest: https://github.com/dylanhogg/gptauthor How It Works Human written story description : you describe your story outline, writing style, characters etc in a story prompt ( an example ) Run GPTAuthor : choosing model, temperature, and number of chapters to write. AI generated synopsis : Given the story prompt, GPTAuthor uses ChatGPT to automatically turn this into a synopsis. Human review of synopsis : You are given a chance to review the synopsis and (optionally) make changes. AI generated story : Each chapter is iteratively written by ChatGPT given the common synopsis and previous chapter. The full story is written as Markdown and HTML folder for your reading pleasure. See an Example of a short story about the OpenAI Leadership Crisis last year \"In the heart of San Francisco, nestled among the city's tech giants and start-up hopefuls, stood the OpenAI office. A hive of activity, it buzzed with the sound of keyboards clacking, coffee machines hissing, and the occasional drone of a philosophical debate about whether AI could develop a taste for late-night taco runs. It was a typical day, or so everyone thought.\" continued... You can even Write your own story easily in Google Colab Writing a few chapters with gpt-3.5-turbo only costs 1 or 2 cents to run with your OpenAI API key. [edit: or you can currently specify a localhost API endpoint, with the ability to set a custom URL coming soon, as mentioned in the comments] The results for the sequel were mixed - the best part was using it for coming up with ideas and creating various puzzles. I hope someone has fun with this :)"},
{"Title": "AI with Automation - create Python Microservices in Minutes", "Author": "u/ValBayArea", "Content": "You can use AI to create a database schema, and API Logic Server to create App and API micro services in minutes. API Logic Server is a an open source project, consisting of a CLI (creates Python projects from databases), and a set of runtime libraries (Flask, SQLAlchemy, etc). 1. AI: Use ChatGPT to create schema You can enter natural language to ChatGPT: Create a sqlite database for customers, orders, items and product Hints: use autonum keys, allow nulls, Decimal types, foreign keys, no check constraints. Create a few rows of only customer and product data. Enforce the Check Credit requirement: Customer.Balance <= CreditLimit Customer.Balance = Sum(Order.AmountTotal where date shipped is null) Order.AmountTotal = Sum(Items.Amount) Items.Amount = Quantity \\* UnitPrice Store the Items.UnitPrice as a copy from Product.UnitPrice ChatGPT will provide SQL DDL.  Paste this into your sql tool to create a new database.  In this example, we created a sqlite database called sample_ai.sqlite . 2. Use API Logic Server: create working software - 1 command API Logic Server creates Python projects from databases: ApiLogicServer create --project_name=sample_ai \\ --db_url=sqlite:///sample_ai.sqlite This command reads the database schema, and creates an executable Python project.  You can open it in your IDE and run it.  The app provides: App Automation: a multi-page, multi-table admin app API Automation: a JSON:API - crud for each table, with filtering, sorting, optimistic locking and pagination.  Plus swagger. Within minutes , front end developers can use the API - no more blocking on server development.  Business users can use the App as a basis for agile collaboration and iteration. 3. Customize the project with your IDE Microservices must implement their semantics for security and integrity.  API Logic Server includes a rule engine that enables you to declare these. Logic Automation means that you can declare spreadsheet-like rules using Python.  Such logic maintains database integrity with multi-table derivations and constraints.  Rules are 40X more concise than traditional code.  The following 5 rules would require 200 lines of Python: \"\"\" Declarative multi-table derivations and constraints, extensible with Python. Use code completion (Rule.) to declare rules here Check Credit - Logic Design (note: translates directly into rules) 1. Customer.Balance <= CreditLimit 2. Customer.Balance = Sum(Order.AmountTotal where unshipped) 3. Order.AmountTotal = Sum(Items.Amount) 4. Items.Amount = Quantity * UnitPrice 5. Items.UnitPrice = copy from Product \"\"\" Rule.constraint(validate=models.Customer, as_condition=lambda row: row.Balance <= row.CreditLimit, error_msg=\"balance ({round(row.Balance, 2)}) exceeds credit ({round(row.CreditLimit, 2)})\") Rule.sum(derive=models.Customer.Balance,     # adjusts... as_sum_of=models.Order.AmountTotal,      # *not* a sql select sum... where=lambda row: row.ShipDate is None) Rule.sum(derive=models.Order.AmountTotal as_sum_of=models.Item.Amount) Rule.formula(derive=models.Item.Amount, as_expression=lambda row: row.UnitPrice * row.Quantity) Rule.copy(derive=models.Item.UnitPrice, from_parent=models.Product.UnitPrice) 4. Iterate: use Python and Standard Libraries Projects are designed for iteration.  You can change the database design, are rebuild the SQLAlchemy models while preserving customizations. You can add Python, .e.g. for Application Integration: def send_order_to_shipping(row: models.Order, old_row: models.Order, logic_row: LogicRow): \"\"\" #als: Send Kafka message formatted by OrderShipping RowDictMapper Format row per shipping requirements, and send Kafka message Args: row (models.Order): inserted Order old_row (models.Order): n/a logic_row (LogicRow): bundles curr/old row, with ins/upd/dlt logic \"\"\" if logic_row.is_inserted(): order_dict = OrderShipping().row_to_dict(row = row) json_order = jsonify({\"order\": order_dict}).data.decode('utf-8') if kafka_producer.producer:  # enabled in config/config.py? try: kafka_producer.producer.produce(value=json_order, topic=\"order_shipping\", key= str(row.Id)) logic_row.log(\"Kafka producer sent message\") except KafkaException as ke: logic_row.log(\"Kafka.produce msg {row.id} error: {ke}\") print(f'\\n\\nSend to Shipping:\\n{json_order}') Rule.after_flush_row_event(on_class=models.Order, calling=send_order_to_shipping)  # see above You can also extend your API to create new endpoints, using Flask. API Logic Server creates scripts to containerize your project, so you can deploy it to your local server or the cloud. You can see a screen shot summary of this project here , or develop it yourself using this tutorial ."},
{"Title": "Ten Python datetime pitfalls, and what libraries are (not) doing about it", "Author": "u/Dlatch", "Content": "Interesting article about datetime in Python: https://dev.arie.bovenberg.net/blog/python-datetime-pitfalls/ The library the author is working on looks really interesting too: https://github.com/ariebovenberg/whenever"},
{"Title": "Apprise ‚Äì A lightweight all-in-one notification solution now supports 100+ services!", "Author": "u/lead2gold", "Content": "I finally achieved a milestone of supporting more then 100+ services with Apprise and just wanted to share with with you all! It is very much a useful devops tool just due to the fact you can trigger notifications from successful builds, deploys, failures, via monitoring, etc. This is a cross post from r/selfhosted ; Mods, please feel free to delete this if it's not acceptable to also share here. What is Apprise? Apprise allows you to send a notification to almost all of the most popular notification services available to us today such as: Telegram, Discord, Slack, Amazon SNS, Gotify, etc. One notification library to rule them all. A common and intuitive notification syntax. Supports the handling of images and attachments (to the notification services that will accept them). It's incredibly lightweight. Amazing response times because all messages sent asynchronously. I still don't get it... ELI5 Apprise is effectively a self-host efficient messaging switchboard. You can automate notifications through: the Command Line Interface (for Admins) it's very easy to use Development Library (for Devs) which is already integrated with many platforms today such as ChangeDetection, Uptime Kuma ( and many others . a web service (you host) that can act as a sidecar. This solution allows you to keep your notification configuration in one place instead of across multiple servers (or within multiple programs). This one is for both Admins and Devs. What else does it do? Emoji Support (:rocket: -> üöÄ) built right into it! File Attachment Support (to the end points that support it) It supports inputs of MARKDOWN , HTML , and TEXT and can easily convert between these depending on the endpoint. For example: HTML provided input would be converted to TEXT before passing it along as a text message. However the same HTML content provided would not be converted if the endpoint accepted it as such (such as Telegram, or Email). It supports breaking large messages into smaller ones to fit the upstream service. Hence a text message (160 characters) or a Tweet (280 characters) would be constructed for you if the notification you sent was larger. It supports configuration files allowing you to securely hide your credentials and map them to simple tags (or identifiers) like family , devops , marketing , etc. There is no limit to the number of tag assignments. It supports a simple TEXT based configuration, as well as a more advanced and configurable YAML based one. Configuration can be hosted via the web (even self-hosted), or just regular (protected) configuration files. Supports \"tagging\" of the Notification Endpoints you wish to notify. Tagging allows you to mask your credentials and upstream services into single word assigned descriptions of them.  Tags can even be grouped together and signaled via their group name instead. Dynamic Module Loading: They load on demand only. Writing a new supported notification is as simple as adding a new file ( see here ) Developer CLI tool (it's like /usr/bin/mail on steroids) It's worth re-mentioning that it has a fully compatible API interface found here or on Dockerhub which has all of the same bells and whistles as defined above. This acts as a great side-car solution! Program Details Entirely a self-hosted solution. Written in Python 99.27% Test Coverage (oof... I'll get it back to 100% soon) BSD-2 License Over 450K downloads a month on PyPi ( source ) Over 2.8 million downloads from Docker Hub I would love to hear any feedback any of you have! Edit: Added link to Apprise :)"},
{"Title": "Article on Collaborative Role-Playing AI", "Author": "Unknown author", "Content": "Hello everyone, Recently, I've been exploring the idea of using multiple GPT models to simulate a role-playing environment where AI agents collaborate to tackle complex problems. In my research, I stumbled upon an this amazing python framework called Crew AI. I found it so intriguing that I decided to create a brief tutorial to share my insights and experiences. For anyone interested, you can find my tutorial here: Meet Your Digital Dream Team: Revolutionizing the Tech World with AI . I hope you find it as exciting and useful as I did! Would love to know your thoughts and ideas around the topic."},
{"Title": "PiSegment (Lightweight python software for semi-supervised image segmentation)", "Author": "u/_agitoz_", "Content": "Hello Lovely Pythonistas, I would like to share my project PiSegment ( https://github.com/aGIToz/PiSegment ), written in python. It allows to segment the regions of images with little bit of human annotation. Typical use cases are background extraction, organ segmentation in medical imaging, creating dataset for semantic segmentation, colorization etc."},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "De4py: A toolkit for python reverse engineering", "Author": "u/AhmedMinegames", "Content": "me and my friend created an open-source tool called De4py that i think would be useful to analyze python malware and monitor their behavior, it got a beautiful UI and some nice features. Deobfuscation: De4py support some popular obfuscators, like: Jawbreaker, BlankOBF, PlusOBF, Wodx, Hyperion, pyobfuscate.com obfuscator. Pycode Execution: Executing your own python code inside the process. Strings Dump: Dumping Strings in the python process and saving it as a file which can be pretty useful to extract data from memory such as webhooks. Behavior Monitoring: De4py can monitor python processes and see if they opened any files handles, opened a process, wrote/readed the memory of other processes and also monitoring if the process terminated other processes, in addition to sockets monitoring. File Analyzer: an analyzer that have many features like detecting if the python program is packed and tries to unpack it if it was using pyinstaller for example, it also got a feature that shows either all strings or suspicious strings (suspicious strings like: IPs, websites, \"discord\", \"leveldb\" and other suspicious strings in the file) and shows them in a nice output window. it also supports pulgins, and some other features that you can discover when viewing the repo. i hope it would be useful for anyone analyzing py malware."},
{"Title": "Iterable... but how many times?", "Author": "u/Kiuhnm", "Content": "I've just noticed that iterators are iterable in Python. I heavily rely on static types, and I thought I had a pretty good idea of what an Iterable object was, so I thought the following (deliberately silly) function was perfectly correct: def print_twice(numbers: Iterable[int]): print(', '.join(str(x) for x in numbers)) print(', '.join(str(x) for x in numbers)) Unfortunately, this is not the case. Indeed, while print_twice([1,2,3,4]) prints twice, print_twice(iter([1,2,3,4])) only prints once. Even worse, the code fails silently: no static nor runtime error! Am I the only one surprised by this? My mental model has always been: Iterable: produces iterators, so I can iterate it multiple times Iterator: I can only use it once Semantically, it doesn't make much sense for an iterator to be iterable. Indeed, iterating over an iterable object shouldn't alter it in any way, which means that the object must remain iterable (and produce the same elements) as long as no one alters it or some external resource the object depends on changes. Am I missing something? Why was it decided to make iterators iterable?"},
{"Title": "Tired of using TKinter", "Author": "Unknown author", "Content": "I sometimes use Tkinter to make a GUI for small projects. The thing is I forget everything everytime I take a break and im tired of coding every single component, is there a better way to create GUIs in python?"},
{"Title": "Tried my hands at Chip-8 Emulator and debugger in Python, using DearPyGui.", "Author": "u/M4K35N0S3N53", "Content": "Yet another Chip8 emulator with debug features, this time using DearPyGui. Yachipy üéÆ Welcome to YaChiPy, a Chip-8 Emulator with Debug Features! üïπÔ∏è Get ready to step back in time and experience the charm of retro gaming, now with powerful debug capabilities! üöÄ Dive into the world of Chip-8 and enjoy classic pixelated graphics. But wait, there's more! This emulator comes equipped with advanced debug features to enhance your gaming and programming experience. üïµÔ∏è‚Äç‚ôÇÔ∏è Pause the emulator at any moment to inspect the state of the virtual machine. Take control of time with the ability to speed up or slow down the CPU clock, allowing you to fine-tune your gaming experience. üîç Peek into the machine's heart! Displayed in real-time are the memory contents, register values, and timer values. Understand the inner workings of the Chip-8 machine as you play and program. ‚è≠Ô∏è Want to understand every step of the action? Execute instructions one tick at a time with our step-by-step mode. Uncover the secrets of Chip-8 programming and gaming at your own pace. üéÆ Load your favorite ROMs, customize settings, and embark on a journey filled with nostalgia and discovery. Whether you're a seasoned enthusiast or a curious newcomer, this emulator is your gateway to the golden age of gaming, now with the added thrill of detailed debugging. It is in pre-release state as of now, as I still have to add few unique features that I really want it to have that will help it stand-out a little among the sea of Chip-8 emulators. Link to Public Repo: YaChiPy"},
{"Title": "Feeder : Turn websites into RSS Feed", "Author": "u/moscow_berlin_paris", "Content": "I have a created an app to created RSS feeds for  websites that don't have them.  I have been using it to mostly keep track of jobs from various sites. The source code is present here : https://github.com/rk1165/feeder Suggestions and feedbacks are welcome. Thanks"},
{"Title": "I made an Expense Tracker using PyQt6 [UPDATE]", "Author": "u/Specialist-Arachnid6", "Content": "CashFlow is an Fiannce manager built using PyQt6 and Python. It offers the facility to store income and expense details (with Graphs), and also calculate investments, interests, etc. GitHub: https://github.com/rohankishore/CashFlow"},
{"Title": "I made a Windows Notepad Replacement Using PyQt6 [UPDATE]", "Author": "u/Specialist-Arachnid6", "Content": "ZenNotes is a Notepad replacement with TTS, Translations, Encryption and much more. GitHub: https://github.com/rohankishore/ZenNotes"},
{"Title": "Complex Discord Bot with Economy Game", "Author": "u/DanielJ-mal", "Content": "Hi I made a discord bot called Echo that has 1500+ lines of code and 50+ commands along with a very complex economy game and a large amount of commands for fun and moderation. Its still in early deveopemnt but I am constantly updating the project. Any suggestions will be appreciated! Source Code: https://github.com/DanielJones02/Echo"},
{"Title": "How many lines are there in your code?", "Author": "u/Mooncake911-_-", "Content": "I saw such a question appear here. I was bored today, so I implemented a program that recognizes the number of lines in (.py) files of your GitHub and draws a couple of simple but informative graphs. You will only need to insert your Token from GitHub Developer Settings. https://github.com/Mooncake911/GitHub-Statistics"},
{"Title": "Dragonfly - Lightweight CPython Debugger", "Author": "u/P403n1x87", "Content": "If you rely on pdb for debugging your Python code and find it slow, then consider giving Dragonfly a try https://github.com/P403n1x87/dragonfly . Dragonfly is a lightweight CPython debugger designed with speed in mind. Contrary to more traditional debuggers, like pdb, Dragonfly does not rely heavily on tracing, allowing the target application to run at full speed in most cases. Occasionally, tracing might be required, so that the slowdown would be similar to that of pdb in the worst case."},
{"Title": "Summary of major Python changes between versions", "Author": "u/nicholashairs", "Content": "TLDR: I've thrown together a one \"page\" reference documenting the major changes to between Python versions . I've spent a fair amount of time recently upgrading some old code-bases and would have found it helpful to have a one page summary of changes between versions. I couldn't find one via Google so decided to create one for myself. It might be useful for others so sharing it ‚ò∫Ô∏è"},
{"Title": "My first ever article: \"Finding the fastest Python JSON library on all Python versions (8 compared)\"", "Author": "u/catnotfoundnear", "Content": "Just published my first ever article! \"Finding the fastest Python JSON library on all Python versions (8 compared)\". Read now, for free, without ads, on my blog: https://catnotfoundnear.github.io/finding-the-fastest-python-json-library-on-all-python-versions-8-compared.html I will truly appreciate your suggestions or recommendations! Thank you! EDIT: I have just uploaded my second tutorial on extending your SaaS Django website to reach a global audience and boost sales with a free method. I'd love to hear your thoughts: https://catnotfoundnear.github.io/the-guide-to-making-your-django-saas-business-worldwide-for-free.html - Anna Willis (Catnotfoundnear)"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "RasterioXYZ: A Python package for tiling georeferenced raster images according to the XYZ tiles standard", "Author": "u/duncanmartyn", "Content": "Hi all! First post here (engagement of any kind, actually) so wanted to present my first Python package. RasterioXYZ is used to tile georeferenced raster images (in the form of Rasterio DatasetReader objects) according to the XYZ tiles standard. Designed to be straightforward, flexible, and memory efficient, RasterioXYZ was created in response to the comparatively manual process used in creating such tiles in a previous role. Right now, it's much slower than equivalent functionality in QGIS, a popular open-source desktop GIS software. However, as noted in the benchmarks and roadmap sections of the README, there's plenty for me to look at to address this, as well as features to add. GitHub: https://github.com/duncanmartyn/rasterioxyz Any feedback is welcome, cheers!"},
{"Title": "Hosting a Flask web app on pythonanywhere : extremely simple but some points to note..", "Author": "u/whoeverdidnt", "Content": "The coding experience you gain is really all the coding dead ends you heavily invested in. So I developed ,on my Linux laptop ,a Flask app with Nginx,Gunicorn,Celery, and Redis Set-ups.I looked at a few hosting providers and the whole process seemed overwhelming ,at least to me. I then stumbled upon pythonanywhere (PA) and hosting became a breeze : I ditched Nginx ,Gunicorn and Celery as pythonanywhere provides it own flavors in the area. Set up a remote ,free, redis db on Redis Labs as PA has no native support for redis. I got an automated SSL Certificate as a courtesy of PA. I simply declared one of my script as a scheduled task. Subscribed to a yearly plan for a very affordable price (down from their standard 5 eur a month). This was all there was to get me hosted using a domain name from namecheap. I then found out that PA using natively a multi workers environment (yes..I am a bit slow...) my web app behaved very differently from the one worker environment provided by my local machine. To get it to work ,as it should ,I had to resort to two extra series of changes in my app design: Replace some heavily used global variables with flask session variables. Add some javascript to a few html selects to disable/enable them for some milliseconds I thought I would share the experience and may be get some comments on this multiple/single worker context ,of which I have a very limited knowledge."},
{"Title": "I made this app in customtkinter", "Author": "u/ekkivox", "Content": "Hey, i made this app called ezres using customtkinter, it's an app that allows you to quickly change your Fortnite in-game resolution and fps lock, also allows to enable exclusive fullscreen for any game. ezres github"},
{"Title": "The Python on Microcontrollers (and Raspberry Pi) Newsletter, a weekly news and project resource, subscribe for free", "Author": "u/HP7933", "Content": "With the Python on Microcontrollers newsletter , you get all the latest information on Python running on hardware in one place! MicroPython, CircuitPython and Python on single Board Computers like Raspberry Pi & many more. The Python on Microcontrollers newsletter is the place for the latest news. It arrives Monday morning with all the week‚Äôs happenings. No advertising, no spam, easy to unsubscribe. 10,817 subscribers - the largest Python on hardware newsletter out there. Catch all the weekly news on Python for Microcontrollers with adafruitdaily.com . This ad-free, spam-free weekly email is filled with CircuitPython, MicroPython, and Python information that you may have missed, all in one place! Ensure you catch the weekly Python on Hardware roundup‚Äì you can cancel anytime ‚Äì try our spam-free newsletter today! https://www.adafruitdaily.com/"},
{"Title": "PyPhotoCollage", "Author": "u/twilsonco", "Content": "Here's a tool I put together that makes nice photo collages. Hope you enjoy! v1.2 released, with image rotation option github link: https://github.com/twilsonco/PyPhotoCollage It's available as vanilla Python (only uses pillow), and comes in a Pythonista (iOS app) version with a nice UI, and as a native iOS/macOS Siri Shortcut ."},
{"Title": "GPTAuthor: open-source CLI tool for writing long form, multi-chapter stories given a story outline", "Author": "u/infocruncher", "Content": "My wife wrote a children's book (8-12yo) a while back, and I took on the challenge of writing a sequel using ChatGPT. It was a fun project and I built a handy tool to automate the book writing given a story outline. It makes iterative API calls so the token count doesn't blow out. Source in case it's of interest: https://github.com/dylanhogg/gptauthor How It Works Human written story description : you describe your story outline, writing style, characters etc in a story prompt ( an example ) Run GPTAuthor : choosing model, temperature, and number of chapters to write. AI generated synopsis : Given the story prompt, GPTAuthor uses ChatGPT to automatically turn this into a synopsis. Human review of synopsis : You are given a chance to review the synopsis and (optionally) make changes. AI generated story : Each chapter is iteratively written by ChatGPT given the common synopsis and previous chapter. The full story is written as Markdown and HTML folder for your reading pleasure. See an Example of a short story about the OpenAI Leadership Crisis last year \"In the heart of San Francisco, nestled among the city's tech giants and start-up hopefuls, stood the OpenAI office. A hive of activity, it buzzed with the sound of keyboards clacking, coffee machines hissing, and the occasional drone of a philosophical debate about whether AI could develop a taste for late-night taco runs. It was a typical day, or so everyone thought.\" continued... You can even Write your own story easily in Google Colab Writing a few chapters with gpt-3.5-turbo only costs 1 or 2 cents to run with your OpenAI API key. [edit: or you can currently specify a localhost API endpoint, with the ability to set a custom URL coming soon, as mentioned in the comments] The results for the sequel were mixed - the best part was using it for coming up with ideas and creating various puzzles. I hope someone has fun with this :)"},
{"Title": "AI with Automation - create Python Microservices in Minutes", "Author": "u/ValBayArea", "Content": "You can use AI to create a database schema, and API Logic Server to create App and API micro services in minutes. API Logic Server is a an open source project, consisting of a CLI (creates Python projects from databases), and a set of runtime libraries (Flask, SQLAlchemy, etc). 1. AI: Use ChatGPT to create schema You can enter natural language to ChatGPT: Create a sqlite database for customers, orders, items and product Hints: use autonum keys, allow nulls, Decimal types, foreign keys, no check constraints. Create a few rows of only customer and product data. Enforce the Check Credit requirement: Customer.Balance <= CreditLimit Customer.Balance = Sum(Order.AmountTotal where date shipped is null) Order.AmountTotal = Sum(Items.Amount) Items.Amount = Quantity \\* UnitPrice Store the Items.UnitPrice as a copy from Product.UnitPrice ChatGPT will provide SQL DDL.  Paste this into your sql tool to create a new database.  In this example, we created a sqlite database called sample_ai.sqlite . 2. Use API Logic Server: create working software - 1 command API Logic Server creates Python projects from databases: ApiLogicServer create --project_name=sample_ai \\ --db_url=sqlite:///sample_ai.sqlite This command reads the database schema, and creates an executable Python project.  You can open it in your IDE and run it.  The app provides: App Automation: a multi-page, multi-table admin app API Automation: a JSON:API - crud for each table, with filtering, sorting, optimistic locking and pagination.  Plus swagger. Within minutes , front end developers can use the API - no more blocking on server development.  Business users can use the App as a basis for agile collaboration and iteration. 3. Customize the project with your IDE Microservices must implement their semantics for security and integrity.  API Logic Server includes a rule engine that enables you to declare these. Logic Automation means that you can declare spreadsheet-like rules using Python.  Such logic maintains database integrity with multi-table derivations and constraints.  Rules are 40X more concise than traditional code.  The following 5 rules would require 200 lines of Python: \"\"\" Declarative multi-table derivations and constraints, extensible with Python. Use code completion (Rule.) to declare rules here Check Credit - Logic Design (note: translates directly into rules) 1. Customer.Balance <= CreditLimit 2. Customer.Balance = Sum(Order.AmountTotal where unshipped) 3. Order.AmountTotal = Sum(Items.Amount) 4. Items.Amount = Quantity * UnitPrice 5. Items.UnitPrice = copy from Product \"\"\" Rule.constraint(validate=models.Customer, as_condition=lambda row: row.Balance <= row.CreditLimit, error_msg=\"balance ({round(row.Balance, 2)}) exceeds credit ({round(row.CreditLimit, 2)})\") Rule.sum(derive=models.Customer.Balance,     # adjusts... as_sum_of=models.Order.AmountTotal,      # *not* a sql select sum... where=lambda row: row.ShipDate is None) Rule.sum(derive=models.Order.AmountTotal as_sum_of=models.Item.Amount) Rule.formula(derive=models.Item.Amount, as_expression=lambda row: row.UnitPrice * row.Quantity) Rule.copy(derive=models.Item.UnitPrice, from_parent=models.Product.UnitPrice) 4. Iterate: use Python and Standard Libraries Projects are designed for iteration.  You can change the database design, are rebuild the SQLAlchemy models while preserving customizations. You can add Python, .e.g. for Application Integration: def send_order_to_shipping(row: models.Order, old_row: models.Order, logic_row: LogicRow): \"\"\" #als: Send Kafka message formatted by OrderShipping RowDictMapper Format row per shipping requirements, and send Kafka message Args: row (models.Order): inserted Order old_row (models.Order): n/a logic_row (LogicRow): bundles curr/old row, with ins/upd/dlt logic \"\"\" if logic_row.is_inserted(): order_dict = OrderShipping().row_to_dict(row = row) json_order = jsonify({\"order\": order_dict}).data.decode('utf-8') if kafka_producer.producer:  # enabled in config/config.py? try: kafka_producer.producer.produce(value=json_order, topic=\"order_shipping\", key= str(row.Id)) logic_row.log(\"Kafka producer sent message\") except KafkaException as ke: logic_row.log(\"Kafka.produce msg {row.id} error: {ke}\") print(f'\\n\\nSend to Shipping:\\n{json_order}') Rule.after_flush_row_event(on_class=models.Order, calling=send_order_to_shipping)  # see above You can also extend your API to create new endpoints, using Flask. API Logic Server creates scripts to containerize your project, so you can deploy it to your local server or the cloud. You can see a screen shot summary of this project here , or develop it yourself using this tutorial ."},
{"Title": "Ten Python datetime pitfalls, and what libraries are (not) doing about it", "Author": "u/Dlatch", "Content": "Interesting article about datetime in Python: https://dev.arie.bovenberg.net/blog/python-datetime-pitfalls/ The library the author is working on looks really interesting too: https://github.com/ariebovenberg/whenever"},
{"Title": "Apprise ‚Äì A lightweight all-in-one notification solution now supports 100+ services!", "Author": "u/lead2gold", "Content": "I finally achieved a milestone of supporting more then 100+ services with Apprise and just wanted to share with with you all! It is very much a useful devops tool just due to the fact you can trigger notifications from successful builds, deploys, failures, via monitoring, etc. This is a cross post from r/selfhosted ; Mods, please feel free to delete this if it's not acceptable to also share here. What is Apprise? Apprise allows you to send a notification to almost all of the most popular notification services available to us today such as: Telegram, Discord, Slack, Amazon SNS, Gotify, etc. One notification library to rule them all. A common and intuitive notification syntax. Supports the handling of images and attachments (to the notification services that will accept them). It's incredibly lightweight. Amazing response times because all messages sent asynchronously. I still don't get it... ELI5 Apprise is effectively a self-host efficient messaging switchboard. You can automate notifications through: the Command Line Interface (for Admins) it's very easy to use Development Library (for Devs) which is already integrated with many platforms today such as ChangeDetection, Uptime Kuma ( and many others . a web service (you host) that can act as a sidecar. This solution allows you to keep your notification configuration in one place instead of across multiple servers (or within multiple programs). This one is for both Admins and Devs. What else does it do? Emoji Support (:rocket: -> üöÄ) built right into it! File Attachment Support (to the end points that support it) It supports inputs of MARKDOWN , HTML , and TEXT and can easily convert between these depending on the endpoint. For example: HTML provided input would be converted to TEXT before passing it along as a text message. However the same HTML content provided would not be converted if the endpoint accepted it as such (such as Telegram, or Email). It supports breaking large messages into smaller ones to fit the upstream service. Hence a text message (160 characters) or a Tweet (280 characters) would be constructed for you if the notification you sent was larger. It supports configuration files allowing you to securely hide your credentials and map them to simple tags (or identifiers) like family , devops , marketing , etc. There is no limit to the number of tag assignments. It supports a simple TEXT based configuration, as well as a more advanced and configurable YAML based one. Configuration can be hosted via the web (even self-hosted), or just regular (protected) configuration files. Supports \"tagging\" of the Notification Endpoints you wish to notify. Tagging allows you to mask your credentials and upstream services into single word assigned descriptions of them.  Tags can even be grouped together and signaled via their group name instead. Dynamic Module Loading: They load on demand only. Writing a new supported notification is as simple as adding a new file ( see here ) Developer CLI tool (it's like /usr/bin/mail on steroids) It's worth re-mentioning that it has a fully compatible API interface found here or on Dockerhub which has all of the same bells and whistles as defined above. This acts as a great side-car solution! Program Details Entirely a self-hosted solution. Written in Python 99.27% Test Coverage (oof... I'll get it back to 100% soon) BSD-2 License Over 450K downloads a month on PyPi ( source ) Over 2.8 million downloads from Docker Hub I would love to hear any feedback any of you have! Edit: Added link to Apprise :)"},
{"Title": "Article on Collaborative Role-Playing AI", "Author": "Unknown author", "Content": "Hello everyone, Recently, I've been exploring the idea of using multiple GPT models to simulate a role-playing environment where AI agents collaborate to tackle complex problems. In my research, I stumbled upon an this amazing python framework called Crew AI. I found it so intriguing that I decided to create a brief tutorial to share my insights and experiences. For anyone interested, you can find my tutorial here: Meet Your Digital Dream Team: Revolutionizing the Tech World with AI . I hope you find it as exciting and useful as I did! Would love to know your thoughts and ideas around the topic."},
{"Title": "PiSegment (Lightweight python software for semi-supervised image segmentation)", "Author": "u/_agitoz_", "Content": "Hello Lovely Pythonistas, I would like to share my project PiSegment ( https://github.com/aGIToz/PiSegment ), written in python. It allows to segment the regions of images with little bit of human annotation. Typical use cases are background extraction, organ segmentation in medical imaging, creating dataset for semantic segmentation, colorization etc."},
{"Title": "Friday Daily Thread: r/Python Meta and Free-Talk Fridays", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "De4py: A toolkit for python reverse engineering", "Author": "u/AhmedMinegames", "Content": "me and my friend created an open-source tool called De4py that i think would be useful to analyze python malware and monitor their behavior, it got a beautiful UI and some nice features. Deobfuscation: De4py support some popular obfuscators, like: Jawbreaker, BlankOBF, PlusOBF, Wodx, Hyperion, pyobfuscate.com obfuscator. Pycode Execution: Executing your own python code inside the process. Strings Dump: Dumping Strings in the python process and saving it as a file which can be pretty useful to extract data from memory such as webhooks. Behavior Monitoring: De4py can monitor python processes and see if they opened any files handles, opened a process, wrote/readed the memory of other processes and also monitoring if the process terminated other processes, in addition to sockets monitoring. File Analyzer: an analyzer that have many features like detecting if the python program is packed and tries to unpack it if it was using pyinstaller for example, it also got a feature that shows either all strings or suspicious strings (suspicious strings like: IPs, websites, \"discord\", \"leveldb\" and other suspicious strings in the file) and shows them in a nice output window. it also supports pulgins, and some other features that you can discover when viewing the repo. i hope it would be useful for anyone analyzing py malware."},
{"Title": "Iterable... but how many times?", "Author": "u/Kiuhnm", "Content": "I've just noticed that iterators are iterable in Python. I heavily rely on static types, and I thought I had a pretty good idea of what an Iterable object was, so I thought the following (deliberately silly) function was perfectly correct: def print_twice(numbers: Iterable[int]): print(', '.join(str(x) for x in numbers)) print(', '.join(str(x) for x in numbers)) Unfortunately, this is not the case. Indeed, while print_twice([1,2,3,4]) prints twice, print_twice(iter([1,2,3,4])) only prints once. Even worse, the code fails silently: no static nor runtime error! Am I the only one surprised by this? My mental model has always been: Iterable: produces iterators, so I can iterate it multiple times Iterator: I can only use it once Semantically, it doesn't make much sense for an iterator to be iterable. Indeed, iterating over an iterable object shouldn't alter it in any way, which means that the object must remain iterable (and produce the same elements) as long as no one alters it or some external resource the object depends on changes. Am I missing something? Why was it decided to make iterators iterable?"},
{"Title": "Tired of using TKinter", "Author": "Unknown author", "Content": "I sometimes use Tkinter to make a GUI for small projects. The thing is I forget everything everytime I take a break and im tired of coding every single component, is there a better way to create GUIs in python?"},
{"Title": "Tried my hands at Chip-8 Emulator and debugger in Python, using DearPyGui.", "Author": "u/M4K35N0S3N53", "Content": "Yet another Chip8 emulator with debug features, this time using DearPyGui. Yachipy üéÆ Welcome to YaChiPy, a Chip-8 Emulator with Debug Features! üïπÔ∏è Get ready to step back in time and experience the charm of retro gaming, now with powerful debug capabilities! üöÄ Dive into the world of Chip-8 and enjoy classic pixelated graphics. But wait, there's more! This emulator comes equipped with advanced debug features to enhance your gaming and programming experience. üïµÔ∏è‚Äç‚ôÇÔ∏è Pause the emulator at any moment to inspect the state of the virtual machine. Take control of time with the ability to speed up or slow down the CPU clock, allowing you to fine-tune your gaming experience. üîç Peek into the machine's heart! Displayed in real-time are the memory contents, register values, and timer values. Understand the inner workings of the Chip-8 machine as you play and program. ‚è≠Ô∏è Want to understand every step of the action? Execute instructions one tick at a time with our step-by-step mode. Uncover the secrets of Chip-8 programming and gaming at your own pace. üéÆ Load your favorite ROMs, customize settings, and embark on a journey filled with nostalgia and discovery. Whether you're a seasoned enthusiast or a curious newcomer, this emulator is your gateway to the golden age of gaming, now with the added thrill of detailed debugging. It is in pre-release state as of now, as I still have to add few unique features that I really want it to have that will help it stand-out a little among the sea of Chip-8 emulators. Link to Public Repo: YaChiPy"},
{"Title": "Feeder : Turn websites into RSS Feed", "Author": "u/moscow_berlin_paris", "Content": "I have a created an app to created RSS feeds for  websites that don't have them.  I have been using it to mostly keep track of jobs from various sites. The source code is present here : https://github.com/rk1165/feeder Suggestions and feedbacks are welcome. Thanks"},
{"Title": "I made an Expense Tracker using PyQt6 [UPDATE]", "Author": "u/Specialist-Arachnid6", "Content": "CashFlow is an Fiannce manager built using PyQt6 and Python. It offers the facility to store income and expense details (with Graphs), and also calculate investments, interests, etc. GitHub: https://github.com/rohankishore/CashFlow"},
{"Title": "I made a Windows Notepad Replacement Using PyQt6 [UPDATE]", "Author": "u/Specialist-Arachnid6", "Content": "ZenNotes is a Notepad replacement with TTS, Translations, Encryption and much more. GitHub: https://github.com/rohankishore/ZenNotes"},
{"Title": "nowpy -automatic venv set up and package installs", "Author": "u/Maximum_Cucumber_988", "Content": "nowpy nowpy is a CLI-tool I made that enables you to run any arbitrary Python script instantly, on the basis that I run code far more often than I write it. It combines python , virtualenv , and pip to launch a dedicated isolated environment, automatically figure out which packages are required, and then run your Python file - all with just one command. nowpy finds packages by performing a recursive lookup for a pyproject.toml OR a requirements.txt , and cross-checks with any import statements inside the Python file. Installation Use the package manager pip to install nowpy . pip install nowpy It might be the last time you have to pip install anything! Usage Here's an example of what happens if you run nowpy on a Python file that imports requsts . First run: nowpy WorldTimeApi.py Creating Virtualenv... Collecting requests ... Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests Successfully installed certifi-2023.11.17 charset-normalizer-3.3.2 idna-3.6 requests-2.31.0 urllib3-2.2.0 Running Script... Current Time in Europe/London Date: 2024-01-30T22:08:52.854140+00:00 Timezone: Europe/London All future runs: nowpy WorldTimeApi.py Running Script... Current Time in Europe/London Date: 2024-01-30T22:08:52.854140+00:00 Timezone: Europe/London nowpy creates a unique virtual environment for every directory you run nowpy from. It also removes unused ones automatically. But if you ever want to reset a particular one that you're using, just use the --reset option: nowpy --reset I've not tested it on anything except my Mac - let me know if there are any issues! Source Code on GitHub: https://github.com/WillDenby/nowpy PyPI Page: https://pypi.org/project/nowpy/"},
{"Title": "I made a Buckshot Roulette game in python", "Author": "u/Unlucky_Basil7767", "Content": "I recreated the game Buckshot roulette in python and wanted to share Here is the source code - https://github.com/ATharvaCoder492/Buckshot_Roulette If you have any suggestions to make this better plz comment"},
{"Title": "Streamlit vs Taipy: Which Tool is Better for Creating Web Apps from Python Scripts?", "Author": "u/bitdoze", "Content": "If you are looking for a tool to turn your Python scripts into web apps, you might have heard of Streamlit and Taipy. They are both popular and promising tools that allow you to create interactive and beautiful web apps from your Python code, without requiring any web development skills. I have created an article here that can help you see the differences: https://www.bitdoze.com/streamlit-vs-taipy/"},
{"Title": "K Lars Lohn uses math and Python to triangulate the nighttime booms disturbing the sleep of his community.", "Author": "u/AlSweigart", "Content": "\"Finding the Air Cannon\" https://www.twobraids.com/2024/01/air-cannon.html It took three people stationed at remote locations miles apart using a synchronized clock on our cell phones. We each waited over the same ten minute period, noting the exact time for each of the five cannon shots that we heard. ... I wrote a program in Python (see source code below) that could iterate all the points in the image in the search area where we suspected the air cannon sat. ... I called the owner of the farm (headquartered in Monmouth) and asked if they used an air cannon on their property near the Corvallis airport. They confirmed that they do. I asked if they run it at night, they said they do not. ... However, in an amazing coincidence, the air cannons stopped that very evening of our phone conversation."},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "pyside6 Qt for Python tutorial for Qt C++ developers", "Author": "u/nmariusp", "Content": "https://www.youtube.com/watch?v=fVzFq909z_k Video tutorial about the Python bindings for the Qt framework named pyside6 or \"Qt for Python\"."},
{"Title": "Profiling your Numba code with Profila", "Author": "u/itamarst", "Content": "If you‚Äôre writing numeric Python code, Numba can be a great way to speed up your program. By compiling a subset of Python to machine code, Numba lets you write for loops and other constructs that would be too slow in normal Python. In other words, it‚Äôs similar to Cython, C, or Rust, in that it lets you write compiled extensions for Python. Numba code isn‚Äôt always as fast as it could be, however. This is where profiling is useful: it can find at least some of the bottlenecks in your code. Learn more about Profila , a new profiler for Numba that I've just released on GitHub ."},
{"Title": "Hacking the Hacker News API for Real-Time Analysis with Grafana, Proton, and Bytewax", "Author": "u/math-bw", "Content": "I wrote this tutorial on how to consume the Hacker News API as a stream and process it with open source tools Bytewax, Proton and Grafana. It is kind of fun to see who all the armchair experts are; weighing in on every story posted to hacker news :P. I thought this might be useful if you are working with real-time data and need to build some live dashboards. Let me know what you think! https://github.com/bytewax/hacking-hacker-news"},
{"Title": "Arrest v0.1.8 is released with OpenAPI integration!", "Author": "u/s_basu", "Content": "Hey everyone, I am back again with some good news. After many requests for OpenAPI support, it has been finally added in 0.1.8 You can install the optional dependencies with pip install arrest[openapi] This provides a CLI interface where you can provide the URL to the OpenAPI specification file (json or yaml). The URL can be an HTTP URL or a filepath. Arrest will generate the necessary boilerplates such as the schema definition Pydantic models (courtesy of datamodel-code-generator ), Arrest resources and services (using Jinja templates). Arrest is a small package to define the structure of RESTful APIs from an external source with type annotations, making it easier to interface with, and call different routes of an external web service from inside a Python application, such as a third-party API, or an internal microservice, while also providing data validation for the request, response, exception handling and retries. For more information please check out the docs and the repository Please note that the OpenAPI integration is an active work in progress. There are many features missing that I will be adding gradually such as extracting the header and query parameters from the specification, or support for composite types like list[Model] or dict[str, Model] will be added soon. I would greatly appreciate your feedback and any areas of improvement. Thank you to everyone here for supporting this project and providing the feedback that made it work! edit: added a bit of context for the package"},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Ollama Python Library - Chat Method, System Message, Streaming & Options", "Author": "u/developer_1010", "Content": "Below you will find the link to my tutorial on using the Ollama Python library . Here I show with the help of examples how to use models like Mistral to create messages and go into more detail about the chat method , the system message and the streaming option . I also show how to use the streaming option. Finally, how to correct code with options like the temperature parameter . Link: Ollama Python Library - Tutorial with Examples"},
{"Title": "Conda now uses lib-mamba by default!", "Author": "u/jmeppley", "Content": "Many of y'all may have seen this already, but I just stumbled upon it this weekend: https://docs.conda.io/projects/conda/en/latest/release-notes.html#id2 About two years ago (which I also missed), conda added mamba libraries as an experimental option. Apparently it went well enough to make it official. Version 10.23 (from October 2023) now uses mamba libraries  by default. I ran some quick and dirty tests, and the new conda is was only about 10% to 50% slower than mamba on some big environments. This is a huge improvement from before where it could an order of magnitude slower. Edit: added date and version of the update"},
{"Title": "Clean `PATH` of non-existent directories with justpath", "Author": "u/iamevpo", "Content": "Wrote and updated a Python package that allows to inspect and correct PATH that may have non-existent directories. justpath unpacks the PATH into lines, annotates errors and can assemble the PATH string back, so that you can use it in shell startup script. Works both for Windows and Linux. To get corrected content of your PATH : justpath show --correct --string Code and more examples: https://github.com/epogrebnyak/justpath Justpath current version (0.0.9) benefited a lot form a previous r/Python discussion here , some main points: on linux for showing path you can simply use echo \"$PATH\" | tr \":\" \"\\n\" | sort ; exploring PYTHONPATH may be as important; may also want to detect and purge duplicate folders from PATH ."},
{"Title": "Enhance Your Images with GFPGAN: Low-Resolution Photo Restoration Tutorial üì∏ [project]", "Author": "u/Feitgemel", "Content": "üöÄ in our latest video tutorial, we will cover photo restoration using GFPGAN! Really cool Python library. The tutorial is divided into four parts: üñºÔ∏è Part 1: Setting up a Conda environment for seamless development and Installing essential Python libraries. üß† Part 2: Cloning the GitHub repository containing the code and resources. üöÄ Part 3: Apply the model on your own images You can find the instructions here : https://github.com/feitgemel/Python-Code-Cool-Stuff/tree/master/GFPGAN The link for the video : https://youtu.be/nPnQm7HFWJs Enjoy Eran #python #GFPGAN #increaseimageresolution #Enhancephoto"},
{"Title": "Scrat: disk memoization for expensive functions", "Author": "u/javiber", "Content": "Sharing a personal project I've been working on for a few months. docs github This project is born from my need for an easy way to memoize to disk the result of expensive functions like Queries or ML-preprocessing. Any feedback or contribution is welcome!"},
{"Title": "Open-source SDK/Python library for Automatic 1111", "Author": "u/Dazzling_Koala6834", "Content": "https://github.com/saketh12/Auto1111SDK Hey everyone, I built an light-weight, open-source Python library for the Automatic 1111 Web UI that allows you to run any Stable Diffusion model locally on your infrastructure. You can easily run: Text-to-Image Image-to-Image Inpainting Outpainting Stable Diffusion Upscale Esrgan Upscale Real Esrgan Upscale Download models directly from Civit AI With any safetensors or Checkpoints file all with a few lines of code!! It is super lightweight and performant. Compared to Huggingface Diffusers, our SDK uses considerably less memory/RAM and we've observed up to a 2x speed increase on all the devices/OS we tested on! Please star our Github repository!!! https://github.com/saketh12/Auto1111SDK ."},
{"Title": "BALanced Execution through Natural Activation : a human-computer interaction methodology for code running is now available on PyPI", "Author": "u/louisbrulenaudet", "Content": "BALENA is a voice interaction framework utilizing state-of-the-art natural language processing and audio processing models to create a system that can interpret voice commands and associate them with predefined actions. The framework leverages the power of transformers and signal processing to understand user intent via spoken language and correlates them with a series of predefined actionable responses. Contributions are welcome on : https://pypi.org/project/balena-cpu/ GitHub : https://github.com/louisbrulenaudet/balena"},
{"Title": "EasyGmail: A Lightweight, minimalistic Python Package for Sending Emails via Gmail", "Author": "u/ayushgun", "Content": "I'm excited to share my first Python package, EasyGmail. It's an open-source package designed to simplify sending emails via Gmail. My goal was to create something lightweight and user-friendly, especially for those who are just starting with Python email automation. üîó GitHub: https://github.com/ayushgun/easygmail Key Features Simplicity. The easygmail.Client interface is designed to be intuitive and minimal. Provides easygmail.EmailBuilder , an intuitive abstraction for creating email.message.EmailMessage objects, Python's standard for structured emails. Flexibility. Multiple way to construct client objects or build structured emails. See the README for more information. Secure. Allows users to provide authentication details via .env files instead of hardcoded credentials. Uses Gmail app passwords instead of account passwords. Quick Start See the README file for a quick start example. I would love to get your feedback on this project. Whether it's suggestions for improvement, feature requests, or just your thoughts on its usability, all feedback is greatly appreciated!"},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Classic Turtle Race", "Author": "u/JamzTyson", "Content": "The \"Turtle Race\" is a classic beginners application using the Turtle library. My implementation takes an OOP approach and I've tried to follow best practices. Feedback, is welcome, but please don't downvote without leaving a comment. https://bitbucket.org/jameztyson/turtlerace/src/main/turtle_race.py"},
{"Title": "(F)ine (G)rade (A)ccess with Graph Engine", "Author": "u/_t1waz_", "Content": "Hi everyone. I'd like to share my design of a service for calculating permissions based on OpenFGA. https://github.com/t1waz/relation_fga I really liked OpenFGA it is a great idea for a service, however, in the implementation I hit a lot of problems. It turns out that on the list of objects relatively slow. I decided to try it with a different approach, graphical, and it seems to be much more efficient. Also graph approach opens up a lot of possibilities for data state analysis I'm open to criticism, I'd love to meet someone who would be interested in the topic."},
{"Title": "Monitor input swapper (Virtual KVM)", "Author": "u/adamjonah", "Content": "Quick overview: This small script runs in the background and scans for an input device (I've used my keyboard) being plugged in or unplugged. If it detects a change then it sends a code to your monitor to swap to the specified input. Now with my cheap USB switch I can swap display inputs with a single button press. Disclaimer - I don't know if this will work with your monitor, or if it could do any damage, I really just threw this together and wanted to share. Background: I've wanted a proper KVM to switch between my Work & Personal computers on my single monitor for a while, but they all seem super expensive (maybe there's better options out there but who knows!). Anyway I realised that my AOC monitor had some windows software call G-Menu and it could swap the inputs on my monitor from windows, instead of going through the relatively cumbersome menu via the monitor's buttons. I started looking into how it worked and found a few resources / repos where people had created their own monitor settings program, and managed to put together this small script. It's configured for my setup so will need adjusting if you want to use it, but I thought it might help someone who had the same issue, but never happened to have that lightbulb moment to realise it could be done pretty easily. Code: https://github.com/a-j-jones/monitor_input_swapper Edit: Clarified that I'm using a cheap USB switch to actually use this code"},
{"Title": "What are the coolest Python automaton projects that you know of?", "Author": "u/voja-kostunica", "Content": "Whether if it's something simple and trivial or complex and robust, small project or well known library, you can describe what it is or even better post a link to the Github repository and elaborate what makes it cool. Edit: \"Python automation projects\" in the title, there is no edit option."},
{"Title": "Use Python to calculate number of upvotes and downvotes", "Author": "u/iamevpo", "Content": "Downvoting with no explanation really hurts on OP ego, but does happen despite r/Python rule #3 \"Please don't downvote without commenting your reasoning for doing so\". One may inquire how many of the downvotes were there in total? On some level at StackOverflow you see the stats on upvote and downvote to your question, but reddit shows you just the net score and provides the upvote rate. Algebraically, this is enough to calculate the raw upvote and downvote numbers. The formulas come from pen and paper solution for this system of two equations: upvotes - downvotes = net_score                    (1) upvote_rate = upvotes / (upvotes + downvotes )     (2) The code: def votes(upvote_rate: float, net_score: int)-> tuple[int, int]: downvotes = net_score * (1-upvote_rate) / (2 * upvote_rate - 1) upvotes = net_score + downvotes return round(upvotes), round(downvotes) print(votes(.8, 3))  # (4, 1) print(votes(.56, 5)) # (23, 18) So, in second example by knowing the upvote rate of 56% and net score of 5 I now know there were 23 upvotes and 18 downvotes (sadly, with no comment)."},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "nowpy -automatic venv set up and package installs", "Author": "u/Maximum_Cucumber_988", "Content": "nowpy nowpy is a CLI-tool I made that enables you to run any arbitrary Python script instantly, on the basis that I run code far more often than I write it. It combines python , virtualenv , and pip to launch a dedicated isolated environment, automatically figure out which packages are required, and then run your Python file - all with just one command. nowpy finds packages by performing a recursive lookup for a pyproject.toml OR a requirements.txt , and cross-checks with any import statements inside the Python file. Installation Use the package manager pip to install nowpy . pip install nowpy It might be the last time you have to pip install anything! Usage Here's an example of what happens if you run nowpy on a Python file that imports requsts . First run: nowpy WorldTimeApi.py Creating Virtualenv... Collecting requests ... Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests Successfully installed certifi-2023.11.17 charset-normalizer-3.3.2 idna-3.6 requests-2.31.0 urllib3-2.2.0 Running Script... Current Time in Europe/London Date: 2024-01-30T22:08:52.854140+00:00 Timezone: Europe/London All future runs: nowpy WorldTimeApi.py Running Script... Current Time in Europe/London Date: 2024-01-30T22:08:52.854140+00:00 Timezone: Europe/London nowpy creates a unique virtual environment for every directory you run nowpy from. It also removes unused ones automatically. But if you ever want to reset a particular one that you're using, just use the --reset option: nowpy --reset I've not tested it on anything except my Mac - let me know if there are any issues! Source Code on GitHub: https://github.com/WillDenby/nowpy PyPI Page: https://pypi.org/project/nowpy/"},
{"Title": "I made a Buckshot Roulette game in python", "Author": "u/Unlucky_Basil7767", "Content": "I recreated the game Buckshot roulette in python and wanted to share Here is the source code - https://github.com/ATharvaCoder492/Buckshot_Roulette If you have any suggestions to make this better plz comment"},
{"Title": "Streamlit vs Taipy: Which Tool is Better for Creating Web Apps from Python Scripts?", "Author": "u/bitdoze", "Content": "If you are looking for a tool to turn your Python scripts into web apps, you might have heard of Streamlit and Taipy. They are both popular and promising tools that allow you to create interactive and beautiful web apps from your Python code, without requiring any web development skills. I have created an article here that can help you see the differences: https://www.bitdoze.com/streamlit-vs-taipy/"},
{"Title": "K Lars Lohn uses math and Python to triangulate the nighttime booms disturbing the sleep of his community.", "Author": "u/AlSweigart", "Content": "\"Finding the Air Cannon\" https://www.twobraids.com/2024/01/air-cannon.html It took three people stationed at remote locations miles apart using a synchronized clock on our cell phones. We each waited over the same ten minute period, noting the exact time for each of the five cannon shots that we heard. ... I wrote a program in Python (see source code below) that could iterate all the points in the image in the search area where we suspected the air cannon sat. ... I called the owner of the farm (headquartered in Monmouth) and asked if they used an air cannon on their property near the Corvallis airport. They confirmed that they do. I asked if they run it at night, they said they do not. ... However, in an amazing coincidence, the air cannons stopped that very evening of our phone conversation."},
{"Title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "pyside6 Qt for Python tutorial for Qt C++ developers", "Author": "u/nmariusp", "Content": "https://www.youtube.com/watch?v=fVzFq909z_k Video tutorial about the Python bindings for the Qt framework named pyside6 or \"Qt for Python\"."},
{"Title": "Profiling your Numba code with Profila", "Author": "u/itamarst", "Content": "If you‚Äôre writing numeric Python code, Numba can be a great way to speed up your program. By compiling a subset of Python to machine code, Numba lets you write for loops and other constructs that would be too slow in normal Python. In other words, it‚Äôs similar to Cython, C, or Rust, in that it lets you write compiled extensions for Python. Numba code isn‚Äôt always as fast as it could be, however. This is where profiling is useful: it can find at least some of the bottlenecks in your code. Learn more about Profila , a new profiler for Numba that I've just released on GitHub ."},
{"Title": "Hacking the Hacker News API for Real-Time Analysis with Grafana, Proton, and Bytewax", "Author": "u/math-bw", "Content": "I wrote this tutorial on how to consume the Hacker News API as a stream and process it with open source tools Bytewax, Proton and Grafana. It is kind of fun to see who all the armchair experts are; weighing in on every story posted to hacker news :P. I thought this might be useful if you are working with real-time data and need to build some live dashboards. Let me know what you think! https://github.com/bytewax/hacking-hacker-news"},
{"Title": "Arrest v0.1.8 is released with OpenAPI integration!", "Author": "u/s_basu", "Content": "Hey everyone, I am back again with some good news. After many requests for OpenAPI support, it has been finally added in 0.1.8 You can install the optional dependencies with pip install arrest[openapi] This provides a CLI interface where you can provide the URL to the OpenAPI specification file (json or yaml). The URL can be an HTTP URL or a filepath. Arrest will generate the necessary boilerplates such as the schema definition Pydantic models (courtesy of datamodel-code-generator ), Arrest resources and services (using Jinja templates). Arrest is a small package to define the structure of RESTful APIs from an external source with type annotations, making it easier to interface with, and call different routes of an external web service from inside a Python application, such as a third-party API, or an internal microservice, while also providing data validation for the request, response, exception handling and retries. For more information please check out the docs and the repository Please note that the OpenAPI integration is an active work in progress. There are many features missing that I will be adding gradually such as extracting the header and query parameters from the specification, or support for composite types like list[Model] or dict[str, Model] will be added soon. I would greatly appreciate your feedback and any areas of improvement. Thank you to everyone here for supporting this project and providing the feedback that made it work! edit: added a bit of context for the package"},
{"Title": "Wednesday Daily Thread: Beginner questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Ollama Python Library - Chat Method, System Message, Streaming & Options", "Author": "u/developer_1010", "Content": "Below you will find the link to my tutorial on using the Ollama Python library . Here I show with the help of examples how to use models like Mistral to create messages and go into more detail about the chat method , the system message and the streaming option . I also show how to use the streaming option. Finally, how to correct code with options like the temperature parameter . Link: Ollama Python Library - Tutorial with Examples"},
{"Title": "Conda now uses lib-mamba by default!", "Author": "u/jmeppley", "Content": "Many of y'all may have seen this already, but I just stumbled upon it this weekend: https://docs.conda.io/projects/conda/en/latest/release-notes.html#id2 About two years ago (which I also missed), conda added mamba libraries as an experimental option. Apparently it went well enough to make it official. Version 10.23 (from October 2023) now uses mamba libraries  by default. I ran some quick and dirty tests, and the new conda is was only about 10% to 50% slower than mamba on some big environments. This is a huge improvement from before where it could an order of magnitude slower. Edit: added date and version of the update"},
{"Title": "Clean `PATH` of non-existent directories with justpath", "Author": "u/iamevpo", "Content": "Wrote and updated a Python package that allows to inspect and correct PATH that may have non-existent directories. justpath unpacks the PATH into lines, annotates errors and can assemble the PATH string back, so that you can use it in shell startup script. Works both for Windows and Linux. To get corrected content of your PATH : justpath show --correct --string Code and more examples: https://github.com/epogrebnyak/justpath Justpath current version (0.0.9) benefited a lot form a previous r/Python discussion here , some main points: on linux for showing path you can simply use echo \"$PATH\" | tr \":\" \"\\n\" | sort ; exploring PYTHONPATH may be as important; may also want to detect and purge duplicate folders from PATH ."},
{"Title": "Enhance Your Images with GFPGAN: Low-Resolution Photo Restoration Tutorial üì∏ [project]", "Author": "u/Feitgemel", "Content": "üöÄ in our latest video tutorial, we will cover photo restoration using GFPGAN! Really cool Python library. The tutorial is divided into four parts: üñºÔ∏è Part 1: Setting up a Conda environment for seamless development and Installing essential Python libraries. üß† Part 2: Cloning the GitHub repository containing the code and resources. üöÄ Part 3: Apply the model on your own images You can find the instructions here : https://github.com/feitgemel/Python-Code-Cool-Stuff/tree/master/GFPGAN The link for the video : https://youtu.be/nPnQm7HFWJs Enjoy Eran #python #GFPGAN #increaseimageresolution #Enhancephoto"},
{"Title": "Scrat: disk memoization for expensive functions", "Author": "u/javiber", "Content": "Sharing a personal project I've been working on for a few months. docs github This project is born from my need for an easy way to memoize to disk the result of expensive functions like Queries or ML-preprocessing. Any feedback or contribution is welcome!"},
{"Title": "Open-source SDK/Python library for Automatic 1111", "Author": "u/Dazzling_Koala6834", "Content": "https://github.com/saketh12/Auto1111SDK Hey everyone, I built an light-weight, open-source Python library for the Automatic 1111 Web UI that allows you to run any Stable Diffusion model locally on your infrastructure. You can easily run: Text-to-Image Image-to-Image Inpainting Outpainting Stable Diffusion Upscale Esrgan Upscale Real Esrgan Upscale Download models directly from Civit AI With any safetensors or Checkpoints file all with a few lines of code!! It is super lightweight and performant. Compared to Huggingface Diffusers, our SDK uses considerably less memory/RAM and we've observed up to a 2x speed increase on all the devices/OS we tested on! Please star our Github repository!!! https://github.com/saketh12/Auto1111SDK ."},
{"Title": "BALanced Execution through Natural Activation : a human-computer interaction methodology for code running is now available on PyPI", "Author": "u/louisbrulenaudet", "Content": "BALENA is a voice interaction framework utilizing state-of-the-art natural language processing and audio processing models to create a system that can interpret voice commands and associate them with predefined actions. The framework leverages the power of transformers and signal processing to understand user intent via spoken language and correlates them with a series of predefined actionable responses. Contributions are welcome on : https://pypi.org/project/balena-cpu/ GitHub : https://github.com/louisbrulenaudet/balena"},
{"Title": "EasyGmail: A Lightweight, minimalistic Python Package for Sending Emails via Gmail", "Author": "u/ayushgun", "Content": "I'm excited to share my first Python package, EasyGmail. It's an open-source package designed to simplify sending emails via Gmail. My goal was to create something lightweight and user-friendly, especially for those who are just starting with Python email automation. üîó GitHub: https://github.com/ayushgun/easygmail Key Features Simplicity. The easygmail.Client interface is designed to be intuitive and minimal. Provides easygmail.EmailBuilder , an intuitive abstraction for creating email.message.EmailMessage objects, Python's standard for structured emails. Flexibility. Multiple way to construct client objects or build structured emails. See the README for more information. Secure. Allows users to provide authentication details via .env files instead of hardcoded credentials. Uses Gmail app passwords instead of account passwords. Quick Start See the README file for a quick start example. I would love to get your feedback on this project. Whether it's suggestions for improvement, feature requests, or just your thoughts on its usability, all feedback is greatly appreciated!"},
{"Title": "Tuesday Daily Thread: Advanced questions", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Classic Turtle Race", "Author": "u/JamzTyson", "Content": "The \"Turtle Race\" is a classic beginners application using the Turtle library. My implementation takes an OOP approach and I've tried to follow best practices. Feedback, is welcome, but please don't downvote without leaving a comment. https://bitbucket.org/jameztyson/turtlerace/src/main/turtle_race.py"},
{"Title": "(F)ine (G)rade (A)ccess with Graph Engine", "Author": "u/_t1waz_", "Content": "Hi everyone. I'd like to share my design of a service for calculating permissions based on OpenFGA. https://github.com/t1waz/relation_fga I really liked OpenFGA it is a great idea for a service, however, in the implementation I hit a lot of problems. It turns out that on the list of objects relatively slow. I decided to try it with a different approach, graphical, and it seems to be much more efficient. Also graph approach opens up a lot of possibilities for data state analysis I'm open to criticism, I'd love to meet someone who would be interested in the topic."},
{"Title": "Monitor input swapper (Virtual KVM)", "Author": "u/adamjonah", "Content": "Quick overview: This small script runs in the background and scans for an input device (I've used my keyboard) being plugged in or unplugged. If it detects a change then it sends a code to your monitor to swap to the specified input. Now with my cheap USB switch I can swap display inputs with a single button press. Disclaimer - I don't know if this will work with your monitor, or if it could do any damage, I really just threw this together and wanted to share. Background: I've wanted a proper KVM to switch between my Work & Personal computers on my single monitor for a while, but they all seem super expensive (maybe there's better options out there but who knows!). Anyway I realised that my AOC monitor had some windows software call G-Menu and it could swap the inputs on my monitor from windows, instead of going through the relatively cumbersome menu via the monitor's buttons. I started looking into how it worked and found a few resources / repos where people had created their own monitor settings program, and managed to put together this small script. It's configured for my setup so will need adjusting if you want to use it, but I thought it might help someone who had the same issue, but never happened to have that lightbulb moment to realise it could be done pretty easily. Code: https://github.com/a-j-jones/monitor_input_swapper Edit: Clarified that I'm using a cheap USB switch to actually use this code"},
{"Title": "What are the coolest Python automaton projects that you know of?", "Author": "u/voja-kostunica", "Content": "Whether if it's something simple and trivial or complex and robust, small project or well known library, you can describe what it is or even better post a link to the Github repository and elaborate what makes it cool. Edit: \"Python automation projects\" in the title, there is no edit option."},
{"Title": "Use Python to calculate number of upvotes and downvotes", "Author": "u/iamevpo", "Content": "Downvoting with no explanation really hurts on OP ego, but does happen despite r/Python rule #3 \"Please don't downvote without commenting your reasoning for doing so\". One may inquire how many of the downvotes were there in total? On some level at StackOverflow you see the stats on upvote and downvote to your question, but reddit shows you just the net score and provides the upvote rate. Algebraically, this is enough to calculate the raw upvote and downvote numbers. The formulas come from pen and paper solution for this system of two equations: upvotes - downvotes = net_score                    (1) upvote_rate = upvotes / (upvotes + downvotes )     (2) The code: def votes(upvote_rate: float, net_score: int)-> tuple[int, int]: downvotes = net_score * (1-upvote_rate) / (2 * upvote_rate - 1) upvotes = net_score + downvotes return round(upvotes), round(downvotes) print(votes(.8, 3))  # (4, 1) print(votes(.56, 5)) # (23, 18) So, in second example by knowing the upvote rate of 56% and net score of 5 I now know there were 23 upvotes and 18 downvotes (sadly, with no comment)."},
{"Title": "Monday Daily Thread: Project ideas!", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Flask-Muck v0.3.0 - Flask extension, OpenAPI/Swagger UI generation and Pydantic support", "Author": "u/beef-runner", "Content": "Flask-Muck is an open-source Flask extension I've been developing that handles generating a complete set of CRUD endpoints for SQLAlchemy models in a couple lines of code. The initial feature set was based on my own experiences writing very similar libraries in production over the years. After publishing the package I got lots of great feedback including quite a bit from right here. I've just released v0.3.0 that incorporates some key features based on that feedback. New Features: Flask Extension: Initially the library only supported a utility style implementation where you were required to have existing Blueprints. This was based on my own experience of always needing to incorporate the library into an existing legacy codebase. Flask-Muck now supports initializing a Flask extension that handles even more of the repetitive work. This is ideal for small services or projects using Flask-Muck from the start. OpenAPI Specification/Swagger UI: When using the Flask extension an OpenAPI specification and Swagger UI documentation are automatically generated. Pydantic Support: Pydantic models are now supported for defining request and response schemas. They can even be mixed and matched if a codebase is transitioning from one to another. GitHub: https://github.com/dtiesling/flask-muck Documentation: https://dtiesling.github.io/flask-muck/ PyPi: https://pypi.org/project/flask-muck/ This style of declarative view has saved me 1000s of lines of boilerplate code and I hope it can do the same for some of you. Thanks again to all those that took the time to check this project out and provide feedback. Please keep the feedback coming and I'd love to hear from anyone using Flask-Muck personally or professionally."},
{"Title": "I made a follow-up package, scikit-multilearn-ng, to the widely used scikit-multilearn package for multilabel et. al. classification", "Author": "u/lord_lardi", "Content": "After needing to use scikit-multilearn and detecting errors, I opened a PR and waited. But after double checking I saw that there hadn't been any commits in 7 months (now 9 months) and that it had not been a release since 2018, I dug in and found out that no one had access to the PyPi credentials and so on. So I opened a discussion about creating a fork and many were eager for it. So after some developing, I'm here to introduce scikit-multilearn-ng (GitHub: https://github.com/scikit-multilearn-ng/scikit-multilearn-ng ), an advanced, open-source tool for multi-label classification in Python. It's a direct successor to scikit-multilearn and brings a host of improvements and new features. What Makes scikit-multilearn-ng Stand Out? Enhanced Integration with scikit-learn : This package not only integrates with the scikit-learn ecosystem but also extends its capabilities, making it a natural fit for those familiar with scikit-learn. Expanded Algorithm Collection : Among its new offerings are StructuredGridSearchCV and the SMiLE algorithm, specifically designed for more complex multi-label classification tasks, including handling missing labels and heterogeneous features. Open Source Philosophy : As a community-driven project, it's free to use and open for contributions, perfect for collaborative development. Why Should You Consider Upgrading? Ease of Transition : For those already using scikit-multilearn, upgrading is as simple as switching the dependency to scikit-multilearn-ng. Your existing code will work without any changes. Active Development and Support : scikit-multilearn-ng offers bug fixes and new features, ensuring your projects stay current and robust. Whether you're a seasoned Python developer or just starting out in machine learning, scikit-multilearn-ng is worth exploring. Some Example Use Cases: A simple example use case is iterative splitting multilabel data between train and test data while trying to maintain the distribution of each label between the training and test sets. This is particularly useful for datasets where certain label combinations are rare. from skmultilearn.model_selection import iterative_train_test_split import numpy as np # Assuming X is your feature matrix and y is your label matrix # X should be a numpy array or a sparse matrix # y should be a binary indicator matrix (each label is either 0 or 1) # Define the size of your test set test_size = 0.2 # Perform the split # The function returns flattened arrays, so you need to reshape them X_train, y_train, X_test, y_test = iterative_train_test_split(X, y, test_size = test_size) # Reshape the outputs back to the original shapes num_labels = y.shape[1] y_train = y_train.reshape(-1, num_labels) y_test = y_test.reshape(-1, num_labels) But it also supports advanced problem transformations to single label problems: from skmultilearn.problem_transform import BinaryRelevance from sklearn.svm import SVC # Initialize and train classifier = BinaryRelevance(classifier=SVC(), require_dense=[False, True]) classifier.fit(X_train, y_train) # Predict predictions = classifier.predict(X_test) Please contribute and star the project! I'm looking forward to your feedback, questions, and how you might use it in your projects!"},
{"Title": "Why Python‚Äôs ‚Äúslowness‚Äù is not slowing anyone down", "Author": "u/Counter-Business", "Content": "Ever wondered why, despite all the grumbles about Python being slow, it‚Äôs still everywhere? Especially when folks dive into coding, one of the first things you hear is ‚ÄúPython‚Äôs slow.‚Äù But, if it‚Äôs such a snail, why do so many people use it for all sorts of heavy-duty stuff? Here‚Äôs the deal: Yes, Python isn‚Äôt the Usain Bolt of programming languages when it comes to raw speed. We‚Äôre talking basic stuff like loops and if statements. But let‚Äôs be real, how often are we in a situation where the speed of a for-loop is the make-or-break of our project? The secret sauce of Python isn‚Äôt in beating speed records. It‚Äôs in its knack for playing nice with super-optimized C libraries. These libraries are the muscle doing the heavy lifting, while Python‚Äôs more like the friendly coach guiding the process. So, your Python code might take a tiny bit longer to run a loop, but when it calls on these C libraries to do the real work, they zip through tasks at lightning speed. So, next time you hear someone knocking Python for being slow, maybe toss this thought their way. Python‚Äôs not just about the speed of typing out code; it‚Äôs about the overall speed and ease of getting stuff done, thanks to all those optimized libraries it wraps around so neatly."},
{"Title": "I made a SQL query builder in python", "Author": "u/deepCelibateValue", "Content": "It's a bit silly, but it was a good exercise. https://github.com/sebastiancarlos/yas-qwin"},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Anyone used match case yet?", "Author": "u/samarthrawat1", "Content": "What are your thoughts on it."},
{"Title": "AbstractAPI Python SDK", "Author": "u/ebram96", "Content": "I'm excited to share about my open source Python library I created to help with using AbstractAPI services. What can you do with the library? It supports using all current AbstractAPI services: - Email validation - Phone number validation - IP geolocation - Lookup holidays of a country on a specific date or date range. - VAT validation/calculation/categories - IBAN validation - Exchange Rates live/conversion/historical rates - Lookup company data using only its domain - Timezone current/conversion - Avatars generation - Website screenshot - Website scrape - Image processing It supports Python >= 3.9 and it is tested on all environments from 3.9 up to 3.12. Tests are run automatically on all supported Python versions by a CI workflow. Check it and maybe give a star? :) Repo: https://github.com/ebram96/AbstractAPI-Python-SDK Doc: https://abstractapi-python-sdk.readthedocs.io/en/latest/index.html Package: https://pypi.org/project/abstract-api/ Contributions are very welcome."},
{"Title": "Leapcell: A Better Alternative for Heroku + Airtable for Python", "Author": "u/OfficeAccomplished45", "Content": "We are thrilled to announce the official launch of Leapcell's Beta public testing. Leapcell: https://leapcell.io/ Leapcell is a Data & Service Hosting Community, providing an application hosting experience comparable to the convenience of Vercel. Additionally, it features a high-performance database with an Airtable-like interface, streamlining data management. The entire platform is fully managed and serverless, enabling users to focus on specific business implementations without dedicating excessive time to infrastructure and DevOps. For more information, please refer to https://docs.leapcell.io/ Our goal is to empower users to concentrate on specific business implementations, allowing more individuals (Product Managers, Marketing professionals, Data Scientists) to participate in content creation and management without spending too much time on infrastructure and DevOps. Here's a Flask example: https://leapcell.io/issac/flask-blog , which contains a database and an application. For documentation on deploying Flask projects, check this link: https://docs.leapcell.io/docs/application/examples/flask . Deploying other projects is also straightforward. Leapcell is currently in beta testing, and we welcome any feedback or questions."},
{"Title": "Apache OpenDAL‚Ñ¢, a Rust library with Python-bindings for accessing data freely, is now graduated and in search for GSoC 2024 projects!", "Author": "u/simonsanone", "Content": "(Disclaimer: Just a bystander of that project, but I feel it deserves some love.) openDAL is a data access layer that allows users to easily and efficiently retrieve data from various storage services in a unified way. It is written in Rust and has bindings for Python available on pypi: https://pypi.org/project/opendal/ Storage services include webdav, ftp, ipfs, redis, rocksdb, s3, gcs, azblob, gdrive, dropbox, onefrive, memcached, and many more. There are also bindings in the works for C, C++, Haskell, LUA, Ruby, Swift, and Zig. With libraries for Java, Node.js, Python already being released. They are also looking for project ideas for the Google Summer of Code in their discussions, so chime in, if you have a good idea, want to help to make the Python-bindings better or want to be a mentor: GSoC 2024 Projects"},
{"Title": "If you are on Linux and work with virtual environments, consider adding this to your .bashrc", "Author": "u/munabedan", "Content": "venv() { # Check if already activated if [[ \"$VIRTUAL_ENV\" != \"\" ]]; then echo -e \"\\n\\e[1;33mDeactivating current virtual environment...\\e[0m\" deactivate return fi # Check if the venv directory exists if [ -d \"venv\" ]; then echo -e \"\\n\\e[1;33mActivating virtual environment...\\e[0m\" source venv/bin/activate else echo -e \"\\n\\e[1;33mCreating and activating virtual environment...\\e[0m\" python3 -m venv venv source venv/bin/activate fi } Now when creating a python project, just go into the folder and call $ venv It should create a virtual environment with a folder named venv, if it exists it will activate it and if already activated it will deactivate it. For reference, here is a link to the script on github: https://gist.github.com/munabedan/6a5e8c104228943a461095a9e103a5af"},
{"Title": "Playing around with Ultra HDR", "Author": "u/albertzeyer", "Content": "This is the ultrahdr.py script , which generates this HDR-demo . (Check gregbenzphotography.com/hdr/ whether you can display HDR properly.) High-dynamic range (HDR) is to extend the usual color range (Standard Dynamic Range (SDR)) and usually also extends the common 8bit color depth to 10bit or more. https://en.wikipedia.org/wiki/High_dynamic_range Some modern displays (~2021) (e.g. MacBook M1, some OLED TVs) support HDR, but it is still a rare feature. There are multiple formats for HDR images, e.g.: OpenEXR AVIF JPEG XT embedded in JPEG XL JPEG XR Ultra HDR (used here) embedded in standard JPEG Ultra HDR uses the JPEG multi-picture format (MPF). It stores the normal SDR JPEG image as the first image, so all existing JPEG decoders can display the normal image. Then it stores a HDR gain map embedded in MPF which can be used to reconstruct the HDR image. Currently, (end of 2023), Google Chrome stable (end of 2023) supports this format. (Another alternative in Google Chrome is AVIF.) (Firefox currently does not support it.) Currently, (end of 2023), Google Pixel phones can capture Ultra HDR images (e.g. when they use night mode). (Note, many websites, e.g. Twitter, will reencode JPEGs after you upload them, and often they don't support Ultra HDR yet, so then it will be lost, and you will just see the normal SDR JPEG image.) About the Ultra HDR format: https://developer.android.com/media/platform/hdr-image-format This document defines the behavior of a new file format that encodes a logarithmic range gain map image in a JPEG image file. Legacy readers that don't support the new format read and display the conventional low dynamic range image from the image file. Readers that support the format combine the primary image with the gain map and render a high dynamic range image on compatible displays. To use the simple script here, for preparation: First, build this: https://github.com/google/libultrahdr Make sure FFMpeg is installed This script does nothing fancy: It just upscales the input JPEG color range (FFmpeg does that here currently) and then encodes the HDR gain map using Google's Ultra HDR encoder (libultrahdr). The effect is that the image will display brighter on HDR displays. Some related issues: https://github.com/ImageMagick/ImageMagick/issues/6377 https://github.com/libvips/libvips/issues/3799"},
{"Title": "pathit - Just show me my PATH in way I can read it", "Author": "u/iamevpo", "Content": "Got tired of scrolling through my PATH variable and decomposing it with something like echo $PATH | tr \":\" \"\\n\" , so I wrote micro package to show the path by line, sort it alphabetically and extract lines of interest. Question Answer What's on my PATH? pathit show or pathit raw Sort alphabetically pathit show --sort Paths with mingw ? pathit show --includes mingw New content PATH without invalid dirs pathit show --purge --string Maybe a better tool exists, but hope it is a useful utility - or at least a demo. Packaging done with `poetry` and few tests added. Code, bash equivalents and install instructions: https://github.com/epogrebnyak/what-the-path Update: thanks to feedback from this thread, the options in pathit become the following. Note --string in specific - it allows to form a string that you can later use in your shell start file. --sort     --no-sort    Sort output alphabetically. --includes TEXT --excludes TEXT --purge    --no-purge   Exclude invalid directories. --expand   --no-expand  Expand environment variables if found inside PATH. --string   --no-string  Print a single string suitable for PATH content. --display-numbers       Indicate directory order in PATH. --color                 Use color to highlight errors. Update 2: package renamed justpath and available through pip install justpath now. Also can display your PATH as JSON now."},
{"Title": "FastCRUD - Powerful CRUD methods and automatic endpoint creation for FastAPI", "Author": "u/igorbenav", "Content": "Hey, guys, for anyone who might benefit (or would like to contribute) FastCRUD is a Python package for FastAPI, offering robust async CRUD operations and flexible endpoint creation utilities, streamlined through advanced features like auto-detected join conditions, dynamic sorting, and offset and cursor pagination. Github: github.com/igorbenav/fastcrud Docs: igorbenav.github.io/fastcrud/ Features: - Fully Async: Leverages Python's async capabilities for non-blocking database operations. - SQLAlchemy 2.0: Works with the latest SQLAlchemy version for robust database interactions. - Powerful CRUD Functionality: Full suite of efficient CRUD operations with support for joins. - Dynamic Query Building: Supports building complex queries dynamically, including filtering, sorting, and pagination. - Advanced Join Operations: Facilitates performing SQL joins with other models with automatic join condition detection. - Built-in Offset Pagination: Comes with ready-to-use offset pagination. - Cursor-based Pagination: Implements efficient pagination for large datasets, ideal for infinite scrolling interfaces. - Modular and Extensible: Designed for easy extension and customization to fit your requirements. - Auto-generated Endpoints: Streamlines the process of adding CRUD endpoints with custom dependencies and configurations. Improvements are coming, issues and pull requests always welcome github.com/igorbenav/fastcrud"},
{"Title": "Patching pydantic settings in pytest", "Author": "u/rednafi", "Content": "TIL: Figured out a way to patch pydantic settings to remove dependencies on environment variables while running unit tests. https://rednafi.com/python/patch_pydantic_settings_in_pytest/"},
{"Title": "Python version of the endless \"brick breaker\" game", "Author": "u/kvyb", "Content": "Decided to make a simple version of an endless \"brick breaker\" type game in pygame, where two balls compete to color each others' domain area squares for eternity. Feel free to contribute: https://github.com/kvyb/endless-game , there is much that can be improved in my opinion, especially collisions."},
{"Title": "PyCon Pakistan 24", "Author": "u/adilfarooq", "Content": "PyCon is happening this March 9th & 10th at LUMS, Lahore, Pakistan. The early bird discount is available till Jan 31st. For more detail, stay updated through PyCon Pakistan‚Äôs social media pages and website https://pycon.pk"},
{"Title": "Custom Wallpaper System with Quotes & Weather", "Author": "u/the-chris-xd", "Content": "Hi, I made a program that automatically sets up a wallpaper on my desktop with a random quote & weather. It does so by loading a image (background) and fetching quotes and weather (as per location entered in the city var) and sets both on the image. Then it saves the temporary image and then puts it on my desktop. This all happens in a loop with interval of 1 hour. I also made a run.bat that runs this program with pythonw and then I put it in my Startup folder. I am just a beginner in Python itself so this project isn't all that good and may contain bugs/inefficient code, I'm sorry for that. If someone can provide feedback/ideas about what I can do with this project, it'll be helpful. Thanks in advance for that. Thanks for reading. Here's the project link: https://github.com/TheChrisGG/AutoPaper"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Simple XML generation from a Python data structure", "Author": "u/nicwolff", "Content": "I'm not a beginner now but I wrote this years ago because the xml.etree.ElementTree and lxml.builder.ElementMaker APIs are too verbose and don't compose well. It's just one function, XML , which takes a Python sequence. The first item is the tag name, the second is an optional dict of attributes, and the rest are strings or nested sequences. If the tag name is None the content replaces the tag. https://pypi.org/project/xml-from-seq/ https://github.com/nicwolff/xml_from_seq from xml_from_seq import XML, INLINE item = [ 'item', {'attr': 123, 'attr_2': None}, 'This is some content of the item.', ['b', INLINE, 'This will be bold and not indented.'], [None, 'This will not be bold.'], ] print(XML(item)) <item attr=\"123\"> This is some content of the item. <b>This will be bold and not indented.</b> This will not be bold. </item>"},
{"Title": "ezgpt - An easy and intuitive interface for OpenAI's GPT API", "Author": "u/Ascyt", "Content": "For a while now I've been using OpenAI's GPT API instead of ChatGPT because it provides so much more control over things and also allows access to GPT-4 while being much cheaper overall with pretty much no rate limits. I made my own Python library that builds on top of OpenAI's openai library, and makes interacting with it much easier. For example, you can just use ezgpt.get(user='Your prompt') to get the response. Most of my effort went into the conversation feature though - it makes it easy to chat, edit, save and load the conversations. To use the conversation, simply import ezgpt and run ezgpt.c() , which, in my case, I have put into a python file which gets run by a .bat file, so I can easily run it from anywhere. Check it out here: https://pypi.org/project/ezgpt GitHub Repository: https://github.com/Ascyt/ezgpt"},
{"Title": "Classical vs. London schools of unit testing", "Author": "u/szymonmiks", "Content": "Test examples: https://github.com/szymon6927/szymonmiks.pl/blob/master/blog/examples/tests/test_classical_vs_london/test_transaction_processor.py My latest blog post delves into the Classical and London schools, offering practical Python examples. https://blog.szymonmiks.pl/p/exploring-different-schools-of-unit-testing-in-python/"},
{"Title": "This is not interview advice: a priority-expiry LRU cache without heaps or trees in Python", "Author": "u/genericlemon24", "Content": "I wrote an in-depth tutorial on a bIG TEch CoDINg InTerVIEW problem, but for fun I restricted myself to the Python standard library only. With a bit of exploration, the solution ended up comparable with the theoretically correct one (which, it turns out, is not the fastest anyway). Link: https://death.andgravity.com/lru-cache"},
{"Title": "Flask-Muck v0.3.0 - Flask extension, OpenAPI/Swagger UI generation and Pydantic support", "Author": "u/beef-runner", "Content": "Flask-Muck is an open-source Flask extension I've been developing that handles generating a complete set of CRUD endpoints for SQLAlchemy models in a couple lines of code. The initial feature set was based on my own experiences writing very similar libraries in production over the years. After publishing the package I got lots of great feedback including quite a bit from right here. I've just released v0.3.0 that incorporates some key features based on that feedback. New Features: Flask Extension: Initially the library only supported a utility style implementation where you were required to have existing Blueprints. This was based on my own experience of always needing to incorporate the library into an existing legacy codebase. Flask-Muck now supports initializing a Flask extension that handles even more of the repetitive work. This is ideal for small services or projects using Flask-Muck from the start. OpenAPI Specification/Swagger UI: When using the Flask extension an OpenAPI specification and Swagger UI documentation are automatically generated. Pydantic Support: Pydantic models are now supported for defining request and response schemas. They can even be mixed and matched if a codebase is transitioning from one to another. GitHub: https://github.com/dtiesling/flask-muck Documentation: https://dtiesling.github.io/flask-muck/ PyPi: https://pypi.org/project/flask-muck/ This style of declarative view has saved me 1000s of lines of boilerplate code and I hope it can do the same for some of you. Thanks again to all those that took the time to check this project out and provide feedback. Please keep the feedback coming and I'd love to hear from anyone using Flask-Muck personally or professionally."},
{"Title": "I made a follow-up package, scikit-multilearn-ng, to the widely used scikit-multilearn package for multilabel et. al. classification", "Author": "u/lord_lardi", "Content": "After needing to use scikit-multilearn and detecting errors, I opened a PR and waited. But after double checking I saw that there hadn't been any commits in 7 months (now 9 months) and that it had not been a release since 2018, I dug in and found out that no one had access to the PyPi credentials and so on. So I opened a discussion about creating a fork and many were eager for it. So after some developing, I'm here to introduce scikit-multilearn-ng (GitHub: https://github.com/scikit-multilearn-ng/scikit-multilearn-ng ), an advanced, open-source tool for multi-label classification in Python. It's a direct successor to scikit-multilearn and brings a host of improvements and new features. What Makes scikit-multilearn-ng Stand Out? Enhanced Integration with scikit-learn : This package not only integrates with the scikit-learn ecosystem but also extends its capabilities, making it a natural fit for those familiar with scikit-learn. Expanded Algorithm Collection : Among its new offerings are StructuredGridSearchCV and the SMiLE algorithm, specifically designed for more complex multi-label classification tasks, including handling missing labels and heterogeneous features. Open Source Philosophy : As a community-driven project, it's free to use and open for contributions, perfect for collaborative development. Why Should You Consider Upgrading? Ease of Transition : For those already using scikit-multilearn, upgrading is as simple as switching the dependency to scikit-multilearn-ng. Your existing code will work without any changes. Active Development and Support : scikit-multilearn-ng offers bug fixes and new features, ensuring your projects stay current and robust. Whether you're a seasoned Python developer or just starting out in machine learning, scikit-multilearn-ng is worth exploring. Some Example Use Cases: A simple example use case is iterative splitting multilabel data between train and test data while trying to maintain the distribution of each label between the training and test sets. This is particularly useful for datasets where certain label combinations are rare. from skmultilearn.model_selection import iterative_train_test_split import numpy as np # Assuming X is your feature matrix and y is your label matrix # X should be a numpy array or a sparse matrix # y should be a binary indicator matrix (each label is either 0 or 1) # Define the size of your test set test_size = 0.2 # Perform the split # The function returns flattened arrays, so you need to reshape them X_train, y_train, X_test, y_test = iterative_train_test_split(X, y, test_size = test_size) # Reshape the outputs back to the original shapes num_labels = y.shape[1] y_train = y_train.reshape(-1, num_labels) y_test = y_test.reshape(-1, num_labels) But it also supports advanced problem transformations to single label problems: from skmultilearn.problem_transform import BinaryRelevance from sklearn.svm import SVC # Initialize and train classifier = BinaryRelevance(classifier=SVC(), require_dense=[False, True]) classifier.fit(X_train, y_train) # Predict predictions = classifier.predict(X_test) Please contribute and star the project! I'm looking forward to your feedback, questions, and how you might use it in your projects!"},
{"Title": "Why Python‚Äôs ‚Äúslowness‚Äù is not slowing anyone down", "Author": "u/Counter-Business", "Content": "Ever wondered why, despite all the grumbles about Python being slow, it‚Äôs still everywhere? Especially when folks dive into coding, one of the first things you hear is ‚ÄúPython‚Äôs slow.‚Äù But, if it‚Äôs such a snail, why do so many people use it for all sorts of heavy-duty stuff? Here‚Äôs the deal: Yes, Python isn‚Äôt the Usain Bolt of programming languages when it comes to raw speed. We‚Äôre talking basic stuff like loops and if statements. But let‚Äôs be real, how often are we in a situation where the speed of a for-loop is the make-or-break of our project? The secret sauce of Python isn‚Äôt in beating speed records. It‚Äôs in its knack for playing nice with super-optimized C libraries. These libraries are the muscle doing the heavy lifting, while Python‚Äôs more like the friendly coach guiding the process. So, your Python code might take a tiny bit longer to run a loop, but when it calls on these C libraries to do the real work, they zip through tasks at lightning speed. So, next time you hear someone knocking Python for being slow, maybe toss this thought their way. Python‚Äôs not just about the speed of typing out code; it‚Äôs about the overall speed and ease of getting stuff done, thanks to all those optimized libraries it wraps around so neatly."},
{"Title": "I made a SQL query builder in python", "Author": "u/deepCelibateValue", "Content": "It's a bit silly, but it was a good exercise. https://github.com/sebastiancarlos/yas-qwin"},
{"Title": "Sunday Daily Thread: What's everyone working on this week?", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Anyone used match case yet?", "Author": "u/samarthrawat1", "Content": "What are your thoughts on it."},
{"Title": "AbstractAPI Python SDK", "Author": "u/ebram96", "Content": "I'm excited to share about my open source Python library I created to help with using AbstractAPI services. What can you do with the library? It supports using all current AbstractAPI services: - Email validation - Phone number validation - IP geolocation - Lookup holidays of a country on a specific date or date range. - VAT validation/calculation/categories - IBAN validation - Exchange Rates live/conversion/historical rates - Lookup company data using only its domain - Timezone current/conversion - Avatars generation - Website screenshot - Website scrape - Image processing It supports Python >= 3.9 and it is tested on all environments from 3.9 up to 3.12. Tests are run automatically on all supported Python versions by a CI workflow. Check it and maybe give a star? :) Repo: https://github.com/ebram96/AbstractAPI-Python-SDK Doc: https://abstractapi-python-sdk.readthedocs.io/en/latest/index.html Package: https://pypi.org/project/abstract-api/ Contributions are very welcome."},
{"Title": "Leapcell: A Better Alternative for Heroku + Airtable for Python", "Author": "u/OfficeAccomplished45", "Content": "We are thrilled to announce the official launch of Leapcell's Beta public testing. Leapcell: https://leapcell.io/ Leapcell is a Data & Service Hosting Community, providing an application hosting experience comparable to the convenience of Vercel. Additionally, it features a high-performance database with an Airtable-like interface, streamlining data management. The entire platform is fully managed and serverless, enabling users to focus on specific business implementations without dedicating excessive time to infrastructure and DevOps. For more information, please refer to https://docs.leapcell.io/ Our goal is to empower users to concentrate on specific business implementations, allowing more individuals (Product Managers, Marketing professionals, Data Scientists) to participate in content creation and management without spending too much time on infrastructure and DevOps. Here's a Flask example: https://leapcell.io/issac/flask-blog , which contains a database and an application. For documentation on deploying Flask projects, check this link: https://docs.leapcell.io/docs/application/examples/flask . Deploying other projects is also straightforward. Leapcell is currently in beta testing, and we welcome any feedback or questions."},
{"Title": "Apache OpenDAL‚Ñ¢, a Rust library with Python-bindings for accessing data freely, is now graduated and in search for GSoC 2024 projects!", "Author": "u/simonsanone", "Content": "(Disclaimer: Just a bystander of that project, but I feel it deserves some love.) openDAL is a data access layer that allows users to easily and efficiently retrieve data from various storage services in a unified way. It is written in Rust and has bindings for Python available on pypi: https://pypi.org/project/opendal/ Storage services include webdav, ftp, ipfs, redis, rocksdb, s3, gcs, azblob, gdrive, dropbox, onefrive, memcached, and many more. There are also bindings in the works for C, C++, Haskell, LUA, Ruby, Swift, and Zig. With libraries for Java, Node.js, Python already being released. They are also looking for project ideas for the Google Summer of Code in their discussions, so chime in, if you have a good idea, want to help to make the Python-bindings better or want to be a mentor: GSoC 2024 Projects"},
{"Title": "If you are on Linux and work with virtual environments, consider adding this to your .bashrc", "Author": "u/munabedan", "Content": "venv() { # Check if already activated if [[ \"$VIRTUAL_ENV\" != \"\" ]]; then echo -e \"\\n\\e[1;33mDeactivating current virtual environment...\\e[0m\" deactivate return fi # Check if the venv directory exists if [ -d \"venv\" ]; then echo -e \"\\n\\e[1;33mActivating virtual environment...\\e[0m\" source venv/bin/activate else echo -e \"\\n\\e[1;33mCreating and activating virtual environment...\\e[0m\" python3 -m venv venv source venv/bin/activate fi } Now when creating a python project, just go into the folder and call $ venv It should create a virtual environment with a folder named venv, if it exists it will activate it and if already activated it will deactivate it. For reference, here is a link to the script on github: https://gist.github.com/munabedan/6a5e8c104228943a461095a9e103a5af"},
{"Title": "Playing around with Ultra HDR", "Author": "u/albertzeyer", "Content": "This is the ultrahdr.py script , which generates this HDR-demo . (Check gregbenzphotography.com/hdr/ whether you can display HDR properly.) High-dynamic range (HDR) is to extend the usual color range (Standard Dynamic Range (SDR)) and usually also extends the common 8bit color depth to 10bit or more. https://en.wikipedia.org/wiki/High_dynamic_range Some modern displays (~2021) (e.g. MacBook M1, some OLED TVs) support HDR, but it is still a rare feature. There are multiple formats for HDR images, e.g.: OpenEXR AVIF JPEG XT embedded in JPEG XL JPEG XR Ultra HDR (used here) embedded in standard JPEG Ultra HDR uses the JPEG multi-picture format (MPF). It stores the normal SDR JPEG image as the first image, so all existing JPEG decoders can display the normal image. Then it stores a HDR gain map embedded in MPF which can be used to reconstruct the HDR image. Currently, (end of 2023), Google Chrome stable (end of 2023) supports this format. (Another alternative in Google Chrome is AVIF.) (Firefox currently does not support it.) Currently, (end of 2023), Google Pixel phones can capture Ultra HDR images (e.g. when they use night mode). (Note, many websites, e.g. Twitter, will reencode JPEGs after you upload them, and often they don't support Ultra HDR yet, so then it will be lost, and you will just see the normal SDR JPEG image.) About the Ultra HDR format: https://developer.android.com/media/platform/hdr-image-format This document defines the behavior of a new file format that encodes a logarithmic range gain map image in a JPEG image file. Legacy readers that don't support the new format read and display the conventional low dynamic range image from the image file. Readers that support the format combine the primary image with the gain map and render a high dynamic range image on compatible displays. To use the simple script here, for preparation: First, build this: https://github.com/google/libultrahdr Make sure FFMpeg is installed This script does nothing fancy: It just upscales the input JPEG color range (FFmpeg does that here currently) and then encodes the HDR gain map using Google's Ultra HDR encoder (libultrahdr). The effect is that the image will display brighter on HDR displays. Some related issues: https://github.com/ImageMagick/ImageMagick/issues/6377 https://github.com/libvips/libvips/issues/3799"},
{"Title": "pathit - Just show me my PATH in way I can read it", "Author": "u/iamevpo", "Content": "Got tired of scrolling through my PATH variable and decomposing it with something like echo $PATH | tr \":\" \"\\n\" , so I wrote micro package to show the path by line, sort it alphabetically and extract lines of interest. Question Answer What's on my PATH? pathit show or pathit raw Sort alphabetically pathit show --sort Paths with mingw ? pathit show --includes mingw New content PATH without invalid dirs pathit show --purge --string Maybe a better tool exists, but hope it is a useful utility - or at least a demo. Packaging done with `poetry` and few tests added. Code, bash equivalents and install instructions: https://github.com/epogrebnyak/what-the-path Update: thanks to feedback from this thread, the options in pathit become the following. Note --string in specific - it allows to form a string that you can later use in your shell start file. --sort     --no-sort    Sort output alphabetically. --includes TEXT --excludes TEXT --purge    --no-purge   Exclude invalid directories. --expand   --no-expand  Expand environment variables if found inside PATH. --string   --no-string  Print a single string suitable for PATH content. --display-numbers       Indicate directory order in PATH. --color                 Use color to highlight errors. Update 2: package renamed justpath and available through pip install justpath now. Also can display your PATH as JSON now."},
{"Title": "FastCRUD - Powerful CRUD methods and automatic endpoint creation for FastAPI", "Author": "u/igorbenav", "Content": "Hey, guys, for anyone who might benefit (or would like to contribute) FastCRUD is a Python package for FastAPI, offering robust async CRUD operations and flexible endpoint creation utilities, streamlined through advanced features like auto-detected join conditions, dynamic sorting, and offset and cursor pagination. Github: github.com/igorbenav/fastcrud Docs: igorbenav.github.io/fastcrud/ Features: - Fully Async: Leverages Python's async capabilities for non-blocking database operations. - SQLAlchemy 2.0: Works with the latest SQLAlchemy version for robust database interactions. - Powerful CRUD Functionality: Full suite of efficient CRUD operations with support for joins. - Dynamic Query Building: Supports building complex queries dynamically, including filtering, sorting, and pagination. - Advanced Join Operations: Facilitates performing SQL joins with other models with automatic join condition detection. - Built-in Offset Pagination: Comes with ready-to-use offset pagination. - Cursor-based Pagination: Implements efficient pagination for large datasets, ideal for infinite scrolling interfaces. - Modular and Extensible: Designed for easy extension and customization to fit your requirements. - Auto-generated Endpoints: Streamlines the process of adding CRUD endpoints with custom dependencies and configurations. Improvements are coming, issues and pull requests always welcome github.com/igorbenav/fastcrud"},
{"Title": "Patching pydantic settings in pytest", "Author": "u/rednafi", "Content": "TIL: Figured out a way to patch pydantic settings to remove dependencies on environment variables while running unit tests. https://rednafi.com/python/patch_pydantic_settings_in_pytest/"},
{"Title": "Python version of the endless \"brick breaker\" game", "Author": "u/kvyb", "Content": "Decided to make a simple version of an endless \"brick breaker\" type game in pygame, where two balls compete to color each others' domain area squares for eternity. Feel free to contribute: https://github.com/kvyb/endless-game , there is much that can be improved in my opinion, especially collisions."},
{"Title": "PyCon Pakistan 24", "Author": "u/adilfarooq", "Content": "PyCon is happening this March 9th & 10th at LUMS, Lahore, Pakistan. The early bird discount is available till Jan 31st. For more detail, stay updated through PyCon Pakistan‚Äôs social media pages and website https://pycon.pk"},
{"Title": "Custom Wallpaper System with Quotes & Weather", "Author": "u/the-chris-xd", "Content": "Hi, I made a program that automatically sets up a wallpaper on my desktop with a random quote & weather. It does so by loading a image (background) and fetching quotes and weather (as per location entered in the city var) and sets both on the image. Then it saves the temporary image and then puts it on my desktop. This all happens in a loop with interval of 1 hour. I also made a run.bat that runs this program with pythonw and then I put it in my Startup folder. I am just a beginner in Python itself so this project isn't all that good and may contain bugs/inefficient code, I'm sorry for that. If someone can provide feedback/ideas about what I can do with this project, it'll be helpful. Thanks in advance for that. Thanks for reading. Here's the project link: https://github.com/TheChrisGG/AutoPaper"},
{"Title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "Author": "u/AutoModerator", "Content": "Daily Thread"},
{"Title": "Simple XML generation from a Python data structure", "Author": "u/nicwolff", "Content": "I'm not a beginner now but I wrote this years ago because the xml.etree.ElementTree and lxml.builder.ElementMaker APIs are too verbose and don't compose well. It's just one function, XML , which takes a Python sequence. The first item is the tag name, the second is an optional dict of attributes, and the rest are strings or nested sequences. If the tag name is None the content replaces the tag. https://pypi.org/project/xml-from-seq/ https://github.com/nicwolff/xml_from_seq from xml_from_seq import XML, INLINE item = [ 'item', {'attr': 123, 'attr_2': None}, 'This is some content of the item.', ['b', INLINE, 'This will be bold and not indented.'], [None, 'This will not be bold.'], ] print(XML(item)) <item attr=\"123\"> This is some content of the item. <b>This will be bold and not indented.</b> This will not be bold. </item>"},
{"Title": "ezgpt - An easy and intuitive interface for OpenAI's GPT API", "Author": "u/Ascyt", "Content": "For a while now I've been using OpenAI's GPT API instead of ChatGPT because it provides so much more control over things and also allows access to GPT-4 while being much cheaper overall with pretty much no rate limits. I made my own Python library that builds on top of OpenAI's openai library, and makes interacting with it much easier. For example, you can just use ezgpt.get(user='Your prompt') to get the response. Most of my effort went into the conversation feature though - it makes it easy to chat, edit, save and load the conversations. To use the conversation, simply import ezgpt and run ezgpt.c() , which, in my case, I have put into a python file which gets run by a .bat file, so I can easily run it from anywhere. Check it out here: https://pypi.org/project/ezgpt GitHub Repository: https://github.com/Ascyt/ezgpt"},
{"Title": "Classical vs. London schools of unit testing", "Author": "u/szymonmiks", "Content": "Test examples: https://github.com/szymon6927/szymonmiks.pl/blob/master/blog/examples/tests/test_classical_vs_london/test_transaction_processor.py My latest blog post delves into the Classical and London schools, offering practical Python examples. https://blog.szymonmiks.pl/p/exploring-different-schools-of-unit-testing-in-python/"},
{"Title": "This is not interview advice: a priority-expiry LRU cache without heaps or trees in Python", "Author": "u/genericlemon24", "Content": "I wrote an in-depth tutorial on a bIG TEch CoDINg InTerVIEW problem, but for fun I restricted myself to the Python standard library only. With a bit of exploration, the solution ended up comparable with the theoretically correct one (which, it turns out, is not the fastest anyway). Link: https://death.andgravity.com/lru-cache"}