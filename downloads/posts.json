[
{"Title": "One of Us:  For YEARS She Recorded Everything on TV", "Author": "u/giantyetifeet", "Content": "No content"},
{"Title": "Archiving ~500 raw comic books", "Author": "u/AndroidAssistant", "Content": "I'm trying to digitize my father's ~500 comic book collection before his birthday in August so he can read them on his tablet. He gifted them to me a while back because, while he wouldn't admit it, his eyes are starting to go, and he couldn't enjoy them anymore. With the tablet he can zoom in as far as needed so I know he would love to revisit the comics once they are in digital form. Everything is in pristine condition with sleeves and backers, but there is nothing insanely valuable in the collection. He has a couple that are worth something, but most sit at >$10. That said, it would crush him if any were damaged so the scanning method must be non-destructive. What I have tried: Czur Aura Pro Super quick, but random cropping and glare for days, even using the side lights. I tried using it with the official light box, outside the box, with 1-5 additional lights, with only external lights at every angle known to man, in a diffusion light box, with polarizers, etc. The only thing that worked 100% of the time was scanning while sitting on my back porch on an overcast day. I opened a ticket with their support and the response was \"Make your room light bright enough, but do not have direct light shining on the book.\". They didn't address the cropping issues. Epson Perfection V600 Epson Scan - Impressive for OEM software but inconsistent and changing the framing is a pain unless things are at perfect right angles. VueScan - Sometimes it decides just not to scan, sometimes it scans and the image is black. When it does scan it does fine, but the auto skew is ~90% accurate which may as well be 0 because I have to adjust every single picture. Silverfast - I was surprised at how much better the scans look, but the scans take 2-3 times as long as the Vue or Epson software. Also I have to take a prescan every time I move the page? At this point I'm ready to hire a service if anyone has any suggestions, but I would really prefer to do the scanning myself because I plan on at least completing the series that he has, and probably expanding on them. I do have an R10 and an M50, so I could try some sort of diybookscanner.com solution, but that is a lot of effort for something that may or may not have different results."},
{"Title": "NAS build... LSI HBA?", "Author": "u/TheRealDarkloud", "Content": "Hey all! I am looking to build an 8 bay NAS very soon with these drives, https://www.ebay.com/itm/156046813385 And I'm trying to make sure I'm buying the right HBA... https://www.ebay.com/itm/156046813385 It's clear to me that I need to read up more on ZFS, but am I to understand that with freenas or unraid, I can use ZFS to combine the 8 drives similar to RAID configurations? I am wanting to have some redundancy as well, similar to RAID mirroring. Off to research ZFS a bit more!"},
{"Title": "What's most efficient and automated way to make copy of storage?", "Author": "u/YousureWannaknow", "Content": "So.. I managed to get my first backup for storage (tight on budget and stuff, and quite specific usecase where CD/DVD dominated HDDs, anyway) and I can't find one opinion about making it copied between two. No it's just cold storage drive I use when I need to access specific stuff, but mostly add to it, so there's no need to make sync after I will make copy of it (I'll just be adding on two different drives same stuff from another storage).. What I found is.. It's reccomended to use software focused on that area of file operations. There are decent open source or free tools.. I have dilemmah wich of these I should use, but maybe there's some hidden gem you could recommend? Rsync - focused with good opinion, but commands line and no idea about speed FreeFileSync - GUI, rest as above did - I know that guy, I know it's missing a bit in speed but is quite tidy and could backup everything from one to another, right? It's just rookie thing, about 4 or 5 Tb and 2 external drives connected via USB 2.0 (probably, tho drives use 3.0, but not device I want to use for it).. And I wonder what way will be best to make copy in less than few days ðŸ˜… sorry for language flops, I'm kinda messed recent days"},
{"Title": "Seeking advice regarding large storage options whether internal or external", "Author": "u/__Y8__", "Content": "I somewhat recently built a PC with one 2TB SSD (Samsung 980 Pro). With all my games, schoolwork, and content creation, I'm beginning to reach the limits of my SSD. I also have a 1TB external Sandisk SSD, but that's meant for my MacBook (I run Final Cut Pro off of it. Can't use FCP on PC unfortunately). I'm essentially looking for the best value regarding large storage, longevity, and decent read/write speeds. Max budget is $200 but preferably $100ish. I don't mind if the storage device is internal or external. I have an Asus Prime Z790-A WIFI motherboard. Despite building my PC, my knowledge regarding them is fairly low. Any advice is greatly appreciated."},
{"Title": "Does my enterprise HDD enclosure need a fan?", "Author": "u/Top_Combination9023", "Content": "I barely know anything about data hoarding, but I want to store PDFs, games and music outside my laptop and flash drives aren't enough anymore. After some research here I'm going to buy a 16TB Exos (I can't afford a whole 3-2-1 setup). I just need the one so I was thinking an enclosure, but I saw mixed opinions on whether my enclosure needed a cooling fan. What would happen if I bought one without?"},
{"Title": "SATA HDD/SSD diagnostic tools? HW or SW?", "Author": "u/trapexit", "Content": "After just having a drive failure (of course it happened while transferring data to a new drive) I was wondering if there was any tooling around that perhaps data recovery folks leveraged to help diagnose issues. I've got two drives right now (8TB Seagate Archive drives) that died suddenly. One seems like it could be a bad logic board and I do have another with not just the same general board but the same batch? (same qr code) but haven't transferred the ROM between the boards to see if that will address it. The other (the one that died today while trying to transfer data to a new drive) will spin up but then makes a sound like the heads are seeking twice and then spins down. As I understand that's likely a hardware problem and not as home fixable. While the data one these drives aren't critical it would be nice to be able to better diagnose the problems to see if it is worth the hassle to try to recover them."},
{"Title": "Can you not download age restricted videos on mp3juices!", "Author": "u/MichaelAftonXFireWal", "Content": "So I use mp3juices to download my videos and I've been trying to download an age restricted video but every time it keeps saying error so I was wondering does that mean I can't download age restricted videos off of mp3juices?"},
{"Title": "Is EaseUS Partition Surface Test Reliable?", "Author": "u/chineke14", "Content": "Just wanted to see if anyone has experience with using Easeus for surface test? I used HD Sentinel in the past, but figured I try Easeus in case it's faster. But in doign that, I am wondering if it's surface test is just as good as HD Sentinels?"},
{"Title": "When an external hard drive dies, what is your strategy?", "Author": "u/DrWho345", "Content": "When Samsung and WD drives start acting up, it will be working for ages, then one day, it will be connected as usual and then, no matter what you do, it won't mount, you can feel it running in your hands, but nothing shows up on your computer, you disconnect, reconnect, try to mount it via command/terminal absolutely nothing is working... what are your strategies/next steps? I have a few that have died/stopped working? and don't know what to do now. I have a Mac by the way, and an Windows 11 NUC running MacDrive, I have connected the drive to both, and have had nothing at all. Edit: when I connect it, I can feel it running when I touch it, but it doesnâ€™t mount at all, I canâ€™t backup anything because I canâ€™t see anything when it is connected."},
{"Title": "Any advice on LTO-6 drive models (2nd hand on eBay) to look out for?", "Author": "u/freshairproject", "Content": "Which LTO6 drives do people usually recommend for Windows 11? I have 20tb that Iâ€™d like to back up in addition to other sata drives. Do used tape drives go bad? Like does it have a certain amount of writes before the magnet goes bad?"},
{"Title": "Is this the right subreddit to be?", "Author": "u/Ok_Pin1735", "Content": "I tend to hoard \"knowledge findings\" from social media that I find valuable or want to implement to improve my life. Often, when I come across these findings. However, I never store them; I don't prioritize backups, I just dont care. I simply like or save findings within my respective accounts. But the inability to search through my scattered findings when I need them or to categorize them has been a frustrating pain point, especially since I only actively recall about 5% of what I save when I get the nudge to use something. Does anyone related?  Recently, I started organizing my findings using r/ObsidianMD , and it's been very helpful. I started with Youtube watch later playlists and I went from 1789 videos to 21."},
{"Title": "Building 2nd Array with existing DAS?", "Author": "u/braillegrenade", "Content": "I'll make this fast! I have: OWC 8-bay DAS RAID 5 via SoftRAID 8x 16TB drives in there now, about 70% full. Let's call this array \"Alpha\" Questions: Can shut it down, I pull those drives, put in 8 fresh ones and build a new array (\"Bravo\")? (pretty sure this is a yes) I presume swapping the array back to Alpha and starting up will just act like nothing happened ? What are the things to watch out for? Drives must go in the same drive slot? What might cause the array to not be recognized after removing and replacing? Thanks so much!"},
{"Title": "Helping a friend archive her deceased husbands digital life.", "Author": "u/SamTheManBrown", "Content": "Iâ€™m helping a friend who recently lost her husband, back up and consolidate all his data. Heâ€™s an editor so he had a lot of computers around the house but luckily he had work computers that I donâ€™t have to worry about. She wants me to just buy a big drive and copy everything from each computer onto the drive. Iâ€™m pretty tech savvy but I know itâ€™s going to be a little bit more complicated than that. I have 3 questions: Is there a good guide on the best way to about this as far as, using a driveâ€¦ cloudâ€¦ blu-ray. Is there a good step by step guide on how to do this? What brands do most people recommend?"},
{"Title": "(QNAP) Split external backup into individual parts?", "Author": "u/HugeFrog24", "Content": "Currently rocking a QNAP TVS-h874 with 2x WD Red Pro 24 TB mirrored as RAIDZ1. As I work on developing a robust backup strategy for my NAS, I wonder if QNAP offers a native solution that allows splitting backups across multiple external drives automatically, without manually dividing my directories into chunks of 4 TB each. The rationale is that a 24 TB backup cannot fit on a single 4 TB external HDD. This becomes even more apparent if I expand to a usable 144 TB in the future. In that case, it would be impossible to find a single 144 TB external drive, as such drives simply do not exist. So, is it possible to distribute backups across multiple external drives using QuTS hero 5? How exactly?"},
{"Title": "New Exos X14: HSMR or HM-SMR?", "Author": "u/SirLouen", "Content": "I've found this drive Exos X14 model ST10000NM0568 Here is the data sheet https://www.seagate.com/www-content/datasheets/pdfs/exos-x-14-channel-DS1974-5-1912GB-en_AU.pdf In the photo i can clearly read that HSMR https://preview.redd.it/new-exos-x14-hsmr-or-hm-smr-v0-djsejf5vk68d1.png But I can't seem to figure out which kind of disk are we talking about. It's H as in Hybrid or as in Host Managed? Not the same here, but is says that X14 used to be TDMR back in 2018 https://www.anandtech.com/show/12557/seagate-announces-exos-x14-a-heliumfilled-14-tb-pmr-hdd Looking at the past threads, it appears that many of these HDD end to be very fishy: https://www.reddit.com/r/DataHoarder/comments/tfm7w9/what_is_st18000nm013j_exos_x20z_hsmr_18tb_hdd/"},
{"Title": "Would it make sense to transocde these video files?", "Author": "u/Over_Variation8700", "Content": "So I have around 150 of these 4K video files with a bit rate ranging from 40 to 70 megabits per seconds and each file being exactly 4GB. However, I don't really need them to have the highest quality possible as they're all screen recording files but I might need them in the future. Would it make any sense to transcode to better format and reduce file size? 13700KF CPU+4070Ti GPU https://preview.redd.it/would-it-make-sense-to-transocde-these-video-files-v0-if5mtqi7n38d1.png"},
{"Title": "WD Red Old Stock being sold on Amazon", "Author": "u/jacraest", "Content": "I guess a warning and a bit of a wakeup call to check your drives and not to take sellers for granted on amazon. Bought a drive advertised as new and was manufactured in Dec 2016. Warranty expired in Dec 2019. 200 reallocated sectors within 40 on hours. Same - age WD Gold drive with 20k power on hours has no issues lol. This occured with the 3TB WD30EFRX model. Never had this issue on amazon before."},
{"Title": "All of my data storage mediums, if you want I can update every Friday I get a new thing to put on the wall as a weekly thing", "Author": "u/LaundryMan2008", "Content": "No content"},
{"Title": "On my way, first pair of 12TB drives up and running", "Author": "u/09Klr650", "Content": "Purchased (2) 12TB drives from serverpartdeals.com.  Just one would hold everything I have in a motley collection of drives. Impressively they came WITH both a pin3 disabled power cable AND a card with a little kapton tape and instructions. As I am using a Sabrent 5-bay (eBay find, decent price for a non-raid external) I used the tape to disable the pin3 reset. Discovered I need to cut back on the caffeine as my hands were shaking WAAAY to much to make that easy. But it is up and running. Figure I will run these two as regular or raid 1, then once I order two more go raid 5. Now to spend some of the weekend using an external SATA dock to try to reclaim files from a stack of old bare drives, then my misc external drives. Suspect this will take a while. Now to research software raid programs . . ."},
{"Title": "Largest SSD is 1,000 TB in 3.5\" size, why not bring 5.25\" mechanical drive?", "Author": "u/Warcraft_Fan", "Content": "The largest hard drive is 30TB but if they would use 5.25\" drive then it'd be possible to get some hundred TB in a standard 5.25\" half height (same as CD and DVD drive) Computer cases are still produced with 5.25\" support as some people still need optical drive or possibly tape drive. So what's keeping them from putting out massive 5.25\" hard drives?"},
{"Title": "Are external hard-drives more prone to failure than internal hard-drives WHEN properly cared for and maintained? + Decision making for new backup storage upgrades advice wanted", "Author": "u/celexaprozac", "Content": "I have a 5TB external Seagate One Touch Rescue Edition that's just sat on top of my tower PC for as long as I've had it as my primary backup method, obviously insanely important materials are still in the cloud or spread across numerous flash drives. However, having only one backup copy of my shit is not a recipe for success. Having one is as good as having none as I like to say, redundancy is crucial when it comes to hoarding my data. Important pictures are backed up, the most important of which are printed out. My consideration here is whether or not I get a high capacity internal HDD @ 7200 RPM or if I buy a high capacity portable HDD that can take abuse and is less likely to be damaged by physical adjustments and movements. This is rarely a concern in my day to day but I live in an earthquake prone area and I would agonise over losing everything to some shaking dirt. I can't do both due to financial constraints at the moment but the decision is as follows; Do I get a second external USB HDD that will be slower and have inferior transfer rates BUT be more protected in event of disaster and any other events that may happen (dog with zoomies for instance.) or Do I get a more efficient and practical internal HDD (I imagine its also cheaper) that I suspect to be significantly more fragile and susceptible to sudden movements and will likely be lost in the aforementioned events. If the internal is selected, should I acquire this before the sale ends? If I should go with an external what should I acquire? I'd like something larger than my current 5TB. I haven't used more than 1.1TB on it but I'd rather kick the can of running out of space a few more years down the road."},
{"Title": "Honey, I Shrunk My Server", "Author": "u/sylinen", "Content": "No content"},
{"Title": "LSI 9207-8i setup", "Author": "u/Mooman5", "Content": "Hey everyone Iâ€™m currently trying to get the LSI 9207-8i to work in my home unraid server. Iâ€™m using it with 4 hba 4tb hddâ€™s, and Iâ€™m having a lot of trouble. The card is detected in my bios and in unraid. Iâ€™m using an asrock b560m-c motherboard. This is my first time trying to make any kind of server, so please have mercy on me. Any advice is greatly appreciated."},
{"Title": "Is microsd good enough as read only storage?", "Author": "u/PRINNTER", "Content": "I am in possesion of a samsung pro ultimate 256gb microsd, And it was unused, so I am in the process of downloading entire wikipedia in my native launguage (polish) with every picture/video and audio file included and hosting it online. My questions are: Is a microsd that stays plugged in and isn't unplugged/plugged back in good enough for a read only sceniario? (so the pins on the microsd aren't wearing out) Are there any implications, like \"after reading x gb from a microsd it will just stop working\"? Note: I am aware, that wikipedia has already many mirrors hosted online, but It's a learning experience for me (both on data hoarding and programming because I need to write code to host the copy of wikipedia online)."},
{"Title": "Best option for a \"go bag\" external?", "Author": "u/Vietname", "Content": "I have a couple of small synology NAS's, each has a dedicated drive for regular backups, but i wanted to get a small external drive (ideally SSD) to put my really important data on for emergencies, e.g. the house is on fire and i can only grab a few things. What would you recommend?"},
{"Title": "Feasibility/Performance of Mini-PC NAS with external HDD/SSD enclosure?", "Author": "u/RXrenesis8", "Content": "Howdy r/DataHoarder ! I am still a little baby hoarder with only about 15-20TB of data floating around my noisy 10 year old desktop-turned-HTPC. I was looking to expand to ~30TB (with room to grow of course) for a couple of initiatives and initially had been set on a rack and rackmount server or NAS chassis with a bunch of 3.5\" bays for price-efficient spinning drives. The cost though! Racks = $$$ Rackmount Chassis or pre-builts = $$$$! Every penny I would save in drive capacity I would pay back in getting a nice (home-office) rack and NAS. So I've come to you questioning a pivot point: What do you think about smaller solutions? Specifically I am thinking about a Mini-PC running unRAID connected to a JBOD enclosure for disks or large/cheap SSDs. What's the performance difference between a setup like the above and a low-end rackmount setup (which is at the limit of what I could realistically afford)? Any pitfalls I should be aware of? Thanks in advance!"},
{"Title": "dm-vdo block-Level deduplication and compression merged into Linux 6.9", "Author": "u/LeichenExpress", "Content": "No content"},
{"Title": "Is it possible to make mergerfs only log in the account that is going to be used?", "Author": "u/tamashika", "Content": "I need help to understand the policy of mergerfs to better fit my needs. I'm using a free cloud storage that does not let a user to have multiple free accounts. I think limiting the log in time would make my accounts less likely being detected, especially not logging in at the same time. I have checked that rclone union would have to log into all accounts when something is being transferred. I wonder if mergerfs also has to do the same. The reason I thought we don't have to log in to view the content is based on the idea from Virtual Volume View (VVV) , which catalogs the content of removable volumes like CD and DVD disks for off-line searching. Would like to know if mergerfs could do the same or similar thing. Thanks."},
{"Title": "I've got a decent sized physical media collection and unsure if it's worth saving lossless copies of it all", "Author": "u/TripleXero", "Content": "I've spent a stupid amount of time backing up all my DVDs and Blu-rays but I've already capped off an 11TB hard drive and split the extras between drives meant for other stuff and I'm still not done. I don't plan on getting rid of the original physical copies but it's much easier to access them digitally. I should also mention what I have backed up has been shrunk down already to decent looking MP4s to shove on iTunes for sleeker organization but I just don't know if it's worth keeping the original 1:1 files after. I shrunk the original files down once and realized after the fact that it wasn't in a flexible or presentable way for some so I'm hesitant to try again or wipe them completely after all the work. Is there maybe a good way to compress them? I've already went through and removed all non-English audio and subtitles"},
{"Title": "Should I use cold cloud storage for small backup (4TB)?", "Author": "u/CreativeDog2024", "Content": "I know about the 3-2-1 rule, but I've not been implementing it. I'm a college student with 3 years and 4 TB worth of movies/TV shows that have taken DAYS to download at times. I have 2 local backups, one on a segate 5tb and the other on a WD 5tb. I don't want to lose these files, I've used a plex add on to export posters and a list of my media but if I were to ever try and get them back from scratch, it would take a long time and I'm not sure I'd ever dedicated that much time. I know about aws deep glacier as a last resort and it being pretty high cost to recover, but I don't really plan to ever recover unless I both of my local backups. What should I know about? What would you do?"},
{"Title": "ServerPartDeals customer service", "Author": "u/demitrixrd", "Content": "I know many people on here already recommended SPD as a source for fair priced disks. I just wanted to add my $.02 after a recent experience with a failed drive I'd purchased a few months ago. A week ago I woke up to a TrueNas notification that a drive was faulted. I did some troubleshooting and wasn't able to recover any kind of connection to the drive. Plugging it in to a external adapter resulted in what sounded like a rock tumbler, so clearly it was beyond rescue. I started a RMA with SPD, and went back to work; expecting a few hours delay before hearing anything back. Later I realized I'd missed their almost immediate reply to the RMA request. I explained there was no way provide smart test results, and attached a video of my new rock tumbler. In short order the RMA# was provided. I shipped the faulted drive, and didn't think much of it. The drive was delivered yesterday and my replacement shipped this morning. Long story short, big kudos to ServerPartDeals for what is unprecedented customer service in the modern world; and to all the unsure shoppers, spend your money with SPD."},
{"Title": "Get WD Purple for DIY NAS in Raid? Located in India", "Author": "u/The_Bipolar_Guy", "Content": "Building my first DIY NAS. Need 2+2 tb to start off with. Trying to be as cheap as possible. Will be storing all memories and irreplaceable things. Will get an external HDD for offsite backup. Also, I do not care about read write speeds. I just want my data to be safe and backed up at the end of the day for as cheap as possible, I will add new data maybe once a month if not even more rarely. I will be using my old motherboard (6 SATA ports) and processor. Will get a new 500 GB ssd for OS and new RAM. Now here is the dilemma. I am from India and WD Purples are selling for 5250 INR each, cheapest. Next up, WD Blue is also selling for 6000+ INR. I can technically afford (but want to avoid at all costs) to get a 10TB UltraStar or EXOS for 22000 INR but I need two physical drives at least and my total storage need for backup right now or even 3 years later will not exceed 4-5TB. so a 10TB HDD will be useless, not to mention spending 45K INR on drives only (which is VERY VERY EXPENSIVE FOR ME). I know WD Purple glosses over write errors and can be bad. Chucking is equally expensive, if not more. Should I just get 2x 2tb WD Purple for 10.5K INR and set up my backup server for now. Will add more HDDs later as needed (1-4 years later). Can I do anything to prevent the write errors? Also, if there is a cheaper way, especially in the long run, I would appreciate it. Currently I have ~2TB data which grows by like 200-400GB a year. I have thought of BluRay drives but I am unable to procure new BluRay drives cheap enough."},
{"Title": "VIA RAID disks question", "Author": "u/thenovum", "Content": "Hey. Is it possible to extract the data from via raid disk members without using a VIA RAID controler? Disks are from 2004."},
{"Title": "Looking for help as a vlogging nomad. I film all content in 4k60p and visiting every country in the world. Guesstimating I'll need an additional ~24TB of storage for my remaining 100 countries. Budget: hopefully <$10k?", "Author": "u/immranderson", "Content": "Hey! I've been living exclusively out of my backpack + carryon luggage for the last decade and have been filming my travels as I go along. I've been traveling around the world and have amassed a large collection of 2/4TB Samsung T-Series SSDs which is beginning to bump up against the physical carrying capacity in my bags. Lucky enough I've got a 128GB Pixel 1 that I use still to this day for daily remote storage + backup, so that has served me well. However, I also like to have local backups, and have been trying to shop around for SSDs. I'm trying to keep things as compact as possible, and I'm traveling so have avoided going the HDD route since my stuff can get pretty rough and tumbled quite often. I try not to edit directly off my external drives, and I've got an 8TB M-class macbook pro that I've been lugging around with me around the world. I'm trying to figure out how to handle my long term storage needs for the next few years of travel. On average, I'm capturing 100GB - 400GB per week of travel. I'm figuring I've got another 100 countries left to visit around the world, so 100 weeks * ~200GB per country maybe lands me in the ~20TB range of storage I'm anticipating I'll need for the remainder of my travels? Does anyone have any suggestions here? Could I get away with getting 2 x 16TB SSDs with external enclosures and that 32TB worth of capacity would generously cover my needs with wiggle room? If so, what SSD + enclosure combo would you recommend?"},
{"Title": "Migrate from Unraid to OMV, SnapRAID, mergerfs. How to keep hard links, that span between two drives?", "Author": "u/ChrisWreck", "Content": "I'm in the middle of migrating from Unraid to OpenMediaVault, where I'll be using Snapraid and mergerfs. I have 3 disks, one parity and two data drives. They're using XFS in Unraid. I want to use ext4 in OMV. I have formatted the parity drive to ext4, and planned to move data from disk 1 to the parity drive, then format disk 1 to ext4, and then move all data from disk 2 to disk 1 and make disk 2 the new parity drive for SnapRAID. However, I just realized I have hard links from the arr suite, spanning over both data disks ... How can I migrate and preserve the hard links? Preferably without having to move all data to one disk in Unraid before migrating. EDIT: For those of you wondering why I'm switching, I'm actually switching from Unraid to Proxmox, but I'll use OMV to handle my disks (combination of ZFS and ext4) and shares."},
{"Title": "Question about Pixiv bulk downloading", "Author": "u/_izix", "Content": "Has anyone been blocked or banned from bulk downloading from pixiv? I've seen rumors that it may happen, but as far as I've been able to determine, there is no evidence of it actually happening. Is there maybe a hidden limit that's very high? I plan to download all images from users I follow with a script to update my archive with any new images every couple days. So far I have downloaded a few thousand images with no issues. Just figured I'd see if anyone has evidence of that happening before I continue. EDIT: I am using gallery-dl"},
{"Title": "Software for organizing manual backups over the last 10 years?", "Author": "u/PrivateAd990", "Content": "What software is available (paid or free) to help analyze my data on an external HD? it's only about a 1GB but 20+ backups (manually copied files over the years to this HD). MacOS or Linux. Wants: find data by extension (file type) find largest files identifying duplicates and handling it manually Accepting other tips of how to sift through data. I plan to organize all data to one folder rather than 20 backup folders."},
{"Title": "Seattle video store says it needs to raise $1.8M or face possible closure", "Author": "u/justreddit2024", "Content": "No content"},
{"Title": "Best free data integrity tools for validating.", "Author": "u/Captain_Starkiller", "Content": "Can anyone recommend some good tools for validating data integrity as a defense against bit rot?"},
{"Title": "What's the best way to transfer all my family's iCloud backups and device storages to an external media server?", "Author": "u/sav-tech", "Content": "Currently, the data is on an iMac Mid 2011, Google Photos and iCloud. We also have spare devices that we want to recycle. Before doing that, we want to save the data on there and store it externally. I am a tech-enthusiast by nature and think that this would be a good opportunity to setup a home lab. I understand it may take some effort, but I am thinking of having folders for each person in our family and their data goes into an encrypted folder to access remotely via Mobile, PC and Samsung Smart TV..."},
{"Title": "Trying To Digitize Old Cassette Tapes", "Author": "u/boosterbear", "Content": "Hello all! I have been hunting for the right location to ask my many, many questions. This may not be technical enough for this subreddit, but it seemed like the right place to go. I'm a big fan of physical media, likely casual to most here but to my friends I am perceived as intensely pro-physical media. As such, whenever people have spare tapes, CDs, DVDs, etc etc, I'm the man they throw them at. Unfortunately, I am horribly unfamiliar with the digitization process for everything except for CDs and DVDs, and even then I occasionally have hiccups. Recently I purchased the JVC RC-EZ38S CD Portable System (link to user manual) from a flea market to play some of my tapes and CDs, and realized I had a few tapes I'm unable to find anywhere online. Usually I wouldn't worry about my tapes growing old from wear because I can download songs and save them that way, but these tapes (mostly Halloween tracks) were impossible for me to find elsewhere, so I've been trying to preserve them. The JVC product I purchased plays CDs, cassette tapes, and the radio. It has one single 3.5mm jack, a headphone output. I have done some googling, and found my best bet to save the audio is through a combination of cables and Audacity. Unfortunately, my computer only recognizes my aux cord as headphones, and I cannot treat my JVC product as a microphone when using Audacity. I have two cables - one aux with two 3.5mm ends, and one cable that has a male 3.5mm on one end and the other end has two male parts, white and yellow RCA jacks. How, if at all, can I use my JVC player to preserve the tapes I have? Is there a special cord combination I may be able to put together that won't put me out of house and home (I'm unemployed and in a somewhat difficult spot financially, even a $20 purchase has me aghast sometimes) or would I be better off looking for a different product to record my tapes? A friend of mine is currently looking to rehome an old car radio with cassette player - Do those typically have RCA plugs, and would that be a way to go about this? Anything helps, even just correcting my terms so I can communicate what I'm looking for a little better - I'm in a space where I truly don't know what it is that I don't know. I'd love to be a part of saving some lost media, even if it seems a little silly. People put work into those Halloween tapes, dammit!"},
{"Title": "What are the best options for adding disks to my setup?", "Author": "u/StarLordOfTheDance", "Content": "Is it possible to transition my setup from mergeFs+Snapraid to zfs without a lot of spare storage that I don't have? I currently have 2x4TB mergeFs setup, with a 4TB drive for Snapraid parity. (Total 8TB usable storage). - currently holding 7TB of data I have purchased 2x4TB more drives. And ideally want to end up in a situation where I have 16TB of usable storage, with 1 parity. (12TB usable would be acceptable but not ideal). The mergeFs harddrives are connected to a Ubuntu server. And I am moving the whole lot to a new server running proxmox (that has a Ubuntu server running in VM). can anyone help me figure out if it's possible to migrate this to a ZFS pool that will do what I want. I also have looked at unRAID because it supports adding disks, but that is its own hypervisor. So are there alternative ways of working that can work inside a proxmox VM?"},
{"Title": "Backup software", "Author": "u/bhudzallmighty", "Content": "I currently have a dedicated optiplex running linux with nextcloud docker for my iphone data. I would like to expand this system to a DIY nas with a single 20tb hdd storage for backup . What is the best software to do so? I would like to be able to backup once a day. Can i back up the whole hdd? Or just folders? Thank you"},
{"Title": "I have question about ugreen enclosure", "Author": "u/Dismal_Award735", "Content": "No content"},
{"Title": "I need help identifying this hard drive, and where i can get the right cable.", "Author": "u/Chippomannen", "Content": "No content"},
{"Title": "Safest method to wipe out a drive without damaging it? I'm looking for paranoid-level shit.", "Author": "u/500xp1", "Content": "Looking for a method that makes it impossible to recover the wiped data."},
{"Title": "500,000 CDs", "Author": "u/Sliced_Apples", "Content": "Hello, I am working for a startup in the sports industry and we have recently come into the possession of about five hundred thousand cds with 20 year old sports footage. We are trying to train an AI model off of them so as such, they need to be digitized. I know a little about burning cds but not much. As I have been made aware, this would be â€œrippingâ€ and not burning. What would be the best way to go about doing this? What storage solution would be the best? Any advice is greatly appreciated. Iâ€™m happy to answer any questions as well."},
{"Title": "Best at-home manual photo scanner?", "Author": "u/Small_Vermicelli9655", "Content": "Looking to digitize a bunch of old family photos and was hoping for specific scanner suggestions. Thereâ€™s lots of conflicting information online and Iâ€™m not very proficient in this area so was hoping for some insight lol. Iâ€™ve heard automatic scanners can mark grooves on the photos and would feel more comfortable with a manual one as time is not a priority. Hoping to stay around a $500 price range and resell after use. Thank you in advance!!"},
{"Title": "Checking for Wiped SMART Data", "Author": "u/eakall", "Content": "So I recently got 35 2TB SSDs (mostly Samsung EVO 860/870) used and when checking the smart data for them I noticed very low power on hours.  The PoH (80-400hrs) is variable across the large set of drives and TBW is about 2-6TB for most of the drives. I was curious if thereâ€™s a way to check if the smart data has been reset? It seems a bit suspicious to me that these drives all are so low in PoH. Drives are also manufactured between 2020-2021 ish"},
{"Title": "Best option for a \"go bag\" external?", "Author": "u/Vietname", "Content": "I have a couple of small synology NAS's, each has a dedicated drive for regular backups, but i wanted to get a small external drive (ideally SSD) to put my really important data on for emergencies, e.g. the house is on fire and i can only grab a few things. What would you recommend?"},
{"Title": "Feasibility/Performance of Mini-PC NAS with external HDD/SSD enclosure?", "Author": "u/RXrenesis8", "Content": "Howdy r/DataHoarder ! I am still a little baby hoarder with only about 15-20TB of data floating around my noisy 10 year old desktop-turned-HTPC. I was looking to expand to ~30TB (with room to grow of course) for a couple of initiatives and initially had been set on a rack and rackmount server or NAS chassis with a bunch of 3.5\" bays for price-efficient spinning drives. The cost though! Racks = $$$ Rackmount Chassis or pre-builts = $$$$! Every penny I would save in drive capacity I would pay back in getting a nice (home-office) rack and NAS. So I've come to you questioning a pivot point: What do you think about smaller solutions? Specifically I am thinking about a Mini-PC running unRAID connected to a JBOD enclosure for disks or large/cheap SSDs. What's the performance difference between a setup like the above and a low-end rackmount setup (which is at the limit of what I could realistically afford)? Any pitfalls I should be aware of? Thanks in advance!"},
{"Title": "dm-vdo block-Level deduplication and compression merged into Linux 6.9", "Author": "u/LeichenExpress", "Content": "No content"},
{"Title": "Is it possible to make mergerfs only log in the account that is going to be used?", "Author": "u/tamashika", "Content": "I need help to understand the policy of mergerfs to better fit my needs. I'm using a free cloud storage that does not let a user to have multiple free accounts. I think limiting the log in time would make my accounts less likely being detected, especially not logging in at the same time. I have checked that rclone union would have to log into all accounts when something is being transferred. I wonder if mergerfs also has to do the same. The reason I thought we don't have to log in to view the content is based on the idea from Virtual Volume View (VVV) , which catalogs the content of removable volumes like CD and DVD disks for off-line searching. Would like to know if mergerfs could do the same or similar thing. Thanks."},
{"Title": "I've got a decent sized physical media collection and unsure if it's worth saving lossless copies of it all", "Author": "u/TripleXero", "Content": "I've spent a stupid amount of time backing up all my DVDs and Blu-rays but I've already capped off an 11TB hard drive and split the extras between drives meant for other stuff and I'm still not done. I don't plan on getting rid of the original physical copies but it's much easier to access them digitally. I should also mention what I have backed up has been shrunk down already to decent looking MP4s to shove on iTunes for sleeker organization but I just don't know if it's worth keeping the original 1:1 files after. I shrunk the original files down once and realized after the fact that it wasn't in a flexible or presentable way for some so I'm hesitant to try again or wipe them completely after all the work. Is there maybe a good way to compress them? I've already went through and removed all non-English audio and subtitles"},
{"Title": "Should I use cold cloud storage for small backup (4TB)?", "Author": "u/CreativeDog2024", "Content": "I know about the 3-2-1 rule, but I've not been implementing it. I'm a college student with 3 years and 4 TB worth of movies/TV shows that have taken DAYS to download at times. I have 2 local backups, one on a segate 5tb and the other on a WD 5tb. I don't want to lose these files, I've used a plex add on to export posters and a list of my media but if I were to ever try and get them back from scratch, it would take a long time and I'm not sure I'd ever dedicated that much time. I know about aws deep glacier as a last resort and it being pretty high cost to recover, but I don't really plan to ever recover unless I both of my local backups. What should I know about? What would you do?"},
{"Title": "ServerPartDeals customer service", "Author": "u/demitrixrd", "Content": "I know many people on here already recommended SPD as a source for fair priced disks. I just wanted to add my $.02 after a recent experience with a failed drive I'd purchased a few months ago. A week ago I woke up to a TrueNas notification that a drive was faulted. I did some troubleshooting and wasn't able to recover any kind of connection to the drive. Plugging it in to a external adapter resulted in what sounded like a rock tumbler, so clearly it was beyond rescue. I started a RMA with SPD, and went back to work; expecting a few hours delay before hearing anything back. Later I realized I'd missed their almost immediate reply to the RMA request. I explained there was no way provide smart test results, and attached a video of my new rock tumbler. In short order the RMA# was provided. I shipped the faulted drive, and didn't think much of it. The drive was delivered yesterday and my replacement shipped this morning. Long story short, big kudos to ServerPartDeals for what is unprecedented customer service in the modern world; and to all the unsure shoppers, spend your money with SPD."},
{"Title": "Get WD Purple for DIY NAS in Raid? Located in India", "Author": "u/The_Bipolar_Guy", "Content": "Building my first DIY NAS. Need 2+2 tb to start off with. Trying to be as cheap as possible. Will be storing all memories and irreplaceable things. Will get an external HDD for offsite backup. Also, I do not care about read write speeds. I just want my data to be safe and backed up at the end of the day for as cheap as possible, I will add new data maybe once a month if not even more rarely. I will be using my old motherboard (6 SATA ports) and processor. Will get a new 500 GB ssd for OS and new RAM. Now here is the dilemma. I am from India and WD Purples are selling for 5250 INR each, cheapest. Next up, WD Blue is also selling for 6000+ INR. I can technically afford (but want to avoid at all costs) to get a 10TB UltraStar or EXOS for 22000 INR but I need two physical drives at least and my total storage need for backup right now or even 3 years later will not exceed 4-5TB. so a 10TB HDD will be useless, not to mention spending 45K INR on drives only (which is VERY VERY EXPENSIVE FOR ME). I know WD Purple glosses over write errors and can be bad. Chucking is equally expensive, if not more. Should I just get 2x 2tb WD Purple for 10.5K INR and set up my backup server for now. Will add more HDDs later as needed (1-4 years later). Can I do anything to prevent the write errors? Also, if there is a cheaper way, especially in the long run, I would appreciate it. Currently I have ~2TB data which grows by like 200-400GB a year. I have thought of BluRay drives but I am unable to procure new BluRay drives cheap enough."},
{"Title": "VIA RAID disks question", "Author": "u/thenovum", "Content": "Hey. Is it possible to extract the data from via raid disk members without using a VIA RAID controler? Disks are from 2004."},
{"Title": "Looking for help as a vlogging nomad. I film all content in 4k60p and visiting every country in the world. Guesstimating I'll need an additional ~24TB of storage for my remaining 100 countries. Budget: hopefully <$10k?", "Author": "u/immranderson", "Content": "Hey! I've been living exclusively out of my backpack + carryon luggage for the last decade and have been filming my travels as I go along. I've been traveling around the world and have amassed a large collection of 2/4TB Samsung T-Series SSDs which is beginning to bump up against the physical carrying capacity in my bags. Lucky enough I've got a 128GB Pixel 1 that I use still to this day for daily remote storage + backup, so that has served me well. However, I also like to have local backups, and have been trying to shop around for SSDs. I'm trying to keep things as compact as possible, and I'm traveling so have avoided going the HDD route since my stuff can get pretty rough and tumbled quite often. I try not to edit directly off my external drives, and I've got an 8TB M-class macbook pro that I've been lugging around with me around the world. I'm trying to figure out how to handle my long term storage needs for the next few years of travel. On average, I'm capturing 100GB - 400GB per week of travel. I'm figuring I've got another 100 countries left to visit around the world, so 100 weeks * ~200GB per country maybe lands me in the ~20TB range of storage I'm anticipating I'll need for the remainder of my travels? Does anyone have any suggestions here? Could I get away with getting 2 x 16TB SSDs with external enclosures and that 32TB worth of capacity would generously cover my needs with wiggle room? If so, what SSD + enclosure combo would you recommend?"},
{"Title": "Migrate from Unraid to OMV, SnapRAID, mergerfs. How to keep hard links, that span between two drives?", "Author": "u/ChrisWreck", "Content": "I'm in the middle of migrating from Unraid to OpenMediaVault, where I'll be using Snapraid and mergerfs. I have 3 disks, one parity and two data drives. They're using XFS in Unraid. I want to use ext4 in OMV. I have formatted the parity drive to ext4, and planned to move data from disk 1 to the parity drive, then format disk 1 to ext4, and then move all data from disk 2 to disk 1 and make disk 2 the new parity drive for SnapRAID. However, I just realized I have hard links from the arr suite, spanning over both data disks ... How can I migrate and preserve the hard links? Preferably without having to move all data to one disk in Unraid before migrating. EDIT: For those of you wondering why I'm switching, I'm actually switching from Unraid to Proxmox, but I'll use OMV to handle my disks (combination of ZFS and ext4) and shares."},
{"Title": "Question about Pixiv bulk downloading", "Author": "u/_izix", "Content": "Has anyone been blocked or banned from bulk downloading from pixiv? I've seen rumors that it may happen, but as far as I've been able to determine, there is no evidence of it actually happening. Is there maybe a hidden limit that's very high? I plan to download all images from users I follow with a script to update my archive with any new images every couple days. So far I have downloaded a few thousand images with no issues. Just figured I'd see if anyone has evidence of that happening before I continue. EDIT: I am using gallery-dl"},
{"Title": "Software for organizing manual backups over the last 10 years?", "Author": "u/PrivateAd990", "Content": "What software is available (paid or free) to help analyze my data on an external HD? it's only about a 1GB but 20+ backups (manually copied files over the years to this HD). MacOS or Linux. Wants: find data by extension (file type) find largest files identifying duplicates and handling it manually Accepting other tips of how to sift through data. I plan to organize all data to one folder rather than 20 backup folders."},
{"Title": "Seattle video store says it needs to raise $1.8M or face possible closure", "Author": "u/justreddit2024", "Content": "No content"},
{"Title": "Best free data integrity tools for validating.", "Author": "u/Captain_Starkiller", "Content": "Can anyone recommend some good tools for validating data integrity as a defense against bit rot?"},
{"Title": "What's the best way to transfer all my family's iCloud backups and device storages to an external media server?", "Author": "u/sav-tech", "Content": "Currently, the data is on an iMac Mid 2011, Google Photos and iCloud. We also have spare devices that we want to recycle. Before doing that, we want to save the data on there and store it externally. I am a tech-enthusiast by nature and think that this would be a good opportunity to setup a home lab. I understand it may take some effort, but I am thinking of having folders for each person in our family and their data goes into an encrypted folder to access remotely via Mobile, PC and Samsung Smart TV..."},
{"Title": "Trying To Digitize Old Cassette Tapes", "Author": "u/boosterbear", "Content": "Hello all! I have been hunting for the right location to ask my many, many questions. This may not be technical enough for this subreddit, but it seemed like the right place to go. I'm a big fan of physical media, likely casual to most here but to my friends I am perceived as intensely pro-physical media. As such, whenever people have spare tapes, CDs, DVDs, etc etc, I'm the man they throw them at. Unfortunately, I am horribly unfamiliar with the digitization process for everything except for CDs and DVDs, and even then I occasionally have hiccups. Recently I purchased the JVC RC-EZ38S CD Portable System (link to user manual) from a flea market to play some of my tapes and CDs, and realized I had a few tapes I'm unable to find anywhere online. Usually I wouldn't worry about my tapes growing old from wear because I can download songs and save them that way, but these tapes (mostly Halloween tracks) were impossible for me to find elsewhere, so I've been trying to preserve them. The JVC product I purchased plays CDs, cassette tapes, and the radio. It has one single 3.5mm jack, a headphone output. I have done some googling, and found my best bet to save the audio is through a combination of cables and Audacity. Unfortunately, my computer only recognizes my aux cord as headphones, and I cannot treat my JVC product as a microphone when using Audacity. I have two cables - one aux with two 3.5mm ends, and one cable that has a male 3.5mm on one end and the other end has two male parts, white and yellow RCA jacks. How, if at all, can I use my JVC player to preserve the tapes I have? Is there a special cord combination I may be able to put together that won't put me out of house and home (I'm unemployed and in a somewhat difficult spot financially, even a $20 purchase has me aghast sometimes) or would I be better off looking for a different product to record my tapes? A friend of mine is currently looking to rehome an old car radio with cassette player - Do those typically have RCA plugs, and would that be a way to go about this? Anything helps, even just correcting my terms so I can communicate what I'm looking for a little better - I'm in a space where I truly don't know what it is that I don't know. I'd love to be a part of saving some lost media, even if it seems a little silly. People put work into those Halloween tapes, dammit!"},
{"Title": "What are the best options for adding disks to my setup?", "Author": "u/StarLordOfTheDance", "Content": "Is it possible to transition my setup from mergeFs+Snapraid to zfs without a lot of spare storage that I don't have? I currently have 2x4TB mergeFs setup, with a 4TB drive for Snapraid parity. (Total 8TB usable storage). - currently holding 7TB of data I have purchased 2x4TB more drives. And ideally want to end up in a situation where I have 16TB of usable storage, with 1 parity. (12TB usable would be acceptable but not ideal). The mergeFs harddrives are connected to a Ubuntu server. And I am moving the whole lot to a new server running proxmox (that has a Ubuntu server running in VM). can anyone help me figure out if it's possible to migrate this to a ZFS pool that will do what I want. I also have looked at unRAID because it supports adding disks, but that is its own hypervisor. So are there alternative ways of working that can work inside a proxmox VM?"},
{"Title": "Backup software", "Author": "u/bhudzallmighty", "Content": "I currently have a dedicated optiplex running linux with nextcloud docker for my iphone data. I would like to expand this system to a DIY nas with a single 20tb hdd storage for backup . What is the best software to do so? I would like to be able to backup once a day. Can i back up the whole hdd? Or just folders? Thank you"},
{"Title": "I have question about ugreen enclosure", "Author": "u/Dismal_Award735", "Content": "No content"},
{"Title": "I need help identifying this hard drive, and where i can get the right cable.", "Author": "u/Chippomannen", "Content": "No content"},
{"Title": "Safest method to wipe out a drive without damaging it? I'm looking for paranoid-level shit.", "Author": "u/500xp1", "Content": "Looking for a method that makes it impossible to recover the wiped data."},
{"Title": "500,000 CDs", "Author": "u/Sliced_Apples", "Content": "Hello, I am working for a startup in the sports industry and we have recently come into the possession of about five hundred thousand cds with 20 year old sports footage. We are trying to train an AI model off of them so as such, they need to be digitized. I know a little about burning cds but not much. As I have been made aware, this would be â€œrippingâ€ and not burning. What would be the best way to go about doing this? What storage solution would be the best? Any advice is greatly appreciated. Iâ€™m happy to answer any questions as well."},
{"Title": "Best at-home manual photo scanner?", "Author": "u/Small_Vermicelli9655", "Content": "Looking to digitize a bunch of old family photos and was hoping for specific scanner suggestions. Thereâ€™s lots of conflicting information online and Iâ€™m not very proficient in this area so was hoping for some insight lol. Iâ€™ve heard automatic scanners can mark grooves on the photos and would feel more comfortable with a manual one as time is not a priority. Hoping to stay around a $500 price range and resell after use. Thank you in advance!!"},
{"Title": "Checking for Wiped SMART Data", "Author": "u/eakall", "Content": "So I recently got 35 2TB SSDs (mostly Samsung EVO 860/870) used and when checking the smart data for them I noticed very low power on hours.  The PoH (80-400hrs) is variable across the large set of drives and TBW is about 2-6TB for most of the drives. I was curious if thereâ€™s a way to check if the smart data has been reset? It seems a bit suspicious to me that these drives all are so low in PoH. Drives are also manufactured between 2020-2021 ish"},
{"Title": "CD ripping compression", "Author": "u/nlj1978", "Content": "So going through my old CDs, some of them are previously burned CDs in MP3 format. I have been ripping discs in FLAC format. If the ripping software is starting with an MP3 file and ripping to FLAC is that problematic? Compressing a compressed file sounds like a bad idea"},
{"Title": "Today I learned something about shucking", "Author": "u/auridas330", "Content": "I bought two WD elements drives, both are same size, manufacture date, drive number, but one needed me to play with the 3.3v pin to show up. Never knew that WD plays Russian roulette with their drives lol"},
{"Title": "Synology DS923+, DS1821+ & DS223j all on sale right now at B&H", "Author": "u/iddrinktothat", "Content": "** The sale has ended. ** Thought id let you guys know because i havent been seeing a lot of discounts on Synology."},
{"Title": "What is the best NVME SSD controller ?", "Author": "u/Yukinoooo", "Content": "I'm looking for an NVME SSD, 5 years warranty or more and a good controller but I don't know which NVME to choose because there are controllers : \"InnoGrit IG5236\", \"InnoGrit IG5666\", \"Phison E18\", \"Phison E26\" and \"SMI SM2264\""},
{"Title": "Longevity of Recordable CDs, DVDs and Blu-rays â€” Canadian Conservation Institute (CCI)", "Author": "u/didyousayboop", "Content": "Important information from the Canadian Conservation Institute, an agency of the federal government of Canada. Table 2: the relative stability of optical disc formats Optical disc formats Average longevity CD-R (phthalocyanine dye, gold metal layer) >100 years CD-R (phthalocyanine dye, silver alloy metal layer) 50 to 100 years DVD-R (gold metal layer) 50 to 100 years CD (read-only, such as an audio CD) 50 to 100 years CD-RW (erasable CD) 20 to 50 years BD-RE (erasable Blu-ray) 20 to 50 years DVD+R (silver alloy metal layer) 20 to 50 years CD-R (cyanine or azo dye, silver alloy metal layer) 20 to 50 years DVD+RW (erasable DVD) 20 to 50 years BD-R (non-dye, gold metal layer) 10 to 20 years DVD-R (silver alloy metal layer) 10 to 20 years DVD and BD (read-only, such as a DVD or Blu-ray movie) 10 to 20 years BD-R (dye or non-dye, single layer or dual layer) 5 to 10 years DVD-RW (erasable DVD) 5 to 10 years DVD+R DL (dual layer) 5 to 10 years"},
{"Title": "16x22TB disk MegaRAID data storage ZFS RaidZ2 or HW Raid 60", "Author": "u/Flat-One-7577", "Content": "Hi, I am going to build a 16 disk data storage. This will mainly be a \"backup\" to store data for some time and not need to recreate it again. Recreation would be expensive. I have few very big files and a lot of small one. Main focus is having a lot of storage. I/O is not so important. Having 16x 22TB disks attached to a MegaRAID 9500 Controller I am asking myself what would be the prefered setup? Variant A: HW Raid60 on top ZFS Variant B: JBOD Mode on Controller for songle Disks ZFS -- create 2 RaidZ2 vdevs (6+2) and stripe them Variant C: JBOD Mode on Controller for songle Disks ZFS -- create one RaidZ3 vdev What would you do? Regards, Joachim"},
{"Title": "Looking for some specific storage design help", "Author": "u/Real_Bad_Horse", "Content": "Hey y'all, I don't know that this is the best place for my question, but I suspect that folks around here will either have advice or know where to point me if I'm better served asking this somewhere else. I built out my homelab to learn and get a job in IT. Mission success there. I've since gotten into all kinds of stuff and have a need to rework my storage, but I'm not sure how best to set things up. Here's what I'm looking to do: Complete separation of storage and compute Compute consists of a handful of VMs running on Proxmox, and critically, a Kubernetes cluster The cluster is currently running in VMs but I have plans to move just about everything into the cluster and move into SFF PCs This is partially why the complete separation is important, for flexibility and future migration Here's what I have currently for equipment: Brocade switch with 48x 1gbe and 8x10gig sfp 2x 12 bay 3.5\" servers, one with Proxmox, one with TrueNAS 1x 24 bay 2.5\" server Repurposed NetApp 24x 3.5\" chassis, modified to work as DAS 4x 16gb spinners, currently in raidz1 ~32 old 2, 3, and 4gb spinners I found that K8s cluster was very unstable with my first setup, which was an iscsi target on the TrueNAS server. The iSCSI share was mounted in Proxmox with LVM to create K8s VM OS and NFS storage for containers. This got much better when I moved to a local 3x SSD zpool on the Proxmox host. I understand that the latency with spinners and over the network was likely the cause, but this doesn't allow for the separation if like. Use here is all over - media server is the main thing currently, which for transcoding needs fast seq read/writes, K8s app dev for work, and all kinds of testing containers for work and home. I'd also like to put all VM storage remote as well to play around with some different compute setups. I'd really love to be able to fully wipe Proxmox and try out XCP-ng, or Azure HCI, stuff like that... It's a lab, after all. I'm competent enough to implement a solution, but I guess not enough experience yet to design. I'm not afraid of complex setup, but I do need redundancy, particularly if putting these small drives to use as I don't trust them - came for free with the NetApp shelf. So... Any suggestions on how to set this up for minimal latency and fast read/write? Ceph? Mirrored ZFS using all those small spinners? Dedicated ZFS pools for each K8s node? Caching layers? Not against additional equipment, within reason, if needed."},
{"Title": "Gallery-dl instagram help", "Author": "u/Graestra", "Content": "I'm trying to scrape an instagram account however gallery-dl isn't downloading all posts from the account. I've tried looking into config settings, but I keep getting errors about double quotes when trying to use a config file, and I'm not sure what the right setting to get it to download all posts is in the first place. Any help would be appreciated."},
{"Title": "No M2TS Files on Blu-ray?", "Author": "u/Carsonsgaming", "Content": "https://preview.redd.it/no-m2ts-files-on-blu-ray-v0-1i3wz54rsn7d1.png As the title says. I'm not finding one anywhere. These are in a folder called AACS, then there's another folder with a bdmv file but that's it. I'm a complete noob to Blu-ray ripping so I'm hoping I'm just missing something. Only interested in ripping the audio."},
{"Title": "13.5 Volt", "Author": "u/Rayuzan_Mojavec", "Content": "I just got a 3.5 inch HDD. It needed 12 volts external power, but I only have 13.5 power adapter. Is it safe?"},
{"Title": "ST12000NTZ01 vs ST12000NTA01", "Author": "u/-Rhialto-", "Content": "The Z is sold by Amazon and the A by Best Buy, could it be just that? Same disk but different number depending on reseller?"},
{"Title": "Need to expand storage. Out of SATA but have PCI-E slot.", "Author": "u/DevanteWeary", "Content": "Hey guys. Using mITX motherboard and out of my four SATA ports, and need four more ports to connect four more drives. My PCIe slot is free and both m.2 slots are taken. What's the best way to get more free SATA ports? It's for my low power streaming server/NAS that is running Unraid. Thank you for any advice!"},
{"Title": "Will adding a new fan to a DAS result in the fan spinning at 100%?", "Author": "u/Apptryiguess", "Content": "I have a DAS and am pretty happy with the temps, but they could be slightly better since the DAS is in a closet and doesn't get that much airflow. My enterprise drives idle at 43-45 C which is ok, but could be a little better. So i thought about adding a new and better fan to the enclosure (Icy Box IB-3805-C31), but the fan never changes speeds, literally never. I don't let my drives spin down but if they were to, the fan does stop spinning after a while, afaik that's the only control the DAS has over the fan. So if i were to add a 2000rpm fan, would it instantly shoot up to that rpm and always spin that fast? Is there a way to control fan speed? I can't see the DAS's fan on any program controlling fan speed so that's something... Any idea on how it would behave? Any idea if there is a way to controll the fan speed? Thanks."},
{"Title": "Help downloading xvideo profiles/playlists", "Author": "u/Thehobbyist916", "Content": "YT-DLP is only able to download one link at a time Anyone have any suggestions or advice? Also, Iâ€™d like to be able to download YouTube RED content Thanks"},
{"Title": "WD Red Plus 8TB vs WD Black 8TB vs IronWolf 8TB?", "Author": "u/GearFourth", "Content": "I ordered a WD Black 8TB, but now the Red Plus is on sale for $34 cheaper, do you guys believe the Black is worth $34 more? The biggest difference that I know of is Black has a longer warranty 5 vs 3. Are there any other differences? I would be using them for media storage."},
{"Title": "Software for splitting video files in bulk?", "Author": "u/Comfortable_Ad_6823", "Content": "I am currently ripping my SpongeBob DVDs. For anyone unaware, (almost) all SpongeBob episodes are split into A and B parts. For example, \"Pizza Delivery\" is only episode 5a, with \"Home Sweet Pineapple\" being episode 5b. Two \"segments\" make up one episode. Normally this isn't a problem, as each segment in the SpongeBob DVDs has its own .mkv file. That is, until season 9, where there are no .mkv files for individual segments, only the combined episode. This is rather annoying as I don't want to scrub to the half-way mark of the videos just to watch the episode I actually want to see. I've thought of splitting the files in half, but it would be a tedious process as seasons are quite long. Are there any programs or tools that would make this easier?"},
{"Title": "Seeking Advice on Cost-Effective Backup Solutions for Multiple Hard Drives (beginner) - Thoughts on Bvckup 2?", "Author": "u/AlvTellez", "Content": "I have several larger hard drives: 5TB portable drive connected to my HTPC for films and series via Plex Two unused 8TB drives 4TB drive containing important media (that doesn't fit in my laptop and/or is more important) Initially, I considered getting a Synology NAS, but with less than 10TB of actual data, it seems like overkill, especially since I rarely access this data and usually keep the drives unplugged, except for the 5TB drive that's always connected to my HTPC (I also don't really need a NAS for my Plex needs, since I already have the HTPC as a server for that). After reading some posts, I thought about purchasing a license for Bvckup 2, which is more cost-effective and would allow me to use my other drives for backup. My plan is to transfer data from the 5TB drive to one of the 8TB drives and periodically back up the data to the other 8TB drive. If I run out of space, I could use the 4TB drive similarly and back up data to the other 5TB drive. While this might sound inefficient to experienced data hoarders, how bad/good is this idea? Are there any other software options that could simplify this process, compared to manually copying and pasting data between drives?"},
{"Title": "chkbit: Check that your files were not corrupted", "Author": "u/laktakk", "Content": "No content"},
{"Title": "Organizing drives vs drive sizes vs raids/ZFS/stuff", "Author": "u/PuzzleHeadPistion", "Content": "Hey, I've got two NAS and a bunch of drives. One EXOS 16Tb, two Barracuda 8Tb, one IronWolf 6Tb, on WD \"white\" 6Tb and on WD Red 3Tb. My main NAS is a desktop with +6 slots, my second NAS is a 4 bay Asustor. Currently I don't use raid/parity of any kind... All drives are single volumes and it just sync's each drive from my main NAS to the second NAS, so I try to make the drive setup equal. However, the recent addition of the EXOS 16Tb kills that. Since my main NAS is changing to a FreeBSD/TrueNAS with a ZFS pool, what's the best setup? I was thinking maybe 8Tb+8Tb+6Tb+6Tb in the ZFS pool, then 16Tb+3Tb as single volumes on my second NAS. This way drives in the pool are as similar as possible in size and if parity wastes one drive, it will be about the same total capacity as the second NAS. Does this make sense?"},
{"Title": "Using CRC or the like for music backup checking", "Author": "u/__99999", "Content": "Hello, I know about the various ways to get a crc and sha etc. I'm trying to find a program that keeps the same crc if I change the Metadata of the audio file. Does anyone know of one? It can be Linux or win or even a script. Basically I'm backing up and archiving my music like most of you. The issue I'm running into is I update my Metadata on my audio randomly. As I have over 10tbs (I know rookie numbers) I can't sit down and bang it out in 1 sitting so I do it in waves. tldr? I'm looking for a program that doesn't alter crc if I change the Metadata of my audio file (mp3 or flac etc)"},
{"Title": "How can I view all DMs in Twitter Archive Data? Not all DMs history shows up.", "Author": "u/niceiambearlythere", "Content": "Hello, I've downloaded my Twitter Archive Data, but not all of the DMs could be seen. It only shows a certain part of the chat, usually the earlier conversations. Is there any way to solve this, so that I can look back on all of the messages again?"},
{"Title": "Youtube server side add injection role out | Where is it already?", "Author": "u/Pommes254", "Content": "I am running ytdlp to archive a large amount of channels and i am kinda worried to contaminate my archive data with add injected versions. The thing is so far i havent seen a single one with it in about 500 videos, i just finished downloading via proxies in germany, uk and japan (downloaded videos that are about 1-2 weeks old that i already have and compared length via script) How many users really get videos served with direct injected adds currently? And in what regions? I hope a feature gets merged into ytdlp that checks video lengths and alerts if it detects more than +/- 1 sec compared to a known DB like sponsorblock."},
{"Title": "How to Download a video from a private vimeo server?", "Author": "u/diradi", "Content": "I subscribed to an online course service, and the provider uploads the class recordings to the platform through a private Vimeo server. I can watch the classes, but it's practically impossible to download them using traditional methods. I was able to download some videos on the platform using IDM (Internet Download Manager), but lately, whenever I try to download a video, a message appears saying \"Unknown error, please try again.\" Can someone help me with a solution? Either a method to download private Vimeo videos or a way to fix the IDM error. Thank you."},
{"Title": "alternative to Cathy or Virtual Volumes view that can do Boolean Search", "Author": "u/another_lease", "Content": "I need to do Boolean search on my unconnected disks. E.g., I need to search for files that contain the word \"confidence\" and \"interval\". If I enter `confidence interval` in the search bar, Cathy will only find files that contain the phrase \"confidence interval\" in their file name. Virtual Volumes View will find me files that contain both words anywhere in the file name, but it will also return files that contain either word in their filename. I know that Cathy and VVV have come up before on this subreddit. I was wondering if anyone's figured out some portable freeware that can do Boolean search. Thanks in advance."},
{"Title": "Need Help archiving important Norwegian Stenography", "Author": "u/VanillaKirby", "Content": "Hello, I need some assistance with archiving a book containing some of the only documentation of one of the only shorthand systems in the Norwegian Language, Wang-Krogdahl. The book is scanned and located in the Norwegian Public Library available here https://www.nb.no/items/URN:NBN:no-nb_digibok_2016011905022 You will need a Norwegian VPN to access the book, (i am using tunnelbear with a free license) I do not know of a way to extract these scans of the book from the website, help appreciated."},
{"Title": "CD ripping compression", "Author": "u/nlj1978", "Content": "So going through my old CDs, some of them are previously burned CDs in MP3 format. I have been ripping discs in FLAC format. If the ripping software is starting with an MP3 file and ripping to FLAC is that problematic? Compressing a compressed file sounds like a bad idea"},
{"Title": "Today I learned something about shucking", "Author": "u/auridas330", "Content": "I bought two WD elements drives, both are same size, manufacture date, drive number, but one needed me to play with the 3.3v pin to show up. Never knew that WD plays Russian roulette with their drives lol"},
{"Title": "Synology DS923+, DS1821+ & DS223j all on sale right now at B&H", "Author": "u/iddrinktothat", "Content": "** The sale has ended. ** Thought id let you guys know because i havent been seeing a lot of discounts on Synology."},
{"Title": "What is the best NVME SSD controller ?", "Author": "u/Yukinoooo", "Content": "I'm looking for an NVME SSD, 5 years warranty or more and a good controller but I don't know which NVME to choose because there are controllers : \"InnoGrit IG5236\", \"InnoGrit IG5666\", \"Phison E18\", \"Phison E26\" and \"SMI SM2264\""},
{"Title": "Longevity of Recordable CDs, DVDs and Blu-rays â€” Canadian Conservation Institute (CCI)", "Author": "u/didyousayboop", "Content": "Important information from the Canadian Conservation Institute, an agency of the federal government of Canada. Table 2: the relative stability of optical disc formats Optical disc formats Average longevity CD-R (phthalocyanine dye, gold metal layer) >100 years CD-R (phthalocyanine dye, silver alloy metal layer) 50 to 100 years DVD-R (gold metal layer) 50 to 100 years CD (read-only, such as an audio CD) 50 to 100 years CD-RW (erasable CD) 20 to 50 years BD-RE (erasable Blu-ray) 20 to 50 years DVD+R (silver alloy metal layer) 20 to 50 years CD-R (cyanine or azo dye, silver alloy metal layer) 20 to 50 years DVD+RW (erasable DVD) 20 to 50 years BD-R (non-dye, gold metal layer) 10 to 20 years DVD-R (silver alloy metal layer) 10 to 20 years DVD and BD (read-only, such as a DVD or Blu-ray movie) 10 to 20 years BD-R (dye or non-dye, single layer or dual layer) 5 to 10 years DVD-RW (erasable DVD) 5 to 10 years DVD+R DL (dual layer) 5 to 10 years"},
{"Title": "16x22TB disk MegaRAID data storage ZFS RaidZ2 or HW Raid 60", "Author": "u/Flat-One-7577", "Content": "Hi, I am going to build a 16 disk data storage. This will mainly be a \"backup\" to store data for some time and not need to recreate it again. Recreation would be expensive. I have few very big files and a lot of small one. Main focus is having a lot of storage. I/O is not so important. Having 16x 22TB disks attached to a MegaRAID 9500 Controller I am asking myself what would be the prefered setup? Variant A: HW Raid60 on top ZFS Variant B: JBOD Mode on Controller for songle Disks ZFS -- create 2 RaidZ2 vdevs (6+2) and stripe them Variant C: JBOD Mode on Controller for songle Disks ZFS -- create one RaidZ3 vdev What would you do? Regards, Joachim"},
{"Title": "Looking for some specific storage design help", "Author": "u/Real_Bad_Horse", "Content": "Hey y'all, I don't know that this is the best place for my question, but I suspect that folks around here will either have advice or know where to point me if I'm better served asking this somewhere else. I built out my homelab to learn and get a job in IT. Mission success there. I've since gotten into all kinds of stuff and have a need to rework my storage, but I'm not sure how best to set things up. Here's what I'm looking to do: Complete separation of storage and compute Compute consists of a handful of VMs running on Proxmox, and critically, a Kubernetes cluster The cluster is currently running in VMs but I have plans to move just about everything into the cluster and move into SFF PCs This is partially why the complete separation is important, for flexibility and future migration Here's what I have currently for equipment: Brocade switch with 48x 1gbe and 8x10gig sfp 2x 12 bay 3.5\" servers, one with Proxmox, one with TrueNAS 1x 24 bay 2.5\" server Repurposed NetApp 24x 3.5\" chassis, modified to work as DAS 4x 16gb spinners, currently in raidz1 ~32 old 2, 3, and 4gb spinners I found that K8s cluster was very unstable with my first setup, which was an iscsi target on the TrueNAS server. The iSCSI share was mounted in Proxmox with LVM to create K8s VM OS and NFS storage for containers. This got much better when I moved to a local 3x SSD zpool on the Proxmox host. I understand that the latency with spinners and over the network was likely the cause, but this doesn't allow for the separation if like. Use here is all over - media server is the main thing currently, which for transcoding needs fast seq read/writes, K8s app dev for work, and all kinds of testing containers for work and home. I'd also like to put all VM storage remote as well to play around with some different compute setups. I'd really love to be able to fully wipe Proxmox and try out XCP-ng, or Azure HCI, stuff like that... It's a lab, after all. I'm competent enough to implement a solution, but I guess not enough experience yet to design. I'm not afraid of complex setup, but I do need redundancy, particularly if putting these small drives to use as I don't trust them - came for free with the NetApp shelf. So... Any suggestions on how to set this up for minimal latency and fast read/write? Ceph? Mirrored ZFS using all those small spinners? Dedicated ZFS pools for each K8s node? Caching layers? Not against additional equipment, within reason, if needed."},
{"Title": "Gallery-dl instagram help", "Author": "u/Graestra", "Content": "I'm trying to scrape an instagram account however gallery-dl isn't downloading all posts from the account. I've tried looking into config settings, but I keep getting errors about double quotes when trying to use a config file, and I'm not sure what the right setting to get it to download all posts is in the first place. Any help would be appreciated."},
{"Title": "No M2TS Files on Blu-ray?", "Author": "u/Carsonsgaming", "Content": "https://preview.redd.it/no-m2ts-files-on-blu-ray-v0-1i3wz54rsn7d1.png As the title says. I'm not finding one anywhere. These are in a folder called AACS, then there's another folder with a bdmv file but that's it. I'm a complete noob to Blu-ray ripping so I'm hoping I'm just missing something. Only interested in ripping the audio."},
{"Title": "13.5 Volt", "Author": "u/Rayuzan_Mojavec", "Content": "I just got a 3.5 inch HDD. It needed 12 volts external power, but I only have 13.5 power adapter. Is it safe?"},
{"Title": "ST12000NTZ01 vs ST12000NTA01", "Author": "u/-Rhialto-", "Content": "The Z is sold by Amazon and the A by Best Buy, could it be just that? Same disk but different number depending on reseller?"},
{"Title": "Need to expand storage. Out of SATA but have PCI-E slot.", "Author": "u/DevanteWeary", "Content": "Hey guys. Using mITX motherboard and out of my four SATA ports, and need four more ports to connect four more drives. My PCIe slot is free and both m.2 slots are taken. What's the best way to get more free SATA ports? It's for my low power streaming server/NAS that is running Unraid. Thank you for any advice!"},
{"Title": "Will adding a new fan to a DAS result in the fan spinning at 100%?", "Author": "u/Apptryiguess", "Content": "I have a DAS and am pretty happy with the temps, but they could be slightly better since the DAS is in a closet and doesn't get that much airflow. My enterprise drives idle at 43-45 C which is ok, but could be a little better. So i thought about adding a new and better fan to the enclosure (Icy Box IB-3805-C31), but the fan never changes speeds, literally never. I don't let my drives spin down but if they were to, the fan does stop spinning after a while, afaik that's the only control the DAS has over the fan. So if i were to add a 2000rpm fan, would it instantly shoot up to that rpm and always spin that fast? Is there a way to control fan speed? I can't see the DAS's fan on any program controlling fan speed so that's something... Any idea on how it would behave? Any idea if there is a way to controll the fan speed? Thanks."},
{"Title": "Help downloading xvideo profiles/playlists", "Author": "u/Thehobbyist916", "Content": "YT-DLP is only able to download one link at a time Anyone have any suggestions or advice? Also, Iâ€™d like to be able to download YouTube RED content Thanks"},
{"Title": "WD Red Plus 8TB vs WD Black 8TB vs IronWolf 8TB?", "Author": "u/GearFourth", "Content": "I ordered a WD Black 8TB, but now the Red Plus is on sale for $34 cheaper, do you guys believe the Black is worth $34 more? The biggest difference that I know of is Black has a longer warranty 5 vs 3. Are there any other differences? I would be using them for media storage."},
{"Title": "Software for splitting video files in bulk?", "Author": "u/Comfortable_Ad_6823", "Content": "I am currently ripping my SpongeBob DVDs. For anyone unaware, (almost) all SpongeBob episodes are split into A and B parts. For example, \"Pizza Delivery\" is only episode 5a, with \"Home Sweet Pineapple\" being episode 5b. Two \"segments\" make up one episode. Normally this isn't a problem, as each segment in the SpongeBob DVDs has its own .mkv file. That is, until season 9, where there are no .mkv files for individual segments, only the combined episode. This is rather annoying as I don't want to scrub to the half-way mark of the videos just to watch the episode I actually want to see. I've thought of splitting the files in half, but it would be a tedious process as seasons are quite long. Are there any programs or tools that would make this easier?"},
{"Title": "Seeking Advice on Cost-Effective Backup Solutions for Multiple Hard Drives (beginner) - Thoughts on Bvckup 2?", "Author": "u/AlvTellez", "Content": "I have several larger hard drives: 5TB portable drive connected to my HTPC for films and series via Plex Two unused 8TB drives 4TB drive containing important media (that doesn't fit in my laptop and/or is more important) Initially, I considered getting a Synology NAS, but with less than 10TB of actual data, it seems like overkill, especially since I rarely access this data and usually keep the drives unplugged, except for the 5TB drive that's always connected to my HTPC (I also don't really need a NAS for my Plex needs, since I already have the HTPC as a server for that). After reading some posts, I thought about purchasing a license for Bvckup 2, which is more cost-effective and would allow me to use my other drives for backup. My plan is to transfer data from the 5TB drive to one of the 8TB drives and periodically back up the data to the other 8TB drive. If I run out of space, I could use the 4TB drive similarly and back up data to the other 5TB drive. While this might sound inefficient to experienced data hoarders, how bad/good is this idea? Are there any other software options that could simplify this process, compared to manually copying and pasting data between drives?"},
{"Title": "chkbit: Check that your files were not corrupted", "Author": "u/laktakk", "Content": "No content"},
{"Title": "Organizing drives vs drive sizes vs raids/ZFS/stuff", "Author": "u/PuzzleHeadPistion", "Content": "Hey, I've got two NAS and a bunch of drives. One EXOS 16Tb, two Barracuda 8Tb, one IronWolf 6Tb, on WD \"white\" 6Tb and on WD Red 3Tb. My main NAS is a desktop with +6 slots, my second NAS is a 4 bay Asustor. Currently I don't use raid/parity of any kind... All drives are single volumes and it just sync's each drive from my main NAS to the second NAS, so I try to make the drive setup equal. However, the recent addition of the EXOS 16Tb kills that. Since my main NAS is changing to a FreeBSD/TrueNAS with a ZFS pool, what's the best setup? I was thinking maybe 8Tb+8Tb+6Tb+6Tb in the ZFS pool, then 16Tb+3Tb as single volumes on my second NAS. This way drives in the pool are as similar as possible in size and if parity wastes one drive, it will be about the same total capacity as the second NAS. Does this make sense?"},
{"Title": "Using CRC or the like for music backup checking", "Author": "u/__99999", "Content": "Hello, I know about the various ways to get a crc and sha etc. I'm trying to find a program that keeps the same crc if I change the Metadata of the audio file. Does anyone know of one? It can be Linux or win or even a script. Basically I'm backing up and archiving my music like most of you. The issue I'm running into is I update my Metadata on my audio randomly. As I have over 10tbs (I know rookie numbers) I can't sit down and bang it out in 1 sitting so I do it in waves. tldr? I'm looking for a program that doesn't alter crc if I change the Metadata of my audio file (mp3 or flac etc)"},
{"Title": "How can I view all DMs in Twitter Archive Data? Not all DMs history shows up.", "Author": "u/niceiambearlythere", "Content": "Hello, I've downloaded my Twitter Archive Data, but not all of the DMs could be seen. It only shows a certain part of the chat, usually the earlier conversations. Is there any way to solve this, so that I can look back on all of the messages again?"},
{"Title": "Youtube server side add injection role out | Where is it already?", "Author": "u/Pommes254", "Content": "I am running ytdlp to archive a large amount of channels and i am kinda worried to contaminate my archive data with add injected versions. The thing is so far i havent seen a single one with it in about 500 videos, i just finished downloading via proxies in germany, uk and japan (downloaded videos that are about 1-2 weeks old that i already have and compared length via script) How many users really get videos served with direct injected adds currently? And in what regions? I hope a feature gets merged into ytdlp that checks video lengths and alerts if it detects more than +/- 1 sec compared to a known DB like sponsorblock."},
{"Title": "How to Download a video from a private vimeo server?", "Author": "u/diradi", "Content": "I subscribed to an online course service, and the provider uploads the class recordings to the platform through a private Vimeo server. I can watch the classes, but it's practically impossible to download them using traditional methods. I was able to download some videos on the platform using IDM (Internet Download Manager), but lately, whenever I try to download a video, a message appears saying \"Unknown error, please try again.\" Can someone help me with a solution? Either a method to download private Vimeo videos or a way to fix the IDM error. Thank you."},
{"Title": "alternative to Cathy or Virtual Volumes view that can do Boolean Search", "Author": "u/another_lease", "Content": "I need to do Boolean search on my unconnected disks. E.g., I need to search for files that contain the word \"confidence\" and \"interval\". If I enter `confidence interval` in the search bar, Cathy will only find files that contain the phrase \"confidence interval\" in their file name. Virtual Volumes View will find me files that contain both words anywhere in the file name, but it will also return files that contain either word in their filename. I know that Cathy and VVV have come up before on this subreddit. I was wondering if anyone's figured out some portable freeware that can do Boolean search. Thanks in advance."},
{"Title": "Need Help archiving important Norwegian Stenography", "Author": "u/VanillaKirby", "Content": "Hello, I need some assistance with archiving a book containing some of the only documentation of one of the only shorthand systems in the Norwegian Language, Wang-Krogdahl. The book is scanned and located in the Norwegian Public Library available here https://www.nb.no/items/URN:NBN:no-nb_digibok_2016011905022 You will need a Norwegian VPN to access the book, (i am using tunnelbear with a free license) I do not know of a way to extract these scans of the book from the website, help appreciated."},
{"Title": "Looking for a software to recognize multiple TB of images", "Author": "u/EasyMoney322", "Content": "Hello, I'm looking for a self-hosted software (that also wouldn't upload photos anywhere) that could do image recognition on the fileshare with an acceptable success rate. I was able to find posts on this sub about nsfw bodyparts recognition, but its not what Im looking for. What level of recognition? It must be able to tell appart photos of mass events, people, pets, documents, roads, buildings etc. Having them organized by a location metadata, perhaps. Finding similar (almost duplicate, but with different hash) images. Would be great if I could select all the tagged images after, re-check them for false-positives, and delete. The fileshare is hosted on OpenSuse VM, but I also can deploy and mount it on any other OS on the same server. I have a lot of processing power, but I'd like to avoid training the AI by myself."},
{"Title": "Easier to get a NAS or just buy another desktop with lots of storage?", "Author": "u/El_Chupachichis", "Content": "I keep contemplating getting more storage, possibly a NAS.  But I'm not doing actual streaming, just collecting an ever larger amount of images, RAW and jpg (I'm an event photographer hobbyist). I would look at the NAS online and see perhaps a cheap 4 bay NAS, then look at the reviews and see a lot of complaints.  Seems like getting a reasonably reliable NAS would be more like getting a high end desktop. For those digital hoarders who don't have a lot of streamable data, do you prefer NAS or just a big desktop with a lot of drive slots, and maybe a software RAID?  I tend to be a cheapskate so historically it's always been \"buy another drive that's larger and copy stuff over\" but I really need to start thinking long term."},
{"Title": "Will youtube ad Injections break music downloads?", "Author": "u/Nerds_r_us45", "Content": "I like downloading some channels in bulk and idk if this will break my ability to hoard music easily or not."},
{"Title": "Is it possible to save an online quiz  to offline and use it offline and recieve results?", "Author": "u/dokha", "Content": "Itâ€™s important to note thats the kind of assessment pages im talking about are the casual ones such as the fun ones you take on Buzzfeed and the ones you see on astrology sites.. I have no idea the correct tools involved and how to use them .. I did try browser addons such as Single File but the offline files never reach the results after taking the quiz.."},
{"Title": "Why isn't rsync checksum or the equivalent enough to verify your backups?", "Author": "u/Ninj_Pizz_ha", "Content": "I expect I'll get some flack from people super immersed into this subculture, but why do people still recommend opening up random files in the backup to make sure the backup actually worked? Why isn't rsync -c or the equivalent sufficient? Personally I only open my backups every once in blue moon. Maybe there's some edge case where rsync checksum itself is faulty or something I guess, but that's not on my list of likely concerns tbh."},
{"Title": "Photo scanning", "Author": "u/dashcash853", "Content": "Is the dpi a huge deal when it comes to this, I know some do 1200 dpi but a lot of the ones in my price range are 600 dpi."},
{"Title": "Best way to back up gallery and general data from phone?", "Author": "u/Topangers", "Content": "I have iCloud but sometimes popups that appear confuse me and it doesn't seem like the images are actually being backed up? I need to make space on my phone but I hoard the images and videos; same with the general data and applications on my device. Any advice would be greatly appreciated! :)"},
{"Title": "Newbie setup questions", "Author": "u/tranrep", "Content": "Hey all, trying to get myself situated and not the most tech-savvy person so apologies in advance if I'm missing the mark. For context, I'm mostly a hobbyist photographer that wants to just keep my data safe and I don't believe I'd have a need for most of what a NAS offers, so I'm looking into DAS/JBOD as a solution. I currently have around 6TB of photo/video I consider \"important\" enough to backup. My current setup is just a single 14 TB WD External HD which is not really being backed up anywhere so I'd like to improve my setup. At the core, I'm currently planning on doing the following: Buying a new 8TB HDD that I'll be \"working off of\", and moving all of my existing data into it Buying a 4 bay DAS, either using 2 drives for now for RAID1 or buying 4 for RAID4/5 (??) to periodically mirror data from the single 8TB drive onto. Using Backblaze to backup my PC + DAS. Does this setup make sense for my needs? If it does make sense, does anybody have any particular product recommendations for the DAS/JBOD and if there's any specific thing to look for in the type of drive(s) to purchase? Is this perhaps overkill for the use case? Please let me know if I'm not providing enough information, thanks."},
{"Title": "I'm using an SD card to USB cable to transfer some photos to my PC but there's only one file with no extension called \"USBC Â¬Ã·\u001f\". Help!", "Author": "u/CorvusTheCryptid", "Content": "The issue is as I describe in the title. I've never had a problem like this before! There's a single file on the device when I plug it in, titled \"USBC Â¬Ã·\u001f\", with no file extension. It's a huge file so I assume that fixing it will allow me access to my files, which have somehow merged into this singular, huge file. Please help, I can't afford to lose these pictures!"},
{"Title": "CD ripping", "Author": "u/nlj1978", "Content": "I have setup a Jellyfin server and have been successfully ripping CDs to flac using EAC. EAC does have one issue I haven't figured out. There doesn't seem to be a way to have the contents of one CD ripped into its own folder in the destination. Jellyfin's file structure wants each CD in its own folder. Is there a method to accomplish this I just haven't found yet? If not is there another ripping software that can do this?"},
{"Title": "Bulk image downloader that can download images linked from thumbnails", "Author": "u/xavierhollis", "Content": "Often I find myself checking out a gallery or a post on reddit that has multiple images. By clicking on the thumbnails I can open up larger versions of the same images. But if I want to save the images it gets tedious and time consuming having to go through them all one by one, opening up dozens of webpages or scrolling through each image to save them. Is there an app that will basically bulk download ALL the images from a gallery, not simply the thumbnails on that one page, but the higher res images the thumbnails link to?"},
{"Title": "Hydrus network server help?", "Author": "u/Head-Ordinary-4349", "Content": "Does anyone here have any experience translating your local Hydrus network onto a publicly available server? I have made a local database of images which I would like to share and have editable by anyone publicly with a link, however I am very inexperienced in this sort of thing. The user resources describe a  hacky server component that can serve your database over https (as also mentioned here ), but honestly I have no idea where to even start. I'm wondering if anyone would be able to point me in the right direction or possibly even give me a bit of guidance on how I should proceed? Thanks!"},
{"Title": "Best non destructive way to scan books with illustrations and photographs?", "Author": "u/green__problem", "Content": "I have a flatbed scanner and a phone with a quality camera. For text-heavy books I use the CamScanner app, and then scan the cover using my flatbed. Non destructive, very effective. For magazines and newspapers, the flatbed is usually enough, as the lack of a solid spine makes scanning with minimal wear very easy. Now comes my problem: I have a lot of image-heavy books that I want to scan, but I have yet to find a good method to do so. CamScanner is horrid at dealing with illustrations and photographs. The flatbed works alright , but not great. Because I avoid breaking the book's spine, there are always visible shadows and both text and images become a little blurry when they're close to the hinge. I'm wondering if there's an app similar to CamScanner but more appropriate for photographs? Or a different method altogether. I know some people melt the glue keeping the spine together, scan the pages individually, and then glue everything back on. This wouldn't work for all of the books in my collection- but I have considered trying it on a handful of them. I'm just a little scared of screwing the process up. Thanks in advance."},
{"Title": "Why is XFS not more popular? Are there are any concerns with XFS still?", "Author": "u/ECrispy", "Content": "This is for home desktop usage, not servers/data centers where XFS is far more common. performance - in every test I could find, XFS is near the top, beating btrfs/ext4. Its esp good for parallel workloads and almost everything on a modern desktop is like that. The only perf concern I read about is it used to have higher cpu usage for updating metadata but I believe thats been fixed and no longer relevant? (I think for most users, performance in benchmarks may not be noticeable and other features matter more, but its still an important consideration) SSD/OS installs - XFS is almost as fast as f2fs for these. I see no reason why anyone would use f2fs on anything other than a sd card or on any NAND device with wear leveling. CoW/snapshots - this is no doubt a very powerful feature of zfs/btrfs. But I see very little mention of reflinks/snapshots on XFS which can achieve a lot of this. They are not atomic but enough to satisfy a lot of use cases. I don't see support for this in the usual tools like snapper/timeshift either. XFS also has support for deduping. All of this comes without the usual cost of CoW other features - dynamic inodes (on ext4 an inode for every 16kb/256kb is wasteful, even if most people never notice it), automatic fsck, journalling (sure, copied from ext3, but thats not a bad thing) stability/reliability - I don't think there should be any doubt about this. Its a proven enterprise class fs with a hallowed pedigree and reputation, is now backed by RHEL and has probably seen more active development than most other file systems. The biggest factor seems to be that the default ext4 is good enough, and frankly most people will not care or know about, and should not care, about the underlying fs. There are also distros like Fedora/OpenSuse that used to use XFS as the default and have switched to btrfs. I don't know of anything that uses XFS as default except unRaid now - unRaid is used to manage TBs by home users and that probably says something. The only concerns I've found are - a) it doesn't support shrinking a volume. how common is this anyway? I've never seen any home user need to do this, 99% of the time you only need this when you are installing another OS on the same ssd/hdd and need to shrink your current /, which is an advanced use case. b)supposedly XFS doesn't handle hw failures. Even on this I found no consensus - some people say its risky and can corrupt with no recovery, others say even with a forced  shutdown its safe. I'm not sure if its any less robust than ext4/btrfs? Is this actually a concern these days?"},
{"Title": "Recertified EXOS X18 constantly reading?", "Author": "u/wiadrovit", "Content": "Hey there Guys, I've bought a recertified EXOS X18 12TB for my NAS. The drive isn't exactly loud and it performs as I would expect, but there's one thing that I can't walk past - it acts as if it was doing something, even when completely idle. The drive lives in my Sabrent DS-SC5B hdd enclosure which I've been using for over a year now and which I'm very satisfied with. Video: https://imgur.com/a/o39cfd8 (it's the second one from the top - as you can see its LED is blinking in a regular manner). I've read that EXOS drives have their own APM feature which can be disabled using seachest tool. I've managed to successfully disable both EPC as well as the power balance feature, but that didn't change anything, the LED still blinks and I can hear a regular, gentle cracking sound as if something was read/written. The drive is mounted on my Debian installation and I've confirmed that no process is using it. What's weird is that activities stop as soon as I unmount the drive (and start again as soon as I mount it back). In fact, this is a second drive that does the same thing. I've returned previous one to the seller as I've felt something is not right with it. Both were manufactured (or rather reassembled?) back in March 2024. I should add that I have another EXOS drive (X16 16TB sitting in my backup device) and it doesn't act this way. Is it normal for these drives or do you think I should return this one as well and go for something else? It isn't that much annoying, I am just worried that if the head keeps flying all the time, the drive will wear sooner and might die prematurely. Thanks for any advice."},
{"Title": "Getting creative and hacky with SFF as NAS.", "Author": "u/kkgmgfn", "Content": "Trigger Warning : This post is for DataHoarders + SFF enthusiasts. So please don't come saying get a 2TB NVME, get a 2.5 SATA SSD and yap yap yap.. And I have other systems too. One EATX, One MFF, Two SFF and One HTPC. So I am not a new builder. Posting here since it will be more appropriate than posting in Homelab or DataHoarder sub. As we know it we have very limited SFF cases with HDD support. Manufacturers hardly make them anymore. Some options that we have today: Node 304: Outdated and front panel is very restrictive. 92mm fans in front are noisy as they are 3pin. In my country I can't find any 92mm fan. Jonsbo N series: Pat on the back for Jonsbo on launching several SFF NAS cases back to back. They feel like they have restrictive airflow. They aren't available in my country. SAMA IM 01 and its copies: Supports 4 - 5. A cheap knock of this is available. But is an option. So my question is have you guys though about squeezing extra HDDs in cases like Deepcool CH 160, Coolermaster NR200P etc. For example if I can use rear fan mount for 120 AIO intake then does 120 fan slot above have motherboard has hinges for 3. 5\"HDD? Does it have space to tuck one below a dual slot dual fan GPU. I know 1 3.5\" HDD can go on front panel when I use a SFX PSU. Similarly for NR200P. More suggestions are welcome. I'll use a Noctua L9i or 120 AIO as I have then lying around. I wish this post to be open for coming years so people can get ideas and inspiration from the comments."},
{"Title": "Feedback for backup plan", "Author": "u/DeadbeatSummer13", "Content": "My dataset is around 10-16tb. I plan on transferring my current externals to 1 big drive. Iâ€™m trying to decide if this working drive is going to be internal or external. Regardless, this will be backed up to a 2nd drive daily. Then, the 2nd drive will backup to backblaze daily. A private encryption key will be set on backblaze. Possibly down the road an off-site drive may be added to be backed up weekly and then disconnected and moved off-site. Feedback is greatly appreciated. What do you think?"},
{"Title": "Should I purchase a NAS for the data integrity features?", "Author": "u/SystemElegant2703", "Content": "Is it necessary to purchase a NAS if all I'm really interested in are the data integrity features (i.e automatic hash checking/recording, file self-healing, datascrubbing, etc.)? Currently I use MultiPar and backup my data to M-discs. However, I would like greater certainty that the hashes are accurate. For instance, I got a trojan recently and now I'm left questioning if the hundreds of files I've downloaded since my last backup have any data corruption, silent or otherwise. Since it's impossible to know without the features I listed previously, should I consider purchasing a NAS or is there a method I haven't thought of to ensure the same level of data integrity?"},
{"Title": "ytarchive vs yt-dlp on video afterwards", "Author": "u/idle_cat", "Content": "On youtube, I archive livestreams of a channel. Is the live archive recording I get by using ytarchive a higher quality then the video I would get with yt-dlp that's processed afterwards? From my understanding the video goes through youtube's compression. Is the compression really strong in your opinion? I am wondering if it's worth getting the vod to save space. Side questions: Why do people put --format \"bv*+ba/b\" or something similar to get when yt-dlp already has it set to get the individual best audio and video as the default? https://github.com/yt-dlp/yt-dlp?tab=readme-ov-file#format-selection"},
{"Title": "Retire a drive after a single pending sector?", "Author": "u/Most_Mix_7505", "Content": "What would you all do?"},
{"Title": "Backup my network pc's to my OMV Nas", "Author": "u/SbM_Yggdrassil", "Content": "Hi all, I'm at that part of my homelab journey where I've setup a bunch of fun stuff and now I'm starting to think about helpful stuff like backups. I have a raspberry pi 4 setup with open media vault and docker running various services. I would like to add one or more services (in containers if necessary) to accomplish the following: Make an image of bootdrives of my computers on a schedule save those images to the attached storage. reimaging solution for recovery Backup secondary, non-bootdrives of my computers (just copying is probably fine) I'm just wondering how to best add services and which would do it. If I should use syncthing/duplicati + something else or if there is one thing that can do it all. I'm not sure how incremental backups fit in here either but I'd like to implement that to reduce the burden of network traffic (in my home we have to use wi-fi a lot). For drive images I've used the free version of Macrium reflect before (and I've heard of acronis), but just on one of my windows pc's. I'd like to have something scheduled from the server side for centralised management. Does it make sense what I'm trying to achieve?"},
{"Title": "Mini PC as NAS, good idea?", "Author": "u/smartyee", "Content": "No content"},
{"Title": "General Reminder Backups are Important", "Author": "u/TeamSylver", "Content": "Everyone here probably already knows that. I've just had all 3 drives in my desktop PC suddenly have problems. Thank god I can still read/write to the drives though. It's just god awful slow, especially during data transfers, where it will render the whole OS unusable until it's done. So that was a lot of pain and agony to temporarily move everything to the spare PC and laptops I have laying around (PC has 1tb, laptops have 1.5tb and 4.5tb). Means I now have no backups at all, since I still haven't finished setting up my work PC to be my off-site backup PC yet (it's basically manage/byo PC at my work BC it's such a small store). Annoying as well since that PC hosts my active directory and vaultwarden as well as the file server (thankfully I had a secondary active directory server set up, but no vaultwarden). Gotta love Crucial NVMes. All of them only 11 months old. Never again. 2 of 3 RMAs processed but I still gotta get data off of the third (OS drive) before I can post that. Edit: Forgot to mention they are Crucial P3 Plus 4TB NVMes"},
{"Title": "Dedup utility that FIRST finds duplicates by name/size/date, and THEN compares their content", "Author": "u/Msprg", "Content": "Hello, Let me preface with: I know there are a million posts about dedup tools already. Dedup by file content, checksum, attributes, similar photos, similar videosâ€¦ Yet somehow, I failed to find any tools that would be able to first filter out the majority of files that differ in filename / date / size and then on the results make sure that files are 100% surely duplicate by comparing their content. I've tried dupeguru, alldup, freefilesync, treesize, czkawka, I tried everything! (By voidtools that is). The point is that I'm either missing something, or that none of the tools offer the option I'm looking for. So here I am. Once again. Seeking answer to the eternal question: How do you deal with duplicates, fellow Data Hoarders?"},
{"Title": "What Non-NAS storage would you recommend?", "Author": "u/Bloodmoonwolf", "Content": "I currently have less than 800GB across 2 clouds and my laptop. I'm hitting my storage limit on Google and looking for a safe, local option. After losing everything on an old laptop that crashed, I started doing cloud storage, which is now becoming expensive. My current laptop is an old HP and I have yet to decide between a new Windows laptop or a Chromebook. I have a Plex library I would like to expand, even bought an external DVD reader to start the library. I don't necessarily need NAS. Plugging something into the TV or my laptop would be fine when I want to watch something on Plex (which isn't very often). The same goes for when I need to do a regular backup of files. I would prefer to buy something once instead of paying a monthly subscription and to not add another constant draw on our power supply. Most of the storage is for movies/shows, photos, and PDF scans of documents from when I went paperless. I would like to add music to this once I figure out a few things. What type/size/brand of local storage would you recommend for my situation?"},
{"Title": "Looking for a software to recognize multiple TB of images", "Author": "u/EasyMoney322", "Content": "Hello, I'm looking for a self-hosted software (that also wouldn't upload photos anywhere) that could do image recognition on the fileshare with an acceptable success rate. I was able to find posts on this sub about nsfw bodyparts recognition, but its not what Im looking for. What level of recognition? It must be able to tell appart photos of mass events, people, pets, documents, roads, buildings etc. Having them organized by a location metadata, perhaps. Finding similar (almost duplicate, but with different hash) images. Would be great if I could select all the tagged images after, re-check them for false-positives, and delete. The fileshare is hosted on OpenSuse VM, but I also can deploy and mount it on any other OS on the same server. I have a lot of processing power, but I'd like to avoid training the AI by myself."},
{"Title": "Easier to get a NAS or just buy another desktop with lots of storage?", "Author": "u/El_Chupachichis", "Content": "I keep contemplating getting more storage, possibly a NAS.  But I'm not doing actual streaming, just collecting an ever larger amount of images, RAW and jpg (I'm an event photographer hobbyist). I would look at the NAS online and see perhaps a cheap 4 bay NAS, then look at the reviews and see a lot of complaints.  Seems like getting a reasonably reliable NAS would be more like getting a high end desktop. For those digital hoarders who don't have a lot of streamable data, do you prefer NAS or just a big desktop with a lot of drive slots, and maybe a software RAID?  I tend to be a cheapskate so historically it's always been \"buy another drive that's larger and copy stuff over\" but I really need to start thinking long term."},
{"Title": "Will youtube ad Injections break music downloads?", "Author": "u/Nerds_r_us45", "Content": "I like downloading some channels in bulk and idk if this will break my ability to hoard music easily or not."},
{"Title": "Is it possible to save an online quiz  to offline and use it offline and recieve results?", "Author": "u/dokha", "Content": "Itâ€™s important to note thats the kind of assessment pages im talking about are the casual ones such as the fun ones you take on Buzzfeed and the ones you see on astrology sites.. I have no idea the correct tools involved and how to use them .. I did try browser addons such as Single File but the offline files never reach the results after taking the quiz.."},
{"Title": "Why isn't rsync checksum or the equivalent enough to verify your backups?", "Author": "u/Ninj_Pizz_ha", "Content": "I expect I'll get some flack from people super immersed into this subculture, but why do people still recommend opening up random files in the backup to make sure the backup actually worked? Why isn't rsync -c or the equivalent sufficient? Personally I only open my backups every once in blue moon. Maybe there's some edge case where rsync checksum itself is faulty or something I guess, but that's not on my list of likely concerns tbh."},
{"Title": "Photo scanning", "Author": "u/dashcash853", "Content": "Is the dpi a huge deal when it comes to this, I know some do 1200 dpi but a lot of the ones in my price range are 600 dpi."},
{"Title": "Best way to back up gallery and general data from phone?", "Author": "u/Topangers", "Content": "I have iCloud but sometimes popups that appear confuse me and it doesn't seem like the images are actually being backed up? I need to make space on my phone but I hoard the images and videos; same with the general data and applications on my device. Any advice would be greatly appreciated! :)"},
{"Title": "Newbie setup questions", "Author": "u/tranrep", "Content": "Hey all, trying to get myself situated and not the most tech-savvy person so apologies in advance if I'm missing the mark. For context, I'm mostly a hobbyist photographer that wants to just keep my data safe and I don't believe I'd have a need for most of what a NAS offers, so I'm looking into DAS/JBOD as a solution. I currently have around 6TB of photo/video I consider \"important\" enough to backup. My current setup is just a single 14 TB WD External HD which is not really being backed up anywhere so I'd like to improve my setup. At the core, I'm currently planning on doing the following: Buying a new 8TB HDD that I'll be \"working off of\", and moving all of my existing data into it Buying a 4 bay DAS, either using 2 drives for now for RAID1 or buying 4 for RAID4/5 (??) to periodically mirror data from the single 8TB drive onto. Using Backblaze to backup my PC + DAS. Does this setup make sense for my needs? If it does make sense, does anybody have any particular product recommendations for the DAS/JBOD and if there's any specific thing to look for in the type of drive(s) to purchase? Is this perhaps overkill for the use case? Please let me know if I'm not providing enough information, thanks."},
{"Title": "I'm using an SD card to USB cable to transfer some photos to my PC but there's only one file with no extension called \"USBC Â¬Ã·\u001f\". Help!", "Author": "u/CorvusTheCryptid", "Content": "The issue is as I describe in the title. I've never had a problem like this before! There's a single file on the device when I plug it in, titled \"USBC Â¬Ã·\u001f\", with no file extension. It's a huge file so I assume that fixing it will allow me access to my files, which have somehow merged into this singular, huge file. Please help, I can't afford to lose these pictures!"},
{"Title": "CD ripping", "Author": "u/nlj1978", "Content": "I have setup a Jellyfin server and have been successfully ripping CDs to flac using EAC. EAC does have one issue I haven't figured out. There doesn't seem to be a way to have the contents of one CD ripped into its own folder in the destination. Jellyfin's file structure wants each CD in its own folder. Is there a method to accomplish this I just haven't found yet? If not is there another ripping software that can do this?"},
{"Title": "Bulk image downloader that can download images linked from thumbnails", "Author": "u/xavierhollis", "Content": "Often I find myself checking out a gallery or a post on reddit that has multiple images. By clicking on the thumbnails I can open up larger versions of the same images. But if I want to save the images it gets tedious and time consuming having to go through them all one by one, opening up dozens of webpages or scrolling through each image to save them. Is there an app that will basically bulk download ALL the images from a gallery, not simply the thumbnails on that one page, but the higher res images the thumbnails link to?"},
{"Title": "Hydrus network server help?", "Author": "u/Head-Ordinary-4349", "Content": "Does anyone here have any experience translating your local Hydrus network onto a publicly available server? I have made a local database of images which I would like to share and have editable by anyone publicly with a link, however I am very inexperienced in this sort of thing. The user resources describe a  hacky server component that can serve your database over https (as also mentioned here ), but honestly I have no idea where to even start. I'm wondering if anyone would be able to point me in the right direction or possibly even give me a bit of guidance on how I should proceed? Thanks!"},
{"Title": "Best non destructive way to scan books with illustrations and photographs?", "Author": "u/green__problem", "Content": "I have a flatbed scanner and a phone with a quality camera. For text-heavy books I use the CamScanner app, and then scan the cover using my flatbed. Non destructive, very effective. For magazines and newspapers, the flatbed is usually enough, as the lack of a solid spine makes scanning with minimal wear very easy. Now comes my problem: I have a lot of image-heavy books that I want to scan, but I have yet to find a good method to do so. CamScanner is horrid at dealing with illustrations and photographs. The flatbed works alright , but not great. Because I avoid breaking the book's spine, there are always visible shadows and both text and images become a little blurry when they're close to the hinge. I'm wondering if there's an app similar to CamScanner but more appropriate for photographs? Or a different method altogether. I know some people melt the glue keeping the spine together, scan the pages individually, and then glue everything back on. This wouldn't work for all of the books in my collection- but I have considered trying it on a handful of them. I'm just a little scared of screwing the process up. Thanks in advance."},
{"Title": "Why is XFS not more popular? Are there are any concerns with XFS still?", "Author": "u/ECrispy", "Content": "This is for home desktop usage, not servers/data centers where XFS is far more common. performance - in every test I could find, XFS is near the top, beating btrfs/ext4. Its esp good for parallel workloads and almost everything on a modern desktop is like that. The only perf concern I read about is it used to have higher cpu usage for updating metadata but I believe thats been fixed and no longer relevant? (I think for most users, performance in benchmarks may not be noticeable and other features matter more, but its still an important consideration) SSD/OS installs - XFS is almost as fast as f2fs for these. I see no reason why anyone would use f2fs on anything other than a sd card or on any NAND device with wear leveling. CoW/snapshots - this is no doubt a very powerful feature of zfs/btrfs. But I see very little mention of reflinks/snapshots on XFS which can achieve a lot of this. They are not atomic but enough to satisfy a lot of use cases. I don't see support for this in the usual tools like snapper/timeshift either. XFS also has support for deduping. All of this comes without the usual cost of CoW other features - dynamic inodes (on ext4 an inode for every 16kb/256kb is wasteful, even if most people never notice it), automatic fsck, journalling (sure, copied from ext3, but thats not a bad thing) stability/reliability - I don't think there should be any doubt about this. Its a proven enterprise class fs with a hallowed pedigree and reputation, is now backed by RHEL and has probably seen more active development than most other file systems. The biggest factor seems to be that the default ext4 is good enough, and frankly most people will not care or know about, and should not care, about the underlying fs. There are also distros like Fedora/OpenSuse that used to use XFS as the default and have switched to btrfs. I don't know of anything that uses XFS as default except unRaid now - unRaid is used to manage TBs by home users and that probably says something. The only concerns I've found are - a) it doesn't support shrinking a volume. how common is this anyway? I've never seen any home user need to do this, 99% of the time you only need this when you are installing another OS on the same ssd/hdd and need to shrink your current /, which is an advanced use case. b)supposedly XFS doesn't handle hw failures. Even on this I found no consensus - some people say its risky and can corrupt with no recovery, others say even with a forced  shutdown its safe. I'm not sure if its any less robust than ext4/btrfs? Is this actually a concern these days?"},
{"Title": "Recertified EXOS X18 constantly reading?", "Author": "u/wiadrovit", "Content": "Hey there Guys, I've bought a recertified EXOS X18 12TB for my NAS. The drive isn't exactly loud and it performs as I would expect, but there's one thing that I can't walk past - it acts as if it was doing something, even when completely idle. The drive lives in my Sabrent DS-SC5B hdd enclosure which I've been using for over a year now and which I'm very satisfied with. Video: https://imgur.com/a/o39cfd8 (it's the second one from the top - as you can see its LED is blinking in a regular manner). I've read that EXOS drives have their own APM feature which can be disabled using seachest tool. I've managed to successfully disable both EPC as well as the power balance feature, but that didn't change anything, the LED still blinks and I can hear a regular, gentle cracking sound as if something was read/written. The drive is mounted on my Debian installation and I've confirmed that no process is using it. What's weird is that activities stop as soon as I unmount the drive (and start again as soon as I mount it back). In fact, this is a second drive that does the same thing. I've returned previous one to the seller as I've felt something is not right with it. Both were manufactured (or rather reassembled?) back in March 2024. I should add that I have another EXOS drive (X16 16TB sitting in my backup device) and it doesn't act this way. Is it normal for these drives or do you think I should return this one as well and go for something else? It isn't that much annoying, I am just worried that if the head keeps flying all the time, the drive will wear sooner and might die prematurely. Thanks for any advice."},
{"Title": "Getting creative and hacky with SFF as NAS.", "Author": "u/kkgmgfn", "Content": "Trigger Warning : This post is for DataHoarders + SFF enthusiasts. So please don't come saying get a 2TB NVME, get a 2.5 SATA SSD and yap yap yap.. And I have other systems too. One EATX, One MFF, Two SFF and One HTPC. So I am not a new builder. Posting here since it will be more appropriate than posting in Homelab or DataHoarder sub. As we know it we have very limited SFF cases with HDD support. Manufacturers hardly make them anymore. Some options that we have today: Node 304: Outdated and front panel is very restrictive. 92mm fans in front are noisy as they are 3pin. In my country I can't find any 92mm fan. Jonsbo N series: Pat on the back for Jonsbo on launching several SFF NAS cases back to back. They feel like they have restrictive airflow. They aren't available in my country. SAMA IM 01 and its copies: Supports 4 - 5. A cheap knock of this is available. But is an option. So my question is have you guys though about squeezing extra HDDs in cases like Deepcool CH 160, Coolermaster NR200P etc. For example if I can use rear fan mount for 120 AIO intake then does 120 fan slot above have motherboard has hinges for 3. 5\"HDD? Does it have space to tuck one below a dual slot dual fan GPU. I know 1 3.5\" HDD can go on front panel when I use a SFX PSU. Similarly for NR200P. More suggestions are welcome. I'll use a Noctua L9i or 120 AIO as I have then lying around. I wish this post to be open for coming years so people can get ideas and inspiration from the comments."},
{"Title": "Feedback for backup plan", "Author": "u/DeadbeatSummer13", "Content": "My dataset is around 10-16tb. I plan on transferring my current externals to 1 big drive. Iâ€™m trying to decide if this working drive is going to be internal or external. Regardless, this will be backed up to a 2nd drive daily. Then, the 2nd drive will backup to backblaze daily. A private encryption key will be set on backblaze. Possibly down the road an off-site drive may be added to be backed up weekly and then disconnected and moved off-site. Feedback is greatly appreciated. What do you think?"},
{"Title": "Should I purchase a NAS for the data integrity features?", "Author": "u/SystemElegant2703", "Content": "Is it necessary to purchase a NAS if all I'm really interested in are the data integrity features (i.e automatic hash checking/recording, file self-healing, datascrubbing, etc.)? Currently I use MultiPar and backup my data to M-discs. However, I would like greater certainty that the hashes are accurate. For instance, I got a trojan recently and now I'm left questioning if the hundreds of files I've downloaded since my last backup have any data corruption, silent or otherwise. Since it's impossible to know without the features I listed previously, should I consider purchasing a NAS or is there a method I haven't thought of to ensure the same level of data integrity?"},
{"Title": "ytarchive vs yt-dlp on video afterwards", "Author": "u/idle_cat", "Content": "On youtube, I archive livestreams of a channel. Is the live archive recording I get by using ytarchive a higher quality then the video I would get with yt-dlp that's processed afterwards? From my understanding the video goes through youtube's compression. Is the compression really strong in your opinion? I am wondering if it's worth getting the vod to save space. Side questions: Why do people put --format \"bv*+ba/b\" or something similar to get when yt-dlp already has it set to get the individual best audio and video as the default? https://github.com/yt-dlp/yt-dlp?tab=readme-ov-file#format-selection"},
{"Title": "Retire a drive after a single pending sector?", "Author": "u/Most_Mix_7505", "Content": "What would you all do?"},
{"Title": "Backup my network pc's to my OMV Nas", "Author": "u/SbM_Yggdrassil", "Content": "Hi all, I'm at that part of my homelab journey where I've setup a bunch of fun stuff and now I'm starting to think about helpful stuff like backups. I have a raspberry pi 4 setup with open media vault and docker running various services. I would like to add one or more services (in containers if necessary) to accomplish the following: Make an image of bootdrives of my computers on a schedule save those images to the attached storage. reimaging solution for recovery Backup secondary, non-bootdrives of my computers (just copying is probably fine) I'm just wondering how to best add services and which would do it. If I should use syncthing/duplicati + something else or if there is one thing that can do it all. I'm not sure how incremental backups fit in here either but I'd like to implement that to reduce the burden of network traffic (in my home we have to use wi-fi a lot). For drive images I've used the free version of Macrium reflect before (and I've heard of acronis), but just on one of my windows pc's. I'd like to have something scheduled from the server side for centralised management. Does it make sense what I'm trying to achieve?"},
{"Title": "Mini PC as NAS, good idea?", "Author": "u/smartyee", "Content": "No content"},
{"Title": "General Reminder Backups are Important", "Author": "u/TeamSylver", "Content": "Everyone here probably already knows that. I've just had all 3 drives in my desktop PC suddenly have problems. Thank god I can still read/write to the drives though. It's just god awful slow, especially during data transfers, where it will render the whole OS unusable until it's done. So that was a lot of pain and agony to temporarily move everything to the spare PC and laptops I have laying around (PC has 1tb, laptops have 1.5tb and 4.5tb). Means I now have no backups at all, since I still haven't finished setting up my work PC to be my off-site backup PC yet (it's basically manage/byo PC at my work BC it's such a small store). Annoying as well since that PC hosts my active directory and vaultwarden as well as the file server (thankfully I had a secondary active directory server set up, but no vaultwarden). Gotta love Crucial NVMes. All of them only 11 months old. Never again. 2 of 3 RMAs processed but I still gotta get data off of the third (OS drive) before I can post that. Edit: Forgot to mention they are Crucial P3 Plus 4TB NVMes"},
{"Title": "Dedup utility that FIRST finds duplicates by name/size/date, and THEN compares their content", "Author": "u/Msprg", "Content": "Hello, Let me preface with: I know there are a million posts about dedup tools already. Dedup by file content, checksum, attributes, similar photos, similar videosâ€¦ Yet somehow, I failed to find any tools that would be able to first filter out the majority of files that differ in filename / date / size and then on the results make sure that files are 100% surely duplicate by comparing their content. I've tried dupeguru, alldup, freefilesync, treesize, czkawka, I tried everything! (By voidtools that is). The point is that I'm either missing something, or that none of the tools offer the option I'm looking for. So here I am. Once again. Seeking answer to the eternal question: How do you deal with duplicates, fellow Data Hoarders?"},
{"Title": "What Non-NAS storage would you recommend?", "Author": "u/Bloodmoonwolf", "Content": "I currently have less than 800GB across 2 clouds and my laptop. I'm hitting my storage limit on Google and looking for a safe, local option. After losing everything on an old laptop that crashed, I started doing cloud storage, which is now becoming expensive. My current laptop is an old HP and I have yet to decide between a new Windows laptop or a Chromebook. I have a Plex library I would like to expand, even bought an external DVD reader to start the library. I don't necessarily need NAS. Plugging something into the TV or my laptop would be fine when I want to watch something on Plex (which isn't very often). The same goes for when I need to do a regular backup of files. I would prefer to buy something once instead of paying a monthly subscription and to not add another constant draw on our power supply. Most of the storage is for movies/shows, photos, and PDF scans of documents from when I went paperless. I would like to add music to this once I figure out a few things. What type/size/brand of local storage would you recommend for my situation?"},
{"Title": "Ripping dvd/blu-rays question for auto ripping? via bash script", "Author": "u/1michaelbrown", "Content": "I have setup a bash script to autorip so far it is working but with errors. So how would I fix the errors or do this a better way. Errors I am having Jun 17 20:24:15 markvm5 (udev-worker)[9984]: sr0: Process '/bin/systemctl start makemkv-rip.service' failed with exit code 1. Jun 17 20:24:15 markvm5 (udev-worker)[9995]: sr1: Process '/bin/systemctl start makemkv-rip.service' failed with exit code 1. Jun 17 20:26:04 markvm5 (udev-worker)[10091]: sr1: Spawned process '/bin/systemctl start makemkv-rip.service' [10094] is taking longer than 56s to complete Jun 17 20:26:04 markvm5 systemd-udevd[467]: sr1: Worker [10091] processing SEQNUM=8075 is taking a long time Jun 17 20:28:04 markvm5 (udev-worker)[10091]: sr1: Spawned process '/bin/systemctl start makemkv-rip.service' [10094] timed out after 2min 56s, killing Jun 17 20:28:04 markvm5 systemd-udevd[467]: sr1: Worker [10091] processing SEQNUM=8075 killed Jun 17 20:28:04 markvm5 systemd-udevd[467]: sr1: Worker [10091] terminated by signal 9 (KILL). Jun 17 20:41:49 markvm5 (udev-worker)[10159]: sr1: Process '/bin/systemctl start makemkv-rip.service' failed with exit code 1. Jun 17 20:52:38 markvm5 (udev-worker)[10201]: sr1: Spawned process '/bin/systemctl start makemkv-rip.service' [10205] is taking longer than 47s to complete Jun 17 20:52:38 markvm5 systemd-udevd[467]: sr1: Worker [10201] processing SEQNUM=8077 is taking a long time Jun 17 20:24:15 markvm5 (udev-worker)[9984]: sr0: Process '/bin/systemctl start makemkv-rip.service' failed with exit code 1. Jun 17 20:24:15 markvm5 (udev-worker)[9995]: sr1: Process '/bin/systemctl start makemkv-rip.service' failed with exit code 1. Jun 17 20:26:04 markvm5 (udev-worker)[10091]: sr1: Spawned process '/bin/systemctl start makemkv-rip.service' [10094] is taking longer than 56s to complete Jun 17 20:26:04 markvm5 systemd-udevd[467]: sr1: Worker [10091] processing SEQNUM=8075 is taking a long time Jun 17 20:28:04 markvm5 (udev-worker)[10091]: sr1: Spawned process '/bin/systemctl start makemkv-rip.service' [10094] timed out after 2min 56s, killing Jun 17 20:28:04 markvm5 systemd-udevd[467]: sr1: Worker [10091] processing SEQNUM=8075 killed Jun 17 20:28:04 markvm5 systemd-udevd[467]: sr1: Worker [10091] terminated by signal 9 (KILL). Jun 17 20:41:49 markvm5 (udev-worker)[10159]: sr1: Process '/bin/systemctl start makemkv-rip.service' failed with exit code 1. Jun 17 20:52:38 markvm5 (udev-worker)[10201]: sr1: Spawned process '/bin/systemctl start makemkv-rip.service' [10205] is taking longer than 47s to complete Jun 17 20:52:38 markvm5 systemd-udevd[467]: sr1: Worker [10201] processing SEQNUM=8077 is taking a long time This is the process I used I setup a udev rule SUBSYSTEM==\"block\", ENV{ID_CDROM}==\"1\", ACTION==\"change\", RUN+=\"/bin/systemctl start makemkv-rip.service\"SUBSYSTEM==\"block\", ENV{ID_CDROM}==\"1\", ACTION==\"change\", RUN+=\"/bin/systemctl start makemkv-rip.service\" and the makemkv-rip.service at /etc/systemd/system/makemkv-rip.service`/etc/systemd/system/makemkv-rip.service [Unit] Description=AutoRip CD on insertion [Service] Type=oneshot RemainAfterExit=no ExecStart=/home/mike/autorip.sh ExecStop=killall autorip.sh [Unit] Description=AutoRip CD on insertion [Service] Type=oneshot RemainAfterExit=no ExecStart=/home/mike/autorip.sh ExecStop=killall autorip.sh It's weird because the script is working but still get these errors. Also need to figure out how to trigger encoding after rip. Also in my autorip script it is finding titles in ` TINFO` should it be finding them in CINFO."},
{"Title": "Extracting Subtitles from Patreon", "Author": "u/pinkwonderwall", "Content": "Is there a way to rip captions from Patreon videos? I'm talking about Patreon videos I already have access to through a paid subscription. I like to save a video's subtitles as a text file so I can ctrl+F search for a particular word and find the moment that topic is discussed. I've tried Chrome extensions, but none of them work with Patreon. I've also looked for other posts of people asking this question, and it seems like not many people are trying to do this lol. I searched Inspect and Page Source and didn't see any obvious solutions, but I'm inexperienced with that so I may be missing something."},
{"Title": "Exausted and burnt out due to caring for my data", "Author": "u/CreativeDog2024", "Content": "I have about 5TB of movies/tv shows/photos. I have 3 backups: one is my main frequently accessed HDD. Another is another HDD i keep in my drawer and the last is google drive. I'm not a programmer so idk how to verify whether there has been data loss but I make sure all the files on each are the same by using freefilesync (mac app). It takes so much time and I don't even know if the files are corrupted or not. Is there some cloud option that I can leave my data on and pay $10-15/month to forget about it, using it as a last resort if all my local backups are corrupted? I read quite a bit on this and people recommend backblaze (B2 I think, how do i even buy it?), AWS glacier and M-disks. I have no idea how to operate any of those because I don't code. I do use rclone for Gdrive though."},
{"Title": "Western Digital DC HC580 (CMR or SMR?) & DS923+", "Author": "u/sulicadiz", "Content": "I just bought a Western Digital DC HC580. I don't know an effective method to know if the drive is CMR or SMR. I have read here in the forum that I should buy a CMR hard drive to use with the NAS (I have a DS923+). Another question, before setting the NAS should I introduce all the drives I would be using? The NAS have 4 slots, so I guess the first step would be to buy 4 hard drive then set up the unit. I suppose If I insert 2 drives then the other 2 I would lose data. Anyway, total noob here, please help"},
{"Title": "Are there some software that provides multiple download links using VPN at once?", "Author": "u/ElonTastical", "Content": "Let's say you wanna download multiple links from keep2share, but in that site it is limited by single download, you have to wait two hours before you can download another file. Is there a way to bypass this like the idea I mentioned in the title?"},
{"Title": "What's the most HDD failures you've seen in a 6 month period?", "Author": "u/the_Athereon", "Content": "Genuine question. How many of you have had a year as bad as mine so far? 5 failures. 1 DOA Parity 1 and 2 went in January The first Replacement Drive was DOA Data Disks 5, 8 and 11 have since failed. I've been able to recover 90% of the data through the use of my backups and catching the problem in time. But seriously. 6 drives have died on me this year. And we're only half way through the year. They're dying so frequently that I can barely afford to replace them. Now. For the details. Parity Disk 2 had a physical fault of some kind. Reallocated sector counts when from 0 to 256 in one night. Parity 1 had a controller board failure (This will be a common cause. I've figured out the problem since this happened.) Data Disk 5 kicked the bucket spectacularly. The Seek Error Rate went from 85% accurate, which is the average in my server due to how many disks are in there. To 1% in the span of 3 days. Making it infuriatingly slow to get any data off the drive but still possible. Data 8 and 11 both experienced controller board failures. Strange drop outs in connection, hang ups, read and write error flags despite no data corruption either reading or writing. Obviously I couldn't trust those drives anymore. But this thing is, only 2 of these failures are genuine faults. The other 3 are my fault. The drives that had controller board failures, at least some of them, were due to how much pressure was being put on the sata connectors when I closed the side panel. Yes, I'm serious. In any other circumstance, the Define R6 would have ample room for sata power and data cables at the rear of the case. But when you have 11 drives and all their cables back there, the thickness of the noise dampening foam presses into those cables and puts dangerous amounts of pressure on the connectors. I proved this by running read checks on the \"failing drives\" with and without the side panel on. With it off, 1 drive had errors 100% of the time. With it on, all drives showed the same errors. Errors which disappeared when I removed the side panel... SMH. So now I need to replace yet more drives, the cables and the case. My server is a bottomless money pit. It has to be."},
{"Title": "New drives, questions about testing", "Author": "u/lilbud2000", "Content": "Yesterday I bought my first \"big\" refurb hard drives (2x12TB HGST drives, upgrade from a 2TB and 4TB drive). The current plan is to have one in my computer, and use the second as a backup with an external enclosure. Probably not the \"best\" way to do it, but it should suffice in the meantime. Currently waiting for them to ship and looking into the whole testing process in the meantime. I was wondering what would be the best way to test them, as I've read about a bunch of different ones (like smartctl, Badblocks, HD Sentinel, etc.) And it's making my head spin a bit. I guess my questions are as followed: What/how many tests need to be run on a refurb drive? I've seen some posts listing multiple long tests and others just saying a few SMART tests. Is there any general consensus? Badblocks is on Linux only, would that work on something like WSL? Or would I have to get a Linux machine/VM setup? I have a Pi 3 collecting dust, could that be used? How long would testing the drives take? I've seen that a full badblocks 4 pass run can take days or even a week of 24/7 running just for one drive. Does that sound right? I was thinking about using a secondary machine like my old Thinkpad if it was going to take a week. My desktop (where the drive will eventually end up) is in use daily, and I'd be a bit concerned about leaving it on but not killing the test accidentally. I'm a bit new to all this, only having a 2TB and 4TB drive for the past few years. Any help in making sense of all this would be appreciated."},
{"Title": "Ripping entire Russian Encyclopedia - viable?", "Author": "u/gulisav", "Content": "I'm not extremely tech savy, so I have some possibly silly questions. Two days ago it's been announced that Great Russian Encyclopedia has been given no funding this year at all and the encyclopedia will be discontinued. (The encyclopedia is a heir to the Great Soviet Encyclopedia, and is fairly decent as far as general encyclopedias go.) Apparently Russia has bigger priorities than funding an encyclopedia... So, I think I might try my hand at saving the encyclopedia's online edition, before it 404s. Now, there are two domains, bigenc and old.bigenc (both .ru domains), and I'll focus on the latter. It seems fairly simple to rip, because each encyclopedic article has a corresponding PDF file, with the URLs only changing their final number (with 6 or 7 digits). I could produce a list of all the possible URLs in Excel. However, if I were to feed that list to a download manager, I'm wondering if that would cause any serious issues on the part of the server. There's probably close to a hundred thousand articles available on the site, and the downloader would also have to check possibly millions of URLs that contain no PDFs. Would this be like a sort of borderline DDOS attack? Could my requests be blocked? Furthermore, even if I rip all that stuff, it would result in thousands of files with nothing in particular to identify them, as the filenames are just numbers. Is there a way to derive the article titles from the text within the PDFs (which ofc include the title of the article) and rename the files accordingly? (The PDFs themselves are small in size, so I'm not worrying about space constraints.)"},
{"Title": "Safe to buy 3 yr old HDD?", "Author": "u/Shumhow", "Content": "Found a seller online selling a 1TB laptop HDD with casing for about 15 USD. Says everything is alright with the HDD, it is from Seagate and 3 years old. I have tried looking up at how old is too old for HDD but I understand there is no definite answer for the 'use'. But would it be advisable to 'buy' one which is 3 years old? I barely have any experience with this, so please do help me out! Thank you!"},
{"Title": "How many percent is recommended to free space on HDD with btrfs under GNU/Linux ?", "Author": "u/Yukinoooo", "Content": "I want my HDD to be efficient, good performance, fast, no error messages like impossible to read folders or read mode, bad sectors... If I want to use my HDD, it's for media files like photos, videos, music..."},
{"Title": "What is your preferred way to archive YouTube videos?", "Author": "u/EfficiencyFine3560", "Content": "Do you just run yt-dlp on a channel and put each channel in its own folder? Do you have a more elaborate directory strcuture? Do you use archive scripts to save stuff like comments and description? Please let me know your system!"},
{"Title": "Best way to catalogue music", "Author": "u/Foreign_Factor4011", "Content": "I know this might not be the right community to ask this question, so if the moderators need to delete this post, go ahead. I have a lot of music on my hard drive (we're talking 1000+ songs) and I'd like to organize everything into playlists. Each .mp3 file has metadata and I have software to organize playlists. I think I'll create the folder like this: Music/Genre/Artist/Album/.mp3 Files Do you think there's a better way to organize it? I did some math and there would be at least 20% artists with maybe 1 music. There's another problem: some tracks aren't even part of an album. How can I improve this, if possible? Is there a better way to do it?"},
{"Title": "Dual NAS Media Backup + Plex (a bit lost)", "Author": "u/PuzzleHeadPistion", "Content": "Hi, I'm a bit lost on how to keep all my data safe. Currently I have an old desktop, i5-4690 + ASUS H97 Pro + 16Gb RAM, with 3Tb WD Red + 6Tb IronWolf + 8Tb Barracuda drives and 2.5GbE + 1GbE interfaces. This works as my Plex server and it's where I dump files from the desktop/laptop, it's running on Windows 10 for now, but about to switch to FreeBSD or TrueNAS (or Proxmox?) with ZFS pool. Now I've added an Asustor AS1102TL Drivestor 2 Lite (2 bays, 1GbE) which is probably being returned for an AS1104T Drivestor 4 (4 bays, 2,5GbE). It is supposed to be a remote NAS using Wireguard, that's why I didn't care for 1GbE, but the initial backup is taking a LONG time. The price difference is only 100â‚¬ for more bays and speed (useful for full copies and full restores if needed). Here there's a 6Tb WD \"white label\" and an 8Tb Barracuda as single volumes (JBOD looks risky and can't use RAID with different drives). Part of my issue is which file transfer protocol to use. NFS? My desktop and laptop are Windows, not sure NFS works properly. FTP? Or SMB? SMB is giving me speed issues, not going over 150MBps and for some reason when cloning the 6Tb IronWolf to the 6Tb WD \"white label\" the speed sinks to 10MBps. It's copying RAW photos and videos, like thousands of 50-100Mb files mostly. Having file access sorted, what's your recommendation for file transfer/backup? Asustor Backup Plan? Paragon Backup and Recovery? Macrium Reflect? Or, since I already own SyncBackPro, just use that? This question is related to both, from my computers to the main NAS and main NAS to the Asustor. A little guidance will be much appreciated, since I want to go through this once and \"forget\". My day job is IT PM so I know my way around a computer, but by far not an expert in this area. ty"},
{"Title": "What is the best way to go about cloning a 1 tb HDD with 48 bad sectors to an ssd?", "Author": "u/mudcakes2000", "Content": "I've installed aoemi backupper and ive seen there is an option to clone \"sector by sector\" however I have heard for large drives this can take hundred of hours. Will cloning the standard way work if I have some bad sectors? Or would this be detrimental? Im also not keen on doing it the sector by sector way as im worried my hard drive might fail during the long process. Does anyone have any experience with this ? Thanks"},
{"Title": "Has anyone tried a drive bigger than 16TB on an older Areca-RAID-card? (1260)", "Author": "u/flac_rules", "Content": "I have an older Areca Raid-card. The manual claims it supports very large drives with 48 bit LBA, but i also found a google cached search result from the areca site that claims the following: \"The maximum capacity of HDDs for Areca RAID controller's old version firmware supports up to 16TB capacity. From firmware version V156-20190124....\" The newest firmware for the card is older than v156, but i don't know if this quote is for a particular card or in general, I can't find a complete changelog in the site for the changelog of the card I have. So i know it is a bit of a long shot, but has anyone tried a larger than 16TB drive on a Areaca 1260-card or something of around that age? And did it work?"},
{"Title": "HBA or raid card?", "Author": "u/Deep-Egg-6167", "Content": "Hello, I'm about to build another system for my movie collection.  I currently have an adaptec 3514-16 with 16x12TB drives in RAID 5.    I'm debating about the size of the drives to get next but I'm guessing I'll get 16x16TB drives in a RAID 5.1 config but I'm not 100% sure   My collection hasn't grown that much in the last 4 years so 50TB will go a long way. I've read many debates saying just get an HBA card instead of a RAID card but I've never actually seen any performance (real world statistics) comparing the two.   I'm still open to considering either one but would appreciate anyone who can point me to some real statistics vs opinion or based on what they've heard.   Your opinion may be 100% correct but I'd love some posted benchmarks."},
{"Title": "When are 30TB Seagate drives expected to become broadly a available?", "Author": "u/coffeenerd_", "Content": "Knowing that they've completed pilot tests a long time ago, are being deployed commercially, and getting mass produced... Wondering when would these 30TB Seagate drives become broadly available for us folks? Any have any real information or seeing any listings across any retailers / wholesalers?"},
{"Title": "YouTube files changed", "Author": "u/clickyk2019", "Content": "I've redownloaded some videos from youtube with yt-dlp (same videos, same yt-dlp options) however when comparing (diff) the files downloaded 6 months ago with the current ones some, but not all, are different. In some cases the new size is almost half of the previous one. Does anyone knows if youtube re-encode or modify videos periodically?"},
{"Title": "Advice on a crazy idea", "Author": "u/Arcau1", "Content": "I have seen a little N100 board with a 4x NVME hat on ali express (also comes in a N305 version) I was wondering if it was possible to maybe use the nvme > 6 sata riser cards ive seen also. So turning this into a little 24 drive beast of a nas, So brain trust of the community i ask you: Is this even possible? Would it cripple the N100? Would the speeds on the disks be just stupid slow? Has anyone tried something like this already and have any words of wisdom? TIA"},
{"Title": "Have joined the club", "Author": "u/mdwkelly", "Content": "Good day all. Well I have joined the club and am now the proud owner of a Supermicro 45 Bay JBOD Expansion Server Shelf 847E16-RJBOD1 with ~30 drives and ~460TB. Currently have it hooked up via 2 SFF-8088 to SFF-8088 cables to a LSI 9201-16e 6Gbps 16-lane external SAS HBA installed in my \"server\". This is replacing my previous setup with the same \"server\", 2 x LSI 9201-16e 6Gbps 16-lane external SAS HBA cards with 8 x SFF-8088 to 4 SATA cables running to 8 x RSV-SATA-Cage-34 that are/were on a couple of shelves. Been up and running for about a week and thoughts so far are: It looks cool in the 4-post 42U rack It is loud! (I knew it was loud but that holy s#$% moment when you first power it up) snapraid backup/scrub is only happening at about 2/3 the speed of before (sort of knew this as well) that was a lot of screws to remove the drives from the RSV caddies and screw them into the JBOD caddies. It looks cool! As mentioned, I have it hooked up via 2 SFF-8088 to SFF-8088 cables, one for each backplane, and am wondering if I can hook up the other 2 SFF-8088 ports on the JBOD with two more cables to get some additional speed. I did read the manual it it discusses the other two ports are used for daisy-chaining but you never know. Not being one to let well enough alone, I am now on the journey to repackage my two \"servers\", one from above and the other used for acquiring linux ISO's, into a couple of 2U cases so I can get them nicely into the rack. For the latter, am thinking about a RackChoice MicroATX/Mini-ITX 2U Rackmount Server Chassis as I am running a mitx MB and a couple of 3.5 drives and a couple of SSD's with no add-on cards so should fit just nice. For the former, the \"server\" hooked to the JBOD, I am looking at this RackChoice 2U Rackmount Server Chassis as it allows full height cards to be used via PCI riser so I can keep using the HBA card I already have. The other option I am looking at is picking up a LSI SAS 9300-8e 12Gb/s SATA/SAS and a couple SFF-8644 to Mini SAS SFF-8088 cables and a 2nd RackChoice MicroATX/Mini-ITX 2U Rackmount Server Chassis but I am not sure if the SAS3 HBA is backwards compatible with SAS2 backplanes. Anyway, the adventure continues .... Thanks"},
{"Title": "Android RClone app mounts remote drives with random seek support", "Author": "u/fourDnet", "Content": "Found this nifty tool recently: RSAF - An Android Storage Access Framework document provider for rclone Based on rclone 1.67 (latest version). First app that I've seen for android that supports mounting arbitrary remote drives (google drive, WebDAV, SFTP) and supports streaming. Had to manually add it to the battery optimization exclusion list on a OnePlus device (like in 3 different places, for background running, Optimize Battery Use etc.), or it would lose connection. But after adding it to the battery exclusion list, I could reach over 280 megabits/s streaming 4K HDR video from a local linux machine! Basically added a config for WebDAV (or SFTP), added an alias to go to the specific folder, and opened that folder within the app. Was able to stream using VLC 4K HDR with very rare stutters via the local wifi."},
{"Title": "How do you keep maxview storage manager open?", "Author": "u/Deep-Egg-6167", "Content": "Hello, I like to check when a raid is building (takes a couple of days) what percent it is at.  It logs out every 15 minutes or so - any guess how to leave it logged in?"},
{"Title": "Archiving Steam games that natively do not have DRM?", "Author": "u/Gwyn777", "Content": "Hello! I recently bought Cruelty Squad from steam. It is DRM free from the get go. What files do I need to save to archive the game? Something where I could theoretically uninstall steam, download it from my storage, then play without steam. Thank you for any help!"},
{"Title": "can anyone tell if this is molded or crimped molex to sata cable?", "Author": "u/KhanhEVB", "Content": "No content"},
{"Title": "Adobe Document Cloud help", "Author": "u/MatooMan", "Content": "Looking to download local copies to my PC from the following publications, anyone able to help me find an automated way to do this? An add on or program ideally. Thanks https://archives.thepipingcentre.co.uk/publications"},
{"Title": "Ripping dvd/blu-rays question for auto ripping? via bash script", "Author": "u/1michaelbrown", "Content": "I have setup a bash script to autorip so far it is working but with errors. So how would I fix the errors or do this a better way. Errors I am having Jun 17 20:24:15 markvm5 (udev-worker)[9984]: sr0: Process '/bin/systemctl start makemkv-rip.service' failed with exit code 1. Jun 17 20:24:15 markvm5 (udev-worker)[9995]: sr1: Process '/bin/systemctl start makemkv-rip.service' failed with exit code 1. Jun 17 20:26:04 markvm5 (udev-worker)[10091]: sr1: Spawned process '/bin/systemctl start makemkv-rip.service' [10094] is taking longer than 56s to complete Jun 17 20:26:04 markvm5 systemd-udevd[467]: sr1: Worker [10091] processing SEQNUM=8075 is taking a long time Jun 17 20:28:04 markvm5 (udev-worker)[10091]: sr1: Spawned process '/bin/systemctl start makemkv-rip.service' [10094] timed out after 2min 56s, killing Jun 17 20:28:04 markvm5 systemd-udevd[467]: sr1: Worker [10091] processing SEQNUM=8075 killed Jun 17 20:28:04 markvm5 systemd-udevd[467]: sr1: Worker [10091] terminated by signal 9 (KILL). Jun 17 20:41:49 markvm5 (udev-worker)[10159]: sr1: Process '/bin/systemctl start makemkv-rip.service' failed with exit code 1. Jun 17 20:52:38 markvm5 (udev-worker)[10201]: sr1: Spawned process '/bin/systemctl start makemkv-rip.service' [10205] is taking longer than 47s to complete Jun 17 20:52:38 markvm5 systemd-udevd[467]: sr1: Worker [10201] processing SEQNUM=8077 is taking a long time Jun 17 20:24:15 markvm5 (udev-worker)[9984]: sr0: Process '/bin/systemctl start makemkv-rip.service' failed with exit code 1. Jun 17 20:24:15 markvm5 (udev-worker)[9995]: sr1: Process '/bin/systemctl start makemkv-rip.service' failed with exit code 1. Jun 17 20:26:04 markvm5 (udev-worker)[10091]: sr1: Spawned process '/bin/systemctl start makemkv-rip.service' [10094] is taking longer than 56s to complete Jun 17 20:26:04 markvm5 systemd-udevd[467]: sr1: Worker [10091] processing SEQNUM=8075 is taking a long time Jun 17 20:28:04 markvm5 (udev-worker)[10091]: sr1: Spawned process '/bin/systemctl start makemkv-rip.service' [10094] timed out after 2min 56s, killing Jun 17 20:28:04 markvm5 systemd-udevd[467]: sr1: Worker [10091] processing SEQNUM=8075 killed Jun 17 20:28:04 markvm5 systemd-udevd[467]: sr1: Worker [10091] terminated by signal 9 (KILL). Jun 17 20:41:49 markvm5 (udev-worker)[10159]: sr1: Process '/bin/systemctl start makemkv-rip.service' failed with exit code 1. Jun 17 20:52:38 markvm5 (udev-worker)[10201]: sr1: Spawned process '/bin/systemctl start makemkv-rip.service' [10205] is taking longer than 47s to complete Jun 17 20:52:38 markvm5 systemd-udevd[467]: sr1: Worker [10201] processing SEQNUM=8077 is taking a long time This is the process I used I setup a udev rule SUBSYSTEM==\"block\", ENV{ID_CDROM}==\"1\", ACTION==\"change\", RUN+=\"/bin/systemctl start makemkv-rip.service\"SUBSYSTEM==\"block\", ENV{ID_CDROM}==\"1\", ACTION==\"change\", RUN+=\"/bin/systemctl start makemkv-rip.service\" and the makemkv-rip.service at /etc/systemd/system/makemkv-rip.service`/etc/systemd/system/makemkv-rip.service [Unit] Description=AutoRip CD on insertion [Service] Type=oneshot RemainAfterExit=no ExecStart=/home/mike/autorip.sh ExecStop=killall autorip.sh [Unit] Description=AutoRip CD on insertion [Service] Type=oneshot RemainAfterExit=no ExecStart=/home/mike/autorip.sh ExecStop=killall autorip.sh It's weird because the script is working but still get these errors. Also need to figure out how to trigger encoding after rip. Also in my autorip script it is finding titles in ` TINFO` should it be finding them in CINFO."},
{"Title": "Extracting Subtitles from Patreon", "Author": "u/pinkwonderwall", "Content": "Is there a way to rip captions from Patreon videos? I'm talking about Patreon videos I already have access to through a paid subscription. I like to save a video's subtitles as a text file so I can ctrl+F search for a particular word and find the moment that topic is discussed. I've tried Chrome extensions, but none of them work with Patreon. I've also looked for other posts of people asking this question, and it seems like not many people are trying to do this lol. I searched Inspect and Page Source and didn't see any obvious solutions, but I'm inexperienced with that so I may be missing something."},
{"Title": "Exausted and burnt out due to caring for my data", "Author": "u/CreativeDog2024", "Content": "I have about 5TB of movies/tv shows/photos. I have 3 backups: one is my main frequently accessed HDD. Another is another HDD i keep in my drawer and the last is google drive. I'm not a programmer so idk how to verify whether there has been data loss but I make sure all the files on each are the same by using freefilesync (mac app). It takes so much time and I don't even know if the files are corrupted or not. Is there some cloud option that I can leave my data on and pay $10-15/month to forget about it, using it as a last resort if all my local backups are corrupted? I read quite a bit on this and people recommend backblaze (B2 I think, how do i even buy it?), AWS glacier and M-disks. I have no idea how to operate any of those because I don't code. I do use rclone for Gdrive though."},
{"Title": "Western Digital DC HC580 (CMR or SMR?) & DS923+", "Author": "u/sulicadiz", "Content": "I just bought a Western Digital DC HC580. I don't know an effective method to know if the drive is CMR or SMR. I have read here in the forum that I should buy a CMR hard drive to use with the NAS (I have a DS923+). Another question, before setting the NAS should I introduce all the drives I would be using? The NAS have 4 slots, so I guess the first step would be to buy 4 hard drive then set up the unit. I suppose If I insert 2 drives then the other 2 I would lose data. Anyway, total noob here, please help"},
{"Title": "Are there some software that provides multiple download links using VPN at once?", "Author": "u/ElonTastical", "Content": "Let's say you wanna download multiple links from keep2share, but in that site it is limited by single download, you have to wait two hours before you can download another file. Is there a way to bypass this like the idea I mentioned in the title?"},
{"Title": "What's the most HDD failures you've seen in a 6 month period?", "Author": "u/the_Athereon", "Content": "Genuine question. How many of you have had a year as bad as mine so far? 5 failures. 1 DOA Parity 1 and 2 went in January The first Replacement Drive was DOA Data Disks 5, 8 and 11 have since failed. I've been able to recover 90% of the data through the use of my backups and catching the problem in time. But seriously. 6 drives have died on me this year. And we're only half way through the year. They're dying so frequently that I can barely afford to replace them. Now. For the details. Parity Disk 2 had a physical fault of some kind. Reallocated sector counts when from 0 to 256 in one night. Parity 1 had a controller board failure (This will be a common cause. I've figured out the problem since this happened.) Data Disk 5 kicked the bucket spectacularly. The Seek Error Rate went from 85% accurate, which is the average in my server due to how many disks are in there. To 1% in the span of 3 days. Making it infuriatingly slow to get any data off the drive but still possible. Data 8 and 11 both experienced controller board failures. Strange drop outs in connection, hang ups, read and write error flags despite no data corruption either reading or writing. Obviously I couldn't trust those drives anymore. But this thing is, only 2 of these failures are genuine faults. The other 3 are my fault. The drives that had controller board failures, at least some of them, were due to how much pressure was being put on the sata connectors when I closed the side panel. Yes, I'm serious. In any other circumstance, the Define R6 would have ample room for sata power and data cables at the rear of the case. But when you have 11 drives and all their cables back there, the thickness of the noise dampening foam presses into those cables and puts dangerous amounts of pressure on the connectors. I proved this by running read checks on the \"failing drives\" with and without the side panel on. With it off, 1 drive had errors 100% of the time. With it on, all drives showed the same errors. Errors which disappeared when I removed the side panel... SMH. So now I need to replace yet more drives, the cables and the case. My server is a bottomless money pit. It has to be."},
{"Title": "New drives, questions about testing", "Author": "u/lilbud2000", "Content": "Yesterday I bought my first \"big\" refurb hard drives (2x12TB HGST drives, upgrade from a 2TB and 4TB drive). The current plan is to have one in my computer, and use the second as a backup with an external enclosure. Probably not the \"best\" way to do it, but it should suffice in the meantime. Currently waiting for them to ship and looking into the whole testing process in the meantime. I was wondering what would be the best way to test them, as I've read about a bunch of different ones (like smartctl, Badblocks, HD Sentinel, etc.) And it's making my head spin a bit. I guess my questions are as followed: What/how many tests need to be run on a refurb drive? I've seen some posts listing multiple long tests and others just saying a few SMART tests. Is there any general consensus? Badblocks is on Linux only, would that work on something like WSL? Or would I have to get a Linux machine/VM setup? I have a Pi 3 collecting dust, could that be used? How long would testing the drives take? I've seen that a full badblocks 4 pass run can take days or even a week of 24/7 running just for one drive. Does that sound right? I was thinking about using a secondary machine like my old Thinkpad if it was going to take a week. My desktop (where the drive will eventually end up) is in use daily, and I'd be a bit concerned about leaving it on but not killing the test accidentally. I'm a bit new to all this, only having a 2TB and 4TB drive for the past few years. Any help in making sense of all this would be appreciated."},
{"Title": "Ripping entire Russian Encyclopedia - viable?", "Author": "u/gulisav", "Content": "I'm not extremely tech savy, so I have some possibly silly questions. Two days ago it's been announced that Great Russian Encyclopedia has been given no funding this year at all and the encyclopedia will be discontinued. (The encyclopedia is a heir to the Great Soviet Encyclopedia, and is fairly decent as far as general encyclopedias go.) Apparently Russia has bigger priorities than funding an encyclopedia... So, I think I might try my hand at saving the encyclopedia's online edition, before it 404s. Now, there are two domains, bigenc and old.bigenc (both .ru domains), and I'll focus on the latter. It seems fairly simple to rip, because each encyclopedic article has a corresponding PDF file, with the URLs only changing their final number (with 6 or 7 digits). I could produce a list of all the possible URLs in Excel. However, if I were to feed that list to a download manager, I'm wondering if that would cause any serious issues on the part of the server. There's probably close to a hundred thousand articles available on the site, and the downloader would also have to check possibly millions of URLs that contain no PDFs. Would this be like a sort of borderline DDOS attack? Could my requests be blocked? Furthermore, even if I rip all that stuff, it would result in thousands of files with nothing in particular to identify them, as the filenames are just numbers. Is there a way to derive the article titles from the text within the PDFs (which ofc include the title of the article) and rename the files accordingly? (The PDFs themselves are small in size, so I'm not worrying about space constraints.)"},
{"Title": "Safe to buy 3 yr old HDD?", "Author": "u/Shumhow", "Content": "Found a seller online selling a 1TB laptop HDD with casing for about 15 USD. Says everything is alright with the HDD, it is from Seagate and 3 years old. I have tried looking up at how old is too old for HDD but I understand there is no definite answer for the 'use'. But would it be advisable to 'buy' one which is 3 years old? I barely have any experience with this, so please do help me out! Thank you!"},
{"Title": "How many percent is recommended to free space on HDD with btrfs under GNU/Linux ?", "Author": "u/Yukinoooo", "Content": "I want my HDD to be efficient, good performance, fast, no error messages like impossible to read folders or read mode, bad sectors... If I want to use my HDD, it's for media files like photos, videos, music..."},
{"Title": "What is your preferred way to archive YouTube videos?", "Author": "u/EfficiencyFine3560", "Content": "Do you just run yt-dlp on a channel and put each channel in its own folder? Do you have a more elaborate directory strcuture? Do you use archive scripts to save stuff like comments and description? Please let me know your system!"},
{"Title": "Best way to catalogue music", "Author": "u/Foreign_Factor4011", "Content": "I know this might not be the right community to ask this question, so if the moderators need to delete this post, go ahead. I have a lot of music on my hard drive (we're talking 1000+ songs) and I'd like to organize everything into playlists. Each .mp3 file has metadata and I have software to organize playlists. I think I'll create the folder like this: Music/Genre/Artist/Album/.mp3 Files Do you think there's a better way to organize it? I did some math and there would be at least 20% artists with maybe 1 music. There's another problem: some tracks aren't even part of an album. How can I improve this, if possible? Is there a better way to do it?"},
{"Title": "Dual NAS Media Backup + Plex (a bit lost)", "Author": "u/PuzzleHeadPistion", "Content": "Hi, I'm a bit lost on how to keep all my data safe. Currently I have an old desktop, i5-4690 + ASUS H97 Pro + 16Gb RAM, with 3Tb WD Red + 6Tb IronWolf + 8Tb Barracuda drives and 2.5GbE + 1GbE interfaces. This works as my Plex server and it's where I dump files from the desktop/laptop, it's running on Windows 10 for now, but about to switch to FreeBSD or TrueNAS (or Proxmox?) with ZFS pool. Now I've added an Asustor AS1102TL Drivestor 2 Lite (2 bays, 1GbE) which is probably being returned for an AS1104T Drivestor 4 (4 bays, 2,5GbE). It is supposed to be a remote NAS using Wireguard, that's why I didn't care for 1GbE, but the initial backup is taking a LONG time. The price difference is only 100â‚¬ for more bays and speed (useful for full copies and full restores if needed). Here there's a 6Tb WD \"white label\" and an 8Tb Barracuda as single volumes (JBOD looks risky and can't use RAID with different drives). Part of my issue is which file transfer protocol to use. NFS? My desktop and laptop are Windows, not sure NFS works properly. FTP? Or SMB? SMB is giving me speed issues, not going over 150MBps and for some reason when cloning the 6Tb IronWolf to the 6Tb WD \"white label\" the speed sinks to 10MBps. It's copying RAW photos and videos, like thousands of 50-100Mb files mostly. Having file access sorted, what's your recommendation for file transfer/backup? Asustor Backup Plan? Paragon Backup and Recovery? Macrium Reflect? Or, since I already own SyncBackPro, just use that? This question is related to both, from my computers to the main NAS and main NAS to the Asustor. A little guidance will be much appreciated, since I want to go through this once and \"forget\". My day job is IT PM so I know my way around a computer, but by far not an expert in this area. ty"},
{"Title": "What is the best way to go about cloning a 1 tb HDD with 48 bad sectors to an ssd?", "Author": "u/mudcakes2000", "Content": "I've installed aoemi backupper and ive seen there is an option to clone \"sector by sector\" however I have heard for large drives this can take hundred of hours. Will cloning the standard way work if I have some bad sectors? Or would this be detrimental? Im also not keen on doing it the sector by sector way as im worried my hard drive might fail during the long process. Does anyone have any experience with this ? Thanks"},
{"Title": "Has anyone tried a drive bigger than 16TB on an older Areca-RAID-card? (1260)", "Author": "u/flac_rules", "Content": "I have an older Areca Raid-card. The manual claims it supports very large drives with 48 bit LBA, but i also found a google cached search result from the areca site that claims the following: \"The maximum capacity of HDDs for Areca RAID controller's old version firmware supports up to 16TB capacity. From firmware version V156-20190124....\" The newest firmware for the card is older than v156, but i don't know if this quote is for a particular card or in general, I can't find a complete changelog in the site for the changelog of the card I have. So i know it is a bit of a long shot, but has anyone tried a larger than 16TB drive on a Areaca 1260-card or something of around that age? And did it work?"},
{"Title": "HBA or raid card?", "Author": "u/Deep-Egg-6167", "Content": "Hello, I'm about to build another system for my movie collection.  I currently have an adaptec 3514-16 with 16x12TB drives in RAID 5.    I'm debating about the size of the drives to get next but I'm guessing I'll get 16x16TB drives in a RAID 5.1 config but I'm not 100% sure   My collection hasn't grown that much in the last 4 years so 50TB will go a long way. I've read many debates saying just get an HBA card instead of a RAID card but I've never actually seen any performance (real world statistics) comparing the two.   I'm still open to considering either one but would appreciate anyone who can point me to some real statistics vs opinion or based on what they've heard.   Your opinion may be 100% correct but I'd love some posted benchmarks."},
{"Title": "When are 30TB Seagate drives expected to become broadly a available?", "Author": "u/coffeenerd_", "Content": "Knowing that they've completed pilot tests a long time ago, are being deployed commercially, and getting mass produced... Wondering when would these 30TB Seagate drives become broadly available for us folks? Any have any real information or seeing any listings across any retailers / wholesalers?"},
{"Title": "YouTube files changed", "Author": "u/clickyk2019", "Content": "I've redownloaded some videos from youtube with yt-dlp (same videos, same yt-dlp options) however when comparing (diff) the files downloaded 6 months ago with the current ones some, but not all, are different. In some cases the new size is almost half of the previous one. Does anyone knows if youtube re-encode or modify videos periodically?"},
{"Title": "Advice on a crazy idea", "Author": "u/Arcau1", "Content": "I have seen a little N100 board with a 4x NVME hat on ali express (also comes in a N305 version) I was wondering if it was possible to maybe use the nvme > 6 sata riser cards ive seen also. So turning this into a little 24 drive beast of a nas, So brain trust of the community i ask you: Is this even possible? Would it cripple the N100? Would the speeds on the disks be just stupid slow? Has anyone tried something like this already and have any words of wisdom? TIA"},
{"Title": "Have joined the club", "Author": "u/mdwkelly", "Content": "Good day all. Well I have joined the club and am now the proud owner of a Supermicro 45 Bay JBOD Expansion Server Shelf 847E16-RJBOD1 with ~30 drives and ~460TB. Currently have it hooked up via 2 SFF-8088 to SFF-8088 cables to a LSI 9201-16e 6Gbps 16-lane external SAS HBA installed in my \"server\". This is replacing my previous setup with the same \"server\", 2 x LSI 9201-16e 6Gbps 16-lane external SAS HBA cards with 8 x SFF-8088 to 4 SATA cables running to 8 x RSV-SATA-Cage-34 that are/were on a couple of shelves. Been up and running for about a week and thoughts so far are: It looks cool in the 4-post 42U rack It is loud! (I knew it was loud but that holy s#$% moment when you first power it up) snapraid backup/scrub is only happening at about 2/3 the speed of before (sort of knew this as well) that was a lot of screws to remove the drives from the RSV caddies and screw them into the JBOD caddies. It looks cool! As mentioned, I have it hooked up via 2 SFF-8088 to SFF-8088 cables, one for each backplane, and am wondering if I can hook up the other 2 SFF-8088 ports on the JBOD with two more cables to get some additional speed. I did read the manual it it discusses the other two ports are used for daisy-chaining but you never know. Not being one to let well enough alone, I am now on the journey to repackage my two \"servers\", one from above and the other used for acquiring linux ISO's, into a couple of 2U cases so I can get them nicely into the rack. For the latter, am thinking about a RackChoice MicroATX/Mini-ITX 2U Rackmount Server Chassis as I am running a mitx MB and a couple of 3.5 drives and a couple of SSD's with no add-on cards so should fit just nice. For the former, the \"server\" hooked to the JBOD, I am looking at this RackChoice 2U Rackmount Server Chassis as it allows full height cards to be used via PCI riser so I can keep using the HBA card I already have. The other option I am looking at is picking up a LSI SAS 9300-8e 12Gb/s SATA/SAS and a couple SFF-8644 to Mini SAS SFF-8088 cables and a 2nd RackChoice MicroATX/Mini-ITX 2U Rackmount Server Chassis but I am not sure if the SAS3 HBA is backwards compatible with SAS2 backplanes. Anyway, the adventure continues .... Thanks"},
{"Title": "Android RClone app mounts remote drives with random seek support", "Author": "u/fourDnet", "Content": "Found this nifty tool recently: RSAF - An Android Storage Access Framework document provider for rclone Based on rclone 1.67 (latest version). First app that I've seen for android that supports mounting arbitrary remote drives (google drive, WebDAV, SFTP) and supports streaming. Had to manually add it to the battery optimization exclusion list on a OnePlus device (like in 3 different places, for background running, Optimize Battery Use etc.), or it would lose connection. But after adding it to the battery exclusion list, I could reach over 280 megabits/s streaming 4K HDR video from a local linux machine! Basically added a config for WebDAV (or SFTP), added an alias to go to the specific folder, and opened that folder within the app. Was able to stream using VLC 4K HDR with very rare stutters via the local wifi."},
{"Title": "How do you keep maxview storage manager open?", "Author": "u/Deep-Egg-6167", "Content": "Hello, I like to check when a raid is building (takes a couple of days) what percent it is at.  It logs out every 15 minutes or so - any guess how to leave it logged in?"},
{"Title": "Archiving Steam games that natively do not have DRM?", "Author": "u/Gwyn777", "Content": "Hello! I recently bought Cruelty Squad from steam. It is DRM free from the get go. What files do I need to save to archive the game? Something where I could theoretically uninstall steam, download it from my storage, then play without steam. Thank you for any help!"},
{"Title": "can anyone tell if this is molded or crimped molex to sata cable?", "Author": "u/KhanhEVB", "Content": "No content"},
{"Title": "Adobe Document Cloud help", "Author": "u/MatooMan", "Content": "Looking to download local copies to my PC from the following publications, anyone able to help me find an automated way to do this? An add on or program ideally. Thanks https://archives.thepipingcentre.co.uk/publications"},
{"Title": "Compression", "Author": "u/Void-ux", "Content": "Hey, I store a relatively small amount of media (movies and tv), and some of it I likely won't watch for decades. Most of it is 1080p, and I keep my fav latest TV shows in 4k. Is there any way to losslessly compress this media? From what I've heard 1080p is best in h.264, which it is. The 4k stuff could be converted to h.265 10-bit, and I have done this with HandBrake, but I'm skeptical of how lossless it is since the file size reductions are ridiculous efficient."},
{"Title": "Would you return this dented 16TB hard drive?", "Author": "u/P10tr3kkk", "Content": "No content"},
{"Title": "Power Disable HDD in QNAP NAS", "Author": "u/CartoonistNo6669", "Content": "I've not gotten either yet, but I'm struggling to figure out of this is compatible. I'm planning on getting a TS-435XeU ( https://www.qnap.com/en-us/product/ts-435xeu# ) And filling it with 4x 10TB renewed Ultrastar He10 HDD ( https://a.co/d/8EcBiGu ) in RAID5. I see that these hard drives have Power Disable, and I can't find a concrete indication on QNAP's website if the NAS supports those. Any advice here?"},
{"Title": "Ripping dubs from dvd and adding to movies", "Author": "u/Wibble_Webble", "Content": "Hi, i want to start a project but i have no idea where to start. I have a barbie movie collection that has dubs in my native language. I found that someone has remastered these movies in HD and i have already downloaded them. The movies are however in English. I want to rip the audio track from the dvd's but just of the main movie and replace the original audio track from the remastered movies. I've already tried to rip the movie with handbreak but the mp4 file came out choppy in image and in audio. I also don't know how to replace the original audio track once i have successfully  ripped the audio from the dvd as well as sync it. I've read about using ffmpeg but i have no experience with it except for yt-dlp. If anyone can provide an incredibly dumbed down explanation, i would really appreciate it."},
{"Title": "VHS to HDMI upscaller", "Author": "u/Point-Frosty", "Content": "Hey yall! Looking for recommendations on a box that will convert composite coming from my vcr to hdmi. I would like to upscale to 1080. I am dealing with old vhs tapes that get snowy at times. I did purchase a Kanex Pro $80 unit and it does pretty good upscaling and interlacing. Unfortunately with my snowy tapes, the unit flashes a big source menu on the screen when snow happens. Does anyone know of a 1080 upscaller that wonâ€™t flash menuâ€™s on the screen during my capture to OBS? Thanks so much for the advice!"},
{"Title": "VHS to DVD/Digital", "Author": "u/Djinnimania", "Content": "For context, I dropped the ball recently by forgetting both Motherâ€™s Day and Fatherâ€™s Day until the day of. They donâ€™t seem upset, but I want to make it up to them by converting their wedding videos from VHS to either digital, DVD, or Blu-Ray. What would be the best way to do so?"},
{"Title": "External, internal hard drives", "Author": "u/WriteCodeBroh", "Content": "Letâ€™s say a friend of mine was thinking about, hypothetically of course, buying a used workstation off of Facebook marketplace. And a lot of those modern used workstations donâ€™t have a whole lot of space for hard drives. What if this friend drilled a hole in the side of the case, ran the required power and data cables (SATA probably) out to a custom built case with a backplane and a whole bunch of hard drives? I assume there are EMI and short related risks but frankly Iâ€™ve seen people run whole computers on open wooden racks with fans blowing on the components, function just fine for years, also donâ€™t plan to touch/move things much and theyâ€™ll be off the floor and out of the way. So anyway, all that being said, how stupid of an idea is this?"},
{"Title": "files taking too much space in external hard drive?", "Author": "u/Tikas92", "Content": "Hello everyone. I have a question and I was wondering if anyone knows what's up with that. I recently transfered 3,53 TB worth of files from a 4TB Seagate to an 8TB Lacie but for some reason those files now take up 4,46 TB on the new drive. Why is that happening?"},
{"Title": "Getting a bit lost when it comes to good syncing/cloud storage options", "Author": "u/Creator13", "Content": "After using a laptop for the past six years and carrying it everywhere, I recently got a good desktop PC again. I've always played it super risky because all my data has never been properly backed up or duplicated. Now I want my files to be available between different (windows) computers. Ideally, every change would automatically get synced between both computers, through a cloud storage provider (that also stores the files for access from outside), as long as either computer is connected to the internet. Now, plenty of services offer this, in its most basic form. I get 1TB OneDrive though my parents' family plan and OneDrive is pretty nice. It integrates well enough into Windows. But there is one major limitation that makes it pretty much unsuitable for me: I don't want all my syncable data to be in one single folder. I want it spread out over different folders and even drives, because I run quite a few different drives with different purposes. Is there a service that basically offers a way to set custom endpoints for syncable folders? Say I have a cloud folder /Files , then I want a client that runs on any connected computer where I can set /Files to be synced to G:\\Files , or to C:\\Users\\some_user\\Documents\\CloudFiles on another computer? Ideally also affordable and reputable/secure of course, and for single individual users?"},
{"Title": "I have nearly 30 TB on external hard drives I want to find a permanent solution to keep my data as i continue increase the amount", "Author": "u/romic007", "Content": "Like the title states i have nearly 30 TB on several external hard drives. The files vary from (videos, fILms, series, pictures, music, documents, etc.) i looking for a permanent solution to keep my data as it continues to grow. I am not very tech savy at all I have seen things about nas, Synology stuff but im not too familiar with that stuff. I have heard good things about icloud but im i bit worried since my files were LEGALLY downloaded. Im worried that they could look at my stuff. I use my external hard drives daily and would like this permanent solution to be accessible whenever and wherever i am. I use my external hard drives on. My laptop via usb port and do not have a pc. I am leaning towards icloud or something equivalent to that since i wouldn't have to use external hard drives as much as i currently am now. Im planning on using this as my main backup system with external hard drives as secondary but like i said i use these hard drives daily would icloud be the best solution for my situation?"},
{"Title": "Data Compression", "Author": "u/elgato123", "Content": "Noticed that a drive was filling up, saw it was mostly log files...49Gb of log files. Broke out the ol' 7-zip and in ultra mode, it compressed it down to 1.8Gb. Wow I am wondering if there is any better compression or if this is about as good as it will get? Either way, I'm very impressed"},
{"Title": "New Build raid/filesystem recommendations", "Author": "u/NextRedditAccount0", "Content": "I have a new HL15 coming. I'm planning to use it for some docker for *arr, deluge, and as a NAS. I have a separate Plex server that will be accessing the files on the HL15. No plans of doing any VMs. I'm trying to figure out the best way to get good performance and some parity in case a drive fails. Yes I do have plenty of backups. My current drives in my synology is 8x8TB, 1x 500GB SSD, 1x 1TB SSD in SHR2. I also have 3x 16TB drives coming. My original plan was to run unraid but I don't want to lose out on performance due to the 8x8TB are all 5400rpms. I'm open to running unraid or truenas or proxmox or etc. My end goal is to get as much speed as possible while being able to survive a drive failure. No future drive expansion is planned ATM but would be nice if the new solution could support it. Also this will be on a 10gbps network. Thoughts?"},
{"Title": "Question re: drives in a RAID array I inherited from a relative", "Author": "u/RhetoricalAnswer-001", "Content": "Hope this is appropriate for the forum. I inherited a full height rackmount cabinet with 12 HDs. It looks like it belongs in a data center. I want to use the drives but my PC won't recognize them. Any tips on software that can \"see\" them, then wipe and reformat them?"},
{"Title": "Internet Archive Forced to Remove 500,000 books Due to Copyright Lawsuit", "Author": "u/Run_the_Line", "Content": "No content"},
{"Title": "How long for an SSD to do it's business after a long stint in cold storage?", "Author": "u/Captain_Starkiller", "Content": "I'm firing up a computer after it was offline for nearly two years. It's SSD is just a windows drive, the data storage drives are spinning rust. That said, I want to make sure the error protection has a chance to do its thing for the SSD, refill the charge traps so bit rot doesn't set in, ect. Does anyone have any idea how long I should let it run for, before turning it off again (for a month or two this time, not years) to make sure it does it's business?"},
{"Title": "Advice/Recommendation on a Personal Desktop Build with a focus on Data Storage that can also be used as a small homeserver for myself.", "Author": "u/curiousdoggo", "Content": "Goal: I know people typically separate their data storage/server/nas from their personal desktop so it can be left to do its designated tasks and so that it won't affect the performance of the desktop (if it has to serve a multitude of people or perform a lot of different tasks), but what if my server/nas needs are minimal and my primary focus is just a good personal desktop with a focus on datahoarding/data storage, some file sharing, running a few VMs, and possibly a few more server features? In this case, is it okay to just build a decently powerful modern personal desktop - killing 2 birds with 1 s tone, instead of building 2 computers, one for desktop (desperately need the upgrade now) and one specifically for a server/nas? Is this totally okay? I live by myself so aside from being a personal computer with data storage, it'll just be serving me alone at home. To be honest, at the moment, I pretty much just consume media on my desktop, so even nas/media server features like plex, jellyfins aren't even 100% necessary - though nice to have perhaps in the future. Summary : Decently powerful personal desktop with a focus on datahoarding/data storage that can also perform some server side of things like running VMs, and other homelab things down the road. OS : FreeBSD with ZFS + ECC Memory CASE : thinking of a big case like Fractal Design Define 7 XL that is capable of holding 14-18 HDDs. MOBO : SSD : What's a good NVMe 2.0 ?(maybe 1TB) CPU : amd or intel? what series/models would you guys recommend? MEM : looking for ECC ram as the main focus will be data storage. but how much memory will I need for zfs (assuming I will fill up the whole case with 18 HDDs down the road)? GPU : a gpu capable of driving maybe a LG DualUp 2560 x 2880 with a 34\" 1440p 3440Ã—1440 ultrawide monitor. I will also be doing some photo editing with darktable, rawtherapee, etc. as well so a designated gpu that is good enough should be enough. (don't think there will be much gaming). PSU : how big of a power supply? keep in mind the full capacity is 18 HDD, with dual monitor, etc. CPU COOLER : HDD : thinking of 18 or 20TB seagate exos (are they too loud to use in the bedroom in a personal desktop? should i go for ironwolf pro?)"},
{"Title": "5x3.5 tool-less drive bay prices", "Author": "u/Doodarazumas", "Content": "I'm putting together a nas and I saw several threads where people recommended these kind of 3x5.25 to 5x3.5 hard drive cages https://www.amazon.com/dp/B00DWHLFMA/ref=cm_sw_r_cp_api_glt_fabc_ZJWWH4ZW3JXHXF89RMN3?_encoding=UTF8&th=1 https://www.amazon.com/dp/B00DWHLFMA/ref=cm_sw_r_cp_api_glt_fabc_ZJWWH4ZW3JXHXF89RMN3?_encoding=UTF8&th=1 plus silverstone/rosewill/etc. Now most of these threads were from a few years back and since then the prices on all these things have tripled or quadrupled. Even no-name aliexpress ones I've found were about $150. Is there some secret method to finding or making something like that (5x 3.5 drives tool-less with a backplane in 3x5.25 bays) for a more reasonable $50-75 or am I just going to be a caveman with a screwdriver: https://www.amazon.com/EMVANV-Stainless-5-25inch-Adapter-Bracket/dp/B0C1BQ36ML/ref=pd_ci_mcx_pspc_dp_d_2_i_1"},
{"Title": "Rackmount case recs with >12 bays", "Author": "u/jtscribe52", "Content": "Iâ€™ve currently maxed out my tower and have been trying to figure out a rack mount replacement that wonâ€™t break the bank. A lot of older recs point to SuperMicro cases, but those have really jumped in price the past few years. Iâ€™ve seen some other post recommending Sliger,  but I donâ€™t see anything with more than 10 bays,  even in a 4U. Not opposed to something from AliExpress, but curious what everyone is using of late. Thanks in advance!"},
{"Title": "Any ideas for how to acquire BluRays of foreign versions of films? (Specifically, Russian Cars 2 and Planes, and Chinese Zootopia.)", "Author": "u/CtrlAltSysRq", "Content": "Up front: I don't mind paying for it. I'm not asking for help doing piracy 101 or anything. Hi fellow hoarders. I'm not sure if this is the right place to post this, but I figure this is a good place to ask fellow hoarders about what they hoard. Feel free to lmk if you think there's a better place to post this. I'm trying to archive the different versions of Disney movies that have regional differences. For example, I have the different versions of Inside Out where Riley hates either broccoli (US) or green bell peppers (Japan). Most movies only have a few variations, but Cars 2 and Planes each has about 7 variations each where the design of a car or a plane changes depending on the region. I've gotten nearly all of them, and almost all the variations of all the other movies, except for three: Chinese Zootopia (secondary newscaster is a panda) Russian Cars 2 (Car is styled as the Russian flag) Russian Planes (the \"sexy\" girl plane is styled after the Russian flag) You can probably see why I'm having trouble with these. Disney+ isn't offered in those countries for geopolitical reasons. I've scoured the internet that is available to me looking for ways to source these either physically or digitally, but even my non-public sources are not very interested in this kind of thing. So I'm reduced to looking for physical Blu-ray's of them and that is going about as well as you might expect. So I'm wondering if anyone has ideas of ways I might be able to swing this. Thanks in advance!"},
{"Title": "For photos and videos backup, is it better to use a cloud-based storage service or buy an SSD drive?", "Author": "u/jesuisapprenant", "Content": "I got a subscription for Dropbox at around $12 per month, and I was wondering if it is worth it to just buy a 2TB SSD drive since overall the cost is much cheaper (a nice SSD with 500mb r&w is about $119 so that's 10 months of storage cost for Dropbox, so I break even at 10 months. The pros of Dropbox is that I can access it anywhere, even on my phone, and I don't have to worry about data loss or drive failure or my disk getting stolen or lost. I also won't have to carry that disk around. The pros of an external drive is that it's much cheaper and it pays for itself in 10 months. I can also transfer data in and out much faster. I also don't need internet to access my files. Which solution is better for my use case? TIA"},
{"Title": "Question about using wget to download images from Newgrounds", "Author": "u/Glen_Garrett_Gayhart", "Content": "I've got a lot of urls like this: https://www.newgrounds.com/art/view/alvinhew/annika where one or more images are displayed. I want to use wget to get the images on these pages that have links like this: https://art.ngfiles.com/images/49000/49087_alvinhew_annika.jpg?f1254528733 but I'm not sure how I should configure wget to go from the first sort of url to target the second sort. I could just open all of the www.newgrounds urls and copy the art.ngfiles urls, but that would defeat the purpose of automating it. I want to download a lot of these, and I've got a batch file that will go through them all. How should I instruct wget to look at urls of the first www.newgrounds sort, and then download everything from urls of the second art.ngfiles sort? I don't mind if I get some extra files, like thumbnails and things, but I don't want wget spidering all over the website and potentially downloading things from pages other than the art.ngfiles links. ` Thanks in advance for any help!"},
{"Title": "GM service manual archiving", "Author": "u/Betelgeuse28", "Content": "Has anyone managed to download the newer service manuals from ACDelco TDS site? I bought a 3 day access pass to the site but I've had zero luck so far. Ive tried wget, Offline Explorer, HTTrack, and Webcopy. I'm not really trying to save an 18k page manual by rightclicking and save to pdf."},
{"Title": "Any thoughts on using something lie LBRY protocol for mass decentralised data hoarding?", "Author": "u/MasterDefibrillator", "Content": "Something like it, or the LBRY protocol itself."},
{"Title": "YouTube seems to be blocking accounts that are used with yt-dlp (by passing cookies)", "Author": "u/BowzasaurusRex", "Content": "No content"},
{"Title": "Looking for an external HDD to backup my NAS - what drives do you recommend?", "Author": "u/kavakravata", "Content": "Hey! I have a NAS with 16TB of active storage using RAID. As a newbie, I didnâ€™t even think about the possibility of my NAS drives failing with time, especially after reading horror stories with seagate ironwolf drives which Iâ€™m currently using. I hate SaaS and cloud, and would much prefer a local backup to my Synology NAS. What drives do you recommend? Been looking at the WD My Book 16TB or Easystore 18TB from Amazon, but Iâ€™m unsure. I donâ€™t care about house fires / theft, so ignore that risk of backup solutions. Taking all ideas! Thank you ðŸ¥°"},
{"Title": "Compression", "Author": "u/Void-ux", "Content": "Hey, I store a relatively small amount of media (movies and tv), and some of it I likely won't watch for decades. Most of it is 1080p, and I keep my fav latest TV shows in 4k. Is there any way to losslessly compress this media? From what I've heard 1080p is best in h.264, which it is. The 4k stuff could be converted to h.265 10-bit, and I have done this with HandBrake, but I'm skeptical of how lossless it is since the file size reductions are ridiculous efficient."},
{"Title": "Would you return this dented 16TB hard drive?", "Author": "u/P10tr3kkk", "Content": "No content"},
{"Title": "Power Disable HDD in QNAP NAS", "Author": "u/CartoonistNo6669", "Content": "I've not gotten either yet, but I'm struggling to figure out of this is compatible. I'm planning on getting a TS-435XeU ( https://www.qnap.com/en-us/product/ts-435xeu# ) And filling it with 4x 10TB renewed Ultrastar He10 HDD ( https://a.co/d/8EcBiGu ) in RAID5. I see that these hard drives have Power Disable, and I can't find a concrete indication on QNAP's website if the NAS supports those. Any advice here?"},
{"Title": "Ripping dubs from dvd and adding to movies", "Author": "u/Wibble_Webble", "Content": "Hi, i want to start a project but i have no idea where to start. I have a barbie movie collection that has dubs in my native language. I found that someone has remastered these movies in HD and i have already downloaded them. The movies are however in English. I want to rip the audio track from the dvd's but just of the main movie and replace the original audio track from the remastered movies. I've already tried to rip the movie with handbreak but the mp4 file came out choppy in image and in audio. I also don't know how to replace the original audio track once i have successfully  ripped the audio from the dvd as well as sync it. I've read about using ffmpeg but i have no experience with it except for yt-dlp. If anyone can provide an incredibly dumbed down explanation, i would really appreciate it."},
{"Title": "VHS to HDMI upscaller", "Author": "u/Point-Frosty", "Content": "Hey yall! Looking for recommendations on a box that will convert composite coming from my vcr to hdmi. I would like to upscale to 1080. I am dealing with old vhs tapes that get snowy at times. I did purchase a Kanex Pro $80 unit and it does pretty good upscaling and interlacing. Unfortunately with my snowy tapes, the unit flashes a big source menu on the screen when snow happens. Does anyone know of a 1080 upscaller that wonâ€™t flash menuâ€™s on the screen during my capture to OBS? Thanks so much for the advice!"},
{"Title": "VHS to DVD/Digital", "Author": "u/Djinnimania", "Content": "For context, I dropped the ball recently by forgetting both Motherâ€™s Day and Fatherâ€™s Day until the day of. They donâ€™t seem upset, but I want to make it up to them by converting their wedding videos from VHS to either digital, DVD, or Blu-Ray. What would be the best way to do so?"},
{"Title": "External, internal hard drives", "Author": "u/WriteCodeBroh", "Content": "Letâ€™s say a friend of mine was thinking about, hypothetically of course, buying a used workstation off of Facebook marketplace. And a lot of those modern used workstations donâ€™t have a whole lot of space for hard drives. What if this friend drilled a hole in the side of the case, ran the required power and data cables (SATA probably) out to a custom built case with a backplane and a whole bunch of hard drives? I assume there are EMI and short related risks but frankly Iâ€™ve seen people run whole computers on open wooden racks with fans blowing on the components, function just fine for years, also donâ€™t plan to touch/move things much and theyâ€™ll be off the floor and out of the way. So anyway, all that being said, how stupid of an idea is this?"},
{"Title": "files taking too much space in external hard drive?", "Author": "u/Tikas92", "Content": "Hello everyone. I have a question and I was wondering if anyone knows what's up with that. I recently transfered 3,53 TB worth of files from a 4TB Seagate to an 8TB Lacie but for some reason those files now take up 4,46 TB on the new drive. Why is that happening?"},
{"Title": "Getting a bit lost when it comes to good syncing/cloud storage options", "Author": "u/Creator13", "Content": "After using a laptop for the past six years and carrying it everywhere, I recently got a good desktop PC again. I've always played it super risky because all my data has never been properly backed up or duplicated. Now I want my files to be available between different (windows) computers. Ideally, every change would automatically get synced between both computers, through a cloud storage provider (that also stores the files for access from outside), as long as either computer is connected to the internet. Now, plenty of services offer this, in its most basic form. I get 1TB OneDrive though my parents' family plan and OneDrive is pretty nice. It integrates well enough into Windows. But there is one major limitation that makes it pretty much unsuitable for me: I don't want all my syncable data to be in one single folder. I want it spread out over different folders and even drives, because I run quite a few different drives with different purposes. Is there a service that basically offers a way to set custom endpoints for syncable folders? Say I have a cloud folder /Files , then I want a client that runs on any connected computer where I can set /Files to be synced to G:\\Files , or to C:\\Users\\some_user\\Documents\\CloudFiles on another computer? Ideally also affordable and reputable/secure of course, and for single individual users?"},
{"Title": "I have nearly 30 TB on external hard drives I want to find a permanent solution to keep my data as i continue increase the amount", "Author": "u/romic007", "Content": "Like the title states i have nearly 30 TB on several external hard drives. The files vary from (videos, fILms, series, pictures, music, documents, etc.) i looking for a permanent solution to keep my data as it continues to grow. I am not very tech savy at all I have seen things about nas, Synology stuff but im not too familiar with that stuff. I have heard good things about icloud but im i bit worried since my files were LEGALLY downloaded. Im worried that they could look at my stuff. I use my external hard drives daily and would like this permanent solution to be accessible whenever and wherever i am. I use my external hard drives on. My laptop via usb port and do not have a pc. I am leaning towards icloud or something equivalent to that since i wouldn't have to use external hard drives as much as i currently am now. Im planning on using this as my main backup system with external hard drives as secondary but like i said i use these hard drives daily would icloud be the best solution for my situation?"},
{"Title": "Data Compression", "Author": "u/elgato123", "Content": "Noticed that a drive was filling up, saw it was mostly log files...49Gb of log files. Broke out the ol' 7-zip and in ultra mode, it compressed it down to 1.8Gb. Wow I am wondering if there is any better compression or if this is about as good as it will get? Either way, I'm very impressed"},
{"Title": "New Build raid/filesystem recommendations", "Author": "u/NextRedditAccount0", "Content": "I have a new HL15 coming. I'm planning to use it for some docker for *arr, deluge, and as a NAS. I have a separate Plex server that will be accessing the files on the HL15. No plans of doing any VMs. I'm trying to figure out the best way to get good performance and some parity in case a drive fails. Yes I do have plenty of backups. My current drives in my synology is 8x8TB, 1x 500GB SSD, 1x 1TB SSD in SHR2. I also have 3x 16TB drives coming. My original plan was to run unraid but I don't want to lose out on performance due to the 8x8TB are all 5400rpms. I'm open to running unraid or truenas or proxmox or etc. My end goal is to get as much speed as possible while being able to survive a drive failure. No future drive expansion is planned ATM but would be nice if the new solution could support it. Also this will be on a 10gbps network. Thoughts?"},
{"Title": "Question re: drives in a RAID array I inherited from a relative", "Author": "u/RhetoricalAnswer-001", "Content": "Hope this is appropriate for the forum. I inherited a full height rackmount cabinet with 12 HDs. It looks like it belongs in a data center. I want to use the drives but my PC won't recognize them. Any tips on software that can \"see\" them, then wipe and reformat them?"},
{"Title": "Internet Archive Forced to Remove 500,000 books Due to Copyright Lawsuit", "Author": "u/Run_the_Line", "Content": "No content"},
{"Title": "How long for an SSD to do it's business after a long stint in cold storage?", "Author": "u/Captain_Starkiller", "Content": "I'm firing up a computer after it was offline for nearly two years. It's SSD is just a windows drive, the data storage drives are spinning rust. That said, I want to make sure the error protection has a chance to do its thing for the SSD, refill the charge traps so bit rot doesn't set in, ect. Does anyone have any idea how long I should let it run for, before turning it off again (for a month or two this time, not years) to make sure it does it's business?"},
{"Title": "Advice/Recommendation on a Personal Desktop Build with a focus on Data Storage that can also be used as a small homeserver for myself.", "Author": "u/curiousdoggo", "Content": "Goal: I know people typically separate their data storage/server/nas from their personal desktop so it can be left to do its designated tasks and so that it won't affect the performance of the desktop (if it has to serve a multitude of people or perform a lot of different tasks), but what if my server/nas needs are minimal and my primary focus is just a good personal desktop with a focus on datahoarding/data storage, some file sharing, running a few VMs, and possibly a few more server features? In this case, is it okay to just build a decently powerful modern personal desktop - killing 2 birds with 1 s tone, instead of building 2 computers, one for desktop (desperately need the upgrade now) and one specifically for a server/nas? Is this totally okay? I live by myself so aside from being a personal computer with data storage, it'll just be serving me alone at home. To be honest, at the moment, I pretty much just consume media on my desktop, so even nas/media server features like plex, jellyfins aren't even 100% necessary - though nice to have perhaps in the future. Summary : Decently powerful personal desktop with a focus on datahoarding/data storage that can also perform some server side of things like running VMs, and other homelab things down the road. OS : FreeBSD with ZFS + ECC Memory CASE : thinking of a big case like Fractal Design Define 7 XL that is capable of holding 14-18 HDDs. MOBO : SSD : What's a good NVMe 2.0 ?(maybe 1TB) CPU : amd or intel? what series/models would you guys recommend? MEM : looking for ECC ram as the main focus will be data storage. but how much memory will I need for zfs (assuming I will fill up the whole case with 18 HDDs down the road)? GPU : a gpu capable of driving maybe a LG DualUp 2560 x 2880 with a 34\" 1440p 3440Ã—1440 ultrawide monitor. I will also be doing some photo editing with darktable, rawtherapee, etc. as well so a designated gpu that is good enough should be enough. (don't think there will be much gaming). PSU : how big of a power supply? keep in mind the full capacity is 18 HDD, with dual monitor, etc. CPU COOLER : HDD : thinking of 18 or 20TB seagate exos (are they too loud to use in the bedroom in a personal desktop? should i go for ironwolf pro?)"},
{"Title": "5x3.5 tool-less drive bay prices", "Author": "u/Doodarazumas", "Content": "I'm putting together a nas and I saw several threads where people recommended these kind of 3x5.25 to 5x3.5 hard drive cages https://www.amazon.com/dp/B00DWHLFMA/ref=cm_sw_r_cp_api_glt_fabc_ZJWWH4ZW3JXHXF89RMN3?_encoding=UTF8&th=1 https://www.amazon.com/dp/B00DWHLFMA/ref=cm_sw_r_cp_api_glt_fabc_ZJWWH4ZW3JXHXF89RMN3?_encoding=UTF8&th=1 plus silverstone/rosewill/etc. Now most of these threads were from a few years back and since then the prices on all these things have tripled or quadrupled. Even no-name aliexpress ones I've found were about $150. Is there some secret method to finding or making something like that (5x 3.5 drives tool-less with a backplane in 3x5.25 bays) for a more reasonable $50-75 or am I just going to be a caveman with a screwdriver: https://www.amazon.com/EMVANV-Stainless-5-25inch-Adapter-Bracket/dp/B0C1BQ36ML/ref=pd_ci_mcx_pspc_dp_d_2_i_1"},
{"Title": "Rackmount case recs with >12 bays", "Author": "u/jtscribe52", "Content": "Iâ€™ve currently maxed out my tower and have been trying to figure out a rack mount replacement that wonâ€™t break the bank. A lot of older recs point to SuperMicro cases, but those have really jumped in price the past few years. Iâ€™ve seen some other post recommending Sliger,  but I donâ€™t see anything with more than 10 bays,  even in a 4U. Not opposed to something from AliExpress, but curious what everyone is using of late. Thanks in advance!"},
{"Title": "Any ideas for how to acquire BluRays of foreign versions of films? (Specifically, Russian Cars 2 and Planes, and Chinese Zootopia.)", "Author": "u/CtrlAltSysRq", "Content": "Up front: I don't mind paying for it. I'm not asking for help doing piracy 101 or anything. Hi fellow hoarders. I'm not sure if this is the right place to post this, but I figure this is a good place to ask fellow hoarders about what they hoard. Feel free to lmk if you think there's a better place to post this. I'm trying to archive the different versions of Disney movies that have regional differences. For example, I have the different versions of Inside Out where Riley hates either broccoli (US) or green bell peppers (Japan). Most movies only have a few variations, but Cars 2 and Planes each has about 7 variations each where the design of a car or a plane changes depending on the region. I've gotten nearly all of them, and almost all the variations of all the other movies, except for three: Chinese Zootopia (secondary newscaster is a panda) Russian Cars 2 (Car is styled as the Russian flag) Russian Planes (the \"sexy\" girl plane is styled after the Russian flag) You can probably see why I'm having trouble with these. Disney+ isn't offered in those countries for geopolitical reasons. I've scoured the internet that is available to me looking for ways to source these either physically or digitally, but even my non-public sources are not very interested in this kind of thing. So I'm reduced to looking for physical Blu-ray's of them and that is going about as well as you might expect. So I'm wondering if anyone has ideas of ways I might be able to swing this. Thanks in advance!"},
{"Title": "For photos and videos backup, is it better to use a cloud-based storage service or buy an SSD drive?", "Author": "u/jesuisapprenant", "Content": "I got a subscription for Dropbox at around $12 per month, and I was wondering if it is worth it to just buy a 2TB SSD drive since overall the cost is much cheaper (a nice SSD with 500mb r&w is about $119 so that's 10 months of storage cost for Dropbox, so I break even at 10 months. The pros of Dropbox is that I can access it anywhere, even on my phone, and I don't have to worry about data loss or drive failure or my disk getting stolen or lost. I also won't have to carry that disk around. The pros of an external drive is that it's much cheaper and it pays for itself in 10 months. I can also transfer data in and out much faster. I also don't need internet to access my files. Which solution is better for my use case? TIA"},
{"Title": "Question about using wget to download images from Newgrounds", "Author": "u/Glen_Garrett_Gayhart", "Content": "I've got a lot of urls like this: https://www.newgrounds.com/art/view/alvinhew/annika where one or more images are displayed. I want to use wget to get the images on these pages that have links like this: https://art.ngfiles.com/images/49000/49087_alvinhew_annika.jpg?f1254528733 but I'm not sure how I should configure wget to go from the first sort of url to target the second sort. I could just open all of the www.newgrounds urls and copy the art.ngfiles urls, but that would defeat the purpose of automating it. I want to download a lot of these, and I've got a batch file that will go through them all. How should I instruct wget to look at urls of the first www.newgrounds sort, and then download everything from urls of the second art.ngfiles sort? I don't mind if I get some extra files, like thumbnails and things, but I don't want wget spidering all over the website and potentially downloading things from pages other than the art.ngfiles links. ` Thanks in advance for any help!"},
{"Title": "GM service manual archiving", "Author": "u/Betelgeuse28", "Content": "Has anyone managed to download the newer service manuals from ACDelco TDS site? I bought a 3 day access pass to the site but I've had zero luck so far. Ive tried wget, Offline Explorer, HTTrack, and Webcopy. I'm not really trying to save an 18k page manual by rightclicking and save to pdf."},
{"Title": "Any thoughts on using something lie LBRY protocol for mass decentralised data hoarding?", "Author": "u/MasterDefibrillator", "Content": "Something like it, or the LBRY protocol itself."},
{"Title": "YouTube seems to be blocking accounts that are used with yt-dlp (by passing cookies)", "Author": "u/BowzasaurusRex", "Content": "No content"},
{"Title": "Looking for an external HDD to backup my NAS - what drives do you recommend?", "Author": "u/kavakravata", "Content": "Hey! I have a NAS with 16TB of active storage using RAID. As a newbie, I didnâ€™t even think about the possibility of my NAS drives failing with time, especially after reading horror stories with seagate ironwolf drives which Iâ€™m currently using. I hate SaaS and cloud, and would much prefer a local backup to my Synology NAS. What drives do you recommend? Been looking at the WD My Book 16TB or Easystore 18TB from Amazon, but Iâ€™m unsure. I donâ€™t care about house fires / theft, so ignore that risk of backup solutions. Taking all ideas! Thank you ðŸ¥°"},
{"Title": "LSI 9650se-16ml rebuild questiom", "Author": "u/SirMeili", "Content": "I know this is an old card. I've been using it since I bought it new in 2008. I'm prepping to do a migration to trunas and a set of larger drives and an also 9305 card but until I can get some new drives I'm limping along with this old card. I currently have a RAID6 array of 9 4tb WD red drives. One degraded and it's been trying to rebuild. I'm currently running windows server 2016 essentials. I think something is causing the is to bsod while rebuilding the array. So I went into the cards bios to rebuild there. Does anyone know if the card will rebuild while I'm in the bios screen of the card? I would rather leave it there and let it rebuild. It could also be the drive is hosed and I need to buy a new one. Not sure why the drive went degraded except that we had a storm the other night and my UPSs battery apparently failed (less than 6 months old) and the server hard shutdown. The drives were unlikely to be writing anything as I don't currently write to that array. It's purely read operations. Any help would be greatly appreciated. *Btw I have backed up data to external drives, but I would still rather not lose the data as I could mount the array and make copying to the new array in trunas faster."},
{"Title": "Cold storage backups", "Author": "u/Vatican87", "Content": "What's the best way to backup important media (Photos / Videos), are cold storage options the best way? As in do they degrade much at all if they're only spinning up every few months to backup files?"},
{"Title": "HD Tune Pro Question", "Author": "u/klaaaay", "Content": "Hello, I have installed the HD Tune Pro program to see how my disk was since in the Crystal Disk Info program it shows me 69% Good in green, the disk is approximately 2 and a half or 3 years old and I wanted to confirm the information, doing the Error Scan test I have noticed that there are rectangles that remain blank, should I be worried? https://preview.redd.it/hd-tune-pro-question-v0-zycqm8oomt6d1.png"},
{"Title": "I upsized from a Fractal Design Node 804 into a Meshify 2 XL!", "Author": "u/shockguard", "Content": "No content"},
{"Title": "First machine at 15 YO", "Author": "u/modestt_rat", "Content": "No content"},
{"Title": "Backup Client From NAS", "Author": "u/FancyRectangle", "Content": "I've been using Arq 7 for quite some time at this point, and have had a Windows 2019 VM running exclusively to mount RO shares from my main NAS and backup to a couple endpoint. This has been on a physically separate box (security), but have been considering moving this VM to just be on the NAS itself and backing up from there. Does anyone else backup straight from NAS to a remote endpoint? I have a separate local NAS that pulls snapshots, but looking into some potential consolidation for a VM that needs to remain on consistently."},
{"Title": "Amazing piece of software: Podcast Bulk Downloader", "Author": "u/didyousayboop", "Content": "Podcast Bulk Downloader is a Windows program (written by u/cnovel ) that allows you to batch download episodes of a podcast simply by copy/pasting in a link to the RSS feed. There is an option to automatically append the release dates of the episodes as a prefix to the file names. Download it on GitHub here ."},
{"Title": "Can you rename files based on metadata within the file?", "Author": "u/Elarionus", "Content": "I take photos with three different devices at work for different purposes, a Pixel, a Samsung, and an iPhone. They all name their files differently. This isn't an issue when they're all in Google Photos, but that's never their permanent home. They all add different pieces and parts to the file names. I know I could run them through a renamer to remove or add certain parts, but I was wondering if there's a software out there that lets me just dump thousands of photos into it and have it spit out filenames based on the metadata of when they were taken. YYYY-MM-DD_HH-MM-SS (Year, Month, Day, Hours, Minutes, Seconds). It would help us out a lot as we would be able to filter images in file explorer much more quickly."},
{"Title": "NEW to RAID and need help with a DIY 4 Bay", "Author": "u/FreasFrames", "Content": "I am new to the whole RAID landscape. I have been looking at the Thunderbay 4 to set up as a RAID 5 with 4x16TB HDD. the question I have is what is the most cost effective way of building one? or is it a smart idea to go with the OWC built 4x16TB so its just plug and play? Would like some help with what sites to source drives and enclosures from if OWC is not my end result Currently on a 2017 iMac Retina with 64MB DDR4 using a 3.4 GHz Quad-Core Intel i5. Running Ventura 13.6.6"},
{"Title": "Long term data storage options", "Author": "u/BroccoliSanchez", "Content": "I currently have a wd 14tb desktop harddrive for my digital backups of my home media. I know consumer drives have a decent shelf life but I was wondering if there is a particular brand or type of drive I should use to back up my main drive. I plan on keeping the backup drive in a fire and flood proof safe and only powering it up to do weekly updates of new files. External options are preferred for the simplicity"},
{"Title": "ZFS write errors on 7200 RPM drives, fine with badblocks", "Author": "u/rhnet", "Content": "I have been struggling adding 7200 RPM refurbished SATA drives (WUH721414AL) to my existing ZFS pool. The drives have no issues with badblocks, and I have tested them on another system as well. When I try to add them to an existing zfs mirror, I run into lots of WRITE/CKSUM errors, and they will eventually fault. Here's the output only 10% into a resilver. NAME                       STATE     READ WRITE CKSUM tank                  DEGRADED     0     0     0 mirror-0                 DEGRADED     0     0     0 disk2_crypt            ONLINE       0     0     0 disk16_crypt           ONLINE       0     4    93  (resilvering) And in dmesg I get stuff like: [Sat Jun 15 12:39:59 2024] zio pool=tank vdev=/dev/mapper/disk16_crypt error=5 type=2 offset=7423255224320 size=12288 flags=1808aa [Sat Jun 15 12:39:59 2024] zio pool=tank vdev=/dev/mapper/disk16_crypt error=5 type=2 offset=7423255212032 size=12288 flags=1808aa I have these attached to an HBA and SAS expander, but I've also tried with the SATA ports on the motherboard directly. Setup: LSI SAS9340-8i ServeRAID M1215 12Gbps SAS (from artofserver) Adaptec 2283400-R AEC-82885T LENOVO 36Port 12Gb/s SAS Expander Card 82885T US 10Gtek# 12G Internal Mini SAS HD SFF-8643 to SFF-8643 Cable, with Sideband, 100-Ohm, 0.5-m(1.6ft), 2 Pack 3x AdcAudx 2Pack SFF-8643 to SATA: 1M SFF-8643 Mini-SAS to SATA-Cable SFF8643 to SATA Mini SAS HD to SATA Forward Breakout (3.3FT) Lots of working fine 5200 RPM data drives: WD101EMAZ-11, WDC WD140EDFZ-11, WDC WD140EDGZ-11, WD80EMAZ-00W, ... Several 7200 RPM drives from Serverparts and goharddrive, all WUH721414AL. I run them with luks (cryptsetup luksFormat -c aes-xts-plain64 -s 512 -h sha256) and in zfs mirror."},
{"Title": "Something about sorting media that is really bugging me", "Author": "u/kyne_ahnung", "Content": "This is one of those questions where I don't know if I'm doing it wrong or so many other people are, but I can't work out why it's so hard to find a way to go through a big directory of videos or images in a gallery/viewer manually and easily say this one should go in folder A, that one in B, this one in D, that one in A; without having to - for each file - close the gallery (memorizing the filename), find it in the current directory among thousands of others, also find the destination and open that in another window, copy/move/drag the file to the destination. What I'm looking for: A video and image gallery Some kind of sidebar with shortcuts to favourited/saved paths The ability to easily move the image or video that I am currently viewing to one of these shortcuts either via drag or clicking move>move to>shortcut to folder A Am I crazy/stupid or is it unneccessarily hard to do this task in an ergonomic way? I have all my photos and videos sent and received on my phone over the years to sort through... deleting is no problem, finding duplicates is easy but how do people actually do the organizing? Any methods, software suggestions welcome. Edit: or another plausible method is being able to easily apply a tag to each file from the viewer, so you watch a video and can quickly click on or type a tag.."},
{"Title": "Download site with login?", "Author": "u/Cyan7988", "Content": "Hello, a website is about to shutdown and there are many links that are only accessible after logging in, when downloading website with httrack it only downloads the non logged in version of thr site which I can't see the download links, how to I make it download the logged in version of thr site? I have the password and username Using HTTRACK"},
{"Title": "A simple way to save a lot of data?", "Author": "u/Jatm4", "Content": "Hi there, I've been having a serious storage problem for a few weeks now. I used to store the media generated by my projects on individual external HDD, but this is no longer an option for me. I found a storage unit on Amazon (WD Elements 18TB) that was exactly what I needed. The problem? I bought it and it came DOA, and after seeing some comments about that device, it seems they are very unreliable, so it's no longer an option. Now I don't know how to solve my problem. I just want a drive to store my backups and where I can access them when needed, not something that needs to be constantly connected, some simple, a NAS seems a bit excessive and I wouldn't want to use a cloud. What solution do you recommend me? Did I just have bad luck with the WD Elements? I live outside the USA, so I would need something that can be purchased and shipped internationally"},
{"Title": "Can I put a 12-to-16 pin adapter inside a USB enclosure?", "Author": "u/biocoder86", "Content": "I have an old SSD drive I want to recover data from and maybe use as a portable drive going forward... Specifically I want to use this old 12 + 16 pin from an old MacBook Air (A1465), with a 12 + 16 to NVMe M key adapter. https://amzn.eu/d/1gMJdGQ Then I want to put those inside a NVMe M key to USB enclosure. https://amzn.eu/d/9ifm71g But the fourth image you scroll to of the adapter says that it cannot work in a USB enclosure, only plugged directly into the motherboard. Then in one of the user questions they seem to say the opposite with no explanation. That so long as it is 12 + 16 on one end and M key on the other it will work. So... will it work? If not, is this universally true or can I get a different adapter?"},
{"Title": "SATA vs SAS", "Author": "u/uberkalden2", "Content": "Genuinely confused here.  Are any of the enterprise drives SATA, or are they all SAS? They all say SATA in the description, but the details will typically also say SAS.  I was going to get a Sabrent external enclosure, but I don't think they work with SAS drives. Maybe my understanding of SATA is wrong?"},
{"Title": "Megaraid 9271-8i Dying - How do I replace it without data loss?", "Author": "u/Barja_Bardagi", "Content": "(Cross-posting a couple places to hopefully find help)  I've got a DIY NAS that I built a number of years ago based on a Megaraid 9271-8icc. There's 8x HDD RAID-50 array on it, and a 6x SSD RAID-6 array. Well, last Sunday, the card started dying. The machine's boot drives are NOT on the raid controller, and every few reboots, the card works for a while and then craps out. So, I bought a new (used) card from eBay. Now, storage is NOT my forte. I know enough to be dangerous to myself. How do I go about replacing the failing card without losing all my data? Will the new card see data on the drives and realize \"Oh, this is part of an array\"? Do I need to try to recover some settings from the old card? Could someone explain this to me like I'm an end user? :P"},
{"Title": "Instaloader hits login wall with VPN", "Author": "u/patagonianlamb", "Content": "A few weeks back, I managed to scrape a good amount of data from Instagram using a VPN and Instaloader without even logging in. It was great! But now, whenever I try to access Instagram through a VPN, it just takes me straight to the login page. Instaloader can't bypass the login wall either. Is this the end of accessing Instagram media without logging in via a VPN?"},
{"Title": "Refurbished NAS Hard Drivesâ€¦ Ye or Ne?", "Author": "u/maximumkush", "Content": "Starting my first NAS to fuel my addiction. Iâ€™m starting off with RAID 5 with 3 12tb hard drives. I found some decent priced refurbished ones. Asking the pros for their honest opinion"},
{"Title": "Would you not use a certified refurbished 20tb drive for parity?", "Author": "u/NotAnADC", "Content": "Asking specifically if I should only use a drive like that for expansion and not parity, cause maybe parity sees more action. Running in unraid. I know a lot of people like and trust serverpartdeals"},
{"Title": "How often should I update/refresh my cold storage?", "Author": "u/largePenisLover", "Content": "For cold/\"off-site\" storage of backups I use few HDD's. Backups go on them and then I store them nice and safe in a brick shed as an off-site kinda thing. They contain full backups. Disk is wiped and then a fresh backup goes on. I do this a few times a year. Not a real schedule to it. I fear bitrot. How often should I refresh them to manage the chance of bitrot happening? Would it even help? I mean yes I wrote new data but the metal still aged."},
{"Title": "almost entirely geoblocked youtube videos?", "Author": "u/koalahugthekoala", "Content": "hi! so i have some important videos that are related to my special interest but have been geoblocked outside kosovo, somaliland, and northern cyprus. i know they still exist so what tool would i use to get them? i have no idea how github or anything works so that kinda rules out youtubedl unless someone did it for me. they are not on the wayback machine. here's the links if this helps you! https://youtube.com/watch?v=b7TYi0FwR1M https://youtube.com/watch?v=xmKYxwXox5g https://youtube.com/watch?v=AJr9VOCe1yY https://youtube.com/watch?v=FMLjygFXB7w https://youtube.com/watch?v=ZhLJyHxRuu8 https://youtube.com/watch?v=mHU6dS07cZQ https://youtube.com/watch?v=Z6S5klRoI_c https://youtube.com/watch?v=AVeE9NMfAxE thank u for any help!"},
{"Title": "How do i rip a copy protected DVD?", "Author": "u/Icy-Composer9021", "Content": "I have some DVDs i wanna rip onto my computer and put on my ipod, specifically some simpsons and avp if it matters. Also region code is 2 or PAL on the simpsons."},
{"Title": "Personal NAS OS: Debian/Ubuntu vs TrueNAS or Rockstor", "Author": "u/PuzzleHeadPistion", "Content": "I have a personal NAS built on a i5-4690 with 16Gb RAM, which I use for archived work files (music, photos and videos), backups from my computers and as Plex Server. Services like ownCloud or something that would allow me to share files via url (like a personal WeTransfer) would be extremely useful. Maybe a system with ZFS would be good. I'm trying to decide whether to build from scratch on a full OS (Debian, Ubuntu or even Windows 2022?) vs something like TrueNAS or Rockstor. I haven't used Linux in almost 10y, but I used to use Arch. Not sure if it's relevant, but I also own an Asustor Drivestor 2 Lite and a ZenWifi XT9 router with a drive attached, use WireGuard VPN, etc."},
{"Title": "Samsung 8tb qvo ssd reliability", "Author": "u/ChillCaptain", "Content": "I bought the 8tb Samsung qvo ssd for a gaming/Roms drive. I know performance is meh and tbw is not great. But I donâ€™t expect to write a lot to the drive. Copy my Roms and games once and switch out the games every so often. Iâ€™m mainly concerned with reliability. Id imagine it would be better than a spinning hard drive. Any opinions on reliability?"},
{"Title": "MEGATHREAD: Archiving the Capitol Hill Riots", "Author": "u/AdamLynch", "Content": "FINAL UPDATE as of January 31st 5:35PM EST: Thank you to everyone who shared content. The content being submitted now from what I'm seeing is duplicates of older content. I will thus no longer be updating this archive. The MEGA will remain untouched, so use that as you please, but that will likely die one day as there is a bandwidth/transfer limit. I will be uploading the content to Internet Archive, as well as other sources, but until then the torrent magnet that I will be seeding for a little while is listed below - my bandwidth isn't the best so please do seed if you can: Magnet: magnet:?xt=urn:btih:c8fc9979cc35f7062cd8715aaaff4da475d2fadc&dn=Trump%20protest%20Jan%2006%202021&tr=udp%3a%2f%2ftracker.torrent.eu.org%3a451%2fannounce&tr=udp%3a%2f%2fpublic.popcorn-tracker.org%3a6969%2fannounce&tr=http%3a%2f%2f104.28.1.30%3a8080%2fannounce&tr=http%3a%2f%2f104.28.16.69%2fannounce&tr=http%3a%2f%2f107.150.14.110%3a6969%2fannounce&tr=http%3a%2f%2f109.121.134.121%3a1337%2fannounce&tr=http%3a%2f%2f114.55.113.60%3a6969%2fannounce&tr=http%3a%2f%2f125.227.35.196%3a6969%2fannounce&tr=http%3a%2f%2f128.199.70.66%3a5944%2fannounce&tr=http%3a%2f%2f157.7.202.64%3a8080%2fannounce&tr=http%3a%2f%2f158.69.146.212%3a7777%2fannounce&tr=http%3a%2f%2f173.254.204.71%3a1096%2fannounce&tr=http%3a%2f%2f178.175.143.27%2fannounce&tr=http%3a%2f%2f178.33.73.26%3a2710%2fannounce&tr=http%3a%2f%2f182.176.139.129%3a6969%2fannounce&tr=http%3a%2f%2f185.5.97.139%3a8089%2fannounce&tr=http%3a%2f%2f188.165.253.109%3a1337%2fannounce&tr=http%3a%2f%2f194.106.216.222%2fannounce&tr=http%3a%2f%2f195.123.209.37%3a1337%2fannounce&tr=http%3a%2f%2f210.244.71.25%3a6969%2fannounce&tr=http%3a%2f%2f210.244.71.26%3a6969%2fannounce&tr=http%3a%2f%2f213.159.215.198%3a6970%2fannounce&tr=http%3a%2f%2f213.163.67.56%3a1337%2fannounce&tr=http%3a%2f%2f37.19.5.139%3a6969%2fannounce&tr=http%3a%2f%2f37.19.5.155%3a6881%2fannounce&tr=http%3a%2f%2f46.4.109.148%3a6969%2fannounce&tr=http%3a%2f%2f5.79.249.77%3a6969%2fannounce&tr=http%3a%2f%2f5.79.83.193%3a2710%2fannounce&tr=http%3a%2f%2f51.254.244.161%3a6969%2fannounce&tr=http%3a%2f%2f59.36.96.77%3a6969%2fannounce&tr=http%3a%2f%2f74.82.52.209%3a6969%2fannounce&tr=http%3a%2f%2f80.246.243.18%3a6969%2fannounce&tr=http%3a%2f%2f81.200.2.231%2fannounce&tr=http%3a%2f%2f85.17.19.180%2fannounce&tr=http%3a%2f%2f87.248.186.252%3a8080%2fannounce&tr=http%3a%2f%2f87.253.152.137%2fannounce&tr=http%3a%2f%2f91.216.110.47%2fannounce&tr=http%3a%2f%2f91.217.91.21%3a3218%2fannounce&tr=http%3a%2f%2f91.218.230.81%3a6969%2fannounce&tr=http%3a%2f%2f93.92.64.5%2fannounce&tr=http%3a%2f%2fatrack.pow7.com%2fannounce&tr=http%3a%2f%2fbt.henbt.com%3a2710%2fannounce&tr=http%3a%2f%2fbt.pusacg.org%3a8080%2fannounce&tr=http%3a%2f%2fbt2.careland.com.cn%3a6969%2fannounce&tr=http%3a%2f%2fexplodie.org%3a6969%2fannounce&tr=http%3a%2f%2fmgtracker.org%3a2710%2fannounce&tr=http%3a%2f%2fmgtracker.org%3a6969%2fannounce&tr=http%3a%2f%2fopen.acgtracker.com%3a1096%2fannounce&tr=http%3a%2f%2fopen.lolicon.eu%3a7777%2fannounce&tr=http%3a%2f%2fopen.touki.ru%2fannounce.php&tr=http%3a%2f%2fp4p.arenabg.ch%3a1337%2fannounce&tr=http%3a%2f%2fp4p.arenabg.com%3a1337%2fannounce&tr=http%3a%2f%2fpow7.com%3a80%2fannounce&tr=http%3a%2f%2fretracker.gorcomnet.ru%2fannounce&tr=http%3a%2f%2fretracker.krs-ix.ru%2fannounce&tr=http%3a%2f%2fretracker.krs-ix.ru%3a80%2fannounce&tr=http%3a%2f%2fsecure.pow7.com%2fannounce&tr=http%3a%2f%2ft1.pow7.com%2fannounce&tr=http%3a%2f%2ft2.pow7.com%2fannounce&tr=http%3a%2f%2fthetracker.org%3a80%2fannounce&tr=http%3a%2f%2ftorrent.gresille.org%2fannounce&tr=http%3a%2f%2ftorrentsmd.com%3a8080%2fannounce&tr=http%3a%2f%2ftracker.aletorrenty.pl%3a2710%2fannounce&tr=http%3a%2f%2ftracker.baravik.org%3a6970%2fannounce&tr=http%3a%2f%2ftracker.bittor.pw%3a1337%2fannounce&tr=http%3a%2f%2ftracker.bittorrent.am%2fannounce&tr=http%3a%2f%2ftracker.calculate.ru%3a6969%2fannounce&tr=http%3a%2f%2ftracker.dler.org%3a6969%2fannounce&tr=http%3a%2f%2ftracker.dutchtracking.com%2fannounce&tr=http%3a%2f%2ftracker.dutchtracking.com%3a80%2fannounce&tr=http%3a%2f%2ftracker.dutchtracking.nl%2fannounce&tr=http%3a%2f%2ftracker.dutchtracking.nl%3a80%2fannounce&tr=http%3a%2f%2ftracker.edoardocolombo.eu%3a6969%2fannounce&tr=http%3a%2f%2ftracker.ex.ua%2fannounce&tr=http%3a%2f%2ftracker.ex.ua%3a80%2fannounce&tr=http%3a%2f%2ftracker.filetracker.pl%3a8089%2fannounce&tr=http%3a%2f%2ftracker.flashtorrents.org%3a6969%2fannounce&tr=http%3a%2f%2ftracker.grepler.com%3a6969%2fannounce&tr=http%3a%2f%2ftracker.internetwarriors.net%3a1337%2fannounce&tr=http%3a%2f%2ftracker.kicks-ass.net%2fannounce&tr=http%3a%2f%2ftracker.kicks-ass.net%3a80%2fannounce&tr=http%3a%2f%2ftracker.kuroy.me%3a5944%2fannounce&tr=http%3a%2f%2ftracker.mg64.net%3a6881%2fannounce&tr=http%3a%2f%2ftracker.opentrackr.org%3a1337%2fannounce&tr=http%3a%2f%2ftracker.skyts.net%3a6969%2fannounce&tr=http%3a%2f%2ftracker.tfile.me%2fannounce&tr=http%3a%2f%2ftracker.tiny-vps.com%3a6969%2fannounce&tr=http%3a%2f%2ftracker.tvunderground.org.ru%3a3218%2fannounce&tr=http%3a%2f%2ftracker.yoshi210.com%3a6969%2fannounce&tr=http%3a%2f%2ftracker1.wasabii.com.tw%3a6969%2fannounce&tr=http%3a%2f%2ftracker2.itzmx.com%3a6961%2fannounce&tr=http%3a%2f%2ftracker2.wasabii.com.tw%3a6969%2fannounce&tr=http%3a%2f%2fwww.wareztorrent.com%2fannounce&tr=http%3a%2f%2fwww.wareztorrent.com%3a80%2fannounce&tr=https%3a%2f%2f104.28.17.69%2fannounce&tr=https%3a%2f%2fwww.wareztorrent.com%2fannounce&tr=udp%3a%2f%2f107.150.14.110%3a6969%2fannounce&tr=udp%3a%2f%2f109.121.134.121%3a1337%2fannounce&tr=udp%3a%2f%2f114.55.113.60%3a6969%2fannounce&tr=udp%3a%2f%2f128.199.70.66%3a5944%2fannounce&tr=udp%3a%2f%2f151.80.120.114%3a2710%2fannounce&tr=udp%3a%2f%2f168.235.67.63%3a6969%2fannounce&tr=udp%3a%2f%2f178.33.73.26%3a2710%2fannounce&tr=udp%3a%2f%2f182.176.139.129%3a6969%2fannounce&tr=udp%3a%2f%2f185.5.97.139%3a8089%2fannounce&tr=udp%3a%2f%2f185.86.149.205%3a1337%2fannounce&tr=udp%3a%2f%2f188.165.253.109%3a1337%2fannounce&tr=udp%3a%2f%2f191.101.229.236%3a1337%2fannounce&tr=udp%3a%2f%2f194.106.216.222%3a80%2fannounce&tr=udp%3a%2f%2f195.123.209.37%3a1337%2fannounce&tr=udp%3a%2f%2f195.123.209.40%3a80%2fannounce&tr=udp%3a%2f%2f208.67.16.113%3a8000%2fannounce&tr=udp%3a%2f%2f213.163.67.56%3a1337%2fannounce&tr=udp%3a%2f%2f37.19.5.155%3a2710%2fannounce&tr=udp%3a%2f%2f46.4.109.148%3a6969%2fannounce&tr=udp%3a%2f%2f5.79.249.77%3a6969%2fannounce&tr=udp%3a%2f%2f5.79.83.193%3a6969%2fannounce&tr=udp%3a%2f%2f51.254.244.161%3a6969%2fannounce&tr=udp%3a%2f%2f62.138.0.158%3a6969%2fannounce&tr=udp%3a%2f%2f62.212.85.66%3a2710%2fannounce&tr=udp%3a%2f%2f74.82.52.209%3a6969%2fannounce&tr=udp%3a%2f%2f85.17.19.180%3a80%2fannounce&tr=udp%3a%2f%2f89.234.156.205%3a80%2fannounce&tr=udp%3a%2f%2f9.rarbg.com%3a2710%2fannounce&tr=udp%3a%2f%2f9.rarbg.me%3a2780%2fannounce&tr=udp%3a%2f%2f9.rarbg.to%3a2730%2fannounce&tr=udp%3a%2f%2f91.218.230.81%3a6969%2fannounce&tr=udp%3a%2f%2f94.23.183.33%3a6969%2fannounce&tr=udp%3a%2f%2fbt.xxx-tracker.com%3a2710%2fannounce&tr=udp%3a%2f%2feddie4.nl%3a6969%2fannounce&tr=udp%3a%2f%2fexplodie.org%3a6969%2fannounce&tr=udp%3a%2f%2fmgtracker.org%3a2710%2fannounce&tr=udp%3a%2f%2fp4p.arenabg.com%3a1337%2fannounce&tr=udp%3a%2f%2fshadowshq.eddie4.nl%3a6969%2fannounce&tr=udp%3a%2f%2fshadowshq.yi.org%3a6969%2fannounce&tr=udp%3a%2f%2ftorrent.gresille.org%3a80%2fannounce&tr=udp%3a%2f%2ftracker.aletorrenty.pl%3a2710%2fannounce&tr=udp%3a%2f%2ftracker.bittor.pw%3a1337%2fannounce&tr=udp%3a%2f%2ftracker.coppersurfer.tk%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.eddie4.nl%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.ex.ua%3a80%2fannounce&tr=udp%3a%2f%2ftracker.filetracker.pl%3a8089%2fannounce&tr=udp%3a%2f%2ftracker.flashtorrents.org%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.grepler.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.ilibr.org%3a80%2fannounce&tr=udp%3a%2f%2ftracker.internetwarriors.net%3a1337%2fannounce&tr=udp%3a%2f%2ftracker.kicks-ass.net%3a80%2fannounce&tr=udp%3a%2f%2ftracker.kuroy.me%3a5944%2fannounce&tr=udp%3a%2f%2ftracker.leechers-paradise.org%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.mg64.net%3a2710%2fannounce&tr=udp%3a%2f%2ftracker.mg64.net%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.opentrackr.org%3a1337%2fannounce&tr=udp%3a%2f%2ftracker.piratepublic.com%3a1337%2fannounce&tr=udp%3a%2f%2ftracker.sktorrent.net%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.skyts.net%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.yoshi210.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker2.indowebster.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker4.piratux.com%3a6969%2fannounce&tr=udp%3a%2f%2fzer0day.ch%3a1337%2fannounce&tr=udp%3a%2f%2fzer0day.to%3a1337%2fannounce&tr=udp%3a%2f%2ftracker.cyberia.is%3a6969%2fannounce&tr=http%3a%2f%2fvps02.net.orel.ru%3a80%2fannounce&tr=https%3a%2f%2ftracker.nanoha.org%3a443%2fannounce&tr=http%3a%2f%2ftracker.files.fm%3a6969%2fannounce&tr=https%3a%2f%2ftracker.nitrix.me%3a443%2fannounce&tr=https%3a%2f%2ftracker.tamersunion.org%3a443%2fannounce&tr=udp%3a%2f%2faaa.army%3a8866%2fannounce&tr=https%3a%2f%2ftracker.imgoingto.icu%3a443%2fannounce&tr=udp%3a%2f%2fblokas.io%3a6969%2fannounce&tr=udp%3a%2f%2fdiscord.heihachi.pw%3a6969%2fannounce&tr=udp%3a%2f%2ffe.dealclub.de%3a6969%2fannounce&tr=udp%3a%2f%2fln.mtahost.co%3a6969%2fannounce&tr=udp%3a%2f%2fvibe.community%3a6969%2fannounce&tr=udp%3a%2f%2ftracker0.ufibox.com%3a6969%2fannounce&tr=udp%3a%2f%2fmail.realliferpg.de%3a6969%2fannounce&tr=udp%3a%2f%2fmovies.zsw.ca%3a6969%2fannounce&tr=udp%3a%2f%2fnagios.tks.sumy.ua%3a80%2fannounce&tr=udp%3a%2f%2f47.ip-51-68-199.eu%3a6969%2fannounce&tr=udp%3a%2f%2fcdn-1.gamecoast.org%3a6969%2fannounce&tr=udp%3a%2f%2faruacfilmes.com.br%3a6969%2fannounce&tr=udp%3a%2f%2fedu.uifr.ru%3a6969%2fannounce&tr=http%3a%2f%2frt.tace.ru%3a80%2fannounce&tr=udp%3a%2f%2fcode2chicken.nl%3a6969%2fannounce&tr=udp%3a%2f%2fus-tracker.publictracker.xyz%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.0x.tf%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.altrosky.nl%3a6969%2fannounce&tr=udp%3a%2f%2ftorrentclub.online%3a54123%2fannounce&tr=http%3a%2f%2f5rt.tace.ru%3a60889%2fannounce&tr=udp%3a%2f%2fapp.icon256.com%3a8000%2fannounce&tr=udp%3a%2f%2ftracker.sigterm.xyz%3a6969%2fannounce&tr=http%3a%2f%2ftracker.loadbt.com%3a6969%2fannounce&tr=http%3a%2f%2fipv4announce.sktorrent.eu%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.openbittorrent.com%3a80%2fannounce&tr=udp%3a%2f%2fwww.torrent.eu.org%3a451%2fannounce&tr=udp%3a%2f%2ftracker.dler.org%3a6969%2fannounce&tr=udp%3a%2f%2fexodus.desync.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker2.dler.org%3a80%2fannounce&tr=udp%3a%2f%2ftracker.shkinev.me%3a6969%2fannounce&tr=udp%3a%2f%2fstorage.groupees.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.v6speed.org%3a6969%2fannounce&tr=udp%3a%2f%2fdaveking.com%3a6969%2fannounce&tr=https%3a%2f%2ftracker.lilithraws.cf%3a443%2fannounce&tr=udp%3a%2f%2ftracker1.bt.moack.co.kr%3a80%2fannounce&tr=udp%3a%2f%2f3rt.tace.ru%3a60889%2fannounce&tr=udp%3a%2f%2fjohnrosen1.com%3a6969%2fannounce&tr=udp%3a%2f%2fretracker.lanta-net.ru%3a2710%2fannounce&tr=udp%3a%2f%2fopentor.org%3a2710%2fannounce&tr=udp%3a%2f%2ft2.leech.ie%3a1337%2fannounce&tr=https%3a%2f%2ftracker.foreverpirates.co%3a443%2fannounce&tr=http%3a%2f%2ftracker.vraphim.com%3a6969%2fannounce&tr=udp%3a%2f%2fopen.stealth.si%3a80%2fannounce&tr=udp%3a%2f%2ftracker.uw0.xyz%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.army%3a6969%2fannounce&tr=udp%3a%2f%2fmts.tvbit.co%3a6969%2fannounce&tr=https%3a%2f%2ftracker.coalition.space%3a443%2fannounce&tr=http%3a%2f%2ftracker-cdn.moeking.me%3a2095%2fannounce&tr=udp%3a%2f%2fline-net.ru%3a6969%2fannounce&tr=udp%3a%2f%2fperu.subventas.com%3a53%2fannounce&tr=udp%3a%2f%2fbt1.archive.org%3a6969%2fannounce&tr=udp%3a%2f%2fengplus.ru%3a6969%2fannounce&tr=udp%3a%2f%2fvalakas.rollo.dnsabr.com%3a2710%2fannounce&tr=udp%3a%2f%2fbt2.archive.org%3a6969%2fannounce&tr=udp%3a%2f%2fipv4.tracker.harry.lu%3a80%2fannounce&tr=udp%3a%2f%2ft1.leech.ie%3a1337%2fannounce&tr=http%3a%2f%2fbt.okmp3.ru%3a2710%2fannounce&tr=http%3a%2f%2fcloud.nyap2p.com%3a8080%2fannounce&tr=http%3a%2f%2ft.overflow.biz%3a6969%2fannounce&tr=udp%3a%2f%2ft3.leech.ie%3a1337%2fannounce&tr=udp%3a%2f%2ftracker.tiny-vps.com%3a6969%2fannounce&tr=http%3a%2f%2ftracker.bt4g.com%3a2095%2fannounce&tr=http%3a%2f%2ft.nyaatracker.com%3a80%2fannounce&tr=udp%3a%2f%2fudp-tracker.shittyurl.org%3a6969%2fannounce&tr=https%3a%2f%2f1337.abcvg.info%3a443%2fannounce&tr=https%3a%2f%2fw.wwwww.wtf%3a443%2fannounce&tr=udp%3a%2f%2fbt2.3kb.xyz%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.ds.is%3a6969%2fannounce&tr=udp%3a%2f%2fopentracker.i2p.rocks%3a6969%2fannounce&tr=udp%3a%2f%2fcdn-2.gamecoast.org%3a6969%2fannounce&tr=udp%3a%2f%2fretracker.netbynet.ru%3a2710%2fannounce&tr=udp%3a%2f%2fteamspeak.value-wolf.org%3a6969%2fannounce&tr=udp%3a%2f%2fcutiegirl.ru%3a6969%2fannounce&tr=http%3a%2f%2fh4.trakx.nibba.trade%3a80%2fannounce Hash (for verification of authenticity etc): c8fc9979cc35f7062cd8715aaaff4da475d2fadc Size: 1,013.12GiB Original post below Archiving videos before potential removal from various websites... Send or comment links of videos you need downloaded. Currently going through POV livestreams/replays. NOTE: livestreams/POV are of the utmost importance. AKA Twitch/dlive/Facebook Live, etc. If you find any of these POV angles, please tag me directly in your comment, or PM me. These generally get taken down VERY fast by the livestream website. UPDATE: Thank you to everyone who shared links (and continue to do so). I am noticing that a lot of the content is now duplicates, or variations (crops, lower quality, etc) of the same content. So I have put all the content I have on MEGA. If I have replied to your comment then the content for sure is in this torrent. MEGA: https://mega.nz/folder/30MlkQib#RDOaGzmtFEHkxSYBaJSzVA (this is the prefered way of downloading) (This is sitting in at ~350GB as of Jan 10 3:00 PM EST. Still adding content) UPDATE 2: Link should be working - MEGA contacted me and reinstated the account (and gave premium so I could upload more). I will be uploading more content that I find to the mega account. Still going through the comments, and the 900+ messages I have. Keep posting comments and I will upload them to the MEGA folder. UPDATE 3: \"Bellingcat\" has created a really efficient way to submit media via a Google Spreadsheet. It's not connected to my archive, but I hope to have a merged final copy in the end. UPDATE 4: IF YOU WANT TO UPLOAD A FILE DIRECTLY TO ME: https://mega.nz/megadrop/fgve0WRa880 (no account registration needed) BACKUPS: Recommended backup: u/tweedge : tweedge commits to making sure this mirror does not fall behind 12h behind, though he'll do his best to keep it within 6h Other backups based on the original MEGA from Jan 06 6:30PM - some might've updated, but no idea if/when; check each link, it should say. Or you can message the user: MAGNET from u/SneakyPieBrown as of Jan 08 2021 : magnet:?xt=urn:btih:fc33c9146c81660ee087dbda756746a978c7c104&dn=Trump%20protest%20Jan-08-2021&tr=udp%3a%2f%2ftracker.openbittorrent.com%3a80 u/firstgrow : hosting direct downloads as well u/nuzzles_u_uwu : Is hosting as well u/tweedge : made a direct download link on s3 u/kenkoda : posted a torrent. u/Deifer : Link u/benediktkr : https://mirrors.deadops.de/capitol2021 and https://mirrors.deadops.de/capitol2021.zip See a familiar face in the archive? https://tips.fbi.gov/digitalmedia/aad18481a3e8f02"},
{"Title": "Justin Roiland, co-creator of Rick and Morty, discovers that Dropbox uses content scanners through the deletion of all his data stored on their servers", "Author": "u/_G0D_M0DE_", "Content": "No content"},
{"Title": "yall might appreciate this", "Author": "u/ian9921", "Content": "No content"},
{"Title": "I just built a collapse-ready laptop. What are some must haves to put on it?", "Author": "u/evanMeaney", "Content": "No content"},
{"Title": "Rescue Mission for Sci-Hub and Open Science: We are the library.", "Author": "u/shrine", "Content": "EFF hears the call: \"Itâ€™s Time to Fight for Open Access\" EFF reports: Activists Mobilize to Fight Censorship and Save Open Science \"Continuing the long tradition of internet hacktivism ... redditors are mobilizing to create an uncensorable back-up of Sci-Hub\" The EFF stands with Sci-Hub in the fight for Open Science, a fight for the human right to benefit and share in human scientific advancement. My wholehearted thanks for every seeder who takes part in this rescue mission, and every person who raises their voice in support of Sci-Hub's vision for Open Science. Rescue Mission Links Quick start to rescuing Sci-Hub : Download 1 random torrent (100GB) from the scimag index of torrents with fewer than 12 seeders , open the .torrent file using a BitTorrent client, then leave your client open to upload (seed) the articles to others. You're now part of an un-censorable library archive! Initial success update: The entire Sci-Hub collection has at least 3 seeders: Let's get it to 5. Let's get it to 7! Letâ€™s get it to 10! Letâ€™s get it to 12! Contribute to open source Sci-Hub projects: freereadorg/awesome-libgen Join r/scihub to stay up to date Note: We have no affiliation with Sci-Hub This effort is completely unaffiliated from Sci-Hub, no one is in touch with Sci-Hub, and I don't speak for Sci-Hub in any form. Always refer to sci-hub.do for the latest from Sci-Hub directly. This is a data preservation effort for just the articles, and does not help Sci-Hub directly.  Sci-Hub is not in any further imminent danger than it always has been, and is not at greater risk of being shut-down than before. A Rescue Mission for Sci-Hub and Open Science Elsevier and the USDOJ have declared war against Sci-Hub and open science. The era of Sci-Hub and Alexandra standing alone in this fight must end. We have to take a stand with her. On May 7th, Sci-Hub's Alexandra Elbakyan revealed that the FBI has been wiretapping her accounts for over 2 years. This news comes after Twitter silenced the official Sci_Hub twitter account because Indian academics were organizing on it against Elsevier. Sci-Hub itself is currently frozen and has not downloaded any new articles since December 2020. This rescue mission is focused on seeding the article collection in order to prepare for a potential Sci-Hub shutdown. Alexandra Elbakyan of Sci-Hub, bookwarrior of Library Genesis , Aaron Swartz , and countless unnamed others have fought to free science from the grips of for-profit publishers. Today, they do it working in hiding, alone, without acknowledgment, in fear of imprisonment, and even now wiretapped by the FBI. They sacrifice everything for one vision: Open Science. Why do they do it? They do it so that humble scholars on the other side of the planet can practice medicine, create science, fight for democracy, teach, and learn. People like Alexandra Elbakyan would give up their personal freedom for that one goal: to free knowledge. For that, Elsevier Corp (RELX, market cap: 50 billion) wants to silence her, wants to see her in prison, and wants to shut Sci-Hub down. It's time we sent Elsevier and the USDOJ a clearer message about the fate of Sci-Hub and open science: we are the library, we do not get silenced, we do not shut down our computers, and we are many. Rescue Mission for Sci-Hub If you have been following the story, then you know that this is not our first rescue mission. We protected the Library Genesis book collection We unlocked over 5,000 COVID-19 research articles We successfully petitioned publishers to unlock their COVID-19 paywalls bookwarrior, the founder of Library Genesis, took his library onto the de-centralized and un-censorable IPFS web Next? Make Sci-Hub un-censorable too. Rescue Target A handful of Library Genesis seeders are currently seeding the Sci-Hub torrents. There are 850 scihub torrents , each containing 100,000 scientific articles, to a total of 85 million scientific articles: 77TB. This is the complete Sci-Hub database. We need to protect this. Rescue Team Wave 1: We need 85 datahoarders to store and seed 1TB of articles each, 10 torrents in total. Download 10 random torrents from the scimag index of < 12 seeders , then load the torrents onto your client and seed for as long as you can. The articles are coded by DOI and in zip files. Wave 2: Reach out to 10 good friends to ask them to grab just 1 random torrent (100GB). That's 850 seeders. We are now the library. Final Wave: Development for an open source Sci-Hub. freereadorg/awesome-libgen is a collection of open source achievements based on the Sci-Hub and Library Genesis databases. Open source de-centralization of Sci-Hub is the ultimate goal here, and this begins with the data, but it is going to take years of developer sweat to carry these libraries into the future. Heartfelt thanks to the r/datahoarder and r/seedboxes communities, seedbox.io and NFOrce for your support for previous missions and your love for science."},
{"Title": "Data transfer to new Lustre storage overwhelms campus network", "Author": "u/e_spider", "Content": "No content"},
{"Title": "Whoops", "Author": "u/MidnightLink", "Content": "No content"},
{"Title": "A funny exchange", "Author": "Unknown author", "Content": "No content"},
{"Title": "One woman's quest to \"never delete anything\" allowed internet archivists to find long-lost Minecraft Alpha 1.1.1.", "Author": "u/BrikenEnglz", "Content": "No content"},
{"Title": "Amazon delivery driver with my new HD", "Author": "u/fancy_pantser", "Content": "No content"},
{"Title": "Apple sued for terminating account with $25,000 worth of apps and videos", "Author": "u/usernamechosen999", "Content": "No content"},
{"Title": "Some of the drives used to store the data to to compile the first picture ever of a black hole.", "Author": "u/the_best_moshe", "Content": "No content"},
{"Title": "Archivists Are Preserving Capitol Hill Riot Livestreams Before Theyâ€™re Deleted", "Author": "u/fairyrocker91", "Content": "No content"},
{"Title": "Twitter to purge accounts that have had no activity at all for several years", "Author": "u/NXGZ", "Content": "No content"},
{"Title": "I can dream", "Author": "u/gammajayy", "Content": "No content"},
{"Title": "This is your regular reminder that Comcast is still a dumpster fire: Comcast to impose home internet data cap of 1.2TB in more than a dozen US states next year", "Author": "u/Snoot_Boopins", "Content": "No content"},
{"Title": "\"If you visit CERN in Geneva, you can buy 1 Terabyte of Large Hadron Collider data from the souvenir shop\"... there goes my kid's college funds", "Author": "Unknown author", "Content": "No content"},
{"Title": "yahoo answers is shutting down", "Author": "u/Fazlul101", "Content": "No content"},
{"Title": "internet archive is being sued", "Author": "u/sersoniko", "Content": "No content"},
{"Title": "Forklift accident", "Author": "u/Enkelie", "Content": "No content"},
{"Title": "Michigan couple must pay son $30,441 for throwing out porn collection", "Author": "u/benjacob", "Content": "No content"},
{"Title": "TIL the Library of Congress has a 2.129 petabyte (and growing) archive of internet culture", "Author": "u/WhitefangdDS", "Content": "No content"},
{"Title": "100Mbps uploads and downloads should be US broadband standard, senators say", "Author": "u/Unlanded", "Content": "No content"},
{"Title": "Art imitates life", "Author": "Unknown author", "Content": "No content"},
{"Title": "Remember this?", "Author": "u/YosoyPabloIscobar", "Content": "No content"},
{"Title": "Well now you know", "Author": "u/emmmmceeee", "Content": "No content"},
{"Title": "Ever wondered what 2 Peta Bytes looks like?", "Author": "u/P_G_R_A", "Content": "No content"},
{"Title": "youtube-dl repo had been DMCA'd", "Author": "u/anakinfredo", "Content": "No content"},
{"Title": "Kids, you up?", "Author": "u/diamondsw", "Content": "No content"},
{"Title": "Problem has been solved, 87 TB Array! No more Panik", "Author": "u/cdeveringham", "Content": "No content"},
{"Title": "Thought you all might find this interesting", "Author": "u/erik530195", "Content": "No content"},
{"Title": "API Clusterfuck! ~ We're locked, read this.", "Author": "u/-Archivist", "Content": "See reopening post..... Hi everyone, we'll keep this short, you already know what's going on. As you've almost certainly heard by now Reddit is locking down their API starting July 1st with the introduction of paid usage. These changes are what killed pushshift.io (full reddit archives and searchable api used by mods and many research/academic papers) and what will kill most (if not all) third-party reddit clients. This is obviously a detriment to everyone, and while Reddit will almost certainly go through with these changes regardless, thousands of subreddits are going to be participating in a 2-day (or longer) blackout. You can read more about the blackouts at r/ModCoord . At the very least, the planned blackout seems to have convinced Reddit to give free API access to accessibility clients. Hopefully it can change their minds further. r/DataHoarder will be locked for an undetermined amount of time, see this thread for reddit data archives, tools, etc. we will also be using this time to update our sidebar links and do some general maintenance in the hopes that this mess doesn't mean the end for us and the many communities that see this as a killing of the Reddit we have loved over the years. Note; during this time no new posts can be made and all comments are black-holed. ~ The Mod Team, ciao for now. Track the blackout here: https://reddark.untone.uk"},
{"Title": "The Coronavirus Papers unlocked: 5,352 scientific articles covering the coronavirus - fully searchable and free.", "Author": "u/shrine", "Content": "2020-04-15 update: the-eye.eu is temporarily down, but the de-centralized Interplanetary File System (IPFS) link remains up. Note, publishers have made most Coronavirus articles free as of March 6th 2020. Visit r/libgen and r/scihub to join the open science revolution. Access Article listing / Links Open Directory Full-text Search Torrent InterPlanetary File System Information In a 2015 New York Times op-ed the chief medical officer of Liberia argued that the Ebola pandemic responsible for the loss of over 2,200 lives could have been prevented if not for a paywall blocking access to an article from 1982. Dividing the worldâ€™s scientists with a paywall in the middle of a global humanitarian crisis is an unacceptable and unforgivable act of criminal greed. In the developing world the price for a single article can amount to as much as half a weekâ€™s salary for a physician. A few days ago, I found an early-release coronavirus article with a $35.95 access fee for non-subscribers. The fury I felt brought tears to my eyes. Me and a few friends share that fury, so we gathered a collection of five-thousand scientific studies covering any article title containing â€œ coronav* â€ from 1968-2020. The scope of the papers spans not only the 7 human coronaviruses, but up to 40 other Coronaviridae family strains. The Ebola virus showed us that every study counts. We are on the first step towards compiling a complete open-access Coronaviridae research catalog for the worldâ€™s scientists, journalists, and virology experts to draw from to fight the virus and save lives. Our project is illegal, but itâ€™s the right thing to do in this crisis. We refuse to put copyright before human lives. Sharing everything we know about the virus is essential, which is why international scientists are openly sharing their coronavirus findings in an unprecedented way . Developing-world scientists often work without article access due to complex and expensive contract agreements between publishers, universities, and hospitals, relying on overseas colleagues to help them hunt down PDF files. The virus is not going to wait for this, so we need to act with conviction, now. To their credit, publishers made a few dozen papers open-access in the last few days, which you can find over at Elsevierâ€™s Novel Coronavirus Information Center and Wileyâ€™s Coronavirus collection. While Wiley is slating to shut down their collection in April, our collection wonâ€™t be shutting down anytime soon. Weâ€™re going to keep growing to help our scientists out, and you can help us complete the catalog by identifying any papers we missed. All extant Coronaviridae research, accessible in seconds, by any scientist in the world. Itâ€™s the least we can do to help. Methodology How did we do it? We scanned Sci-Hub 's 80 million title collection for the coronavirus, then we extracted the titles and Digital Object Identifiers (DOI) to an index, and exported the PDF files to upload them to The-Eye.euâ€™s full-text search repository. How can I help? We always need developers. You can also help us identify new articles by joining our team spreadsheet here . Request access and you can begin adding new article titles to the list. You can also help share word of the collection with the scientific community by reaching out to journalists. Who is helping us? Our brave host is The-Eye.eu , a â€œnon-profit, community driven platform dedicated to the archiving and long-term preservation of any and all data,â€ making this project just one of the many public access preservation projects they stand behind. You can aid projects like this one by donating toward their server bills. A thank you to Sci-Hub and Library Genesis. Last year communities across reddit (including r/seedboxes and r/DataHoarder ) came together in a mission to secure and preserve Sci-Hub and Library Genesis, collectively the two largest free and open non-profit library collections in the world: Sci-Hubâ€™s 80-million scientific article database that made this project possible, and LibGenâ€™s 2.5-million scientific-book collection. The libraries fulfill United Nations world development goals mandating the removal of restrictions on access to science, and they serve developing world doctors, academic researchers, and other experts in society with the knowledge they need to build a better world. Keeping these libraries open and thriving means saving lives, educating the world, and providing invaluable science to humanityâ€™s global experts. Thank you to everyone involved in the project, The-Eye.eu for their support, and to all the scientists around the world working on behalf of humanity today."},
{"Title": "Twitter will remove free access to the Twitter API from 9 Feb 2023. Probably a good time to archive notable accounts now.", "Author": "u/babelfishery", "Content": "No content"},
{"Title": "Imgur is updating their TOS on May 15, 2023: All NSFW content to be banned", "Author": "u/trd86", "Content": "No content"},
{"Title": "Thought you guys would appreciate this.", "Author": "Unknown author", "Content": "No content"},
{"Title": "The only Nintendo 64 and 64 Disk Drive Development Data Tapes known to exist are now resting happily in my collection, and happy to say 5/6 are dumped and preserved. I'm told the last one has no data on it, but I will be working to recheck and verify that. Data can be found at ultra64.ca", "Author": "u/Carl_Sammons", "Content": "No content"},
{"Title": "I've collected all the iFixit repair guides in PDF format - 38,893 files", "Author": "u/makeworld", "Content": "iFixit and their guides are a great source for learning how to repair and fix electronics. They offer all their guides in PDF format, which I thought might be easier for viewing and self-containment then HTML. I've downloaded all their guides as PDFs, and put them into a single torrent. I think this is information that is very valuable to have offline - for power outages, remote travel/backpacking, the end of the world, etc. I'm hoping this can join some of your collections, beside Wikipedia and first aid pamphlets. Magnet link: magnet:?xt=urn:btih:ed9889445d52d7882e844bd926e1b547a2c00781&dn=pdfs.zip&tr=udp%3A%2F%2Ftracker.coppersurfer.tk%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337%2Fannounce&tr=udp%3A%2F%2Ftracker.leechers-paradise.org%3A6969%2Fannounce&tr=udp%3A%2F%2Fp4p.arenabg.com%3A1337%2Fannounce Torrent file The torrent is just a single ZIP file named pdfs.zip , that contains all the guides. It is about 60 gigabytes in total. Each guide is named by the name of the guide, for easy searching. Duplicate names were fixed by adding numbers to the end, as in guide name [2].pdf and guide name [3].pdf . All filenames are Windows safe. Keep in mind that my upload speeds are slow, and it may take a bit for your computer to find mine. But I have always-on server that is seeding it, so it will download eventually. The contents of the torrent will also be up on the Internet Archive here , they are downloading it now. If you want to replicate what I've done, or update the archive yourself (I will try and update it every so often), there are wget and python3 scripts and source files (like lists of urls) in that archive as well. Those files are not part of the torrent. If you have any questions, or plan on seeding, let me know below! EDIT: As I mentioned above, my upload speed is slow. The torrent will take a very long time initially, and there's not much I can do about that. Feel free to come back in a couple days when there will be more than just me with a full copy."},
{"Title": "YouTube-dlâ€™s repository has been restored", "Author": "u/DisastrousRhubarb", "Content": "No content"},
{"Title": "Z-Library isn't really gone, but that maybe up to you.", "Author": "u/-Archivist", "Content": "UPDATE2 TorrentFreak is covering this continuing story as new details come to light. https://torrentfreak.com/tag/zlibrary/ UPDATE ~ Z-Library Aftermath Reveals The Feds Seized Dozens of Domain Names @ TorrentFreak We'd also like to address some of the comments here asking \"how do I extract a book from this data\" . r/DataHoader isn't a piracy supporting subreddit, a guide on how to extract books from these archives was purposefully left out. These torrents are presented as a preservation only archive and are not meant to aid book piracy or add books to your curated collections. Once upon a time in this sub this explanation wouldn't have been necessary. The thread will be cleaned and comment locked. Original Thread Millions woke up to news today that Z-Library domains have been seized, cries that z-lib is gone were heard from red core to black sky!... but that's not really the case so here is what you, a humble datahoarder can do about it. In case you missed it a unique to z-lib (deduped against LibGen) backup was made and published by u/pilimi_anna a little over a month ago. While you did a great job with SciHub, there's still work be done to ensure the preservation of all written works and cultural heritage. So here is the 5,998,794 book 27.8TB z-lib archive for you to hold, hoard, preserve, seed and proliferate. Database | Mirror ~ (metadata, extensions) Torrents | TOR Mirror Related Reading U.S. Authorities Seize Z-Library Domain Names @ TorrentFreak TikTok Blocks Z-Library Hashtag @ TorrentFreak ZLibrary domains have been seized @ HackerNews ISBNdb Dump â€“ How many books are preserved forever? @ Annas-Blog Mission to preserve SciHub @ r/DataHoarder Alternative Libraries / Free eBook Hosts OpenLibrary Library Genesis | IPFS PDF Drive Sci-Hub Gutenberg Obooko ManyBooks FreeBookSpot The Anarchist Library Closing Support authors you love.. But abolish the strangle hold of DRM and licensing that kills ownership, seek to squash abuse of the DMCA, move to limit copyright terms and above all aim to ensure Alexandria doesn't burn twice. Ukraine Crisis ^Megathread will replace this thread again within 7 days."},
{"Title": "The dream ðŸ™", "Author": "u/DragoniteChamp", "Content": "No content"},
{"Title": "Not just SATA . . .", "Author": "Unknown author", "Content": "No content"},
{"Title": "Anon loses 8 terabytes of data", "Author": "u/Epoxhy", "Content": "No content"},
{"Title": "Been watching everyone panic over HBO Max gutting it's library like a fish.", "Author": "u/AshleyUncia", "Content": "No content"},
{"Title": "In 1999 Amazon stored .5GB or â€œabout 350 floppy disksâ€ of data about its users every day", "Author": "u/the_best_moshe", "Content": "No content"},
{"Title": "I'm sorry Hasan. :(", "Author": "u/hobbseltoff", "Content": "No content"},
{"Title": "URGENT: Hong Kong Stand News to cease operations immediately after directors arrested this morning. Please help backup social media and website!", "Author": "u/TheIrishPanther", "Content": "No content"},
{"Title": "Two months ago, when I go a 16TB swapped for an 8TB from Amazon, and everyone told me I was getting catfished by someone pretending to be Seagate's head of global security? Here's the free 10TB Exos Seagate sent me direct from HQ 'for my trouble'. :P", "Author": "u/AshleyUncia", "Content": "No content"},
{"Title": "Youtube deleted a channel that made educational ethical hacking videos. Other Channels on the same topic face a risk of being taken down. Can you help and archive the channels that havenâ€™t been deleted yet.", "Author": "u/latuziti", "Content": "The channel zSecurity was was deleted and it seem like other youtube channels that make ethical hacking content might face the same risk. If you can help please backup that following youtube channels. HackerSploit - https://www.youtube.com/channel/UC0ZTPkdxlAKf-V33tqXwi3Q/videos Sam Bowne - https://www.youtube.com/user/sambowne/videos LiveOverflow - https://www.youtube.com/channel/UClcE-kVhqyiHCcjYwcpfj9w/videos Null Byte - https://www.youtube.com/channel/UCgTNupxATBfWmfehv21ym-g/videos InsiderPhD - https://www.youtube.com/c/InsiderPhD/videos Loi Liang Yang - https://www.youtube.com/channel/UC1szFCBUWXY3ESff8dJjjzw/videos STÃ–K - https://www.youtube.com/c/STOKfredrik/videos The Cyber Mentor - https://www.youtube.com/channel/UC0ArlFuFYMpEewyRBzdLHiw Guided Hacking - https://www.youtube.com/user/L4DL4D2EUROPE/videos Null - https://www.youtube.com/channel/UCZF93Qrt6yMAabRnlND4YsQ/videos Cheat The Game - https://www.youtube.com/user/BloodFayte/videos Stephen Chapman - https://www.youtube.com/user/seowhistleblower/videos Cristi Vlad - https://www.youtube.com/user/cristivlad25/videos DC CyberSec - https://www.youtube.com/channel/UC3sccPO4v8YqCTn8sezZGTw/videos Joseph Delgadillo  - https://www.youtube.com/c/JosephDelgadillo/videos Nahamsec - https://www.youtube.com/c/Nahamsec/videos I.T Security Labs - https://www.youtube.com/c/ITSecurityLabs/videos Red Team Village - https://www.youtube.com/c/RedTeamVillage/videos People have recommended other youtube channels to archive. I'll add them here Deviant Ollam - https://www.youtube.com/user/DeviantOllam LockpickingLawyer - https://www.youtube.com/channel/UCm9K6rby98W8JigLoZOh6FQ BosnianBill - https://www.youtube.com/user/bosnianbill 13Cubed - https://www.youtube.com/c/13cubed/videos Jose Barrientos - https://www.youtube.com/user/Greiko Update 2: Cyberspatial - https://www.youtube.com/c/Cyberspatial/videos Nahamsec - https://www.youtube.com/c/Nahamsec/videos pwn.college - https://www.youtube.com/channel/UCBaWwFw7KmCN8YlfX4ERYKg/videos Farah Hawa - https://www.youtube.com/c/FarahHawa/videos Web Development Tutorials - https://www.youtube.com/c/yaworsk1/videos OALabs - https://www.youtube.com/c/OALabs/videos Hacksplained - https://www.youtube.com/c/Hacksplained/videos Update 3: zecurity channel has been restored. It should be backedup just in case something like this happens again. https://www.youtube.com/zsecurity"},
{"Title": "Data hoarding is older than we thought! MAD Magazine 215 from 1980", "Author": "u/Hong-Hong-Hang-Hang", "Content": "No content"},
{"Title": "Hello, r/DataHoarder! Weâ€™re iFixit, and we just launched the worldâ€™s most comprehensive medical equipment repair database", "Author": "u/Craig_iFixit", "Content": "After my recent comment on here blew up, I figured you all would get a kick out of this. TL;DR: We scraped the internet for any and all medical equipment repair documentation we could find. We ended up with over 13,000 PDFs across 5,000 medical devices, all uploaded to iFixit.com and available for free to anyone and everyone. ----- Hospitals are having trouble getting service information to fix medical equipment and manufacturers canâ€™t keep pace with the growing demand for repair of critical hospital equipment. On top of that, biomedical technicians spend countless hours scouring the internet searching for crucial repair information. This is not a great way to run a health system. So weâ€™ve been fixing that. Over the last two months, weâ€™ve pivoted half our company to build the worldâ€™s most comprehensive medical equipment service database . We just posted more than 13,000 PDFs from hundreds of manufacturersâ€”online and available for free. You can find them in our Medical Device category. This has been an absolutely massive undertaking, and we were fortunate to have the help and support of over 200 librarians and archivists from across the country. Archivists from university and public libraries, research institutes, insurance and software companies, and of course biomedical technicians themselves, all donated their valuable time. Collectively, theyâ€™ve contributed thousands of hours organizing piles of documents into a navigable, searchable system. Some medical manufacturers, like Mindray , allow biomeds to access their manuals freely. A few more released select documents after the outbreak of COVID-19. But for their day-to-day work, biomeds have long relied on a rag-tag set of web resources to get the job done. Among the most popular is Frankâ€™s Hospital Workshop , a Tanzania-based site that hosts hundreds of medical device manualsâ€”itâ€™s the unofficial biomed bible. But we wanted to make it easier for anyone to find the right manual, especially in an emergency. Some of the documents in our collection were already available. Others were not publicly posted until now . And it was important to us that this resource didnâ€™t just duplicate existing resources, but improved accessibility in a meaningful way. To be very clear: iFixit will not make any money off of this project . We are providing hosting and curation free of charge, and free of advertising, to the medical community. We welcome manufacturers to join us and contribute toward an up-to-date central repository for the biomedical community, as well as biomedical technicians around the world to join iFixitâ€™s repair community . No technician is an island, and we hope to facilitate an exchange of knowledge and troubleshooting. This medical repository is most useful if itâ€™s collaboratively moderated by biomedical technicians, with our assistance."},
{"Title": "[Offtopic] Girlfriend took one look at the shell of a shucked EasyStore... â€œYou need that?â€", "Author": "u/DannyVFilms", "Content": "No content"},
{"Title": "Russianaircraft.net scrubs all military aircraft in a likely effort to prevent identification of downed Russian aircraft - If you ever needed a better justification for datahoarding, here it is.", "Author": "u/AshleyUncia", "Content": "No content"},
{"Title": "LSI 9650se-16ml rebuild questiom", "Author": "u/SirMeili", "Content": "I know this is an old card. I've been using it since I bought it new in 2008. I'm prepping to do a migration to trunas and a set of larger drives and an also 9305 card but until I can get some new drives I'm limping along with this old card. I currently have a RAID6 array of 9 4tb WD red drives. One degraded and it's been trying to rebuild. I'm currently running windows server 2016 essentials. I think something is causing the is to bsod while rebuilding the array. So I went into the cards bios to rebuild there. Does anyone know if the card will rebuild while I'm in the bios screen of the card? I would rather leave it there and let it rebuild. It could also be the drive is hosed and I need to buy a new one. Not sure why the drive went degraded except that we had a storm the other night and my UPSs battery apparently failed (less than 6 months old) and the server hard shutdown. The drives were unlikely to be writing anything as I don't currently write to that array. It's purely read operations. Any help would be greatly appreciated. *Btw I have backed up data to external drives, but I would still rather not lose the data as I could mount the array and make copying to the new array in trunas faster."},
{"Title": "Cold storage backups", "Author": "u/Vatican87", "Content": "What's the best way to backup important media (Photos / Videos), are cold storage options the best way? As in do they degrade much at all if they're only spinning up every few months to backup files?"},
{"Title": "HD Tune Pro Question", "Author": "u/klaaaay", "Content": "Hello, I have installed the HD Tune Pro program to see how my disk was since in the Crystal Disk Info program it shows me 69% Good in green, the disk is approximately 2 and a half or 3 years old and I wanted to confirm the information, doing the Error Scan test I have noticed that there are rectangles that remain blank, should I be worried? https://preview.redd.it/hd-tune-pro-question-v0-zycqm8oomt6d1.png"},
{"Title": "I upsized from a Fractal Design Node 804 into a Meshify 2 XL!", "Author": "u/shockguard", "Content": "No content"},
{"Title": "First machine at 15 YO", "Author": "u/modestt_rat", "Content": "No content"},
{"Title": "Backup Client From NAS", "Author": "u/FancyRectangle", "Content": "I've been using Arq 7 for quite some time at this point, and have had a Windows 2019 VM running exclusively to mount RO shares from my main NAS and backup to a couple endpoint. This has been on a physically separate box (security), but have been considering moving this VM to just be on the NAS itself and backing up from there. Does anyone else backup straight from NAS to a remote endpoint? I have a separate local NAS that pulls snapshots, but looking into some potential consolidation for a VM that needs to remain on consistently."},
{"Title": "Amazing piece of software: Podcast Bulk Downloader", "Author": "u/didyousayboop", "Content": "Podcast Bulk Downloader is a Windows program (written by u/cnovel ) that allows you to batch download episodes of a podcast simply by copy/pasting in a link to the RSS feed. There is an option to automatically append the release dates of the episodes as a prefix to the file names. Download it on GitHub here ."},
{"Title": "Can you rename files based on metadata within the file?", "Author": "u/Elarionus", "Content": "I take photos with three different devices at work for different purposes, a Pixel, a Samsung, and an iPhone. They all name their files differently. This isn't an issue when they're all in Google Photos, but that's never their permanent home. They all add different pieces and parts to the file names. I know I could run them through a renamer to remove or add certain parts, but I was wondering if there's a software out there that lets me just dump thousands of photos into it and have it spit out filenames based on the metadata of when they were taken. YYYY-MM-DD_HH-MM-SS (Year, Month, Day, Hours, Minutes, Seconds). It would help us out a lot as we would be able to filter images in file explorer much more quickly."},
{"Title": "NEW to RAID and need help with a DIY 4 Bay", "Author": "u/FreasFrames", "Content": "I am new to the whole RAID landscape. I have been looking at the Thunderbay 4 to set up as a RAID 5 with 4x16TB HDD. the question I have is what is the most cost effective way of building one? or is it a smart idea to go with the OWC built 4x16TB so its just plug and play? Would like some help with what sites to source drives and enclosures from if OWC is not my end result Currently on a 2017 iMac Retina with 64MB DDR4 using a 3.4 GHz Quad-Core Intel i5. Running Ventura 13.6.6"},
{"Title": "Long term data storage options", "Author": "u/BroccoliSanchez", "Content": "I currently have a wd 14tb desktop harddrive for my digital backups of my home media. I know consumer drives have a decent shelf life but I was wondering if there is a particular brand or type of drive I should use to back up my main drive. I plan on keeping the backup drive in a fire and flood proof safe and only powering it up to do weekly updates of new files. External options are preferred for the simplicity"},
{"Title": "ZFS write errors on 7200 RPM drives, fine with badblocks", "Author": "u/rhnet", "Content": "I have been struggling adding 7200 RPM refurbished SATA drives (WUH721414AL) to my existing ZFS pool. The drives have no issues with badblocks, and I have tested them on another system as well. When I try to add them to an existing zfs mirror, I run into lots of WRITE/CKSUM errors, and they will eventually fault. Here's the output only 10% into a resilver. NAME                       STATE     READ WRITE CKSUM tank                  DEGRADED     0     0     0 mirror-0                 DEGRADED     0     0     0 disk2_crypt            ONLINE       0     0     0 disk16_crypt           ONLINE       0     4    93  (resilvering) And in dmesg I get stuff like: [Sat Jun 15 12:39:59 2024] zio pool=tank vdev=/dev/mapper/disk16_crypt error=5 type=2 offset=7423255224320 size=12288 flags=1808aa [Sat Jun 15 12:39:59 2024] zio pool=tank vdev=/dev/mapper/disk16_crypt error=5 type=2 offset=7423255212032 size=12288 flags=1808aa I have these attached to an HBA and SAS expander, but I've also tried with the SATA ports on the motherboard directly. Setup: LSI SAS9340-8i ServeRAID M1215 12Gbps SAS (from artofserver) Adaptec 2283400-R AEC-82885T LENOVO 36Port 12Gb/s SAS Expander Card 82885T US 10Gtek# 12G Internal Mini SAS HD SFF-8643 to SFF-8643 Cable, with Sideband, 100-Ohm, 0.5-m(1.6ft), 2 Pack 3x AdcAudx 2Pack SFF-8643 to SATA: 1M SFF-8643 Mini-SAS to SATA-Cable SFF8643 to SATA Mini SAS HD to SATA Forward Breakout (3.3FT) Lots of working fine 5200 RPM data drives: WD101EMAZ-11, WDC WD140EDFZ-11, WDC WD140EDGZ-11, WD80EMAZ-00W, ... Several 7200 RPM drives from Serverparts and goharddrive, all WUH721414AL. I run them with luks (cryptsetup luksFormat -c aes-xts-plain64 -s 512 -h sha256) and in zfs mirror."},
{"Title": "Something about sorting media that is really bugging me", "Author": "u/kyne_ahnung", "Content": "This is one of those questions where I don't know if I'm doing it wrong or so many other people are, but I can't work out why it's so hard to find a way to go through a big directory of videos or images in a gallery/viewer manually and easily say this one should go in folder A, that one in B, this one in D, that one in A; without having to - for each file - close the gallery (memorizing the filename), find it in the current directory among thousands of others, also find the destination and open that in another window, copy/move/drag the file to the destination. What I'm looking for: A video and image gallery Some kind of sidebar with shortcuts to favourited/saved paths The ability to easily move the image or video that I am currently viewing to one of these shortcuts either via drag or clicking move>move to>shortcut to folder A Am I crazy/stupid or is it unneccessarily hard to do this task in an ergonomic way? I have all my photos and videos sent and received on my phone over the years to sort through... deleting is no problem, finding duplicates is easy but how do people actually do the organizing? Any methods, software suggestions welcome. Edit: or another plausible method is being able to easily apply a tag to each file from the viewer, so you watch a video and can quickly click on or type a tag.."},
{"Title": "Download site with login?", "Author": "u/Cyan7988", "Content": "Hello, a website is about to shutdown and there are many links that are only accessible after logging in, when downloading website with httrack it only downloads the non logged in version of thr site which I can't see the download links, how to I make it download the logged in version of thr site? I have the password and username Using HTTRACK"},
{"Title": "A simple way to save a lot of data?", "Author": "u/Jatm4", "Content": "Hi there, I've been having a serious storage problem for a few weeks now. I used to store the media generated by my projects on individual external HDD, but this is no longer an option for me. I found a storage unit on Amazon (WD Elements 18TB) that was exactly what I needed. The problem? I bought it and it came DOA, and after seeing some comments about that device, it seems they are very unreliable, so it's no longer an option. Now I don't know how to solve my problem. I just want a drive to store my backups and where I can access them when needed, not something that needs to be constantly connected, some simple, a NAS seems a bit excessive and I wouldn't want to use a cloud. What solution do you recommend me? Did I just have bad luck with the WD Elements? I live outside the USA, so I would need something that can be purchased and shipped internationally"},
{"Title": "Can I put a 12-to-16 pin adapter inside a USB enclosure?", "Author": "u/biocoder86", "Content": "I have an old SSD drive I want to recover data from and maybe use as a portable drive going forward... Specifically I want to use this old 12 + 16 pin from an old MacBook Air (A1465), with a 12 + 16 to NVMe M key adapter. https://amzn.eu/d/1gMJdGQ Then I want to put those inside a NVMe M key to USB enclosure. https://amzn.eu/d/9ifm71g But the fourth image you scroll to of the adapter says that it cannot work in a USB enclosure, only plugged directly into the motherboard. Then in one of the user questions they seem to say the opposite with no explanation. That so long as it is 12 + 16 on one end and M key on the other it will work. So... will it work? If not, is this universally true or can I get a different adapter?"},
{"Title": "SATA vs SAS", "Author": "u/uberkalden2", "Content": "Genuinely confused here.  Are any of the enterprise drives SATA, or are they all SAS? They all say SATA in the description, but the details will typically also say SAS.  I was going to get a Sabrent external enclosure, but I don't think they work with SAS drives. Maybe my understanding of SATA is wrong?"},
{"Title": "Megaraid 9271-8i Dying - How do I replace it without data loss?", "Author": "u/Barja_Bardagi", "Content": "(Cross-posting a couple places to hopefully find help)  I've got a DIY NAS that I built a number of years ago based on a Megaraid 9271-8icc. There's 8x HDD RAID-50 array on it, and a 6x SSD RAID-6 array. Well, last Sunday, the card started dying. The machine's boot drives are NOT on the raid controller, and every few reboots, the card works for a while and then craps out. So, I bought a new (used) card from eBay. Now, storage is NOT my forte. I know enough to be dangerous to myself. How do I go about replacing the failing card without losing all my data? Will the new card see data on the drives and realize \"Oh, this is part of an array\"? Do I need to try to recover some settings from the old card? Could someone explain this to me like I'm an end user? :P"},
{"Title": "Instaloader hits login wall with VPN", "Author": "u/patagonianlamb", "Content": "A few weeks back, I managed to scrape a good amount of data from Instagram using a VPN and Instaloader without even logging in. It was great! But now, whenever I try to access Instagram through a VPN, it just takes me straight to the login page. Instaloader can't bypass the login wall either. Is this the end of accessing Instagram media without logging in via a VPN?"},
{"Title": "Refurbished NAS Hard Drivesâ€¦ Ye or Ne?", "Author": "u/maximumkush", "Content": "Starting my first NAS to fuel my addiction. Iâ€™m starting off with RAID 5 with 3 12tb hard drives. I found some decent priced refurbished ones. Asking the pros for their honest opinion"},
{"Title": "Would you not use a certified refurbished 20tb drive for parity?", "Author": "u/NotAnADC", "Content": "Asking specifically if I should only use a drive like that for expansion and not parity, cause maybe parity sees more action. Running in unraid. I know a lot of people like and trust serverpartdeals"},
{"Title": "How often should I update/refresh my cold storage?", "Author": "u/largePenisLover", "Content": "For cold/\"off-site\" storage of backups I use few HDD's. Backups go on them and then I store them nice and safe in a brick shed as an off-site kinda thing. They contain full backups. Disk is wiped and then a fresh backup goes on. I do this a few times a year. Not a real schedule to it. I fear bitrot. How often should I refresh them to manage the chance of bitrot happening? Would it even help? I mean yes I wrote new data but the metal still aged."},
{"Title": "almost entirely geoblocked youtube videos?", "Author": "u/koalahugthekoala", "Content": "hi! so i have some important videos that are related to my special interest but have been geoblocked outside kosovo, somaliland, and northern cyprus. i know they still exist so what tool would i use to get them? i have no idea how github or anything works so that kinda rules out youtubedl unless someone did it for me. they are not on the wayback machine. here's the links if this helps you! https://youtube.com/watch?v=b7TYi0FwR1M https://youtube.com/watch?v=xmKYxwXox5g https://youtube.com/watch?v=AJr9VOCe1yY https://youtube.com/watch?v=FMLjygFXB7w https://youtube.com/watch?v=ZhLJyHxRuu8 https://youtube.com/watch?v=mHU6dS07cZQ https://youtube.com/watch?v=Z6S5klRoI_c https://youtube.com/watch?v=AVeE9NMfAxE thank u for any help!"},
{"Title": "How do i rip a copy protected DVD?", "Author": "u/Icy-Composer9021", "Content": "I have some DVDs i wanna rip onto my computer and put on my ipod, specifically some simpsons and avp if it matters. Also region code is 2 or PAL on the simpsons."},
{"Title": "Personal NAS OS: Debian/Ubuntu vs TrueNAS or Rockstor", "Author": "u/PuzzleHeadPistion", "Content": "I have a personal NAS built on a i5-4690 with 16Gb RAM, which I use for archived work files (music, photos and videos), backups from my computers and as Plex Server. Services like ownCloud or something that would allow me to share files via url (like a personal WeTransfer) would be extremely useful. Maybe a system with ZFS would be good. I'm trying to decide whether to build from scratch on a full OS (Debian, Ubuntu or even Windows 2022?) vs something like TrueNAS or Rockstor. I haven't used Linux in almost 10y, but I used to use Arch. Not sure if it's relevant, but I also own an Asustor Drivestor 2 Lite and a ZenWifi XT9 router with a drive attached, use WireGuard VPN, etc."},
{"Title": "Samsung 8tb qvo ssd reliability", "Author": "u/ChillCaptain", "Content": "I bought the 8tb Samsung qvo ssd for a gaming/Roms drive. I know performance is meh and tbw is not great. But I donâ€™t expect to write a lot to the drive. Copy my Roms and games once and switch out the games every so often. Iâ€™m mainly concerned with reliability. Id imagine it would be better than a spinning hard drive. Any opinions on reliability?"},
{"Title": "[HDD] 14TB Refreshed WD Ultrastar Hard Drive w/ 5 Year Warranty - $99.99 (NewEgg)", "Author": "u/Axodique", "Content": "No content"},
{"Title": "Quick question about IDM", "Author": "u/redskies1991", "Content": "Can i save my download location to an external drive (external ssd) ? Does it work.. coz i want to lessen the \"rebuilding\" time when i download large file sizes.."},
{"Title": "How to Download a Video off of Europeana?", "Author": "u/thxdley", "Content": "Sorry, i dunno if this belongs here but i dont know where else to put it. I'm pretty sure Europeana embeds the videos or something (i have no clue how any of it works) but when i try downloading the video it gives me a \"raw.html\"  which my computer wont download. Is it impossible to get my hands on this? I dont want to screen record it because of background noise/low quality/lag. Below is the link to the video im trying to download on Europeana. https://www.europeana.eu/item/2051918/data_euscreenXL_EUS_0BCE32473CEBA87D2DB36A87B5511130"},
{"Title": "Does anyone know the IBM TS3500 Tape Library (IBM 3592 JC/JY)?", "Author": "u/_c0der", "Content": "Hi all, does anyone know the IBM TS3500 Tape Library with the corresponding IBM 3592 JC/JY type drives and tapes? I can have a full rack for free - where is the catch? Is it worth it? Power and space isn't an issue. Pictures: https://imgur.com/a/mjwmvI6"},
{"Title": "Is there info on failure rate of SSD vs spinning HD", "Author": "u/SlothChunks", "Content": "Hi everybody, I am trying to decide whether to buy a larger capacity drive to replace my two old Samsung SSDs that are 1TB and 500gb. When I looked at Amazon I saw that there are many options of new models if standard spinning drives for much less. As far as I know from reading over the years spinning drives are more likely to fail than the spinning disk drives.  But I havenâ€™t ever actually seen the test data to see just how likely or unlikely they are to break compared to SSD. If this drive is going to be used only at home and not carried anywhere, does anyone have any idea just how much more likely it is to fail than an SSD? Bonus: question, are WesternDigital SSDs any better or worse than Samsung? Or are there any established brands are are absolutely better to avoid?"},
{"Title": "Equipment recommendations", "Author": "u/Broke_Bearded_Guy", "Content": "I was looking at buying a storage drawer preferably a 4U 60bay. I've seen some of the newer HGST units available but I've seen some posts about requiring proprietary drives I didn't know if anyone could make a recommendation my only real requirement is 120v compatible"},
{"Title": "just wondering if others with IDM also have this problem", "Author": "u/Street_Mine_1969", "Content": "for the last 3 days my IDM pop up download bar does not show on youtube and youtube only. other other video site is still okay and can download without problem. anyone else with IDM have this problem with youtube?"},
{"Title": "how do you batch select on czkawka?", "Author": "u/randomusername11222", "Content": "there is this select windows https://preview.redd.it/how-do-you-batch-select-on-czkawka-v0-u0bmvpoaim6d1.png But I cant like multiselect the files that I want manually?"},
{"Title": "Which SATA Ssd should I buy in 2024?", "Author": "u/eliosferre3", "Content": "Hi! I need it to be a sata ssd, I recently bought a Crucial MX500 which failed while I was installing Windows and wasn't able to detect again. Now I'm thinking on a 1TB Samsung 870 Evo, but reading some posts in this subreddit apparently it has a high failure rate as well? I thought Samsung would be reliable but I don't know what to expect or buy anymore."},
{"Title": "Recommended temporary external drives?", "Author": "u/DisasterSpinach", "Content": "I might not be asking in the right place because my goal isn't to set up something that is highly systematic or robust at this time. The place I'm renting has serious roof issues and a bunch of my stuff was water damaged. I'm trying to salvage what I can to some portable drives and get the most important belongings out as soon as I can. In the past I remember Easystores/WD Elements being popular here, but apparently they have different drives inside now or high failure rates or something? And then I was just going to go with whatever Costco carried, but at this time that's only the doomed Sandisk SSDs. Anyone have any recommendations? Doesn't have to be perfect, just reasonably priced. Just looking to buy two drives for about $300-500 total. I guess HDDs preferred if they are decently reliable and not SMR. EDIT: OK externals are all apparently crap? https://www.reddit.com/r/DataHoarder/comments/146hb9k/information_about_cmr_to_smr_manufacturer/ Maybe I'll just roll the dice on those Sandisk SSDs.."},
{"Title": "Has anyone else ever experienced this? 20TB Exos white label results in my ssd activity spiking to 100% and boot issues", "Author": "u/good4y0u", "Content": "I recently purchased a 20TB Exos white label drive from eBay, which was advertised as a Seagate refurbished drive. The serial numbers do trace back to Seagate, but I'm experiencing some unusual issues with it. Has anyone else encountered this? Here's what's happening when I test it with my Windows machine: With the machine powered off, I plug in the drive, but the machine doesn't boot. With the machine powered off, I boot to BIOS, and the disk is found. However, after rebooting, the machine doesn't boot. If I unplug the drive after step 2, the machine boots normally. If I plug the drive back in at the login screen, the machine freezes. If I log in first and then plug the drive in, I can see the task manager showing max activity on my main OS SSD before the machine freezes. Unplugging the drive at this point unfreezes the machine. I've bought a number of drives this way before and never had a serious issue. Any insights or suggestions would be greatly appreciated. In the meantime I am reaching out to the seller. The other drive I got seems to work fine."},
{"Title": "Modular NAS build", "Author": "u/Mother_Occasion_8076", "Content": "No content"},
{"Title": "I finally migrated off my old Drobo! What's the bottom line wit SoftRaid? Do I need to pay for the premium license?", "Author": "u/Splitsurround", "Content": "So against ALL odds I did get all my data safely off a dying Drobo, onto a High Point 4 bay enclosure with four 16Tb drives. I made it Raid 5 using Softeraid's trial, and so far, so good. I'm trying to wrap my head around paying for the SoftRaid premium or not: I've read all the FAQs and as far as I can tell, the only things I can't do without premium is create new raids and use their monitoring feature. I understand about the new volume thing but...if I don't upgrade to premium, since I won't be able to use their monitoring, how will I know when a disk is dying or is dead?"},
{"Title": "Data management approach", "Author": "u/nlj1978", "Content": "Narrowing down my approach and looking for some additional guidance. Complete novice to home server operation.  After discussion in another thread, I've decided to not go with Windows OS on the server and instead run a Linux based OS so some sort. My hardware is a i5-9500 with 8gb or ram. In terms of storage I have a 256gb m2 NVME, 2x 10tb HDD and a 256g 2.5\" SSD if it is useful in some way. This leaves me with 2 sata3 ports(1 if I use the 2.5\" SSD) and 1 m2 slot available for further expansion of storage. Initial intended uses are to run JellyFin media server, Home Assistant and an image management app for pics, all local. I would like full redundancy via raid1, this is where I'm a bit uncertain on approach. I assume I'll run the OS and any apps on the NVME, not the HDD. Since i have the additional NVME slot available is there benefit to adding a second NVME drive for cache or even a mirror of the OS and apps? Should I be looking to manage my raid with an application of some sort or in the Debian OS itself? If the raid is setup in Debian, how does that affect future expansion? Do i have to add pairs of drives or could I for example just add a 18tb random drive? If for example a year from now I add 2 additional drives, at that time would there be a better solution than raid1? Thanks for your time."},
{"Title": "PlutoTV - Downloading episodes from live channel (ondemand not available)", "Author": "u/pstNN", "Content": "Hi all, I've been waiting for this show to be released online for ages and it just released this week, however, there doesn't seem to be any plans to make it on demand, which means I cant use StreamFab. Does anyone know of a way to download episodes without the ads/commercials from a live stream on PlutoTV france? This is the link. If there is a paid software for it, i have no problem buying it. https://pluto.tv/fr/live-tv/662a622b5e2370000800f392 Thanks!"},
{"Title": "Donating space", "Author": "u/noideawhatimdoing444", "Content": "I'm not at the place where I can donate space on my server but eventually I will be. I'd like to give support to pages like Wikipedia or any other project I feel aligns with my values. Is that something thats even feasible?"},
{"Title": "Couple question to get started", "Author": "u/EvenRD", "Content": "Hello, i tried checking in the wiki or in FAQs but i couldn't find anything of use. I am considering starting hoarding files on physical drives since my online cloud is full and i don't want to pay a monthly fee. A couple questions: Should i? I need space to save personal files, photos and videos. Right now i think i could have all my storage done with 1TB, but for the sake of futereproofing and redundancy i think i should aim for 4TB. I still don't know if its convenient or should i just suck it up and pay online cloud services Do i need a NAS? I do all my work from my main computer so what i am basically thinking is just to add a RAID 5 HDD to my existing setup and call it a day, is there some reason i should NOT do? Since HDD tend to get noisy i'd like to shut them off when they are not needed, this way i can save energy as well"},
{"Title": "Twitter extension to download videos inside the page", "Author": "u/findthatgayporn", "Content": "Anyone knows a good one? I have two installed but have to open a new tab for each video I want to download; I need a extension to click and start downloading"},
{"Title": "Sending large files online solution?", "Author": "u/backflipbadboy", "Content": "I found the website https://tempfile.me who are claiming privacy + encryption when uploading large files online up to 10GB per file has anyone used such a service?"},
{"Title": "G-technology 6tb fair used price? Reliability for Family Backup?", "Author": "u/myfacenotmyaccount", "Content": "Some guy near me has a couple of these for $90 each, I was thinking to buy a couple to back up all my families data and pictures, wondering if it was a good idea? https://www.amazon.com/G-Technology-Thunderbolt-High-Performance-Solution-0G04023/dp/B00QJJ5362?th=1"},
{"Title": "New HDD test sequence", "Author": "u/sobo5o", "Content": "Got another 5TB external Seagate Expansion HDD and want to optimize my routine. The drive is originally in exFAT with some warranty content on it. I have Windows 10, so not using badblocks, but have HD Sentinel. My order is this: Minimum: Short Long self-test (which will do an initial READ) Surface WRITE+Read with default 0's Quick format (to NTFS for use) Extensive: Short self-test Quick format (to NTFS, solely for the next step) Filling disk with large files ( using TeraCopy verify+test using H2TestW) Surface READ test Surface WRITE+Read with default 0's Surface WRITE+Read with 1's ( 0xFF) Surface WRITE+Read back with 0's Quick format (to NTFS) Alternatively, badblocks instead of steps 5-7, or full u/ EchoGecko795 routine below in the comments. Thanks to everyone, all questions answered! Some notes: Extended self-test can be skipped, as it just short self-test + READ test sequence within the drive, just less informative than the Surface read test as doesn't consider connectivity performance Full format in Windows isn't needed, as it's the same as Surface READ + WRITE (with 0's) tests, maybe more limited, and quick format is enough just to change the filesystem Different surface write+read patterns emulate the 4-pass badblocks test A few things I wanted to clarify: how redundant is filling the drive with files, considering further WRITE+Read surface test? Does it only serve as another WRITE pass, just with different data? do I really need the Read test before the WRITE+Read test , or is the latter enough? (i.e. can a READ before WRITE indicate something that a READ after WRITE won't?). My idea was to see the initial READ after the drive is filled with files, then overwriting it with 0's and reading again how important is changing the pattern/flipping 0's to 1's? should I flip 1's to 0's back again? Can the 1's pattern remain, following a quick format prior to using the drive? And finally, is the more time-consuming 3-4 pass procedure really worth it, and not an overkill?"},
{"Title": "Regarding Uloz.to", "Author": "u/HakimOne", "Content": "Hi, Any thoughts on https://ulozto.net ? Their plans look too good to be true. 10Â TB for 6 EURO, 50Â TB for only 15 EURO. Rclone recently added Uloz.to support."},
{"Title": "Shucking an old WD My Book?", "Author": "u/EdiblePwncakes", "Content": "I have this really old WD MyBook that I'm trying to backup the data off of. However the drive doesn't seem to spin at all even though the enclosure is powered on. It's a very old model, so much so that I can't quite find much info on it through a search online - its model is WD5000P302. I want to shuck it in hopes of preserving the data - however I've heard that these old WD MyBooks have some sort of drive encryption that might prevent this? Does anyone have any experience with this? The drive must be at least 10 years old if I were to guess. Or if the drive doesn't spin at all then might it be completely dead? Thanks for any advice. https://preview.redd.it/shucking-an-old-wd-my-book-v0-hyto1szn9l6d1.jpg https://preview.redd.it/shucking-an-old-wd-my-book-v0-sr1olvho9l6d1.jpg"},
{"Title": "How do you go about pruning your hoard?", "Author": "u/Clive1792", "Content": "Personally I feel snowed under with what I've got. There's countless files on my PC scattered everywhere. Documents, pictures (probably in the 100,000s), audio files, video files. It's most certainly not organised at all, it's scattered all over the hard drives on this PC in various folders & folders of folders. But to make it better I had no real backup plan. I'd just drag & drop and then not really remember what I'd backed up and what I hadn't so then I'd buy a new hard drive & make another copy because the last drive was fairly full. I now have a number of drives that surely have many multiples of various single files. My end goal is I would like some organisation rather than random files & folders scattered here there & everywhere. I'd like to get rid of the multiple duplicate files & also delete files I no longer want/need. Have any of you attacked this kind of task? Any tips or do you really just do it file-by-file? Which to be honest with what I've got & the fact I only have limited time outside of work, this is going to take me many months if not 12months plus .... and that's if I get stuck in & stay focused doing it day in day out."},
{"Title": "how to download videos from just for fans?", "Author": "u/m300n", "Content": "hi, i was wondering if anyone here knows how to save videos from just for fans?"},
{"Title": "[HDD] 14TB Refreshed WD Ultrastar Hard Drive w/ 5 Year Warranty - $99.99 (NewEgg)", "Author": "u/Axodique", "Content": "No content"},
{"Title": "Quick question about IDM", "Author": "u/redskies1991", "Content": "Can i save my download location to an external drive (external ssd) ? Does it work.. coz i want to lessen the \"rebuilding\" time when i download large file sizes.."},
{"Title": "MEGATHREAD: Archiving the Capitol Hill Riots", "Author": "u/AdamLynch", "Content": "FINAL UPDATE as of January 31st 5:35PM EST: Thank you to everyone who shared content. The content being submitted now from what I'm seeing is duplicates of older content. I will thus no longer be updating this archive. The MEGA will remain untouched, so use that as you please, but that will likely die one day as there is a bandwidth/transfer limit. I will be uploading the content to Internet Archive, as well as other sources, but until then the torrent magnet that I will be seeding for a little while is listed below - my bandwidth isn't the best so please do seed if you can: Magnet: magnet:?xt=urn:btih:c8fc9979cc35f7062cd8715aaaff4da475d2fadc&dn=Trump%20protest%20Jan%2006%202021&tr=udp%3a%2f%2ftracker.torrent.eu.org%3a451%2fannounce&tr=udp%3a%2f%2fpublic.popcorn-tracker.org%3a6969%2fannounce&tr=http%3a%2f%2f104.28.1.30%3a8080%2fannounce&tr=http%3a%2f%2f104.28.16.69%2fannounce&tr=http%3a%2f%2f107.150.14.110%3a6969%2fannounce&tr=http%3a%2f%2f109.121.134.121%3a1337%2fannounce&tr=http%3a%2f%2f114.55.113.60%3a6969%2fannounce&tr=http%3a%2f%2f125.227.35.196%3a6969%2fannounce&tr=http%3a%2f%2f128.199.70.66%3a5944%2fannounce&tr=http%3a%2f%2f157.7.202.64%3a8080%2fannounce&tr=http%3a%2f%2f158.69.146.212%3a7777%2fannounce&tr=http%3a%2f%2f173.254.204.71%3a1096%2fannounce&tr=http%3a%2f%2f178.175.143.27%2fannounce&tr=http%3a%2f%2f178.33.73.26%3a2710%2fannounce&tr=http%3a%2f%2f182.176.139.129%3a6969%2fannounce&tr=http%3a%2f%2f185.5.97.139%3a8089%2fannounce&tr=http%3a%2f%2f188.165.253.109%3a1337%2fannounce&tr=http%3a%2f%2f194.106.216.222%2fannounce&tr=http%3a%2f%2f195.123.209.37%3a1337%2fannounce&tr=http%3a%2f%2f210.244.71.25%3a6969%2fannounce&tr=http%3a%2f%2f210.244.71.26%3a6969%2fannounce&tr=http%3a%2f%2f213.159.215.198%3a6970%2fannounce&tr=http%3a%2f%2f213.163.67.56%3a1337%2fannounce&tr=http%3a%2f%2f37.19.5.139%3a6969%2fannounce&tr=http%3a%2f%2f37.19.5.155%3a6881%2fannounce&tr=http%3a%2f%2f46.4.109.148%3a6969%2fannounce&tr=http%3a%2f%2f5.79.249.77%3a6969%2fannounce&tr=http%3a%2f%2f5.79.83.193%3a2710%2fannounce&tr=http%3a%2f%2f51.254.244.161%3a6969%2fannounce&tr=http%3a%2f%2f59.36.96.77%3a6969%2fannounce&tr=http%3a%2f%2f74.82.52.209%3a6969%2fannounce&tr=http%3a%2f%2f80.246.243.18%3a6969%2fannounce&tr=http%3a%2f%2f81.200.2.231%2fannounce&tr=http%3a%2f%2f85.17.19.180%2fannounce&tr=http%3a%2f%2f87.248.186.252%3a8080%2fannounce&tr=http%3a%2f%2f87.253.152.137%2fannounce&tr=http%3a%2f%2f91.216.110.47%2fannounce&tr=http%3a%2f%2f91.217.91.21%3a3218%2fannounce&tr=http%3a%2f%2f91.218.230.81%3a6969%2fannounce&tr=http%3a%2f%2f93.92.64.5%2fannounce&tr=http%3a%2f%2fatrack.pow7.com%2fannounce&tr=http%3a%2f%2fbt.henbt.com%3a2710%2fannounce&tr=http%3a%2f%2fbt.pusacg.org%3a8080%2fannounce&tr=http%3a%2f%2fbt2.careland.com.cn%3a6969%2fannounce&tr=http%3a%2f%2fexplodie.org%3a6969%2fannounce&tr=http%3a%2f%2fmgtracker.org%3a2710%2fannounce&tr=http%3a%2f%2fmgtracker.org%3a6969%2fannounce&tr=http%3a%2f%2fopen.acgtracker.com%3a1096%2fannounce&tr=http%3a%2f%2fopen.lolicon.eu%3a7777%2fannounce&tr=http%3a%2f%2fopen.touki.ru%2fannounce.php&tr=http%3a%2f%2fp4p.arenabg.ch%3a1337%2fannounce&tr=http%3a%2f%2fp4p.arenabg.com%3a1337%2fannounce&tr=http%3a%2f%2fpow7.com%3a80%2fannounce&tr=http%3a%2f%2fretracker.gorcomnet.ru%2fannounce&tr=http%3a%2f%2fretracker.krs-ix.ru%2fannounce&tr=http%3a%2f%2fretracker.krs-ix.ru%3a80%2fannounce&tr=http%3a%2f%2fsecure.pow7.com%2fannounce&tr=http%3a%2f%2ft1.pow7.com%2fannounce&tr=http%3a%2f%2ft2.pow7.com%2fannounce&tr=http%3a%2f%2fthetracker.org%3a80%2fannounce&tr=http%3a%2f%2ftorrent.gresille.org%2fannounce&tr=http%3a%2f%2ftorrentsmd.com%3a8080%2fannounce&tr=http%3a%2f%2ftracker.aletorrenty.pl%3a2710%2fannounce&tr=http%3a%2f%2ftracker.baravik.org%3a6970%2fannounce&tr=http%3a%2f%2ftracker.bittor.pw%3a1337%2fannounce&tr=http%3a%2f%2ftracker.bittorrent.am%2fannounce&tr=http%3a%2f%2ftracker.calculate.ru%3a6969%2fannounce&tr=http%3a%2f%2ftracker.dler.org%3a6969%2fannounce&tr=http%3a%2f%2ftracker.dutchtracking.com%2fannounce&tr=http%3a%2f%2ftracker.dutchtracking.com%3a80%2fannounce&tr=http%3a%2f%2ftracker.dutchtracking.nl%2fannounce&tr=http%3a%2f%2ftracker.dutchtracking.nl%3a80%2fannounce&tr=http%3a%2f%2ftracker.edoardocolombo.eu%3a6969%2fannounce&tr=http%3a%2f%2ftracker.ex.ua%2fannounce&tr=http%3a%2f%2ftracker.ex.ua%3a80%2fannounce&tr=http%3a%2f%2ftracker.filetracker.pl%3a8089%2fannounce&tr=http%3a%2f%2ftracker.flashtorrents.org%3a6969%2fannounce&tr=http%3a%2f%2ftracker.grepler.com%3a6969%2fannounce&tr=http%3a%2f%2ftracker.internetwarriors.net%3a1337%2fannounce&tr=http%3a%2f%2ftracker.kicks-ass.net%2fannounce&tr=http%3a%2f%2ftracker.kicks-ass.net%3a80%2fannounce&tr=http%3a%2f%2ftracker.kuroy.me%3a5944%2fannounce&tr=http%3a%2f%2ftracker.mg64.net%3a6881%2fannounce&tr=http%3a%2f%2ftracker.opentrackr.org%3a1337%2fannounce&tr=http%3a%2f%2ftracker.skyts.net%3a6969%2fannounce&tr=http%3a%2f%2ftracker.tfile.me%2fannounce&tr=http%3a%2f%2ftracker.tiny-vps.com%3a6969%2fannounce&tr=http%3a%2f%2ftracker.tvunderground.org.ru%3a3218%2fannounce&tr=http%3a%2f%2ftracker.yoshi210.com%3a6969%2fannounce&tr=http%3a%2f%2ftracker1.wasabii.com.tw%3a6969%2fannounce&tr=http%3a%2f%2ftracker2.itzmx.com%3a6961%2fannounce&tr=http%3a%2f%2ftracker2.wasabii.com.tw%3a6969%2fannounce&tr=http%3a%2f%2fwww.wareztorrent.com%2fannounce&tr=http%3a%2f%2fwww.wareztorrent.com%3a80%2fannounce&tr=https%3a%2f%2f104.28.17.69%2fannounce&tr=https%3a%2f%2fwww.wareztorrent.com%2fannounce&tr=udp%3a%2f%2f107.150.14.110%3a6969%2fannounce&tr=udp%3a%2f%2f109.121.134.121%3a1337%2fannounce&tr=udp%3a%2f%2f114.55.113.60%3a6969%2fannounce&tr=udp%3a%2f%2f128.199.70.66%3a5944%2fannounce&tr=udp%3a%2f%2f151.80.120.114%3a2710%2fannounce&tr=udp%3a%2f%2f168.235.67.63%3a6969%2fannounce&tr=udp%3a%2f%2f178.33.73.26%3a2710%2fannounce&tr=udp%3a%2f%2f182.176.139.129%3a6969%2fannounce&tr=udp%3a%2f%2f185.5.97.139%3a8089%2fannounce&tr=udp%3a%2f%2f185.86.149.205%3a1337%2fannounce&tr=udp%3a%2f%2f188.165.253.109%3a1337%2fannounce&tr=udp%3a%2f%2f191.101.229.236%3a1337%2fannounce&tr=udp%3a%2f%2f194.106.216.222%3a80%2fannounce&tr=udp%3a%2f%2f195.123.209.37%3a1337%2fannounce&tr=udp%3a%2f%2f195.123.209.40%3a80%2fannounce&tr=udp%3a%2f%2f208.67.16.113%3a8000%2fannounce&tr=udp%3a%2f%2f213.163.67.56%3a1337%2fannounce&tr=udp%3a%2f%2f37.19.5.155%3a2710%2fannounce&tr=udp%3a%2f%2f46.4.109.148%3a6969%2fannounce&tr=udp%3a%2f%2f5.79.249.77%3a6969%2fannounce&tr=udp%3a%2f%2f5.79.83.193%3a6969%2fannounce&tr=udp%3a%2f%2f51.254.244.161%3a6969%2fannounce&tr=udp%3a%2f%2f62.138.0.158%3a6969%2fannounce&tr=udp%3a%2f%2f62.212.85.66%3a2710%2fannounce&tr=udp%3a%2f%2f74.82.52.209%3a6969%2fannounce&tr=udp%3a%2f%2f85.17.19.180%3a80%2fannounce&tr=udp%3a%2f%2f89.234.156.205%3a80%2fannounce&tr=udp%3a%2f%2f9.rarbg.com%3a2710%2fannounce&tr=udp%3a%2f%2f9.rarbg.me%3a2780%2fannounce&tr=udp%3a%2f%2f9.rarbg.to%3a2730%2fannounce&tr=udp%3a%2f%2f91.218.230.81%3a6969%2fannounce&tr=udp%3a%2f%2f94.23.183.33%3a6969%2fannounce&tr=udp%3a%2f%2fbt.xxx-tracker.com%3a2710%2fannounce&tr=udp%3a%2f%2feddie4.nl%3a6969%2fannounce&tr=udp%3a%2f%2fexplodie.org%3a6969%2fannounce&tr=udp%3a%2f%2fmgtracker.org%3a2710%2fannounce&tr=udp%3a%2f%2fp4p.arenabg.com%3a1337%2fannounce&tr=udp%3a%2f%2fshadowshq.eddie4.nl%3a6969%2fannounce&tr=udp%3a%2f%2fshadowshq.yi.org%3a6969%2fannounce&tr=udp%3a%2f%2ftorrent.gresille.org%3a80%2fannounce&tr=udp%3a%2f%2ftracker.aletorrenty.pl%3a2710%2fannounce&tr=udp%3a%2f%2ftracker.bittor.pw%3a1337%2fannounce&tr=udp%3a%2f%2ftracker.coppersurfer.tk%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.eddie4.nl%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.ex.ua%3a80%2fannounce&tr=udp%3a%2f%2ftracker.filetracker.pl%3a8089%2fannounce&tr=udp%3a%2f%2ftracker.flashtorrents.org%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.grepler.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.ilibr.org%3a80%2fannounce&tr=udp%3a%2f%2ftracker.internetwarriors.net%3a1337%2fannounce&tr=udp%3a%2f%2ftracker.kicks-ass.net%3a80%2fannounce&tr=udp%3a%2f%2ftracker.kuroy.me%3a5944%2fannounce&tr=udp%3a%2f%2ftracker.leechers-paradise.org%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.mg64.net%3a2710%2fannounce&tr=udp%3a%2f%2ftracker.mg64.net%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.opentrackr.org%3a1337%2fannounce&tr=udp%3a%2f%2ftracker.piratepublic.com%3a1337%2fannounce&tr=udp%3a%2f%2ftracker.sktorrent.net%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.skyts.net%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.yoshi210.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker2.indowebster.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker4.piratux.com%3a6969%2fannounce&tr=udp%3a%2f%2fzer0day.ch%3a1337%2fannounce&tr=udp%3a%2f%2fzer0day.to%3a1337%2fannounce&tr=udp%3a%2f%2ftracker.cyberia.is%3a6969%2fannounce&tr=http%3a%2f%2fvps02.net.orel.ru%3a80%2fannounce&tr=https%3a%2f%2ftracker.nanoha.org%3a443%2fannounce&tr=http%3a%2f%2ftracker.files.fm%3a6969%2fannounce&tr=https%3a%2f%2ftracker.nitrix.me%3a443%2fannounce&tr=https%3a%2f%2ftracker.tamersunion.org%3a443%2fannounce&tr=udp%3a%2f%2faaa.army%3a8866%2fannounce&tr=https%3a%2f%2ftracker.imgoingto.icu%3a443%2fannounce&tr=udp%3a%2f%2fblokas.io%3a6969%2fannounce&tr=udp%3a%2f%2fdiscord.heihachi.pw%3a6969%2fannounce&tr=udp%3a%2f%2ffe.dealclub.de%3a6969%2fannounce&tr=udp%3a%2f%2fln.mtahost.co%3a6969%2fannounce&tr=udp%3a%2f%2fvibe.community%3a6969%2fannounce&tr=udp%3a%2f%2ftracker0.ufibox.com%3a6969%2fannounce&tr=udp%3a%2f%2fmail.realliferpg.de%3a6969%2fannounce&tr=udp%3a%2f%2fmovies.zsw.ca%3a6969%2fannounce&tr=udp%3a%2f%2fnagios.tks.sumy.ua%3a80%2fannounce&tr=udp%3a%2f%2f47.ip-51-68-199.eu%3a6969%2fannounce&tr=udp%3a%2f%2fcdn-1.gamecoast.org%3a6969%2fannounce&tr=udp%3a%2f%2faruacfilmes.com.br%3a6969%2fannounce&tr=udp%3a%2f%2fedu.uifr.ru%3a6969%2fannounce&tr=http%3a%2f%2frt.tace.ru%3a80%2fannounce&tr=udp%3a%2f%2fcode2chicken.nl%3a6969%2fannounce&tr=udp%3a%2f%2fus-tracker.publictracker.xyz%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.0x.tf%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.altrosky.nl%3a6969%2fannounce&tr=udp%3a%2f%2ftorrentclub.online%3a54123%2fannounce&tr=http%3a%2f%2f5rt.tace.ru%3a60889%2fannounce&tr=udp%3a%2f%2fapp.icon256.com%3a8000%2fannounce&tr=udp%3a%2f%2ftracker.sigterm.xyz%3a6969%2fannounce&tr=http%3a%2f%2ftracker.loadbt.com%3a6969%2fannounce&tr=http%3a%2f%2fipv4announce.sktorrent.eu%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.openbittorrent.com%3a80%2fannounce&tr=udp%3a%2f%2fwww.torrent.eu.org%3a451%2fannounce&tr=udp%3a%2f%2ftracker.dler.org%3a6969%2fannounce&tr=udp%3a%2f%2fexodus.desync.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker2.dler.org%3a80%2fannounce&tr=udp%3a%2f%2ftracker.shkinev.me%3a6969%2fannounce&tr=udp%3a%2f%2fstorage.groupees.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.v6speed.org%3a6969%2fannounce&tr=udp%3a%2f%2fdaveking.com%3a6969%2fannounce&tr=https%3a%2f%2ftracker.lilithraws.cf%3a443%2fannounce&tr=udp%3a%2f%2ftracker1.bt.moack.co.kr%3a80%2fannounce&tr=udp%3a%2f%2f3rt.tace.ru%3a60889%2fannounce&tr=udp%3a%2f%2fjohnrosen1.com%3a6969%2fannounce&tr=udp%3a%2f%2fretracker.lanta-net.ru%3a2710%2fannounce&tr=udp%3a%2f%2fopentor.org%3a2710%2fannounce&tr=udp%3a%2f%2ft2.leech.ie%3a1337%2fannounce&tr=https%3a%2f%2ftracker.foreverpirates.co%3a443%2fannounce&tr=http%3a%2f%2ftracker.vraphim.com%3a6969%2fannounce&tr=udp%3a%2f%2fopen.stealth.si%3a80%2fannounce&tr=udp%3a%2f%2ftracker.uw0.xyz%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.army%3a6969%2fannounce&tr=udp%3a%2f%2fmts.tvbit.co%3a6969%2fannounce&tr=https%3a%2f%2ftracker.coalition.space%3a443%2fannounce&tr=http%3a%2f%2ftracker-cdn.moeking.me%3a2095%2fannounce&tr=udp%3a%2f%2fline-net.ru%3a6969%2fannounce&tr=udp%3a%2f%2fperu.subventas.com%3a53%2fannounce&tr=udp%3a%2f%2fbt1.archive.org%3a6969%2fannounce&tr=udp%3a%2f%2fengplus.ru%3a6969%2fannounce&tr=udp%3a%2f%2fvalakas.rollo.dnsabr.com%3a2710%2fannounce&tr=udp%3a%2f%2fbt2.archive.org%3a6969%2fannounce&tr=udp%3a%2f%2fipv4.tracker.harry.lu%3a80%2fannounce&tr=udp%3a%2f%2ft1.leech.ie%3a1337%2fannounce&tr=http%3a%2f%2fbt.okmp3.ru%3a2710%2fannounce&tr=http%3a%2f%2fcloud.nyap2p.com%3a8080%2fannounce&tr=http%3a%2f%2ft.overflow.biz%3a6969%2fannounce&tr=udp%3a%2f%2ft3.leech.ie%3a1337%2fannounce&tr=udp%3a%2f%2ftracker.tiny-vps.com%3a6969%2fannounce&tr=http%3a%2f%2ftracker.bt4g.com%3a2095%2fannounce&tr=http%3a%2f%2ft.nyaatracker.com%3a80%2fannounce&tr=udp%3a%2f%2fudp-tracker.shittyurl.org%3a6969%2fannounce&tr=https%3a%2f%2f1337.abcvg.info%3a443%2fannounce&tr=https%3a%2f%2fw.wwwww.wtf%3a443%2fannounce&tr=udp%3a%2f%2fbt2.3kb.xyz%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.ds.is%3a6969%2fannounce&tr=udp%3a%2f%2fopentracker.i2p.rocks%3a6969%2fannounce&tr=udp%3a%2f%2fcdn-2.gamecoast.org%3a6969%2fannounce&tr=udp%3a%2f%2fretracker.netbynet.ru%3a2710%2fannounce&tr=udp%3a%2f%2fteamspeak.value-wolf.org%3a6969%2fannounce&tr=udp%3a%2f%2fcutiegirl.ru%3a6969%2fannounce&tr=http%3a%2f%2fh4.trakx.nibba.trade%3a80%2fannounce Hash (for verification of authenticity etc): c8fc9979cc35f7062cd8715aaaff4da475d2fadc Size: 1,013.12GiB Original post below Archiving videos before potential removal from various websites... Send or comment links of videos you need downloaded. Currently going through POV livestreams/replays. NOTE: livestreams/POV are of the utmost importance. AKA Twitch/dlive/Facebook Live, etc. If you find any of these POV angles, please tag me directly in your comment, or PM me. These generally get taken down VERY fast by the livestream website. UPDATE: Thank you to everyone who shared links (and continue to do so). I am noticing that a lot of the content is now duplicates, or variations (crops, lower quality, etc) of the same content. So I have put all the content I have on MEGA. If I have replied to your comment then the content for sure is in this torrent. MEGA: https://mega.nz/folder/30MlkQib#RDOaGzmtFEHkxSYBaJSzVA (this is the prefered way of downloading) (This is sitting in at ~350GB as of Jan 10 3:00 PM EST. Still adding content) UPDATE 2: Link should be working - MEGA contacted me and reinstated the account (and gave premium so I could upload more). I will be uploading more content that I find to the mega account. Still going through the comments, and the 900+ messages I have. Keep posting comments and I will upload them to the MEGA folder. UPDATE 3: \"Bellingcat\" has created a really efficient way to submit media via a Google Spreadsheet. It's not connected to my archive, but I hope to have a merged final copy in the end. UPDATE 4: IF YOU WANT TO UPLOAD A FILE DIRECTLY TO ME: https://mega.nz/megadrop/fgve0WRa880 (no account registration needed) BACKUPS: Recommended backup: u/tweedge : tweedge commits to making sure this mirror does not fall behind 12h behind, though he'll do his best to keep it within 6h Other backups based on the original MEGA from Jan 06 6:30PM - some might've updated, but no idea if/when; check each link, it should say. Or you can message the user: MAGNET from u/SneakyPieBrown as of Jan 08 2021 : magnet:?xt=urn:btih:fc33c9146c81660ee087dbda756746a978c7c104&dn=Trump%20protest%20Jan-08-2021&tr=udp%3a%2f%2ftracker.openbittorrent.com%3a80 u/firstgrow : hosting direct downloads as well u/nuzzles_u_uwu : Is hosting as well u/tweedge : made a direct download link on s3 u/kenkoda : posted a torrent. u/Deifer : Link u/benediktkr : https://mirrors.deadops.de/capitol2021 and https://mirrors.deadops.de/capitol2021.zip See a familiar face in the archive? https://tips.fbi.gov/digitalmedia/aad18481a3e8f02"},
{"Title": "How to Download a Video off of Europeana?", "Author": "u/thxdley", "Content": "Sorry, i dunno if this belongs here but i dont know where else to put it. I'm pretty sure Europeana embeds the videos or something (i have no clue how any of it works) but when i try downloading the video it gives me a \"raw.html\"  which my computer wont download. Is it impossible to get my hands on this? I dont want to screen record it because of background noise/low quality/lag. Below is the link to the video im trying to download on Europeana. https://www.europeana.eu/item/2051918/data_euscreenXL_EUS_0BCE32473CEBA87D2DB36A87B5511130"},
{"Title": "Justin Roiland, co-creator of Rick and Morty, discovers that Dropbox uses content scanners through the deletion of all his data stored on their servers", "Author": "u/_G0D_M0DE_", "Content": "No content"},
{"Title": "yall might appreciate this", "Author": "u/ian9921", "Content": "No content"},
{"Title": "Does anyone know the IBM TS3500 Tape Library (IBM 3592 JC/JY)?", "Author": "u/_c0der", "Content": "Hi all, does anyone know the IBM TS3500 Tape Library with the corresponding IBM 3592 JC/JY type drives and tapes? I can have a full rack for free - where is the catch? Is it worth it? Power and space isn't an issue. Pictures: https://imgur.com/a/mjwmvI6"},
{"Title": "I just built a collapse-ready laptop. What are some must haves to put on it?", "Author": "u/evanMeaney", "Content": "No content"},
{"Title": "Is there info on failure rate of SSD vs spinning HD", "Author": "u/SlothChunks", "Content": "Hi everybody, I am trying to decide whether to buy a larger capacity drive to replace my two old Samsung SSDs that are 1TB and 500gb. When I looked at Amazon I saw that there are many options of new models if standard spinning drives for much less. As far as I know from reading over the years spinning drives are more likely to fail than the spinning disk drives.  But I havenâ€™t ever actually seen the test data to see just how likely or unlikely they are to break compared to SSD. If this drive is going to be used only at home and not carried anywhere, does anyone have any idea just how much more likely it is to fail than an SSD? Bonus: question, are WesternDigital SSDs any better or worse than Samsung? Or are there any established brands are are absolutely better to avoid?"},
{"Title": "Rescue Mission for Sci-Hub and Open Science: We are the library.", "Author": "u/shrine", "Content": "EFF hears the call: \"Itâ€™s Time to Fight for Open Access\" EFF reports: Activists Mobilize to Fight Censorship and Save Open Science \"Continuing the long tradition of internet hacktivism ... redditors are mobilizing to create an uncensorable back-up of Sci-Hub\" The EFF stands with Sci-Hub in the fight for Open Science, a fight for the human right to benefit and share in human scientific advancement. My wholehearted thanks for every seeder who takes part in this rescue mission, and every person who raises their voice in support of Sci-Hub's vision for Open Science. Rescue Mission Links Quick start to rescuing Sci-Hub : Download 1 random torrent (100GB) from the scimag index of torrents with fewer than 12 seeders , open the .torrent file using a BitTorrent client, then leave your client open to upload (seed) the articles to others. You're now part of an un-censorable library archive! Initial success update: The entire Sci-Hub collection has at least 3 seeders: Let's get it to 5. Let's get it to 7! Letâ€™s get it to 10! Letâ€™s get it to 12! Contribute to open source Sci-Hub projects: freereadorg/awesome-libgen Join r/scihub to stay up to date Note: We have no affiliation with Sci-Hub This effort is completely unaffiliated from Sci-Hub, no one is in touch with Sci-Hub, and I don't speak for Sci-Hub in any form. Always refer to sci-hub.do for the latest from Sci-Hub directly. This is a data preservation effort for just the articles, and does not help Sci-Hub directly.  Sci-Hub is not in any further imminent danger than it always has been, and is not at greater risk of being shut-down than before. A Rescue Mission for Sci-Hub and Open Science Elsevier and the USDOJ have declared war against Sci-Hub and open science. The era of Sci-Hub and Alexandra standing alone in this fight must end. We have to take a stand with her. On May 7th, Sci-Hub's Alexandra Elbakyan revealed that the FBI has been wiretapping her accounts for over 2 years. This news comes after Twitter silenced the official Sci_Hub twitter account because Indian academics were organizing on it against Elsevier. Sci-Hub itself is currently frozen and has not downloaded any new articles since December 2020. This rescue mission is focused on seeding the article collection in order to prepare for a potential Sci-Hub shutdown. Alexandra Elbakyan of Sci-Hub, bookwarrior of Library Genesis , Aaron Swartz , and countless unnamed others have fought to free science from the grips of for-profit publishers. Today, they do it working in hiding, alone, without acknowledgment, in fear of imprisonment, and even now wiretapped by the FBI. They sacrifice everything for one vision: Open Science. Why do they do it? They do it so that humble scholars on the other side of the planet can practice medicine, create science, fight for democracy, teach, and learn. People like Alexandra Elbakyan would give up their personal freedom for that one goal: to free knowledge. For that, Elsevier Corp (RELX, market cap: 50 billion) wants to silence her, wants to see her in prison, and wants to shut Sci-Hub down. It's time we sent Elsevier and the USDOJ a clearer message about the fate of Sci-Hub and open science: we are the library, we do not get silenced, we do not shut down our computers, and we are many. Rescue Mission for Sci-Hub If you have been following the story, then you know that this is not our first rescue mission. We protected the Library Genesis book collection We unlocked over 5,000 COVID-19 research articles We successfully petitioned publishers to unlock their COVID-19 paywalls bookwarrior, the founder of Library Genesis, took his library onto the de-centralized and un-censorable IPFS web Next? Make Sci-Hub un-censorable too. Rescue Target A handful of Library Genesis seeders are currently seeding the Sci-Hub torrents. There are 850 scihub torrents , each containing 100,000 scientific articles, to a total of 85 million scientific articles: 77TB. This is the complete Sci-Hub database. We need to protect this. Rescue Team Wave 1: We need 85 datahoarders to store and seed 1TB of articles each, 10 torrents in total. Download 10 random torrents from the scimag index of < 12 seeders , then load the torrents onto your client and seed for as long as you can. The articles are coded by DOI and in zip files. Wave 2: Reach out to 10 good friends to ask them to grab just 1 random torrent (100GB). That's 850 seeders. We are now the library. Final Wave: Development for an open source Sci-Hub. freereadorg/awesome-libgen is a collection of open source achievements based on the Sci-Hub and Library Genesis databases. Open source de-centralization of Sci-Hub is the ultimate goal here, and this begins with the data, but it is going to take years of developer sweat to carry these libraries into the future. Heartfelt thanks to the r/datahoarder and r/seedboxes communities, seedbox.io and NFOrce for your support for previous missions and your love for science."},
{"Title": "Equipment recommendations", "Author": "u/Broke_Bearded_Guy", "Content": "I was looking at buying a storage drawer preferably a 4U 60bay. I've seen some of the newer HGST units available but I've seen some posts about requiring proprietary drives I didn't know if anyone could make a recommendation my only real requirement is 120v compatible"},
{"Title": "MEGATHREAD: Archiving the Capitol Hill Riots", "Author": "u/AdamLynch", "Content": "FINAL UPDATE as of January 31st 5:35PM EST: Thank you to everyone who shared content. The content being submitted now from what I'm seeing is duplicates of older content. I will thus no longer be updating this archive. The MEGA will remain untouched, so use that as you please, but that will likely die one day as there is a bandwidth/transfer limit. I will be uploading the content to Internet Archive, as well as other sources, but until then the torrent magnet that I will be seeding for a little while is listed below - my bandwidth isn't the best so please do seed if you can: Magnet: magnet:?xt=urn:btih:c8fc9979cc35f7062cd8715aaaff4da475d2fadc&dn=Trump%20protest%20Jan%2006%202021&tr=udp%3a%2f%2ftracker.torrent.eu.org%3a451%2fannounce&tr=udp%3a%2f%2fpublic.popcorn-tracker.org%3a6969%2fannounce&tr=http%3a%2f%2f104.28.1.30%3a8080%2fannounce&tr=http%3a%2f%2f104.28.16.69%2fannounce&tr=http%3a%2f%2f107.150.14.110%3a6969%2fannounce&tr=http%3a%2f%2f109.121.134.121%3a1337%2fannounce&tr=http%3a%2f%2f114.55.113.60%3a6969%2fannounce&tr=http%3a%2f%2f125.227.35.196%3a6969%2fannounce&tr=http%3a%2f%2f128.199.70.66%3a5944%2fannounce&tr=http%3a%2f%2f157.7.202.64%3a8080%2fannounce&tr=http%3a%2f%2f158.69.146.212%3a7777%2fannounce&tr=http%3a%2f%2f173.254.204.71%3a1096%2fannounce&tr=http%3a%2f%2f178.175.143.27%2fannounce&tr=http%3a%2f%2f178.33.73.26%3a2710%2fannounce&tr=http%3a%2f%2f182.176.139.129%3a6969%2fannounce&tr=http%3a%2f%2f185.5.97.139%3a8089%2fannounce&tr=http%3a%2f%2f188.165.253.109%3a1337%2fannounce&tr=http%3a%2f%2f194.106.216.222%2fannounce&tr=http%3a%2f%2f195.123.209.37%3a1337%2fannounce&tr=http%3a%2f%2f210.244.71.25%3a6969%2fannounce&tr=http%3a%2f%2f210.244.71.26%3a6969%2fannounce&tr=http%3a%2f%2f213.159.215.198%3a6970%2fannounce&tr=http%3a%2f%2f213.163.67.56%3a1337%2fannounce&tr=http%3a%2f%2f37.19.5.139%3a6969%2fannounce&tr=http%3a%2f%2f37.19.5.155%3a6881%2fannounce&tr=http%3a%2f%2f46.4.109.148%3a6969%2fannounce&tr=http%3a%2f%2f5.79.249.77%3a6969%2fannounce&tr=http%3a%2f%2f5.79.83.193%3a2710%2fannounce&tr=http%3a%2f%2f51.254.244.161%3a6969%2fannounce&tr=http%3a%2f%2f59.36.96.77%3a6969%2fannounce&tr=http%3a%2f%2f74.82.52.209%3a6969%2fannounce&tr=http%3a%2f%2f80.246.243.18%3a6969%2fannounce&tr=http%3a%2f%2f81.200.2.231%2fannounce&tr=http%3a%2f%2f85.17.19.180%2fannounce&tr=http%3a%2f%2f87.248.186.252%3a8080%2fannounce&tr=http%3a%2f%2f87.253.152.137%2fannounce&tr=http%3a%2f%2f91.216.110.47%2fannounce&tr=http%3a%2f%2f91.217.91.21%3a3218%2fannounce&tr=http%3a%2f%2f91.218.230.81%3a6969%2fannounce&tr=http%3a%2f%2f93.92.64.5%2fannounce&tr=http%3a%2f%2fatrack.pow7.com%2fannounce&tr=http%3a%2f%2fbt.henbt.com%3a2710%2fannounce&tr=http%3a%2f%2fbt.pusacg.org%3a8080%2fannounce&tr=http%3a%2f%2fbt2.careland.com.cn%3a6969%2fannounce&tr=http%3a%2f%2fexplodie.org%3a6969%2fannounce&tr=http%3a%2f%2fmgtracker.org%3a2710%2fannounce&tr=http%3a%2f%2fmgtracker.org%3a6969%2fannounce&tr=http%3a%2f%2fopen.acgtracker.com%3a1096%2fannounce&tr=http%3a%2f%2fopen.lolicon.eu%3a7777%2fannounce&tr=http%3a%2f%2fopen.touki.ru%2fannounce.php&tr=http%3a%2f%2fp4p.arenabg.ch%3a1337%2fannounce&tr=http%3a%2f%2fp4p.arenabg.com%3a1337%2fannounce&tr=http%3a%2f%2fpow7.com%3a80%2fannounce&tr=http%3a%2f%2fretracker.gorcomnet.ru%2fannounce&tr=http%3a%2f%2fretracker.krs-ix.ru%2fannounce&tr=http%3a%2f%2fretracker.krs-ix.ru%3a80%2fannounce&tr=http%3a%2f%2fsecure.pow7.com%2fannounce&tr=http%3a%2f%2ft1.pow7.com%2fannounce&tr=http%3a%2f%2ft2.pow7.com%2fannounce&tr=http%3a%2f%2fthetracker.org%3a80%2fannounce&tr=http%3a%2f%2ftorrent.gresille.org%2fannounce&tr=http%3a%2f%2ftorrentsmd.com%3a8080%2fannounce&tr=http%3a%2f%2ftracker.aletorrenty.pl%3a2710%2fannounce&tr=http%3a%2f%2ftracker.baravik.org%3a6970%2fannounce&tr=http%3a%2f%2ftracker.bittor.pw%3a1337%2fannounce&tr=http%3a%2f%2ftracker.bittorrent.am%2fannounce&tr=http%3a%2f%2ftracker.calculate.ru%3a6969%2fannounce&tr=http%3a%2f%2ftracker.dler.org%3a6969%2fannounce&tr=http%3a%2f%2ftracker.dutchtracking.com%2fannounce&tr=http%3a%2f%2ftracker.dutchtracking.com%3a80%2fannounce&tr=http%3a%2f%2ftracker.dutchtracking.nl%2fannounce&tr=http%3a%2f%2ftracker.dutchtracking.nl%3a80%2fannounce&tr=http%3a%2f%2ftracker.edoardocolombo.eu%3a6969%2fannounce&tr=http%3a%2f%2ftracker.ex.ua%2fannounce&tr=http%3a%2f%2ftracker.ex.ua%3a80%2fannounce&tr=http%3a%2f%2ftracker.filetracker.pl%3a8089%2fannounce&tr=http%3a%2f%2ftracker.flashtorrents.org%3a6969%2fannounce&tr=http%3a%2f%2ftracker.grepler.com%3a6969%2fannounce&tr=http%3a%2f%2ftracker.internetwarriors.net%3a1337%2fannounce&tr=http%3a%2f%2ftracker.kicks-ass.net%2fannounce&tr=http%3a%2f%2ftracker.kicks-ass.net%3a80%2fannounce&tr=http%3a%2f%2ftracker.kuroy.me%3a5944%2fannounce&tr=http%3a%2f%2ftracker.mg64.net%3a6881%2fannounce&tr=http%3a%2f%2ftracker.opentrackr.org%3a1337%2fannounce&tr=http%3a%2f%2ftracker.skyts.net%3a6969%2fannounce&tr=http%3a%2f%2ftracker.tfile.me%2fannounce&tr=http%3a%2f%2ftracker.tiny-vps.com%3a6969%2fannounce&tr=http%3a%2f%2ftracker.tvunderground.org.ru%3a3218%2fannounce&tr=http%3a%2f%2ftracker.yoshi210.com%3a6969%2fannounce&tr=http%3a%2f%2ftracker1.wasabii.com.tw%3a6969%2fannounce&tr=http%3a%2f%2ftracker2.itzmx.com%3a6961%2fannounce&tr=http%3a%2f%2ftracker2.wasabii.com.tw%3a6969%2fannounce&tr=http%3a%2f%2fwww.wareztorrent.com%2fannounce&tr=http%3a%2f%2fwww.wareztorrent.com%3a80%2fannounce&tr=https%3a%2f%2f104.28.17.69%2fannounce&tr=https%3a%2f%2fwww.wareztorrent.com%2fannounce&tr=udp%3a%2f%2f107.150.14.110%3a6969%2fannounce&tr=udp%3a%2f%2f109.121.134.121%3a1337%2fannounce&tr=udp%3a%2f%2f114.55.113.60%3a6969%2fannounce&tr=udp%3a%2f%2f128.199.70.66%3a5944%2fannounce&tr=udp%3a%2f%2f151.80.120.114%3a2710%2fannounce&tr=udp%3a%2f%2f168.235.67.63%3a6969%2fannounce&tr=udp%3a%2f%2f178.33.73.26%3a2710%2fannounce&tr=udp%3a%2f%2f182.176.139.129%3a6969%2fannounce&tr=udp%3a%2f%2f185.5.97.139%3a8089%2fannounce&tr=udp%3a%2f%2f185.86.149.205%3a1337%2fannounce&tr=udp%3a%2f%2f188.165.253.109%3a1337%2fannounce&tr=udp%3a%2f%2f191.101.229.236%3a1337%2fannounce&tr=udp%3a%2f%2f194.106.216.222%3a80%2fannounce&tr=udp%3a%2f%2f195.123.209.37%3a1337%2fannounce&tr=udp%3a%2f%2f195.123.209.40%3a80%2fannounce&tr=udp%3a%2f%2f208.67.16.113%3a8000%2fannounce&tr=udp%3a%2f%2f213.163.67.56%3a1337%2fannounce&tr=udp%3a%2f%2f37.19.5.155%3a2710%2fannounce&tr=udp%3a%2f%2f46.4.109.148%3a6969%2fannounce&tr=udp%3a%2f%2f5.79.249.77%3a6969%2fannounce&tr=udp%3a%2f%2f5.79.83.193%3a6969%2fannounce&tr=udp%3a%2f%2f51.254.244.161%3a6969%2fannounce&tr=udp%3a%2f%2f62.138.0.158%3a6969%2fannounce&tr=udp%3a%2f%2f62.212.85.66%3a2710%2fannounce&tr=udp%3a%2f%2f74.82.52.209%3a6969%2fannounce&tr=udp%3a%2f%2f85.17.19.180%3a80%2fannounce&tr=udp%3a%2f%2f89.234.156.205%3a80%2fannounce&tr=udp%3a%2f%2f9.rarbg.com%3a2710%2fannounce&tr=udp%3a%2f%2f9.rarbg.me%3a2780%2fannounce&tr=udp%3a%2f%2f9.rarbg.to%3a2730%2fannounce&tr=udp%3a%2f%2f91.218.230.81%3a6969%2fannounce&tr=udp%3a%2f%2f94.23.183.33%3a6969%2fannounce&tr=udp%3a%2f%2fbt.xxx-tracker.com%3a2710%2fannounce&tr=udp%3a%2f%2feddie4.nl%3a6969%2fannounce&tr=udp%3a%2f%2fexplodie.org%3a6969%2fannounce&tr=udp%3a%2f%2fmgtracker.org%3a2710%2fannounce&tr=udp%3a%2f%2fp4p.arenabg.com%3a1337%2fannounce&tr=udp%3a%2f%2fshadowshq.eddie4.nl%3a6969%2fannounce&tr=udp%3a%2f%2fshadowshq.yi.org%3a6969%2fannounce&tr=udp%3a%2f%2ftorrent.gresille.org%3a80%2fannounce&tr=udp%3a%2f%2ftracker.aletorrenty.pl%3a2710%2fannounce&tr=udp%3a%2f%2ftracker.bittor.pw%3a1337%2fannounce&tr=udp%3a%2f%2ftracker.coppersurfer.tk%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.eddie4.nl%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.ex.ua%3a80%2fannounce&tr=udp%3a%2f%2ftracker.filetracker.pl%3a8089%2fannounce&tr=udp%3a%2f%2ftracker.flashtorrents.org%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.grepler.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.ilibr.org%3a80%2fannounce&tr=udp%3a%2f%2ftracker.internetwarriors.net%3a1337%2fannounce&tr=udp%3a%2f%2ftracker.kicks-ass.net%3a80%2fannounce&tr=udp%3a%2f%2ftracker.kuroy.me%3a5944%2fannounce&tr=udp%3a%2f%2ftracker.leechers-paradise.org%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.mg64.net%3a2710%2fannounce&tr=udp%3a%2f%2ftracker.mg64.net%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.opentrackr.org%3a1337%2fannounce&tr=udp%3a%2f%2ftracker.piratepublic.com%3a1337%2fannounce&tr=udp%3a%2f%2ftracker.sktorrent.net%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.skyts.net%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.yoshi210.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker2.indowebster.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker4.piratux.com%3a6969%2fannounce&tr=udp%3a%2f%2fzer0day.ch%3a1337%2fannounce&tr=udp%3a%2f%2fzer0day.to%3a1337%2fannounce&tr=udp%3a%2f%2ftracker.cyberia.is%3a6969%2fannounce&tr=http%3a%2f%2fvps02.net.orel.ru%3a80%2fannounce&tr=https%3a%2f%2ftracker.nanoha.org%3a443%2fannounce&tr=http%3a%2f%2ftracker.files.fm%3a6969%2fannounce&tr=https%3a%2f%2ftracker.nitrix.me%3a443%2fannounce&tr=https%3a%2f%2ftracker.tamersunion.org%3a443%2fannounce&tr=udp%3a%2f%2faaa.army%3a8866%2fannounce&tr=https%3a%2f%2ftracker.imgoingto.icu%3a443%2fannounce&tr=udp%3a%2f%2fblokas.io%3a6969%2fannounce&tr=udp%3a%2f%2fdiscord.heihachi.pw%3a6969%2fannounce&tr=udp%3a%2f%2ffe.dealclub.de%3a6969%2fannounce&tr=udp%3a%2f%2fln.mtahost.co%3a6969%2fannounce&tr=udp%3a%2f%2fvibe.community%3a6969%2fannounce&tr=udp%3a%2f%2ftracker0.ufibox.com%3a6969%2fannounce&tr=udp%3a%2f%2fmail.realliferpg.de%3a6969%2fannounce&tr=udp%3a%2f%2fmovies.zsw.ca%3a6969%2fannounce&tr=udp%3a%2f%2fnagios.tks.sumy.ua%3a80%2fannounce&tr=udp%3a%2f%2f47.ip-51-68-199.eu%3a6969%2fannounce&tr=udp%3a%2f%2fcdn-1.gamecoast.org%3a6969%2fannounce&tr=udp%3a%2f%2faruacfilmes.com.br%3a6969%2fannounce&tr=udp%3a%2f%2fedu.uifr.ru%3a6969%2fannounce&tr=http%3a%2f%2frt.tace.ru%3a80%2fannounce&tr=udp%3a%2f%2fcode2chicken.nl%3a6969%2fannounce&tr=udp%3a%2f%2fus-tracker.publictracker.xyz%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.0x.tf%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.altrosky.nl%3a6969%2fannounce&tr=udp%3a%2f%2ftorrentclub.online%3a54123%2fannounce&tr=http%3a%2f%2f5rt.tace.ru%3a60889%2fannounce&tr=udp%3a%2f%2fapp.icon256.com%3a8000%2fannounce&tr=udp%3a%2f%2ftracker.sigterm.xyz%3a6969%2fannounce&tr=http%3a%2f%2ftracker.loadbt.com%3a6969%2fannounce&tr=http%3a%2f%2fipv4announce.sktorrent.eu%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.openbittorrent.com%3a80%2fannounce&tr=udp%3a%2f%2fwww.torrent.eu.org%3a451%2fannounce&tr=udp%3a%2f%2ftracker.dler.org%3a6969%2fannounce&tr=udp%3a%2f%2fexodus.desync.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker2.dler.org%3a80%2fannounce&tr=udp%3a%2f%2ftracker.shkinev.me%3a6969%2fannounce&tr=udp%3a%2f%2fstorage.groupees.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.v6speed.org%3a6969%2fannounce&tr=udp%3a%2f%2fdaveking.com%3a6969%2fannounce&tr=https%3a%2f%2ftracker.lilithraws.cf%3a443%2fannounce&tr=udp%3a%2f%2ftracker1.bt.moack.co.kr%3a80%2fannounce&tr=udp%3a%2f%2f3rt.tace.ru%3a60889%2fannounce&tr=udp%3a%2f%2fjohnrosen1.com%3a6969%2fannounce&tr=udp%3a%2f%2fretracker.lanta-net.ru%3a2710%2fannounce&tr=udp%3a%2f%2fopentor.org%3a2710%2fannounce&tr=udp%3a%2f%2ft2.leech.ie%3a1337%2fannounce&tr=https%3a%2f%2ftracker.foreverpirates.co%3a443%2fannounce&tr=http%3a%2f%2ftracker.vraphim.com%3a6969%2fannounce&tr=udp%3a%2f%2fopen.stealth.si%3a80%2fannounce&tr=udp%3a%2f%2ftracker.uw0.xyz%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.army%3a6969%2fannounce&tr=udp%3a%2f%2fmts.tvbit.co%3a6969%2fannounce&tr=https%3a%2f%2ftracker.coalition.space%3a443%2fannounce&tr=http%3a%2f%2ftracker-cdn.moeking.me%3a2095%2fannounce&tr=udp%3a%2f%2fline-net.ru%3a6969%2fannounce&tr=udp%3a%2f%2fperu.subventas.com%3a53%2fannounce&tr=udp%3a%2f%2fbt1.archive.org%3a6969%2fannounce&tr=udp%3a%2f%2fengplus.ru%3a6969%2fannounce&tr=udp%3a%2f%2fvalakas.rollo.dnsabr.com%3a2710%2fannounce&tr=udp%3a%2f%2fbt2.archive.org%3a6969%2fannounce&tr=udp%3a%2f%2fipv4.tracker.harry.lu%3a80%2fannounce&tr=udp%3a%2f%2ft1.leech.ie%3a1337%2fannounce&tr=http%3a%2f%2fbt.okmp3.ru%3a2710%2fannounce&tr=http%3a%2f%2fcloud.nyap2p.com%3a8080%2fannounce&tr=http%3a%2f%2ft.overflow.biz%3a6969%2fannounce&tr=udp%3a%2f%2ft3.leech.ie%3a1337%2fannounce&tr=udp%3a%2f%2ftracker.tiny-vps.com%3a6969%2fannounce&tr=http%3a%2f%2ftracker.bt4g.com%3a2095%2fannounce&tr=http%3a%2f%2ft.nyaatracker.com%3a80%2fannounce&tr=udp%3a%2f%2fudp-tracker.shittyurl.org%3a6969%2fannounce&tr=https%3a%2f%2f1337.abcvg.info%3a443%2fannounce&tr=https%3a%2f%2fw.wwwww.wtf%3a443%2fannounce&tr=udp%3a%2f%2fbt2.3kb.xyz%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.ds.is%3a6969%2fannounce&tr=udp%3a%2f%2fopentracker.i2p.rocks%3a6969%2fannounce&tr=udp%3a%2f%2fcdn-2.gamecoast.org%3a6969%2fannounce&tr=udp%3a%2f%2fretracker.netbynet.ru%3a2710%2fannounce&tr=udp%3a%2f%2fteamspeak.value-wolf.org%3a6969%2fannounce&tr=udp%3a%2f%2fcutiegirl.ru%3a6969%2fannounce&tr=http%3a%2f%2fh4.trakx.nibba.trade%3a80%2fannounce Hash (for verification of authenticity etc): c8fc9979cc35f7062cd8715aaaff4da475d2fadc Size: 1,013.12GiB Original post below Archiving videos before potential removal from various websites... Send or comment links of videos you need downloaded. Currently going through POV livestreams/replays. NOTE: livestreams/POV are of the utmost importance. AKA Twitch/dlive/Facebook Live, etc. If you find any of these POV angles, please tag me directly in your comment, or PM me. These generally get taken down VERY fast by the livestream website. UPDATE: Thank you to everyone who shared links (and continue to do so). I am noticing that a lot of the content is now duplicates, or variations (crops, lower quality, etc) of the same content. So I have put all the content I have on MEGA. If I have replied to your comment then the content for sure is in this torrent. MEGA: https://mega.nz/folder/30MlkQib#RDOaGzmtFEHkxSYBaJSzVA (this is the prefered way of downloading) (This is sitting in at ~350GB as of Jan 10 3:00 PM EST. Still adding content) UPDATE 2: Link should be working - MEGA contacted me and reinstated the account (and gave premium so I could upload more). I will be uploading more content that I find to the mega account. Still going through the comments, and the 900+ messages I have. Keep posting comments and I will upload them to the MEGA folder. UPDATE 3: \"Bellingcat\" has created a really efficient way to submit media via a Google Spreadsheet. It's not connected to my archive, but I hope to have a merged final copy in the end. UPDATE 4: IF YOU WANT TO UPLOAD A FILE DIRECTLY TO ME: https://mega.nz/megadrop/fgve0WRa880 (no account registration needed) BACKUPS: Recommended backup: u/tweedge : tweedge commits to making sure this mirror does not fall behind 12h behind, though he'll do his best to keep it within 6h Other backups based on the original MEGA from Jan 06 6:30PM - some might've updated, but no idea if/when; check each link, it should say. Or you can message the user: MAGNET from u/SneakyPieBrown as of Jan 08 2021 : magnet:?xt=urn:btih:fc33c9146c81660ee087dbda756746a978c7c104&dn=Trump%20protest%20Jan-08-2021&tr=udp%3a%2f%2ftracker.openbittorrent.com%3a80 u/firstgrow : hosting direct downloads as well u/nuzzles_u_uwu : Is hosting as well u/tweedge : made a direct download link on s3 u/kenkoda : posted a torrent. u/Deifer : Link u/benediktkr : https://mirrors.deadops.de/capitol2021 and https://mirrors.deadops.de/capitol2021.zip See a familiar face in the archive? https://tips.fbi.gov/digitalmedia/aad18481a3e8f02"},
{"Title": "Data transfer to new Lustre storage overwhelms campus network", "Author": "u/e_spider", "Content": "No content"},
{"Title": "just wondering if others with IDM also have this problem", "Author": "u/Street_Mine_1969", "Content": "for the last 3 days my IDM pop up download bar does not show on youtube and youtube only. other other video site is still okay and can download without problem. anyone else with IDM have this problem with youtube?"},
{"Title": "Justin Roiland, co-creator of Rick and Morty, discovers that Dropbox uses content scanners through the deletion of all his data stored on their servers", "Author": "u/_G0D_M0DE_", "Content": "No content"},
{"Title": "Whoops", "Author": "u/MidnightLink", "Content": "No content"},
{"Title": "how do you batch select on czkawka?", "Author": "u/randomusername11222", "Content": "there is this select windows https://preview.redd.it/how-do-you-batch-select-on-czkawka-v0-u0bmvpoaim6d1.png But I cant like multiselect the files that I want manually?"},
{"Title": "yall might appreciate this", "Author": "u/ian9921", "Content": "No content"},
{"Title": "A funny exchange", "Author": "Unknown author", "Content": "No content"},
{"Title": "Which SATA Ssd should I buy in 2024?", "Author": "u/eliosferre3", "Content": "Hi! I need it to be a sata ssd, I recently bought a Crucial MX500 which failed while I was installing Windows and wasn't able to detect again. Now I'm thinking on a 1TB Samsung 870 Evo, but reading some posts in this subreddit apparently it has a high failure rate as well? I thought Samsung would be reliable but I don't know what to expect or buy anymore."},
{"Title": "I just built a collapse-ready laptop. What are some must haves to put on it?", "Author": "u/evanMeaney", "Content": "No content"},
{"Title": "One woman's quest to \"never delete anything\" allowed internet archivists to find long-lost Minecraft Alpha 1.1.1.", "Author": "u/BrikenEnglz", "Content": "No content"},
{"Title": "Recommended temporary external drives?", "Author": "u/DisasterSpinach", "Content": "I might not be asking in the right place because my goal isn't to set up something that is highly systematic or robust at this time. The place I'm renting has serious roof issues and a bunch of my stuff was water damaged. I'm trying to salvage what I can to some portable drives and get the most important belongings out as soon as I can. In the past I remember Easystores/WD Elements being popular here, but apparently they have different drives inside now or high failure rates or something? And then I was just going to go with whatever Costco carried, but at this time that's only the doomed Sandisk SSDs. Anyone have any recommendations? Doesn't have to be perfect, just reasonably priced. Just looking to buy two drives for about $300-500 total. I guess HDDs preferred if they are decently reliable and not SMR. EDIT: OK externals are all apparently crap? https://www.reddit.com/r/DataHoarder/comments/146hb9k/information_about_cmr_to_smr_manufacturer/ Maybe I'll just roll the dice on those Sandisk SSDs.."},
{"Title": "Rescue Mission for Sci-Hub and Open Science: We are the library.", "Author": "u/shrine", "Content": "EFF hears the call: \"Itâ€™s Time to Fight for Open Access\" EFF reports: Activists Mobilize to Fight Censorship and Save Open Science \"Continuing the long tradition of internet hacktivism ... redditors are mobilizing to create an uncensorable back-up of Sci-Hub\" The EFF stands with Sci-Hub in the fight for Open Science, a fight for the human right to benefit and share in human scientific advancement. My wholehearted thanks for every seeder who takes part in this rescue mission, and every person who raises their voice in support of Sci-Hub's vision for Open Science. Rescue Mission Links Quick start to rescuing Sci-Hub : Download 1 random torrent (100GB) from the scimag index of torrents with fewer than 12 seeders , open the .torrent file using a BitTorrent client, then leave your client open to upload (seed) the articles to others. You're now part of an un-censorable library archive! Initial success update: The entire Sci-Hub collection has at least 3 seeders: Let's get it to 5. Let's get it to 7! Letâ€™s get it to 10! Letâ€™s get it to 12! Contribute to open source Sci-Hub projects: freereadorg/awesome-libgen Join r/scihub to stay up to date Note: We have no affiliation with Sci-Hub This effort is completely unaffiliated from Sci-Hub, no one is in touch with Sci-Hub, and I don't speak for Sci-Hub in any form. Always refer to sci-hub.do for the latest from Sci-Hub directly. This is a data preservation effort for just the articles, and does not help Sci-Hub directly.  Sci-Hub is not in any further imminent danger than it always has been, and is not at greater risk of being shut-down than before. A Rescue Mission for Sci-Hub and Open Science Elsevier and the USDOJ have declared war against Sci-Hub and open science. The era of Sci-Hub and Alexandra standing alone in this fight must end. We have to take a stand with her. On May 7th, Sci-Hub's Alexandra Elbakyan revealed that the FBI has been wiretapping her accounts for over 2 years. This news comes after Twitter silenced the official Sci_Hub twitter account because Indian academics were organizing on it against Elsevier. Sci-Hub itself is currently frozen and has not downloaded any new articles since December 2020. This rescue mission is focused on seeding the article collection in order to prepare for a potential Sci-Hub shutdown. Alexandra Elbakyan of Sci-Hub, bookwarrior of Library Genesis , Aaron Swartz , and countless unnamed others have fought to free science from the grips of for-profit publishers. Today, they do it working in hiding, alone, without acknowledgment, in fear of imprisonment, and even now wiretapped by the FBI. They sacrifice everything for one vision: Open Science. Why do they do it? They do it so that humble scholars on the other side of the planet can practice medicine, create science, fight for democracy, teach, and learn. People like Alexandra Elbakyan would give up their personal freedom for that one goal: to free knowledge. For that, Elsevier Corp (RELX, market cap: 50 billion) wants to silence her, wants to see her in prison, and wants to shut Sci-Hub down. It's time we sent Elsevier and the USDOJ a clearer message about the fate of Sci-Hub and open science: we are the library, we do not get silenced, we do not shut down our computers, and we are many. Rescue Mission for Sci-Hub If you have been following the story, then you know that this is not our first rescue mission. We protected the Library Genesis book collection We unlocked over 5,000 COVID-19 research articles We successfully petitioned publishers to unlock their COVID-19 paywalls bookwarrior, the founder of Library Genesis, took his library onto the de-centralized and un-censorable IPFS web Next? Make Sci-Hub un-censorable too. Rescue Target A handful of Library Genesis seeders are currently seeding the Sci-Hub torrents. There are 850 scihub torrents , each containing 100,000 scientific articles, to a total of 85 million scientific articles: 77TB. This is the complete Sci-Hub database. We need to protect this. Rescue Team Wave 1: We need 85 datahoarders to store and seed 1TB of articles each, 10 torrents in total. Download 10 random torrents from the scimag index of < 12 seeders , then load the torrents onto your client and seed for as long as you can. The articles are coded by DOI and in zip files. Wave 2: Reach out to 10 good friends to ask them to grab just 1 random torrent (100GB). That's 850 seeders. We are now the library. Final Wave: Development for an open source Sci-Hub. freereadorg/awesome-libgen is a collection of open source achievements based on the Sci-Hub and Library Genesis databases. Open source de-centralization of Sci-Hub is the ultimate goal here, and this begins with the data, but it is going to take years of developer sweat to carry these libraries into the future. Heartfelt thanks to the r/datahoarder and r/seedboxes communities, seedbox.io and NFOrce for your support for previous missions and your love for science."},
{"Title": "Amazon delivery driver with my new HD", "Author": "u/fancy_pantser", "Content": "No content"},
{"Title": "Has anyone else ever experienced this? 20TB Exos white label results in my ssd activity spiking to 100% and boot issues", "Author": "u/good4y0u", "Content": "I recently purchased a 20TB Exos white label drive from eBay, which was advertised as a Seagate refurbished drive. The serial numbers do trace back to Seagate, but I'm experiencing some unusual issues with it. Has anyone else encountered this? Here's what's happening when I test it with my Windows machine: With the machine powered off, I plug in the drive, but the machine doesn't boot. With the machine powered off, I boot to BIOS, and the disk is found. However, after rebooting, the machine doesn't boot. If I unplug the drive after step 2, the machine boots normally. If I plug the drive back in at the login screen, the machine freezes. If I log in first and then plug the drive in, I can see the task manager showing max activity on my main OS SSD before the machine freezes. Unplugging the drive at this point unfreezes the machine. I've bought a number of drives this way before and never had a serious issue. Any insights or suggestions would be greatly appreciated. In the meantime I am reaching out to the seller. The other drive I got seems to work fine."},
{"Title": "Apple sued for terminating account with $25,000 worth of apps and videos", "Author": "u/usernamechosen999", "Content": "No content"},
{"Title": "Data transfer to new Lustre storage overwhelms campus network", "Author": "u/e_spider", "Content": "No content"},
{"Title": "Modular NAS build", "Author": "u/Mother_Occasion_8076", "Content": "No content"},
{"Title": "Some of the drives used to store the data to to compile the first picture ever of a black hole.", "Author": "u/the_best_moshe", "Content": "No content"},
{"Title": "Whoops", "Author": "u/MidnightLink", "Content": "No content"},
{"Title": "I finally migrated off my old Drobo! What's the bottom line wit SoftRaid? Do I need to pay for the premium license?", "Author": "u/Splitsurround", "Content": "So against ALL odds I did get all my data safely off a dying Drobo, onto a High Point 4 bay enclosure with four 16Tb drives. I made it Raid 5 using Softeraid's trial, and so far, so good. I'm trying to wrap my head around paying for the SoftRaid premium or not: I've read all the FAQs and as far as I can tell, the only things I can't do without premium is create new raids and use their monitoring feature. I understand about the new volume thing but...if I don't upgrade to premium, since I won't be able to use their monitoring, how will I know when a disk is dying or is dead?"},
{"Title": "Archivists Are Preserving Capitol Hill Riot Livestreams Before Theyâ€™re Deleted", "Author": "u/fairyrocker91", "Content": "No content"},
{"Title": "A funny exchange", "Author": "Unknown author", "Content": "No content"},
{"Title": "Data management approach", "Author": "u/nlj1978", "Content": "Narrowing down my approach and looking for some additional guidance. Complete novice to home server operation.  After discussion in another thread, I've decided to not go with Windows OS on the server and instead run a Linux based OS so some sort. My hardware is a i5-9500 with 8gb or ram. In terms of storage I have a 256gb m2 NVME, 2x 10tb HDD and a 256g 2.5\" SSD if it is useful in some way. This leaves me with 2 sata3 ports(1 if I use the 2.5\" SSD) and 1 m2 slot available for further expansion of storage. Initial intended uses are to run JellyFin media server, Home Assistant and an image management app for pics, all local. I would like full redundancy via raid1, this is where I'm a bit uncertain on approach. I assume I'll run the OS and any apps on the NVME, not the HDD. Since i have the additional NVME slot available is there benefit to adding a second NVME drive for cache or even a mirror of the OS and apps? Should I be looking to manage my raid with an application of some sort or in the Debian OS itself? If the raid is setup in Debian, how does that affect future expansion? Do i have to add pairs of drives or could I for example just add a 18tb random drive? If for example a year from now I add 2 additional drives, at that time would there be a better solution than raid1? Thanks for your time."},
{"Title": "Twitter to purge accounts that have had no activity at all for several years", "Author": "u/NXGZ", "Content": "No content"},
{"Title": "One woman's quest to \"never delete anything\" allowed internet archivists to find long-lost Minecraft Alpha 1.1.1.", "Author": "u/BrikenEnglz", "Content": "No content"},
{"Title": "PlutoTV - Downloading episodes from live channel (ondemand not available)", "Author": "u/pstNN", "Content": "Hi all, I've been waiting for this show to be released online for ages and it just released this week, however, there doesn't seem to be any plans to make it on demand, which means I cant use StreamFab. Does anyone know of a way to download episodes without the ads/commercials from a live stream on PlutoTV france? This is the link. If there is a paid software for it, i have no problem buying it. https://pluto.tv/fr/live-tv/662a622b5e2370000800f392 Thanks!"},
{"Title": "I can dream", "Author": "u/gammajayy", "Content": "No content"},
{"Title": "Amazon delivery driver with my new HD", "Author": "u/fancy_pantser", "Content": "No content"},
{"Title": "Donating space", "Author": "u/noideawhatimdoing444", "Content": "I'm not at the place where I can donate space on my server but eventually I will be. I'd like to give support to pages like Wikipedia or any other project I feel aligns with my values. Is that something thats even feasible?"},
{"Title": "This is your regular reminder that Comcast is still a dumpster fire: Comcast to impose home internet data cap of 1.2TB in more than a dozen US states next year", "Author": "u/Snoot_Boopins", "Content": "No content"},
{"Title": "Apple sued for terminating account with $25,000 worth of apps and videos", "Author": "u/usernamechosen999", "Content": "No content"},
{"Title": "Couple question to get started", "Author": "u/EvenRD", "Content": "Hello, i tried checking in the wiki or in FAQs but i couldn't find anything of use. I am considering starting hoarding files on physical drives since my online cloud is full and i don't want to pay a monthly fee. A couple questions: Should i? I need space to save personal files, photos and videos. Right now i think i could have all my storage done with 1TB, but for the sake of futereproofing and redundancy i think i should aim for 4TB. I still don't know if its convenient or should i just suck it up and pay online cloud services Do i need a NAS? I do all my work from my main computer so what i am basically thinking is just to add a RAID 5 HDD to my existing setup and call it a day, is there some reason i should NOT do? Since HDD tend to get noisy i'd like to shut them off when they are not needed, this way i can save energy as well"},
{"Title": "\"If you visit CERN in Geneva, you can buy 1 Terabyte of Large Hadron Collider data from the souvenir shop\"... there goes my kid's college funds", "Author": "Unknown author", "Content": "No content"},
{"Title": "Some of the drives used to store the data to to compile the first picture ever of a black hole.", "Author": "u/the_best_moshe", "Content": "No content"},
{"Title": "Twitter extension to download videos inside the page", "Author": "u/findthatgayporn", "Content": "Anyone knows a good one? I have two installed but have to open a new tab for each video I want to download; I need a extension to click and start downloading"},
{"Title": "yahoo answers is shutting down", "Author": "u/Fazlul101", "Content": "No content"},
{"Title": "Archivists Are Preserving Capitol Hill Riot Livestreams Before Theyâ€™re Deleted", "Author": "u/fairyrocker91", "Content": "No content"},
{"Title": "Sending large files online solution?", "Author": "u/backflipbadboy", "Content": "I found the website https://tempfile.me who are claiming privacy + encryption when uploading large files online up to 10GB per file has anyone used such a service?"},
{"Title": "internet archive is being sued", "Author": "u/sersoniko", "Content": "No content"},
{"Title": "Twitter to purge accounts that have had no activity at all for several years", "Author": "u/NXGZ", "Content": "No content"},
{"Title": "G-technology 6tb fair used price? Reliability for Family Backup?", "Author": "u/myfacenotmyaccount", "Content": "Some guy near me has a couple of these for $90 each, I was thinking to buy a couple to back up all my families data and pictures, wondering if it was a good idea? https://www.amazon.com/G-Technology-Thunderbolt-High-Performance-Solution-0G04023/dp/B00QJJ5362?th=1"},
{"Title": "Forklift accident", "Author": "u/Enkelie", "Content": "No content"},
{"Title": "I can dream", "Author": "u/gammajayy", "Content": "No content"},
{"Title": "New HDD test sequence", "Author": "u/sobo5o", "Content": "Got another 5TB external Seagate Expansion HDD and want to optimize my routine. The drive is originally in exFAT with some warranty content on it. I have Windows 10, so not using badblocks, but have HD Sentinel. My order is this: Minimum: Short Long self-test (which will do an initial READ) Surface WRITE+Read with default 0's Quick format (to NTFS for use) Extensive: Short self-test Quick format (to NTFS, solely for the next step) Filling disk with large files ( using TeraCopy verify+test using H2TestW) Surface READ test Surface WRITE+Read with default 0's Surface WRITE+Read with 1's ( 0xFF) Surface WRITE+Read back with 0's Quick format (to NTFS) Alternatively, badblocks instead of steps 5-7, or full u/ EchoGecko795 routine below in the comments. Thanks to everyone, all questions answered! Some notes: Extended self-test can be skipped, as it just short self-test + READ test sequence within the drive, just less informative than the Surface read test as doesn't consider connectivity performance Full format in Windows isn't needed, as it's the same as Surface READ + WRITE (with 0's) tests, maybe more limited, and quick format is enough just to change the filesystem Different surface write+read patterns emulate the 4-pass badblocks test A few things I wanted to clarify: how redundant is filling the drive with files, considering further WRITE+Read surface test? Does it only serve as another WRITE pass, just with different data? do I really need the Read test before the WRITE+Read test , or is the latter enough? (i.e. can a READ before WRITE indicate something that a READ after WRITE won't?). My idea was to see the initial READ after the drive is filled with files, then overwriting it with 0's and reading again how important is changing the pattern/flipping 0's to 1's? should I flip 1's to 0's back again? Can the 1's pattern remain, following a quick format prior to using the drive? And finally, is the more time-consuming 3-4 pass procedure really worth it, and not an overkill?"},
{"Title": "Michigan couple must pay son $30,441 for throwing out porn collection", "Author": "u/benjacob", "Content": "No content"},
{"Title": "This is your regular reminder that Comcast is still a dumpster fire: Comcast to impose home internet data cap of 1.2TB in more than a dozen US states next year", "Author": "u/Snoot_Boopins", "Content": "No content"},
{"Title": "Regarding Uloz.to", "Author": "u/HakimOne", "Content": "Hi, Any thoughts on https://ulozto.net ? Their plans look too good to be true. 10Â TB for 6 EURO, 50Â TB for only 15 EURO. Rclone recently added Uloz.to support."},
{"Title": "TIL the Library of Congress has a 2.129 petabyte (and growing) archive of internet culture", "Author": "u/WhitefangdDS", "Content": "No content"},
{"Title": "\"If you visit CERN in Geneva, you can buy 1 Terabyte of Large Hadron Collider data from the souvenir shop\"... there goes my kid's college funds", "Author": "Unknown author", "Content": "No content"},
{"Title": "Shucking an old WD My Book?", "Author": "u/EdiblePwncakes", "Content": "I have this really old WD MyBook that I'm trying to backup the data off of. However the drive doesn't seem to spin at all even though the enclosure is powered on. It's a very old model, so much so that I can't quite find much info on it through a search online - its model is WD5000P302. I want to shuck it in hopes of preserving the data - however I've heard that these old WD MyBooks have some sort of drive encryption that might prevent this? Does anyone have any experience with this? The drive must be at least 10 years old if I were to guess. Or if the drive doesn't spin at all then might it be completely dead? Thanks for any advice. https://preview.redd.it/shucking-an-old-wd-my-book-v0-hyto1szn9l6d1.jpg https://preview.redd.it/shucking-an-old-wd-my-book-v0-sr1olvho9l6d1.jpg"},
{"Title": "100Mbps uploads and downloads should be US broadband standard, senators say", "Author": "u/Unlanded", "Content": "No content"},
{"Title": "yahoo answers is shutting down", "Author": "u/Fazlul101", "Content": "No content"},
{"Title": "How do you go about pruning your hoard?", "Author": "u/Clive1792", "Content": "Personally I feel snowed under with what I've got. There's countless files on my PC scattered everywhere. Documents, pictures (probably in the 100,000s), audio files, video files. It's most certainly not organised at all, it's scattered all over the hard drives on this PC in various folders & folders of folders. But to make it better I had no real backup plan. I'd just drag & drop and then not really remember what I'd backed up and what I hadn't so then I'd buy a new hard drive & make another copy because the last drive was fairly full. I now have a number of drives that surely have many multiples of various single files. My end goal is I would like some organisation rather than random files & folders scattered here there & everywhere. I'd like to get rid of the multiple duplicate files & also delete files I no longer want/need. Have any of you attacked this kind of task? Any tips or do you really just do it file-by-file? Which to be honest with what I've got & the fact I only have limited time outside of work, this is going to take me many months if not 12months plus .... and that's if I get stuck in & stay focused doing it day in day out."},
{"Title": "Art imitates life", "Author": "Unknown author", "Content": "No content"},
{"Title": "internet archive is being sued", "Author": "u/sersoniko", "Content": "No content"},
{"Title": "how to download videos from just for fans?", "Author": "u/m300n", "Content": "hi, i was wondering if anyone here knows how to save videos from just for fans?"},
{"Title": "Remember this?", "Author": "u/YosoyPabloIscobar", "Content": "No content"},
{"Title": "Forklift accident", "Author": "u/Enkelie", "Content": "No content"},
{"Title": "Well now you know", "Author": "u/emmmmceeee", "Content": "No content"},
{"Title": "Michigan couple must pay son $30,441 for throwing out porn collection", "Author": "u/benjacob", "Content": "No content"},
{"Title": "Ever wondered what 2 Peta Bytes looks like?", "Author": "u/P_G_R_A", "Content": "No content"},
{"Title": "TIL the Library of Congress has a 2.129 petabyte (and growing) archive of internet culture", "Author": "u/WhitefangdDS", "Content": "No content"},
{"Title": "youtube-dl repo had been DMCA'd", "Author": "u/anakinfredo", "Content": "No content"},
{"Title": "100Mbps uploads and downloads should be US broadband standard, senators say", "Author": "u/Unlanded", "Content": "No content"},
{"Title": "Kids, you up?", "Author": "u/diamondsw", "Content": "No content"},
{"Title": "Art imitates life", "Author": "Unknown author", "Content": "No content"},
{"Title": "Problem has been solved, 87 TB Array! No more Panik", "Author": "u/cdeveringham", "Content": "No content"},
{"Title": "Remember this?", "Author": "u/YosoyPabloIscobar", "Content": "No content"},
{"Title": "Thought you all might find this interesting", "Author": "u/erik530195", "Content": "No content"},
{"Title": "Well now you know", "Author": "u/emmmmceeee", "Content": "No content"},
{"Title": "API Clusterfuck! ~ We're locked, read this.", "Author": "u/-Archivist", "Content": "See reopening post..... Hi everyone, we'll keep this short, you already know what's going on. As you've almost certainly heard by now Reddit is locking down their API starting July 1st with the introduction of paid usage. These changes are what killed pushshift.io (full reddit archives and searchable api used by mods and many research/academic papers) and what will kill most (if not all) third-party reddit clients. This is obviously a detriment to everyone, and while Reddit will almost certainly go through with these changes regardless, thousands of subreddits are going to be participating in a 2-day (or longer) blackout. You can read more about the blackouts at r/ModCoord . At the very least, the planned blackout seems to have convinced Reddit to give free API access to accessibility clients. Hopefully it can change their minds further. r/DataHoarder will be locked for an undetermined amount of time, see this thread for reddit data archives, tools, etc. we will also be using this time to update our sidebar links and do some general maintenance in the hopes that this mess doesn't mean the end for us and the many communities that see this as a killing of the Reddit we have loved over the years. Note; during this time no new posts can be made and all comments are black-holed. ~ The Mod Team, ciao for now. Track the blackout here: https://reddark.untone.uk"},
{"Title": "Ever wondered what 2 Peta Bytes looks like?", "Author": "u/P_G_R_A", "Content": "No content"},
{"Title": "The Coronavirus Papers unlocked: 5,352 scientific articles covering the coronavirus - fully searchable and free.", "Author": "u/shrine", "Content": "2020-04-15 update: the-eye.eu is temporarily down, but the de-centralized Interplanetary File System (IPFS) link remains up. Note, publishers have made most Coronavirus articles free as of March 6th 2020. Visit r/libgen and r/scihub to join the open science revolution. Access Article listing / Links Open Directory Full-text Search Torrent InterPlanetary File System Information In a 2015 New York Times op-ed the chief medical officer of Liberia argued that the Ebola pandemic responsible for the loss of over 2,200 lives could have been prevented if not for a paywall blocking access to an article from 1982. Dividing the worldâ€™s scientists with a paywall in the middle of a global humanitarian crisis is an unacceptable and unforgivable act of criminal greed. In the developing world the price for a single article can amount to as much as half a weekâ€™s salary for a physician. A few days ago, I found an early-release coronavirus article with a $35.95 access fee for non-subscribers. The fury I felt brought tears to my eyes. Me and a few friends share that fury, so we gathered a collection of five-thousand scientific studies covering any article title containing â€œ coronav* â€ from 1968-2020. The scope of the papers spans not only the 7 human coronaviruses, but up to 40 other Coronaviridae family strains. The Ebola virus showed us that every study counts. We are on the first step towards compiling a complete open-access Coronaviridae research catalog for the worldâ€™s scientists, journalists, and virology experts to draw from to fight the virus and save lives. Our project is illegal, but itâ€™s the right thing to do in this crisis. We refuse to put copyright before human lives. Sharing everything we know about the virus is essential, which is why international scientists are openly sharing their coronavirus findings in an unprecedented way . Developing-world scientists often work without article access due to complex and expensive contract agreements between publishers, universities, and hospitals, relying on overseas colleagues to help them hunt down PDF files. The virus is not going to wait for this, so we need to act with conviction, now. To their credit, publishers made a few dozen papers open-access in the last few days, which you can find over at Elsevierâ€™s Novel Coronavirus Information Center and Wileyâ€™s Coronavirus collection. While Wiley is slating to shut down their collection in April, our collection wonâ€™t be shutting down anytime soon. Weâ€™re going to keep growing to help our scientists out, and you can help us complete the catalog by identifying any papers we missed. All extant Coronaviridae research, accessible in seconds, by any scientist in the world. Itâ€™s the least we can do to help. Methodology How did we do it? We scanned Sci-Hub 's 80 million title collection for the coronavirus, then we extracted the titles and Digital Object Identifiers (DOI) to an index, and exported the PDF files to upload them to The-Eye.euâ€™s full-text search repository. How can I help? We always need developers. You can also help us identify new articles by joining our team spreadsheet here . Request access and you can begin adding new article titles to the list. You can also help share word of the collection with the scientific community by reaching out to journalists. Who is helping us? Our brave host is The-Eye.eu , a â€œnon-profit, community driven platform dedicated to the archiving and long-term preservation of any and all data,â€ making this project just one of the many public access preservation projects they stand behind. You can aid projects like this one by donating toward their server bills. A thank you to Sci-Hub and Library Genesis. Last year communities across reddit (including r/seedboxes and r/DataHoarder ) came together in a mission to secure and preserve Sci-Hub and Library Genesis, collectively the two largest free and open non-profit library collections in the world: Sci-Hubâ€™s 80-million scientific article database that made this project possible, and LibGenâ€™s 2.5-million scientific-book collection. The libraries fulfill United Nations world development goals mandating the removal of restrictions on access to science, and they serve developing world doctors, academic researchers, and other experts in society with the knowledge they need to build a better world. Keeping these libraries open and thriving means saving lives, educating the world, and providing invaluable science to humanityâ€™s global experts. Thank you to everyone involved in the project, The-Eye.eu for their support, and to all the scientists around the world working on behalf of humanity today."},
{"Title": "youtube-dl repo had been DMCA'd", "Author": "u/anakinfredo", "Content": "No content"},
{"Title": "Twitter will remove free access to the Twitter API from 9 Feb 2023. Probably a good time to archive notable accounts now.", "Author": "u/babelfishery", "Content": "No content"},
{"Title": "Kids, you up?", "Author": "u/diamondsw", "Content": "No content"},
{"Title": "Imgur is updating their TOS on May 15, 2023: All NSFW content to be banned", "Author": "u/trd86", "Content": "No content"},
{"Title": "Problem has been solved, 87 TB Array! No more Panik", "Author": "u/cdeveringham", "Content": "No content"},
{"Title": "Thought you guys would appreciate this.", "Author": "Unknown author", "Content": "No content"},
{"Title": "Thought you all might find this interesting", "Author": "u/erik530195", "Content": "No content"},
{"Title": "The only Nintendo 64 and 64 Disk Drive Development Data Tapes known to exist are now resting happily in my collection, and happy to say 5/6 are dumped and preserved. I'm told the last one has no data on it, but I will be working to recheck and verify that. Data can be found at ultra64.ca", "Author": "u/Carl_Sammons", "Content": "No content"},
{"Title": "API Clusterfuck! ~ We're locked, read this.", "Author": "u/-Archivist", "Content": "See reopening post..... Hi everyone, we'll keep this short, you already know what's going on. As you've almost certainly heard by now Reddit is locking down their API starting July 1st with the introduction of paid usage. These changes are what killed pushshift.io (full reddit archives and searchable api used by mods and many research/academic papers) and what will kill most (if not all) third-party reddit clients. This is obviously a detriment to everyone, and while Reddit will almost certainly go through with these changes regardless, thousands of subreddits are going to be participating in a 2-day (or longer) blackout. You can read more about the blackouts at r/ModCoord . At the very least, the planned blackout seems to have convinced Reddit to give free API access to accessibility clients. Hopefully it can change their minds further. r/DataHoarder will be locked for an undetermined amount of time, see this thread for reddit data archives, tools, etc. we will also be using this time to update our sidebar links and do some general maintenance in the hopes that this mess doesn't mean the end for us and the many communities that see this as a killing of the Reddit we have loved over the years. Note; during this time no new posts can be made and all comments are black-holed. ~ The Mod Team, ciao for now. Track the blackout here: https://reddark.untone.uk"},
{"Title": "I've collected all the iFixit repair guides in PDF format - 38,893 files", "Author": "u/makeworld", "Content": "iFixit and their guides are a great source for learning how to repair and fix electronics. They offer all their guides in PDF format, which I thought might be easier for viewing and self-containment then HTML. I've downloaded all their guides as PDFs, and put them into a single torrent. I think this is information that is very valuable to have offline - for power outages, remote travel/backpacking, the end of the world, etc. I'm hoping this can join some of your collections, beside Wikipedia and first aid pamphlets. Magnet link: magnet:?xt=urn:btih:ed9889445d52d7882e844bd926e1b547a2c00781&dn=pdfs.zip&tr=udp%3A%2F%2Ftracker.coppersurfer.tk%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337%2Fannounce&tr=udp%3A%2F%2Ftracker.leechers-paradise.org%3A6969%2Fannounce&tr=udp%3A%2F%2Fp4p.arenabg.com%3A1337%2Fannounce Torrent file The torrent is just a single ZIP file named pdfs.zip , that contains all the guides. It is about 60 gigabytes in total. Each guide is named by the name of the guide, for easy searching. Duplicate names were fixed by adding numbers to the end, as in guide name [2].pdf and guide name [3].pdf . All filenames are Windows safe. Keep in mind that my upload speeds are slow, and it may take a bit for your computer to find mine. But I have always-on server that is seeding it, so it will download eventually. The contents of the torrent will also be up on the Internet Archive here , they are downloading it now. If you want to replicate what I've done, or update the archive yourself (I will try and update it every so often), there are wget and python3 scripts and source files (like lists of urls) in that archive as well. Those files are not part of the torrent. If you have any questions, or plan on seeding, let me know below! EDIT: As I mentioned above, my upload speed is slow. The torrent will take a very long time initially, and there's not much I can do about that. Feel free to come back in a couple days when there will be more than just me with a full copy."},
{"Title": "The Coronavirus Papers unlocked: 5,352 scientific articles covering the coronavirus - fully searchable and free.", "Author": "u/shrine", "Content": "2020-04-15 update: the-eye.eu is temporarily down, but the de-centralized Interplanetary File System (IPFS) link remains up. Note, publishers have made most Coronavirus articles free as of March 6th 2020. Visit r/libgen and r/scihub to join the open science revolution. Access Article listing / Links Open Directory Full-text Search Torrent InterPlanetary File System Information In a 2015 New York Times op-ed the chief medical officer of Liberia argued that the Ebola pandemic responsible for the loss of over 2,200 lives could have been prevented if not for a paywall blocking access to an article from 1982. Dividing the worldâ€™s scientists with a paywall in the middle of a global humanitarian crisis is an unacceptable and unforgivable act of criminal greed. In the developing world the price for a single article can amount to as much as half a weekâ€™s salary for a physician. A few days ago, I found an early-release coronavirus article with a $35.95 access fee for non-subscribers. The fury I felt brought tears to my eyes. Me and a few friends share that fury, so we gathered a collection of five-thousand scientific studies covering any article title containing â€œ coronav* â€ from 1968-2020. The scope of the papers spans not only the 7 human coronaviruses, but up to 40 other Coronaviridae family strains. The Ebola virus showed us that every study counts. We are on the first step towards compiling a complete open-access Coronaviridae research catalog for the worldâ€™s scientists, journalists, and virology experts to draw from to fight the virus and save lives. Our project is illegal, but itâ€™s the right thing to do in this crisis. We refuse to put copyright before human lives. Sharing everything we know about the virus is essential, which is why international scientists are openly sharing their coronavirus findings in an unprecedented way . Developing-world scientists often work without article access due to complex and expensive contract agreements between publishers, universities, and hospitals, relying on overseas colleagues to help them hunt down PDF files. The virus is not going to wait for this, so we need to act with conviction, now. To their credit, publishers made a few dozen papers open-access in the last few days, which you can find over at Elsevierâ€™s Novel Coronavirus Information Center and Wileyâ€™s Coronavirus collection. While Wiley is slating to shut down their collection in April, our collection wonâ€™t be shutting down anytime soon. Weâ€™re going to keep growing to help our scientists out, and you can help us complete the catalog by identifying any papers we missed. All extant Coronaviridae research, accessible in seconds, by any scientist in the world. Itâ€™s the least we can do to help. Methodology How did we do it? We scanned Sci-Hub 's 80 million title collection for the coronavirus, then we extracted the titles and Digital Object Identifiers (DOI) to an index, and exported the PDF files to upload them to The-Eye.euâ€™s full-text search repository. How can I help? We always need developers. You can also help us identify new articles by joining our team spreadsheet here . Request access and you can begin adding new article titles to the list. You can also help share word of the collection with the scientific community by reaching out to journalists. Who is helping us? Our brave host is The-Eye.eu , a â€œnon-profit, community driven platform dedicated to the archiving and long-term preservation of any and all data,â€ making this project just one of the many public access preservation projects they stand behind. You can aid projects like this one by donating toward their server bills. A thank you to Sci-Hub and Library Genesis. Last year communities across reddit (including r/seedboxes and r/DataHoarder ) came together in a mission to secure and preserve Sci-Hub and Library Genesis, collectively the two largest free and open non-profit library collections in the world: Sci-Hubâ€™s 80-million scientific article database that made this project possible, and LibGenâ€™s 2.5-million scientific-book collection. The libraries fulfill United Nations world development goals mandating the removal of restrictions on access to science, and they serve developing world doctors, academic researchers, and other experts in society with the knowledge they need to build a better world. Keeping these libraries open and thriving means saving lives, educating the world, and providing invaluable science to humanityâ€™s global experts. Thank you to everyone involved in the project, The-Eye.eu for their support, and to all the scientists around the world working on behalf of humanity today."},
{"Title": "YouTube-dlâ€™s repository has been restored", "Author": "u/DisastrousRhubarb", "Content": "No content"},
{"Title": "Twitter will remove free access to the Twitter API from 9 Feb 2023. Probably a good time to archive notable accounts now.", "Author": "u/babelfishery", "Content": "No content"},
{"Title": "Z-Library isn't really gone, but that maybe up to you.", "Author": "u/-Archivist", "Content": "UPDATE2 TorrentFreak is covering this continuing story as new details come to light. https://torrentfreak.com/tag/zlibrary/ UPDATE ~ Z-Library Aftermath Reveals The Feds Seized Dozens of Domain Names @ TorrentFreak We'd also like to address some of the comments here asking \"how do I extract a book from this data\" . r/DataHoader isn't a piracy supporting subreddit, a guide on how to extract books from these archives was purposefully left out. These torrents are presented as a preservation only archive and are not meant to aid book piracy or add books to your curated collections. Once upon a time in this sub this explanation wouldn't have been necessary. The thread will be cleaned and comment locked. Original Thread Millions woke up to news today that Z-Library domains have been seized, cries that z-lib is gone were heard from red core to black sky!... but that's not really the case so here is what you, a humble datahoarder can do about it. In case you missed it a unique to z-lib (deduped against LibGen) backup was made and published by u/pilimi_anna a little over a month ago. While you did a great job with SciHub, there's still work be done to ensure the preservation of all written works and cultural heritage. So here is the 5,998,794 book 27.8TB z-lib archive for you to hold, hoard, preserve, seed and proliferate. Database | Mirror ~ (metadata, extensions) Torrents | TOR Mirror Related Reading U.S. Authorities Seize Z-Library Domain Names @ TorrentFreak TikTok Blocks Z-Library Hashtag @ TorrentFreak ZLibrary domains have been seized @ HackerNews ISBNdb Dump â€“ How many books are preserved forever? @ Annas-Blog Mission to preserve SciHub @ r/DataHoarder Alternative Libraries / Free eBook Hosts OpenLibrary Library Genesis | IPFS PDF Drive Sci-Hub Gutenberg Obooko ManyBooks FreeBookSpot The Anarchist Library Closing Support authors you love.. But abolish the strangle hold of DRM and licensing that kills ownership, seek to squash abuse of the DMCA, move to limit copyright terms and above all aim to ensure Alexandria doesn't burn twice. Ukraine Crisis ^Megathread will replace this thread again within 7 days."},
{"Title": "Imgur is updating their TOS on May 15, 2023: All NSFW content to be banned", "Author": "u/trd86", "Content": "No content"},
{"Title": "The dream ðŸ™", "Author": "u/DragoniteChamp", "Content": "No content"},
{"Title": "Thought you guys would appreciate this.", "Author": "Unknown author", "Content": "No content"},
{"Title": "Not just SATA . . .", "Author": "Unknown author", "Content": "No content"},
{"Title": "The only Nintendo 64 and 64 Disk Drive Development Data Tapes known to exist are now resting happily in my collection, and happy to say 5/6 are dumped and preserved. I'm told the last one has no data on it, but I will be working to recheck and verify that. Data can be found at ultra64.ca", "Author": "u/Carl_Sammons", "Content": "No content"},
{"Title": "Anon loses 8 terabytes of data", "Author": "u/Epoxhy", "Content": "No content"},
{"Title": "I've collected all the iFixit repair guides in PDF format - 38,893 files", "Author": "u/makeworld", "Content": "iFixit and their guides are a great source for learning how to repair and fix electronics. They offer all their guides in PDF format, which I thought might be easier for viewing and self-containment then HTML. I've downloaded all their guides as PDFs, and put them into a single torrent. I think this is information that is very valuable to have offline - for power outages, remote travel/backpacking, the end of the world, etc. I'm hoping this can join some of your collections, beside Wikipedia and first aid pamphlets. Magnet link: magnet:?xt=urn:btih:ed9889445d52d7882e844bd926e1b547a2c00781&dn=pdfs.zip&tr=udp%3A%2F%2Ftracker.coppersurfer.tk%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337%2Fannounce&tr=udp%3A%2F%2Ftracker.leechers-paradise.org%3A6969%2Fannounce&tr=udp%3A%2F%2Fp4p.arenabg.com%3A1337%2Fannounce Torrent file The torrent is just a single ZIP file named pdfs.zip , that contains all the guides. It is about 60 gigabytes in total. Each guide is named by the name of the guide, for easy searching. Duplicate names were fixed by adding numbers to the end, as in guide name [2].pdf and guide name [3].pdf . All filenames are Windows safe. Keep in mind that my upload speeds are slow, and it may take a bit for your computer to find mine. But I have always-on server that is seeding it, so it will download eventually. The contents of the torrent will also be up on the Internet Archive here , they are downloading it now. If you want to replicate what I've done, or update the archive yourself (I will try and update it every so often), there are wget and python3 scripts and source files (like lists of urls) in that archive as well. Those files are not part of the torrent. If you have any questions, or plan on seeding, let me know below! EDIT: As I mentioned above, my upload speed is slow. The torrent will take a very long time initially, and there's not much I can do about that. Feel free to come back in a couple days when there will be more than just me with a full copy."},
{"Title": "Been watching everyone panic over HBO Max gutting it's library like a fish.", "Author": "u/AshleyUncia", "Content": "No content"},
{"Title": "YouTube-dlâ€™s repository has been restored", "Author": "u/DisastrousRhubarb", "Content": "No content"},
{"Title": "In 1999 Amazon stored .5GB or â€œabout 350 floppy disksâ€ of data about its users every day", "Author": "u/the_best_moshe", "Content": "No content"},
{"Title": "Z-Library isn't really gone, but that maybe up to you.", "Author": "u/-Archivist", "Content": "UPDATE2 TorrentFreak is covering this continuing story as new details come to light. https://torrentfreak.com/tag/zlibrary/ UPDATE ~ Z-Library Aftermath Reveals The Feds Seized Dozens of Domain Names @ TorrentFreak We'd also like to address some of the comments here asking \"how do I extract a book from this data\" . r/DataHoader isn't a piracy supporting subreddit, a guide on how to extract books from these archives was purposefully left out. These torrents are presented as a preservation only archive and are not meant to aid book piracy or add books to your curated collections. Once upon a time in this sub this explanation wouldn't have been necessary. The thread will be cleaned and comment locked. Original Thread Millions woke up to news today that Z-Library domains have been seized, cries that z-lib is gone were heard from red core to black sky!... but that's not really the case so here is what you, a humble datahoarder can do about it. In case you missed it a unique to z-lib (deduped against LibGen) backup was made and published by u/pilimi_anna a little over a month ago. While you did a great job with SciHub, there's still work be done to ensure the preservation of all written works and cultural heritage. So here is the 5,998,794 book 27.8TB z-lib archive for you to hold, hoard, preserve, seed and proliferate. Database | Mirror ~ (metadata, extensions) Torrents | TOR Mirror Related Reading U.S. Authorities Seize Z-Library Domain Names @ TorrentFreak TikTok Blocks Z-Library Hashtag @ TorrentFreak ZLibrary domains have been seized @ HackerNews ISBNdb Dump â€“ How many books are preserved forever? @ Annas-Blog Mission to preserve SciHub @ r/DataHoarder Alternative Libraries / Free eBook Hosts OpenLibrary Library Genesis | IPFS PDF Drive Sci-Hub Gutenberg Obooko ManyBooks FreeBookSpot The Anarchist Library Closing Support authors you love.. But abolish the strangle hold of DRM and licensing that kills ownership, seek to squash abuse of the DMCA, move to limit copyright terms and above all aim to ensure Alexandria doesn't burn twice. Ukraine Crisis ^Megathread will replace this thread again within 7 days."},
{"Title": "I'm sorry Hasan. :(", "Author": "u/hobbseltoff", "Content": "No content"},
{"Title": "The dream ðŸ™", "Author": "u/DragoniteChamp", "Content": "No content"},
{"Title": "URGENT: Hong Kong Stand News to cease operations immediately after directors arrested this morning. Please help backup social media and website!", "Author": "u/TheIrishPanther", "Content": "No content"},
{"Title": "Not just SATA . . .", "Author": "Unknown author", "Content": "No content"},
{"Title": "Two months ago, when I go a 16TB swapped for an 8TB from Amazon, and everyone told me I was getting catfished by someone pretending to be Seagate's head of global security? Here's the free 10TB Exos Seagate sent me direct from HQ 'for my trouble'. :P", "Author": "u/AshleyUncia", "Content": "No content"},
{"Title": "Anon loses 8 terabytes of data", "Author": "u/Epoxhy", "Content": "No content"},
{"Title": "Youtube deleted a channel that made educational ethical hacking videos. Other Channels on the same topic face a risk of being taken down. Can you help and archive the channels that havenâ€™t been deleted yet.", "Author": "u/latuziti", "Content": "The channel zSecurity was was deleted and it seem like other youtube channels that make ethical hacking content might face the same risk. If you can help please backup that following youtube channels. HackerSploit - https://www.youtube.com/channel/UC0ZTPkdxlAKf-V33tqXwi3Q/videos Sam Bowne - https://www.youtube.com/user/sambowne/videos LiveOverflow - https://www.youtube.com/channel/UClcE-kVhqyiHCcjYwcpfj9w/videos Null Byte - https://www.youtube.com/channel/UCgTNupxATBfWmfehv21ym-g/videos InsiderPhD - https://www.youtube.com/c/InsiderPhD/videos Loi Liang Yang - https://www.youtube.com/channel/UC1szFCBUWXY3ESff8dJjjzw/videos STÃ–K - https://www.youtube.com/c/STOKfredrik/videos The Cyber Mentor - https://www.youtube.com/channel/UC0ArlFuFYMpEewyRBzdLHiw Guided Hacking - https://www.youtube.com/user/L4DL4D2EUROPE/videos Null - https://www.youtube.com/channel/UCZF93Qrt6yMAabRnlND4YsQ/videos Cheat The Game - https://www.youtube.com/user/BloodFayte/videos Stephen Chapman - https://www.youtube.com/user/seowhistleblower/videos Cristi Vlad - https://www.youtube.com/user/cristivlad25/videos DC CyberSec - https://www.youtube.com/channel/UC3sccPO4v8YqCTn8sezZGTw/videos Joseph Delgadillo  - https://www.youtube.com/c/JosephDelgadillo/videos Nahamsec - https://www.youtube.com/c/Nahamsec/videos I.T Security Labs - https://www.youtube.com/c/ITSecurityLabs/videos Red Team Village - https://www.youtube.com/c/RedTeamVillage/videos People have recommended other youtube channels to archive. I'll add them here Deviant Ollam - https://www.youtube.com/user/DeviantOllam LockpickingLawyer - https://www.youtube.com/channel/UCm9K6rby98W8JigLoZOh6FQ BosnianBill - https://www.youtube.com/user/bosnianbill 13Cubed - https://www.youtube.com/c/13cubed/videos Jose Barrientos - https://www.youtube.com/user/Greiko Update 2: Cyberspatial - https://www.youtube.com/c/Cyberspatial/videos Nahamsec - https://www.youtube.com/c/Nahamsec/videos pwn.college - https://www.youtube.com/channel/UCBaWwFw7KmCN8YlfX4ERYKg/videos Farah Hawa - https://www.youtube.com/c/FarahHawa/videos Web Development Tutorials - https://www.youtube.com/c/yaworsk1/videos OALabs - https://www.youtube.com/c/OALabs/videos Hacksplained - https://www.youtube.com/c/Hacksplained/videos Update 3: zecurity channel has been restored. It should be backedup just in case something like this happens again. https://www.youtube.com/zsecurity"},
{"Title": "Been watching everyone panic over HBO Max gutting it's library like a fish.", "Author": "u/AshleyUncia", "Content": "No content"},
{"Title": "Data hoarding is older than we thought! MAD Magazine 215 from 1980", "Author": "u/Hong-Hong-Hang-Hang", "Content": "No content"},
{"Title": "In 1999 Amazon stored .5GB or â€œabout 350 floppy disksâ€ of data about its users every day", "Author": "u/the_best_moshe", "Content": "No content"},
{"Title": "Hello, r/DataHoarder! Weâ€™re iFixit, and we just launched the worldâ€™s most comprehensive medical equipment repair database", "Author": "u/Craig_iFixit", "Content": "After my recent comment on here blew up, I figured you all would get a kick out of this. TL;DR: We scraped the internet for any and all medical equipment repair documentation we could find. We ended up with over 13,000 PDFs across 5,000 medical devices, all uploaded to iFixit.com and available for free to anyone and everyone. ----- Hospitals are having trouble getting service information to fix medical equipment and manufacturers canâ€™t keep pace with the growing demand for repair of critical hospital equipment. On top of that, biomedical technicians spend countless hours scouring the internet searching for crucial repair information. This is not a great way to run a health system. So weâ€™ve been fixing that. Over the last two months, weâ€™ve pivoted half our company to build the worldâ€™s most comprehensive medical equipment service database . We just posted more than 13,000 PDFs from hundreds of manufacturersâ€”online and available for free. You can find them in our Medical Device category. This has been an absolutely massive undertaking, and we were fortunate to have the help and support of over 200 librarians and archivists from across the country. Archivists from university and public libraries, research institutes, insurance and software companies, and of course biomedical technicians themselves, all donated their valuable time. Collectively, theyâ€™ve contributed thousands of hours organizing piles of documents into a navigable, searchable system. Some medical manufacturers, like Mindray , allow biomeds to access their manuals freely. A few more released select documents after the outbreak of COVID-19. But for their day-to-day work, biomeds have long relied on a rag-tag set of web resources to get the job done. Among the most popular is Frankâ€™s Hospital Workshop , a Tanzania-based site that hosts hundreds of medical device manualsâ€”itâ€™s the unofficial biomed bible. But we wanted to make it easier for anyone to find the right manual, especially in an emergency. Some of the documents in our collection were already available. Others were not publicly posted until now . And it was important to us that this resource didnâ€™t just duplicate existing resources, but improved accessibility in a meaningful way. To be very clear: iFixit will not make any money off of this project . We are providing hosting and curation free of charge, and free of advertising, to the medical community. We welcome manufacturers to join us and contribute toward an up-to-date central repository for the biomedical community, as well as biomedical technicians around the world to join iFixitâ€™s repair community . No technician is an island, and we hope to facilitate an exchange of knowledge and troubleshooting. This medical repository is most useful if itâ€™s collaboratively moderated by biomedical technicians, with our assistance."},
{"Title": "I'm sorry Hasan. :(", "Author": "u/hobbseltoff", "Content": "No content"},
{"Title": "[Offtopic] Girlfriend took one look at the shell of a shucked EasyStore... â€œYou need that?â€", "Author": "u/DannyVFilms", "Content": "No content"},
{"Title": "URGENT: Hong Kong Stand News to cease operations immediately after directors arrested this morning. Please help backup social media and website!", "Author": "u/TheIrishPanther", "Content": "No content"},
{"Title": "Russianaircraft.net scrubs all military aircraft in a likely effort to prevent identification of downed Russian aircraft - If you ever needed a better justification for datahoarding, here it is.", "Author": "u/AshleyUncia", "Content": "No content"},
{"Title": "Two months ago, when I go a 16TB swapped for an 8TB from Amazon, and everyone told me I was getting catfished by someone pretending to be Seagate's head of global security? Here's the free 10TB Exos Seagate sent me direct from HQ 'for my trouble'. :P", "Author": "u/AshleyUncia", "Content": "No content"},
{"Title": "Youtube deleted a channel that made educational ethical hacking videos. Other Channels on the same topic face a risk of being taken down. Can you help and archive the channels that havenâ€™t been deleted yet.", "Author": "u/latuziti", "Content": "The channel zSecurity was was deleted and it seem like other youtube channels that make ethical hacking content might face the same risk. If you can help please backup that following youtube channels. HackerSploit - https://www.youtube.com/channel/UC0ZTPkdxlAKf-V33tqXwi3Q/videos Sam Bowne - https://www.youtube.com/user/sambowne/videos LiveOverflow - https://www.youtube.com/channel/UClcE-kVhqyiHCcjYwcpfj9w/videos Null Byte - https://www.youtube.com/channel/UCgTNupxATBfWmfehv21ym-g/videos InsiderPhD - https://www.youtube.com/c/InsiderPhD/videos Loi Liang Yang - https://www.youtube.com/channel/UC1szFCBUWXY3ESff8dJjjzw/videos STÃ–K - https://www.youtube.com/c/STOKfredrik/videos The Cyber Mentor - https://www.youtube.com/channel/UC0ArlFuFYMpEewyRBzdLHiw Guided Hacking - https://www.youtube.com/user/L4DL4D2EUROPE/videos Null - https://www.youtube.com/channel/UCZF93Qrt6yMAabRnlND4YsQ/videos Cheat The Game - https://www.youtube.com/user/BloodFayte/videos Stephen Chapman - https://www.youtube.com/user/seowhistleblower/videos Cristi Vlad - https://www.youtube.com/user/cristivlad25/videos DC CyberSec - https://www.youtube.com/channel/UC3sccPO4v8YqCTn8sezZGTw/videos Joseph Delgadillo  - https://www.youtube.com/c/JosephDelgadillo/videos Nahamsec - https://www.youtube.com/c/Nahamsec/videos I.T Security Labs - https://www.youtube.com/c/ITSecurityLabs/videos Red Team Village - https://www.youtube.com/c/RedTeamVillage/videos People have recommended other youtube channels to archive. I'll add them here Deviant Ollam - https://www.youtube.com/user/DeviantOllam LockpickingLawyer - https://www.youtube.com/channel/UCm9K6rby98W8JigLoZOh6FQ BosnianBill - https://www.youtube.com/user/bosnianbill 13Cubed - https://www.youtube.com/c/13cubed/videos Jose Barrientos - https://www.youtube.com/user/Greiko Update 2: Cyberspatial - https://www.youtube.com/c/Cyberspatial/videos Nahamsec - https://www.youtube.com/c/Nahamsec/videos pwn.college - https://www.youtube.com/channel/UCBaWwFw7KmCN8YlfX4ERYKg/videos Farah Hawa - https://www.youtube.com/c/FarahHawa/videos Web Development Tutorials - https://www.youtube.com/c/yaworsk1/videos OALabs - https://www.youtube.com/c/OALabs/videos Hacksplained - https://www.youtube.com/c/Hacksplained/videos Update 3: zecurity channel has been restored. It should be backedup just in case something like this happens again. https://www.youtube.com/zsecurity"},
{"Title": "Data hoarding is older than we thought! MAD Magazine 215 from 1980", "Author": "u/Hong-Hong-Hang-Hang", "Content": "No content"},
{"Title": "Hello, r/DataHoarder! Weâ€™re iFixit, and we just launched the worldâ€™s most comprehensive medical equipment repair database", "Author": "u/Craig_iFixit", "Content": "After my recent comment on here blew up, I figured you all would get a kick out of this. TL;DR: We scraped the internet for any and all medical equipment repair documentation we could find. We ended up with over 13,000 PDFs across 5,000 medical devices, all uploaded to iFixit.com and available for free to anyone and everyone. ----- Hospitals are having trouble getting service information to fix medical equipment and manufacturers canâ€™t keep pace with the growing demand for repair of critical hospital equipment. On top of that, biomedical technicians spend countless hours scouring the internet searching for crucial repair information. This is not a great way to run a health system. So weâ€™ve been fixing that. Over the last two months, weâ€™ve pivoted half our company to build the worldâ€™s most comprehensive medical equipment service database . We just posted more than 13,000 PDFs from hundreds of manufacturersâ€”online and available for free. You can find them in our Medical Device category. This has been an absolutely massive undertaking, and we were fortunate to have the help and support of over 200 librarians and archivists from across the country. Archivists from university and public libraries, research institutes, insurance and software companies, and of course biomedical technicians themselves, all donated their valuable time. Collectively, theyâ€™ve contributed thousands of hours organizing piles of documents into a navigable, searchable system. Some medical manufacturers, like Mindray , allow biomeds to access their manuals freely. A few more released select documents after the outbreak of COVID-19. But for their day-to-day work, biomeds have long relied on a rag-tag set of web resources to get the job done. Among the most popular is Frankâ€™s Hospital Workshop , a Tanzania-based site that hosts hundreds of medical device manualsâ€”itâ€™s the unofficial biomed bible. But we wanted to make it easier for anyone to find the right manual, especially in an emergency. Some of the documents in our collection were already available. Others were not publicly posted until now . And it was important to us that this resource didnâ€™t just duplicate existing resources, but improved accessibility in a meaningful way. To be very clear: iFixit will not make any money off of this project . We are providing hosting and curation free of charge, and free of advertising, to the medical community. We welcome manufacturers to join us and contribute toward an up-to-date central repository for the biomedical community, as well as biomedical technicians around the world to join iFixitâ€™s repair community . No technician is an island, and we hope to facilitate an exchange of knowledge and troubleshooting. This medical repository is most useful if itâ€™s collaboratively moderated by biomedical technicians, with our assistance."},
{"Title": "[Offtopic] Girlfriend took one look at the shell of a shucked EasyStore... â€œYou need that?â€", "Author": "u/DannyVFilms", "Content": "No content"},
{"Title": "Russianaircraft.net scrubs all military aircraft in a likely effort to prevent identification of downed Russian aircraft - If you ever needed a better justification for datahoarding, here it is.", "Author": "u/AshleyUncia", "Content": "No content"},
{"Title": "Did you know that at the Internet Archive a light blinks on one of their servers every time you use their collections?", "Author": "u/storytracer", "Content": "No content"},
{"Title": "Zippyshare is shutting down", "Author": "u/giratina143", "Content": "No content"},
{"Title": "relatable to you guys?", "Author": "u/MysteriousK69420", "Content": "No content"},
{"Title": "Fujifilm refuses to pay ransomware demand, relies on backups", "Author": "u/razeus", "Content": "No content"},
{"Title": "ArchiveTeam has saved over 10.8 BILLION Reddit links so far. We need YOUR help running ArchiveTeam Warrior to archive subreddits before they're gone indefinitely after June 12th!", "Author": "u/BananaBus43", "Content": "ArchiveTeam has been archiving Reddit posts for a while now, but we are running out of time. So far, we have archived 10.81 billion links, with 150 million to go . Recent news of the Reddit API cost changes will force many of the top 3rd party Reddit apps to shut down. This will not only affect how people use Reddit, but it will also cause issues with many subreddit moderation bots which rely on the API to function. Many subreddits have agreed to shut down for 48 hours on June 12th, while others will be gone indefinitely unless this issue is resolved. We are archiving Reddit posts so that in the event that the API cost change is never addressed, we can still access posts from those closed subreddits. Here is how you can help: Choose the \"host\" that matches your current PC, probably Windows or macOS Download ArchiveTeam Warrior In VirtualBox, click File > Import Appliance and open the file. Start the virtual machine. It will fetch the latest updates and will eventually tell you to start your web browser. Once youâ€™ve started your warrior: Go to http://localhost:8001/ and check the Settings page. Choose a username â€” weâ€™ll show your progress on the leaderboard. Go to the \"All projects\" tab and select ArchiveTeamâ€™s Choice to let your warrior work on the most urgent project. (This will be Reddit). Alternative Method: Docker Download Docker on your \"host\" (Windows, macOS, Linux) Follow the instructions on the ArchiveTeam website to set up Docker When setting up the project container, it will ask you to enter this command: docker run -d --name archiveteam --label=com.centurylinklabs.watchtower.enable=true --restart=unless-stopped [image address] --concurrent 1 [username] Make sure to replace the [image address] with the Reddit project address (removing brackets): atdr.meo.ws/archiveteam/reddit-grab Also change the [username] to whatever you'd like, no need to register for anything. More information about running this project: Information about setting up the project ArchiveTeam Wiki page on the Reddit project ArchiveTeam IRC Channel for the Reddit Project (#shreddit on hackint) There are a lot more items that are waiting to be queued into the tracker (approximately 758 million), so 150 million is not an accurate number. This is due to Redis limitations - the tracker is a Ruby and Redis monolith that serves multiple projects with around hundreds of millions of items. You can see all the Reddit items here . The maximum concurrency that you can run is 10 per IP (this is stated in the IRC channel topic). 5 works better for datacenter IPs. Information about Docker errors: If you are seeing RSYNC errors: If the error is about max connections (either -1 or 400), then this is normal. This is our (not amazingly intuitive) method of telling clients to try another target server (we have many of them). Just let it retry, it'll work eventually. If the error is not about max connections, please contact ArchiveTeam on IRC. If you are seeing HOSTERRs, check your DNS. We use Quad9 for our containers. If you need support or wish to discuss, contact ArchiveTeam on IRC Information on what ArchiveTeam archives and how to access the data (from u/rewbycraft ): We archive the posts and comments directly with this project. The things being linked to by the posts (and comments) are put in a queue that we'll process once we've got some more spare capacity. After a few days this stuff ends up in the Internet Archive's Wayback Machine. So, if you have an URL, you can put it in there and retrieve the post. (Note: We save the links without any query parameters and generally using permalinks, so if your URL has ?<and other stuff> at the end, remove that. And try to use permalinks if possible.) It takes a few days because there's a lot of processing logic going on behind the scenes. If you want to be sure something is archived and aren't sure we're covering it, feel free to talk to us on IRC. We're trying to archive literally everything. IMPORTANT: Do NOT modify scripts or the Warrior client! Edit 4: Weâ€™re over 12 billion links archived. Keep running the warrior/Docker during the blackout we still have a lot of posts left. Check this website to see when a subreddit goes private. Edit 3: Added a more prominent link to the Reddit IRC channel. Added more info about Docker errors and the project data. Edit 2: If you want check how much you've contributed, go to the project tracker website , press \"show all\" and type ctrl/cmd - F (find in page on mobile), and search your username. It should show you the number of items and the size of data that you've archived. Edit 1: Added more project info given by u/signalhunter ."},
{"Title": "The 276TB monster under the bed", "Author": "u/spazatk", "Content": "No content"},
{"Title": "introducing hard drive fold", "Author": "u/Astavicious", "Content": "No content"},
{"Title": "I built a book scanner and scanned all the yearbooks at a school", "Author": "u/camwow13", "Content": "No content"},
{"Title": "Reject the \"EARN IT Act\" (s. 3398) which threatens free speech, encryption, privacy, and the nation's cybersecurity.", "Author": "u/peoplearemean78", "Content": "No content"},
{"Title": "Hackers leak 37GB of Microsoft's source code (Bing, Cortana and more)", "Author": "u/harrro", "Content": "No content"},
{"Title": "Due to the new Audacity Terms of Service, I present 31 versions of Audacity and Github source code for 18 versions", "Author": "u/coasterghost", "Content": "No content"},
{"Title": "\"Ever wonder what 200PB of tape looks like?\"", "Author": "u/equalunique", "Content": "No content"},
{"Title": "On the zeroth day of Christmas, my true love sent to me eight 8TB drives.", "Author": "u/YourMJK", "Content": "No content"},
{"Title": "Please stop posting photos of your hard drives.", "Author": "u/Nooco24", "Content": "Hey, quick reminder, posting pictures of a bunch of HDDs you just bought ISN'T interesting, it's boring and I'm tired of seeing them all, I'm tired of seeing \"Am I part of this now?\" or \"Am I doing this the right way?\" posts. It's not because you have 100TB free of storage on your server that you are a data hoarder, and there is no \"good way to do this\". Data hoarding isn't about just buying $3000 worth of hard drives just for posting them here. What's interesting is what you do with your storage. If you just have 1TB of storage but you do something freakin' cool with it, what you can share here is way more important than someone buying 30TB of storage and never post again here. Please, focus on what we love, the DATA, not the storage medium, please focus on projects, on archiving, on digital preservation. Thanks. Post inspired by u/Nooco24"},
{"Title": "Internet Archive justifies its vast 'copyright infringing' National Emergency Library of 1.4 million books by pointing out that libraries are closed", "Author": "u/Rincey_nz", "Content": "No content"},
{"Title": "My mother just passed away. She wrote extensively on this website. What can I do to archive everything she wrote?", "Author": "u/anthonyridad", "Content": "Hey guys, my mother just passed away a few days ago from heart surgery. I always knew that she used to write in this one website. She has around 1400 entries that I want to archive, on the off chance that the website goes down. What's the best way to save her articles and stuff? I want to get around to reading them one day. Here's a link to her stuff: https://www.mylot.com/ridingbet/posts I tried using archive.org, but it only saves the main URL. Thanks in advance. :)"},
{"Title": "Office Depot offered me a free charcoal grill with my hard drive purchase", "Author": "u/mrtramplefoot", "Content": "No content"},
{"Title": "Bounty: $1000USD (keeps updating). Help me find the whole videotape of Donald Trump on The Oprah Winfrey Show, 25th of April, 1988. Season 3, Episode 5 (60 min. episode). Saw it on Facebook back in 2015. Then, it vanished. I haven't found it after that. Help is greatly appreciated!", "Author": "u/trycoconutoil", "Content": "ACTUAL BOUNTY: $1310USD https://bountyhunters.world/ TRANSCRIPTS:  $100USD Must be copy of the original Journal Graphics transcript I created a Discord for anyone interested: https://discord.gg/JD8Je3v Sources that are relevant for the hunt: https://thetvdb.com/series/the-oprah-winfrey-show/allseasons/official (air date) 3 min. part from the original video https://www.reddit.com/r/politicalfactchecking/comments/4nt6iw/donald_trump_republicans_are_dumb_why_did_this/ (Frustrated Redditors) https://www.youtube.com/watch?v=AZtfSIgsH1o (some YouTube video that may not be important, but the comments on it are relevant) http://www.oprah.com/own-oprahshow/what-donald-trump-told-oprah-about-his-presidential-hopes-video (an old link that might have shown the full episode in the past) interesting article (Interesting article on the disappearance of the video.) Another Reddit post by someone else asking for the same episode. A Reddit post about the missing episode, and what OP of this post thinks about it . Hopefully, any of you got it :) Post (over 1000 upvotes) https://www.reddit.com/r/DataHoarder/comments/j2xd4r/bounty_300usd_help_me_find_the_whole_videotape_of/ (locked cause of some Redditors not following community guidelines, I respect that, hence why I made this sub.) I'll literally give the person who can find it and send it to me $1000USD (if you want). I'll give it through cryptocurrency, anonymous PayPal system, product(s), or other means if that is desired. There are several people from Reddit who help give more to raise the bounty. That's why it's updating. NOTE! Ignore the images and fact-checker sites. Those are red herrings and so points towards something different from the year 1998. I want the entire video from The Oprah Winfrey Show, 25th of April, 1988, season 3, Episode 5, 60 minutes of the show. DM me here on Reddit or post it publically. And, the episode does not exist on The Oprah Winfrey Show: 20th Anniversary Collection. ( https://www.amazon.com/dp/B000B91N3S/ref=nav_timeline_asin?_encoding=UTF8&psc=1 ). And also not here: http://www.oprah.com/own-oprahshow/what-donald-trump-told-oprah-about-his-presidential-hopes-video (I even requested a transcript) **Since some ask. Why do I want it?** I want it cause it's been suppressed on the internet, and various sites and people tell me what I saw and heard didn't happen, and it bothers me. Been bothering me for years. I want to prove them wrong by showing the people this episode. I do not have any political intentions; I want the truth. 3.10.20 UPDATE. More useful resources: Identify who has the Marion Stokes collection and ask them. Try asking people at LostMediaWiki In LA and NY, there are TV libraries where I can requestâ€¦ NY Small town libraries? Reach out to the team responsible for producing the new Oprah Winfrey podcast I posted to LostemediaWIKI (might come valuable information there) https://www.facebook.com/trumpvideogone (a page someone made on Facebook about finding this video, 2017) This one is interesting. Just going to leave this here: https://en.wikipedia.org/wiki/Streisand_effect Good luck. Add: I see my posts are being either locked or removed on different subreddits. If you have any idea where to post that would help! UPDATE: 9:35 PM (UTC) 10.10.20 25th, April 1988 on the Oprah Winfrey Show. Production company WLS-TV. Harpo productions should have the transcripts, cause they bought the inventory of a prior transcript company. I made a comment here: https://www.tvtime.com/en/show/75010/episode/665033# I contacted an archivist from OWN and he said they only give it to commerce and airing. They have it. I won't stop. If someone here wants to use it for broadcast or for commerce's sake. DM me and I can give you contact to the right person, who is responsible for this archive. I could not get it because they don't distribute for\"Personal viewing\". I created a Discord for anyone interested: https://discord.gg/JD8Je3v"},
{"Title": "I can't wait to buy this on eBay for $100 in a few years", "Author": "u/smacksaw", "Content": "No content"},
{"Title": "This fits here.", "Author": "u/peaceman12824", "Content": "No content"},
{"Title": "2500 Dos Games", "Author": "u/dutchsingh", "Content": "No content"},
{"Title": "Remember when YouTube said security and hacking videos would/could be removed? This is why we hoard.", "Author": "u/-this-guy-fucks-", "Content": "No content"},
{"Title": "Wife's B'Day Present, She Knows Me So Well", "Author": "u/slider383", "Content": "No content"},
{"Title": "A major lawsuit against the nonprofit Internet Archive threatens the future of all libraries. Big publishers are suing to cut off librariesâ€™ ownership and control of digital books, opening new paths for censorship. Oral arguments are on March 20.", "Author": "u/BananaBus43", "Content": "No content"},
{"Title": "$700 kitty butt warmer", "Author": "u/blueskyn01se", "Content": "No content"},
{"Title": "Did you know that at the Internet Archive a light blinks on one of their servers every time you use their collections?", "Author": "u/storytracer", "Content": "No content"},
{"Title": "Zippyshare is shutting down", "Author": "u/giratina143", "Content": "No content"},
{"Title": "relatable to you guys?", "Author": "u/MysteriousK69420", "Content": "No content"},
{"Title": "Fujifilm refuses to pay ransomware demand, relies on backups", "Author": "u/razeus", "Content": "No content"},
{"Title": "ArchiveTeam has saved over 10.8 BILLION Reddit links so far. We need YOUR help running ArchiveTeam Warrior to archive subreddits before they're gone indefinitely after June 12th!", "Author": "u/BananaBus43", "Content": "ArchiveTeam has been archiving Reddit posts for a while now, but we are running out of time. So far, we have archived 10.81 billion links, with 150 million to go . Recent news of the Reddit API cost changes will force many of the top 3rd party Reddit apps to shut down. This will not only affect how people use Reddit, but it will also cause issues with many subreddit moderation bots which rely on the API to function. Many subreddits have agreed to shut down for 48 hours on June 12th, while others will be gone indefinitely unless this issue is resolved. We are archiving Reddit posts so that in the event that the API cost change is never addressed, we can still access posts from those closed subreddits. Here is how you can help: Choose the \"host\" that matches your current PC, probably Windows or macOS Download ArchiveTeam Warrior In VirtualBox, click File > Import Appliance and open the file. Start the virtual machine. It will fetch the latest updates and will eventually tell you to start your web browser. Once youâ€™ve started your warrior: Go to http://localhost:8001/ and check the Settings page. Choose a username â€” weâ€™ll show your progress on the leaderboard. Go to the \"All projects\" tab and select ArchiveTeamâ€™s Choice to let your warrior work on the most urgent project. (This will be Reddit). Alternative Method: Docker Download Docker on your \"host\" (Windows, macOS, Linux) Follow the instructions on the ArchiveTeam website to set up Docker When setting up the project container, it will ask you to enter this command: docker run -d --name archiveteam --label=com.centurylinklabs.watchtower.enable=true --restart=unless-stopped [image address] --concurrent 1 [username] Make sure to replace the [image address] with the Reddit project address (removing brackets): atdr.meo.ws/archiveteam/reddit-grab Also change the [username] to whatever you'd like, no need to register for anything. More information about running this project: Information about setting up the project ArchiveTeam Wiki page on the Reddit project ArchiveTeam IRC Channel for the Reddit Project (#shreddit on hackint) There are a lot more items that are waiting to be queued into the tracker (approximately 758 million), so 150 million is not an accurate number. This is due to Redis limitations - the tracker is a Ruby and Redis monolith that serves multiple projects with around hundreds of millions of items. You can see all the Reddit items here . The maximum concurrency that you can run is 10 per IP (this is stated in the IRC channel topic). 5 works better for datacenter IPs. Information about Docker errors: If you are seeing RSYNC errors: If the error is about max connections (either -1 or 400), then this is normal. This is our (not amazingly intuitive) method of telling clients to try another target server (we have many of them). Just let it retry, it'll work eventually. If the error is not about max connections, please contact ArchiveTeam on IRC. If you are seeing HOSTERRs, check your DNS. We use Quad9 for our containers. If you need support or wish to discuss, contact ArchiveTeam on IRC Information on what ArchiveTeam archives and how to access the data (from u/rewbycraft ): We archive the posts and comments directly with this project. The things being linked to by the posts (and comments) are put in a queue that we'll process once we've got some more spare capacity. After a few days this stuff ends up in the Internet Archive's Wayback Machine. So, if you have an URL, you can put it in there and retrieve the post. (Note: We save the links without any query parameters and generally using permalinks, so if your URL has ?<and other stuff> at the end, remove that. And try to use permalinks if possible.) It takes a few days because there's a lot of processing logic going on behind the scenes. If you want to be sure something is archived and aren't sure we're covering it, feel free to talk to us on IRC. We're trying to archive literally everything. IMPORTANT: Do NOT modify scripts or the Warrior client! Edit 4: Weâ€™re over 12 billion links archived. Keep running the warrior/Docker during the blackout we still have a lot of posts left. Check this website to see when a subreddit goes private. Edit 3: Added a more prominent link to the Reddit IRC channel. Added more info about Docker errors and the project data. Edit 2: If you want check how much you've contributed, go to the project tracker website , press \"show all\" and type ctrl/cmd - F (find in page on mobile), and search your username. It should show you the number of items and the size of data that you've archived. Edit 1: Added more project info given by u/signalhunter ."},
{"Title": "The 276TB monster under the bed", "Author": "u/spazatk", "Content": "No content"},
{"Title": "introducing hard drive fold", "Author": "u/Astavicious", "Content": "No content"},
{"Title": "I built a book scanner and scanned all the yearbooks at a school", "Author": "u/camwow13", "Content": "No content"},
{"Title": "Reject the \"EARN IT Act\" (s. 3398) which threatens free speech, encryption, privacy, and the nation's cybersecurity.", "Author": "u/peoplearemean78", "Content": "No content"},
{"Title": "Hackers leak 37GB of Microsoft's source code (Bing, Cortana and more)", "Author": "u/harrro", "Content": "No content"},
{"Title": "Due to the new Audacity Terms of Service, I present 31 versions of Audacity and Github source code for 18 versions", "Author": "u/coasterghost", "Content": "No content"},
{"Title": "\"Ever wonder what 200PB of tape looks like?\"", "Author": "u/equalunique", "Content": "No content"},
{"Title": "On the zeroth day of Christmas, my true love sent to me eight 8TB drives.", "Author": "u/YourMJK", "Content": "No content"},
{"Title": "Please stop posting photos of your hard drives.", "Author": "u/Nooco24", "Content": "Hey, quick reminder, posting pictures of a bunch of HDDs you just bought ISN'T interesting, it's boring and I'm tired of seeing them all, I'm tired of seeing \"Am I part of this now?\" or \"Am I doing this the right way?\" posts. It's not because you have 100TB free of storage on your server that you are a data hoarder, and there is no \"good way to do this\". Data hoarding isn't about just buying $3000 worth of hard drives just for posting them here. What's interesting is what you do with your storage. If you just have 1TB of storage but you do something freakin' cool with it, what you can share here is way more important than someone buying 30TB of storage and never post again here. Please, focus on what we love, the DATA, not the storage medium, please focus on projects, on archiving, on digital preservation. Thanks. Post inspired by u/Nooco24"},
{"Title": "Internet Archive justifies its vast 'copyright infringing' National Emergency Library of 1.4 million books by pointing out that libraries are closed", "Author": "u/Rincey_nz", "Content": "No content"},
{"Title": "My mother just passed away. She wrote extensively on this website. What can I do to archive everything she wrote?", "Author": "u/anthonyridad", "Content": "Hey guys, my mother just passed away a few days ago from heart surgery. I always knew that she used to write in this one website. She has around 1400 entries that I want to archive, on the off chance that the website goes down. What's the best way to save her articles and stuff? I want to get around to reading them one day. Here's a link to her stuff: https://www.mylot.com/ridingbet/posts I tried using archive.org, but it only saves the main URL. Thanks in advance. :)"},
{"Title": "Office Depot offered me a free charcoal grill with my hard drive purchase", "Author": "u/mrtramplefoot", "Content": "No content"},
{"Title": "Bounty: $1000USD (keeps updating). Help me find the whole videotape of Donald Trump on The Oprah Winfrey Show, 25th of April, 1988. Season 3, Episode 5 (60 min. episode). Saw it on Facebook back in 2015. Then, it vanished. I haven't found it after that. Help is greatly appreciated!", "Author": "u/trycoconutoil", "Content": "ACTUAL BOUNTY: $1310USD https://bountyhunters.world/ TRANSCRIPTS:  $100USD Must be copy of the original Journal Graphics transcript I created a Discord for anyone interested: https://discord.gg/JD8Je3v Sources that are relevant for the hunt: https://thetvdb.com/series/the-oprah-winfrey-show/allseasons/official (air date) 3 min. part from the original video https://www.reddit.com/r/politicalfactchecking/comments/4nt6iw/donald_trump_republicans_are_dumb_why_did_this/ (Frustrated Redditors) https://www.youtube.com/watch?v=AZtfSIgsH1o (some YouTube video that may not be important, but the comments on it are relevant) http://www.oprah.com/own-oprahshow/what-donald-trump-told-oprah-about-his-presidential-hopes-video (an old link that might have shown the full episode in the past) interesting article (Interesting article on the disappearance of the video.) Another Reddit post by someone else asking for the same episode. A Reddit post about the missing episode, and what OP of this post thinks about it . Hopefully, any of you got it :) Post (over 1000 upvotes) https://www.reddit.com/r/DataHoarder/comments/j2xd4r/bounty_300usd_help_me_find_the_whole_videotape_of/ (locked cause of some Redditors not following community guidelines, I respect that, hence why I made this sub.) I'll literally give the person who can find it and send it to me $1000USD (if you want). I'll give it through cryptocurrency, anonymous PayPal system, product(s), or other means if that is desired. There are several people from Reddit who help give more to raise the bounty. That's why it's updating. NOTE! Ignore the images and fact-checker sites. Those are red herrings and so points towards something different from the year 1998. I want the entire video from The Oprah Winfrey Show, 25th of April, 1988, season 3, Episode 5, 60 minutes of the show. DM me here on Reddit or post it publically. And, the episode does not exist on The Oprah Winfrey Show: 20th Anniversary Collection. ( https://www.amazon.com/dp/B000B91N3S/ref=nav_timeline_asin?_encoding=UTF8&psc=1 ). And also not here: http://www.oprah.com/own-oprahshow/what-donald-trump-told-oprah-about-his-presidential-hopes-video (I even requested a transcript) **Since some ask. Why do I want it?** I want it cause it's been suppressed on the internet, and various sites and people tell me what I saw and heard didn't happen, and it bothers me. Been bothering me for years. I want to prove them wrong by showing the people this episode. I do not have any political intentions; I want the truth. 3.10.20 UPDATE. More useful resources: Identify who has the Marion Stokes collection and ask them. Try asking people at LostMediaWiki In LA and NY, there are TV libraries where I can requestâ€¦ NY Small town libraries? Reach out to the team responsible for producing the new Oprah Winfrey podcast I posted to LostemediaWIKI (might come valuable information there) https://www.facebook.com/trumpvideogone (a page someone made on Facebook about finding this video, 2017) This one is interesting. Just going to leave this here: https://en.wikipedia.org/wiki/Streisand_effect Good luck. Add: I see my posts are being either locked or removed on different subreddits. If you have any idea where to post that would help! UPDATE: 9:35 PM (UTC) 10.10.20 25th, April 1988 on the Oprah Winfrey Show. Production company WLS-TV. Harpo productions should have the transcripts, cause they bought the inventory of a prior transcript company. I made a comment here: https://www.tvtime.com/en/show/75010/episode/665033# I contacted an archivist from OWN and he said they only give it to commerce and airing. They have it. I won't stop. If someone here wants to use it for broadcast or for commerce's sake. DM me and I can give you contact to the right person, who is responsible for this archive. I could not get it because they don't distribute for\"Personal viewing\". I created a Discord for anyone interested: https://discord.gg/JD8Je3v"},
{"Title": "I can't wait to buy this on eBay for $100 in a few years", "Author": "u/smacksaw", "Content": "No content"},
{"Title": "This fits here.", "Author": "u/peaceman12824", "Content": "No content"},
{"Title": "2500 Dos Games", "Author": "u/dutchsingh", "Content": "No content"},
{"Title": "Remember when YouTube said security and hacking videos would/could be removed? This is why we hoard.", "Author": "u/-this-guy-fucks-", "Content": "No content"},
{"Title": "Wife's B'Day Present, She Knows Me So Well", "Author": "u/slider383", "Content": "No content"},
{"Title": "A major lawsuit against the nonprofit Internet Archive threatens the future of all libraries. Big publishers are suing to cut off librariesâ€™ ownership and control of digital books, opening new paths for censorship. Oral arguments are on March 20.", "Author": "u/BananaBus43", "Content": "No content"},
{"Title": "$700 kitty butt warmer", "Author": "u/blueskyn01se", "Content": "No content"},
{"Title": "Recommendations for changing my 40 drive setup", "Author": "u/Professional_Lychee9", "Content": "I currently have a 45bay supermicro SAS2 JBOD enclosure with 40x 16TB Exos X16 enterprise drives in it. I would like to upgrade to SAS3. I have 2 options: Keep all drives together and use a single server to connect to it Split the drives into 2-4 enclosures and connect them each to a server I have a kubernetes cluster with 3x R630s and 1x R730. and looking to use Rook/Ceph to surface these drives to Minio. IT is currently connected to the R730 via an LSI SAS3 16e card. what enclosures might you recommend? I was looking at the D60 but cant tell if it will take SATA drives. I also cant seem to find smaller enclosures (12-15 bays) that dont also need CPU/Mobo/RAM. just want a JBOD enclosure"},
{"Title": "Moving large files online?", "Author": "u/ParticularTreacle446", "Content": "I need to send a large number of files over the internet without the need of buying hardware. i came accross this website has anyone used it? if so please share your thoughts. tempfile.me"},
{"Title": "I know iCloud is not considered a backup but..", "Author": "u/askinghawking", "Content": "I know iCloud is more considered as a sync service but do you think I can use it as a backup how I do it? I have a Mac that is only a backup tool. Means that I get all photos and documents to the Mac + iCloud there from my iphone - with a different iCloud account. So it couldnâ€˜t happen that I delete a picture from my phone and is deleted everywhere. Would you consider it a cloud backup or would you use something else in addition? (Or course, it is also all backup up on a ssd to follow 3-2-1)"},
{"Title": "what are your predictions of IA lawsuit and is there any website archiving the archive?", "Author": "u/legz2006", "Content": "saw vid more muta(someordinarygames) about the lawsuit, something ive been keeping up with adn was wondering, will all that data just vanish?"},
{"Title": "Quiet HL15 Build help", "Author": "u/NextRedditAccount0", "Content": "I'm planning to pick up an HL15 to replace my Synology DS2415+. My current DS2415+ is not the quietest thing in the world but the noise is manageable considering its a few feet away from my desk and bed. I'm also running 8x8TB shucked WD easystore drives in there. My plan is to pickup the HL15 and move all the drives over AND purchase some new larger drives since my pool is roughly 70% full and i'm planning to fill it up some more. Some questions. How loud is the HL15? I'm planning on picking up the HL15 with the Noctua fans option. Any recommendations for quiet or quietish 12tb+ drives? I'm going to grab them from serverpartdeals.com I saw some videos with PWL drives and that thumping would drive me crazy. I hope the HL15 has some form of noise damping if PWL drives are my only option. Would love some feedback before I hit the buy button on this one."},
{"Title": "RAID 5 controller for Win 11 Workstation", "Author": "u/Heinrich_v_Schimmer", "Content": "As a photographer, I generate more than a TByte of data per month and therefore have a RAID 5 on my work system. The controller used for this RAID runs into problems with the energy saving modes of Windows; when the system wakes up from sleep mode, the drive is no longer visible in Explorer until a complete reboot. Does anyone know of a RAID 5 controller that can cope with the energy saving modes of Windows 11? PS: Software (OS) RAIDs are not an option."},
{"Title": "Using a homelab as a cache server for my cloud storage?", "Author": "u/sqenixs", "Content": "I'm looking to set up a homelab pc where it will have about 10TB of storage.  I also have a backblaze B2 bucket I will use for cloud storage.  I would like to use the homelab PC as a sort of cache drive where when I am at home I can use it, and when I am away I can connect to my backblaze b2 bucket directly from my laptop.  For example, I could open a word doc, make a few changes, and it would sync automatically to my cloud and NAS.  However the cloud would have more storage so I envision a scenario where my laptop would run out of space so I would keep some files \"cloud only\" and they would be on the NAS when at home or in the backblaze cloud when away.  However, some files I might want to only keep in the cloud and not on my laptop or NAS.  Is this possible?"},
{"Title": "looking for something similar to Qsync", "Author": "u/niloproject", "Content": "Hi everyone, I could use some help. I've put together a NAS for my business and I was wondering if there were any alternatives to Qnap Qsync that would work with Truenas. (I also asked this in the truenas server but figured it would hurt to ask here too). The functionality in specific that I'm looking for is the ability to create users that can then establish a two way sync with the server, but similar to with Qsync, there would ideally be a smart sync that would only download what users actually need locally and not anything that isn't necessary to their work so they're not having to deal with terabytes of storage being synced.  They could then work directly on the mounted drive from their machines and anything they create will be synced to everyone and they won't have to worry about manually downloading/duplicating files on the server. When building this I didn't realize how hard it would be to find a tool for this, and everything I've found just isn't very useful. Syncthing is not helpful, its not smart and would require everyone mirror 20tb of files (unless im misunderstanding what it does.) SMB sharing technically works but is too slow for people to work off of for our purposes as our files have huge datarates (multilayer EXR sequences, 1gb a frame kind of stuff).  I'm also experimenting with Google Workspace to see if that works any better for this but I'd figure I'd ask for help before I go down that road. If any of you have any experience with this or could maybe point me to a client-side software that can manage client side downloads from the server in a smart way like Qsync that would be amazing."},
{"Title": "Need recommendations for USB3 or eSata external drive to connect to router for FTP server.", "Author": "u/mufasis", "Content": "Whatâ€™s going on data hoarders. Iâ€™m looking for a network drive I can connect to my router, USB3 or eSata. Will be used to store stuff on my home network as well as serve files by FTP, mostly music files and projects I need to share. Also what would be better USB3 or eSata? Looking for speed and reliability, thanks!"},
{"Title": "External Hard Drive for High Humidity Conditions", "Author": "u/BrEichen", "Content": "I'm gonna be living in a peat swamp forest in Borneo for a year and am looking to bring a hard drive to store movies, TV, and photos on as I will not have reliable internet access. Where I'll be the humidity is so high clothes and technology are stored in plastic bins w/ silica packets when not in use. I was wondering if anyone could recommend an external hard drive that could hold up to these conditions (if such a thing even exists). Sorry if this is a dumb or already answered question, pretty new to this stuff. Thanks!"},
{"Title": "YouTube is testing server-side ad injection into video streams (per SponsorBlock Twitter)", "Author": "u/ThePixelHunter", "Content": "No content"},
{"Title": "Is there a way to sync between devices using a usb cable?", "Author": "u/vrtqlwlpl", "Content": "Currently there is a programme called Syncthing which enables the wireless syncing of files between devices. It does not store data on the servers - rather, two devices have to be online at the same time for data to be synced. It's been a while so I don't remember the mechanisms through which it operates There are a couple of downsides. First being that wireless syncing is slow, second being that errors are more common with wireless syncing (from my experiences). Imagine a folder and imagine that every day I make a few changes to the files within it. I need the ability to sync all the changes within the folder for 2 devices. If I do not sync them, I would have to comb the entire folder for every change and apply them one by one OR delete the folder and paste the updated folder from my PC. It is around 50GB so it would take a while to paste I'd probably sync twice a week"},
{"Title": "Increasing the redundancy/error-correction of a partition?", "Author": "u/metal_wires", "Content": "I have a 2TB external HDD, and I have created a 256GB partition which is an encrypted VeraCrypt volume. I was thinking, that since I have more than 1.5TB space leftover, is it possible to use some of that remaining space to add parity data or error correction for this VeraCrypt partition, to protect against bit rot? I have tried searching all over the Internet for a tool that would let me create a \"error correction partition\" which stores ECC for the VeraCrypt partition, but I cannot find it. At most, I can find references to RAID 5, but I don't have multiple drives, just this one."},
{"Title": "I bought a sas controller LSI 9240-8i SAS SATA RAID Controller Card drives", "Author": "u/Unusual-Ingenuity831", "Content": "I need help installing it i bought this to use my sas hard drive but i have plugged it in and everything and it still wont work so i need help"},
{"Title": "How to download the video content from Microsoft Lean On-demand training?", "Author": "u/HardLearner01", "Content": "How to download the video content of this site? DP-100 Design a machine learning solution (1 of 6) | Microsoft Learn"},
{"Title": "NVMe SSD based storage computer", "Author": "u/aes100", "Content": "If you were to build a storage computer based on NVMe SSDs, how would you do it? Nor datarate, nor storage space is priority. Just sheer number of NVMe SSDs... and you hate cables. Build#1: AMD Threadripper with PCIe to M.2 adapter cards. Because I think that is the only motherboard that has more than one or two PCIe slots. But Threadripper is overkill expensive for just storage and not worth it. I didn't do research on Intel side. Build#2: Get a server motherboard with lots of SATA ports and lots of SATA to M.2 adapters. Put the NVMe's in, and connect them to SATA ports. But you don't like cables. Build#3: Get a LattePanda Mu based on Intel N100 which they advertise a NAS carrier but I don't know if it exists yet or not. Or get a FriendlyELEC CM3588 based on Rockchip. You are free of cables, but are you gonna regret accessing the data on there because it is slow? Or is it actually fast enough to, I don't know, watch a video from it? What do you think about the above builds? What downs and ups do you think they have? How would you do it? Why?"},
{"Title": "Can storage have a maximum amount of storage space?", "Author": "u/Card_Boxy", "Content": "Sorry if this question was worded weirdly, but I am looking to buy a new hard drive for my pre-built Lenovo IdeaCentre Gaming 5 17IAB7. I noticed that, according to the specs on the Product Specifications Reference it could only fit 2 3.5\" HDDs with a maximum of 2 TB, and 1 M.2 SSD with a max of 1 TB. What would happen if I were to buy and install a HDD / SSD larger than the reported maximum on the specifications sheet? Here's the actual pages I was looking at. https://psref.lenovo.com/Detail/IdeaCentre/IdeaCentre_Gaming_5_17IAB7?M=90T00003US https://psref.lenovo.com/Product/IdeaCentre/IdeaCentre_Gaming_5_17IAB7 Thanks in advance for your times."},
{"Title": "Need help figuring how do archive part of a soon-to-die site", "Author": "u/Mode7GFX", "Content": "I posted this on stack overflow and they called me stupid (I am) and locked the question when someone was in the middle of actually helping me so I'm asking here because it's probably a more welcoming community for this sort of thing. https://prcm.jp/list/akb48%20%E3%83%97%E3%83%AA So this (formerly) huge Japanese image sharing site is shutting down in 2 weeks and my sister's begging me to archive at least some of it. I was trying to find a python script to automate it, cuz it will take me forever going one by one on tens of thousands of images, but I can't seem to find one that can archive this sort of website. It only goes by pages of 9 and the thumbnail images are shrunk down a ton so you have to click on the image twice to open the full size. Luckily it seems like every thumbnail image shares a name with the source image, but with an added suffix of the preview size, so I imagine it would be possible to have the script delete the underscore and re-add the .jpeg extension. As for going through pages, I'm not so sure on, but if I can input a list of URLs and just batch copy the original URL adding every page number (I know how to do this without a script), I could just use that. I was only going to download AKB48 related images, because most of these images aren't available anywhere else online, and soon won't be anywhere period. They're all old and thus fairly small, so I'm not too worried about it taking a long time to download. If anyone can direct me to a script that can do this or has some other mass image downloading method you're aware of, please let me know. Thanks!"},
{"Title": "Any hardware Raid 4-bay enclosures out there that ARE \"power disable\" compatible?", "Author": "u/09Klr650", "Content": "Perhaps my search-fu is lacking but all the small 4-bay hardware raid enclosures I find specifically state they are not Power Disable compatible. Before I go the whole \"kapton tape in pin 3\" route is there a manufacturer/model out there that is capable? Without costing an arm, leg and earlobe preferably.  This will be my first such setup and I want to KISS for the moment. Figure that (4) 12TB Reman HGST would be a good start. At Raid 5 that's approx 36TB. Or approximately 4x all my current motley collection of externals combined. Thanks."},
{"Title": "Best way to transfer data from one external ssd to another external ssd?", "Author": "u/alucvrdofficial", "Content": "Not sure if this is the right subreddit to post in, but as the title states, I purchased another external ssd, and I want to upload all of the data on my current ssd to this new one. Does anybody have suggestions for how to go about this? Can I just plug em both in and drag the folders over? Also, if this isn't the right place to ask, does anyone have suggestions on where to look?"},
{"Title": "Extension cable to disable Power Disable Feature on HDD?", "Author": "u/feedmememes", "Content": "I just purchased this HDD of Amazon: https://www.amazon.com/gp/product/B08T3PBV57/ref=ppx_yo_dt_b_asin_title_o00_s00?ie=UTF8&th=1 However it did not come with the power extension cable mentioned in the product description. I have purchased this drive before and it would not work without that cable, so it is necessary for me to get one for the HDD to work. My question is what would I search for the get the correct part? Would any sata-to-sata power extension work or would it need to have some special spec? I've read a 4-pin Molex to SATA connector might be the solution, but I would like to confirm before purchasing because I am dumb!! Something like this for example: https://www.amazon.com/Cable-Matters-3-Pack-Molex-Power/dp/B00STNUB04/ref=sr_1_2?crid=B3ZGL1OM3IR1&dib=eyJ2IjoiMSJ9.f9jru-Gy4n_1s6sxvx6tecrDs0kKyQc_KXFNnctPSyGpgk3wiTyXQRVjv8latROCZFMth35JZj6AzNa09K_WespKmumHEQjA1Qn9tuzw8MBtUCX9qyoxPfAfS1R9c3JLDgIR1NHveuutkNGIKTrnouyP5DBd2e2yp3U9bkzTz5b_jPWn4NWf9pusLWEVS3OvqcRsb74HYfa7bE5vtR5EWmod_f3B-fJJnotQO7qTLbE.4mkQvoblx472TPQkw3pPm3UzP8ULyibyZTNH-zpyBg4&dib_tag=se&keywords=Sata-to-sata+power+adapter+cable&qid=1718302912&sprefix=sata-to-sata+power+adapter+cable+%2Caps%2C84&sr=8-2 Or if there are any other solutions that would be helpful too. Sorry for noob question and thank you in advanced for any help!"},
{"Title": "Need help with magazine scanning - resolution vs size", "Author": "u/MyBallsSmellFruity", "Content": "So I'm scanning magazines at 300 dpi.  Any less, and the images look kind of cruddy.  Of course, the image size is pretty large, so I'm having to batch convert them to manually make the image height 1200 (width is auto calculated). Is this pretty much the only way to get both decent quality and an acceptable image size?  The actual filesize difference in the PDFs is pretty noticeable - and significant when doing hundreds of magazines. Or is there an easier way that I just haven't stumbled across?"},
{"Title": "Attempting to digitize some Beta tapes, but getting this rainbow banding through the picture.", "Author": "u/hmhsbritannic12", "Content": "No content"},
{"Title": "I'm 98% of the way there â€”Â help me across the finish line! (Syncing help)", "Author": "u/740990929974739", "Content": "MY FRIENDS! I'm finally doing it. Bought a 12 TB External HDD + Backblaze (Computer Backup, not B2) so I can back up my scattered external drives to the big HDD and have that backup automatically to Backblaze. That should allow me, I hope, to finally delete files from my computer's SSD as well as my 3 other external SSDs that are all clogged up with old photos and videos. (I'm a photographer + videographer + audio engineer so I produce a lot of files â€”Â I'm out of working space and it's ruining my life!) My issue: I backed up 2/3 external SSDs to the big HDD about a month ago, but since then, new files have been added. I want to add only the new files to the HDD, without having to re-copy the entire folder, since I suspect that would also mess with Backblaze's file detection. I think this means I need a file syncing and/or deduplication program. I've seen FreeFileSync as a recommended service. Is this what I need? Thanks so much for your help!"},
{"Title": "Upgrading and future-proofing my Plex Media server & library setup, looking for advice", "Author": "u/Reasonable_Jelly9435", "Content": "TL;DR - Nearing storage cap on my current external HDD. Want to upgrade storage and reasonably future-proof as best as possible. Considering changing my setup from PC (Plex Media Server) + external HDD (media storage), to a mini PC (Plex) + DAS. Could use advice. Hi, I've done some research on upgrading my current Plex / general media storage setup and I've come up with a plan that looks something like the below. Please share any constructive criticisms, tips, things I might have overlooked, etc. I want to do this right the first time around. Use case: 1-2 simultaneous 1080p streams, most clients will be able to directly play my mostly HEVC (h.265) content without needing to transcode. Not all, though, so transcoding option is nice. I have Plex Pass so HW transcoding is also an option. My current setup: PC [Plex Media Server] External Hard drive (12TB) [ ~10TB personal media ] What I'm thinking of moving to: Mini PC ( Beelink Mini S12 ) [Plex Media Server] DAS ( 4-bay QNAP ) 3.5 12TB HDD (shuck my current drive) [ ~10TB personal media ] 3.5 12TB HDD (buy a new one) [ RAID backup? ] empty (add a 12TB in future for more storage?) empty (add a 12TB in future for more storage?) If this looks good, I have a bonus question: How do I safely move my content on my current 12TB external hard drive to the DAS? Is it as simple as shucking the drive and installing in the DAS? I've also read that setting up RAID will wipe the drives; will it be necessary to buy a third 12TB drive before moving my media? Thanks in advance!"},
{"Title": "Did you know that at the Internet Archive a light blinks on one of their servers every time you use their collections?", "Author": "u/storytracer", "Content": "No content"},
{"Title": "Zippyshare is shutting down", "Author": "u/giratina143", "Content": "No content"},
{"Title": "relatable to you guys?", "Author": "u/MysteriousK69420", "Content": "No content"},
{"Title": "Fujifilm refuses to pay ransomware demand, relies on backups", "Author": "u/razeus", "Content": "No content"},
{"Title": "ArchiveTeam has saved over 10.8 BILLION Reddit links so far. We need YOUR help running ArchiveTeam Warrior to archive subreddits before they're gone indefinitely after June 12th!", "Author": "u/BananaBus43", "Content": "ArchiveTeam has been archiving Reddit posts for a while now, but we are running out of time. So far, we have archived 10.81 billion links, with 150 million to go . Recent news of the Reddit API cost changes will force many of the top 3rd party Reddit apps to shut down. This will not only affect how people use Reddit, but it will also cause issues with many subreddit moderation bots which rely on the API to function. Many subreddits have agreed to shut down for 48 hours on June 12th, while others will be gone indefinitely unless this issue is resolved. We are archiving Reddit posts so that in the event that the API cost change is never addressed, we can still access posts from those closed subreddits. Here is how you can help: Choose the \"host\" that matches your current PC, probably Windows or macOS Download ArchiveTeam Warrior In VirtualBox, click File > Import Appliance and open the file. Start the virtual machine. It will fetch the latest updates and will eventually tell you to start your web browser. Once youâ€™ve started your warrior: Go to http://localhost:8001/ and check the Settings page. Choose a username â€” weâ€™ll show your progress on the leaderboard. Go to the \"All projects\" tab and select ArchiveTeamâ€™s Choice to let your warrior work on the most urgent project. (This will be Reddit). Alternative Method: Docker Download Docker on your \"host\" (Windows, macOS, Linux) Follow the instructions on the ArchiveTeam website to set up Docker When setting up the project container, it will ask you to enter this command: docker run -d --name archiveteam --label=com.centurylinklabs.watchtower.enable=true --restart=unless-stopped [image address] --concurrent 1 [username] Make sure to replace the [image address] with the Reddit project address (removing brackets): atdr.meo.ws/archiveteam/reddit-grab Also change the [username] to whatever you'd like, no need to register for anything. More information about running this project: Information about setting up the project ArchiveTeam Wiki page on the Reddit project ArchiveTeam IRC Channel for the Reddit Project (#shreddit on hackint) There are a lot more items that are waiting to be queued into the tracker (approximately 758 million), so 150 million is not an accurate number. This is due to Redis limitations - the tracker is a Ruby and Redis monolith that serves multiple projects with around hundreds of millions of items. You can see all the Reddit items here . The maximum concurrency that you can run is 10 per IP (this is stated in the IRC channel topic). 5 works better for datacenter IPs. Information about Docker errors: If you are seeing RSYNC errors: If the error is about max connections (either -1 or 400), then this is normal. This is our (not amazingly intuitive) method of telling clients to try another target server (we have many of them). Just let it retry, it'll work eventually. If the error is not about max connections, please contact ArchiveTeam on IRC. If you are seeing HOSTERRs, check your DNS. We use Quad9 for our containers. If you need support or wish to discuss, contact ArchiveTeam on IRC Information on what ArchiveTeam archives and how to access the data (from u/rewbycraft ): We archive the posts and comments directly with this project. The things being linked to by the posts (and comments) are put in a queue that we'll process once we've got some more spare capacity. After a few days this stuff ends up in the Internet Archive's Wayback Machine. So, if you have an URL, you can put it in there and retrieve the post. (Note: We save the links without any query parameters and generally using permalinks, so if your URL has ?<and other stuff> at the end, remove that. And try to use permalinks if possible.) It takes a few days because there's a lot of processing logic going on behind the scenes. If you want to be sure something is archived and aren't sure we're covering it, feel free to talk to us on IRC. We're trying to archive literally everything. IMPORTANT: Do NOT modify scripts or the Warrior client! Edit 4: Weâ€™re over 12 billion links archived. Keep running the warrior/Docker during the blackout we still have a lot of posts left. Check this website to see when a subreddit goes private. Edit 3: Added a more prominent link to the Reddit IRC channel. Added more info about Docker errors and the project data. Edit 2: If you want check how much you've contributed, go to the project tracker website , press \"show all\" and type ctrl/cmd - F (find in page on mobile), and search your username. It should show you the number of items and the size of data that you've archived. Edit 1: Added more project info given by u/signalhunter ."},
{"Title": "The 276TB monster under the bed", "Author": "u/spazatk", "Content": "No content"},
{"Title": "introducing hard drive fold", "Author": "u/Astavicious", "Content": "No content"},
{"Title": "I built a book scanner and scanned all the yearbooks at a school", "Author": "u/camwow13", "Content": "No content"},
{"Title": "Reject the \"EARN IT Act\" (s. 3398) which threatens free speech, encryption, privacy, and the nation's cybersecurity.", "Author": "u/peoplearemean78", "Content": "No content"},
{"Title": "Did you know that at the Internet Archive a light blinks on one of their servers every time you use their collections?", "Author": "u/storytracer", "Content": "No content"},
{"Title": "Hackers leak 37GB of Microsoft's source code (Bing, Cortana and more)", "Author": "u/harrro", "Content": "No content"},
{"Title": "Zippyshare is shutting down", "Author": "u/giratina143", "Content": "No content"},
{"Title": "Due to the new Audacity Terms of Service, I present 31 versions of Audacity and Github source code for 18 versions", "Author": "u/coasterghost", "Content": "No content"},
{"Title": "relatable to you guys?", "Author": "u/MysteriousK69420", "Content": "No content"},
{"Title": "\"Ever wonder what 200PB of tape looks like?\"", "Author": "u/equalunique", "Content": "No content"},
{"Title": "Fujifilm refuses to pay ransomware demand, relies on backups", "Author": "u/razeus", "Content": "No content"},
{"Title": "On the zeroth day of Christmas, my true love sent to me eight 8TB drives.", "Author": "u/YourMJK", "Content": "No content"},
{"Title": "ArchiveTeam has saved over 10.8 BILLION Reddit links so far. We need YOUR help running ArchiveTeam Warrior to archive subreddits before they're gone indefinitely after June 12th!", "Author": "u/BananaBus43", "Content": "ArchiveTeam has been archiving Reddit posts for a while now, but we are running out of time. So far, we have archived 10.81 billion links, with 150 million to go . Recent news of the Reddit API cost changes will force many of the top 3rd party Reddit apps to shut down. This will not only affect how people use Reddit, but it will also cause issues with many subreddit moderation bots which rely on the API to function. Many subreddits have agreed to shut down for 48 hours on June 12th, while others will be gone indefinitely unless this issue is resolved. We are archiving Reddit posts so that in the event that the API cost change is never addressed, we can still access posts from those closed subreddits. Here is how you can help: Choose the \"host\" that matches your current PC, probably Windows or macOS Download ArchiveTeam Warrior In VirtualBox, click File > Import Appliance and open the file. Start the virtual machine. It will fetch the latest updates and will eventually tell you to start your web browser. Once youâ€™ve started your warrior: Go to http://localhost:8001/ and check the Settings page. Choose a username â€” weâ€™ll show your progress on the leaderboard. Go to the \"All projects\" tab and select ArchiveTeamâ€™s Choice to let your warrior work on the most urgent project. (This will be Reddit). Alternative Method: Docker Download Docker on your \"host\" (Windows, macOS, Linux) Follow the instructions on the ArchiveTeam website to set up Docker When setting up the project container, it will ask you to enter this command: docker run -d --name archiveteam --label=com.centurylinklabs.watchtower.enable=true --restart=unless-stopped [image address] --concurrent 1 [username] Make sure to replace the [image address] with the Reddit project address (removing brackets): atdr.meo.ws/archiveteam/reddit-grab Also change the [username] to whatever you'd like, no need to register for anything. More information about running this project: Information about setting up the project ArchiveTeam Wiki page on the Reddit project ArchiveTeam IRC Channel for the Reddit Project (#shreddit on hackint) There are a lot more items that are waiting to be queued into the tracker (approximately 758 million), so 150 million is not an accurate number. This is due to Redis limitations - the tracker is a Ruby and Redis monolith that serves multiple projects with around hundreds of millions of items. You can see all the Reddit items here . The maximum concurrency that you can run is 10 per IP (this is stated in the IRC channel topic). 5 works better for datacenter IPs. Information about Docker errors: If you are seeing RSYNC errors: If the error is about max connections (either -1 or 400), then this is normal. This is our (not amazingly intuitive) method of telling clients to try another target server (we have many of them). Just let it retry, it'll work eventually. If the error is not about max connections, please contact ArchiveTeam on IRC. If you are seeing HOSTERRs, check your DNS. We use Quad9 for our containers. If you need support or wish to discuss, contact ArchiveTeam on IRC Information on what ArchiveTeam archives and how to access the data (from u/rewbycraft ): We archive the posts and comments directly with this project. The things being linked to by the posts (and comments) are put in a queue that we'll process once we've got some more spare capacity. After a few days this stuff ends up in the Internet Archive's Wayback Machine. So, if you have an URL, you can put it in there and retrieve the post. (Note: We save the links without any query parameters and generally using permalinks, so if your URL has ?<and other stuff> at the end, remove that. And try to use permalinks if possible.) It takes a few days because there's a lot of processing logic going on behind the scenes. If you want to be sure something is archived and aren't sure we're covering it, feel free to talk to us on IRC. We're trying to archive literally everything. IMPORTANT: Do NOT modify scripts or the Warrior client! Edit 4: Weâ€™re over 12 billion links archived. Keep running the warrior/Docker during the blackout we still have a lot of posts left. Check this website to see when a subreddit goes private. Edit 3: Added a more prominent link to the Reddit IRC channel. Added more info about Docker errors and the project data. Edit 2: If you want check how much you've contributed, go to the project tracker website , press \"show all\" and type ctrl/cmd - F (find in page on mobile), and search your username. It should show you the number of items and the size of data that you've archived. Edit 1: Added more project info given by u/signalhunter ."},
{"Title": "Please stop posting photos of your hard drives.", "Author": "u/Nooco24", "Content": "Hey, quick reminder, posting pictures of a bunch of HDDs you just bought ISN'T interesting, it's boring and I'm tired of seeing them all, I'm tired of seeing \"Am I part of this now?\" or \"Am I doing this the right way?\" posts. It's not because you have 100TB free of storage on your server that you are a data hoarder, and there is no \"good way to do this\". Data hoarding isn't about just buying $3000 worth of hard drives just for posting them here. What's interesting is what you do with your storage. If you just have 1TB of storage but you do something freakin' cool with it, what you can share here is way more important than someone buying 30TB of storage and never post again here. Please, focus on what we love, the DATA, not the storage medium, please focus on projects, on archiving, on digital preservation. Thanks. Post inspired by u/Nooco24"},
{"Title": "The 276TB monster under the bed", "Author": "u/spazatk", "Content": "No content"},
{"Title": "Internet Archive justifies its vast 'copyright infringing' National Emergency Library of 1.4 million books by pointing out that libraries are closed", "Author": "u/Rincey_nz", "Content": "No content"},
{"Title": "introducing hard drive fold", "Author": "u/Astavicious", "Content": "No content"},
{"Title": "My mother just passed away. She wrote extensively on this website. What can I do to archive everything she wrote?", "Author": "u/anthonyridad", "Content": "Hey guys, my mother just passed away a few days ago from heart surgery. I always knew that she used to write in this one website. She has around 1400 entries that I want to archive, on the off chance that the website goes down. What's the best way to save her articles and stuff? I want to get around to reading them one day. Here's a link to her stuff: https://www.mylot.com/ridingbet/posts I tried using archive.org, but it only saves the main URL. Thanks in advance. :)"},
{"Title": "I built a book scanner and scanned all the yearbooks at a school", "Author": "u/camwow13", "Content": "No content"},
{"Title": "Office Depot offered me a free charcoal grill with my hard drive purchase", "Author": "u/mrtramplefoot", "Content": "No content"},
{"Title": "Reject the \"EARN IT Act\" (s. 3398) which threatens free speech, encryption, privacy, and the nation's cybersecurity.", "Author": "u/peoplearemean78", "Content": "No content"},
{"Title": "Bounty: $1000USD (keeps updating). Help me find the whole videotape of Donald Trump on The Oprah Winfrey Show, 25th of April, 1988. Season 3, Episode 5 (60 min. episode). Saw it on Facebook back in 2015. Then, it vanished. I haven't found it after that. Help is greatly appreciated!", "Author": "u/trycoconutoil", "Content": "ACTUAL BOUNTY: $1310USD https://bountyhunters.world/ TRANSCRIPTS:  $100USD Must be copy of the original Journal Graphics transcript I created a Discord for anyone interested: https://discord.gg/JD8Je3v Sources that are relevant for the hunt: https://thetvdb.com/series/the-oprah-winfrey-show/allseasons/official (air date) 3 min. part from the original video https://www.reddit.com/r/politicalfactchecking/comments/4nt6iw/donald_trump_republicans_are_dumb_why_did_this/ (Frustrated Redditors) https://www.youtube.com/watch?v=AZtfSIgsH1o (some YouTube video that may not be important, but the comments on it are relevant) http://www.oprah.com/own-oprahshow/what-donald-trump-told-oprah-about-his-presidential-hopes-video (an old link that might have shown the full episode in the past) interesting article (Interesting article on the disappearance of the video.) Another Reddit post by someone else asking for the same episode. A Reddit post about the missing episode, and what OP of this post thinks about it . Hopefully, any of you got it :) Post (over 1000 upvotes) https://www.reddit.com/r/DataHoarder/comments/j2xd4r/bounty_300usd_help_me_find_the_whole_videotape_of/ (locked cause of some Redditors not following community guidelines, I respect that, hence why I made this sub.) I'll literally give the person who can find it and send it to me $1000USD (if you want). I'll give it through cryptocurrency, anonymous PayPal system, product(s), or other means if that is desired. There are several people from Reddit who help give more to raise the bounty. That's why it's updating. NOTE! Ignore the images and fact-checker sites. Those are red herrings and so points towards something different from the year 1998. I want the entire video from The Oprah Winfrey Show, 25th of April, 1988, season 3, Episode 5, 60 minutes of the show. DM me here on Reddit or post it publically. And, the episode does not exist on The Oprah Winfrey Show: 20th Anniversary Collection. ( https://www.amazon.com/dp/B000B91N3S/ref=nav_timeline_asin?_encoding=UTF8&psc=1 ). And also not here: http://www.oprah.com/own-oprahshow/what-donald-trump-told-oprah-about-his-presidential-hopes-video (I even requested a transcript) **Since some ask. Why do I want it?** I want it cause it's been suppressed on the internet, and various sites and people tell me what I saw and heard didn't happen, and it bothers me. Been bothering me for years. I want to prove them wrong by showing the people this episode. I do not have any political intentions; I want the truth. 3.10.20 UPDATE. More useful resources: Identify who has the Marion Stokes collection and ask them. Try asking people at LostMediaWiki In LA and NY, there are TV libraries where I can requestâ€¦ NY Small town libraries? Reach out to the team responsible for producing the new Oprah Winfrey podcast I posted to LostemediaWIKI (might come valuable information there) https://www.facebook.com/trumpvideogone (a page someone made on Facebook about finding this video, 2017) This one is interesting. Just going to leave this here: https://en.wikipedia.org/wiki/Streisand_effect Good luck. Add: I see my posts are being either locked or removed on different subreddits. If you have any idea where to post that would help! UPDATE: 9:35 PM (UTC) 10.10.20 25th, April 1988 on the Oprah Winfrey Show. Production company WLS-TV. Harpo productions should have the transcripts, cause they bought the inventory of a prior transcript company. I made a comment here: https://www.tvtime.com/en/show/75010/episode/665033# I contacted an archivist from OWN and he said they only give it to commerce and airing. They have it. I won't stop. If someone here wants to use it for broadcast or for commerce's sake. DM me and I can give you contact to the right person, who is responsible for this archive. I could not get it because they don't distribute for\"Personal viewing\". I created a Discord for anyone interested: https://discord.gg/JD8Je3v"},
{"Title": "Hackers leak 37GB of Microsoft's source code (Bing, Cortana and more)", "Author": "u/harrro", "Content": "No content"},
{"Title": "I can't wait to buy this on eBay for $100 in a few years", "Author": "u/smacksaw", "Content": "No content"},
{"Title": "Due to the new Audacity Terms of Service, I present 31 versions of Audacity and Github source code for 18 versions", "Author": "u/coasterghost", "Content": "No content"},
{"Title": "This fits here.", "Author": "u/peaceman12824", "Content": "No content"},
{"Title": "\"Ever wonder what 200PB of tape looks like?\"", "Author": "u/equalunique", "Content": "No content"},
{"Title": "2500 Dos Games", "Author": "u/dutchsingh", "Content": "No content"},
{"Title": "On the zeroth day of Christmas, my true love sent to me eight 8TB drives.", "Author": "u/YourMJK", "Content": "No content"},
{"Title": "Remember when YouTube said security and hacking videos would/could be removed? This is why we hoard.", "Author": "u/-this-guy-fucks-", "Content": "No content"},
{"Title": "Please stop posting photos of your hard drives.", "Author": "u/Nooco24", "Content": "Hey, quick reminder, posting pictures of a bunch of HDDs you just bought ISN'T interesting, it's boring and I'm tired of seeing them all, I'm tired of seeing \"Am I part of this now?\" or \"Am I doing this the right way?\" posts. It's not because you have 100TB free of storage on your server that you are a data hoarder, and there is no \"good way to do this\". Data hoarding isn't about just buying $3000 worth of hard drives just for posting them here. What's interesting is what you do with your storage. If you just have 1TB of storage but you do something freakin' cool with it, what you can share here is way more important than someone buying 30TB of storage and never post again here. Please, focus on what we love, the DATA, not the storage medium, please focus on projects, on archiving, on digital preservation. Thanks. Post inspired by u/Nooco24"},
{"Title": "Wife's B'Day Present, She Knows Me So Well", "Author": "u/slider383", "Content": "No content"},
{"Title": "Internet Archive justifies its vast 'copyright infringing' National Emergency Library of 1.4 million books by pointing out that libraries are closed", "Author": "u/Rincey_nz", "Content": "No content"},
{"Title": "A major lawsuit against the nonprofit Internet Archive threatens the future of all libraries. Big publishers are suing to cut off librariesâ€™ ownership and control of digital books, opening new paths for censorship. Oral arguments are on March 20.", "Author": "u/BananaBus43", "Content": "No content"},
{"Title": "My mother just passed away. She wrote extensively on this website. What can I do to archive everything she wrote?", "Author": "u/anthonyridad", "Content": "Hey guys, my mother just passed away a few days ago from heart surgery. I always knew that she used to write in this one website. She has around 1400 entries that I want to archive, on the off chance that the website goes down. What's the best way to save her articles and stuff? I want to get around to reading them one day. Here's a link to her stuff: https://www.mylot.com/ridingbet/posts I tried using archive.org, but it only saves the main URL. Thanks in advance. :)"},
{"Title": "$700 kitty butt warmer", "Author": "u/blueskyn01se", "Content": "No content"},
{"Title": "Office Depot offered me a free charcoal grill with my hard drive purchase", "Author": "u/mrtramplefoot", "Content": "No content"},
{"Title": "Hey folks, here's the entire Computer Science curriculum organized in 1000 YouTube videos that you can just play and start learning. There are 40 courses in total, further organized in 4 academic years, each containing 2 semesters. I hope that everyone who wants to learn, will find this helpful.", "Author": "u/volunteervancouver", "Content": "No content"},
{"Title": "Bounty: $1000USD (keeps updating). Help me find the whole videotape of Donald Trump on The Oprah Winfrey Show, 25th of April, 1988. Season 3, Episode 5 (60 min. episode). Saw it on Facebook back in 2015. Then, it vanished. I haven't found it after that. Help is greatly appreciated!", "Author": "u/trycoconutoil", "Content": "ACTUAL BOUNTY: $1310USD https://bountyhunters.world/ TRANSCRIPTS:  $100USD Must be copy of the original Journal Graphics transcript I created a Discord for anyone interested: https://discord.gg/JD8Je3v Sources that are relevant for the hunt: https://thetvdb.com/series/the-oprah-winfrey-show/allseasons/official (air date) 3 min. part from the original video https://www.reddit.com/r/politicalfactchecking/comments/4nt6iw/donald_trump_republicans_are_dumb_why_did_this/ (Frustrated Redditors) https://www.youtube.com/watch?v=AZtfSIgsH1o (some YouTube video that may not be important, but the comments on it are relevant) http://www.oprah.com/own-oprahshow/what-donald-trump-told-oprah-about-his-presidential-hopes-video (an old link that might have shown the full episode in the past) interesting article (Interesting article on the disappearance of the video.) Another Reddit post by someone else asking for the same episode. A Reddit post about the missing episode, and what OP of this post thinks about it . Hopefully, any of you got it :) Post (over 1000 upvotes) https://www.reddit.com/r/DataHoarder/comments/j2xd4r/bounty_300usd_help_me_find_the_whole_videotape_of/ (locked cause of some Redditors not following community guidelines, I respect that, hence why I made this sub.) I'll literally give the person who can find it and send it to me $1000USD (if you want). I'll give it through cryptocurrency, anonymous PayPal system, product(s), or other means if that is desired. There are several people from Reddit who help give more to raise the bounty. That's why it's updating. NOTE! Ignore the images and fact-checker sites. Those are red herrings and so points towards something different from the year 1998. I want the entire video from The Oprah Winfrey Show, 25th of April, 1988, season 3, Episode 5, 60 minutes of the show. DM me here on Reddit or post it publically. And, the episode does not exist on The Oprah Winfrey Show: 20th Anniversary Collection. ( https://www.amazon.com/dp/B000B91N3S/ref=nav_timeline_asin?_encoding=UTF8&psc=1 ). And also not here: http://www.oprah.com/own-oprahshow/what-donald-trump-told-oprah-about-his-presidential-hopes-video (I even requested a transcript) **Since some ask. Why do I want it?** I want it cause it's been suppressed on the internet, and various sites and people tell me what I saw and heard didn't happen, and it bothers me. Been bothering me for years. I want to prove them wrong by showing the people this episode. I do not have any political intentions; I want the truth. 3.10.20 UPDATE. More useful resources: Identify who has the Marion Stokes collection and ask them. Try asking people at LostMediaWiki In LA and NY, there are TV libraries where I can requestâ€¦ NY Small town libraries? Reach out to the team responsible for producing the new Oprah Winfrey podcast I posted to LostemediaWIKI (might come valuable information there) https://www.facebook.com/trumpvideogone (a page someone made on Facebook about finding this video, 2017) This one is interesting. Just going to leave this here: https://en.wikipedia.org/wiki/Streisand_effect Good luck. Add: I see my posts are being either locked or removed on different subreddits. If you have any idea where to post that would help! UPDATE: 9:35 PM (UTC) 10.10.20 25th, April 1988 on the Oprah Winfrey Show. Production company WLS-TV. Harpo productions should have the transcripts, cause they bought the inventory of a prior transcript company. I made a comment here: https://www.tvtime.com/en/show/75010/episode/665033# I contacted an archivist from OWN and he said they only give it to commerce and airing. They have it. I won't stop. If someone here wants to use it for broadcast or for commerce's sake. DM me and I can give you contact to the right person, who is responsible for this archive. I could not get it because they don't distribute for\"Personal viewing\". I created a Discord for anyone interested: https://discord.gg/JD8Je3v"},
{"Title": "We're gonna need another napster soon", "Author": "u/Scuczu2", "Content": "No content"},
{"Title": "I can't wait to buy this on eBay for $100 in a few years", "Author": "u/smacksaw", "Content": "No content"},
{"Title": "Hoarding =/= Preservation", "Author": "u/Lee__Jieun", "Content": "No content"},
{"Title": "This fits here.", "Author": "u/peaceman12824", "Content": "No content"},
{"Title": "HDD destruction day at work today", "Author": "u/AnxietyBytes", "Content": "No content"},
{"Title": "2500 Dos Games", "Author": "u/dutchsingh", "Content": "No content"},
{"Title": "The Internet Archive is now preserving Flash animations and games", "Author": "u/retrac1324", "Content": "No content"},
{"Title": "Remember when YouTube said security and hacking videos would/could be removed? This is why we hoard.", "Author": "u/-this-guy-fucks-", "Content": "No content"},
{"Title": "The Ripinator restoration/build (15$ thrift store steal)", "Author": "u/Crossheart963", "Content": "No content"},
{"Title": "Wife's B'Day Present, She Knows Me So Well", "Author": "u/slider383", "Content": "No content"},
{"Title": "In 1886, the US Government Commissioned 7,500 Watercolor Paintings of Every Known Fruit in the World: Download Them in High Resolution", "Author": "u/fawkesdotbe", "Content": "No content"},
{"Title": "A major lawsuit against the nonprofit Internet Archive threatens the future of all libraries. Big publishers are suing to cut off librariesâ€™ ownership and control of digital books, opening new paths for censorship. Oral arguments are on March 20.", "Author": "u/BananaBus43", "Content": "No content"},
{"Title": "4TB marked down to $21 at Walmart.", "Author": "u/TheSkinnyD", "Content": "No content"},
{"Title": "$700 kitty butt warmer", "Author": "u/blueskyn01se", "Content": "No content"},
{"Title": "Don't Let Reddit Kill 3rd Party Apps!", "Author": "u/MagicDalsi", "Content": "EDIT: Don't use this post any more: it's been crossposted so widely that it breaks Reddit when trying to open it! It's been locked. Further discussion (and crossposts) should go HERE. What's going on? A recent Reddit policy change threatens to kill many beloved third-party mobile apps, making a great many quality-of-life features not seen in the official mobile app permanently inaccessible to users. On May 31, 2023, Reddit announced they were raising the price to make calls to their API from being free to a level that will kill every third party app on Reddit, from Apollo to Reddit is Fun to Narwhal to BaconReader . Even if you're not a mobile user and don't use any of those apps, this is a step toward killing other ways of customizing Reddit, such as Reddit Enhancement Suite or the use of the old.reddit.com desktop interface . This isn't only a problem on the user level: many subreddit moderators depend on tools only available outside the official app to keep their communities on-topic and spam-free. What's the plan? On June 12th, many subreddits will be going dark to protest this policy. Some will return after 48 hours: others will go away permanently unless the issue is adequately addressed, since many moderators aren't able to put in the work they do with the poor tools available through the official app. This isn't something any of us do lightly: we do what we do because we love Reddit , and we truly believe this change will make it impossible to keep doing what we love. The two-day blackout isn't the goal , and it isn't the end. Should things reach the 14th with no sign of Reddit choosing to fix what they've broken, we'll use the community and buzz we've built between then and now as a tool for further action. What can you do? Complain. Message the mods of r/reddit .com, who are the admins of the site: message u/reddit : submit a support request : comment in relevant threads on r/reddit , such as this one , leave a negative review on their official iOS or Android app- and sign your username in support to this post. Spread the word. Rabble-rouse on related subreddits. Meme it up, make it spicy. Bitch about it to your cat. Suggest anyone you know who moderates a subreddit join us at our sister sub at r/ModCoord - but please don't pester mods you don't know by simply spamming their modmail. Boycott and spread the word...to Reddit's competition! Stay off Reddit entirely on June 12th through the 13th- instead, take to your favorite non -Reddit platform of choice and make some noise in support! Don't be a jerk. As upsetting this may be, threats, profanity and vandalism will be worse than useless in getting people on our side. Please make every effort to be as restrained, polite, reasonable and law-abiding as possible. This includes not harassing moderators of subreddits who have chosen not to take part : no one likes a missionary, a used-car salesman, or a flame warrior."},
{"Title": "Hey folks, here's the entire Computer Science curriculum organized in 1000 YouTube videos that you can just play and start learning. There are 40 courses in total, further organized in 4 academic years, each containing 2 semesters. I hope that everyone who wants to learn, will find this helpful.", "Author": "u/volunteervancouver", "Content": "No content"},
{"Title": "Low Disk Space", "Author": "u/Flying-T", "Content": "No content"},
{"Title": "We're gonna need another napster soon", "Author": "u/Scuczu2", "Content": "No content"},
{"Title": "The Internet Archive lost their court case", "Author": "u/safels2", "Content": "kys u/spez"},
{"Title": "Hoarding =/= Preservation", "Author": "u/Lee__Jieun", "Content": "No content"},
{"Title": "\"The Truth is Paywalled But the Lies Are Free\": Notes on why I hoard data", "Author": "u/gabefair", "Content": "I came across a beautifully written article by Nathan J. Robinson about how quality work costs money to access and propaganda is freely given. The article makes some good points on why it is important for data to be more free, which I will summarize below: Nobody is allowed to build a giant free database of everything human beings have ever produced. 2) Copyright law can be an intensive restriction on the freedom of speech and determines what information you can (and not) share with others. 3) The concept of a public community library needs to evolve. As books, and other content move online, our communities have as well. 4)  Human creativity and potential is phenomenally leashed when human knowledge is limited. 5) Free and affordable libraries/sources of wisdom are dying. This got me thinking about why I care about hoarding data. Data is invaluable! A digital dark age is forming around us and we can do what we can to prevent it. A lot of people here will hoard data for personal reasons. I hoard data for others. The things the people in this subreddit hoard whether it be movies, Youtube, pictures, news articles, websites, all of it is culture. Its history. Even memes and social media are not crap. Even literal shit is valuable to a scatologist. Can you imagine if we were able to find the preserved excrement from a long extinct animal? What one sees as shit, is so much more to someone else who is trained and educated. Its data. The internet and social media around us is Art and Culture from our time. This is history for the future to use and learn. Things go viral for a reason. The information shared in the jokes and content are snapshots of the public's thinking and perspective on the world. Invaluable data for future scholars. Imagine we found a Viking warship and on it was a perfectly preserved book of jokes. Sure many at the time might have thought they were shit jokes made at the expense of others. But we would learn so much about their customs, society, and the evolution of human civilization if this book was preserved and found. And the book's contents were made available to the world. Also a lot of political content is shared on social media and comment sections as well. Our understanding of politics  will be carved up in units of memes, and shared on thousands of siloed paywalled platforms and mediums over time. And our role is to collect and consolidate them. This is but a small sliver of the documentation of how our world is changing around us. And we can do our part to save and make free to others as much of it as we can. P.S. Many reddit accounts unknowingly (like maybe yours) are being used by bots to vote for content. Please enable 2FA to stop this practice. Instructions P.P.S. Summer of 2020 is time for contingency preparedness. There is no time to get started like the present. Buy your disks now to be prepared for when history needs you. P.P.P.S. Thank you all for the support and discussion so far. You are some good folks! A song that I enjoy due to it relating to the importance preserving history is \"Amnesia\" by Dead Can Dance. It has a line in the song that I find quite chilling, \"Can you really plan the future when you no longer have the past?\" P.P.P.P.S. Some people like to use the plural verb \"data are\" instead of the singular \"data is\" since data are used to refer to a collection. \"The fish are being collected\". I merely mention this as a factoid in celebration of this discussion receiving so much attention. P.P.P.P.P.S. Take a look at this list of site-deaths to remind us of all the now dead sites that once existed. P.P.P.P.P.P.S For further motivation, consider how: Facebook is deleting evidence of war crimes"},
{"Title": "HDD destruction day at work today", "Author": "u/AnxietyBytes", "Content": "No content"},
{"Title": "This meme speaks to me", "Author": "u/Fox_Uni_Charlie_Kilo", "Content": "No content"},
{"Title": "The Internet Archive is now preserving Flash animations and games", "Author": "u/retrac1324", "Content": "No content"},
{"Title": "Absolutely unacceptable - Newegg shipped me drives like this", "Author": "u/redditor1101", "Content": "No content"},
{"Title": "The Ripinator restoration/build (15$ thrift store steal)", "Author": "u/Crossheart963", "Content": "No content"},
{"Title": "Why throw away old ~60GB drives? Why not make something to commemorate their service?", "Author": "u/aforsberg", "Content": "No content"},
{"Title": "In 1886, the US Government Commissioned 7,500 Watercolor Paintings of Every Known Fruit in the World: Download Them in High Resolution", "Author": "u/fawkesdotbe", "Content": "No content"},
{"Title": "Itâ€™s because of youtube-dl that we have the audio recordings of Bitfinex executive admitting to bank fraud", "Author": "u/dharmatech", "Content": "No content"},
{"Title": "4TB marked down to $21 at Walmart.", "Author": "u/TheSkinnyD", "Content": "No content"},
{"Title": "35+ years worth TV, 24/7. A true data hoarding legend", "Author": "u/orochimaru1999", "Content": "No content"},
{"Title": "Don't Let Reddit Kill 3rd Party Apps!", "Author": "u/MagicDalsi", "Content": "EDIT: Don't use this post any more: it's been crossposted so widely that it breaks Reddit when trying to open it! It's been locked. Further discussion (and crossposts) should go HERE. What's going on? A recent Reddit policy change threatens to kill many beloved third-party mobile apps, making a great many quality-of-life features not seen in the official mobile app permanently inaccessible to users. On May 31, 2023, Reddit announced they were raising the price to make calls to their API from being free to a level that will kill every third party app on Reddit, from Apollo to Reddit is Fun to Narwhal to BaconReader . Even if you're not a mobile user and don't use any of those apps, this is a step toward killing other ways of customizing Reddit, such as Reddit Enhancement Suite or the use of the old.reddit.com desktop interface . This isn't only a problem on the user level: many subreddit moderators depend on tools only available outside the official app to keep their communities on-topic and spam-free. What's the plan? On June 12th, many subreddits will be going dark to protest this policy. Some will return after 48 hours: others will go away permanently unless the issue is adequately addressed, since many moderators aren't able to put in the work they do with the poor tools available through the official app. This isn't something any of us do lightly: we do what we do because we love Reddit , and we truly believe this change will make it impossible to keep doing what we love. The two-day blackout isn't the goal , and it isn't the end. Should things reach the 14th with no sign of Reddit choosing to fix what they've broken, we'll use the community and buzz we've built between then and now as a tool for further action. What can you do? Complain. Message the mods of r/reddit .com, who are the admins of the site: message u/reddit : submit a support request : comment in relevant threads on r/reddit , such as this one , leave a negative review on their official iOS or Android app- and sign your username in support to this post. Spread the word. Rabble-rouse on related subreddits. Meme it up, make it spicy. Bitch about it to your cat. Suggest anyone you know who moderates a subreddit join us at our sister sub at r/ModCoord - but please don't pester mods you don't know by simply spamming their modmail. Boycott and spread the word...to Reddit's competition! Stay off Reddit entirely on June 12th through the 13th- instead, take to your favorite non -Reddit platform of choice and make some noise in support! Don't be a jerk. As upsetting this may be, threats, profanity and vandalism will be worse than useless in getting people on our side. Please make every effort to be as restrained, polite, reasonable and law-abiding as possible. This includes not harassing moderators of subreddits who have chosen not to take part : no one likes a missionary, a used-car salesman, or a flame warrior."},
{"Title": "It was a good electronics recycling day at work today.", "Author": "u/VertexBeatz", "Content": "No content"},
{"Title": "Low Disk Space", "Author": "u/Flying-T", "Content": "No content"},
{"Title": "Tokyo Resident who's been filming scenes in Japan since 1990 has over 12,000 videos on youtube", "Author": "u/Ayit_Sevi", "Content": "So, I've found myself downloading a lot of historical footage and I stumbled upon this guy, Lyle Hiroshi Saxon . The dude has been on youtube since 2007 and over the period of 14 years has uploaded 12,967 videos . He's been a resident since 1984 and has footage dating from 1990-1993 and from 2008-present. It's by far the biggest channel I've ever downloaded. He even has a webpage/blog Even if it looks like he hasn't updated it in a while. Thought it was interesting enough to share"},
{"Title": "The Internet Archive lost their court case", "Author": "u/safels2", "Content": "kys u/spez"},
{"Title": "Going to add some actual drives to it when I get the chance... Until then:", "Author": "Unknown author", "Content": "No content"},
{"Title": "\"The Truth is Paywalled But the Lies Are Free\": Notes on why I hoard data", "Author": "u/gabefair", "Content": "I came across a beautifully written article by Nathan J. Robinson about how quality work costs money to access and propaganda is freely given. The article makes some good points on why it is important for data to be more free, which I will summarize below: Nobody is allowed to build a giant free database of everything human beings have ever produced. 2) Copyright law can be an intensive restriction on the freedom of speech and determines what information you can (and not) share with others. 3) The concept of a public community library needs to evolve. As books, and other content move online, our communities have as well. 4)  Human creativity and potential is phenomenally leashed when human knowledge is limited. 5) Free and affordable libraries/sources of wisdom are dying. This got me thinking about why I care about hoarding data. Data is invaluable! A digital dark age is forming around us and we can do what we can to prevent it. A lot of people here will hoard data for personal reasons. I hoard data for others. The things the people in this subreddit hoard whether it be movies, Youtube, pictures, news articles, websites, all of it is culture. Its history. Even memes and social media are not crap. Even literal shit is valuable to a scatologist. Can you imagine if we were able to find the preserved excrement from a long extinct animal? What one sees as shit, is so much more to someone else who is trained and educated. Its data. The internet and social media around us is Art and Culture from our time. This is history for the future to use and learn. Things go viral for a reason. The information shared in the jokes and content are snapshots of the public's thinking and perspective on the world. Invaluable data for future scholars. Imagine we found a Viking warship and on it was a perfectly preserved book of jokes. Sure many at the time might have thought they were shit jokes made at the expense of others. But we would learn so much about their customs, society, and the evolution of human civilization if this book was preserved and found. And the book's contents were made available to the world. Also a lot of political content is shared on social media and comment sections as well. Our understanding of politics  will be carved up in units of memes, and shared on thousands of siloed paywalled platforms and mediums over time. And our role is to collect and consolidate them. This is but a small sliver of the documentation of how our world is changing around us. And we can do our part to save and make free to others as much of it as we can. P.S. Many reddit accounts unknowingly (like maybe yours) are being used by bots to vote for content. Please enable 2FA to stop this practice. Instructions P.P.S. Summer of 2020 is time for contingency preparedness. There is no time to get started like the present. Buy your disks now to be prepared for when history needs you. P.P.P.S. Thank you all for the support and discussion so far. You are some good folks! A song that I enjoy due to it relating to the importance preserving history is \"Amnesia\" by Dead Can Dance. It has a line in the song that I find quite chilling, \"Can you really plan the future when you no longer have the past?\" P.P.P.P.S. Some people like to use the plural verb \"data are\" instead of the singular \"data is\" since data are used to refer to a collection. \"The fish are being collected\". I merely mention this as a factoid in celebration of this discussion receiving so much attention. P.P.P.P.P.S. Take a look at this list of site-deaths to remind us of all the now dead sites that once existed. P.P.P.P.P.P.S For further motivation, consider how: Facebook is deleting evidence of war crimes"},
{"Title": "At least 223 companies have manufactured hard disk drives. Most of that industry has vanished through bankruptcy or mergers and acquisitions. None of the first four entrants continue in the industry today.", "Author": "u/dabderax", "Content": "No content"},
{"Title": "This meme speaks to me", "Author": "u/Fox_Uni_Charlie_Kilo", "Content": "No content"},
{"Title": "A full house.", "Author": "u/ast3r3x", "Content": "No content"},
{"Title": "Absolutely unacceptable - Newegg shipped me drives like this", "Author": "u/redditor1101", "Content": "No content"},
{"Title": "Only in San Francisco ðŸ¤£", "Author": "u/anxman", "Content": "No content"},
{"Title": "Why throw away old ~60GB drives? Why not make something to commemorate their service?", "Author": "u/aforsberg", "Content": "No content"},
{"Title": "House Speaker deleted his podcast. Hoarders to the rescue", "Author": "u/harrro", "Content": "No content"},
{"Title": "Itâ€™s because of youtube-dl that we have the audio recordings of Bitfinex executive admitting to bank fraud", "Author": "u/dharmatech", "Content": "No content"},
{"Title": "Complete US PlayStation 2 manual collection posted to archive.org", "Author": "u/K1rkl4nd", "Content": "To celebrate the PlayStation 2's 22nd anniversary on Wednesday I have uploaded my complete US manual collection- personally scanned and edited to 4K resolution- to archive.org.  17GB of goodiness across 1795 titles plus an additional ~100 variants, art books, mini-guides, and comics.  The upload is done- it's \"processing\" now.  Be sure to download the original files, not anything archive.org generates (sometimes they recompress things poorly trying to OCR). https://archive.org/details/kirklands-manual-labor-sony-playstation-2-usa-4k-version"},
{"Title": "35+ years worth TV, 24/7. A true data hoarding legend", "Author": "u/orochimaru1999", "Content": "No content"},
{"Title": "It was a good electronics recycling day at work today.", "Author": "u/VertexBeatz", "Content": "No content"},
{"Title": "Tokyo Resident who's been filming scenes in Japan since 1990 has over 12,000 videos on youtube", "Author": "u/Ayit_Sevi", "Content": "So, I've found myself downloading a lot of historical footage and I stumbled upon this guy, Lyle Hiroshi Saxon . The dude has been on youtube since 2007 and over the period of 14 years has uploaded 12,967 videos . He's been a resident since 1984 and has footage dating from 1990-1993 and from 2008-present. It's by far the biggest channel I've ever downloaded. He even has a webpage/blog Even if it looks like he hasn't updated it in a while. Thought it was interesting enough to share"},
{"Title": "Going to add some actual drives to it when I get the chance... Until then:", "Author": "Unknown author", "Content": "No content"},
{"Title": "At least 223 companies have manufactured hard disk drives. Most of that industry has vanished through bankruptcy or mergers and acquisitions. None of the first four entrants continue in the industry today.", "Author": "u/dabderax", "Content": "No content"},
{"Title": "A full house.", "Author": "u/ast3r3x", "Content": "No content"},
{"Title": "Only in San Francisco ðŸ¤£", "Author": "u/anxman", "Content": "No content"},
{"Title": "House Speaker deleted his podcast. Hoarders to the rescue", "Author": "u/harrro", "Content": "No content"},
{"Title": "Complete US PlayStation 2 manual collection posted to archive.org", "Author": "u/K1rkl4nd", "Content": "To celebrate the PlayStation 2's 22nd anniversary on Wednesday I have uploaded my complete US manual collection- personally scanned and edited to 4K resolution- to archive.org.  17GB of goodiness across 1795 titles plus an additional ~100 variants, art books, mini-guides, and comics.  The upload is done- it's \"processing\" now.  Be sure to download the original files, not anything archive.org generates (sometimes they recompress things poorly trying to OCR). https://archive.org/details/kirklands-manual-labor-sony-playstation-2-usa-4k-version"},
{"Title": "First TrueNAS SCALE build - I'd like to begin doing weekly snapshots of Wikipedia. Any tips?", "Author": "u/saltyspicehead", "Content": "6 x 12TB Drives, Raidz2, NVMe L2ARC + SLOG etc etc etc roughly 45 TB to work with. I'd like to start by creating a weekly Wikipedia backup job to track change history. I'm familiar enough about storage tech to know that this should be possible without consuming a new ~120GB of storage every week, since only the delta will actually consume any disk space. However, I'm not familiar enough with non-enterprise solutions for this task. I see a few tools in the wiki but I'm not sure which one is best suited for this situation. Any solutions or guides you can recommend? I'd prefer not to enable dedup on my primary pool, but if that's the best option then I'll likely just create a secondary pool for this specific task."},
{"Title": "The Internet Archive solution and a creative way to compress large text files", "Author": "u/mazemadman12346", "Content": "As many of us are aware the internet archive is under legal flak for sharing books during covid to the dismay of book publishers This makes me wonder. What if they never actually shared any media or websites, and instead gave a series of links to libraryofbabel.info (or similar website modified to generate random html code) which would conatain either the direct text or the source code of the designated media? By doing this they would be able to circumvent copyright laws as anything they share would be the product of randomly generated code and they would simply be telling you where to find it as opposed to sharing the image secondly, what if we made a website like this but for compression? Instead of needing 400mb of compressed files you could have a 2mb text file that links you to the necessary pages TLDR instead of compressing the entire bee movie script you would only need to compress a text folder containing 1-10 of \" https://libraryofbabel.info/bookmark.cgi?bee_movie_script_random )\" and write a program to decrypt them"},
{"Title": "Message Board Posts", "Author": "u/Expensive_Elk_3618", "Content": "I am trying to figure out how to download all of the message board posts in a certain category along with all of the responses.  I can get the first level done but not every level with responses.  I've tried HTTrack without success.  Years ago I used to do this sort of thing and it wasn't all that hard to figure out but I am older and everything is a little stricter. :-) So, I would need the easiest tool.  Thanks for any advice you can offer.   Here is one of the sites. https://www.genealogy.com/forum/surnames/topics/reed/"},
{"Title": "Weird Method to Check Legitimacy of Drives", "Author": "u/miko-zee", "Content": "So I recommended a seller to my brother who is highly regarded as providing mostly legitimate drives. The problem is they seem to be of varying quality almost like shucking a drive. Sometimes they're totally new, sometimes unused old stock and sometimes manufacturer refurbished. My brother got a years old stock that had zero on power hours. However, a review of the seller suggested the following methodology. They were reviewing a 12TB Exos \"According to my tests, this drive is legit. I was able to: Verify the serial number at the Seagate website-  - - -    - Format the drive into 11 partitions. Put files into the first and last 2 partitions and was able to read back the files. Note that the drive has 11,175.98 GB of actual free space. The missing 825 GB may have been allocated to the file mapping table.\" Why do you need to partition it n-1 TB times then write data on the first and last partition? Is this even sound? I think they suggested it because it's quicker and more painless than stress testing the whole drive for a legitimacy test. EDIT. I want to clarify I know the scam of declaring high capacities using a smaller capacity medium. Also most of these drives usually have valid warranties just in a different region when checked via the their respective manufacturers website even the refurbished ones."},
{"Title": "Transport 3.5\" HDD with tray weekly in small casing - can't re-use packaging forever", "Author": "u/No-Balance-8038", "Content": "I want basically a similar small case that allows for a 3.5\" with tray which is 17cm long, but as small and portable! I will get a bigger antistatic bag still. The issue is that cardboard will wear down. I looked into Pelican cases and they are just too big to fit in my backpack! I already have 8 of https://www.amazon.de/gp/product/B01JOCHO80/ but they do not have enough space for a 3.5\" with tray on ( ~ 170mm) Dimensions are: 7cm height 20cm length 13cm width https://preview.redd.it/transport-3-5-hdd-with-tray-weekly-in-small-casing-cant-re-v0-pfl9kd0ytc6d1.png https://preview.redd.it/transport-3-5-hdd-with-tray-weekly-in-small-casing-cant-re-v0-uifnnwg2uc6d1.png"},
{"Title": "Im looking for the best way to store my hentai collections preferbely cloud and external hard drive as backup. whats the best cloud service that can be used and external ssd. I want to start with 2TB for now as my collection is only 500gb and more to come in few days.", "Author": "u/RealComposer5655", "Content": "as title says"},
{"Title": "Macrium Reflect backup to NVMe drive in USB-C enclosure is very slow? Normal copying to same drive is fast...", "Author": "u/ozzuneoj", "Content": "Relevant Specs: Windows 10 22H2 Ryzen 7 5800X3D Gigabyte X570 Aorus Elite 64GB DDR4-3600 Soldigm P44 Pro 2TB NVMe Gen4 Macrium Reflect Free 8 To keep things simple, I'm just going to focus on making a macrium image of my system partitions (C: and a couple without letters; about 100GB of data total) as a backup . I have been doing this for years but lately I've been trying to streamline and speed up the process. I have a SATA dock installed inside my system (straight SATA, no USB), and when I slot in my old 1TB Crucial MX500 drive, I can run the macrium backup in about 6 minutes. I have lots of spare NVMe drives, so I got one of these Orico 10Gbps USB 3.2 NVMe enclosures ( https://www.amazon.com/ORICO-NVMe-Thunderbolt-Compatible-SSDs-PWM2-BK/dp/B0BJ23JTXX ) several months ago and it has worked fine for transferring files. However, when I run the same macrium backup as before to this NVMe drive it seems to hang for a second, then shows an estimate of 6 minutes, then within a few seconds the transfer rate drops from 1+gbps to under 100mbps and the estimated time shoots up to 55 minutes. You would think that the symptoms point to a crappy USB enclosure or a problem with the drive... but when I copy the same image file directly from the SATA drive to the NVMe drive manually in Windows the transfer rate is pegged at over 550MB\\sec (megabytes) and it is done in just over a minute. Is there something I can adjust in Macrium or elsewhere that would fix this seeming incompatibility with this NVMe USB enclosure? I'd love to use it (to free up my old 1TB MX500) but having to wait nearly 10 times as long for backups is unacceptable. Thank you for any help you can offer."},
{"Title": "Samsung SSD cache sizes for 1TB", "Author": "u/madurbad", "Content": "Looking to buy a Samsung SSD. I only have the money for 1TB, but I'm not sure which model to get given the prices (I can only afford 1TB). Obviously I want performance to be the best, but that largely comes down to cache size between the Samsung T7, T7 shield, and T9. Unfortunately, I can't take advantage of USB 3.2x2. My question is: what are the cache sizes for each of these SSDs with their 1TB models (I heard cache sizes differ with storage capacity)? If theyre all the same, I'm looking to save money and just buy the T7. Thanks"},
{"Title": "How unlimited is Flickr Pro?", "Author": "u/jammsession", "Content": "A friend of mine is a photographer. He currently uses the good old \"put all photos onto one external HDD and labeling them\" method for storing his files. He now got to a point where he is scared of loosing data and asked about Flickr. So I was wondering if anyone else here has around 10TB of photos on Flickr Pro. Is it really unlimited? Price seems almost too good to be true."},
{"Title": "Help on selecting file system / software raid solution", "Author": "u/Altruistic-Coyote731", "Content": "Hi, I know this issue has been discussed to death but pleas bear with me! I am running a server for backups, metabase (BI Tool) and some other apps. I have 4 x 4 tb ssd drives. Right now I use Minio for data management as it was the easiest to setup but I have 2 issues with it: Memory usage: I have only 16gb available and Minio uses most of it during large syncs. For most of my use cases I need the fs directly eg. samba or nfs, databases, etc. You can mount  S3 buckets as drives but that just feels redundant. I researched the topic a bit and came up with 3 potential options described below that meet my requirements. I would greatly appreciate feedback form the community on: What is the memory usage like? Ease of setup? Ease of recovery in case of disk failure? (note that I have never had to deal with raid recovery before so total noob!) Anything i missed out from the pros/cons table below Options Option 1: mdadm RAID10 + xfs Option 2: mdadm RAID10 + btrfs Options 3: btrfs RAID10 Pros mdadm and xfs are stable solutions; good performance; snapshots snapshots; native RAID; healing; Cons missing features such as snapshots ??? reliability of btrfs RAID I didn't include ZFS because, to my understanding, it's RAM intensive and has a seep learning curve. I am open to other solutions if they make more sense as well! :)"},
{"Title": "How to upgrade windows PC using with storage spaces, 6 hdds", "Author": "u/OC2k16", "Content": "Hello, I currently run a PC with win 10, older hardware. I have 6 3TB hdds, set up using storage spaces, two-way mirror, so 9TB total. They are all connected via sata. I want to upgrade to a different PC, and transfer the data to newer hard drives. Will it be as simple as: disconnecting some drives, connecting new ones, transfering the files to new hdds, and connecting new hdds to new PC? Or will windows get angry with that? I am going from win 10 to win 11 if that matters. Perhaps I should just transfer the files to the single drives, put those in the new PC, then add their backups and do the storage spaces then? Any advice would be appreciated!"},
{"Title": "Rsync Ran With Read Only Destination", "Author": "u/klnadler", "Content": "Hi all, I was transfering some data from my Unraid to an external drive mounted as an unassigned drive. I had to tweak the command a little to work with the permissions due to the drive being ExFat. After the transfer I realized the drive was set to read only through the Unassigned Disk settings, but the files transferred? How and why? Command used: rsync -avPh --no-owner --no-group --no-perms"},
{"Title": "What my boss has collecting dust (brand new)", "Author": "u/ResponsibilityIll888", "Content": "No content"},
{"Title": "Anyone know of a device that can easily back-up data portably off a memory card onto an SSD?", "Author": "u/Ok-Cartographer1745", "Content": "I bought a Wolverine 80gb like 15 years ago. You plug in a memory card (not specifically Sony Memory Stick. I mean like Compact Flash, SD Card, and so on), press a button, and it copies your card to a folder on the hard drive. But that think is old and low capacity and I probably can't upgrade the hard drive due to the form factor and firmware likely only allowing 80gb drives. Anyway, anyone know of a similar device that is more modern?  I want it to: be portable and battery powered (nothing huge and nothing that needs to be plugged in to work) have a large SSD (500 GB or more) be self sustained (I don't want to have to plug it into a PC nor Bluetooth it to my phone) accept at least SD cards (more slots would be nice, but not necessary) I mainly want to do it to easily hoard pictures off my camera, especially if I need to quickly empty out my more expensive fast cards Also, a link to show what I already have. I'm not the one selling it and I don't recommend buying it because it's so outdated and slow (and the battery probably doesn't work considering how old it is).  So don't consider this advertising. https://www.ebay.com/itm/125748798949?_trkparms=amclksrc%3DITM%26aid%3D1110006%26algo%3DHOMESPLICE.SIM%26ao%3D1%26asc%3D266917%2C266785%26meid%3D0caf8ebbb6584b81a8ed70c03fd786fd%26pid%3D101875%26rk%3D1%26rkt%3D5%26sd%3D285760155205%26itm%3D125748798949%26pmt%3D1%26noa%3D1%26pg%3D2332490%26algv%3DSimplAMLv11WebTrimmedV3MskuWithLambda85KnnRecallV1V2V4ItemNrtInQueryAndCassiniVisualRankerAndBertRecallWithVMEV3CPCAutoWithCassiniEmbRecall%26brand%3DWolverine&_trksid=p2332490.c101875.m1851"},
{"Title": "YouTube is A/B testing requiring login for video playback", "Author": "u/ThePixelHunter", "Content": "No content"},
{"Title": "Youtube channel downloader and sync for 1000s of videos?", "Author": "u/No_Doubt8973", "Content": "Hi first of all, I am on windows but I can install linux on a virtual machine. I have about 50 channels I am downloading videos from manually and it's so tiring I stopped for a year. I started again and found a lot of channels deleted videos or the channel is closed. And some of the channels have since uploaded thousands of videos. I used to manually go on youtube on my pc and grab the links but it stops loading after 1000 videos or it overloaded the chrome and crash. Would a channel downloader be able to download thousands of videos from one channel? How does the leading youtube downloaders know which videos were has been downloaded so that it doesn't download it again? Which program allows me to select preferred codec for a specific resolution? If it is possible I want to avoid av1 codec because my tablet can't play it unless it's 8k because youtube uses only av1 on 8k resolution, I would play it on my pc when I upgrade my video card black friday."},
{"Title": "Need help setting up pool", "Author": "u/Kriznick", "Content": "Hey, so new baby data hoarder here, built a pool using Windows Storage Spaces and 12hdds and 4ssds I had laying around. It's working fine so far, but I can't help but wondering if my performance and pool efficency is suffering. So I've got: 1tb ssd x4 4tb hdd x1 // 3tb hdd x2 // 1tb hdd x1 // .75tb hdd x4 // .64tb hdd x1 // .5tb hdd x3 Totaling 4tb ssd and 16.4tb hdd. HOWEVER, I'm only getting 6tb of storage I THINK. I'm set to parity, which I thought was supposed to be more efficient for the storage, but it doesn't math out.... I've heard about this SHR-2 thing, but I have no idea about using NAS or whatever. Storage Spaces was nice because it was pretty push-pull click-click. Is there another simple setup program that's more efficient? I'm getting 1.7mb/s write speed, and thought it should have been faster since it's writing to multiple disk spaces simultaneously rather than just 1 drive... Am I just dumb and should just stick with what I have? I've tried researching it, but I can't make heads or tails of it. Thanks yall"},
{"Title": "Best cooled NVME enclosure", "Author": "u/JaL3J", "Content": "Suggestions for best cooled USB-C - NVME M.2 enclosure? Looking for: Single or dual NVME M.2 full length enclosure 2. Cooling fan in housing and good heat transfer 3. Low cost, 20-50usd, not 150+. 4. USB-C 3.2 gen2 controller (10Gb/s or more). 5. Preferrably a minimalistic design I've seen a couple of products on amazon etc., but usually too large and nothing with dual stick. Low profile housing is optimal. What would you suggest?"},
{"Title": "What drives do you recommend?", "Author": "u/Endeavour1988", "Content": "I currently have 3x 6TB Western Digital Golds for my media server.  2x Host content and one for spare (backups). * I do back up externally/offsite I've always had a bad history with Seagate but some of my external drives have been solid which are Seagate so I'm open minded.  Can anyone recommend from experience what brands to go for?  The Golds are 7 year old now so I would like to look at replacing 2x with 12TB and possibly the third with something larger like 16TB.  Which should hopefully see me through another 6 years."},
{"Title": "Are any HDD manufacturer RMAs not predatory?", "Author": "u/Vile-The-Terrible", "Content": "I bought a NAS less than two years ago from B&H that had Seagate Ironwolf Pro 16tbs in it. One of the drives started to fail so I began the RMA process with Seagate. They charged to have expedited delivery but it took them two weeks to process my order before shipping it. They then send me a nonfunctional drive. I now have to go through the process of RMAing the RMA drive, and the kicker? I have to pay the return shipping on the drive that failed and the broken drive they sent me. So as the title asks, are there any other companies I should be spending my dollars with going forward? Edit: In case it wasnâ€™t clear. I understand that it is standard practice to pay for return shipping on the drive youâ€™re using the warranty for. The problem is paying shipping again to return the faulty drive they sent me."},
{"Title": "Two 2TB external HDDs or one 4TB external HDD?", "Author": "u/Fonti_Sgasao", "Content": "Hey there, I need an external HDD just to store data, I would buy a 4TB HDD (probably Toshiba Canvio, never had a problem with it) but I heard that it's more likely to suffer data loss than a 2TB HDD, is it true?"},
{"Title": "The.tnk.loft Instagram profile is about to be deleted - best way to archive everything?", "Author": "u/studioviper", "Content": "As the title says, the fantastic and amazing the.tnk.loft Instagram account is about to be deleted. A lot of really great and inspiring content will be lost forever. Is there an easy way to back everything up? Thanks!"},
{"Title": "ATA security doesn't play well with WD USB-SATA bridge devices", "Author": "u/Visual-East8300", "Content": "A couple of years ago, I bricked a 10TB WD easystore by issuing \"hdparm --security-set-pass\" on the USB drive, un-bricked it by plugging the HDD to another USB adapter then \"hdparm --security-disable\". Today, I bricked a 4TB WD easystore (2.5 inch), out of luck this time because inside the enclosure it's an USB on-board WD Blue drive. Both are using ASMedia ASM1151W. Does anyone have ideas how to recover from this? Or it's not worth the time trying to fix?"},
{"Title": "Why is fluid RAID not more widely adopted by NAS makers?", "Author": "u/digitalanalog0524", "Content": "Synology and Terramaster both allow for fluid RAID setups (SHR and TRAID). I understand there is no black magic here and both are just using clever partitioning and a mix of RAID types. How come QNAP and Asustor do not offer these out of the box? I struggle to understand how Terramaster could out-innovate those two companies."},
{"Title": "Dreams and Aspirations", "Author": "u/thenazgul80", "Content": "Do I have this right? I want to build a nas to store my digital library and stream it to my and my family's devices. Currently relying on 5 external 2.5\" drives and windows media share. However I can't outright purchase something like a Synology nas due to cost. I have my old PC collecting dust. Amd fx 4100 Radeon 7700hd and 8 gb of ddr3 ram. Here is the plan. Undevolt the fx4100 to the lowest it can go. Diy mod my Asus Vento a8 PC case for cooling and dust prevention. Buy a 16-20 TB drive and start the nas. Once I can afford a second drive i plan on using them in raid 1. Once a 3rd drive is added I want to use them in raid 5. Over all I want to end up with 40-60tb of useable storage with some redundancy and keep the external 2.5\" drives for backups. From what I can gather, truenas won't let me just add drives later on down the line. Is unRAID my only option if I want to start with the bare minimum and expand later on as I can afford more drives?"},
{"Title": "Building up my personal media server, and I have additional questions about sourcing storage", "Author": "u/TheXypris", "Content": "So I asked r/Plex a few days ago and got some really great advice One of which was to send me here So along side what I was told there I had more questions How is shucking more affordable than just buying the regular drive? And how do I actually find those deals Are Seagate drives really that bad? What is the general consensus on buying drives second hand off eBay or through a liquidation auction? Bad idea or risky at best? I'm assuming Facebook marketplace/Craigslist should be avoided, but is there any merit in looking there? Like if I find a second hand external I could shuck? How important are RPMs sata type and all that other non capacity info? What's good and what's bad? Tradeoffs? How long will a healthy high capacity drive realistically last? People keep saying raid is mandatory and act like drives fail because you look at them funny. I can't afford a raid array right now and anything I'm putting on it would be easily required if time consuming. Are there price monitoring sites I can use to keep an eye out for deals or sales? And where is the best place to get sata and power cables for the drives?"},
{"Title": "Came across a Reddit Archive project (Pushshift alternative)", "Author": "u/marinluv", "Content": "Came across this post yesterday. That user and u/RaiderBDev are archiving Reddit data. The data is around 3-4Tb roughly from what I have seen. The GitHub Repo to archive and access the data: Here To download and search Subreddit and user data manually: Here Post on r/pushshift for the 2.5TB dump: Here"},
{"Title": "Need help with converting and compressing old VOB files", "Author": "u/martian_doggo", "Content": "So i have a bunch of old vob files, and i want to convert them to mkv or mp4 or something. I admit i don't have much experience in video manipulation but this is turning out to be exceptionally hard. I have around 5 movies which have 5 vob files each. The movies are around 4 hours long but around 12gb in size for barely 704x570p so i would love to concatinate and compress them. Now here's the problem, none of the video editor software is picking up the subtitles, not handbrake, not even ffmpeg but mpv somehow plays with the subtitles, though it takes mpv 10ish seconds to load the subtitles (weird ik). Now if i join 5 of the files to make 1 complete movie using ffmpeg, the subtitles stop loading. I have no idea what to do :( thanks"},
{"Title": "\"Vimm's Lair\" receives notice to remove tons of games.", "Author": "u/blackletum", "Content": "No content"},
{"Title": "10 year old Cold storage 4TB HGST drive 4 weak sectors 1200 bad sectors? User error ? Expert advice needed ðŸ« ", "Author": "u/Positive_Minds", "Content": "This 4TB drive and also my 10 year old 2TB HGST drive. have popped up bad and weak sectors , they havenâ€™t been switched on in 10 years .. So was this basically my fault for not powering them up every and using disk fresh etc or is it just a case of age related demagnetisation or mechanical failure ? I just donâ€™t get as most of my 10 year old western digital drives or seagate have been fine when tested after 10 years Iâ€™m seriously thinking of doing yearly refreshes of my drives from now on .. My other question is can the weak sectors be reallocated or removed ? I read this on super user , is this true ? In order to keep the data signal from fading, you need to re-write the data. This is often known as â€œhard disk maintenanceâ€, and should be done 3 or 4 times a year. While it does not prevent data from being corrupted or deleted, it can go a long way towards ensuring that the magnetic signal does not fade away completely. The way it works is to read every sector of the drive, and then re-write the data found there, provided the drive reported no errors. If this is done on a regular basis, the magnetic signal of every part of the drive will be refreshed long before the signal fades or becomes ambiguous. This technique also gives the drive controller the opportunity to decide whether to retire any sectors that are becoming too unreliable, before any important data is lost. Magnetic Field Breakdown Most sources state that permanent magnets lose their magnetic field strength at a rate of 1% per year. Assuming this is valid, after ~69 years, we can assume that half of the sectors in a hard drive would be corrupted (since they all lost half of their strength by this time). Obviously, this is quite a long time, but this risk is easily mitigated - simply re-write the data to the drive. How frequently you need to do this depends on the following two issues (I also go over this in my conclusion). To periodically refresh the data on the drive, simply transfer it to another location, and re-writing it back to the drive. That way, the magnetic domains in the physical disk surface will be renewed with their original strength (because you just re-wrote the files back to the disk). If you're concerned about filesystem corruption, you can also format the disk before transferring the data back. You can also help to avoid this issue by archiving your data with recovery data and error correction when you put the data onto the drive. Many archive formats support the inclusion of data recovery algorithms, so even if you have a few corrupted sectors, you can still re-build the lost data. Depending on the priority of the data you've stored, you may want to refresh the hard disk more often. If it is essential data, I would recommend no less then 2 years at maximum. If you can withstand some chance of minor data loss (e.g. a few corrupted sectors here and there), go with 5 years. It doesn't take long to copy the data off the drive, and copy it back. One thing not considered is the servo tracks and markings. These are written one time at the factory and never again (on modern disks). No amount of re-writes by the user or so-called low-level formatting freshens these. Once they fade, they fade! It's different with the first stepper motor disks of the 80's. They don't have servo tracks and a low-level format writes ALL of the bits - fresh."},
{"Title": "Backing up MacBook and multiple SSDs to single HDD", "Author": "u/cloudfortynine", "Content": "Hi everyone, I could use some guidance as I'm unsure what the simplest and low maintenance solution is for me. I'm looking to purchase an external HDD to backup the following: Macbook 1TB SSD with my travel photos to be edited 1TB SSD with my partners travel photos to be edited 2TB SSD with music libraries for music production (Kontakt, Omnisphere etc) At first I was thinking of buying a 8TB external HDD and just backing everything up via Time Machine by having it all plugged in at once. However people online had also mentioned CCC and I'm unsure how that works? I was also hoping to back this all up to a portable HDD but it seems that 8TB HDDs can only be plugged in is that true? I'm not interested in NAS etc. as this is too complicated and expensive for me. As a sidetone I'm also planning on backing up to BackBlaze as well. Thanks in advance for any pointers!"},
{"Title": "Is a dual bay HDD docking station right for me?", "Author": "u/Vanillinn", "Content": "Hello! I only have my toe dipped in data hoarding but I've been meaning to buy the orico 6228US3 dual bay non cloning docking station. Here is my use case: Use my extra 3.5\" 500GB HDD for my modded wii u (to be formatted for the wii u) it will be where my games are installed while running the console back up photos, videos, school files, and game ROMs I have on future storage expansions (1-4TB at a time) have two drives connected to my pc at the same time on a very tight budget as a student In the future, I would like to build my own NAS if I get into bigger data hoarding and archiving. It's something I want to do but do not have the budget for right now. I have a lot of files currently but 1 or 2 more extra drives would be enough. If I could, I'd like to own two copies of my files. I have read some posts here on the subreddit raising issues on cooling, partitions, and corrupted drives. On cooling, would it still be an issue for a dual bay? I don't exactly understand the formatting problem of the partitions so I'd like to ask about that too. On corrupted drives, I'll always wait for the disk to stop spinning before ejection and also not move it as much as possible. Is a dual bay docking station a good low cost entry into data hoarding or is there an alternative more appropriate for my use case?"},
{"Title": "A New Idea for Data Storage: Combining Piezoelectric Materials and 3D NAND", "Author": "u/Mysterious_Crazy9606", "Content": "Hey everyone, Iâ€™ve been thinking about a new way to revolutionize data storage by combining piezoelectric materials with 3D NAND technology. Hereâ€™s the gist of my idea: The Concept â€¢\tHigh-Speed Piezoelectric Module: Use piezoelectric crystals that can oscillate at frequencies in the gigahertz range as an ultra-fast data buffer. This could potentially give us read and write speeds way beyond what we have with current tech. â€¢\tMain 3D NAND Storage: Use 3D NAND for the main long-term storage. We all know itâ€™s reliable and has a high capacity. How It Would Work 1.\tWriting Data: Incoming data would first go to the piezoelectric module at super high speeds. 2.\tTransferring Data: The data would then be transferred to the main 3D NAND storage for long-term keeping. 3.\tReading Data: For reads, the system would first check the piezoelectric buffer for quick access. If the data isnâ€™t there, it would pull from the 3D NAND storage. Benefits â€¢\tSpeed: This setup could drastically reduce latency and boost read/write speeds. â€¢\tEnergy Efficiency: Piezoelectric materials might be more energy-efficient for rapid operations. â€¢\tNew Storage Architecture: Combining the speed of piezoelectrics with the capacity of 3D NAND could create a super-efficient storage solution. Challenges â€¢\tTech Integration: Making sure the piezoelectric and 3D NAND components work seamlessly together. â€¢\tCost: High-quality piezoelectric materials and the complexity of this setup might be pricey. â€¢\tDurability and Reliability: The materials need to handle high-frequency oscillations over long periods without wearing out. Potential Impact If we can make this work, it could be a game-changer for data centers, mobile devices, and industrial applications that need ultra-fast response times. What do you all think? Could this actually work?"},
{"Title": "help with a complicated wget string : how do -A and -R args stack, and is there any means of boolean filtering ?", "Author": "u/ImaginaryCheetah", "Content": "good afternoon, i'm not particularly familiar with wget so i'm asking the experts for assistance... my current string of wget -A \"*(USA)*zip\" -R -m -p -E -k -K -np -nd -w 2 https://target works fine to pull all files with \"(USA)\" included in the name, but i'd like to understand if i can get more complicated. does the -R arg work in conjunction with -A or would it override ? for example, wget -A \"*(USA)*zip\" -R \"*(Demo)*\" [etc] would this return all files with \"(USA)\" in the title unless it also had \"(Demo)\" in the file name ? is there any way of passing a boolean criteria through wget ? \"get files with (Europe) in the title if same file with (USA) in the title doesn't exist\" kind of thing ? i expect that might require grabbing a list of files with lftp and processing it instead of having wget do that kind of logic."},
{"Title": "Lsi hba 9207-8i", "Author": "u/Crystal_Jayhawk", "Content": "No content"},
{"Title": "Looking for an alternative to dban to wipe an old computer with SSD", "Author": "u/ilovecokeslurpees", "Content": "I have an old computer (turned it off 2.5 years ago but it was a mid-spec PC built in 2016 I bought second hand in 2017). I can barely boot it these days. I want to wipe the 1 TB SSD before throwing the whole PC into the trash or recycle or whatever. Most parts are unsalvageable. I used to use dban way back when but it appears to not have been updated in almost a decade. The PC is running Windows 10, but I would like a simple solution bootable from a USB to ensure complete data erasure. I tried ABAN but it couldn't detect anything. Any suggestions?"},
{"Title": "Historical data hoarders at the library of Alexandria lost untolds amount of work and knowledge after the library was burned. Sumerian texts survived 4000+ years due to being written on clay tablets. Is there any efforts to transcribe some of our knowledge into more permanent media?", "Author": "u/rrybwyb", "Content": "I mostly hoard books. Its amazing how many I can fit onto just a small 10 tb hard drive. But if that gets wet, dropped, or someone holds a magnet to it, I've lost millions of hours of research and knowledge from 10's of thousands of authors. Even looking at the history of the dead sea scrolls, Whatever idiots were in charge of transporting those did an awful job. They were taken from the dry desert to a place where they could get humid and rot. Are there any organizations out there that have transcribed some of our more important items into stone or clay? I've been looking more into Sumerian history and the reason we have many of these items still is because they were carved into clay - and clay can last a pretty long time. Its kind of short sighted of humans to think we're immune to a giant asteroid or nuclear winter. In that situation, What would humans 5,000 years in the future after a major catastrophe be able to look back on and decipher about the 2000s? Edit: also I can't believe I forgot about plastic. That supposedly forever product that never breaks down. Does anyone know if it is really as permanent as something like ceramic? I've seen it become quite brittle and disintegrate when left out in the sun for even 6-12 months"},
{"Title": "Issues with LSI 9300-16i wiping partition tables", "Author": "u/Electronic-Papaya", "Content": "I'm trying to free up some PCIe slots in my system so I'm switching from 2 x LSI 2008 to a single 16 port LSI 9300-16i.  I'm running Linux and using mdadm to run 3 arrays. Before I attached any of my drives with data and arrays configured, I installed the card with no drives attached and update the firmware to 16.00.12.00 , and made sure it was in IT mode.  As a test I then connected one of my arrays to the controller and booted the system.  After booting up, the drives were detected fine (4 x 2TB) but the array was gone.  It appears that the metadata was erased, mdadm didn't recognize any drive as being part of an array. I was able to recover the data and the array by following the steps here: https://raid.wiki.kernel.org/index.php/Recovering_a_damaged_RAID However, if I reboot again the same thing happens, the metadata is lost and mdadm does not recognize that the drives are part of an array.   I do not have this issue with the older LSI controllers. Any idea what's going on here?  When I created the array I used the entre drive, so I did not create a Linux RAID partition on each drive.  The array is configured using /dev/sda to /dev/sdd, and not /dev/sda1 to /dev/sdd1.  Not sure that has anything to do with it. Edit:  Seems to be an issue with the controller and GPT partition tables.  As a test, I created an array with a couple 120gb SSD's I had laying around.  I created the array with the whole drive, rebooted, and after a reboot the array was still present.  I realized my other drives are configured as GPT.  So I wiped the SSDs, switched them to GPT and again created the array.  This time after a reboot the array was gone, mdadm does not recognize the drives as being part of an array.  Not sure how to fix this."},
{"Title": "Buying drives from eBay?", "Author": "u/Seventeen-Oncelers", "Content": "I was considering buying a 10tb HDD on eBay and was wondering if this is considered a good place to buy? It's certified eBay refurbished, 30 day returns etc. Just curious on you guy's thoughts on buying hard drives on eBay."},
{"Title": "Cost Efficient Storage Revamp", "Author": "u/iLOLZU", "Content": "I'm want to get into archival with all the shenanigans with the Internet Archive and game preservation. I'm in need of a storage upgrade anyways. I have a Lian Li O11 with the 2x 3.5\" drive cage unpopulated and 2x 2.5\" HDDs (2.5TB total) installed, was thinking of getting a pair of 3.5\" HDDs & a pair of SATA SSDs. Considering 8TB Seagate Firecuda + 2TB Samsung 870s. I know higher capacity 3.5\" HDDs exist, but heard that buying a single high capacity drive is not worth while in terms of cost and data loss."},
{"Title": "Help needed with my western digital elements hdd", "Author": "u/Tiredcardinal", "Content": "I have a 4 year old WD elements 1.5tb hdd It was working completely fine until the starting of this year but all of a sudden it started giving buggy video outputs and I couldn't run games on it anymore So recently I decided to format it , only for it to be faulty a day later now I can copy any data to hdd normally but I cannot transfer anything for it Any suggestions or troubleshoots are welcome and appreciated"},
{"Title": "Software to recover data from a formatted and written disk?", "Author": "u/lsgz3", "Content": "The disk has been formatted twice and written with New stuff. Is there any chance of recovering old files?"},
{"Title": "3D printed 8-Bay DAS with Supermicro backplane, trays, PSUs and external SAS-8088 connectors", "Author": "u/kschaffner", "Content": "https://imgur.com/a/YcbODga https://makerworld.com/en/models/491457 I've been working hard on this project for the past couple months in my spare time trying to make a product that I couldn't really find on the market. I had some extra PSUs and fans and supermicro trays so I figured why not design around that. I've probably put no less than 40+ hours into design, print, redesign, print, try fitment etc. Not the most experience with Fusion360 or CAD in general. The front LEDs do light up during activity :). I've been thinking of expanding this into a mATX or Mini-ITX supported case as well. I know this is also pretty much on the heals of shaztech_info but I think we have enough differences between us and I was kinda shocked to see someone else coming out with a similar idea around the same time lol. Let me know what you fellas and fellettes think! From Makerworld: I was unable to find a product that met my specifications of 8 hot-swap bays, a SAS backplane and external SAS connectors for easy connectivity, so I decided to design and build my own 8-bay Direct Attached Storage (DAS). This DAS features a Supermicro SAS833TQ backplane with Gen 5.5 hot-swap 3.5\" trays. It is powered by either a 200W PWS-203-1H (as-shown) or a 350W PWS-351-1H (or similar units of the same dimensions: PWS-351-1H 100 x 40 x 220mm, PWS-203-1H 76 x 40.3 x 192mm) power supply. Additionally, it includes an external SAS SFF-8088 to SFF-8087 adapter to cut down on internal wires. I printed this on my X1C and have realized that making it available for smaller print beds should also be done instead of 1 large single print. The body is 255 W x 190 H x 245mm D, it is printed as a single print, no supports needed. The body at 10% infill, 2 walls with gyroid infill .20mm layer height with a 0.4mm nozzle. This print requires NO supports to print. The print of the body will use approximately 984.5g of filament if no painting is done of the logo or numbers, and 982.6g if painted of a single color. This print took roughly 20 hours to print. I realize this might also be a turn off being a large print but I wasn't wanting to have to glue or screw it together if I could help it. If you are wanting this, let me know and I can work it into a new version. There are 2 version of the back plate for the different sizes of PSUs mentioned. Here is the list of parts used for this build: Part Price Supermicro Gen 5.5 3.5\" trays (MCP-220-00075-0B) x8 ~$50 for 8 on eBay Supermicro SAS833TQ 8-bay SAS backplane ~$35 on eBay SFF-8087 to SATA breakout cable x2 ~$16 SFF-8088 to SFF-8087 adapter ~$30 Supermicro 1U PSU of dimensions 100 x 40 x 220mm or 76 x 40.3 x 192mm (As seen PWS-203-1H) ~32$ on eBay, also designed screw hole for PWS-351-1H ~$30 on eBay Molex Y-cable ~$6 120mm of your choice x2 (Noctua NF-P12 shown) ~$16 ea. ATX power jumper cable w/ switch ~$11 This required a tool to remove the pins from the connector to feed it through the hole ~$17, you don't have to get one like this, but I wanted the other pin extractors for future projects.) Grand Total of parts: $210, could save $32 with some random 120mm fans as long as they can pull through all the trays. For hardware needed: Part qty M3x4x5 Heatset inserts 7 M4x6x6 Heatset inserts 6 M3x6 socket head screw 3 M3x12 socket head screw 4 M4x8 socket head screw 6 Some optional parts that might be desired: 120mm wire fan grills ~$8 Some rubber feet for the bottom"},
{"Title": "Can my old (possibly infected?) digital cameraâ€™s SD card and/or photos transfer a virus to my iPhone via an adapter?", "Author": "u/knOn0", "Content": "Hello! (Iâ€™ve asked this already but wanted to widen the net of insight before I proceed) I have an old digital camera from 2009-2011. I recently bought a new charger for it, turned it on, and it works perfectly! When I first turned it on, I was able to see tons of old pictures that Iâ€™d forgotten about; now, I can see my old photos but any new photo that I try to view has the â€œFile Errorâ€ message. I did buy an SD card adapter to put my photos from the SD card to my iPhone, and now Iâ€™m seeing a â€œcontent unavailable: files could not be opened due to an unknown error.â€ (The iPhone is an 11, IOS 17). Does my SD card have a virus? Do the photos on them have a virus? Should I get a brand new adapter and SD card? TYIA"},
{"Title": "Which is more safe, a partitioned SSD (OS/Data) or two separate SSDs (one for OS, one for Data)", "Author": "u/cs_legend_93", "Content": "Hello all I'm trying to settle a debate with a other redditor. Assume both of these scenarios are basic setups without drive pools or raid. The redditor suggests and recommends that using a single SSD with a partition for the OS and Data drive is more safe than using dual drives. I believe using a partitioned SSD will both double your chances of drive failure due to writes and reads, and it will make it a pain in the ass to restore the backup. I suggested using two separate SSDs, and the redditor said that this indeed doubles the chances of drive failure due to two drives. I disagreed and said that it halves it due to the decreased reads and writes. I also suggested that dual drives will make it easier to restore a backup drive if one fails. Which scenario is better? In both scenerios there are backups, like a mirrored drive using Acronis disk imager or something like that. But it's still not a drive pool or RAID Here is the debate: https://www.reddit.com/r/buildapc/s/ArnZMYuQSD"},
{"Title": "Is there a software that batch reverse search images and download the best version of it?", "Author": "u/BulgyBoy123", "Content": "Hi guys, I'm looking for a software that is able to batch reverse search some images. I downloaded all of my pinterest boards, but some of the files are really tiny. I wouldn't mind being able to download bigger versions of said files without having to spend weeks doing that manually."},
{"Title": "Price Point", "Author": "u/Crafty_Future4829", "Content": "I apologize in advanced as I know there are a lot related posts to buying refurbished/used hard drives for non critical data.  On eBay,  it seems you can get 16tb exos drives for around 160.00 or 10 per tb.  They say refurbished with zero hours and the reseller (goharddrives) offers a 5 year warranty. Where do these hard drives from?  Do they really have zero hours as opposed to having smart data wiped?  Is this a good deal? I read another post being able to get used drives for around 5 dollars per tb. What is your sweet spot and reliable ebay sellers you would buy from? Thanks"},
{"Title": "Huge Collection", "Author": "u/CoreDreamStudiosLLC", "Content": "Good evening, hope all is well. I lost my dad on Saturday and was going through some things to get bills paid/cancelled, etc. He had over 430 media discs at his apartment which I've taken back to me. Most are DVD-R, about 20 are DVD, and the rest are DVD+RW and Divx. Is it worth ripping each one of the DVD-R's at minimum? Some are very rare and streaming sites probably won't even half this collection."},
{"Title": "Internet Download Manager stopped detecting YouTube MKV format videos to download", "Author": "u/TheHighImperial", "Content": "Just today I discovered that it detects them if the video is embedded on a forum but not showing for the same video directly at YouTubes page. Ive updated the browser, IDM extension and even software. I also tried it on a different computer... same problem."},
{"Title": "is this samsung 512 gb micro sd fake?", "Author": "u/UlisesDeveloper", "Content": "No content"},
{"Title": "Hey folks, here's the entire Computer Science curriculum organized in 1000 YouTube videos that you can just play and start learning. There are 40 courses in total, further organized in 4 academic years, each containing 2 semesters. I hope that everyone who wants to learn, will find this helpful.", "Author": "u/volunteervancouver", "Content": "No content"},
{"Title": "We're gonna need another napster soon", "Author": "u/Scuczu2", "Content": "No content"},
{"Title": "Hoarding =/= Preservation", "Author": "u/Lee__Jieun", "Content": "No content"},
{"Title": "Hey folks, here's the entire Computer Science curriculum organized in 1000 YouTube videos that you can just play and start learning. There are 40 courses in total, further organized in 4 academic years, each containing 2 semesters. I hope that everyone who wants to learn, will find this helpful.", "Author": "u/volunteervancouver", "Content": "No content"},
{"Title": "HDD destruction day at work today", "Author": "u/AnxietyBytes", "Content": "No content"},
{"Title": "We're gonna need another napster soon", "Author": "u/Scuczu2", "Content": "No content"},
{"Title": "The Internet Archive is now preserving Flash animations and games", "Author": "u/retrac1324", "Content": "No content"},
{"Title": "Hoarding =/= Preservation", "Author": "u/Lee__Jieun", "Content": "No content"},
{"Title": "The Ripinator restoration/build (15$ thrift store steal)", "Author": "u/Crossheart963", "Content": "No content"},
{"Title": "HDD destruction day at work today", "Author": "u/AnxietyBytes", "Content": "No content"},
{"Title": "In 1886, the US Government Commissioned 7,500 Watercolor Paintings of Every Known Fruit in the World: Download Them in High Resolution", "Author": "u/fawkesdotbe", "Content": "No content"},
{"Title": "The Internet Archive is now preserving Flash animations and games", "Author": "u/retrac1324", "Content": "No content"},
{"Title": "4TB marked down to $21 at Walmart.", "Author": "u/TheSkinnyD", "Content": "No content"},
{"Title": "The Ripinator restoration/build (15$ thrift store steal)", "Author": "u/Crossheart963", "Content": "No content"},
{"Title": "Don't Let Reddit Kill 3rd Party Apps!", "Author": "u/MagicDalsi", "Content": "EDIT: Don't use this post any more: it's been crossposted so widely that it breaks Reddit when trying to open it! It's been locked. Further discussion (and crossposts) should go HERE. What's going on? A recent Reddit policy change threatens to kill many beloved third-party mobile apps, making a great many quality-of-life features not seen in the official mobile app permanently inaccessible to users. On May 31, 2023, Reddit announced they were raising the price to make calls to their API from being free to a level that will kill every third party app on Reddit, from Apollo to Reddit is Fun to Narwhal to BaconReader . Even if you're not a mobile user and don't use any of those apps, this is a step toward killing other ways of customizing Reddit, such as Reddit Enhancement Suite or the use of the old.reddit.com desktop interface . This isn't only a problem on the user level: many subreddit moderators depend on tools only available outside the official app to keep their communities on-topic and spam-free. What's the plan? On June 12th, many subreddits will be going dark to protest this policy. Some will return after 48 hours: others will go away permanently unless the issue is adequately addressed, since many moderators aren't able to put in the work they do with the poor tools available through the official app. This isn't something any of us do lightly: we do what we do because we love Reddit , and we truly believe this change will make it impossible to keep doing what we love. The two-day blackout isn't the goal , and it isn't the end. Should things reach the 14th with no sign of Reddit choosing to fix what they've broken, we'll use the community and buzz we've built between then and now as a tool for further action. What can you do? Complain. Message the mods of r/reddit .com, who are the admins of the site: message u/reddit : submit a support request : comment in relevant threads on r/reddit , such as this one , leave a negative review on their official iOS or Android app- and sign your username in support to this post. Spread the word. Rabble-rouse on related subreddits. Meme it up, make it spicy. Bitch about it to your cat. Suggest anyone you know who moderates a subreddit join us at our sister sub at r/ModCoord - but please don't pester mods you don't know by simply spamming their modmail. Boycott and spread the word...to Reddit's competition! Stay off Reddit entirely on June 12th through the 13th- instead, take to your favorite non -Reddit platform of choice and make some noise in support! Don't be a jerk. As upsetting this may be, threats, profanity and vandalism will be worse than useless in getting people on our side. Please make every effort to be as restrained, polite, reasonable and law-abiding as possible. This includes not harassing moderators of subreddits who have chosen not to take part : no one likes a missionary, a used-car salesman, or a flame warrior."},
{"Title": "In 1886, the US Government Commissioned 7,500 Watercolor Paintings of Every Known Fruit in the World: Download Them in High Resolution", "Author": "u/fawkesdotbe", "Content": "No content"},
{"Title": "Low Disk Space", "Author": "u/Flying-T", "Content": "No content"},
{"Title": "4TB marked down to $21 at Walmart.", "Author": "u/TheSkinnyD", "Content": "No content"},
{"Title": "The Internet Archive lost their court case", "Author": "u/safels2", "Content": "kys u/spez"},
{"Title": "Don't Let Reddit Kill 3rd Party Apps!", "Author": "u/MagicDalsi", "Content": "EDIT: Don't use this post any more: it's been crossposted so widely that it breaks Reddit when trying to open it! It's been locked. Further discussion (and crossposts) should go HERE. What's going on? A recent Reddit policy change threatens to kill many beloved third-party mobile apps, making a great many quality-of-life features not seen in the official mobile app permanently inaccessible to users. On May 31, 2023, Reddit announced they were raising the price to make calls to their API from being free to a level that will kill every third party app on Reddit, from Apollo to Reddit is Fun to Narwhal to BaconReader . Even if you're not a mobile user and don't use any of those apps, this is a step toward killing other ways of customizing Reddit, such as Reddit Enhancement Suite or the use of the old.reddit.com desktop interface . This isn't only a problem on the user level: many subreddit moderators depend on tools only available outside the official app to keep their communities on-topic and spam-free. What's the plan? On June 12th, many subreddits will be going dark to protest this policy. Some will return after 48 hours: others will go away permanently unless the issue is adequately addressed, since many moderators aren't able to put in the work they do with the poor tools available through the official app. This isn't something any of us do lightly: we do what we do because we love Reddit , and we truly believe this change will make it impossible to keep doing what we love. The two-day blackout isn't the goal , and it isn't the end. Should things reach the 14th with no sign of Reddit choosing to fix what they've broken, we'll use the community and buzz we've built between then and now as a tool for further action. What can you do? Complain. Message the mods of r/reddit .com, who are the admins of the site: message u/reddit : submit a support request : comment in relevant threads on r/reddit , such as this one , leave a negative review on their official iOS or Android app- and sign your username in support to this post. Spread the word. Rabble-rouse on related subreddits. Meme it up, make it spicy. Bitch about it to your cat. Suggest anyone you know who moderates a subreddit join us at our sister sub at r/ModCoord - but please don't pester mods you don't know by simply spamming their modmail. Boycott and spread the word...to Reddit's competition! Stay off Reddit entirely on June 12th through the 13th- instead, take to your favorite non -Reddit platform of choice and make some noise in support! Don't be a jerk. As upsetting this may be, threats, profanity and vandalism will be worse than useless in getting people on our side. Please make every effort to be as restrained, polite, reasonable and law-abiding as possible. This includes not harassing moderators of subreddits who have chosen not to take part : no one likes a missionary, a used-car salesman, or a flame warrior."},
{"Title": "\"The Truth is Paywalled But the Lies Are Free\": Notes on why I hoard data", "Author": "u/gabefair", "Content": "I came across a beautifully written article by Nathan J. Robinson about how quality work costs money to access and propaganda is freely given. The article makes some good points on why it is important for data to be more free, which I will summarize below: Nobody is allowed to build a giant free database of everything human beings have ever produced. 2) Copyright law can be an intensive restriction on the freedom of speech and determines what information you can (and not) share with others. 3) The concept of a public community library needs to evolve. As books, and other content move online, our communities have as well. 4)  Human creativity and potential is phenomenally leashed when human knowledge is limited. 5) Free and affordable libraries/sources of wisdom are dying. This got me thinking about why I care about hoarding data. Data is invaluable! A digital dark age is forming around us and we can do what we can to prevent it. A lot of people here will hoard data for personal reasons. I hoard data for others. The things the people in this subreddit hoard whether it be movies, Youtube, pictures, news articles, websites, all of it is culture. Its history. Even memes and social media are not crap. Even literal shit is valuable to a scatologist. Can you imagine if we were able to find the preserved excrement from a long extinct animal? What one sees as shit, is so much more to someone else who is trained and educated. Its data. The internet and social media around us is Art and Culture from our time. This is history for the future to use and learn. Things go viral for a reason. The information shared in the jokes and content are snapshots of the public's thinking and perspective on the world. Invaluable data for future scholars. Imagine we found a Viking warship and on it was a perfectly preserved book of jokes. Sure many at the time might have thought they were shit jokes made at the expense of others. But we would learn so much about their customs, society, and the evolution of human civilization if this book was preserved and found. And the book's contents were made available to the world. Also a lot of political content is shared on social media and comment sections as well. Our understanding of politics  will be carved up in units of memes, and shared on thousands of siloed paywalled platforms and mediums over time. And our role is to collect and consolidate them. This is but a small sliver of the documentation of how our world is changing around us. And we can do our part to save and make free to others as much of it as we can. P.S. Many reddit accounts unknowingly (like maybe yours) are being used by bots to vote for content. Please enable 2FA to stop this practice. Instructions P.P.S. Summer of 2020 is time for contingency preparedness. There is no time to get started like the present. Buy your disks now to be prepared for when history needs you. P.P.P.S. Thank you all for the support and discussion so far. You are some good folks! A song that I enjoy due to it relating to the importance preserving history is \"Amnesia\" by Dead Can Dance. It has a line in the song that I find quite chilling, \"Can you really plan the future when you no longer have the past?\" P.P.P.P.S. Some people like to use the plural verb \"data are\" instead of the singular \"data is\" since data are used to refer to a collection. \"The fish are being collected\". I merely mention this as a factoid in celebration of this discussion receiving so much attention. P.P.P.P.P.S. Take a look at this list of site-deaths to remind us of all the now dead sites that once existed. P.P.P.P.P.P.S For further motivation, consider how: Facebook is deleting evidence of war crimes"},
{"Title": "Low Disk Space", "Author": "u/Flying-T", "Content": "No content"},
{"Title": "This meme speaks to me", "Author": "u/Fox_Uni_Charlie_Kilo", "Content": "No content"},
{"Title": "The Internet Archive lost their court case", "Author": "u/safels2", "Content": "kys u/spez"},
{"Title": "Absolutely unacceptable - Newegg shipped me drives like this", "Author": "u/redditor1101", "Content": "No content"},
{"Title": "\"The Truth is Paywalled But the Lies Are Free\": Notes on why I hoard data", "Author": "u/gabefair", "Content": "I came across a beautifully written article by Nathan J. Robinson about how quality work costs money to access and propaganda is freely given. The article makes some good points on why it is important for data to be more free, which I will summarize below: Nobody is allowed to build a giant free database of everything human beings have ever produced. 2) Copyright law can be an intensive restriction on the freedom of speech and determines what information you can (and not) share with others. 3) The concept of a public community library needs to evolve. As books, and other content move online, our communities have as well. 4)  Human creativity and potential is phenomenally leashed when human knowledge is limited. 5) Free and affordable libraries/sources of wisdom are dying. This got me thinking about why I care about hoarding data. Data is invaluable! A digital dark age is forming around us and we can do what we can to prevent it. A lot of people here will hoard data for personal reasons. I hoard data for others. The things the people in this subreddit hoard whether it be movies, Youtube, pictures, news articles, websites, all of it is culture. Its history. Even memes and social media are not crap. Even literal shit is valuable to a scatologist. Can you imagine if we were able to find the preserved excrement from a long extinct animal? What one sees as shit, is so much more to someone else who is trained and educated. Its data. The internet and social media around us is Art and Culture from our time. This is history for the future to use and learn. Things go viral for a reason. The information shared in the jokes and content are snapshots of the public's thinking and perspective on the world. Invaluable data for future scholars. Imagine we found a Viking warship and on it was a perfectly preserved book of jokes. Sure many at the time might have thought they were shit jokes made at the expense of others. But we would learn so much about their customs, society, and the evolution of human civilization if this book was preserved and found. And the book's contents were made available to the world. Also a lot of political content is shared on social media and comment sections as well. Our understanding of politics  will be carved up in units of memes, and shared on thousands of siloed paywalled platforms and mediums over time. And our role is to collect and consolidate them. This is but a small sliver of the documentation of how our world is changing around us. And we can do our part to save and make free to others as much of it as we can. P.S. Many reddit accounts unknowingly (like maybe yours) are being used by bots to vote for content. Please enable 2FA to stop this practice. Instructions P.P.S. Summer of 2020 is time for contingency preparedness. There is no time to get started like the present. Buy your disks now to be prepared for when history needs you. P.P.P.S. Thank you all for the support and discussion so far. You are some good folks! A song that I enjoy due to it relating to the importance preserving history is \"Amnesia\" by Dead Can Dance. It has a line in the song that I find quite chilling, \"Can you really plan the future when you no longer have the past?\" P.P.P.P.S. Some people like to use the plural verb \"data are\" instead of the singular \"data is\" since data are used to refer to a collection. \"The fish are being collected\". I merely mention this as a factoid in celebration of this discussion receiving so much attention. P.P.P.P.P.S. Take a look at this list of site-deaths to remind us of all the now dead sites that once existed. P.P.P.P.P.P.S For further motivation, consider how: Facebook is deleting evidence of war crimes"},
{"Title": "Why throw away old ~60GB drives? Why not make something to commemorate their service?", "Author": "u/aforsberg", "Content": "No content"},
{"Title": "This meme speaks to me", "Author": "u/Fox_Uni_Charlie_Kilo", "Content": "No content"},
{"Title": "Itâ€™s because of youtube-dl that we have the audio recordings of Bitfinex executive admitting to bank fraud", "Author": "u/dharmatech", "Content": "No content"},
{"Title": "Absolutely unacceptable - Newegg shipped me drives like this", "Author": "u/redditor1101", "Content": "No content"},
{"Title": "35+ years worth TV, 24/7. A true data hoarding legend", "Author": "u/orochimaru1999", "Content": "No content"},
{"Title": "Why throw away old ~60GB drives? Why not make something to commemorate their service?", "Author": "u/aforsberg", "Content": "No content"},
{"Title": "It was a good electronics recycling day at work today.", "Author": "u/VertexBeatz", "Content": "No content"},
{"Title": "Itâ€™s because of youtube-dl that we have the audio recordings of Bitfinex executive admitting to bank fraud", "Author": "u/dharmatech", "Content": "No content"},
{"Title": "Tokyo Resident who's been filming scenes in Japan since 1990 has over 12,000 videos on youtube", "Author": "u/Ayit_Sevi", "Content": "So, I've found myself downloading a lot of historical footage and I stumbled upon this guy, Lyle Hiroshi Saxon . The dude has been on youtube since 2007 and over the period of 14 years has uploaded 12,967 videos . He's been a resident since 1984 and has footage dating from 1990-1993 and from 2008-present. It's by far the biggest channel I've ever downloaded. He even has a webpage/blog Even if it looks like he hasn't updated it in a while. Thought it was interesting enough to share"},
{"Title": "35+ years worth TV, 24/7. A true data hoarding legend", "Author": "u/orochimaru1999", "Content": "No content"},
{"Title": "Going to add some actual drives to it when I get the chance... Until then:", "Author": "Unknown author", "Content": "No content"},
{"Title": "It was a good electronics recycling day at work today.", "Author": "u/VertexBeatz", "Content": "No content"},
{"Title": "At least 223 companies have manufactured hard disk drives. Most of that industry has vanished through bankruptcy or mergers and acquisitions. None of the first four entrants continue in the industry today.", "Author": "u/dabderax", "Content": "No content"},
{"Title": "Tokyo Resident who's been filming scenes in Japan since 1990 has over 12,000 videos on youtube", "Author": "u/Ayit_Sevi", "Content": "So, I've found myself downloading a lot of historical footage and I stumbled upon this guy, Lyle Hiroshi Saxon . The dude has been on youtube since 2007 and over the period of 14 years has uploaded 12,967 videos . He's been a resident since 1984 and has footage dating from 1990-1993 and from 2008-present. It's by far the biggest channel I've ever downloaded. He even has a webpage/blog Even if it looks like he hasn't updated it in a while. Thought it was interesting enough to share"},
{"Title": "A full house.", "Author": "u/ast3r3x", "Content": "No content"},
{"Title": "Going to add some actual drives to it when I get the chance... Until then:", "Author": "Unknown author", "Content": "No content"},
{"Title": "Only in San Francisco ðŸ¤£", "Author": "u/anxman", "Content": "No content"},
{"Title": "At least 223 companies have manufactured hard disk drives. Most of that industry has vanished through bankruptcy or mergers and acquisitions. None of the first four entrants continue in the industry today.", "Author": "u/dabderax", "Content": "No content"},
{"Title": "House Speaker deleted his podcast. Hoarders to the rescue", "Author": "u/harrro", "Content": "No content"},
{"Title": "A full house.", "Author": "u/ast3r3x", "Content": "No content"},
{"Title": "Complete US PlayStation 2 manual collection posted to archive.org", "Author": "u/K1rkl4nd", "Content": "To celebrate the PlayStation 2's 22nd anniversary on Wednesday I have uploaded my complete US manual collection- personally scanned and edited to 4K resolution- to archive.org.  17GB of goodiness across 1795 titles plus an additional ~100 variants, art books, mini-guides, and comics.  The upload is done- it's \"processing\" now.  Be sure to download the original files, not anything archive.org generates (sometimes they recompress things poorly trying to OCR). https://archive.org/details/kirklands-manual-labor-sony-playstation-2-usa-4k-version"},
{"Title": "Only in San Francisco ðŸ¤£", "Author": "u/anxman", "Content": "No content"},
{"Title": "60 hard drives, spinning down", "Author": "u/geerlingguy", "Content": "No content"},
{"Title": "House Speaker deleted his podcast. Hoarders to the rescue", "Author": "u/harrro", "Content": "No content"},
{"Title": "Mistakes were made.", "Author": "u/Miss_Zia", "Content": "No content"},
{"Title": "Complete US PlayStation 2 manual collection posted to archive.org", "Author": "u/K1rkl4nd", "Content": "To celebrate the PlayStation 2's 22nd anniversary on Wednesday I have uploaded my complete US manual collection- personally scanned and edited to 4K resolution- to archive.org.  17GB of goodiness across 1795 titles plus an additional ~100 variants, art books, mini-guides, and comics.  The upload is done- it's \"processing\" now.  Be sure to download the original files, not anything archive.org generates (sometimes they recompress things poorly trying to OCR). https://archive.org/details/kirklands-manual-labor-sony-playstation-2-usa-4k-version"},
{"Title": "55.5GB of Lego Instructions, time to find the rest!", "Author": "u/xiyatumerica", "Content": "No content"},
{"Title": "60 hard drives, spinning down", "Author": "u/geerlingguy", "Content": "No content"},
{"Title": "YouTube deleted \"SemperVideo\" a 13-year-old german educational channel focused on IT", "Author": "u/AndaPlays", "Content": "Yeah, see the title. Just a reminder that nothing is safe on YouTube and you should backup your favorite stuff. This channel had thousands of videos. Was great stuff. From how to set up basic stuff in windows to how to configures your Linux server correctly. His Twitter: https://twitter.com/SemperVideo/status/1352503560525795328 https://socialblade.com/youtube/user/sempervideo Edit: He received his first Strike for a VPN video and the second one for showing how to get the new Windows 10 Version 2004 start menu. For the third one, he didn't even receive an email or notification yet for what video he got taken down. http://sempervideo.de/youtube-hat-sempervideo-geloescht/ Edit 2: The channel is backup. Just for how long If their ridiculous policy and bots are still up. https://twitter.com/SemperVideo/status/1352708103125561344 https://www.youtube.com/user/SemperVideo Edit 3: So the channel is up again, but he still has the previous two strikes from the VPN video and the win 10 video. So It's just a matter of time again till another video will get a strike again. And If you don't know after 3 strikes on YouTube, your channels get deleted. That's what happened here. His perspective(german): https://www.youtube.com/watch?v=ZiYLPjCWoXU In summary, he already deleted over 1k videos from YouTube after he received the second strike a few weeks ago. But with over a few thousand videos you can't really pinpoint which videos will get you maybe a strike again. If a user reports a video from him, a \"YouTube Specialist\" will look at the video who doesn't know shit about IT and then says: \"Yeah, that could be dangerous\" so here you have a strike. And after 3 you're gone."},
{"Title": "Mistakes were made.", "Author": "u/Miss_Zia", "Content": "No content"},
{"Title": "Intel suffers massive data breach involving confidential company and CPU information revealing hardcoded backdoors.", "Author": "u/kurtstir", "Content": "Intel  suffered a massive data breach earlier this year and as of today the  first associated data has begun being released.  Some users are  reporting finding hardcoded backdoors in the intel code. Some of the contents of this first release: - Intel ME Bringup guides + (flash) tooling + samples for various platforms -  Kabylake (Purley Platform) BIOS Reference Code and Sample Code +  Initialization code (some of it as exported git repos with full history) - Intel CEFDK (Consumer Electronics Firmware Development Kit (Bootloader stuff)) SOURCES - Silicon / FSP source code packages for various platforms - Various Intel Development and Debugging Tools - Simics Simulation for Rocket Lake S and potentially other platforms - Various roadmaps and other documents - Binaries for Camera drivers Intel made for SpaceX - Schematics, Docs, Tools + Firmware for the unreleased Tiger Lake platform - (very horrible) Kabylake FDK training videos - Intel Trace Hub + decoder files for various Intel ME versions - Elkhart Lake Silicon Reference and Platform Sample Code - Some Verilog stuff for various Xeon Platforms, unsure what it is exactly. - Debug BIOS/TXE builds for various Platforms - Bootguard SDK (encrypted zip) - Intel Snowridge / Snowfish Process Simulator ADK - Various schematics - Intel Marketing Material Templates (InDesign) - Lots of other things https://twitter.com/deletescape/status/1291405688204402689"},
{"Title": "55.5GB of Lego Instructions, time to find the rest!", "Author": "u/xiyatumerica", "Content": "No content"},
{"Title": "How books are scanned.", "Author": "u/ReturnMuch9510", "Content": "No content"},
{"Title": "YouTube deleted \"SemperVideo\" a 13-year-old german educational channel focused on IT", "Author": "u/AndaPlays", "Content": "Yeah, see the title. Just a reminder that nothing is safe on YouTube and you should backup your favorite stuff. This channel had thousands of videos. Was great stuff. From how to set up basic stuff in windows to how to configures your Linux server correctly. His Twitter: https://twitter.com/SemperVideo/status/1352503560525795328 https://socialblade.com/youtube/user/sempervideo Edit: He received his first Strike for a VPN video and the second one for showing how to get the new Windows 10 Version 2004 start menu. For the third one, he didn't even receive an email or notification yet for what video he got taken down. http://sempervideo.de/youtube-hat-sempervideo-geloescht/ Edit 2: The channel is backup. Just for how long If their ridiculous policy and bots are still up. https://twitter.com/SemperVideo/status/1352708103125561344 https://www.youtube.com/user/SemperVideo Edit 3: So the channel is up again, but he still has the previous two strikes from the VPN video and the win 10 video. So It's just a matter of time again till another video will get a strike again. And If you don't know after 3 strikes on YouTube, your channels get deleted. That's what happened here. His perspective(german): https://www.youtube.com/watch?v=ZiYLPjCWoXU In summary, he already deleted over 1k videos from YouTube after he received the second strike a few weeks ago. But with over a few thousand videos you can't really pinpoint which videos will get you maybe a strike again. If a user reports a video from him, a \"YouTube Specialist\" will look at the video who doesn't know shit about IT and then says: \"Yeah, that could be dangerous\" so here you have a strike. And after 3 you're gone."},
{"Title": "Boss OGs", "Author": "u/kin_zindestroyer", "Content": "No content"},
{"Title": "Intel suffers massive data breach involving confidential company and CPU information revealing hardcoded backdoors.", "Author": "u/kurtstir", "Content": "Intel  suffered a massive data breach earlier this year and as of today the  first associated data has begun being released.  Some users are  reporting finding hardcoded backdoors in the intel code. Some of the contents of this first release: - Intel ME Bringup guides + (flash) tooling + samples for various platforms -  Kabylake (Purley Platform) BIOS Reference Code and Sample Code +  Initialization code (some of it as exported git repos with full history) - Intel CEFDK (Consumer Electronics Firmware Development Kit (Bootloader stuff)) SOURCES - Silicon / FSP source code packages for various platforms - Various Intel Development and Debugging Tools - Simics Simulation for Rocket Lake S and potentially other platforms - Various roadmaps and other documents - Binaries for Camera drivers Intel made for SpaceX - Schematics, Docs, Tools + Firmware for the unreleased Tiger Lake platform - (very horrible) Kabylake FDK training videos - Intel Trace Hub + decoder files for various Intel ME versions - Elkhart Lake Silicon Reference and Platform Sample Code - Some Verilog stuff for various Xeon Platforms, unsure what it is exactly. - Debug BIOS/TXE builds for various Platforms - Bootguard SDK (encrypted zip) - Intel Snowridge / Snowfish Process Simulator ADK - Various schematics - Intel Marketing Material Templates (InDesign) - Lots of other things https://twitter.com/deletescape/status/1291405688204402689"},
{"Title": "Zippyshare is now officially dead. o7", "Author": "u/verpejas", "Content": "No content"},
{"Title": "How books are scanned.", "Author": "u/ReturnMuch9510", "Content": "No content"},
{"Title": "You might be right Alexa", "Author": "u/chicoquadcore", "Content": "No content"},
{"Title": "Yahoo and Verizon are blocking Archive.org from archiving Groups. Archive.org is facing up to 80% data loss.", "Author": "u/AnthropicMachine", "Content": "No content"},
{"Title": "Boss OGs", "Author": "u/kin_zindestroyer", "Content": "No content"},
{"Title": "Company closed down. All PCs were on sale for $32. I chose the most expensive one", "Author": "u/yusoffb01", "Content": "No content"},
{"Title": "Zippyshare is now officially dead. o7", "Author": "u/verpejas", "Content": "No content"},
{"Title": "I just stopped the hoarding", "Author": "u/Houderebaese", "Content": "So I just deleted 5TB worth of movies I never watch and then sold my 2x12 Tb drives. To think I had a NAS with >32TB at some point... I decided/realised that the senseless hording itself made my unhappy and had me constantly occupied with backing things up, noisy hardware and fixing server infrastructure. No more, my important data now fits on 2x5 TB 2.5 inch drives + offsite backup. No idea what the point of this post is but I kind of needed to let it out ðŸ˜„ðŸ‘"},
{"Title": "You might be right Alexa", "Author": "u/chicoquadcore", "Content": "No content"},
{"Title": "My pain for the last few weeks.", "Author": "u/AshleyUncia", "Content": "No content"},
{"Title": "Yahoo and Verizon are blocking Archive.org from archiving Groups. Archive.org is facing up to 80% data loss.", "Author": "u/AnthropicMachine", "Content": "No content"},
{"Title": "My Home Setup with 350tb", "Author": "u/Lintux", "Content": "No content"},
{"Title": "Company closed down. All PCs were on sale for $32. I chose the most expensive one", "Author": "u/yusoffb01", "Content": "No content"},
{"Title": "Archive Link In Comments. 40 years of Televison", "Author": "Unknown author", "Content": "No content"},
{"Title": "I just stopped the hoarding", "Author": "u/Houderebaese", "Content": "So I just deleted 5TB worth of movies I never watch and then sold my 2x12 Tb drives. To think I had a NAS with >32TB at some point... I decided/realised that the senseless hording itself made my unhappy and had me constantly occupied with backing things up, noisy hardware and fixing server infrastructure. No more, my important data now fits on 2x5 TB 2.5 inch drives + offsite backup. No idea what the point of this post is but I kind of needed to let it out ðŸ˜„ðŸ‘"},
{"Title": "Got this letter from TDS Fiber gigabit plan ..", "Author": "u/TheMonDon", "Content": "No content"},
{"Title": "My pain for the last few weeks.", "Author": "u/AshleyUncia", "Content": "No content"},
{"Title": ".Webp is the bane of my existence", "Author": "u/ElonTastical", "Content": "No content"},
{"Title": "My Home Setup with 350tb", "Author": "u/Lintux", "Content": "No content"},
{"Title": "My ZFS 645TB server. Dual intel gold 5222, 512 GB ram, 60 bay, 6 nvme (zil powered by Intel Optane + P4610 Intel as L2ARC + 4x 10GbE X710 SFP+. Absolute beast, which is backup server, though faster than production ðŸ˜‚", "Author": "u/kumits-u", "Content": "No content"},
{"Title": "Archive Link In Comments. 40 years of Televison", "Author": "Unknown author", "Content": "No content"},
{"Title": "My little blu-ray digitizing setup", "Author": "u/xjtian", "Content": "No content"},
{"Title": "Got this letter from TDS Fiber gigabit plan ..", "Author": "u/TheMonDon", "Content": "No content"},
{"Title": "The EUâ€™s new \"Data Act\" will let the user of a tech products (like wearables) access all the data it generates. Imagine all the personal data we will have access to!", "Author": "u/anonboxis", "Content": "No content"},
{"Title": ".Webp is the bane of my existence", "Author": "u/ElonTastical", "Content": "No content"},
{"Title": "Maybe I can fit one more..... :)", "Author": "u/Mara25x", "Content": "No content"},
{"Title": "My ZFS 645TB server. Dual intel gold 5222, 512 GB ram, 60 bay, 6 nvme (zil powered by Intel Optane + P4610 Intel as L2ARC + 4x 10GbE X710 SFP+. Absolute beast, which is backup server, though faster than production ðŸ˜‚", "Author": "u/kumits-u", "Content": "No content"},
{"Title": "When it's only 1080p on disc, but 4K on streaming.", "Author": "u/AshleyUncia", "Content": "No content"},
{"Title": "Google Drive now flagging my illicit .DS_Store files", "Author": "u/haveasuperday", "Content": "No content"},
{"Title": "My little blu-ray digitizing setup", "Author": "u/xjtian", "Content": "No content"},
{"Title": "[WIP/concept] Browser extension that restores privated/deleted videos in a YouTube playlist", "Author": "u/rebane2001", "Content": "No content"},
{"Title": "The EUâ€™s new \"Data Act\" will let the user of a tech products (like wearables) access all the data it generates. Imagine all the personal data we will have access to!", "Author": "u/anonboxis", "Content": "No content"},
{"Title": "Can anyone challenge this Verizon representative?", "Author": "u/Squiggledog", "Content": "No content"},
{"Title": "Maybe I can fit one more..... :)", "Author": "u/Mara25x", "Content": "No content"},
{"Title": "When it's only 1080p on disc, but 4K on streaming.", "Author": "u/AshleyUncia", "Content": "No content"},
{"Title": "Google Drive now flagging my illicit .DS_Store files", "Author": "u/haveasuperday", "Content": "No content"},
{"Title": "[WIP/concept] Browser extension that restores privated/deleted videos in a YouTube playlist", "Author": "u/rebane2001", "Content": "No content"},
{"Title": "Can anyone challenge this Verizon representative?", "Author": "u/Squiggledog", "Content": "No content"},
{"Title": "Building up my personal media server, and I have additional questions about sourcing storage", "Author": "u/TheXypris", "Content": "So I asked r/Plex a few days ago and got some really great advice One of which was to send me here So along side what I was told there I had more questions How is shucking more affordable than just buying the regular drive? And how do I actually find those deals Are Seagate drives really that bad? What is the general consensus on buying drives second hand off eBay or through a liquidation auction? Bad idea or risky at best? I'm assuming Facebook marketplace/Craigslist should be avoided, but is there any merit in looking there? Like if I find a second hand external I could shuck? How important are RPMs sata type and all that other non capacity info? What's good and what's bad? Tradeoffs? How long will a healthy high capacity drive realistically last? People keep saying raid is mandatory and act like drives fail because you look at them funny. I can't afford a raid array right now and anything I'm putting on it would be easily required if time consuming. Are there price monitoring sites I can use to keep an eye out for deals or sales? And where is the best place to get sata and power cables for the drives?"},
{"Title": "Came across a Reddit Archive project (Pushshift alternative)", "Author": "u/marinluv", "Content": "Came across this post yesterday. That user and u/RaiderBDev are archiving Reddit data. The data is around 3-4Tb roughly from what I have seen. The GitHub Repo to archive and access the data: Here To download and search Subreddit and user data manually: Here Post on r/pushshift for the 2.5TB dump: Here"},
{"Title": "Need help with converting and compressing old VOB files", "Author": "u/martian_doggo", "Content": "So i have a bunch of old vob files, and i want to convert them to mkv or mp4 or something. I admit i don't have much experience in video manipulation but this is turning out to be exceptionally hard. I have around 5 movies which have 5 vob files each. The movies are around 4 hours long but around 12gb in size for barely 704x570p so i would love to concatinate and compress them. Now here's the problem, none of the video editor software is picking up the subtitles, not handbrake, not even ffmpeg but mpv somehow plays with the subtitles, though it takes mpv 10ish seconds to load the subtitles (weird ik). Now if i join 5 of the files to make 1 complete movie using ffmpeg, the subtitles stop loading. I have no idea what to do :( thanks"},
{"Title": "\"Vimm's Lair\" receives notice to remove tons of games.", "Author": "u/blackletum", "Content": "No content"},
{"Title": "10 year old Cold storage 4TB HGST drive 4 weak sectors 1200 bad sectors? User error ? Expert advice needed ðŸ« ", "Author": "u/Positive_Minds", "Content": "This 4TB drive and also my 10 year old 2TB HGST drive. have popped up bad and weak sectors , they havenâ€™t been switched on in 10 years .. So was this basically my fault for not powering them up every and using disk fresh etc or is it just a case of age related demagnetisation or mechanical failure ? I just donâ€™t get as most of my 10 year old western digital drives or seagate have been fine when tested after 10 years Iâ€™m seriously thinking of doing yearly refreshes of my drives from now on .. My other question is can the weak sectors be reallocated or removed ? I read this on super user , is this true ? In order to keep the data signal from fading, you need to re-write the data. This is often known as â€œhard disk maintenanceâ€, and should be done 3 or 4 times a year. While it does not prevent data from being corrupted or deleted, it can go a long way towards ensuring that the magnetic signal does not fade away completely. The way it works is to read every sector of the drive, and then re-write the data found there, provided the drive reported no errors. If this is done on a regular basis, the magnetic signal of every part of the drive will be refreshed long before the signal fades or becomes ambiguous. This technique also gives the drive controller the opportunity to decide whether to retire any sectors that are becoming too unreliable, before any important data is lost. Magnetic Field Breakdown Most sources state that permanent magnets lose their magnetic field strength at a rate of 1% per year. Assuming this is valid, after ~69 years, we can assume that half of the sectors in a hard drive would be corrupted (since they all lost half of their strength by this time). Obviously, this is quite a long time, but this risk is easily mitigated - simply re-write the data to the drive. How frequently you need to do this depends on the following two issues (I also go over this in my conclusion). To periodically refresh the data on the drive, simply transfer it to another location, and re-writing it back to the drive. That way, the magnetic domains in the physical disk surface will be renewed with their original strength (because you just re-wrote the files back to the disk). If you're concerned about filesystem corruption, you can also format the disk before transferring the data back. You can also help to avoid this issue by archiving your data with recovery data and error correction when you put the data onto the drive. Many archive formats support the inclusion of data recovery algorithms, so even if you have a few corrupted sectors, you can still re-build the lost data. Depending on the priority of the data you've stored, you may want to refresh the hard disk more often. If it is essential data, I would recommend no less then 2 years at maximum. If you can withstand some chance of minor data loss (e.g. a few corrupted sectors here and there), go with 5 years. It doesn't take long to copy the data off the drive, and copy it back. One thing not considered is the servo tracks and markings. These are written one time at the factory and never again (on modern disks). No amount of re-writes by the user or so-called low-level formatting freshens these. Once they fade, they fade! It's different with the first stepper motor disks of the 80's. They don't have servo tracks and a low-level format writes ALL of the bits - fresh."},
{"Title": "Backing up MacBook and multiple SSDs to single HDD", "Author": "u/cloudfortynine", "Content": "Hi everyone, I could use some guidance as I'm unsure what the simplest and low maintenance solution is for me. I'm looking to purchase an external HDD to backup the following: Macbook 1TB SSD with my travel photos to be edited 1TB SSD with my partners travel photos to be edited 2TB SSD with music libraries for music production (Kontakt, Omnisphere etc) At first I was thinking of buying a 8TB external HDD and just backing everything up via Time Machine by having it all plugged in at once. However people online had also mentioned CCC and I'm unsure how that works? I was also hoping to back this all up to a portable HDD but it seems that 8TB HDDs can only be plugged in is that true? I'm not interested in NAS etc. as this is too complicated and expensive for me. As a sidetone I'm also planning on backing up to BackBlaze as well. Thanks in advance for any pointers!"},
{"Title": "Is a dual bay HDD docking station right for me?", "Author": "u/Vanillinn", "Content": "Hello! I only have my toe dipped in data hoarding but I've been meaning to buy the orico 6228US3 dual bay non cloning docking station. Here is my use case: Use my extra 3.5\" 500GB HDD for my modded wii u (to be formatted for the wii u) it will be where my games are installed while running the console back up photos, videos, school files, and game ROMs I have on future storage expansions (1-4TB at a time) have two drives connected to my pc at the same time on a very tight budget as a student In the future, I would like to build my own NAS if I get into bigger data hoarding and archiving. It's something I want to do but do not have the budget for right now. I have a lot of files currently but 1 or 2 more extra drives would be enough. If I could, I'd like to own two copies of my files. I have read some posts here on the subreddit raising issues on cooling, partitions, and corrupted drives. On cooling, would it still be an issue for a dual bay? I don't exactly understand the formatting problem of the partitions so I'd like to ask about that too. On corrupted drives, I'll always wait for the disk to stop spinning before ejection and also not move it as much as possible. Is a dual bay docking station a good low cost entry into data hoarding or is there an alternative more appropriate for my use case?"},
{"Title": "A New Idea for Data Storage: Combining Piezoelectric Materials and 3D NAND", "Author": "u/Mysterious_Crazy9606", "Content": "Hey everyone, Iâ€™ve been thinking about a new way to revolutionize data storage by combining piezoelectric materials with 3D NAND technology. Hereâ€™s the gist of my idea: The Concept â€¢\tHigh-Speed Piezoelectric Module: Use piezoelectric crystals that can oscillate at frequencies in the gigahertz range as an ultra-fast data buffer. This could potentially give us read and write speeds way beyond what we have with current tech. â€¢\tMain 3D NAND Storage: Use 3D NAND for the main long-term storage. We all know itâ€™s reliable and has a high capacity. How It Would Work 1.\tWriting Data: Incoming data would first go to the piezoelectric module at super high speeds. 2.\tTransferring Data: The data would then be transferred to the main 3D NAND storage for long-term keeping. 3.\tReading Data: For reads, the system would first check the piezoelectric buffer for quick access. If the data isnâ€™t there, it would pull from the 3D NAND storage. Benefits â€¢\tSpeed: This setup could drastically reduce latency and boost read/write speeds. â€¢\tEnergy Efficiency: Piezoelectric materials might be more energy-efficient for rapid operations. â€¢\tNew Storage Architecture: Combining the speed of piezoelectrics with the capacity of 3D NAND could create a super-efficient storage solution. Challenges â€¢\tTech Integration: Making sure the piezoelectric and 3D NAND components work seamlessly together. â€¢\tCost: High-quality piezoelectric materials and the complexity of this setup might be pricey. â€¢\tDurability and Reliability: The materials need to handle high-frequency oscillations over long periods without wearing out. Potential Impact If we can make this work, it could be a game-changer for data centers, mobile devices, and industrial applications that need ultra-fast response times. What do you all think? Could this actually work?"},
{"Title": "help with a complicated wget string : how do -A and -R args stack, and is there any means of boolean filtering ?", "Author": "u/ImaginaryCheetah", "Content": "good afternoon, i'm not particularly familiar with wget so i'm asking the experts for assistance... my current string of wget -A \"*(USA)*zip\" -R -m -p -E -k -K -np -nd -w 2 https://target works fine to pull all files with \"(USA)\" included in the name, but i'd like to understand if i can get more complicated. does the -R arg work in conjunction with -A or would it override ? for example, wget -A \"*(USA)*zip\" -R \"*(Demo)*\" [etc] would this return all files with \"(USA)\" in the title unless it also had \"(Demo)\" in the file name ? is there any way of passing a boolean criteria through wget ? \"get files with (Europe) in the title if same file with (USA) in the title doesn't exist\" kind of thing ? i expect that might require grabbing a list of files with lftp and processing it instead of having wget do that kind of logic."},
{"Title": "Lsi hba 9207-8i", "Author": "u/Crystal_Jayhawk", "Content": "No content"},
{"Title": "Looking for an alternative to dban to wipe an old computer with SSD", "Author": "u/ilovecokeslurpees", "Content": "I have an old computer (turned it off 2.5 years ago but it was a mid-spec PC built in 2016 I bought second hand in 2017). I can barely boot it these days. I want to wipe the 1 TB SSD before throwing the whole PC into the trash or recycle or whatever. Most parts are unsalvageable. I used to use dban way back when but it appears to not have been updated in almost a decade. The PC is running Windows 10, but I would like a simple solution bootable from a USB to ensure complete data erasure. I tried ABAN but it couldn't detect anything. Any suggestions?"},
{"Title": "Historical data hoarders at the library of Alexandria lost untolds amount of work and knowledge after the library was burned. Sumerian texts survived 4000+ years due to being written on clay tablets. Is there any efforts to transcribe some of our knowledge into more permanent media?", "Author": "u/rrybwyb", "Content": "I mostly hoard books. Its amazing how many I can fit onto just a small 10 tb hard drive. But if that gets wet, dropped, or someone holds a magnet to it, I've lost millions of hours of research and knowledge from 10's of thousands of authors. Even looking at the history of the dead sea scrolls, Whatever idiots were in charge of transporting those did an awful job. They were taken from the dry desert to a place where they could get humid and rot. Are there any organizations out there that have transcribed some of our more important items into stone or clay? I've been looking more into Sumerian history and the reason we have many of these items still is because they were carved into clay - and clay can last a pretty long time. Its kind of short sighted of humans to think we're immune to a giant asteroid or nuclear winter. In that situation, What would humans 5,000 years in the future after a major catastrophe be able to look back on and decipher about the 2000s? Edit: also I can't believe I forgot about plastic. That supposedly forever product that never breaks down. Does anyone know if it is really as permanent as something like ceramic? I've seen it become quite brittle and disintegrate when left out in the sun for even 6-12 months"},
{"Title": "Issues with LSI 9300-16i wiping partition tables", "Author": "u/Electronic-Papaya", "Content": "I'm trying to free up some PCIe slots in my system so I'm switching from 2 x LSI 2008 to a single 16 port LSI 9300-16i.  I'm running Linux and using mdadm to run 3 arrays. Before I attached any of my drives with data and arrays configured, I installed the card with no drives attached and update the firmware to 16.00.12.00 , and made sure it was in IT mode.  As a test I then connected one of my arrays to the controller and booted the system.  After booting up, the drives were detected fine (4 x 2TB) but the array was gone.  It appears that the metadata was erased, mdadm didn't recognize any drive as being part of an array. I was able to recover the data and the array by following the steps here: https://raid.wiki.kernel.org/index.php/Recovering_a_damaged_RAID However, if I reboot again the same thing happens, the metadata is lost and mdadm does not recognize that the drives are part of an array.   I do not have this issue with the older LSI controllers. Any idea what's going on here?  When I created the array I used the entre drive, so I did not create a Linux RAID partition on each drive.  The array is configured using /dev/sda to /dev/sdd, and not /dev/sda1 to /dev/sdd1.  Not sure that has anything to do with it. Edit:  Seems to be an issue with the controller and GPT partition tables.  As a test, I created an array with a couple 120gb SSD's I had laying around.  I created the array with the whole drive, rebooted, and after a reboot the array was still present.  I realized my other drives are configured as GPT.  So I wiped the SSDs, switched them to GPT and again created the array.  This time after a reboot the array was gone, mdadm does not recognize the drives as being part of an array.  Not sure how to fix this."},
{"Title": "Buying drives from eBay?", "Author": "u/Seventeen-Oncelers", "Content": "I was considering buying a 10tb HDD on eBay and was wondering if this is considered a good place to buy? It's certified eBay refurbished, 30 day returns etc. Just curious on you guy's thoughts on buying hard drives on eBay."},
{"Title": "Cost Efficient Storage Revamp", "Author": "u/iLOLZU", "Content": "I'm want to get into archival with all the shenanigans with the Internet Archive and game preservation. I'm in need of a storage upgrade anyways. I have a Lian Li O11 with the 2x 3.5\" drive cage unpopulated and 2x 2.5\" HDDs (2.5TB total) installed, was thinking of getting a pair of 3.5\" HDDs & a pair of SATA SSDs. Considering 8TB Seagate Firecuda + 2TB Samsung 870s. I know higher capacity 3.5\" HDDs exist, but heard that buying a single high capacity drive is not worth while in terms of cost and data loss."},
{"Title": "Help needed with my western digital elements hdd", "Author": "u/Tiredcardinal", "Content": "I have a 4 year old WD elements 1.5tb hdd It was working completely fine until the starting of this year but all of a sudden it started giving buggy video outputs and I couldn't run games on it anymore So recently I decided to format it , only for it to be faulty a day later now I can copy any data to hdd normally but I cannot transfer anything for it Any suggestions or troubleshoots are welcome and appreciated"},
{"Title": "Software to recover data from a formatted and written disk?", "Author": "u/lsgz3", "Content": "The disk has been formatted twice and written with New stuff. Is there any chance of recovering old files?"},
{"Title": "3D printed 8-Bay DAS with Supermicro backplane, trays, PSUs and external SAS-8088 connectors", "Author": "u/kschaffner", "Content": "https://imgur.com/a/YcbODga https://makerworld.com/en/models/491457 I've been working hard on this project for the past couple months in my spare time trying to make a product that I couldn't really find on the market. I had some extra PSUs and fans and supermicro trays so I figured why not design around that. I've probably put no less than 40+ hours into design, print, redesign, print, try fitment etc. Not the most experience with Fusion360 or CAD in general. The front LEDs do light up during activity :). I've been thinking of expanding this into a mATX or Mini-ITX supported case as well. I know this is also pretty much on the heals of shaztech_info but I think we have enough differences between us and I was kinda shocked to see someone else coming out with a similar idea around the same time lol. Let me know what you fellas and fellettes think! From Makerworld: I was unable to find a product that met my specifications of 8 hot-swap bays, a SAS backplane and external SAS connectors for easy connectivity, so I decided to design and build my own 8-bay Direct Attached Storage (DAS). This DAS features a Supermicro SAS833TQ backplane with Gen 5.5 hot-swap 3.5\" trays. It is powered by either a 200W PWS-203-1H (as-shown) or a 350W PWS-351-1H (or similar units of the same dimensions: PWS-351-1H 100 x 40 x 220mm, PWS-203-1H 76 x 40.3 x 192mm) power supply. Additionally, it includes an external SAS SFF-8088 to SFF-8087 adapter to cut down on internal wires. I printed this on my X1C and have realized that making it available for smaller print beds should also be done instead of 1 large single print. The body is 255 W x 190 H x 245mm D, it is printed as a single print, no supports needed. The body at 10% infill, 2 walls with gyroid infill .20mm layer height with a 0.4mm nozzle. This print requires NO supports to print. The print of the body will use approximately 984.5g of filament if no painting is done of the logo or numbers, and 982.6g if painted of a single color. This print took roughly 20 hours to print. I realize this might also be a turn off being a large print but I wasn't wanting to have to glue or screw it together if I could help it. If you are wanting this, let me know and I can work it into a new version. There are 2 version of the back plate for the different sizes of PSUs mentioned. Here is the list of parts used for this build: Part Price Supermicro Gen 5.5 3.5\" trays (MCP-220-00075-0B) x8 ~$50 for 8 on eBay Supermicro SAS833TQ 8-bay SAS backplane ~$35 on eBay SFF-8087 to SATA breakout cable x2 ~$16 SFF-8088 to SFF-8087 adapter ~$30 Supermicro 1U PSU of dimensions 100 x 40 x 220mm or 76 x 40.3 x 192mm (As seen PWS-203-1H) ~32$ on eBay, also designed screw hole for PWS-351-1H ~$30 on eBay Molex Y-cable ~$6 120mm of your choice x2 (Noctua NF-P12 shown) ~$16 ea. ATX power jumper cable w/ switch ~$11 This required a tool to remove the pins from the connector to feed it through the hole ~$17, you don't have to get one like this, but I wanted the other pin extractors for future projects.) Grand Total of parts: $210, could save $32 with some random 120mm fans as long as they can pull through all the trays. For hardware needed: Part qty M3x4x5 Heatset inserts 7 M4x6x6 Heatset inserts 6 M3x6 socket head screw 3 M3x12 socket head screw 4 M4x8 socket head screw 6 Some optional parts that might be desired: 120mm wire fan grills ~$8 Some rubber feet for the bottom"},
{"Title": "Can my old (possibly infected?) digital cameraâ€™s SD card and/or photos transfer a virus to my iPhone via an adapter?", "Author": "u/knOn0", "Content": "Hello! (Iâ€™ve asked this already but wanted to widen the net of insight before I proceed) I have an old digital camera from 2009-2011. I recently bought a new charger for it, turned it on, and it works perfectly! When I first turned it on, I was able to see tons of old pictures that Iâ€™d forgotten about; now, I can see my old photos but any new photo that I try to view has the â€œFile Errorâ€ message. I did buy an SD card adapter to put my photos from the SD card to my iPhone, and now Iâ€™m seeing a â€œcontent unavailable: files could not be opened due to an unknown error.â€ (The iPhone is an 11, IOS 17). Does my SD card have a virus? Do the photos on them have a virus? Should I get a brand new adapter and SD card? TYIA"},
{"Title": "Which is more safe, a partitioned SSD (OS/Data) or two separate SSDs (one for OS, one for Data)", "Author": "u/cs_legend_93", "Content": "Hello all I'm trying to settle a debate with a other redditor. Assume both of these scenarios are basic setups without drive pools or raid. The redditor suggests and recommends that using a single SSD with a partition for the OS and Data drive is more safe than using dual drives. I believe using a partitioned SSD will both double your chances of drive failure due to writes and reads, and it will make it a pain in the ass to restore the backup. I suggested using two separate SSDs, and the redditor said that this indeed doubles the chances of drive failure due to two drives. I disagreed and said that it halves it due to the decreased reads and writes. I also suggested that dual drives will make it easier to restore a backup drive if one fails. Which scenario is better? In both scenerios there are backups, like a mirrored drive using Acronis disk imager or something like that. But it's still not a drive pool or RAID Here is the debate: https://www.reddit.com/r/buildapc/s/ArnZMYuQSD"},
{"Title": "Is there a software that batch reverse search images and download the best version of it?", "Author": "u/BulgyBoy123", "Content": "Hi guys, I'm looking for a software that is able to batch reverse search some images. I downloaded all of my pinterest boards, but some of the files are really tiny. I wouldn't mind being able to download bigger versions of said files without having to spend weeks doing that manually."},
{"Title": "Price Point", "Author": "u/Crafty_Future4829", "Content": "I apologize in advanced as I know there are a lot related posts to buying refurbished/used hard drives for non critical data.  On eBay,  it seems you can get 16tb exos drives for around 160.00 or 10 per tb.  They say refurbished with zero hours and the reseller (goharddrives) offers a 5 year warranty. Where do these hard drives from?  Do they really have zero hours as opposed to having smart data wiped?  Is this a good deal? I read another post being able to get used drives for around 5 dollars per tb. What is your sweet spot and reliable ebay sellers you would buy from? Thanks"},
{"Title": "Huge Collection", "Author": "u/CoreDreamStudiosLLC", "Content": "Good evening, hope all is well. I lost my dad on Saturday and was going through some things to get bills paid/cancelled, etc. He had over 430 media discs at his apartment which I've taken back to me. Most are DVD-R, about 20 are DVD, and the rest are DVD+RW and Divx. Is it worth ripping each one of the DVD-R's at minimum? Some are very rare and streaming sites probably won't even half this collection."},
{"Title": "Internet Download Manager stopped detecting YouTube MKV format videos to download", "Author": "u/TheHighImperial", "Content": "Just today I discovered that it detects them if the video is embedded on a forum but not showing for the same video directly at YouTubes page. Ive updated the browser, IDM extension and even software. I also tried it on a different computer... same problem."},
{"Title": "is this samsung 512 gb micro sd fake?", "Author": "u/UlisesDeveloper", "Content": "No content"},
{"Title": "60 hard drives, spinning down", "Author": "u/geerlingguy", "Content": "No content"},
{"Title": "Mistakes were made.", "Author": "u/Miss_Zia", "Content": "No content"},
{"Title": "55.5GB of Lego Instructions, time to find the rest!", "Author": "u/xiyatumerica", "Content": "No content"},
{"Title": "YouTube deleted \"SemperVideo\" a 13-year-old german educational channel focused on IT", "Author": "u/AndaPlays", "Content": "Yeah, see the title. Just a reminder that nothing is safe on YouTube and you should backup your favorite stuff. This channel had thousands of videos. Was great stuff. From how to set up basic stuff in windows to how to configures your Linux server correctly. His Twitter: https://twitter.com/SemperVideo/status/1352503560525795328 https://socialblade.com/youtube/user/sempervideo Edit: He received his first Strike for a VPN video and the second one for showing how to get the new Windows 10 Version 2004 start menu. For the third one, he didn't even receive an email or notification yet for what video he got taken down. http://sempervideo.de/youtube-hat-sempervideo-geloescht/ Edit 2: The channel is backup. Just for how long If their ridiculous policy and bots are still up. https://twitter.com/SemperVideo/status/1352708103125561344 https://www.youtube.com/user/SemperVideo Edit 3: So the channel is up again, but he still has the previous two strikes from the VPN video and the win 10 video. So It's just a matter of time again till another video will get a strike again. And If you don't know after 3 strikes on YouTube, your channels get deleted. That's what happened here. His perspective(german): https://www.youtube.com/watch?v=ZiYLPjCWoXU In summary, he already deleted over 1k videos from YouTube after he received the second strike a few weeks ago. But with over a few thousand videos you can't really pinpoint which videos will get you maybe a strike again. If a user reports a video from him, a \"YouTube Specialist\" will look at the video who doesn't know shit about IT and then says: \"Yeah, that could be dangerous\" so here you have a strike. And after 3 you're gone."},
{"Title": "Intel suffers massive data breach involving confidential company and CPU information revealing hardcoded backdoors.", "Author": "u/kurtstir", "Content": "Intel  suffered a massive data breach earlier this year and as of today the  first associated data has begun being released.  Some users are  reporting finding hardcoded backdoors in the intel code. Some of the contents of this first release: - Intel ME Bringup guides + (flash) tooling + samples for various platforms -  Kabylake (Purley Platform) BIOS Reference Code and Sample Code +  Initialization code (some of it as exported git repos with full history) - Intel CEFDK (Consumer Electronics Firmware Development Kit (Bootloader stuff)) SOURCES - Silicon / FSP source code packages for various platforms - Various Intel Development and Debugging Tools - Simics Simulation for Rocket Lake S and potentially other platforms - Various roadmaps and other documents - Binaries for Camera drivers Intel made for SpaceX - Schematics, Docs, Tools + Firmware for the unreleased Tiger Lake platform - (very horrible) Kabylake FDK training videos - Intel Trace Hub + decoder files for various Intel ME versions - Elkhart Lake Silicon Reference and Platform Sample Code - Some Verilog stuff for various Xeon Platforms, unsure what it is exactly. - Debug BIOS/TXE builds for various Platforms - Bootguard SDK (encrypted zip) - Intel Snowridge / Snowfish Process Simulator ADK - Various schematics - Intel Marketing Material Templates (InDesign) - Lots of other things https://twitter.com/deletescape/status/1291405688204402689"},
{"Title": "60 hard drives, spinning down", "Author": "u/geerlingguy", "Content": "No content"},
{"Title": "SATA only 50 to 500 mating cycles - what do you use for backup?", "Author": "u/No-Balance-8038", "Content": "I've got currently 5 backup 3.5\" HDD disks which are planned to be stored offsite. And backup is earliest weekly. But the real issue is that according to the SATA specification, after 50 times replugging the disk, both the Server slot for 3.5\" or the HDD itself can fail! What do I do? Put the disks in a USB enclosure and never remove it again? But what about keeping the HDD running in proper temperature? My current usb cases do not have fans, and USB is not as reliable. I know turning off UASP helps for that, but I am still kind of disappointed that I am not meant to regularly take backups from SATA disks. I know theres also eSATA, but  I am not sure which combination of that would be reliable! I could basically instead have a PCIe eSATA card and eSATA cases. I even found two suitable cases https://www.turtlecase.eu/5-hdd/40-35-hard-drive-hdd-5-capacity-long-slots-waterproof-hd-5-turtle-case.html or https://www.feldherr.com/feldherr-esd-schaumstoff-set-euro-box-mit-16-faecher-fuer-3-5-zoll-festplatten/a-61567 What do you guys do recommend?"},
{"Title": "How books are scanned.", "Author": "u/ReturnMuch9510", "Content": "No content"},
{"Title": "Mistakes were made.", "Author": "u/Miss_Zia", "Content": "No content"},
{"Title": "Hulkshare Hacks?", "Author": "u/mamba_regime19", "Content": "Is there an easy way to download larger songs/mp3's from Hulkshare? Found some old shows no longer available anywhere else that I wouldn't mind having but the files stop playing after like 10 mins so my downloaders stop as well. Is there anything like YoutubetoMp3 or SoundcloudtoMp3 that works? Saw some older posts from 11years ago....not sure if anybody has figured it out since then. Thanks!"},
{"Title": "Boss OGs", "Author": "u/kin_zindestroyer", "Content": "No content"},
{"Title": "55.5GB of Lego Instructions, time to find the rest!", "Author": "u/xiyatumerica", "Content": "No content"},
{"Title": "Ways to summarize rsync logs? Too verbose so I never read them", "Author": "u/Ninj_Pizz_ha", "Content": "There's too much output from my rsync such that I end up not reading what was added/deleted. I think part of the issue is that it's logging stupid stuff like if a file timestamp got changed. Really, I'd like a summary at the top that shows like \"x number of files from folder y were deleted/added.\" I'm using the -aih flags. Apologies for the somewhat vague question."},
{"Title": "Zippyshare is now officially dead. o7", "Author": "u/verpejas", "Content": "No content"},
{"Title": "YouTube deleted \"SemperVideo\" a 13-year-old german educational channel focused on IT", "Author": "u/AndaPlays", "Content": "Yeah, see the title. Just a reminder that nothing is safe on YouTube and you should backup your favorite stuff. This channel had thousands of videos. Was great stuff. From how to set up basic stuff in windows to how to configures your Linux server correctly. His Twitter: https://twitter.com/SemperVideo/status/1352503560525795328 https://socialblade.com/youtube/user/sempervideo Edit: He received his first Strike for a VPN video and the second one for showing how to get the new Windows 10 Version 2004 start menu. For the third one, he didn't even receive an email or notification yet for what video he got taken down. http://sempervideo.de/youtube-hat-sempervideo-geloescht/ Edit 2: The channel is backup. Just for how long If their ridiculous policy and bots are still up. https://twitter.com/SemperVideo/status/1352708103125561344 https://www.youtube.com/user/SemperVideo Edit 3: So the channel is up again, but he still has the previous two strikes from the VPN video and the win 10 video. So It's just a matter of time again till another video will get a strike again. And If you don't know after 3 strikes on YouTube, your channels get deleted. That's what happened here. His perspective(german): https://www.youtube.com/watch?v=ZiYLPjCWoXU In summary, he already deleted over 1k videos from YouTube after he received the second strike a few weeks ago. But with over a few thousand videos you can't really pinpoint which videos will get you maybe a strike again. If a user reports a video from him, a \"YouTube Specialist\" will look at the video who doesn't know shit about IT and then says: \"Yeah, that could be dangerous\" so here you have a strike. And after 3 you're gone."},
{"Title": "NAS vs just in PC", "Author": "u/EnigmatheEgg", "Content": "Hi all! I have started to trust the internet less and less and have decided to save as much as I can in my own drives at home. I managed to find a Fractal design R5 with all drive sleds still there and thought it would make a great NAS Chassi. But now I'm wondering, do I need a NAS or can I just move my PC components there and just have a PC with 30+TB of space? All of the other people on the network don't have much data to store and would rather I help them anyway to save things. What other reasons would there be to have my data storage in a seperate chassi?"},
{"Title": "You might be right Alexa", "Author": "u/chicoquadcore", "Content": "No content"},
{"Title": "Intel suffers massive data breach involving confidential company and CPU information revealing hardcoded backdoors.", "Author": "u/kurtstir", "Content": "Intel  suffered a massive data breach earlier this year and as of today the  first associated data has begun being released.  Some users are  reporting finding hardcoded backdoors in the intel code. Some of the contents of this first release: - Intel ME Bringup guides + (flash) tooling + samples for various platforms -  Kabylake (Purley Platform) BIOS Reference Code and Sample Code +  Initialization code (some of it as exported git repos with full history) - Intel CEFDK (Consumer Electronics Firmware Development Kit (Bootloader stuff)) SOURCES - Silicon / FSP source code packages for various platforms - Various Intel Development and Debugging Tools - Simics Simulation for Rocket Lake S and potentially other platforms - Various roadmaps and other documents - Binaries for Camera drivers Intel made for SpaceX - Schematics, Docs, Tools + Firmware for the unreleased Tiger Lake platform - (very horrible) Kabylake FDK training videos - Intel Trace Hub + decoder files for various Intel ME versions - Elkhart Lake Silicon Reference and Platform Sample Code - Some Verilog stuff for various Xeon Platforms, unsure what it is exactly. - Debug BIOS/TXE builds for various Platforms - Bootguard SDK (encrypted zip) - Intel Snowridge / Snowfish Process Simulator ADK - Various schematics - Intel Marketing Material Templates (InDesign) - Lots of other things https://twitter.com/deletescape/status/1291405688204402689"},
{"Title": "My home backup method is bad. I need help re-thinking it.", "Author": "u/Zach83", "Content": "I don't know a lot about networking, storage, and proper backing up. I'm a 3d artist and do some coding on the side so I don't get scared by technical manuals or learning new tech, but proper sys adminning is not my strength. I just did some reading on things like windows backup and mirroring and now I think I need to update/change/modernize how I backup my stuff. Apparently the things I am relying on are NOT reliable at all. So I dove in and now I can't see the forest for the trees anymore. I need help thinking this out. My current setup I have a work/gaming pc with 2 2TB drives and a synology 1 disk(2TB) NAS device. The windows PC contains important data, the NAS serves media to a media pc(that needs no backing up) and some tablets To back these up I have a windows based \"NAS\" with 2 mirrored volumes, F and G. 2 4TB volumes (so 4 disks, 4TB each, 2 used for each mirorred volume) On the Work PC there are three folders that I manually backup to G while working. Daily backups sort off. Every week I run a Bat file with a few robocopy commands on the windows \"NAS\". The BAT maps the work PC's drives and the synology NAS drive and then does a copy with options like /mir and /purge. Essentially an incremental backup of the NAS and work PC drives to the windows \"NAS\". A few times a year a I run a second bat script. It copies everything to external devices that I store in my shed. Best offsite backup available to me. my questions How to mirror? Apparently windows mirrored volumes can not be relied on? I use it to protect against disk failure and apparently there are situations where the still working disk will NOT be read by another windows pc. Kinda defeats the point. I want such protection on the windows \"NAS\", an automatic and fast mirror copy. I want to be able to put any disk of the mirror pair in any other windows pc and have it appear like a normal disk, so it should be NTFS. What should I do? There are so many options and I have no idea what is reliable. How to backup without Robocopy (in a file system that windows can read like ntfs, and without puting files into a proprietary binary blob)? I'm told I shouldn't use Robocopy for incremental backups because it's too easy to fuck up, and should power cut out while copying I could be fucked. Googling I went, searching for a way with the following requirements: I want to backup the synology nas and the 2 disks on the work pc weekly, to the windows \"NAS\". I only want to copy changed or new files and I want to remove files that no longer exist in the original source. The backup should not go into a binary blob, I want loose files. No need for file versions/history. Timestamps(date modifed/created) are important to me. And I found nothing. I probably don't even know the right terms to search for. How should I do this backup? Ideally this should not involve signing up for a service or managed via a web interface hosted by some company. I want to keep everything in home and away from the internet. Free would be nice too."},
{"Title": "Yahoo and Verizon are blocking Archive.org from archiving Groups. Archive.org is facing up to 80% data loss.", "Author": "u/AnthropicMachine", "Content": "No content"},
{"Title": "How books are scanned.", "Author": "u/ReturnMuch9510", "Content": "No content"},
{"Title": "Any good suggestions for hardware to transfer analogue camcorder footage to a computer?", "Author": "u/Zazabar11", "Content": "My camcorder has av out and I have cables going from the camcorder to a framemister, which converts the signal to digital and uses an HDMI cable to transfer the signal to an elgato hd 60s, which ends with a USB C cable going to my computer for capture. It's a bit, but it has worked for older VHS tapes pretty well. I'm also using the basic capture software which comes with the elgato software. While trying to get footage from my camcorder, I noticed the audio and video footage stops getting captured on my PC when the tracking gets real messy, even if only for a couple seconds, and it ruins lots of the tape. Any suggestions for a capturing device which can handle any sort of tracking that may be required by the tapes? To note, I'm using a Sony CCD-TRV57, which uses VHS-C tapes (they're a compact form of regular VHS tapes)."},
{"Title": "Company closed down. All PCs were on sale for $32. I chose the most expensive one", "Author": "u/yusoffb01", "Content": "No content"},
{"Title": "Boss OGs", "Author": "u/kin_zindestroyer", "Content": "No content"},
{"Title": "winhttrack problem with cloning a single landing page. Any solutions?", "Author": "u/SolidShowerr", "Content": "I want to clone a landing page, I do everything right and download the files, but when I try to go to the index, it just opens the \"Loading screen\" from that page but not the page, it stays stuck on \"Loading\". Do you have a solution? Thank you https://preview.redd.it/winhttrack-problem-with-cloning-a-single-landing-page-any-v0-9ywe6e7ja26d1.jpg"},
{"Title": "I just stopped the hoarding", "Author": "u/Houderebaese", "Content": "So I just deleted 5TB worth of movies I never watch and then sold my 2x12 Tb drives. To think I had a NAS with >32TB at some point... I decided/realised that the senseless hording itself made my unhappy and had me constantly occupied with backing things up, noisy hardware and fixing server infrastructure. No more, my important data now fits on 2x5 TB 2.5 inch drives + offsite backup. No idea what the point of this post is but I kind of needed to let it out ðŸ˜„ðŸ‘"},
{"Title": "Zippyshare is now officially dead. o7", "Author": "u/verpejas", "Content": "No content"},
{"Title": "When to consider pre-emptive replacement of drives?", "Author": "u/MagicPracticalFlame", "Content": "I've got a small NAS with 4 x 6tb Drives in. The drives have just ticked over the 40,000 (grim dark) power-on time. Given that all the drives where purchased and installed at the same time, I'm worried about one crapping out and causing a domino effect when I get the replacement in to rebuild the array. The drives are Seagate ST6000VN0033 drives, health status shows as good on all counts. Just wondering when you guys start to consider replacement or migration to a new NAS (which is what I'd probably do)."},
{"Title": "My pain for the last few weeks.", "Author": "u/AshleyUncia", "Content": "No content"},
{"Title": "You might be right Alexa", "Author": "u/chicoquadcore", "Content": "No content"},
{"Title": "Advice/guidance needed for a first time NAS builder", "Author": "u/de4thr4sher", "Content": "Hello everyone! I've been lurking around here for quite some time and all I'll say is that I've been inspired by your posts so much that I've decided to dive into the realm of building my first NAS and be a data hoarder myself. But I'll need your help with that as I've been very confused with a lot of things and obviously, I don't want to screw everything up. So, first things first. My intention with the setup is purely just hoarding data on a separate machine other than my main PC while i keep everything safe and secure as much as possible. I don't need it to be accessible from anywhere or anything like that, just locally from my main PC. Like an enormous external HDD to put it simply. I don't need crazy speeds either as I won't be using it for work or anything similar. It's main use will be storing and playing my audio/video files from there and ofc, store a lot of other files and having backups of my main system and other devices. I'll be starting with the hardware I already own and the ones I plan on buying (suggestions are welcome ofc) *Already owned hardware* Option 1 for mobo/CPU/RAM would be my old setup: A Z390-E paired with a 9700k (8C/8T) and 32GB (DDR4-3000) of RAM Option 2 would be a setup I got recently for free: An Asus ROG Maximus VII paired with a 4790k (4C/8T) and 16GB (DDR3-1600). A 700W Coolermaster PSU that I also got with the free setup, an old spare Kingston SSDNow V300 240GB and if I manage to finally build this NAS, I'll be getting rid of a 4TB WD drive and an 8TB Ironwolf drive. Will I have to use the 9700k setup in this case or even the 4790k will be enough? *Hardware I plan on buying* The goal for now is to get 4 drives so I can have 2 of them for redundancy and add another 4 next year and hopefully I won't have a storage issue for a long time. I've locked my eyes on Exos X16 14TB and on X18 16TB drives and a Define 7 XL R2 case for this whole setup as all these are in a fairly good price right now where I'm from. Seagate lists these drives as CMR so I guess shouldn't be concerned...? Later on I know I'll also have to get a Pci-e to 4x sata or something like that. That's about it as far as hardware goes. Let's dive into software now, shall we? The most confusing part for me and where I need your advice the most. What should I go for? I see a lot of people recommending unRAID for basic setups like this. I got very confused when I was trying to decide between that or TrueNAS and eventually I gave it up and decided to ask the experts here. What would you do? Is it worth spending extra for an unRAID licence? I know this post is stupid for 99.8% of you but it's obviously not my field of expertise and I'm trying to not spend anywhere else right now, except mainly for HDDs and a case. Suggestions AND roasts are welcome! :) Thanks in advance!"},
{"Title": "My Home Setup with 350tb", "Author": "u/Lintux", "Content": "No content"},
{"Title": "Yahoo and Verizon are blocking Archive.org from archiving Groups. Archive.org is facing up to 80% data loss.", "Author": "u/AnthropicMachine", "Content": "No content"},
{"Title": "How much would it cost me to have and maintain a 1 TB database?", "Author": "u/AtwoodEnterprise", "Content": "I have an SEO SaaS and I store a lot of data for keywords and backlinks. My database is about 50gb and Iâ€™m currently paying about $80/mo for my webhosting VPS and up to 75gb of database storage. Currently, I use a lot of APIâ€™s to pull SEO data down because itâ€™s such a hassle to store that much data, but over the next year I want to try and up my game a bit. So Iâ€™m gonna try and start off by increasing my keyword, and my backlink data to around 1 TB total if possible. The majority of the data is gonna be due to the backlink data of course. Would anyone have any estimates for how much something like this might cost? Or what factors I should consider/ask when obtaining pricing?"},
{"Title": "Archive Link In Comments. 40 years of Televison", "Author": "Unknown author", "Content": "No content"},
{"Title": "Company closed down. All PCs were on sale for $32. I chose the most expensive one", "Author": "u/yusoffb01", "Content": "No content"},
{"Title": "Simple web scraping Chrome extension side project", "Author": "u/Fair_Perspective_761", "Content": "I made a free Chrome extension that scrapes all plain text from the active webpage and download's it as a plain text .txt file. It works great on articles, blogs, online forums (Reddit), wiki's, and more. It has a filter option that filters out smaller text elements and keeps large elements. I made this extension as a tool to 'pre-process' large websites before analyzing them with ChatGPT. Cmd + A, Cmd + C, and Cmd + V wasn't cutting it for me, and ChatGPT 4o can process almost 100x more words when fed a plain text .txt file. Any feedback would be great. I don't track data and everything runs native in the users browser. My extension: Web.txt"},
{"Title": "Got this letter from TDS Fiber gigabit plan ..", "Author": "u/TheMonDon", "Content": "No content"},
{"Title": "I just stopped the hoarding", "Author": "u/Houderebaese", "Content": "So I just deleted 5TB worth of movies I never watch and then sold my 2x12 Tb drives. To think I had a NAS with >32TB at some point... I decided/realised that the senseless hording itself made my unhappy and had me constantly occupied with backing things up, noisy hardware and fixing server infrastructure. No more, my important data now fits on 2x5 TB 2.5 inch drives + offsite backup. No idea what the point of this post is but I kind of needed to let it out ðŸ˜„ðŸ‘"},
{"Title": "Where to find US Trademark Data", "Author": "u/Acrobatic_Stay_9221", "Content": "Hi, I'm looking for granular US trademark data that includes the name of the company that filed the trademark (I'm trying to view summary statistics on trademark filed by company in the US). I've tried: https://tmsearch.uspto.gov/search/search-resultsbut but can't find out how to get the aggregate data out of this. I've been told that this data should be publicly available, but am stumped on where to find it. Has anyone \"hoarded\" a data set that would have this data? Alternatively, does anyone know how to scrape data from this lookup above?"},
{"Title": ".Webp is the bane of my existence", "Author": "u/ElonTastical", "Content": "No content"},
{"Title": "My pain for the last few weeks.", "Author": "u/AshleyUncia", "Content": "No content"},
{"Title": "Is the video conversion offered by VIDBOX (was Honestech) a reliable product also, how would I go about digitizing old VHS/VCR videos?", "Author": "u/Duck_Dur", "Content": "Hello All, Is the video conservation service offered by VIDBOX (previously Honestech) a reliable product and how would you go about digitizing old VHS/VCR videos and if you were to recommend another brand, what would it be? EDIT: Fixed grammer"},
{"Title": "My ZFS 645TB server. Dual intel gold 5222, 512 GB ram, 60 bay, 6 nvme (zil powered by Intel Optane + P4610 Intel as L2ARC + 4x 10GbE X710 SFP+. Absolute beast, which is backup server, though faster than production ðŸ˜‚", "Author": "u/kumits-u", "Content": "No content"},
{"Title": "My Home Setup with 350tb", "Author": "u/Lintux", "Content": "No content"},
{"Title": "Any optical media ratings / testers out there checking media surfacing recently? Ridata Valor bd-r 10x brand?", "Author": "u/Gbxx69", "Content": "I lost track of the definitive site(s) for reviewing optical media or at least blu-ray discs / brands / sub brands.. can anyone send a link to who's doing the due diligence on media these days? I've seen Ridata Valor 10x bd-r discs which seem to be sold cheaper than PlexDisc or even the 6x RiData bd-r's... so what gives.. why is it somewhat cheaper?!? Were these sitting in a warehouse somewhere since before 2020 or something??!? I am looking to use the media as backup of videos, software and occasionally important data (of which I would make multiple backups over time)."},
{"Title": "My little blu-ray digitizing setup", "Author": "u/xjtian", "Content": "No content"},
{"Title": "Archive Link In Comments. 40 years of Televison", "Author": "Unknown author", "Content": "No content"},
{"Title": "Samsung 970 evo plus M2 ssd boot drive not recognizing any more. Now what?", "Author": "u/tobyisthecoolest", "Content": "Iâ€™ve been using this drive for about 1.5years and today got the boot drive not detected error. I opened bios, but the only shows up sometimes. I have a m2 to usb adapter, but the drive isnâ€™t showing up on my other computers, so I canâ€™t seem to copy the data off it easily. Iâ€™ve found posts with other ppl having problems with the EVO plus drives. What should I do now? If I buy a replacement boot drive how do I get windows onto it? My back ups arenâ€™t perfect, so is there data recovery possible? Thanks"},
{"Title": "The EUâ€™s new \"Data Act\" will let the user of a tech products (like wearables) access all the data it generates. Imagine all the personal data we will have access to!", "Author": "u/anonboxis", "Content": "No content"},
{"Title": "Got this letter from TDS Fiber gigabit plan ..", "Author": "u/TheMonDon", "Content": "No content"},
{"Title": "Chown Errors Running Rsnyc Going from Unraid to Unassigned Device", "Author": "u/klnadler", "Content": "Hi everyone, I'm syncing some files from my unraid to an external drive and I'm having issues with permissions, I did run newperms on the unraid side but I think this is a destination problem. The files seem to be transferring but not with the correct permissions Command: rsync -avPh 05/2019-05-01/DSC00001(1)-2.ARW 32.77K   0%  248.06kB/s    0:01:40  rsync: [receiver] chown \"destination/2019/05/2019-05-01/.DSC00001(1)-1.ARW.sDzkxQ\" failed: Operation not permitted (1) 24.85M 100%   92.20MB/s    0:00:00 (xfr#2, ir-chk=1498/1516) 05/2019-05-01/DSC00001(1)-positive-1.tif 32.77K   0%  118.96kB/s    0:07:37  rsync: [receiver] chown \"destination/05/2019-05-01/.DSC00001(1)-2.ARW.Q8mP3g\" failed: Operation not permitted (1) 54.49M 100%   99.36MB/s    0:00:00 (xfr#3, ir-chk=1497/1516)"},
{"Title": "Maybe I can fit one more..... :)", "Author": "u/Mara25x", "Content": "No content"},
{"Title": ".Webp is the bane of my existence", "Author": "u/ElonTastical", "Content": "No content"},
{"Title": "M2 Pro/M3 Pro w/ 990 Pro NVMe + 40 Gb/s external", "Author": "u/Direct-Button1358", "Content": "Hi all, Systems: Mac Mini M2 Pro, MBP M3 Pro Thank you for your time. I am trying to put together an external drive using the Samsung 990 PRO NVMe with an external enclosure rated at 40 Gb/s like the ugreen or acasis. I do have a few questions: What external enclosure would work best for use with my M2 Pro and M3 Pro, I mostly do research and will be using it as an extended mobile storage solution . Is this overkill for my usage? Keep in mind, the difference in price between this SSD and others that write at half the speed is maybe $50 .  I am using my University start up funds for the purchase. Additionally: I am interested in eventually setting up NAS using this SSD paired with something like a Yottamaster enclosure. Thanks!"},
{"Title": "When it's only 1080p on disc, but 4K on streaming.", "Author": "u/AshleyUncia", "Content": "No content"},
{"Title": "My ZFS 645TB server. Dual intel gold 5222, 512 GB ram, 60 bay, 6 nvme (zil powered by Intel Optane + P4610 Intel as L2ARC + 4x 10GbE X710 SFP+. Absolute beast, which is backup server, though faster than production ðŸ˜‚", "Author": "u/kumits-u", "Content": "No content"},
{"Title": "VirtualDub won't record audio from capture card but sound works fine outside of recording", "Author": "u/Throwaway173638o", "Content": "I'm trying to capture video and audio from my JVC HR-S7100U through a GV-USB2 capture card device.  I am using S-Video as source with composite as my audio sources.  I set the audio source to the device. When I try to capture video, the audio doesn't get recorded.  But when I'm not recording under Overview mode, the volume is playing fine.  Preview mode doesn't work for audio too. I also don't have any issues recording video and audio from OBS.  However I want to stick with Virtualdub for the best quality. I don't know what I can really do to fix this?"},
{"Title": "Google Drive now flagging my illicit .DS_Store files", "Author": "u/haveasuperday", "Content": "No content"},
{"Title": "My little blu-ray digitizing setup", "Author": "u/xjtian", "Content": "No content"},
{"Title": "How to download Twitch vod?", "Author": "u/RainBowSwift71532", "Content": "I've been trying to download a twitch vod but have had no luck in doing so. If I want to download say a YouTube video. Search download YouTube video and a bunch of site pop up. Click on one copy and paste the YouTube link. Pick resolution and click download mad easy. But for some reason it's harder for a twitch vod. I have yet to find a site that will allow me to download a twitch vod. I'm trying to download a streamers recent twitch vod. I've tried a Twitch downloader for windows and it didn't work. It wouldn't download the video. So does anyone know a good site to download twitch vods? Please let me know!!"},
{"Title": "[WIP/concept] Browser extension that restores privated/deleted videos in a YouTube playlist", "Author": "u/rebane2001", "Content": "No content"},
{"Title": "The EUâ€™s new \"Data Act\" will let the user of a tech products (like wearables) access all the data it generates. Imagine all the personal data we will have access to!", "Author": "u/anonboxis", "Content": "No content"},
{"Title": "Do I need to reformat my drives?", "Author": "u/Sinnagangsta", "Content": "My Plex server storage is running off a WD MyCloud EX4. When I first started my server I only had 1 4TB drive, but I quickly outgrew that. The drive mode is set as JBOD â€œone driveâ€. I have another 4TB drive that I want to put in my NAS, but when I go in to change the RAID mode, I get warnings saying that all data will be lost. I know my only RAID options are 0 and 1. I believe I can also use JBOD as well with two drives? Iâ€™m not worried about redundancy as I can easily redownload content but would not like to have my drives erased just to add another drive. What would be the best way to move forward?"},
{"Title": "Can anyone challenge this Verizon representative?", "Author": "u/Squiggledog", "Content": "No content"},
{"Title": "Maybe I can fit one more..... :)", "Author": "u/Mara25x", "Content": "No content"},
{"Title": "Download from Google Sites Embedded Image Pages", "Author": "u/kylemj89", "Content": "I am trying to download batch urls and renaming each image in sequence order. The images are saves with unique urls from google sites and fails to download after multiple saves using python BeautifulSoup and request Wget runs into the same HTTrack fails to download the webpages as dispayed (haven't yet got as far as renaming sequence Bulk Image downloader doesn't have the ability to rename or save in folders from https://tigerlovefish.com/ https://sites.google.com/view/smashhitsremembered1995-1999/home/1997/472-1st-january-14th-january-1997 https://sites.google.com/view/smashhitsremembered1995-1999/home/1997/474-29th-january-11th-february-1997 https://sites.google.com/view/smashhitsremembered1995-1999/home/1996/467-23rdoctober-5th-november-1996 https://sites.google.com/view/smashhitsremembered1995-1999/home/1997/478-26th-march-8th-april-1997 https://sites.google.com/view/smashhitsremembered1995-1999/home/1997/480-23rd-april-6th-may-1997 https://sites.google.com/view/smashhitsremembered1995-1999/home/1997/482-21st-may-3rd-june-1997 https://sites.google.com/view/smashhitsremembered1995-1999/home/1997/484-18th-june-1st-july-1997 https://sites.google.com/view/smashhitsremembered1995-1999/home/1997/486-17th-july-29th-july-1997 https://sites.google.com/view/smashhitsremembered1995-1999/home/1997/490-10th-september-23rd-september-1997 https://sites.google.com/view/smashhitsremembered1995-1999/home/1997/492-8th-october-21st-october-1997 https://sites.google.com/view/smashhitsremembered1995-1999/home/1997/494-5th-november-18th-november-1997 https://sites.google.com/view/smashhitsremembered1995-1999/home/1997/496-3rd-december-17th-december-1997 https://sites.google.com/view/smashhitsremembered1995-1999/home/1997/tour-programme-1997 https://sites.google.com/view/smashhitsremembered1995-1999/home/1997/477-12th-march-25th-march-1997 https://sites.google.com/view/smashhitsremembered1995-1999/home/1997/479-9th-april-22nd-april-1997 https://sites.google.com/view/smashhitsremembered1995-1999/home/1997/481-7th-may-20th-may-1997 https://sites.google.com/view/smashhitsremembered1995-1999/home/1997/483-4th-june-17th-june-1997 https://sites.google.com/view/smashhitsremembered1995-1999/home/1997/485-2nd-july-16th-july-1997 https://sites.google.com/view/smashhitsremembered1995-1999/home/1997/487-30th-july-12th-august-1997 https://sites.google.com/view/smashhitsremembered1995-1999/home/1997/489-27th-august-9th-september-1997 https://sites.google.com/view/smashhitsremembered1995-1999/home/1997/491-24th-september-7th-october-1997 https://sites.google.com/view/smashhitsremembered1995-1999/home/1997/493-22nd-october-4th-november-1997 https://sites.google.com/view/smashhitsremembered1995-1999/home/1997/495-19th-november-2nd-december-1997 https://sites.google.com/view/smashhitsremembered1995-1999/home/1997/497-17th-december-31st-december-1997 https://sites.google.com/view/smashhitsremembered1995-1999/home/1998/498-1st-january-13th-january-1998 https://sites.google.com/view/smashhitsremembered1995-1999/home/1998/500-28th-january-10th-february-1998 https://sites.google.com/view/smashhitsremembered1995-1999/home/1998/502-25th-february-10th-march-1998 https://sites.google.com/view/smashhitsremembered1995-1999/home/1998/504-25th-march-7th-april-1998 https://sites.google.com/view/smashhitsremembered1995-1999/home/1998/506-22nd-april-5th-may-1998 https://sites.google.com/view/smashhitsremembered1995-1999/home/1998/508-20th-may-2nd-june-1998 https://sites.google.com/view/smashhitsremembered1995-1999/home/1998/510-17th-june-30th-june-1998 https://sites.google.com/view/smashhitsremembered1995-1999/home/1998/512-15th-july-28th-july-1998 https://sites.google.com/view/smashhitsremembered1995-1999/home/1998/514-12th-august-25th-august-1998 https://sites.google.com/view/smashhitsremembered1995-1999/home/1998/516-9th-september-22nd-september-1998 https://sites.google.com/view/smashhitsremembered1995-1999/home/1998/518-7th-october-20th-october-1998 https://sites.google.com/view/smashhitsremembered1995-1999/home/1998/520-4th-november-17th-november-1998 https://sites.google.com/view/smashhitsremembered1995-1999/home/1998/522-2nd-december-15th-december-1998 https://sites.google.com/view/smashhitsremembered1995-1999/home/1998/524-30th-december-1998-12th-january-1999 https://sites.google.com/view/smashhitsremembered1995-1999/home/1998/499-14th-january-27th-january-1998 https://sites.google.com/view/smashhitsremembered1995-1999/home/1998/501-11th-february-24th-february-1998 https://sites.google.com/view/smashhitsremembered1995-1999/home/1998/503-11th-march-24th-march-1998 https://sites.google.com/view/smashhitsremembered1995-1999/home/1998/505-8th-april-21st-april-1998 https://sites.google.com/view/smashhitsremembered1995-1999/home/1998/507-6th-may-19th-may-1998 https://sites.google.com/view/smashhitsremembered1995-1999/home/1998/509-3rd-june-16th-june-1998 https://sites.google.com/view/smashhitsremembered1995-1999/home/1998/511-1st-july-14th-july-1998 https://sites.google.com/view/smashhitsremembered1995-1999/home/1998/513-29th-july-11th-august-1998 https://sites.google.com/view/smashhitsremembered1995-1999/home/1998/515-26th-august-8th-september-1998 https://sites.google.com/view/smashhitsremembered1995-1999/home/1998/517-23rd-september-6th-october-1998 https://sites.google.com/view/smashhitsremembered1995-1999/home/1998/519-21st-october-3rd-november-1998 https://sites.google.com/view/smashhitsremembered1995-1999/home/1998/521-18th-november-1st-december-1998 https://sites.google.com/view/smashhitsremembered1995-1999/home/1998/523-16th-december-29th-december-1998 https://sites.google.com/view/smashhitsremembered1995-1999/home/1998/tour-programme-1998 https://sites.google.com/view/smashhitsremembered1995-1999/home/1999/525-13th-january-26th-january-1999 https://sites.google.com/view/smashhitsremembered1995-1999/home/1999/527-10th-february-23rd-february-1999 https://sites.google.com/view/smashhitsremembered1995-1999/home/1999/529-10th-march-23rd-march-1999 https://sites.google.com/view/smashhitsremembered1995-1999/home/1999/531-7th-april-20th-april-1999 https://sites.google.com/view/smashhitsremembered1995-1999/home/1999/533-5th-may-18th-may-1999 https://sites.google.com/view/smashhitsremembered1995-1999/home/1999/535-2nd-june-15th-june-1999 https://sites.google.com/view/smashhitsremembered1995-1999/home/1999/537-30th-june-13th-july-1999 https://sites.google.com/view/smashhitsremembered1995-1999/home/1999/539-28th-july-10th-august-1999 https://sites.google.com/view/smashhitsremembered1995-1999/home/1999/541-25th-august-7th-september-1999 https://sites.google.com/view/smashhitsremembered1995-1999/home/1999/543-22nd-september-5th-october-1999 https://sites.google.com/view/smashhitsremembered1995-1999/home/1999/545-20th-october-2nd-november-1999 https://sites.google.com/view/smashhitsremembered1995-1999/home/1999/547-17th-november-30th-november-1999 https://sites.google.com/view/smashhitsremembered1995-1999/home/1999/549-15th-december-28th-december-1999 https://sites.google.com/view/smashhitsremembered1995-1999/home/1999/poll-winners-party-and-tour-programme-1999 https://sites.google.com/view/smashhitsremembered1995-1999/home/1999/526-27th-january-9th-february-1999 https://sites.google.com/view/smashhitsremembered1995-1999/home/1999/528-24th-february-9th-march-1999 https://sites.google.com/view/smashhitsremembered1995-1999/home/1999/530-24th-march-6th-april-1999 https://sites.google.com/view/smashhitsremembered1995-1999/home/1999/532-21st-april-4th-may-1999 https://sites.google.com/view/smashhitsremembered1995-1999/home/1999/534-19th-may-1st-june-1999 https://sites.google.com/view/smashhitsremembered1995-1999/home/1999/536-16th-june-29th-june-1999 https://sites.google.com/view/smashhitsremembered1995-1999/home/1999/538-14th-july-27th-july-1999 https://sites.google.com/view/smashhitsremembered1995-1999/home/1999/540-11th-august-24th-august-1999 https://sites.google.com/view/smashhitsremembered1995-1999/home/1999/542-8th-september-21st-september-1999 https://sites.google.com/view/smashhitsremembered1995-1999/home/1999/544-6th-october-19th-october-1999 https://sites.google.com/view/smashhitsremembered1995-1999/home/1999/546-3rd-november-16th-november-1999 https://sites.google.com/view/smashhitsremembered1995-1999/home/1999/548-1st-december-14th-december-1999 https://sites.google.com/view/smashhitsremembered1995-1999/home/1999/550-29thdecember-1989-11th-january-2000 https://sites.google.com/view/smashhitsremembered1995-1999/home/1999/poll-winners-party-and-tour-programme-1999 https://sites.google.com/view/smashhitsremebered2000s/home/2000/551-12thjanuary-25thjanuary-2000 https://sites.google.com/view/smashhitsremebered2000s/home/2000/553-9th-february-22nd-february-2000 https://sites.google.com/view/smashhitsremebered2000s/home/2000/555-8th-march-21st-march-2000 https://sites.google.com/view/smashhitsremebered2000s/home/2000/557-5th-april-18th-april-2000 https://sites.google.com/view/smashhitsremebered2000s/home/2000/559-3rd-may-16th-may-2000 https://sites.google.com/view/smashhitsremebered2000s/home/2000/561-31stmay-13thjune-2000 https://sites.google.com/view/smashhitsremebered2000s/home/2000/563-28thjune-11thjuly-2000 https://sites.google.com/view/smashhitsremebered2000s/home/2000/565-26th-july-8th-august-2000 https://sites.google.com/view/smashhitsremebered2000s/home/2000/567-23rd-august-5th-september-2000 https://sites.google.com/view/smashhitsremebered2000s/home/2000/569-20th-september-3rd-october-2000 https://sites.google.com/view/smashhitsremebered2000s/home/2000/571-18thoctober-31stoctober2000 https://sites.google.com/view/smashhitsremebered2000s/home/2000/573-15thnovember-28thnovember-2000 https://sites.google.com/view/smashhitsremebered2000s/home/2000/575-13thdecember-9th-january-2001 https://sites.google.com/view/smashhitsremebered2000s/home/2000/tour-programme-2000 https://sites.google.com/view/smashhitsremebered2000s/home/2000/552-26th-january-8th-february-2000 https://sites.google.com/view/smashhitsremebered2000s/home/2000/554-23rd-february-8th-march-2000 https://sites.google.com/view/smashhitsremebered2000s/home/2000/556-22nd-march-4th-april-2000 https://sites.google.com/view/smashhitsremebered2000s/home/2000/558-19th-april-2nd-may-2000 https://sites.google.com/view/smashhitsremebered2000s/home/2000/560-17thmay-30thmay2000 https://sites.google.com/view/smashhitsremebered2000s/home/2000/562-14thjune-27thjune-2000 https://sites.google.com/view/smashhitsremebered2000s/home/2000/564-12thjuly-25thjuly-2000 https://sites.google.com/view/smashhitsremebered2000s/home/2000/566-9th-august-22nd-august-2000 https://sites.google.com/view/smashhitsremebered2000s/home/2000/568-6th-september-19th-september-2000 https://sites.google.com/view/smashhitsremebered2000s/home/2000/570-4th-october-17thoctober-2000 https://sites.google.com/view/smashhitsremebered2000s/home/2000/572-1st-november-14th-november-2000 https://sites.google.com/view/smashhitsremebered2000s/home/2000/574-29th-november-12th-december-2000 https://sites.google.com/view/smashhitsremebered2000s/home/2000/576-free-with-issue-575 https://sites.google.com/view/smashhitsremebered2000s/home/2001/577-10thjanuary-23rdjanuary-2001 https://sites.google.com/view/smashhitsremebered2000s/home/2001/579-7th-february-20th-february-2001 https://sites.google.com/view/smashhitsremebered2000s/home/2001/581-7th-march-20th-march-2001 https://sites.google.com/view/smashhitsremebered2000s/home/2001/583-4th-april-17th-april-2001 https://sites.google.com/view/smashhitsremebered2000s/home/2001/585-2nd-may-15th-may-2001 https://sites.google.com/view/smashhitsremebered2000s/home/2001/587-30th-may-12th-june-2001 https://sites.google.com/view/smashhitsremebered2000s/home/2001/589-27thjune-10thjuly-2001 https://sites.google.com/view/smashhitsremebered2000s/home/2001/591-25th-july-7th-august-2001 https://sites.google.com/view/smashhitsremebered2000s/home/2001/593-22nd-august-4th-september-2001 https://sites.google.com/view/smashhitsremebered2000s/home/2001/595-19th-september-2nd-october-2001 https://sites.google.com/view/smashhitsremebered2000s/home/2001/597-17th-october-30th-october-2001 https://sites.google.com/view/smashhitsremebered2000s/home/2001/599-14th-november-27th-november-2001 https://sites.google.com/view/smashhitsremebered2000s/home/2001/601-19th-december-2001-8th-january-2002 https://sites.google.com/view/smashhitsremebered2000s/home/2001/smash-hits-brits-special-1st-march-2001 https://sites.google.com/view/smashhitsremebered2000s/home/2001/578-24thjanuary-6thfebruary-2001 https://sites.google.com/view/smashhitsremebered2000s/home/2001/580-21st-february-6th-march-2001 https://sites.google.com/view/smashhitsremebered2000s/home/2001/582-21st-march-3rd-april-2001 https://sites.google.com/view/smashhitsremebered2000s/home/2001/584-18th-april-1st-may-2001 https://sites.google.com/view/smashhitsremebered2000s/home/2001/586-16th-may-29th-may-2001 https://sites.google.com/view/smashhitsremebered2000s/home/2001/588-13thjune-26thjune-2001 https://sites.google.com/view/smashhitsremebered2000s/home/2001/590-11thjuly-24thjuly-2001 https://sites.google.com/view/smashhitsremebered2000s/home/2001/592-8th-august-21st-august-2001 https://sites.google.com/view/smashhitsremebered2000s/home/2001/594-5th-september-18th-september-2001 https://sites.google.com/view/smashhitsremebered2000s/home/2001/596-3rd-october-16th-october-2001 https://sites.google.com/view/smashhitsremebered2000s/home/2001/598-31st-october-13th-november-2001 https://sites.google.com/view/smashhitsremebered2000s/home/2001/600-28th-november-18th-december-2001 https://sites.google.com/view/smashhitsremebered2000s/home/2001/602-19th-december-2001-8th-january-2002-free-with-issue-601 https://sites.google.com/view/smashhitsremembered01/home/2002/603-9th-january-22nd-january-2002 https://sites.google.com/view/smashhitsremebered2000s/home/2002/605-6th-february-19th-february-2002 https://sites.google.com/view/smash-hits-remembered-01/home/2002/607-6th-march-19th-march-2002 https://sites.google.com/view/smashhitsremembered01/home/2002/609-3rd-april-16th-april-2002 https://sites.google.com/view/smashhitsremebered2000s/home/2002/611-1st-may-14th-may-2002 https://sites.google.com/view/smashhitsremembered01/home/2002/613-29th-may-11th-june-2002 https://sites.google.com/view/smashhitsremembered01/home/2002/615-26th-june-9th-july-2002 https://sites.google.com/view/smashhitsremebered2000s/home/2002/617-24th-july-6th-august-2002 https://sites.google.com/view/smashhitsremebered2000s/home/2002/619-23rd-august-3rd-september-2002 https://sites.google.com/view/smashhitsremembered01/home/2002/621-18th-september-1st-october-2002 https://sites.google.com/view/smashhitsremebered2000s/home/2002/623-16th-october-29th-october-2002 https://sites.google.com/view/smashhitsremebered2000s/home/2002/625-13th-november-26th-november-2002 https://sites.google.com/view/smashhitsremembered01/home/2002/627-18th-december-2002-7th-january-2003 https://sites.google.com/view/smash-hits-remembered-01/home/2002/604-23rd-january-5th-february-2002 https://sites.google.com/view/smashhitsremebered2000s/home/2002/606-20th-february-5th-march-2002 https://sites.google.com/view/smashhitsremembered01/home/2002/608-20th-march-2nd-april-2002 https://sites.google.com/view/smashhitsremebered2000s/home/2002/610-17th-april-30th-april-2002 https://sites.google.com/view/smashhitsremebered2000s/home/2002/612-15th-may-28th-may-2002 https://sites.google.com/view/smashhitsremebered2000s/home/2002/614-12th-june-25th-june-2002 https://sites.google.com/view/smashhitsremebered2000s/home/2002/616-10th-july-23rd-july-2002 https://sites.google.com/view/smashhitsremebered2000s/home/2002/618-7th-august-20th-august-2002 https://sites.google.com/view/smashhitsremebered2000s/home/2002/620-4th-september-17th-september-2002 https://sites.google.com/view/smashhitsremebered2000s/home/2002/622-2nd-october-15th-october-2002 https://sites.google.com/view/smashhitsremebered2000s/home/2002/624-30th-october-12th-november-2002 https://sites.google.com/view/smashhitsremebered2000s/home/2002/626-27th-november-17th-december-2002 https://sites.google.com/view/smashhitsremebered2000s/home/2002/sneak-preview-magazine https://sites.google.com/view/smashhitsremebered2000s/home/2003/636-30th-april-13th-may-2003 https://sites.google.com/view/smashhitsremebered2000s/home/2003/638-28th-may-10th-june-2003 https://sites.google.com/view/smashhitsremebered2000s/home/2003/642-23rd-july-5th-august-2003 https://sites.google.com/view/smashhitsremembered01/home/2003/summer-quiz-book https://sites.google.com/view/smashhitsremebered2000s/home/2003/635-16th-april-29th-april-2003 https://sites.google.com/view/smashhitsremebered2000s/home/2004/659-5th-march-18th-march-2004 https://sites.google.com/view/smashhitsremebered2000s/home/2004/659-5th-march-18th-march-2004"},
{"Title": "Free Modular NAS Enclosure - Stackable Drive Expansion, ITX Vertical Layout, 3D Print Files Included", "Author": "u/ethanross1a", "Content": "No content"},
{"Title": "When it's only 1080p on disc, but 4K on streaming.", "Author": "u/AshleyUncia", "Content": "No content"},
{"Title": "Media storage guide for a highschooler", "Author": "u/MintedMince", "Content": "Hi! So I am a highschooler from a third world country and recently I have been struggling to keep all of my data intact. I have been juggling around 1 or 2 spare hard-drives and google accounts to somehow store family photos and other important media with as many backups as I can make. The thing is, I can't really afford a lot to invest into some more high-end, and I really do not want to loose my (had a few close calls, though my backups saved me). So any advice on improving the situation or do most people hang around doing the same thing? Would love to hear from yall :)"},
{"Title": "Hello my name is Nikon_Justus and I am a DataHoarder", "Author": "u/Nikon_Justus", "Content": "No content"},
{"Title": "Google Drive now flagging my illicit .DS_Store files", "Author": "u/haveasuperday", "Content": "No content"},
{"Title": "Best way to contact Seagate for warranty?", "Author": "u/green314159", "Content": "*** Update - I got in contact with Seagate and they wanted me to send them the drive and pay for the shipping costs when sending it in to them. They will cover the shipping costs for the return trip. Does this seem like standard procedure? It's my first time needing to RMA during the warranty period. I live in the USA. Original post: A Seagate Barracuda 8TB drive with warranty expiration date sometime in 2025 has failed the other day. What is the best way to get in contact with technical support and get the warranty and replacement going? Sorry if this is the wrong place on Reddit to ask"},
{"Title": "Took an old IBM hard drive from the 80s and turned it into an art piece", "Author": "u/NO-LAN", "Content": "No content"},
{"Title": "[WIP/concept] Browser extension that restores privated/deleted videos in a YouTube playlist", "Author": "u/rebane2001", "Content": "No content"},
{"Title": "Rosewill hot swap bay source/alternative?", "Author": "u/thecaramelbandit", "Content": "I've got a rosewill 4U case with one of their 4x3.5 hot swap enclosures. I want to add a second but they seem to be out of stock everywhere. Anyone have a source for one, or know of an alternative that will also fit?"},
{"Title": "After hoarding over 50k YouTube videos, here is the youtube-dl command I settled on.", "Author": "u/Veloldo", "Content": "EDIT: If you are reading this, I've made a few small changes. You can find the actual scripts I use here: https://github.com/velodo/youtube-dl_script . While my serup works great for me, if you're looking for a more robust solution, please check out TheFrenchGhosty's scripts here: https://github.com/TheFrenchGhosty/TheFrenchGhostys-YouTube-DL-Archivist-Scripts , with the associated reddit thread here: https://redd.it/h7q4nz . After seeing all of the posts recently regarding youtube-dl, I figured I would chime in on the options I use. There are a few things I want to implement as some point, see the bottom of this post for those. Also, if anyone sees anything that can be done better, please let me know as I am always looking for ways to improve everything I do! Also, this post isn't intended to be a guide on how to use youtube-dl, this is more for the arguments I use and why I use them. If you need help getting youtube-dl running, setting up a batch script, etc. there are plenty of guides for that sort of thing elsewhere. The command (DONT COPY PASTE THIS ONE): youtube-dl --download-archive \"archive.log\" -i --add-metadata --all-subs --embed-subs --embed-thumbnail --match-filter \"playlist_title != 'Liked videos' & playlist_title != 'Favorites'\" -f \"(bestvideo[vcodec^=av01][height>=1080][fps>30]/bestvideo[vcodec=vp9.2] [height>=1080][fps>30]/bestvideo[vcodec=vp9][height>=1080][fps>30]/bestvideo[vcodec^=av01] [height>=1080]/bestvideo[vcodec=vp9.2][height>=1080]/bestvideo[vcodec=vp9] [height>=1080]/bestvideo[height>=1080]/bestvideo[vcodec^=av01][height>=720] [fps>30]/bestvideo[vcodec=vp9.2][height>=720][fps>30]/bestvideo[vcodec=vp9][height>=720] [fps>30]/bestvideo[vcodec^=av01][height>=720]/bestvideo[vcodec=vp9.2] [height>=720]/bestvideo[vcodec=vp9][height>=720]/bestvideo[height>=720]/bestvideo)+ (bestaudio[acodec=opus]/bestaudio)/best\" --merge-output-format mkv -o \"%cd%/%% (playlist_uploader)s/%%(playlist)s/%%(playlist_index)s - %%(title)s - %%(id)s.%%(ext)s\" \"[URL HERE TO CHANNELS PLAYLISTS]\" The command again (copy paste friendly): youtube-dl --download-archive \"archive.log\" -i --add-metadata --all-subs --embed-subs --embed-thumbnail --match-filter \"playlist_title != 'Liked videos' & playlist_title != 'Favorites'\" -f \"(bestvideo[vcodec^=av01][height>=1080][fps>30]/bestvideo[vcodec=vp9.2][height>=1080][fps>30]/bestvideo[vcodec=vp9][height>=1080][fps>30]/bestvideo[vcodec^=av01][height>=1080]/bestvideo[vcodec=vp9.2][height>=1080]/bestvideo[vcodec=vp9][height>=1080]/bestvideo[height>=1080]/bestvideo[vcodec^=av01][height>=720][fps>30]/bestvideo[vcodec=vp9.2][height>=720][fps>30]/bestvideo[vcodec=vp9][height>=720][fps>30]/bestvideo[vcodec^=av01][height>=720]/bestvideo[vcodec=vp9.2][height>=720]/bestvideo[vcodec=vp9][height>=720]/bestvideo[height>=720]/bestvideo)+(bestaudio[acodec=opus]/bestaudio)/best\" --merge-output-format mkv -o \"%cd%/%%(playlist_uploader)s/%%(playlist)s/%%(playlist_index)s - %%(title)s - %%(id)s.%%(ext)s\" \"[URL HERE TO CHANNELS PLAYLISTS]\" I know it looks long and scary, let me break it down a little bit: --download-archive \"archive.log\" This keeps track of all the videos you have downloaded so they can be skipped over the next time it's ran or the next time it finds that video. -i Ignore any errors that occur while downloading. Occasionally they will happen and this just ensures things keep moving along as intended. Don't worry, the next time it is ran any videos that didn't fully download will most likely be picked right back up where it left off! --add-metadata --all-subs --embed-subs --embed-thumbnail These just embed metadata into the video once it's done downloading. You never know when this will come in handy, and having it all right in the video's container is nice. Just a little note, at the time of writing this post, ffmpeg can't embed images into a mkv, but the image is still downloaded and stored in the same location and with the same name as the video. --match-filter \"playlist_title != 'Liked videos' & playlist_title != 'Favorites'\" This will filter videos out that you don't want to download. Here is just a basic example of filtering out playlists with the title \"Liked Videos\" and \"Favorites\". I find this especially useful for filtering out playlists that contain a bunch of videos from other playlists. For example, if I'm downloading videos from a gaming channel and they have a playlist for \"Gmod\" and one for \"Minecraft PC\", but they also have one called \"PC Games\" that contains the contents of both the Gmod and the Minecraft playlists, I sometimes will want to keep those separate, so I will filter out the \"PC Games\" playlist. If there are videos in that playlist you still want, you can always add another youtube-dl command to your script with that playlist specifically. Depending on the channel, this can get rather annoying to manage, but its a good way to keep things better organized. -f \"(bestvideo[vcodec^=av01][height>=1080][fps>30]/bestvideo[vcodec=vp9.2][height>=1080] [fps>30]/bestvideo[vcodec=vp9][height>=1080][fps>30]/bestvideo[vcodec^=av01] [height>=1080]/bestvideo[vcodec=vp9.2][height>=1080]/bestvideo[vcodec=vp9] [height>=1080]/bestvideo[height>=1080]/bestvideo[vcodec^=av01][height>=720] [fps>30]/bestvideo[vcodec=vp9.2][height>=720][fps>30]/bestvideo[vcodec=vp9][height>=720] [fps>30]/bestvideo[vcodec^=av01][height>=720]/bestvideo[vcodec=vp9.2][height>=720]/ bestvideo[vcodec=vp9][height>=720]/bestvideo[height>=720]/bestvideo)+ (bestaudio[acodec=opus]/bestaudio)/best\" Ok. This is where things get a little tricky. I first want to start off by saying that this isn't totally necessary as all the -f command does is allows you to set preferences on what video and audio streams you want to download. If you want a basic rundown on how this works, the youtube-dl readme explains it better than I ever could. For my case here, I want to download video streams in certain codecs, which have a hierarchy of [av1>vp9.2>vp9>whatever is available]. It will keep going down the list until one is found that meets my criteria. You can also see that I prefer videos in 1080 with more than 30 fps, then 1080 30 fps, and that repeats for 720. I also prefer to get audio in opus if it's available. Just a side note for anyone wondering what vp9.2 is, it is the vp9 codec with HDR. Why bother with all of that nonsense when youtube-dl will automatically pick the best streams for you? Well, the way youtube-dl picks the best stream is based solely on bitrate. This means that for video it will usually chose the avc1 codec, which is pretty old at this point, and while it still looks good, I've found that the other codecs offer a smaller file size and similar or better quality. You may find otherwise and want to do things differently, but for me, this is how I do it as it saves hard drive space and I find the quality good. Also, as you will notice, I don't have any resolutions higher than 1080 on there. The way I have it, it should catch those higher res streams, but as of now, I don't archive many youtubers' videos that upload in higher res so I haven't found the need, but some day I'm sure I will change it. I already know your asking yourself, \"If this will catch the higher resolution streams, why don't you just leave the 720 options in there and remove the 1080?\". Well, it's because I've noticed that youtube has started to transcode many videos to the newer av1 codec, but so far most videos that I've seen only go up to 720 for the av1 codec. This means that if that stream is available, but there isn't a 1080p av1 stream, then it will always download those videos in 720p even if a higher res stream is available. --merge-output-format mkv -o \"%cd%/%%(playlist_uploader)s/%%(playlist)s/%%(playlist_index)s - %%(title)s - %%(id)s.%%(ext)s\" This just tells youtube-dl where I want the file and that I want it in an mkv container. It's pretty self explanatory, but I basically want a folder structure of \"[CHANNEL NAME]/[PLAYLIST NAME]/[PLAYLIST INDEX] - [VIDEO TITLE] - [YOUTUBE VIDEO ID].[EXTENSION]\". Feel free to customize this however you see fit. Please note that I used double % in some of these due to my script being a batch file ran on a windows VM. Things I want to do: - Create a docker container that runs the script. While the Windows VM is working perfect for this, it's the only thing the VM does now (used to be used for much more, but that has all been offloaded). It should be pretty easy since I leave the executables and the script all in a network share as is, so all it would need is the dependencies (which I think is only python if I'm not mistaken) and to set up a cron job. - Simplify the script a bit by using the -a argument. This would allow me to set up a file with the links I want to download. This would allow me to group a bunch of commands that all have the same arguments into 1 command. - Write a script that will move videos that were downloaded before they were put into playlists into their respective playlists once the uploader adds them. Right now what I do is download all of the uploader's playlists, then download all of their videos (using the same archive file so it doesn't re-download any). This means if the uploader is slow to add the video to a playlist, it will just be downloaded to a \"No Playlist\" folder. The other way I could do this would be to find a way to deduplicate all of the videos in the \"No Playlist\" folder and just use separate archive files for the playlist and non playlist videos, which might download some videos twice, but then later deduplicated. Final Thoughts: Youtube-dl is a wonderful and powerful tool, and with all of the crap going down on YouTube, you can never be too sure what videos you love might be taken down. Just what I've managed to download has already helped me and some of my friends out. It definitely is worth your time to automate downloading videos from channels you enjoy, and with a little know-how and experimentation, it doesn't take much time or effort to get something to a point where you can set it and forget it. Anyway, that was certainly longer than I thought it would be, but I really hope it helps some of you guys out. I've gained so much knowledge from this subreddit and it would mean a lot if I gave back and helped one of you out in return. Happy Hoarding! Just a quick edit: Be sure to check out the comments for some excellent ideas and more information on some things! As always, take this information and adapt it to your use case. Maybe my configuration will work perfectly for you, but more than likely you will have to tweak it a bit to get it just right for you. If you have any questions, please ask! Another quick edit: Some of the comments have brought up the fact that us as viewers of YouTube content, and even youtube-dl itself don't have any way to watch or download the original quality of the material as YouTube will automatically transcode videos when they are uploaded. This can be a problem for people who are trying to preserve things in the best quality they possibly can. If you are one of these people, you might want to try looking elsewhere for better quality releases of the content. The one example that immediately comes to mind for me is content from Rooster Teeth. The quality when downloaded directly from their website seems to be better quality than what you can pull from YouTube. For me personally, I will download some movies, and TV Shows and also most music and images in the best possible quality I can find, but when it comes to YouTube content, I just don't care as much and find the convenience of ripping directly from YouTube hard to beat. I also think the content tends to look great, especially for the file sizes, but this is obviously all up to you to decide."},
{"Title": "Can anyone challenge this Verizon representative?", "Author": "u/Squiggledog", "Content": "No content"},
{"Title": "Raid controller advices", "Author": "u/Surax98", "Content": "Hello guys, I am looking for a raid controller to be used over USB 3.2 10Gbps (I know, it's far from being advisable but that's the only thing I can do right now, since my server is a laptop) and I have to choose between the following: IBM M5015 (PCIe 2.0 x8, SAS 6Gbps, 512MB cache DDR2 800MHz) for 20â‚¬ and located in my city LSI 9271 (PCIe 3.0 x8, SAS 6Gbps, 1GB cache DDR3 idk-how-many MHz) for 33â‚¬ shipped from China. Now, since I am over USB 10Gbps, I am limited in both scenarios, but I'd like to know if that faster and more cache of the 9271 will actually make a difference for a domestic usage. To help you better understand my use cases, I will list them: Plex and Jellyfin media server IMMICH for photo synching Nextcloud for everything else These are the main I/O bound containers I am running on my server. What do you think? Any other suggestions for a raid controller, instead (which costs less than 40â‚¬)?"},
{"Title": "192TB beauty. What to do with it ?", "Author": "u/henk1313", "Content": "No content"},
{"Title": "Free Modular NAS Enclosure - Stackable Drive Expansion, ITX Vertical Layout, 3D Print Files Included", "Author": "u/ethanross1a", "Content": "No content"},
{"Title": "How much damage have I caused from leaving a HDD in the sun?", "Author": "u/JerichoBlows", "Content": "I ordered a refurbished 12TB Seagate Enterprise HDD and it was delivered today while I wasn't home. It was over +100Â°F (+38Â°C) degrees today and the sun was beaming directly on the mailbox for 4 hours before I got to it. I suspected the box could have easily been 130Â°F (55Â°C) or possibly much hotter having been in the direct sunlight for so long. I immediately put the entire box in the refrigerator. After about 5 minutes in the fridge, I decided to measure the temp using a digital BBQ thermometer. It measured at 104Â°F (40Â°C) but this was likely not accurate because I was using a thermometer that is meant to be shoved inside of meat and the box had already been in the fridge for 5 mins. How much damage could have been caused from it sitting in the direct +100Â°F (+38Â°C) degree sunlight for just over 4 hours? Thanks!"},
{"Title": "File size of Endgame's Cinema DCP", "Author": "Unknown author", "Content": "No content"},
{"Title": "Hello my name is Nikon_Justus and I am a DataHoarder", "Author": "u/Nikon_Justus", "Content": "No content"},
{"Title": "How to organize 10+ years of computer and phone backups? ~3TB", "Author": "u/PrivateAd990", "Content": "I'm looking for strategies, tools / software and tips to make my long journey of organizing these backups easier. Main questions at the bottom for a TLDR. About the content: ~3tb total, 15 laptop backups, 12 phone backups there will be overlap / duplicates in content between backups backups contain folders I manually dragged onto the portable drive. I never plan to do a full restore of a backup. backups may contain photos, videos, downloads, photo editing files, code and projects I wrote, a lot of junk I'd like to scrap Hardware: A recent MacBook Pro empty Samsung T9 4TB SSD, ~2000MB/s WD 2.5\" passport HDD 5TB, ~110MB/s A second HDD with a copy of the above for redundancy Plan: first bring everything on the HDD to the SSD since the SSD is way faster sort through everything. I need help with this part move the organized backup back to the HDD since SSD's aren't suitable for cold storage implement a plan for the 3 2 1 backup method Questions: software or tools to sort, organize, de-dupe, delete through everything on the drive. Free or paid tips for how to search through everything instead of going folder by folder? I'm guessing software can help here. output folder structure suggestions? Should I just flatten all backups to one? Let's say, all photos I took with my phone from all the backups to one folder? Or is that a bad idea"},
{"Title": "A friend calls and asks \"I can't find this video on any streaming service. Any chance you have it?\"", "Author": "u/TNightster", "Content": "No content"},
{"Title": "Took an old IBM hard drive from the 80s and turned it into an art piece", "Author": "u/NO-LAN", "Content": "No content"},
{"Title": "Live sync of data drive over the internet or simple backup. Lsyncd, DRDB or Kopia backup", "Author": "u/1000Zebras", "Content": "Hi, I'm curious what you guys would implement in this situation in order to, above all, simply maintain at the very least one solid, off-site backup of all of my data files and also, in the event of something happening to my main data drive on-site, reduce downtime as much as possible. Here is my current setup as is relevant to the question at hand: OrangePi 5 Plus running dietpi (so pretty much just debian) as my main server on-site One eMMC boot drive on the OrangePi containing the OS and all of my docker-compose files, as well as the OS itself Recently acquired 14TB external USB drive that houses purely my data for all of my docker containers (and then some outside of those, as well, but not much) OrangePi is running Tailscale A second RPi that lives at my brother's house also running Tailscale (so any connection between the two will more than likely be running over the interwebs, but through Tailscale) and with a second 14tb drive identical to the other connected to it, ready for data storage What I'm wondering is what may be the best strategy for maintaining a backup of the main data drive on the secondary drive, ideally in a mirrored fashion such that were the main drive to fail, I'd simply be able to plug in the secondary drive to the OrangePi, mount it at the same mountpoint as primary would have been, and I'd be back up and running nearly immediately (once the drive was physically moved between locations, of course). It's worth noting that, at present, I am dealing with nearly 4.5TB of data on main data drive (also currently backed up to the cloud via Kopia and iDrive E2) I've been considering: Trying out lsyncd or DRDB in order to literally have the drives mirror each other in as near realtime as the connection will allow. I have not used either of these tools yet, however, so I'm not familiar with exactly how they work behind the scene. And also, I realize that it is a lot of data to keep in sync over an internet connection, especially at file or block-level granularity as I believe those tools are designed for. In \"normal\" usage, I am not necessarily adding or changing all that much data on a day to day basis, but were I to make any major shifts in organization, or simply to add a lot more data into the mix suddenly, I'm wondering if the tools would be able to keep up Running an rsync job over ssh at a specified interval (say, maybe, a couple of times a day) in order to keep the two up date. I would of course again run into the same problem that would arise with the first option were I to make any drastic changes, but theoretically I'd eventually always have a 1 to 1 sync/backup between the two drives Simply running some sort of backup program from the main Orangepi data drive to the RPi's data drive, again at whatever specified interval (say, maybe, daily). I'd probably have to run some sort of webDAV server on the secondary RPi in order to facilitate backups between the two were I to use Kopia. Or, I suppose I could even run the data drive on RPi on a minio instance and have Kopia backup via the S3 protocol, but this seems perhaps like a little bit of overkill, and it wouldn't necessarily be the sort of 1 to 1 sync I'm shooting for as Kopia would organize the backup data in a fashion that it understands. This would be acceptable, though, as again at the end of the day the most important thing is to have all of the data itself stored safely in both locations, one way or another. How would you guys go about keeping things in sync between the two data drives? Or, should I just eschew that idea given the limitations of the bandwidth/connection between the two and go for straight backups using Kopia, or some othe rbackup system? Please, if you have any thoughts on how you'd architect this scenario, I'd very much appreciate any and perspectives/insights. Hopefully that all makes sense. If you need anything clarified, by all means speak up and I'll do my best to address. Thank you so very much for your time, expertise, and patience with my rambling question. I look forward to hearing how people weigh in. Kind Regards, LS"},
{"Title": "Remember to backup your data, you never know when a spinning disk is going to fail and then you end up with a lot of shiny drinks coasters", "Author": "u/bri999", "Content": "No content"},
{"Title": "After hoarding over 50k YouTube videos, here is the youtube-dl command I settled on.", "Author": "u/Veloldo", "Content": "EDIT: If you are reading this, I've made a few small changes. You can find the actual scripts I use here: https://github.com/velodo/youtube-dl_script . While my serup works great for me, if you're looking for a more robust solution, please check out TheFrenchGhosty's scripts here: https://github.com/TheFrenchGhosty/TheFrenchGhostys-YouTube-DL-Archivist-Scripts , with the associated reddit thread here: https://redd.it/h7q4nz . After seeing all of the posts recently regarding youtube-dl, I figured I would chime in on the options I use. There are a few things I want to implement as some point, see the bottom of this post for those. Also, if anyone sees anything that can be done better, please let me know as I am always looking for ways to improve everything I do! Also, this post isn't intended to be a guide on how to use youtube-dl, this is more for the arguments I use and why I use them. If you need help getting youtube-dl running, setting up a batch script, etc. there are plenty of guides for that sort of thing elsewhere. The command (DONT COPY PASTE THIS ONE): youtube-dl --download-archive \"archive.log\" -i --add-metadata --all-subs --embed-subs --embed-thumbnail --match-filter \"playlist_title != 'Liked videos' & playlist_title != 'Favorites'\" -f \"(bestvideo[vcodec^=av01][height>=1080][fps>30]/bestvideo[vcodec=vp9.2] [height>=1080][fps>30]/bestvideo[vcodec=vp9][height>=1080][fps>30]/bestvideo[vcodec^=av01] [height>=1080]/bestvideo[vcodec=vp9.2][height>=1080]/bestvideo[vcodec=vp9] [height>=1080]/bestvideo[height>=1080]/bestvideo[vcodec^=av01][height>=720] [fps>30]/bestvideo[vcodec=vp9.2][height>=720][fps>30]/bestvideo[vcodec=vp9][height>=720] [fps>30]/bestvideo[vcodec^=av01][height>=720]/bestvideo[vcodec=vp9.2] [height>=720]/bestvideo[vcodec=vp9][height>=720]/bestvideo[height>=720]/bestvideo)+ (bestaudio[acodec=opus]/bestaudio)/best\" --merge-output-format mkv -o \"%cd%/%% (playlist_uploader)s/%%(playlist)s/%%(playlist_index)s - %%(title)s - %%(id)s.%%(ext)s\" \"[URL HERE TO CHANNELS PLAYLISTS]\" The command again (copy paste friendly): youtube-dl --download-archive \"archive.log\" -i --add-metadata --all-subs --embed-subs --embed-thumbnail --match-filter \"playlist_title != 'Liked videos' & playlist_title != 'Favorites'\" -f \"(bestvideo[vcodec^=av01][height>=1080][fps>30]/bestvideo[vcodec=vp9.2][height>=1080][fps>30]/bestvideo[vcodec=vp9][height>=1080][fps>30]/bestvideo[vcodec^=av01][height>=1080]/bestvideo[vcodec=vp9.2][height>=1080]/bestvideo[vcodec=vp9][height>=1080]/bestvideo[height>=1080]/bestvideo[vcodec^=av01][height>=720][fps>30]/bestvideo[vcodec=vp9.2][height>=720][fps>30]/bestvideo[vcodec=vp9][height>=720][fps>30]/bestvideo[vcodec^=av01][height>=720]/bestvideo[vcodec=vp9.2][height>=720]/bestvideo[vcodec=vp9][height>=720]/bestvideo[height>=720]/bestvideo)+(bestaudio[acodec=opus]/bestaudio)/best\" --merge-output-format mkv -o \"%cd%/%%(playlist_uploader)s/%%(playlist)s/%%(playlist_index)s - %%(title)s - %%(id)s.%%(ext)s\" \"[URL HERE TO CHANNELS PLAYLISTS]\" I know it looks long and scary, let me break it down a little bit: --download-archive \"archive.log\" This keeps track of all the videos you have downloaded so they can be skipped over the next time it's ran or the next time it finds that video. -i Ignore any errors that occur while downloading. Occasionally they will happen and this just ensures things keep moving along as intended. Don't worry, the next time it is ran any videos that didn't fully download will most likely be picked right back up where it left off! --add-metadata --all-subs --embed-subs --embed-thumbnail These just embed metadata into the video once it's done downloading. You never know when this will come in handy, and having it all right in the video's container is nice. Just a little note, at the time of writing this post, ffmpeg can't embed images into a mkv, but the image is still downloaded and stored in the same location and with the same name as the video. --match-filter \"playlist_title != 'Liked videos' & playlist_title != 'Favorites'\" This will filter videos out that you don't want to download. Here is just a basic example of filtering out playlists with the title \"Liked Videos\" and \"Favorites\". I find this especially useful for filtering out playlists that contain a bunch of videos from other playlists. For example, if I'm downloading videos from a gaming channel and they have a playlist for \"Gmod\" and one for \"Minecraft PC\", but they also have one called \"PC Games\" that contains the contents of both the Gmod and the Minecraft playlists, I sometimes will want to keep those separate, so I will filter out the \"PC Games\" playlist. If there are videos in that playlist you still want, you can always add another youtube-dl command to your script with that playlist specifically. Depending on the channel, this can get rather annoying to manage, but its a good way to keep things better organized. -f \"(bestvideo[vcodec^=av01][height>=1080][fps>30]/bestvideo[vcodec=vp9.2][height>=1080] [fps>30]/bestvideo[vcodec=vp9][height>=1080][fps>30]/bestvideo[vcodec^=av01] [height>=1080]/bestvideo[vcodec=vp9.2][height>=1080]/bestvideo[vcodec=vp9] [height>=1080]/bestvideo[height>=1080]/bestvideo[vcodec^=av01][height>=720] [fps>30]/bestvideo[vcodec=vp9.2][height>=720][fps>30]/bestvideo[vcodec=vp9][height>=720] [fps>30]/bestvideo[vcodec^=av01][height>=720]/bestvideo[vcodec=vp9.2][height>=720]/ bestvideo[vcodec=vp9][height>=720]/bestvideo[height>=720]/bestvideo)+ (bestaudio[acodec=opus]/bestaudio)/best\" Ok. This is where things get a little tricky. I first want to start off by saying that this isn't totally necessary as all the -f command does is allows you to set preferences on what video and audio streams you want to download. If you want a basic rundown on how this works, the youtube-dl readme explains it better than I ever could. For my case here, I want to download video streams in certain codecs, which have a hierarchy of [av1>vp9.2>vp9>whatever is available]. It will keep going down the list until one is found that meets my criteria. You can also see that I prefer videos in 1080 with more than 30 fps, then 1080 30 fps, and that repeats for 720. I also prefer to get audio in opus if it's available. Just a side note for anyone wondering what vp9.2 is, it is the vp9 codec with HDR. Why bother with all of that nonsense when youtube-dl will automatically pick the best streams for you? Well, the way youtube-dl picks the best stream is based solely on bitrate. This means that for video it will usually chose the avc1 codec, which is pretty old at this point, and while it still looks good, I've found that the other codecs offer a smaller file size and similar or better quality. You may find otherwise and want to do things differently, but for me, this is how I do it as it saves hard drive space and I find the quality good. Also, as you will notice, I don't have any resolutions higher than 1080 on there. The way I have it, it should catch those higher res streams, but as of now, I don't archive many youtubers' videos that upload in higher res so I haven't found the need, but some day I'm sure I will change it. I already know your asking yourself, \"If this will catch the higher resolution streams, why don't you just leave the 720 options in there and remove the 1080?\". Well, it's because I've noticed that youtube has started to transcode many videos to the newer av1 codec, but so far most videos that I've seen only go up to 720 for the av1 codec. This means that if that stream is available, but there isn't a 1080p av1 stream, then it will always download those videos in 720p even if a higher res stream is available. --merge-output-format mkv -o \"%cd%/%%(playlist_uploader)s/%%(playlist)s/%%(playlist_index)s - %%(title)s - %%(id)s.%%(ext)s\" This just tells youtube-dl where I want the file and that I want it in an mkv container. It's pretty self explanatory, but I basically want a folder structure of \"[CHANNEL NAME]/[PLAYLIST NAME]/[PLAYLIST INDEX] - [VIDEO TITLE] - [YOUTUBE VIDEO ID].[EXTENSION]\". Feel free to customize this however you see fit. Please note that I used double % in some of these due to my script being a batch file ran on a windows VM. Things I want to do: - Create a docker container that runs the script. While the Windows VM is working perfect for this, it's the only thing the VM does now (used to be used for much more, but that has all been offloaded). It should be pretty easy since I leave the executables and the script all in a network share as is, so all it would need is the dependencies (which I think is only python if I'm not mistaken) and to set up a cron job. - Simplify the script a bit by using the -a argument. This would allow me to set up a file with the links I want to download. This would allow me to group a bunch of commands that all have the same arguments into 1 command. - Write a script that will move videos that were downloaded before they were put into playlists into their respective playlists once the uploader adds them. Right now what I do is download all of the uploader's playlists, then download all of their videos (using the same archive file so it doesn't re-download any). This means if the uploader is slow to add the video to a playlist, it will just be downloaded to a \"No Playlist\" folder. The other way I could do this would be to find a way to deduplicate all of the videos in the \"No Playlist\" folder and just use separate archive files for the playlist and non playlist videos, which might download some videos twice, but then later deduplicated. Final Thoughts: Youtube-dl is a wonderful and powerful tool, and with all of the crap going down on YouTube, you can never be too sure what videos you love might be taken down. Just what I've managed to download has already helped me and some of my friends out. It definitely is worth your time to automate downloading videos from channels you enjoy, and with a little know-how and experimentation, it doesn't take much time or effort to get something to a point where you can set it and forget it. Anyway, that was certainly longer than I thought it would be, but I really hope it helps some of you guys out. I've gained so much knowledge from this subreddit and it would mean a lot if I gave back and helped one of you out in return. Happy Hoarding! Just a quick edit: Be sure to check out the comments for some excellent ideas and more information on some things! As always, take this information and adapt it to your use case. Maybe my configuration will work perfectly for you, but more than likely you will have to tweak it a bit to get it just right for you. If you have any questions, please ask! Another quick edit: Some of the comments have brought up the fact that us as viewers of YouTube content, and even youtube-dl itself don't have any way to watch or download the original quality of the material as YouTube will automatically transcode videos when they are uploaded. This can be a problem for people who are trying to preserve things in the best quality they possibly can. If you are one of these people, you might want to try looking elsewhere for better quality releases of the content. The one example that immediately comes to mind for me is content from Rooster Teeth. The quality when downloaded directly from their website seems to be better quality than what you can pull from YouTube. For me personally, I will download some movies, and TV Shows and also most music and images in the best possible quality I can find, but when it comes to YouTube content, I just don't care as much and find the convenience of ripping directly from YouTube hard to beat. I also think the content tends to look great, especially for the file sizes, but this is obviously all up to you to decide."},
{"Title": "Python script to help identify hot swapped drives", "Author": "u/radialmonster", "Content": "No content"},
{"Title": "This is what \"frustration-free\" packaging means in Europe", "Author": "u/FigureOfEight", "Content": "No content"},
{"Title": "192TB beauty. What to do with it ?", "Author": "u/henk1313", "Content": "No content"},
{"Title": "Question related to the voiceover feature (headphone symbol) in internet archive", "Author": "u/69PepperoniPickles69", "Content": "Do you know if there's a way to upload my own files so that listen to them instead of reading? Or does it have to include actually uploading books and them getting approved, scripted and so on? And if so do you guys know any alternative website where we can upload large texts for listening with decent quality?"},
{"Title": "80 TB homeserver for datahoarding", "Author": "u/kirjeveitsi", "Content": "No content"},
{"Title": "File size of Endgame's Cinema DCP", "Author": "Unknown author", "Content": "No content"},
{"Title": "How best to migrate to new hardware?", "Author": "u/SlayterDevAgain", "Content": "I'm about to build a new NAS. With my current NAS I just kind of threw drives at it as I aquired them (2x 2TB in RAID 1, a 4TB and a 6TB drive in pools by themselves, and 5 6TB drives in an external enclosure in RAID 5). In the new build I have 3 10TB drives I'll be adding. I don't necessarily need to keep all the drives from the old build (I at least want the 5x 6TB drives but not externally) but what would be the best way to migrate the data to the new build? Any advice is appreciated. Further info: Current build is FreeNAS and I'll probably keep with that or TrueNAS."},
{"Title": "Please do not mirror YouTube on the Internet Archive in Bulk", "Author": "u/textfiles", "Content": "https://twitter.com/textfiles/status/1492209816730808331 I posted this in a twitter thread, but I thought I'd mention this (obvious) thread here as well: Every once in a while, someone gets a brilliant idea, which is not a brilliant idea, and the first step for a mountain of heartache.  The idea is \"The Internet Archive is permanency-minded, and Youtube is full of things. I should back up Youtube on Internet Archive\". Depending on the person's capabilities and their drive, they may back up a couple videos here and there, or, as sometimes people are capable of doing, they set up a massive operation to just start jamming thousands of YouTube videos in \"just in case\".  Do not do this. YouTube is a massive ecosystem of videos, ranging from: Mirrors of neat stuff from video sources Archival copies of things on other media Businesses/Channels, ad-reliant, putting out shows And more. It's actually rather complicated and there's lots of considerations. When you decide, on your own, to \"help\" by downloading dozens of terabytes of videos, sometimes sans metadata, other times with random filenames, and just shove them into the Internet Archive, you're just hurting a non-profit by doing so. You are not a hero.  Please don't. Going to say it again: Please don't.  If you have a legitimate concern of a specific situation (creator has died, the material is some sort of culturally-relevant \"leak\" or unique situation, etc.) then communicate with the Archive (or me) about it, we'll work something out. Today's writing was brought to you by someone who could have used this information in their lives 2 months ago. UPDATE: I responded to one of the threads generated in a way that probably applies to 90% of the issues brought up."},
{"Title": "A friend calls and asks \"I can't find this video on any streaming service. Any chance you have it?\"", "Author": "u/TNightster", "Content": "No content"},
{"Title": "4x5.25\" to 7x3.5\" adapter?", "Author": "u/Adam1394", "Content": "Hello, I look for aforementioned adapter for my Define XL R2 case."},
{"Title": "Ok which one of you did this?", "Author": "u/drbennett75", "Content": "No content"},
{"Title": "Remember to backup your data, you never know when a spinning disk is going to fail and then you end up with a lot of shiny drinks coasters", "Author": "u/bri999", "Content": "No content"},
{"Title": "Backing up Google calendar", "Author": "u/c05d", "Content": "Hi, Iâ€™ve been using Google calendar for close to 20 years now. Itâ€™s started to bother me that my entire life is in Googleâ€™s hands and Id like to back this up & transfer to another service like Outlook whatâ€™s the best way to do this? I want everything including tasks etc to transfer thanks"},
{"Title": "Seagate's flex - 3 ZB shipped", "Author": "u/greasythug", "Content": "No content"},
{"Title": "This is what \"frustration-free\" packaging means in Europe", "Author": "u/FigureOfEight", "Content": "No content"},
{"Title": "What is the best free(non trial) software for converting a dvd to mp4 (ideally with subtitles)?", "Author": "u/Immediate-Risk-7569", "Content": "Every software I found is a trial with either a time limit or an ugly watermark."},
{"Title": "Just a reminder of what we have lost, and why we feel how we feel about data. I guarantee everybody that sees this can find an artist that is near to their heart. This is completely unconscionable and my heart aches looking at this list.", "Author": "u/PoorWill", "Content": "No content"},
{"Title": "80 TB homeserver for datahoarding", "Author": "u/kirjeveitsi", "Content": "No content"},
{"Title": "\"Best\" 3.5\" 8Tb HDD Brand?", "Author": "u/Large_Medium_8984", "Content": "This question pops up all the time on here but I only see specific use cases when others ask and nothing really close to what my situation is. I'm looking to bring together all of mine and my families ancient Hard drives, laptop backups, flash drives, externals, family photo scans and videos all onto a few backup HDDs. Just over a Million files @ under 6TB that I'd like to put into an 8Tb HDD (or 2). Photos, Videos, Text Docs, and whatever else might be in there. These drives will not be used for gaming at all, so no need to worry about being rough on the drive with pulling all the time, as I see a lot of people looking for gaming AND storage when asking this. Just something to have peace of mind that nothing could go wrong in a reasonable amount of time with them. I've read good and bad things about both Seagate Barracuda and Western Digital Blue. Are there other 3.5\"s I should also look in to? I'm not expecting these drives to live dormant 5-10 years, but it would be nice. I'd like to read any and all personal experiences users have had over the years."},
{"Title": "Mines still going strong ðŸ¤·", "Author": "u/apeironone", "Content": "No content"},
{"Title": "Please do not mirror YouTube on the Internet Archive in Bulk", "Author": "u/textfiles", "Content": "https://twitter.com/textfiles/status/1492209816730808331 I posted this in a twitter thread, but I thought I'd mention this (obvious) thread here as well: Every once in a while, someone gets a brilliant idea, which is not a brilliant idea, and the first step for a mountain of heartache.  The idea is \"The Internet Archive is permanency-minded, and Youtube is full of things. I should back up Youtube on Internet Archive\". Depending on the person's capabilities and their drive, they may back up a couple videos here and there, or, as sometimes people are capable of doing, they set up a massive operation to just start jamming thousands of YouTube videos in \"just in case\".  Do not do this. YouTube is a massive ecosystem of videos, ranging from: Mirrors of neat stuff from video sources Archival copies of things on other media Businesses/Channels, ad-reliant, putting out shows And more. It's actually rather complicated and there's lots of considerations. When you decide, on your own, to \"help\" by downloading dozens of terabytes of videos, sometimes sans metadata, other times with random filenames, and just shove them into the Internet Archive, you're just hurting a non-profit by doing so. You are not a hero.  Please don't. Going to say it again: Please don't.  If you have a legitimate concern of a specific situation (creator has died, the material is some sort of culturally-relevant \"leak\" or unique situation, etc.) then communicate with the Archive (or me) about it, we'll work something out. Today's writing was brought to you by someone who could have used this information in their lives 2 months ago. UPDATE: I responded to one of the threads generated in a way that probably applies to 90% of the issues brought up."},
{"Title": "What Podcasts to Hoard?", "Author": "u/4bstractals", "Content": "So, I just discovered PodcastBulkDownloader thanks to a recent thread , and it's got we wondering... If I am going to start assembling a podcast hoard, what are the criteria that I might use to decide what gets included? Obviously, podcasts I like would be the primary metric -- but I can download all of those in a couple of hours, and I have a lot more space. So... what about podcasts at risk of going behind a paywall? Podcasts of significant cultural importance? How does one best serve as a casual archivist for such a massive amount of data?"},
{"Title": "Youtube educational hacking content getting banned", "Author": "u/ignaloidas", "Content": "No content"},
{"Title": "Ok which one of you did this?", "Author": "u/drbennett75", "Content": "No content"},
{"Title": "[YoYotta] How can I change destination folder in LTO Tape", "Author": "u/ddd102", "Content": "https://preview.redd.it/yoyotta-how-can-i-change-destination-folder-in-lto-tape-v0-6t4uhiti1w5d1.png Hi, there. I'm very newbie on YoYotta. Today, I do my first copy job. 3.5 inch HDD to LTO 8 Tape by YoYotta. But, I wonder how can I change destination folder trees. This software create just same folder trees from the source folders. I don't want that way. I want to create different folder trees on LTO Tapes which I'll do back up my data. Anyone knows how can do that? And is there any LTO or YoYotta user community? even though subreddit. I need more information. Official website of YoYotta, already I checked, but I need to story from real users. Thanks!"},
{"Title": "I was told I belong here", "Author": "u/dshbak", "Content": "No content"},
{"Title": "Seagate's flex - 3 ZB shipped", "Author": "u/greasythug", "Content": "No content"},
{"Title": "Vimms Lair, the largest collections of ROMs, is being taken down.", "Author": "u/snowysysadmin59", "Content": "Corporate greed at it again. Anyone got a backup? ðŸ¥º"},
{"Title": "Twitter will soon begin suspending accounts that have been inactive for 30 days", "Author": "Unknown author", "Content": "No content"},
{"Title": "Just a reminder of what we have lost, and why we feel how we feel about data. I guarantee everybody that sees this can find an artist that is near to their heart. This is completely unconscionable and my heart aches looking at this list.", "Author": "u/PoorWill", "Content": "No content"},
{"Title": "Best Way To Dump/Mirror 16'000 mp3 (Podcast) Files?", "Author": "u/Redditarianist", "Content": "As the title states. I'm looking to upload around 16 thousand podcasted files to the Internet Archive & am looking for the best way. Is there an RSS ingest system at all?"},
{"Title": "Twitter removed a studentâ€™s tweets critical of exam monitoring tool due to DMCA notice; EFF claims it is textbook example of fair use", "Author": "u/retrac1324", "Content": "No content"},
{"Title": "Mines still going strong ðŸ¤·", "Author": "u/apeironone", "Content": "No content"},
{"Title": "Any good Kemono/coomer bulk downloaders?", "Author": "u/TravDeMan", "Content": "I've been looking for a while and im struggling to find any"},
{"Title": "Tumblr will ban all adult content on December 17th.", "Author": "u/yashendra2797", "Content": "Get ready to mass download your favorite adult blogs guys. This is gonna be such a punch in the gut to so many talented creators. Tumblr had some of the best captions, 3D Porn, 2D Porn, TG Porn, Art Porn, Alt Porn, and Erotica. If anyone knows of a way to mass rip sites please post them in the comments. Source: The Verge Tumblr Community Policy Tumblr Announcement EDIT: Oh boy. 106 replies meant my phone was buzzing all night. Woke up 2 hours late because my phones were dead. Thanks for getting me extra sleep guys!"},
{"Title": "Youtube educational hacking content getting banned", "Author": "u/ignaloidas", "Content": "No content"},
{"Title": "Would it be possible to recover previously-deleted photos from a 1998 digital camera?", "Author": "u/Throwaway173638o", "Content": "I was curious in recovering any previously-deleted and current photos off a vintage camera.  The catch is that it doesn't have an SD card and that taking any additional photos after its filled starts to delete them. It does use a 3.5 mm cord with some kind of port for vintage computers. I have no problem getting a 3.5 mm to USB cord. Is there a similar process with data recovery for SD cards and hard drives that I can do with recovering data from the camera?  Would I also need some drivers for this camera to detect too? For context, the digital camera is a Mattel Barbie Digital Camera from 1998."},
{"Title": "A backup everyone must have (the storage media may be different)", "Author": "u/psych_1337", "Content": "No content"},
{"Title": "I was told I belong here", "Author": "u/dshbak", "Content": "No content"},
{"Title": "WD MyPassport Ultra vs WD Black [6TB]", "Author": "u/Previous_Day4842", "Content": "Is one of these drives better than the other? It seems on here some people mention the WD black being total rubish. I really like these drives for the aesthetic, but have never owned one. I have owned the MyPassport models for years and years and have never had an issue. Are they equal and i'm safe to get the WD Black for aesthetics? Or would it be wiser to get the MyPassport Ultra, with the metal build and USB-C Connection? I would be using this drive for time machine backups. Aesthetics are rather important to me, so it is a bummer that the MyPassport ultra does not come in black."},
{"Title": "When your boss is an insurance agent... ioSafe 1019+", "Author": "u/wirerogue", "Content": "No content"},
{"Title": "Twitter will soon begin suspending accounts that have been inactive for 30 days", "Author": "Unknown author", "Content": "No content"},
{"Title": "How much free space should I leave on my 1TB HDD with a btrfs file system ?", "Author": "u/Yukinoooo", "Content": "Is it dangerous to leave 80GB on my 1TB HDD (only files without OS) ? I'm using GNU/Linux + KDE"},
{"Title": "NAS in transit to new house. Baby in trunk.", "Author": "u/therealschwartz", "Content": "No content"},
{"Title": "Twitter removed a studentâ€™s tweets critical of exam monitoring tool due to DMCA notice; EFF claims it is textbook example of fair use", "Author": "u/retrac1324", "Content": "No content"},
{"Title": "Starting my self owned drive journey", "Author": "u/tessereis", "Content": "I'm moving from various cloud drives to local backups. I really would like some suggestion from this sub. I've 2 backups. SSD 1TB - formatted to exFat. I've heard bad things about this fs but I really need this drive to be cross compatible and plug and play. This seemed to be the only sane choice. HDD 2TB - formatted to ext4/zfs. Haven't decided on this yet, will probably go with zfs because it auto recovers and doesn't need to run something like fsck. Increased size for versioning. I'm planning to keep my SSD portable (on a trip etc) and whenever I'm at home, connect it to a system (rasp pi or a regular PC) with photoprism or librephotos installed. HDD is going to be the cold storage. I tried to follow the 3-2-1 rule but can't afford to get another drive just yet. Although, I'm thinking to get S3 Glacier for that offsite storage."},
{"Title": "PSA: Verbatim no longer sells real M Discs, now puts regular BD-Rs in M Disc packaging", "Author": "u/JazzKazz", "Content": "TLDR: instead of selling real M Discs, Verbatim now puts their cheap organic BD-Rs into M Disc cases and charges M Disc prices for them In July, I bought 25GB Verbatim M Discs from Amazon. Even though I bought them directly from Amazon Europe, the discs I received were not real M Discs but regular Verbatim BD-Rs with an organic layer that were made to look like M Discs. I noticed right away because the MID of the discs was VERBAT-IMe-000 , which is the code for their regular BD-Rs, instead of MILLEN-MR1-000 which is the MID that all 25GB M Discs have. At this point I assumed I'd been sold fakes, but 3 months later I again ordered Verbatim M Discs, this time from German retail chain Saturn, and once again received these discs that I assumed are fakes. I emailed Verbatim's customer service and prepared a bunch of images that show these fake M Discs next to real ones. But to my surprise, after a debate with customer service they told me that these are not fakes, and that these \"are the only M Discs that are going to be sold from now on\" (quote). What's insane is that these discs currently being sold are not M Discs at all, but regular organic layer Verbatim BD-Rs, yet Verbatim still calls these M Disc. When I tried calling them out on their lies by pointing out things such as the discs' MID being the same as that of regular BD-Rs and the discs having 6x burn speed despite real M Discs being 4x speed, they just chucked it up to \"the discs being completely reworked, and we moved production facility hence the new DISC IDs\". The most ridiculous part is, these \"new M Discs\" (as Verbatim support calls them) are writable in any standard Blu Ray drive, you don't even need a drive that supports M Disc burning! For those unaware, M Discs require an M Disc capable drive to be burned, because M Discs need a stronger laser than what is used for regular BDs. This stronger laser is only in M Disc drives and there is no way you could ever write a real M Disc in a non M Disc drive. Yet here we have customers being sold cheap organic layer BD-Rs and being deceived into thinking they're buying M Discs. I find this absolutely insane as people burn hundreds of these discs a day, trusting them to reliably hold precious data, yet most people aren't aware they're not burning a real M Disc, but just a garden variety BD-R that has none of the M Disc advantages that you pay for. So far the only mention of this that I've found online is a German thread from August where somebody received these same VERBAT-IMe-000 discs as me and thinks they're fake, not aware that Verbatim themselves are behind these discs. Some stores still have real M Discs in stock, but the majority of them (at least in Germany) now sell the new, fake kind, as I've ordered M Discs from various stores over the past few weeks and 90% of the time received the new fake kind which I returned. It probably also depends on region, I have no idea about discs in the US or other countries. Check the IDs of your discs people. Quick check: A real M Disc has a copper/gold tint on the back, the new fake ones are silver A real M Disc (25GB) has the MID/DISC ID: MILLEN-MR1-000, no matter what brand A real M Disc only burns in a drive with M Disc support"},
{"Title": "Tumblr will ban all adult content on December 17th.", "Author": "u/yashendra2797", "Content": "Get ready to mass download your favorite adult blogs guys. This is gonna be such a punch in the gut to so many talented creators. Tumblr had some of the best captions, 3D Porn, 2D Porn, TG Porn, Art Porn, Alt Porn, and Erotica. If anyone knows of a way to mass rip sites please post them in the comments. Source: The Verge Tumblr Community Policy Tumblr Announcement EDIT: Oh boy. 106 replies meant my phone was buzzing all night. Woke up 2 hours late because my phones were dead. Thanks for getting me extra sleep guys!"},
{"Title": "SFTP/FTP/Local File Move", "Author": "u/Dking2204", "Content": "Coming from MacOS using Transit to Windows, I would like to move a large number of personal files from the Mac drives to the new Windows machine. What software is recommended? I keep seeing that Filezilla needs to be more secure, and I'm unsure about others. I appreciate any help you can provide."},
{"Title": "The Internet Archive is in danger", "Author": "u/Cereal_is_great", "Content": "No content"},
{"Title": "A backup everyone must have (the storage media may be different)", "Author": "u/psych_1337", "Content": "No content"},
{"Title": "Offline served and auto-updating Wikipedia instance", "Author": "u/BarthoAz", "Content": "Hello! I was wondering if there was some tool out there that could: download the entire Wikipedia database (w/ images and in maybe 2~3 languages) keep it synced/updated being able to serve it statically through a self-hosted website bonus: keeping all of the data in files easily readable by humans, even without any tool Quite a wishlist, but I want to know if something like this (or similar) already exists before trying to do it myself!"},
{"Title": "When your boss is an insurance agent... ioSafe 1019+", "Author": "u/wirerogue", "Content": "No content"},
{"Title": "NOAA Coast Survey is shutting down the Raster Navigational Chart Tile Service (RNC) and other related services", "Author": "u/TheHornedGod", "Content": "I stumbled across this while doing some research and noticed there are no threads here about it. I'm late to the party on this but I thought if there are some people already on the case then maybe users from this subreddit might be interested in finding them to help with backups and hosting. The NOAA is shutting down some of their online and printed services and they are removing that data from their websites. This project actually began in 2021 and is set to be completed in 2025. So what is the RNC? NOAA's RNC Tile Service (WMTS) The NOAA RNC Tile Service provides standardized nautical chart tilesets for the public, eliminating the need for application developers to regularly undergo the cumbersome process of transforming NOAA BSB files into tilesets. It provides geo-referenced charts compatible with the Web Map Tile Specifications (WMTS) and Tile Map Service Specification (TMS). All tilesets are published on a weekly basis. Original website (already deleted?): https://tileservice.charts.noaa.gov Their annoucement: NOAA will shut down its Raster Navigational Chart (RNC) Tile Service and the online RNC Viewer on October 1, 2021. The NOAA Seamless Raster Navigational Chart Services will be shut down on January 1, 2022. This is part of a larger NOAA program to end production and maintenance of all NOAA traditional paper and raster nautical charts that was announced in the Federal Register in November 2019. Cancellation of traditional NOAA paper nautical charts and associated raster nautical chart products, such as BookletChartsâ„¢ and Raster Navigational Charts (RNC) will occur over the next four years and be completed by January 2025. More information about this overarching program to â€œsunsetâ€ traditional  nautical chart products is available on the â€œ Farewell to Traditional Nautical Charts â€ web page. -- source Why help with this project? The abandoned nautical tilesets and paper charts have been replaced with GIS offerings for modern equipment, however these older documents were a failsafe for smaller vessels and operations. They may not have the technical know how to backup and retrieve this data otherwise."},
{"Title": "NAS in transit to new house. Baby in trunk.", "Author": "u/therealschwartz", "Content": "No content"},
{"Title": "Photo deduplication on Mac", "Author": "u/ChumboChili", "Content": "Hello all - I have a significant volume of photo dupes to work through, but I am a bit particular as to how I want to proceed through them, and so I wanted to ask about those with experience with deduplication apps. I want to create a master set of photos, organized by year and device.  Accordingly, in a serial fashion, I want to use a folder as a reference source, compare its contents to a second folder, and in the second folder I would like to be able to: --easily see the non-duplicate photos; --also be able to see the duplicate photos, and delete them. I would also like to be able to identify and delete duplicates WITHIN a folder and its subfolders, although I understand that functionality is pretty standard. One final question, is whether this can also be performed for video files. Any recommendations to achieve this workflow would be most appreciated.  Thanks all."},
{"Title": "PSA: Verbatim no longer sells real M Discs, now puts regular BD-Rs in M Disc packaging", "Author": "u/JazzKazz", "Content": "TLDR: instead of selling real M Discs, Verbatim now puts their cheap organic BD-Rs into M Disc cases and charges M Disc prices for them In July, I bought 25GB Verbatim M Discs from Amazon. Even though I bought them directly from Amazon Europe, the discs I received were not real M Discs but regular Verbatim BD-Rs with an organic layer that were made to look like M Discs. I noticed right away because the MID of the discs was VERBAT-IMe-000 , which is the code for their regular BD-Rs, instead of MILLEN-MR1-000 which is the MID that all 25GB M Discs have. At this point I assumed I'd been sold fakes, but 3 months later I again ordered Verbatim M Discs, this time from German retail chain Saturn, and once again received these discs that I assumed are fakes. I emailed Verbatim's customer service and prepared a bunch of images that show these fake M Discs next to real ones. But to my surprise, after a debate with customer service they told me that these are not fakes, and that these \"are the only M Discs that are going to be sold from now on\" (quote). What's insane is that these discs currently being sold are not M Discs at all, but regular organic layer Verbatim BD-Rs, yet Verbatim still calls these M Disc. When I tried calling them out on their lies by pointing out things such as the discs' MID being the same as that of regular BD-Rs and the discs having 6x burn speed despite real M Discs being 4x speed, they just chucked it up to \"the discs being completely reworked, and we moved production facility hence the new DISC IDs\". The most ridiculous part is, these \"new M Discs\" (as Verbatim support calls them) are writable in any standard Blu Ray drive, you don't even need a drive that supports M Disc burning! For those unaware, M Discs require an M Disc capable drive to be burned, because M Discs need a stronger laser than what is used for regular BDs. This stronger laser is only in M Disc drives and there is no way you could ever write a real M Disc in a non M Disc drive. Yet here we have customers being sold cheap organic layer BD-Rs and being deceived into thinking they're buying M Discs. I find this absolutely insane as people burn hundreds of these discs a day, trusting them to reliably hold precious data, yet most people aren't aware they're not burning a real M Disc, but just a garden variety BD-R that has none of the M Disc advantages that you pay for. So far the only mention of this that I've found online is a German thread from August where somebody received these same VERBAT-IMe-000 discs as me and thinks they're fake, not aware that Verbatim themselves are behind these discs. Some stores still have real M Discs in stock, but the majority of them (at least in Germany) now sell the new, fake kind, as I've ordered M Discs from various stores over the past few weeks and 90% of the time received the new fake kind which I returned. It probably also depends on region, I have no idea about discs in the US or other countries. Check the IDs of your discs people. Quick check: A real M Disc has a copper/gold tint on the back, the new fake ones are silver A real M Disc (25GB) has the MID/DISC ID: MILLEN-MR1-000, no matter what brand A real M Disc only burns in a drive with M Disc support"},
{"Title": "How do you archive emails?", "Author": "u/Commercial_Union_296", "Content": "How were you able to save your emails from many years back?"},
{"Title": "The Internet Archive is in danger", "Author": "u/Cereal_is_great", "Content": "No content"},
{"Title": "For Those That Archive YouTube Videos From Your Favorite Channels, Do You Archive Your Videos in the Highest Possible Quality or Do You Limit The Quality of Your Downloads to Save Storage Space?", "Author": "u/Ripcitytoker", "Content": "I personally always save videos in the highest quality possible, regardless of how much storage it takes u. Does anyone else do this or do you download videos in lower quality (like 1080p instead of 4k) in order to use up less storage space?"},
{"Title": "How much damage have I caused from leaving a HDD in the sun?", "Author": "u/JerichoBlows", "Content": "I ordered a refurbished 12TB Seagate Enterprise HDD and it was delivered today while I wasn't home. It was over +100Â°F (+38Â°C) degrees today and the sun was beaming directly on the mailbox for 4 hours before I got to it. I suspected the box could have easily been 130Â°F (55Â°C) or possibly much hotter having been in the direct sunlight for so long. I immediately put the entire box in the refrigerator. After about 5 minutes in the fridge, I decided to measure the temp using a digital BBQ thermometer. It measured at 104Â°F (40Â°C) but this was likely not accurate because I was using a thermometer that is meant to be shoved inside of meat and the box had already been in the fridge for 5 mins. How much damage could have been caused from it sitting in the direct +100Â°F (+38Â°C) degree sunlight for just over 4 hours? Thanks!"},
{"Title": "How to organize 10+ years of computer and phone backups? ~3TB", "Author": "u/PrivateAd990", "Content": "I'm looking for strategies, tools / software and tips to make my long journey of organizing these backups easier. Main questions at the bottom for a TLDR. About the content: ~3tb total, 15 laptop backups, 12 phone backups there will be overlap / duplicates in content between backups backups contain folders I manually dragged onto the portable drive. I never plan to do a full restore of a backup. backups may contain photos, videos, downloads, photo editing files, code and projects I wrote, a lot of junk I'd like to scrap Hardware: A recent MacBook Pro empty Samsung T9 4TB SSD, ~2000MB/s WD 2.5\" passport HDD 5TB, ~110MB/s A second HDD with a copy of the above for redundancy Plan: first bring everything on the HDD to the SSD since the SSD is way faster sort through everything. I need help with this part move the organized backup back to the HDD since SSD's aren't suitable for cold storage implement a plan for the 3 2 1 backup method Questions: software or tools to sort, organize, de-dupe, delete through everything on the drive. Free or paid tips for how to search through everything instead of going folder by folder? I'm guessing software can help here. output folder structure suggestions? Should I just flatten all backups to one? Let's say, all photos I took with my phone from all the backups to one folder? Or is that a bad idea"},
{"Title": "Live sync of data drive over the internet or simple backup. Lsyncd, DRDB or Kopia backup", "Author": "u/1000Zebras", "Content": "Hi, I'm curious what you guys would implement in this situation in order to, above all, simply maintain at the very least one solid, off-site backup of all of my data files and also, in the event of something happening to my main data drive on-site, reduce downtime as much as possible. Here is my current setup as is relevant to the question at hand: OrangePi 5 Plus running dietpi (so pretty much just debian) as my main server on-site One eMMC boot drive on the OrangePi containing the OS and all of my docker-compose files, as well as the OS itself Recently acquired 14TB external USB drive that houses purely my data for all of my docker containers (and then some outside of those, as well, but not much) OrangePi is running Tailscale A second RPi that lives at my brother's house also running Tailscale (so any connection between the two will more than likely be running over the interwebs, but through Tailscale) and with a second 14tb drive identical to the other connected to it, ready for data storage What I'm wondering is what may be the best strategy for maintaining a backup of the main data drive on the secondary drive, ideally in a mirrored fashion such that were the main drive to fail, I'd simply be able to plug in the secondary drive to the OrangePi, mount it at the same mountpoint as primary would have been, and I'd be back up and running nearly immediately (once the drive was physically moved between locations, of course). It's worth noting that, at present, I am dealing with nearly 4.5TB of data on main data drive (also currently backed up to the cloud via Kopia and iDrive E2) I've been considering: Trying out lsyncd or DRDB in order to literally have the drives mirror each other in as near realtime as the connection will allow. I have not used either of these tools yet, however, so I'm not familiar with exactly how they work behind the scene. And also, I realize that it is a lot of data to keep in sync over an internet connection, especially at file or block-level granularity as I believe those tools are designed for. In \"normal\" usage, I am not necessarily adding or changing all that much data on a day to day basis, but were I to make any major shifts in organization, or simply to add a lot more data into the mix suddenly, I'm wondering if the tools would be able to keep up Running an rsync job over ssh at a specified interval (say, maybe, a couple of times a day) in order to keep the two up date. I would of course again run into the same problem that would arise with the first option were I to make any drastic changes, but theoretically I'd eventually always have a 1 to 1 sync/backup between the two drives Simply running some sort of backup program from the main Orangepi data drive to the RPi's data drive, again at whatever specified interval (say, maybe, daily). I'd probably have to run some sort of webDAV server on the secondary RPi in order to facilitate backups between the two were I to use Kopia. Or, I suppose I could even run the data drive on RPi on a minio instance and have Kopia backup via the S3 protocol, but this seems perhaps like a little bit of overkill, and it wouldn't necessarily be the sort of 1 to 1 sync I'm shooting for as Kopia would organize the backup data in a fashion that it understands. This would be acceptable, though, as again at the end of the day the most important thing is to have all of the data itself stored safely in both locations, one way or another. How would you guys go about keeping things in sync between the two data drives? Or, should I just eschew that idea given the limitations of the bandwidth/connection between the two and go for straight backups using Kopia, or some othe rbackup system? Please, if you have any thoughts on how you'd architect this scenario, I'd very much appreciate any and perspectives/insights. Hopefully that all makes sense. If you need anything clarified, by all means speak up and I'll do my best to address. Thank you so very much for your time, expertise, and patience with my rambling question. I look forward to hearing how people weigh in. Kind Regards, LS"},
{"Title": "Python script to help identify hot swapped drives", "Author": "u/radialmonster", "Content": "No content"},
{"Title": "Question related to the voiceover feature (headphone symbol) in internet archive", "Author": "u/69PepperoniPickles69", "Content": "Do you know if there's a way to upload my own files so that listen to them instead of reading? Or does it have to include actually uploading books and them getting approved, scripted and so on? And if so do you guys know any alternative website where we can upload large texts for listening with decent quality?"},
{"Title": "How best to migrate to new hardware?", "Author": "u/SlayterDevAgain", "Content": "I'm about to build a new NAS. With my current NAS I just kind of threw drives at it as I aquired them (2x 2TB in RAID 1, a 4TB and a 6TB drive in pools by themselves, and 5 6TB drives in an external enclosure in RAID 5). In the new build I have 3 10TB drives I'll be adding. I don't necessarily need to keep all the drives from the old build (I at least want the 5x 6TB drives but not externally) but what would be the best way to migrate the data to the new build? Any advice is appreciated. Further info: Current build is FreeNAS and I'll probably keep with that or TrueNAS."},
{"Title": "Free Modular NAS Enclosure - Stackable Drive Expansion, ITX Vertical Layout, 3D Print Files Included", "Author": "u/ethanross1a", "Content": "No content"},
{"Title": "Free Modular NAS Enclosure - Stackable Drive Expansion, ITX Vertical Layout, 3D Print Files Included", "Author": "u/ethanross1a", "Content": "No content"},
{"Title": "4x5.25\" to 7x3.5\" adapter?", "Author": "u/Adam1394", "Content": "Hello, I look for aforementioned adapter for my Define XL R2 case."},
{"Title": "Hello my name is Nikon_Justus and I am a DataHoarder", "Author": "u/Nikon_Justus", "Content": "No content"},
{"Title": "Hello my name is Nikon_Justus and I am a DataHoarder", "Author": "u/Nikon_Justus", "Content": "No content"},
{"Title": "Backing up Google calendar", "Author": "u/c05d", "Content": "Hi, Iâ€™ve been using Google calendar for close to 20 years now. Itâ€™s started to bother me that my entire life is in Googleâ€™s hands and Id like to back this up & transfer to another service like Outlook whatâ€™s the best way to do this? I want everything including tasks etc to transfer thanks"},
{"Title": "Took an old IBM hard drive from the 80s and turned it into an art piece", "Author": "u/NO-LAN", "Content": "No content"},
{"Title": "Took an old IBM hard drive from the 80s and turned it into an art piece", "Author": "u/NO-LAN", "Content": "No content"},
{"Title": "What is the best free(non trial) software for converting a dvd to mp4 (ideally with subtitles)?", "Author": "u/Immediate-Risk-7569", "Content": "Every software I found is a trial with either a time limit or an ugly watermark."},
{"Title": "After hoarding over 50k YouTube videos, here is the youtube-dl command I settled on.", "Author": "u/Veloldo", "Content": "EDIT: If you are reading this, I've made a few small changes. You can find the actual scripts I use here: https://github.com/velodo/youtube-dl_script . While my serup works great for me, if you're looking for a more robust solution, please check out TheFrenchGhosty's scripts here: https://github.com/TheFrenchGhosty/TheFrenchGhostys-YouTube-DL-Archivist-Scripts , with the associated reddit thread here: https://redd.it/h7q4nz . After seeing all of the posts recently regarding youtube-dl, I figured I would chime in on the options I use. There are a few things I want to implement as some point, see the bottom of this post for those. Also, if anyone sees anything that can be done better, please let me know as I am always looking for ways to improve everything I do! Also, this post isn't intended to be a guide on how to use youtube-dl, this is more for the arguments I use and why I use them. If you need help getting youtube-dl running, setting up a batch script, etc. there are plenty of guides for that sort of thing elsewhere. The command (DONT COPY PASTE THIS ONE): youtube-dl --download-archive \"archive.log\" -i --add-metadata --all-subs --embed-subs --embed-thumbnail --match-filter \"playlist_title != 'Liked videos' & playlist_title != 'Favorites'\" -f \"(bestvideo[vcodec^=av01][height>=1080][fps>30]/bestvideo[vcodec=vp9.2] [height>=1080][fps>30]/bestvideo[vcodec=vp9][height>=1080][fps>30]/bestvideo[vcodec^=av01] [height>=1080]/bestvideo[vcodec=vp9.2][height>=1080]/bestvideo[vcodec=vp9] [height>=1080]/bestvideo[height>=1080]/bestvideo[vcodec^=av01][height>=720] [fps>30]/bestvideo[vcodec=vp9.2][height>=720][fps>30]/bestvideo[vcodec=vp9][height>=720] [fps>30]/bestvideo[vcodec^=av01][height>=720]/bestvideo[vcodec=vp9.2] [height>=720]/bestvideo[vcodec=vp9][height>=720]/bestvideo[height>=720]/bestvideo)+ (bestaudio[acodec=opus]/bestaudio)/best\" --merge-output-format mkv -o \"%cd%/%% (playlist_uploader)s/%%(playlist)s/%%(playlist_index)s - %%(title)s - %%(id)s.%%(ext)s\" \"[URL HERE TO CHANNELS PLAYLISTS]\" The command again (copy paste friendly): youtube-dl --download-archive \"archive.log\" -i --add-metadata --all-subs --embed-subs --embed-thumbnail --match-filter \"playlist_title != 'Liked videos' & playlist_title != 'Favorites'\" -f \"(bestvideo[vcodec^=av01][height>=1080][fps>30]/bestvideo[vcodec=vp9.2][height>=1080][fps>30]/bestvideo[vcodec=vp9][height>=1080][fps>30]/bestvideo[vcodec^=av01][height>=1080]/bestvideo[vcodec=vp9.2][height>=1080]/bestvideo[vcodec=vp9][height>=1080]/bestvideo[height>=1080]/bestvideo[vcodec^=av01][height>=720][fps>30]/bestvideo[vcodec=vp9.2][height>=720][fps>30]/bestvideo[vcodec=vp9][height>=720][fps>30]/bestvideo[vcodec^=av01][height>=720]/bestvideo[vcodec=vp9.2][height>=720]/bestvideo[vcodec=vp9][height>=720]/bestvideo[height>=720]/bestvideo)+(bestaudio[acodec=opus]/bestaudio)/best\" --merge-output-format mkv -o \"%cd%/%%(playlist_uploader)s/%%(playlist)s/%%(playlist_index)s - %%(title)s - %%(id)s.%%(ext)s\" \"[URL HERE TO CHANNELS PLAYLISTS]\" I know it looks long and scary, let me break it down a little bit: --download-archive \"archive.log\" This keeps track of all the videos you have downloaded so they can be skipped over the next time it's ran or the next time it finds that video. -i Ignore any errors that occur while downloading. Occasionally they will happen and this just ensures things keep moving along as intended. Don't worry, the next time it is ran any videos that didn't fully download will most likely be picked right back up where it left off! --add-metadata --all-subs --embed-subs --embed-thumbnail These just embed metadata into the video once it's done downloading. You never know when this will come in handy, and having it all right in the video's container is nice. Just a little note, at the time of writing this post, ffmpeg can't embed images into a mkv, but the image is still downloaded and stored in the same location and with the same name as the video. --match-filter \"playlist_title != 'Liked videos' & playlist_title != 'Favorites'\" This will filter videos out that you don't want to download. Here is just a basic example of filtering out playlists with the title \"Liked Videos\" and \"Favorites\". I find this especially useful for filtering out playlists that contain a bunch of videos from other playlists. For example, if I'm downloading videos from a gaming channel and they have a playlist for \"Gmod\" and one for \"Minecraft PC\", but they also have one called \"PC Games\" that contains the contents of both the Gmod and the Minecraft playlists, I sometimes will want to keep those separate, so I will filter out the \"PC Games\" playlist. If there are videos in that playlist you still want, you can always add another youtube-dl command to your script with that playlist specifically. Depending on the channel, this can get rather annoying to manage, but its a good way to keep things better organized. -f \"(bestvideo[vcodec^=av01][height>=1080][fps>30]/bestvideo[vcodec=vp9.2][height>=1080] [fps>30]/bestvideo[vcodec=vp9][height>=1080][fps>30]/bestvideo[vcodec^=av01] [height>=1080]/bestvideo[vcodec=vp9.2][height>=1080]/bestvideo[vcodec=vp9] [height>=1080]/bestvideo[height>=1080]/bestvideo[vcodec^=av01][height>=720] [fps>30]/bestvideo[vcodec=vp9.2][height>=720][fps>30]/bestvideo[vcodec=vp9][height>=720] [fps>30]/bestvideo[vcodec^=av01][height>=720]/bestvideo[vcodec=vp9.2][height>=720]/ bestvideo[vcodec=vp9][height>=720]/bestvideo[height>=720]/bestvideo)+ (bestaudio[acodec=opus]/bestaudio)/best\" Ok. This is where things get a little tricky. I first want to start off by saying that this isn't totally necessary as all the -f command does is allows you to set preferences on what video and audio streams you want to download. If you want a basic rundown on how this works, the youtube-dl readme explains it better than I ever could. For my case here, I want to download video streams in certain codecs, which have a hierarchy of [av1>vp9.2>vp9>whatever is available]. It will keep going down the list until one is found that meets my criteria. You can also see that I prefer videos in 1080 with more than 30 fps, then 1080 30 fps, and that repeats for 720. I also prefer to get audio in opus if it's available. Just a side note for anyone wondering what vp9.2 is, it is the vp9 codec with HDR. Why bother with all of that nonsense when youtube-dl will automatically pick the best streams for you? Well, the way youtube-dl picks the best stream is based solely on bitrate. This means that for video it will usually chose the avc1 codec, which is pretty old at this point, and while it still looks good, I've found that the other codecs offer a smaller file size and similar or better quality. You may find otherwise and want to do things differently, but for me, this is how I do it as it saves hard drive space and I find the quality good. Also, as you will notice, I don't have any resolutions higher than 1080 on there. The way I have it, it should catch those higher res streams, but as of now, I don't archive many youtubers' videos that upload in higher res so I haven't found the need, but some day I'm sure I will change it. I already know your asking yourself, \"If this will catch the higher resolution streams, why don't you just leave the 720 options in there and remove the 1080?\". Well, it's because I've noticed that youtube has started to transcode many videos to the newer av1 codec, but so far most videos that I've seen only go up to 720 for the av1 codec. This means that if that stream is available, but there isn't a 1080p av1 stream, then it will always download those videos in 720p even if a higher res stream is available. --merge-output-format mkv -o \"%cd%/%%(playlist_uploader)s/%%(playlist)s/%%(playlist_index)s - %%(title)s - %%(id)s.%%(ext)s\" This just tells youtube-dl where I want the file and that I want it in an mkv container. It's pretty self explanatory, but I basically want a folder structure of \"[CHANNEL NAME]/[PLAYLIST NAME]/[PLAYLIST INDEX] - [VIDEO TITLE] - [YOUTUBE VIDEO ID].[EXTENSION]\". Feel free to customize this however you see fit. Please note that I used double % in some of these due to my script being a batch file ran on a windows VM. Things I want to do: - Create a docker container that runs the script. While the Windows VM is working perfect for this, it's the only thing the VM does now (used to be used for much more, but that has all been offloaded). It should be pretty easy since I leave the executables and the script all in a network share as is, so all it would need is the dependencies (which I think is only python if I'm not mistaken) and to set up a cron job. - Simplify the script a bit by using the -a argument. This would allow me to set up a file with the links I want to download. This would allow me to group a bunch of commands that all have the same arguments into 1 command. - Write a script that will move videos that were downloaded before they were put into playlists into their respective playlists once the uploader adds them. Right now what I do is download all of the uploader's playlists, then download all of their videos (using the same archive file so it doesn't re-download any). This means if the uploader is slow to add the video to a playlist, it will just be downloaded to a \"No Playlist\" folder. The other way I could do this would be to find a way to deduplicate all of the videos in the \"No Playlist\" folder and just use separate archive files for the playlist and non playlist videos, which might download some videos twice, but then later deduplicated. Final Thoughts: Youtube-dl is a wonderful and powerful tool, and with all of the crap going down on YouTube, you can never be too sure what videos you love might be taken down. Just what I've managed to download has already helped me and some of my friends out. It definitely is worth your time to automate downloading videos from channels you enjoy, and with a little know-how and experimentation, it doesn't take much time or effort to get something to a point where you can set it and forget it. Anyway, that was certainly longer than I thought it would be, but I really hope it helps some of you guys out. I've gained so much knowledge from this subreddit and it would mean a lot if I gave back and helped one of you out in return. Happy Hoarding! Just a quick edit: Be sure to check out the comments for some excellent ideas and more information on some things! As always, take this information and adapt it to your use case. Maybe my configuration will work perfectly for you, but more than likely you will have to tweak it a bit to get it just right for you. If you have any questions, please ask! Another quick edit: Some of the comments have brought up the fact that us as viewers of YouTube content, and even youtube-dl itself don't have any way to watch or download the original quality of the material as YouTube will automatically transcode videos when they are uploaded. This can be a problem for people who are trying to preserve things in the best quality they possibly can. If you are one of these people, you might want to try looking elsewhere for better quality releases of the content. The one example that immediately comes to mind for me is content from Rooster Teeth. The quality when downloaded directly from their website seems to be better quality than what you can pull from YouTube. For me personally, I will download some movies, and TV Shows and also most music and images in the best possible quality I can find, but when it comes to YouTube content, I just don't care as much and find the convenience of ripping directly from YouTube hard to beat. I also think the content tends to look great, especially for the file sizes, but this is obviously all up to you to decide."},
{"Title": "After hoarding over 50k YouTube videos, here is the youtube-dl command I settled on.", "Author": "u/Veloldo", "Content": "EDIT: If you are reading this, I've made a few small changes. You can find the actual scripts I use here: https://github.com/velodo/youtube-dl_script . While my serup works great for me, if you're looking for a more robust solution, please check out TheFrenchGhosty's scripts here: https://github.com/TheFrenchGhosty/TheFrenchGhostys-YouTube-DL-Archivist-Scripts , with the associated reddit thread here: https://redd.it/h7q4nz . After seeing all of the posts recently regarding youtube-dl, I figured I would chime in on the options I use. There are a few things I want to implement as some point, see the bottom of this post for those. Also, if anyone sees anything that can be done better, please let me know as I am always looking for ways to improve everything I do! Also, this post isn't intended to be a guide on how to use youtube-dl, this is more for the arguments I use and why I use them. If you need help getting youtube-dl running, setting up a batch script, etc. there are plenty of guides for that sort of thing elsewhere. The command (DONT COPY PASTE THIS ONE): youtube-dl --download-archive \"archive.log\" -i --add-metadata --all-subs --embed-subs --embed-thumbnail --match-filter \"playlist_title != 'Liked videos' & playlist_title != 'Favorites'\" -f \"(bestvideo[vcodec^=av01][height>=1080][fps>30]/bestvideo[vcodec=vp9.2] [height>=1080][fps>30]/bestvideo[vcodec=vp9][height>=1080][fps>30]/bestvideo[vcodec^=av01] [height>=1080]/bestvideo[vcodec=vp9.2][height>=1080]/bestvideo[vcodec=vp9] [height>=1080]/bestvideo[height>=1080]/bestvideo[vcodec^=av01][height>=720] [fps>30]/bestvideo[vcodec=vp9.2][height>=720][fps>30]/bestvideo[vcodec=vp9][height>=720] [fps>30]/bestvideo[vcodec^=av01][height>=720]/bestvideo[vcodec=vp9.2] [height>=720]/bestvideo[vcodec=vp9][height>=720]/bestvideo[height>=720]/bestvideo)+ (bestaudio[acodec=opus]/bestaudio)/best\" --merge-output-format mkv -o \"%cd%/%% (playlist_uploader)s/%%(playlist)s/%%(playlist_index)s - %%(title)s - %%(id)s.%%(ext)s\" \"[URL HERE TO CHANNELS PLAYLISTS]\" The command again (copy paste friendly): youtube-dl --download-archive \"archive.log\" -i --add-metadata --all-subs --embed-subs --embed-thumbnail --match-filter \"playlist_title != 'Liked videos' & playlist_title != 'Favorites'\" -f \"(bestvideo[vcodec^=av01][height>=1080][fps>30]/bestvideo[vcodec=vp9.2][height>=1080][fps>30]/bestvideo[vcodec=vp9][height>=1080][fps>30]/bestvideo[vcodec^=av01][height>=1080]/bestvideo[vcodec=vp9.2][height>=1080]/bestvideo[vcodec=vp9][height>=1080]/bestvideo[height>=1080]/bestvideo[vcodec^=av01][height>=720][fps>30]/bestvideo[vcodec=vp9.2][height>=720][fps>30]/bestvideo[vcodec=vp9][height>=720][fps>30]/bestvideo[vcodec^=av01][height>=720]/bestvideo[vcodec=vp9.2][height>=720]/bestvideo[vcodec=vp9][height>=720]/bestvideo[height>=720]/bestvideo)+(bestaudio[acodec=opus]/bestaudio)/best\" --merge-output-format mkv -o \"%cd%/%%(playlist_uploader)s/%%(playlist)s/%%(playlist_index)s - %%(title)s - %%(id)s.%%(ext)s\" \"[URL HERE TO CHANNELS PLAYLISTS]\" I know it looks long and scary, let me break it down a little bit: --download-archive \"archive.log\" This keeps track of all the videos you have downloaded so they can be skipped over the next time it's ran or the next time it finds that video. -i Ignore any errors that occur while downloading. Occasionally they will happen and this just ensures things keep moving along as intended. Don't worry, the next time it is ran any videos that didn't fully download will most likely be picked right back up where it left off! --add-metadata --all-subs --embed-subs --embed-thumbnail These just embed metadata into the video once it's done downloading. You never know when this will come in handy, and having it all right in the video's container is nice. Just a little note, at the time of writing this post, ffmpeg can't embed images into a mkv, but the image is still downloaded and stored in the same location and with the same name as the video. --match-filter \"playlist_title != 'Liked videos' & playlist_title != 'Favorites'\" This will filter videos out that you don't want to download. Here is just a basic example of filtering out playlists with the title \"Liked Videos\" and \"Favorites\". I find this especially useful for filtering out playlists that contain a bunch of videos from other playlists. For example, if I'm downloading videos from a gaming channel and they have a playlist for \"Gmod\" and one for \"Minecraft PC\", but they also have one called \"PC Games\" that contains the contents of both the Gmod and the Minecraft playlists, I sometimes will want to keep those separate, so I will filter out the \"PC Games\" playlist. If there are videos in that playlist you still want, you can always add another youtube-dl command to your script with that playlist specifically. Depending on the channel, this can get rather annoying to manage, but its a good way to keep things better organized. -f \"(bestvideo[vcodec^=av01][height>=1080][fps>30]/bestvideo[vcodec=vp9.2][height>=1080] [fps>30]/bestvideo[vcodec=vp9][height>=1080][fps>30]/bestvideo[vcodec^=av01] [height>=1080]/bestvideo[vcodec=vp9.2][height>=1080]/bestvideo[vcodec=vp9] [height>=1080]/bestvideo[height>=1080]/bestvideo[vcodec^=av01][height>=720] [fps>30]/bestvideo[vcodec=vp9.2][height>=720][fps>30]/bestvideo[vcodec=vp9][height>=720] [fps>30]/bestvideo[vcodec^=av01][height>=720]/bestvideo[vcodec=vp9.2][height>=720]/ bestvideo[vcodec=vp9][height>=720]/bestvideo[height>=720]/bestvideo)+ (bestaudio[acodec=opus]/bestaudio)/best\" Ok. This is where things get a little tricky. I first want to start off by saying that this isn't totally necessary as all the -f command does is allows you to set preferences on what video and audio streams you want to download. If you want a basic rundown on how this works, the youtube-dl readme explains it better than I ever could. For my case here, I want to download video streams in certain codecs, which have a hierarchy of [av1>vp9.2>vp9>whatever is available]. It will keep going down the list until one is found that meets my criteria. You can also see that I prefer videos in 1080 with more than 30 fps, then 1080 30 fps, and that repeats for 720. I also prefer to get audio in opus if it's available. Just a side note for anyone wondering what vp9.2 is, it is the vp9 codec with HDR. Why bother with all of that nonsense when youtube-dl will automatically pick the best streams for you? Well, the way youtube-dl picks the best stream is based solely on bitrate. This means that for video it will usually chose the avc1 codec, which is pretty old at this point, and while it still looks good, I've found that the other codecs offer a smaller file size and similar or better quality. You may find otherwise and want to do things differently, but for me, this is how I do it as it saves hard drive space and I find the quality good. Also, as you will notice, I don't have any resolutions higher than 1080 on there. The way I have it, it should catch those higher res streams, but as of now, I don't archive many youtubers' videos that upload in higher res so I haven't found the need, but some day I'm sure I will change it. I already know your asking yourself, \"If this will catch the higher resolution streams, why don't you just leave the 720 options in there and remove the 1080?\". Well, it's because I've noticed that youtube has started to transcode many videos to the newer av1 codec, but so far most videos that I've seen only go up to 720 for the av1 codec. This means that if that stream is available, but there isn't a 1080p av1 stream, then it will always download those videos in 720p even if a higher res stream is available. --merge-output-format mkv -o \"%cd%/%%(playlist_uploader)s/%%(playlist)s/%%(playlist_index)s - %%(title)s - %%(id)s.%%(ext)s\" This just tells youtube-dl where I want the file and that I want it in an mkv container. It's pretty self explanatory, but I basically want a folder structure of \"[CHANNEL NAME]/[PLAYLIST NAME]/[PLAYLIST INDEX] - [VIDEO TITLE] - [YOUTUBE VIDEO ID].[EXTENSION]\". Feel free to customize this however you see fit. Please note that I used double % in some of these due to my script being a batch file ran on a windows VM. Things I want to do: - Create a docker container that runs the script. While the Windows VM is working perfect for this, it's the only thing the VM does now (used to be used for much more, but that has all been offloaded). It should be pretty easy since I leave the executables and the script all in a network share as is, so all it would need is the dependencies (which I think is only python if I'm not mistaken) and to set up a cron job. - Simplify the script a bit by using the -a argument. This would allow me to set up a file with the links I want to download. This would allow me to group a bunch of commands that all have the same arguments into 1 command. - Write a script that will move videos that were downloaded before they were put into playlists into their respective playlists once the uploader adds them. Right now what I do is download all of the uploader's playlists, then download all of their videos (using the same archive file so it doesn't re-download any). This means if the uploader is slow to add the video to a playlist, it will just be downloaded to a \"No Playlist\" folder. The other way I could do this would be to find a way to deduplicate all of the videos in the \"No Playlist\" folder and just use separate archive files for the playlist and non playlist videos, which might download some videos twice, but then later deduplicated. Final Thoughts: Youtube-dl is a wonderful and powerful tool, and with all of the crap going down on YouTube, you can never be too sure what videos you love might be taken down. Just what I've managed to download has already helped me and some of my friends out. It definitely is worth your time to automate downloading videos from channels you enjoy, and with a little know-how and experimentation, it doesn't take much time or effort to get something to a point where you can set it and forget it. Anyway, that was certainly longer than I thought it would be, but I really hope it helps some of you guys out. I've gained so much knowledge from this subreddit and it would mean a lot if I gave back and helped one of you out in return. Happy Hoarding! Just a quick edit: Be sure to check out the comments for some excellent ideas and more information on some things! As always, take this information and adapt it to your use case. Maybe my configuration will work perfectly for you, but more than likely you will have to tweak it a bit to get it just right for you. If you have any questions, please ask! Another quick edit: Some of the comments have brought up the fact that us as viewers of YouTube content, and even youtube-dl itself don't have any way to watch or download the original quality of the material as YouTube will automatically transcode videos when they are uploaded. This can be a problem for people who are trying to preserve things in the best quality they possibly can. If you are one of these people, you might want to try looking elsewhere for better quality releases of the content. The one example that immediately comes to mind for me is content from Rooster Teeth. The quality when downloaded directly from their website seems to be better quality than what you can pull from YouTube. For me personally, I will download some movies, and TV Shows and also most music and images in the best possible quality I can find, but when it comes to YouTube content, I just don't care as much and find the convenience of ripping directly from YouTube hard to beat. I also think the content tends to look great, especially for the file sizes, but this is obviously all up to you to decide."},
{"Title": "\"Best\" 3.5\" 8Tb HDD Brand?", "Author": "u/Large_Medium_8984", "Content": "This question pops up all the time on here but I only see specific use cases when others ask and nothing really close to what my situation is. I'm looking to bring together all of mine and my families ancient Hard drives, laptop backups, flash drives, externals, family photo scans and videos all onto a few backup HDDs. Just over a Million files @ under 6TB that I'd like to put into an 8Tb HDD (or 2). Photos, Videos, Text Docs, and whatever else might be in there. These drives will not be used for gaming at all, so no need to worry about being rough on the drive with pulling all the time, as I see a lot of people looking for gaming AND storage when asking this. Just something to have peace of mind that nothing could go wrong in a reasonable amount of time with them. I've read good and bad things about both Seagate Barracuda and Western Digital Blue. Are there other 3.5\"s I should also look in to? I'm not expecting these drives to live dormant 5-10 years, but it would be nice. I'd like to read any and all personal experiences users have had over the years."},
{"Title": "192TB beauty. What to do with it ?", "Author": "u/henk1313", "Content": "No content"},
{"Title": "192TB beauty. What to do with it ?", "Author": "u/henk1313", "Content": "No content"},
{"Title": "What Podcasts to Hoard?", "Author": "u/4bstractals", "Content": "So, I just discovered PodcastBulkDownloader thanks to a recent thread , and it's got we wondering... If I am going to start assembling a podcast hoard, what are the criteria that I might use to decide what gets included? Obviously, podcasts I like would be the primary metric -- but I can download all of those in a couple of hours, and I have a lot more space. So... what about podcasts at risk of going behind a paywall? Podcasts of significant cultural importance? How does one best serve as a casual archivist for such a massive amount of data?"},
{"Title": "File size of Endgame's Cinema DCP", "Author": "Unknown author", "Content": "No content"},
{"Title": "File size of Endgame's Cinema DCP", "Author": "Unknown author", "Content": "No content"},
{"Title": "[YoYotta] How can I change destination folder in LTO Tape", "Author": "u/ddd102", "Content": "https://preview.redd.it/yoyotta-how-can-i-change-destination-folder-in-lto-tape-v0-6t4uhiti1w5d1.png Hi, there. I'm very newbie on YoYotta. Today, I do my first copy job. 3.5 inch HDD to LTO 8 Tape by YoYotta. But, I wonder how can I change destination folder trees. This software create just same folder trees from the source folders. I don't want that way. I want to create different folder trees on LTO Tapes which I'll do back up my data. Anyone knows how can do that? And is there any LTO or YoYotta user community? even though subreddit. I need more information. Official website of YoYotta, already I checked, but I need to story from real users. Thanks!"},
{"Title": "A friend calls and asks \"I can't find this video on any streaming service. Any chance you have it?\"", "Author": "u/TNightster", "Content": "No content"},
{"Title": "A friend calls and asks \"I can't find this video on any streaming service. Any chance you have it?\"", "Author": "u/TNightster", "Content": "No content"},
{"Title": "Vimms Lair, the largest collections of ROMs, is being taken down.", "Author": "u/snowysysadmin59", "Content": "Corporate greed at it again. Anyone got a backup? ðŸ¥º"},
{"Title": "Remember to backup your data, you never know when a spinning disk is going to fail and then you end up with a lot of shiny drinks coasters", "Author": "u/bri999", "Content": "No content"},
{"Title": "Remember to backup your data, you never know when a spinning disk is going to fail and then you end up with a lot of shiny drinks coasters", "Author": "u/bri999", "Content": "No content"},
{"Title": "Best Way To Dump/Mirror 16'000 mp3 (Podcast) Files?", "Author": "u/Redditarianist", "Content": "As the title states. I'm looking to upload around 16 thousand podcasted files to the Internet Archive & am looking for the best way. Is there an RSS ingest system at all?"},
{"Title": "This is what \"frustration-free\" packaging means in Europe", "Author": "u/FigureOfEight", "Content": "No content"},
{"Title": "This is what \"frustration-free\" packaging means in Europe", "Author": "u/FigureOfEight", "Content": "No content"},
{"Title": "Any good Kemono/coomer bulk downloaders?", "Author": "u/TravDeMan", "Content": "I've been looking for a while and im struggling to find any"},
{"Title": "80 TB homeserver for datahoarding", "Author": "u/kirjeveitsi", "Content": "No content"},
{"Title": "80 TB homeserver for datahoarding", "Author": "u/kirjeveitsi", "Content": "No content"},
{"Title": "Would it be possible to recover previously-deleted photos from a 1998 digital camera?", "Author": "u/Throwaway173638o", "Content": "I was curious in recovering any previously-deleted and current photos off a vintage camera.  The catch is that it doesn't have an SD card and that taking any additional photos after its filled starts to delete them. It does use a 3.5 mm cord with some kind of port for vintage computers. I have no problem getting a 3.5 mm to USB cord. Is there a similar process with data recovery for SD cards and hard drives that I can do with recovering data from the camera?  Would I also need some drivers for this camera to detect too? For context, the digital camera is a Mattel Barbie Digital Camera from 1998."},
{"Title": "Please do not mirror YouTube on the Internet Archive in Bulk", "Author": "u/textfiles", "Content": "https://twitter.com/textfiles/status/1492209816730808331 I posted this in a twitter thread, but I thought I'd mention this (obvious) thread here as well: Every once in a while, someone gets a brilliant idea, which is not a brilliant idea, and the first step for a mountain of heartache.  The idea is \"The Internet Archive is permanency-minded, and Youtube is full of things. I should back up Youtube on Internet Archive\". Depending on the person's capabilities and their drive, they may back up a couple videos here and there, or, as sometimes people are capable of doing, they set up a massive operation to just start jamming thousands of YouTube videos in \"just in case\".  Do not do this. YouTube is a massive ecosystem of videos, ranging from: Mirrors of neat stuff from video sources Archival copies of things on other media Businesses/Channels, ad-reliant, putting out shows And more. It's actually rather complicated and there's lots of considerations. When you decide, on your own, to \"help\" by downloading dozens of terabytes of videos, sometimes sans metadata, other times with random filenames, and just shove them into the Internet Archive, you're just hurting a non-profit by doing so. You are not a hero.  Please don't. Going to say it again: Please don't.  If you have a legitimate concern of a specific situation (creator has died, the material is some sort of culturally-relevant \"leak\" or unique situation, etc.) then communicate with the Archive (or me) about it, we'll work something out. Today's writing was brought to you by someone who could have used this information in their lives 2 months ago. UPDATE: I responded to one of the threads generated in a way that probably applies to 90% of the issues brought up."},
{"Title": "Please do not mirror YouTube on the Internet Archive in Bulk", "Author": "u/textfiles", "Content": "https://twitter.com/textfiles/status/1492209816730808331 I posted this in a twitter thread, but I thought I'd mention this (obvious) thread here as well: Every once in a while, someone gets a brilliant idea, which is not a brilliant idea, and the first step for a mountain of heartache.  The idea is \"The Internet Archive is permanency-minded, and Youtube is full of things. I should back up Youtube on Internet Archive\". Depending on the person's capabilities and their drive, they may back up a couple videos here and there, or, as sometimes people are capable of doing, they set up a massive operation to just start jamming thousands of YouTube videos in \"just in case\".  Do not do this. YouTube is a massive ecosystem of videos, ranging from: Mirrors of neat stuff from video sources Archival copies of things on other media Businesses/Channels, ad-reliant, putting out shows And more. It's actually rather complicated and there's lots of considerations. When you decide, on your own, to \"help\" by downloading dozens of terabytes of videos, sometimes sans metadata, other times with random filenames, and just shove them into the Internet Archive, you're just hurting a non-profit by doing so. You are not a hero.  Please don't. Going to say it again: Please don't.  If you have a legitimate concern of a specific situation (creator has died, the material is some sort of culturally-relevant \"leak\" or unique situation, etc.) then communicate with the Archive (or me) about it, we'll work something out. Today's writing was brought to you by someone who could have used this information in their lives 2 months ago. UPDATE: I responded to one of the threads generated in a way that probably applies to 90% of the issues brought up."},
{"Title": "WD MyPassport Ultra vs WD Black [6TB]", "Author": "u/Previous_Day4842", "Content": "Is one of these drives better than the other? It seems on here some people mention the WD black being total rubish. I really like these drives for the aesthetic, but have never owned one. I have owned the MyPassport models for years and years and have never had an issue. Are they equal and i'm safe to get the WD Black for aesthetics? Or would it be wiser to get the MyPassport Ultra, with the metal build and USB-C Connection? I would be using this drive for time machine backups. Aesthetics are rather important to me, so it is a bummer that the MyPassport ultra does not come in black."},
{"Title": "Ok which one of you did this?", "Author": "u/drbennett75", "Content": "No content"},
{"Title": "Ok which one of you did this?", "Author": "u/drbennett75", "Content": "No content"},
{"Title": "How much free space should I leave on my 1TB HDD with a btrfs file system ?", "Author": "u/Yukinoooo", "Content": "Is it dangerous to leave 80GB on my 1TB HDD (only files without OS) ? I'm using GNU/Linux + KDE"},
{"Title": "Seagate's flex - 3 ZB shipped", "Author": "u/greasythug", "Content": "No content"},
{"Title": "Seagate's flex - 3 ZB shipped", "Author": "u/greasythug", "Content": "No content"},
{"Title": "Starting my self owned drive journey", "Author": "u/tessereis", "Content": "I'm moving from various cloud drives to local backups. I really would like some suggestion from this sub. I've 2 backups. SSD 1TB - formatted to exFat. I've heard bad things about this fs but I really need this drive to be cross compatible and plug and play. This seemed to be the only sane choice. HDD 2TB - formatted to ext4/zfs. Haven't decided on this yet, will probably go with zfs because it auto recovers and doesn't need to run something like fsck. Increased size for versioning. I'm planning to keep my SSD portable (on a trip etc) and whenever I'm at home, connect it to a system (rasp pi or a regular PC) with photoprism or librephotos installed. HDD is going to be the cold storage. I tried to follow the 3-2-1 rule but can't afford to get another drive just yet. Although, I'm thinking to get S3 Glacier for that offsite storage."},
{"Title": "Just a reminder of what we have lost, and why we feel how we feel about data. I guarantee everybody that sees this can find an artist that is near to their heart. This is completely unconscionable and my heart aches looking at this list.", "Author": "u/PoorWill", "Content": "No content"},
{"Title": "Just a reminder of what we have lost, and why we feel how we feel about data. I guarantee everybody that sees this can find an artist that is near to their heart. This is completely unconscionable and my heart aches looking at this list.", "Author": "u/PoorWill", "Content": "No content"},
{"Title": "SFTP/FTP/Local File Move", "Author": "u/Dking2204", "Content": "Coming from MacOS using Transit to Windows, I would like to move a large number of personal files from the Mac drives to the new Windows machine. What software is recommended? I keep seeing that Filezilla needs to be more secure, and I'm unsure about others. I appreciate any help you can provide."},
{"Title": "Mines still going strong ðŸ¤·", "Author": "u/apeironone", "Content": "No content"},
{"Title": "Mines still going strong ðŸ¤·", "Author": "u/apeironone", "Content": "No content"},
{"Title": "Offline served and auto-updating Wikipedia instance", "Author": "u/BarthoAz", "Content": "Hello! I was wondering if there was some tool out there that could: download the entire Wikipedia database (w/ images and in maybe 2~3 languages) keep it synced/updated being able to serve it statically through a self-hosted website bonus: keeping all of the data in files easily readable by humans, even without any tool Quite a wishlist, but I want to know if something like this (or similar) already exists before trying to do it myself!"},
{"Title": "Youtube educational hacking content getting banned", "Author": "u/ignaloidas", "Content": "No content"},
{"Title": "Youtube educational hacking content getting banned", "Author": "u/ignaloidas", "Content": "No content"},
{"Title": "NOAA Coast Survey is shutting down the Raster Navigational Chart Tile Service (RNC) and other related services", "Author": "u/TheHornedGod", "Content": "I stumbled across this while doing some research and noticed there are no threads here about it. I'm late to the party on this but I thought if there are some people already on the case then maybe users from this subreddit might be interested in finding them to help with backups and hosting. The NOAA is shutting down some of their online and printed services and they are removing that data from their websites. This project actually began in 2021 and is set to be completed in 2025. So what is the RNC? NOAA's RNC Tile Service (WMTS) The NOAA RNC Tile Service provides standardized nautical chart tilesets for the public, eliminating the need for application developers to regularly undergo the cumbersome process of transforming NOAA BSB files into tilesets. It provides geo-referenced charts compatible with the Web Map Tile Specifications (WMTS) and Tile Map Service Specification (TMS). All tilesets are published on a weekly basis. Original website (already deleted?): https://tileservice.charts.noaa.gov Their annoucement: NOAA will shut down its Raster Navigational Chart (RNC) Tile Service and the online RNC Viewer on October 1, 2021. The NOAA Seamless Raster Navigational Chart Services will be shut down on January 1, 2022. This is part of a larger NOAA program to end production and maintenance of all NOAA traditional paper and raster nautical charts that was announced in the Federal Register in November 2019. Cancellation of traditional NOAA paper nautical charts and associated raster nautical chart products, such as BookletChartsâ„¢ and Raster Navigational Charts (RNC) will occur over the next four years and be completed by January 2025. More information about this overarching program to â€œsunsetâ€ traditional  nautical chart products is available on the â€œ Farewell to Traditional Nautical Charts â€ web page. -- source Why help with this project? The abandoned nautical tilesets and paper charts have been replaced with GIS offerings for modern equipment, however these older documents were a failsafe for smaller vessels and operations. They may not have the technical know how to backup and retrieve this data otherwise."},
{"Title": "I was told I belong here", "Author": "u/dshbak", "Content": "No content"},
{"Title": "I was told I belong here", "Author": "u/dshbak", "Content": "No content"},
{"Title": "Photo deduplication on Mac", "Author": "u/ChumboChili", "Content": "Hello all - I have a significant volume of photo dupes to work through, but I am a bit particular as to how I want to proceed through them, and so I wanted to ask about those with experience with deduplication apps. I want to create a master set of photos, organized by year and device.  Accordingly, in a serial fashion, I want to use a folder as a reference source, compare its contents to a second folder, and in the second folder I would like to be able to: --easily see the non-duplicate photos; --also be able to see the duplicate photos, and delete them. I would also like to be able to identify and delete duplicates WITHIN a folder and its subfolders, although I understand that functionality is pretty standard. One final question, is whether this can also be performed for video files. Any recommendations to achieve this workflow would be most appreciated.  Thanks all."},
{"Title": "How do you archive emails?", "Author": "u/Commercial_Union_296", "Content": "How were you able to save your emails from many years back?"},
{"Title": "Twitter will soon begin suspending accounts that have been inactive for 30 days", "Author": "Unknown author", "Content": "No content"},
{"Title": "Twitter removed a studentâ€™s tweets critical of exam monitoring tool due to DMCA notice; EFF claims it is textbook example of fair use", "Author": "u/retrac1324", "Content": "No content"},
{"Title": "For Those That Archive YouTube Videos From Your Favorite Channels, Do You Archive Your Videos in the Highest Possible Quality or Do You Limit The Quality of Your Downloads to Save Storage Space?", "Author": "u/Ripcitytoker", "Content": "I personally always save videos in the highest quality possible, regardless of how much storage it takes u. Does anyone else do this or do you download videos in lower quality (like 1080p instead of 4k) in order to use up less storage space?"},
{"Title": "Twitter will soon begin suspending accounts that have been inactive for 30 days", "Author": "Unknown author", "Content": "No content"},
{"Title": "Tumblr will ban all adult content on December 17th.", "Author": "u/yashendra2797", "Content": "Get ready to mass download your favorite adult blogs guys. This is gonna be such a punch in the gut to so many talented creators. Tumblr had some of the best captions, 3D Porn, 2D Porn, TG Porn, Art Porn, Alt Porn, and Erotica. If anyone knows of a way to mass rip sites please post them in the comments. Source: The Verge Tumblr Community Policy Tumblr Announcement EDIT: Oh boy. 106 replies meant my phone was buzzing all night. Woke up 2 hours late because my phones were dead. Thanks for getting me extra sleep guys!"},
{"Title": "Good Databasing software for digital media (movies in particular)?", "Author": "u/sbourwest", "Content": "My Dad has a pretty extensive collection of movies (close to 6,000), and his only real method for cataloguing is a private streaming platform (Emby), but he's interested in creating a database for all of his content, but doesn't want to have to populate the whole thing himself, he'd rather be able to just type in the media and have it pull from online databases like IMDB or the like. He would prefer PC software, no mobile apps or online-only (have to register an account) platforms, browser-based is fine though. I've tried doing a bit of looking myself but most personal film cataloguing software is focused on physical media (DVDs, Blu-Rays, etc.) and not a digital collection."},
{"Title": "Twitter removed a studentâ€™s tweets critical of exam monitoring tool due to DMCA notice; EFF claims it is textbook example of fair use", "Author": "u/retrac1324", "Content": "No content"},
{"Title": "A backup everyone must have (the storage media may be different)", "Author": "u/psych_1337", "Content": "No content"},
{"Title": "Best software to automatically catalog / search your data", "Author": "u/ripperdoc", "Content": "New to this subreddit. For long believed in manual cataloguing but Iâ€™m starting to feel I donâ€™t have time for that. Whatâ€™s the go to software for automatically cataloguing data, from documents to websites to media?"},
{"Title": "Tumblr will ban all adult content on December 17th.", "Author": "u/yashendra2797", "Content": "Get ready to mass download your favorite adult blogs guys. This is gonna be such a punch in the gut to so many talented creators. Tumblr had some of the best captions, 3D Porn, 2D Porn, TG Porn, Art Porn, Alt Porn, and Erotica. If anyone knows of a way to mass rip sites please post them in the comments. Source: The Verge Tumblr Community Policy Tumblr Announcement EDIT: Oh boy. 106 replies meant my phone was buzzing all night. Woke up 2 hours late because my phones were dead. Thanks for getting me extra sleep guys!"},
{"Title": "When your boss is an insurance agent... ioSafe 1019+", "Author": "u/wirerogue", "Content": "No content"},
{"Title": "Uploading files to a shared folder consumes my storage space?", "Author": "u/IseeYouLater", "Content": "Hello, as mentioned in the headline I am slightly confused. A friend shared a google drive folder with me to upload our vacation pics - I created a Subfolder in his shared folder and now when I upload my pics to the subfolder that I created, the files count against my storage limit. Which is bad cuz I am already very close to max. Is this intended? Everywhere on the net it says subfolders should count against the parent folder's storage limit. What am I missing here?"},
{"Title": "A backup everyone must have (the storage media may be different)", "Author": "u/psych_1337", "Content": "No content"},
{"Title": "NAS in transit to new house. Baby in trunk.", "Author": "u/therealschwartz", "Content": "No content"},
{"Title": "How often does the pCloud discount $890 10TB lifetime come around? I need to move my GSuite data soon to a new place.", "Author": "u/n4n4n4n4n", "Content": "(I will be using cryptomator to mask the data)"},
{"Title": "When your boss is an insurance agent... ioSafe 1019+", "Author": "u/wirerogue", "Content": "No content"},
{"Title": "PSA: Verbatim no longer sells real M Discs, now puts regular BD-Rs in M Disc packaging", "Author": "u/JazzKazz", "Content": "TLDR: instead of selling real M Discs, Verbatim now puts their cheap organic BD-Rs into M Disc cases and charges M Disc prices for them In July, I bought 25GB Verbatim M Discs from Amazon. Even though I bought them directly from Amazon Europe, the discs I received were not real M Discs but regular Verbatim BD-Rs with an organic layer that were made to look like M Discs. I noticed right away because the MID of the discs was VERBAT-IMe-000 , which is the code for their regular BD-Rs, instead of MILLEN-MR1-000 which is the MID that all 25GB M Discs have. At this point I assumed I'd been sold fakes, but 3 months later I again ordered Verbatim M Discs, this time from German retail chain Saturn, and once again received these discs that I assumed are fakes. I emailed Verbatim's customer service and prepared a bunch of images that show these fake M Discs next to real ones. But to my surprise, after a debate with customer service they told me that these are not fakes, and that these \"are the only M Discs that are going to be sold from now on\" (quote). What's insane is that these discs currently being sold are not M Discs at all, but regular organic layer Verbatim BD-Rs, yet Verbatim still calls these M Disc. When I tried calling them out on their lies by pointing out things such as the discs' MID being the same as that of regular BD-Rs and the discs having 6x burn speed despite real M Discs being 4x speed, they just chucked it up to \"the discs being completely reworked, and we moved production facility hence the new DISC IDs\". The most ridiculous part is, these \"new M Discs\" (as Verbatim support calls them) are writable in any standard Blu Ray drive, you don't even need a drive that supports M Disc burning! For those unaware, M Discs require an M Disc capable drive to be burned, because M Discs need a stronger laser than what is used for regular BDs. This stronger laser is only in M Disc drives and there is no way you could ever write a real M Disc in a non M Disc drive. Yet here we have customers being sold cheap organic layer BD-Rs and being deceived into thinking they're buying M Discs. I find this absolutely insane as people burn hundreds of these discs a day, trusting them to reliably hold precious data, yet most people aren't aware they're not burning a real M Disc, but just a garden variety BD-R that has none of the M Disc advantages that you pay for. So far the only mention of this that I've found online is a German thread from August where somebody received these same VERBAT-IMe-000 discs as me and thinks they're fake, not aware that Verbatim themselves are behind these discs. Some stores still have real M Discs in stock, but the majority of them (at least in Germany) now sell the new, fake kind, as I've ordered M Discs from various stores over the past few weeks and 90% of the time received the new fake kind which I returned. It probably also depends on region, I have no idea about discs in the US or other countries. Check the IDs of your discs people. Quick check: A real M Disc has a copper/gold tint on the back, the new fake ones are silver A real M Disc (25GB) has the MID/DISC ID: MILLEN-MR1-000, no matter what brand A real M Disc only burns in a drive with M Disc support"},
{"Title": "Multipath aggregation aware file copy software?", "Author": "u/pally_nid", "Content": "I barely know how to ask this question... Would anyone know of a file copying software that can handle a source/destination like a NAS with multiple 1gb network interfaces. Maybe its best if I describe the effect I am looking for. Start a copy job, reach the avg-peak of 100MB/s and then the software becomes aware of this and then continues to enumerate folders as subtasks and starts these folders on the other IP addresses available from the NAS. So, if the NAS had 2 gb/s NICs, both would be saturated at 100MB/s, rather than only one granting higher thorough-put. Thank you for reading."},
{"Title": "NAS in transit to new house. Baby in trunk.", "Author": "u/therealschwartz", "Content": "No content"},
{"Title": "The Internet Archive is in danger", "Author": "u/Cereal_is_great", "Content": "No content"},
{"Title": "FLAC Server + Player that handles Ratings well, last played?", "Author": "u/th_teacher", "Content": "I believe nothing works well with \"standard\" tags, only stored in the database? I do not need multi-user handling. But I do want to be able to periodically export from the database and store the ratings in the FLAC tags for playlist creation hoping other servers/players will at least use them read-only Last played would be nice too, is played - count a thing?"},
{"Title": "PSA: Verbatim no longer sells real M Discs, now puts regular BD-Rs in M Disc packaging", "Author": "u/JazzKazz", "Content": "TLDR: instead of selling real M Discs, Verbatim now puts their cheap organic BD-Rs into M Disc cases and charges M Disc prices for them In July, I bought 25GB Verbatim M Discs from Amazon. Even though I bought them directly from Amazon Europe, the discs I received were not real M Discs but regular Verbatim BD-Rs with an organic layer that were made to look like M Discs. I noticed right away because the MID of the discs was VERBAT-IMe-000 , which is the code for their regular BD-Rs, instead of MILLEN-MR1-000 which is the MID that all 25GB M Discs have. At this point I assumed I'd been sold fakes, but 3 months later I again ordered Verbatim M Discs, this time from German retail chain Saturn, and once again received these discs that I assumed are fakes. I emailed Verbatim's customer service and prepared a bunch of images that show these fake M Discs next to real ones. But to my surprise, after a debate with customer service they told me that these are not fakes, and that these \"are the only M Discs that are going to be sold from now on\" (quote). What's insane is that these discs currently being sold are not M Discs at all, but regular organic layer Verbatim BD-Rs, yet Verbatim still calls these M Disc. When I tried calling them out on their lies by pointing out things such as the discs' MID being the same as that of regular BD-Rs and the discs having 6x burn speed despite real M Discs being 4x speed, they just chucked it up to \"the discs being completely reworked, and we moved production facility hence the new DISC IDs\". The most ridiculous part is, these \"new M Discs\" (as Verbatim support calls them) are writable in any standard Blu Ray drive, you don't even need a drive that supports M Disc burning! For those unaware, M Discs require an M Disc capable drive to be burned, because M Discs need a stronger laser than what is used for regular BDs. This stronger laser is only in M Disc drives and there is no way you could ever write a real M Disc in a non M Disc drive. Yet here we have customers being sold cheap organic layer BD-Rs and being deceived into thinking they're buying M Discs. I find this absolutely insane as people burn hundreds of these discs a day, trusting them to reliably hold precious data, yet most people aren't aware they're not burning a real M Disc, but just a garden variety BD-R that has none of the M Disc advantages that you pay for. So far the only mention of this that I've found online is a German thread from August where somebody received these same VERBAT-IMe-000 discs as me and thinks they're fake, not aware that Verbatim themselves are behind these discs. Some stores still have real M Discs in stock, but the majority of them (at least in Germany) now sell the new, fake kind, as I've ordered M Discs from various stores over the past few weeks and 90% of the time received the new fake kind which I returned. It probably also depends on region, I have no idea about discs in the US or other countries. Check the IDs of your discs people. Quick check: A real M Disc has a copper/gold tint on the back, the new fake ones are silver A real M Disc (25GB) has the MID/DISC ID: MILLEN-MR1-000, no matter what brand A real M Disc only burns in a drive with M Disc support"},
{"Title": "PSA: It is unwise to 3D print your HDD holders out of PLA in this heatwave. Also, RAID is not a backup", "Author": "u/theartlav", "Content": "No content"},
{"Title": "How to backup an entire website from a few days ago?", "Author": "u/TheGoodSir1", "Content": "Since vimm.net got in trouble with Nintendo I can't get new games for my GBC (I have a flash cart) and I was wondering if it would be possible to back up the entirety of vimm.net to archive.org   or a personal file/files, I know you can save individual webpages but since in some sections of the website there is near 20,000 pages, it would take forever. Are there any ways too?"},
{"Title": "The Internet Archive is in danger", "Author": "u/Cereal_is_great", "Content": "No content"},
{"Title": "SaveVideo bot is safe for now - the takedown notice was not sent by Reddit but an impersonator", "Author": "u/i_have_20_bucks", "Content": "No content"},
{"Title": "using gallery-dl for downloading sub. only content in deviantart", "Author": "u/South-Order2046", "Content": "I want to download a gallery of a creator that I have been subscribed to. I first tried download them with using \"WFDownloaderApp\" but it only downloaded the public images and gave me an error while trying to download the private images (subscribers only). Then, I tried my chances with gallery-dl. First, I had tried without adding any .conf file, but it gave me \"API responded with 429 Too Many Requests.\" error.  It doesn't work with any amounts of delay. Next, I added the .conf file, put my own client_idÂ andÂ client_secret values into that file. But it still give me the same error. How can I solve this problem? Thank you all for your answers."},
{"Title": "PSA: It is unwise to 3D print your HDD holders out of PLA in this heatwave. Also, RAID is not a backup", "Author": "u/theartlav", "Content": "No content"},
{"Title": "I just put these together at work. 48 7.68TB drives in 2 Dell 740xd servers with dual Xeon Platinum processors and 768GB of RAM each.", "Author": "u/Legionnaire1856", "Content": "No content"},
{"Title": "Does anyone know how to look into .rpk files? (Related to EA games)", "Author": "u/EUOS_the_cat", "Content": "Let me know if this isn't the right sub for this, and if it isn't, point me in the right direction please. I'm on a mission to extract models from old Littlest Pet Shop games (PC, Wii), and unfortunately they were made by EA. Me and another person have looked into it, and the models are stored in the .big format. These have ways to be opened, however they lead to the .rpk files that have no information that either of us could find on how to get into. The only info about them I see is that they're used as skins for RadLight, which obviously isn't what these files are. If anyone knows a way to get into these files, I'd be very thankful."},
{"Title": "SaveVideo bot is safe for now - the takedown notice was not sent by Reddit but an impersonator", "Author": "u/i_have_20_bucks", "Content": "No content"},
{"Title": "What happened to Pornhub is a sign of things to come. Be prepared for The Great Digitial Purge.", "Author": "u/Watchmen1986", "Content": "Transitional Justice is coming. Whether it is YouTube, instagram, facebook or whatever platform you are using, a wave of self-censorship is surging. Be smart enough to save things now. Like right now."},
{"Title": "CGSociety - Site Rip", "Author": "u/valdearg", "Content": "Completely forgot about this from https://old.reddit.com/r/DataHoarder/comments/18tlukv/cgsociety_is_closing_up_soon_decades_of_valuable/ https://archive.org/details/cgsocietyarchive It's about 300GB+ Images has JSON files with their metadata alongside it. Torrent needs to be regenerated I think, not sure if there's a way to do that."},
{"Title": "I just put these together at work. 48 7.68TB drives in 2 Dell 740xd servers with dual Xeon Platinum processors and 768GB of RAM each.", "Author": "u/Legionnaire1856", "Content": "No content"},
{"Title": "Took a chance on buying 30 drives from eBay, USPS says they care, prognosis isn't good.", "Author": "u/the1337moderate", "Content": "No content"},
{"Title": "How far does your email archive go back to?", "Author": "u/Commercial_Union_296", "Content": "My sent email archive goes back to 2013."},
{"Title": "What happened to Pornhub is a sign of things to come. Be prepared for The Great Digitial Purge.", "Author": "u/Watchmen1986", "Content": "Transitional Justice is coming. Whether it is YouTube, instagram, facebook or whatever platform you are using, a wave of self-censorship is surging. Be smart enough to save things now. Like right now."},
{"Title": "Wikia, known for deleting wikis that aren't active enough, has acquired gaming history including Gamefaqs, Gamespot, Metacritic and other sites - They are in critical danger of being purged", "Author": "u/CreationBlues", "Content": "No content"},
{"Title": "Sorting tens of thousands of recovered video files by presence of audio?", "Author": "u/CyberpunkLover", "Content": "I've been collecting music video clips for about 15 years now, and collected something like 85k clips. I've been putting everything on hard drives, then after filling them up, moving everything to larger drive. The last drive I've used was some Western Digital Blue 8TB drive, and last week it filled up, so I purchased a new 20TB drive and was about to copy everything into it, but before I could do that, the WD drive failed. I've used few file restoration programs, and managed to salvage about 80% of everything that was on the drive, but the problem is, like 70% of what I've restored is either corrupted, or doesn't have audio. And all the files were put into ~75 different folders, with around 900 or so files each. I'd like to sort out the files with audio and without, in order to save the good ones and get rid of the bad ones. All files were renamed to random letters and numbers by recovery software, so the corrupted and muted files are basically completely useless to me, since I can't even use the file names to find out what files those are. All the files are either .mp4, .mkv, .avi, .mov or .wmv. Sorting through tens of thousands of files would take me months, and I just don't have the time or patience to do that. I've had the idea of importing files into video editing soft like Premiere and look at the generated waveform to find out good files,  but there's quite a lot of files with codecs unsupported by Premiere, like VP09 and such, so vast majority of files, even with audio present show up as flat lines on Premiere waveform, thus this method is completely useless to me. Like, importing a folder of restored files only generates maybe 20-30 waveforms, the rest are flat lines, and so manual sorting is still required. Anyone know of any software that can scan thousands of files and either mark or separate them by presence of audio, or have some other solution?"},
{"Title": "Took a chance on buying 30 drives from eBay, USPS says they care, prognosis isn't good.", "Author": "u/the1337moderate", "Content": "No content"},
{"Title": "He gets it", "Author": "u/ThePandaMan1110", "Content": "No content"},
{"Title": "Hard Drive Formatting from Windows to Mac  Catch-22 Situation", "Author": "u/djensenteeken", "Content": "Hey everyone, so i have a situation i need help with. I recently switched over from my HP Omen laptop to a new MacBook Pro. Not knowing about the data formatting situation, I backed up all my data from my old laptop onto my Seagate Expansion Portable Drive 2TB. When using my MacBook, I noticed I couldn't upload files to this harddrive because the file formatting was different. I found on the internet that to switch the external harddrive from NTFS to APFS I can use Disk Utility to reformat the device. For this to happen however i have to backup the files that I have on my external harddrive, because to reformat it the harddrive has to be erased. The thing is, if i back-up the files to my old HP Omen, I can't reupload the files back to my external hard drive because it has been reformatted to APFS, and the files are NTFS. I also can't back-up the files to my new MacBook, because the size of my files is way to big for the internal storage of my Macbook! Is the only option for this really just to buy another external harddrive coded to APFS, or keep my files on my hard drive on my old dusty laptop that barely functions? This feels like a catch-22 situation to me. Sorry if this question has a) been answered already or b) is not relevant to this subreddit. Any help is appreciated!"},
{"Title": "Wikia, known for deleting wikis that aren't active enough, has acquired gaming history including Gamefaqs, Gamespot, Metacritic and other sites - They are in critical danger of being purged", "Author": "u/CreationBlues", "Content": "No content"},
{"Title": "The entirety of Twitch has reportedly been leaked", "Author": "u/Megalan", "Content": "No content"},
{"Title": "NAS nvme build options", "Author": "u/AHappyGaijin", "Content": "Hello, i am currently building my first NAS using a Topton N100 board. Im currently deciding on which nvme to use as a system drive - currently leaning for a WD Blue SN580 2Tb. My question is if one should get 2 of the same drive to mirror the operating System? Using 2 nvme drives would limit the pcie expansion possibility of the board because the second nvme slot shares lanes with the nvme slot. Though currently i have no plans to use a pcie expansion card."},
{"Title": "He gets it", "Author": "u/ThePandaMan1110", "Content": "No content"},
{"Title": "Developer banned from Steam after using Steam Workshop of unreleased software as a porn stash. Which one of you did this?", "Author": "u/Metastasis3", "Content": "No content"},
{"Title": "NVMe SSD enclosure vs Portable external SSD?", "Author": "u/ProFalseIdol", "Content": "I am looking for at least 1TB to store videos and pictures. Travel a lot, so it has to be as convenient and compact as a Flash thumb drive. I saw plenty of recommendations of using NVMe + enclosure for portable consumer storage. Then I saw the SSK Portable options (while looking for their enclosure).. Also the Crucial X6.. https://www.amazon.com/SSK-Portable-External-MacBook-Laptops/dp/B0CL94LX9W?ref_=ast_sto_dp&th=1 What are the pros and cons? For 87 USD, you have a ready to use USB C or A portable solution. Got a new laptop with USB 4 and USB-C 3.2 Gen 2 ports (I assume I can get that sweet 2000 MB/s read/write).. Versus an M.2 NVMe + enclosure that I probably will never upgrade and will be permanently external as if I bought a Thumb drive.. TIA!"},
{"Title": "The entirety of Twitch has reportedly been leaked", "Author": "u/Megalan", "Content": "No content"},
{"Title": "Hard drives - from 5.25 inch to 1.0 inch", "Author": "u/HTWingNut", "Content": "No content"},
{"Title": "Is it worth archiving random pages aimlessly?", "Author": "u/peliciego", "Content": "Hi folks. I wonder if it is worth using the \"wayback machine\" and \"archive.ph\" addons for archiving random websites when you are navigating across the internet. Sometimes I focus on old websites (*index.html or other dork commands). Other times, it is local news in my surroundings. And from time to time, websites in minority languages. I don't save locally in large quantities. Do you think this strategy is worth it? Sometimes it is like planting a forest. You may never see the results in 100 years."},
{"Title": "Developer banned from Steam after using Steam Workshop of unreleased software as a porn stash. Which one of you did this?", "Author": "u/Metastasis3", "Content": "No content"},
{"Title": "Home made, non GMO, cruelty free, off-site backup", "Author": "u/subtepass", "Content": "No content"},
{"Title": "Looking for a cloud backup service", "Author": "u/aradbe", "Content": "Hey all, so pretty simple, im looking for a cloud service where can i backup some of my important files so ill have them in case i lose them. i dont know much about this kinda stuff amd im looking for something simple and reliable, i was interested to see if thers a service where i can start off with purchasing a sub for a set amount of storage and if i want more later i can add a few dollars and expand the storage. like maybe start out with 100gb and expand it to 200gb if i need it later. If there nothing like then i would just like a recommendation for a good reliable backup service that will do the job. Thanks for any replies !"},
{"Title": "Hard drives - from 5.25 inch to 1.0 inch", "Author": "u/HTWingNut", "Content": "No content"},
{"Title": "Czkawka 3.1.0 - new version of my app to find duplicates, similar images, same music, broken files etc.", "Author": "u/krutkrutrar", "Content": "No content"},
{"Title": "What's your guys system for archiving entire youtube channels look like?", "Author": "u/glowcialist", "Content": "I tried TubeArchivist, and it worked great for maybe 500 vids, but now seems to be completely blocked, I'm assuming that any script utilizing yt-dlp would face similar issues? 4k tube seemed to work alright for individual playlists at first, but now seems to be consistently throttled to like 50 kb/s. Any ideas? Edit: Please forgive the missing apostrophe in the title. I won't do it again."},
{"Title": "Home made, non GMO, cruelty free, off-site backup", "Author": "u/subtepass", "Content": "No content"},
{"Title": "My Dad and Iâ€™s second home NAS build with a total of 80tb and plenty of streaming capability.", "Author": "u/benmilek", "Content": "No content"},
{"Title": "How do I download a certain video from the wayback machine?", "Author": "u/wiicrafttech", "Content": "This is the link I'm using https://web.archive.org/web/20230520032047/https://www.youtube.com/watch?v=Oy2c6Mt9KwY#"},
{"Title": "Czkawka 3.1.0 - new version of my app to find duplicates, similar images, same music, broken files etc.", "Author": "u/krutkrutrar", "Content": "No content"},
{"Title": "I hit a bit of a milestone today", "Author": "u/Beaston02", "Content": "No content"},
{"Title": "Looking for some critiquing of my plan.", "Author": "u/OnenonlyAl", "Content": "Thanks for taking the time to read and comment. I'm looking to backup a zpool of media. I had tried to setup a truenas for the data but I didn't love the UI and I have what I need on Ubuntu with docker. My plan is to destroy a zpool of the truenas made of 4x10tb raidz1 and move that pool into the primary server (add a pcie with more sata ports) then use zpool attach to mirror my current 4x8tb raidz1 (mnt) to the newly recreated zpool of the 4x10tb from the old truenas server (named mntbackup). So zpool attach mnt to mntbackup. Sounds like this automatically starts mirroring. I know this isn't ideal without being an off-site backup but I'm not the most literate at actually managing this and want something easy. Would I be able to just plug in backup, then unplug the backup pool and cold storage? I don't need perfect redundancy. Would I be able to plug the mntbackup raidz1 back in sometime in the future and the missing new data automatically sync or would I need scripts of rysnc or zfs send receive to add new data? Thanks for any insight and help!"},
{"Title": "My Dad and Iâ€™s second home NAS build with a total of 80tb and plenty of streaming capability.", "Author": "u/benmilek", "Content": "No content"},
{"Title": "Epic Games shuts down the Unreal Engine wiki, basically the only ressource for learning the C++ aspect of it, without any real warning", "Author": "u/WPLibrar2", "Content": "No content"},
{"Title": "Suitcase for 3.5\" disks with screwed on HotSwap", "Author": "u/No-Balance-8038", "Content": "Need a way to protect my backup disks that are meant to be stored offsite. I already got cases, but they are too small to account for a 3.5\" HDD plus the mount for SilverStone Technology RM43-320-RS. What would you guys recommend? I live in Germany. From Amazon would be cool. I already have 3.5\" Protection cases but they are not long enough! The HDDs are 17cm long with it."},
{"Title": "I hit a bit of a milestone today", "Author": "u/Beaston02", "Content": "No content"},
{"Title": "this is my new nas please like and subscribe", "Author": "u/jesuswhathaveidone", "Content": "No content"},
{"Title": "Software for finding all files with identical names on a drive", "Author": "u/MisterPenishead", "Content": "I have a hard drive with thousands of files spread across different folders and some of the files have the same name in spite of them having different content. Is there a software I can use to find all files with identical names so that I can rename them?"},
{"Title": "Epic Games shuts down the Unreal Engine wiki, basically the only ressource for learning the C++ aspect of it, without any real warning", "Author": "u/WPLibrar2", "Content": "No content"},
{"Title": "Z-Library Website Is Alive Again", "Author": "u/vadhavaniyafaijan", "Content": "No content"},
{"Title": "AWS deep freeze pricing", "Author": "u/KingRollos", "Content": "I'm thinking of moving most of my backup to Amazon's deep freeze service. I'm sure it's made complicated deliberately. I'm thinking of adding it in bits. Will this cost me more? Do I need to gather it all together and then transfer in 1 go?"},
{"Title": "this is my new nas please like and subscribe", "Author": "u/jesuswhathaveidone", "Content": "No content"},
{"Title": "Unboxed my new NAS server just now and sheâ€™s already sitting on it", "Author": "u/blueskyn01se", "Content": "No content"},
{"Title": "Im looking to transfer highest quality from Hi-Fi camera for 8mm Tapes.  Which is better: Mini-USB or FireWire?", "Author": "u/Throwaway173638o", "Content": "I have a Sony Hi-Fi camera for 8mm videos that has a FireWire, Mini USB, A/V, and S-video ports.  I want to get the highest quality as possible.  I narrowed down to FireWire or Mini USB transfer for better quality. Which of the two is better overall or are they the same? Edit: For context, the camera is a Sony DCR-TRV730 and the tapes are Sony Hi8 MP 8mm Video Cassette.  Cassette also lists it as Digital8 with \"60\" while the Hi8 is listed as \"120\""},
{"Title": "Z-Library Website Is Alive Again", "Author": "u/vadhavaniyafaijan", "Content": "No content"},
{"Title": "Fitted out my old case with room enough for 20 HDD's of hoarding!", "Author": "u/lkashl", "Content": "No content"},
{"Title": "Sort backup raid 1", "Author": "u/sweetestpeach94", "Content": "Hi, I hope I can get some help. I have a Lacie 2big thunderbolt 2 set as Raid 1. Iâ€™ve just completed the backup of almost 6TB of data (mainly pics), however half of it are just the mirror copies. Now I would move the original half on another disk, the problem is that the backup isnâ€™t like sort out in different folders, itâ€™s just every file and its copy back to back in sequence all together (file A, file A-2 , file B, file B-2, file C, file C - 2 etc.). So here is my question, how I select just one half of the files to move it in the new disk without their duplicate?"},
{"Title": "Unboxed my new NAS server just now and sheâ€™s already sitting on it", "Author": "u/blueskyn01se", "Content": "No content"},
{"Title": "iFixit now has a How to guide on shucking drives", "Author": "u/Glenta3924", "Content": "No content"},
{"Title": "Fitted out my old case with room enough for 20 HDD's of hoarding!", "Author": "u/lkashl", "Content": "No content"},
{"Title": "This is why we exist - prime arguing you donâ€™t own what you pay for", "Author": "u/imajes", "Content": "No content"},
{"Title": "iFixit now has a How to guide on shucking drives", "Author": "u/Glenta3924", "Content": "No content"},
{"Title": "A terabyte isnâ€™t what it used to be - 14% of Internet customers use more", "Author": "u/It_Is1-24PM", "Content": "No content"},
{"Title": "This is why we exist - prime arguing you donâ€™t own what you pay for", "Author": "u/imajes", "Content": "No content"},
{"Title": "Who said they raise the price Before Black Friday?", "Author": "u/thenextbranson95", "Content": "No content"},
{"Title": "A terabyte isnâ€™t what it used to be - 14% of Internet customers use more", "Author": "u/It_Is1-24PM", "Content": "No content"},
{"Title": "Let's Say You Wanted to Back Up The Internet Archive", "Author": "u/textfiles", "Content": "So, you think you want to back up the Internet Archive. This is a gargantuan project and not something to be taken lightly. Definitely consider why you think you need to do this, and what exactly you hope to have at the end. There's thousands of subcollections at the Archive and maybe you actually want a smaller set of it. These instructions work for those smaller sets and you'll get it much faster. Or you're just curious as to what it would take to get everything . Well, first, bear in mind there's different classes of material in the Archive's 50+ petabytes of data storage. There's material that can be downloaded, material that can only be viewed/streamed, and material that is used internally like the wayback machine or database storage. We'll set aside the 20+ petabytes of material under the wayback for the purpose of this discussion other than you can get websites by directly downloading and mirroring as you would any web page. That leaves the many collections and items you can reach directly. They tend to be in the form of https://archive.org/details/identifier where identifier is the \"item identifier\", more like a directory scattered among dozens and dozens of racks that hold the items. By default, these are completely open to downloads, unless they're set to be a variety of \"stream/sample\" settings, at which point, for the sake of this tutorial, can't be downloaded at all - just viewed. To see the directory version of an item, switch details to download , like archive.org/download/identifier - this will show you all the files residing for an item, both Original, System, and Derived. Let's talk about those three. Original files are what were uploaded into the identifier by the user or script. They are never modifier or touched by the system. Unless something goes wrong, what you download of an original file is exactly what was uploaded. Derived files are then created by the scripts and handlers within the archive to make them easier to interact with. For example, PDF files are \"derived\" into EPUBs, jpeg-sets, OCR'd textfiles, and so on. System files are created by the processes of the Archive's scripts to either keep track of metadata, of information about the item, and so on. They are generally *.xml files, or thumbnails, or so on. In general, you only want the Original files as well as the metadata (from the *.xml files) to have the \"core\" of an item. This will save you a lot of disk space - the derived files can always be recreated later. So Anyway The best of the ways to download from Internet Archive is using the official client. I wrote an introduction to the IA client here: http://blog.archive.org/2019/06/05/the-ia-client-the-swiss-army-knife-of-internet-archive/ The direct link to the IA client is here: https://github.com/jjjake/internetarchive So, an initial experiment would be to download the entirety of a specific collection. To get a collection's items, do ia search collection:collection-name --itemlist Then, use ia download to download each individual item. You can do this with a script, and even do it in parallel. There's also the --retries command, in case systems hit load or other issues arise. (I advise checking the documentation and reading thoroughly - perhaps people can reply with recipes of what they have found. There are over 63,000,000 individual items at the Archive. Choose wisely. And good luck. Edit, Next Day: As is often the case when the Internet Archive's collections are discussed in this way, people are proposing the usual solutions, which I call the Big Three: Organize an ad-hoc/professional/simple/complicated shared storage scheme Go to a [corporate entity] and get some sort of discount/free service/hardware Send Over a Bunch of Hard Drives and Make a Copy I appreciate people giving thought to these solutions and will respond to them (or make new stand-along messages) in the thread. In the meantime, I will say that the Archive has endorsed and worked with a concept called The Distributed Web which has both included discussions and meetings as well as proposed technologies - at the very least, it's interesting and along the lines that people think of when they think of \"sharing\" the load. A FAQ: https://blog.archive.org/2018/07/21/decentralized-web-faq/"},
{"Title": "Who said they raise the price Before Black Friday?", "Author": "u/thenextbranson95", "Content": "No content"},
{"Title": "Donâ€™t lie, if they actually made it most of us would buy itâ€¦ RS-232 port and all.", "Author": "u/pdmcmahon", "Content": "No content"},
{"Title": "Let's Say You Wanted to Back Up The Internet Archive", "Author": "u/textfiles", "Content": "So, you think you want to back up the Internet Archive. This is a gargantuan project and not something to be taken lightly. Definitely consider why you think you need to do this, and what exactly you hope to have at the end. There's thousands of subcollections at the Archive and maybe you actually want a smaller set of it. These instructions work for those smaller sets and you'll get it much faster. Or you're just curious as to what it would take to get everything . Well, first, bear in mind there's different classes of material in the Archive's 50+ petabytes of data storage. There's material that can be downloaded, material that can only be viewed/streamed, and material that is used internally like the wayback machine or database storage. We'll set aside the 20+ petabytes of material under the wayback for the purpose of this discussion other than you can get websites by directly downloading and mirroring as you would any web page. That leaves the many collections and items you can reach directly. They tend to be in the form of https://archive.org/details/identifier where identifier is the \"item identifier\", more like a directory scattered among dozens and dozens of racks that hold the items. By default, these are completely open to downloads, unless they're set to be a variety of \"stream/sample\" settings, at which point, for the sake of this tutorial, can't be downloaded at all - just viewed. To see the directory version of an item, switch details to download , like archive.org/download/identifier - this will show you all the files residing for an item, both Original, System, and Derived. Let's talk about those three. Original files are what were uploaded into the identifier by the user or script. They are never modifier or touched by the system. Unless something goes wrong, what you download of an original file is exactly what was uploaded. Derived files are then created by the scripts and handlers within the archive to make them easier to interact with. For example, PDF files are \"derived\" into EPUBs, jpeg-sets, OCR'd textfiles, and so on. System files are created by the processes of the Archive's scripts to either keep track of metadata, of information about the item, and so on. They are generally *.xml files, or thumbnails, or so on. In general, you only want the Original files as well as the metadata (from the *.xml files) to have the \"core\" of an item. This will save you a lot of disk space - the derived files can always be recreated later. So Anyway The best of the ways to download from Internet Archive is using the official client. I wrote an introduction to the IA client here: http://blog.archive.org/2019/06/05/the-ia-client-the-swiss-army-knife-of-internet-archive/ The direct link to the IA client is here: https://github.com/jjjake/internetarchive So, an initial experiment would be to download the entirety of a specific collection. To get a collection's items, do ia search collection:collection-name --itemlist Then, use ia download to download each individual item. You can do this with a script, and even do it in parallel. There's also the --retries command, in case systems hit load or other issues arise. (I advise checking the documentation and reading thoroughly - perhaps people can reply with recipes of what they have found. There are over 63,000,000 individual items at the Archive. Choose wisely. And good luck. Edit, Next Day: As is often the case when the Internet Archive's collections are discussed in this way, people are proposing the usual solutions, which I call the Big Three: Organize an ad-hoc/professional/simple/complicated shared storage scheme Go to a [corporate entity] and get some sort of discount/free service/hardware Send Over a Bunch of Hard Drives and Make a Copy I appreciate people giving thought to these solutions and will respond to them (or make new stand-along messages) in the thread. In the meantime, I will say that the Archive has endorsed and worked with a concept called The Distributed Web which has both included discussions and meetings as well as proposed technologies - at the very least, it's interesting and along the lines that people think of when they think of \"sharing\" the load. A FAQ: https://blog.archive.org/2018/07/21/decentralized-web-faq/"},
{"Title": "Donâ€™t lie, if they actually made it most of us would buy itâ€¦ RS-232 port and all.", "Author": "u/pdmcmahon", "Content": "No content"},
{"Title": "PSA: It is unwise to 3D print your HDD holders out of PLA in this heatwave. Also, RAID is not a backup", "Author": "u/theartlav", "Content": "No content"},
{"Title": "SaveVideo bot is safe for now - the takedown notice was not sent by Reddit but an impersonator", "Author": "u/i_have_20_bucks", "Content": "No content"},
{"Title": "I just put these together at work. 48 7.68TB drives in 2 Dell 740xd servers with dual Xeon Platinum processors and 768GB of RAM each.", "Author": "u/Legionnaire1856", "Content": "No content"},
{"Title": "What happened to Pornhub is a sign of things to come. Be prepared for The Great Digitial Purge.", "Author": "u/Watchmen1986", "Content": "Transitional Justice is coming. Whether it is YouTube, instagram, facebook or whatever platform you are using, a wave of self-censorship is surging. Be smart enough to save things now. Like right now."},
{"Title": "Took a chance on buying 30 drives from eBay, USPS says they care, prognosis isn't good.", "Author": "u/the1337moderate", "Content": "No content"},
{"Title": "Wikia, known for deleting wikis that aren't active enough, has acquired gaming history including Gamefaqs, Gamespot, Metacritic and other sites - They are in critical danger of being purged", "Author": "u/CreationBlues", "Content": "No content"},
{"Title": "He gets it", "Author": "u/ThePandaMan1110", "Content": "No content"},
{"Title": "The entirety of Twitch has reportedly been leaked", "Author": "u/Megalan", "Content": "No content"},
{"Title": "Developer banned from Steam after using Steam Workshop of unreleased software as a porn stash. Which one of you did this?", "Author": "u/Metastasis3", "Content": "No content"},
{"Title": "Hard drives - from 5.25 inch to 1.0 inch", "Author": "u/HTWingNut", "Content": "No content"},
{"Title": "Home made, non GMO, cruelty free, off-site backup", "Author": "u/subtepass", "Content": "No content"},
{"Title": "Czkawka 3.1.0 - new version of my app to find duplicates, similar images, same music, broken files etc.", "Author": "u/krutkrutrar", "Content": "No content"},
{"Title": "My Dad and Iâ€™s second home NAS build with a total of 80tb and plenty of streaming capability.", "Author": "u/benmilek", "Content": "No content"},
{"Title": "I hit a bit of a milestone today", "Author": "u/Beaston02", "Content": "No content"},
{"Title": "Epic Games shuts down the Unreal Engine wiki, basically the only ressource for learning the C++ aspect of it, without any real warning", "Author": "u/WPLibrar2", "Content": "No content"},
{"Title": "this is my new nas please like and subscribe", "Author": "u/jesuswhathaveidone", "Content": "No content"},
{"Title": "Z-Library Website Is Alive Again", "Author": "u/vadhavaniyafaijan", "Content": "No content"},
{"Title": "Unboxed my new NAS server just now and sheâ€™s already sitting on it", "Author": "u/blueskyn01se", "Content": "No content"},
{"Title": "Fitted out my old case with room enough for 20 HDD's of hoarding!", "Author": "u/lkashl", "Content": "No content"},
{"Title": "iFixit now has a How to guide on shucking drives", "Author": "u/Glenta3924", "Content": "No content"},
{"Title": "This is why we exist - prime arguing you donâ€™t own what you pay for", "Author": "u/imajes", "Content": "No content"},
{"Title": "A terabyte isnâ€™t what it used to be - 14% of Internet customers use more", "Author": "u/It_Is1-24PM", "Content": "No content"},
{"Title": "Who said they raise the price Before Black Friday?", "Author": "u/thenextbranson95", "Content": "No content"},
{"Title": "Let's Say You Wanted to Back Up The Internet Archive", "Author": "u/textfiles", "Content": "So, you think you want to back up the Internet Archive. This is a gargantuan project and not something to be taken lightly. Definitely consider why you think you need to do this, and what exactly you hope to have at the end. There's thousands of subcollections at the Archive and maybe you actually want a smaller set of it. These instructions work for those smaller sets and you'll get it much faster. Or you're just curious as to what it would take to get everything . Well, first, bear in mind there's different classes of material in the Archive's 50+ petabytes of data storage. There's material that can be downloaded, material that can only be viewed/streamed, and material that is used internally like the wayback machine or database storage. We'll set aside the 20+ petabytes of material under the wayback for the purpose of this discussion other than you can get websites by directly downloading and mirroring as you would any web page. That leaves the many collections and items you can reach directly. They tend to be in the form of https://archive.org/details/identifier where identifier is the \"item identifier\", more like a directory scattered among dozens and dozens of racks that hold the items. By default, these are completely open to downloads, unless they're set to be a variety of \"stream/sample\" settings, at which point, for the sake of this tutorial, can't be downloaded at all - just viewed. To see the directory version of an item, switch details to download , like archive.org/download/identifier - this will show you all the files residing for an item, both Original, System, and Derived. Let's talk about those three. Original files are what were uploaded into the identifier by the user or script. They are never modifier or touched by the system. Unless something goes wrong, what you download of an original file is exactly what was uploaded. Derived files are then created by the scripts and handlers within the archive to make them easier to interact with. For example, PDF files are \"derived\" into EPUBs, jpeg-sets, OCR'd textfiles, and so on. System files are created by the processes of the Archive's scripts to either keep track of metadata, of information about the item, and so on. They are generally *.xml files, or thumbnails, or so on. In general, you only want the Original files as well as the metadata (from the *.xml files) to have the \"core\" of an item. This will save you a lot of disk space - the derived files can always be recreated later. So Anyway The best of the ways to download from Internet Archive is using the official client. I wrote an introduction to the IA client here: http://blog.archive.org/2019/06/05/the-ia-client-the-swiss-army-knife-of-internet-archive/ The direct link to the IA client is here: https://github.com/jjjake/internetarchive So, an initial experiment would be to download the entirety of a specific collection. To get a collection's items, do ia search collection:collection-name --itemlist Then, use ia download to download each individual item. You can do this with a script, and even do it in parallel. There's also the --retries command, in case systems hit load or other issues arise. (I advise checking the documentation and reading thoroughly - perhaps people can reply with recipes of what they have found. There are over 63,000,000 individual items at the Archive. Choose wisely. And good luck. Edit, Next Day: As is often the case when the Internet Archive's collections are discussed in this way, people are proposing the usual solutions, which I call the Big Three: Organize an ad-hoc/professional/simple/complicated shared storage scheme Go to a [corporate entity] and get some sort of discount/free service/hardware Send Over a Bunch of Hard Drives and Make a Copy I appreciate people giving thought to these solutions and will respond to them (or make new stand-along messages) in the thread. In the meantime, I will say that the Archive has endorsed and worked with a concept called The Distributed Web which has both included discussions and meetings as well as proposed technologies - at the very least, it's interesting and along the lines that people think of when they think of \"sharing\" the load. A FAQ: https://blog.archive.org/2018/07/21/decentralized-web-faq/"},
{"Title": "Donâ€™t lie, if they actually made it most of us would buy itâ€¦ RS-232 port and all.", "Author": "u/pdmcmahon", "Content": "No content"},
{"Title": "PSA: It is unwise to 3D print your HDD holders out of PLA in this heatwave. Also, RAID is not a backup", "Author": "u/theartlav", "Content": "No content"},
{"Title": "Good Databasing software for digital media (movies in particular)?", "Author": "u/sbourwest", "Content": "My Dad has a pretty extensive collection of movies (close to 6,000), and his only real method for cataloguing is a private streaming platform (Emby), but he's interested in creating a database for all of his content, but doesn't want to have to populate the whole thing himself, he'd rather be able to just type in the media and have it pull from online databases like IMDB or the like. He would prefer PC software, no mobile apps or online-only (have to register an account) platforms, browser-based is fine though. I've tried doing a bit of looking myself but most personal film cataloguing software is focused on physical media (DVDs, Blu-Rays, etc.) and not a digital collection."},
{"Title": "SaveVideo bot is safe for now - the takedown notice was not sent by Reddit but an impersonator", "Author": "u/i_have_20_bucks", "Content": "No content"},
{"Title": "Best software to automatically catalog / search your data", "Author": "u/ripperdoc", "Content": "New to this subreddit. For long believed in manual cataloguing but Iâ€™m starting to feel I donâ€™t have time for that. Whatâ€™s the go to software for automatically cataloguing data, from documents to websites to media?"},
{"Title": "I just put these together at work. 48 7.68TB drives in 2 Dell 740xd servers with dual Xeon Platinum processors and 768GB of RAM each.", "Author": "u/Legionnaire1856", "Content": "No content"},
{"Title": "Uploading files to a shared folder consumes my storage space?", "Author": "u/IseeYouLater", "Content": "Hello, as mentioned in the headline I am slightly confused. A friend shared a google drive folder with me to upload our vacation pics - I created a Subfolder in his shared folder and now when I upload my pics to the subfolder that I created, the files count against my storage limit. Which is bad cuz I am already very close to max. Is this intended? Everywhere on the net it says subfolders should count against the parent folder's storage limit. What am I missing here?"},
{"Title": "What happened to Pornhub is a sign of things to come. Be prepared for The Great Digitial Purge.", "Author": "u/Watchmen1986", "Content": "Transitional Justice is coming. Whether it is YouTube, instagram, facebook or whatever platform you are using, a wave of self-censorship is surging. Be smart enough to save things now. Like right now."},
{"Title": "How often does the pCloud discount $890 10TB lifetime come around? I need to move my GSuite data soon to a new place.", "Author": "u/n4n4n4n4n", "Content": "(I will be using cryptomator to mask the data)"},
{"Title": "Took a chance on buying 30 drives from eBay, USPS says they care, prognosis isn't good.", "Author": "u/the1337moderate", "Content": "No content"},
{"Title": "Multipath aggregation aware file copy software?", "Author": "u/pally_nid", "Content": "I barely know how to ask this question... Would anyone know of a file copying software that can handle a source/destination like a NAS with multiple 1gb network interfaces. Maybe its best if I describe the effect I am looking for. Start a copy job, reach the avg-peak of 100MB/s and then the software becomes aware of this and then continues to enumerate folders as subtasks and starts these folders on the other IP addresses available from the NAS. So, if the NAS had 2 gb/s NICs, both would be saturated at 100MB/s, rather than only one granting higher thorough-put. Thank you for reading."},
{"Title": "Wikia, known for deleting wikis that aren't active enough, has acquired gaming history including Gamefaqs, Gamespot, Metacritic and other sites - They are in critical danger of being purged", "Author": "u/CreationBlues", "Content": "No content"},
{"Title": "FLAC Server + Player that handles Ratings well, last played?", "Author": "u/th_teacher", "Content": "I believe nothing works well with \"standard\" tags, only stored in the database? I do not need multi-user handling. But I do want to be able to periodically export from the database and store the ratings in the FLAC tags for playlist creation hoping other servers/players will at least use them read-only Last played would be nice too, is played - count a thing?"},
{"Title": "He gets it", "Author": "u/ThePandaMan1110", "Content": "No content"},
{"Title": "How to backup an entire website from a few days ago?", "Author": "u/TheGoodSir1", "Content": "Since vimm.net got in trouble with Nintendo I can't get new games for my GBC (I have a flash cart) and I was wondering if it would be possible to back up the entirety of vimm.net to archive.org   or a personal file/files, I know you can save individual webpages but since in some sections of the website there is near 20,000 pages, it would take forever. Are there any ways too?"},
{"Title": "The entirety of Twitch has reportedly been leaked", "Author": "u/Megalan", "Content": "No content"},
{"Title": "using gallery-dl for downloading sub. only content in deviantart", "Author": "u/South-Order2046", "Content": "I want to download a gallery of a creator that I have been subscribed to. I first tried download them with using \"WFDownloaderApp\" but it only downloaded the public images and gave me an error while trying to download the private images (subscribers only). Then, I tried my chances with gallery-dl. First, I had tried without adding any .conf file, but it gave me \"API responded with 429 Too Many Requests.\" error.  It doesn't work with any amounts of delay. Next, I added the .conf file, put my own client_idÂ andÂ client_secret values into that file. But it still give me the same error. How can I solve this problem? Thank you all for your answers."},
{"Title": "Developer banned from Steam after using Steam Workshop of unreleased software as a porn stash. Which one of you did this?", "Author": "u/Metastasis3", "Content": "No content"},
{"Title": "Does anyone know how to look into .rpk files? (Related to EA games)", "Author": "u/EUOS_the_cat", "Content": "Let me know if this isn't the right sub for this, and if it isn't, point me in the right direction please. I'm on a mission to extract models from old Littlest Pet Shop games (PC, Wii), and unfortunately they were made by EA. Me and another person have looked into it, and the models are stored in the .big format. These have ways to be opened, however they lead to the .rpk files that have no information that either of us could find on how to get into. The only info about them I see is that they're used as skins for RadLight, which obviously isn't what these files are. If anyone knows a way to get into these files, I'd be very thankful."},
{"Title": "Hard drives - from 5.25 inch to 1.0 inch", "Author": "u/HTWingNut", "Content": "No content"},
{"Title": "CGSociety - Site Rip", "Author": "u/valdearg", "Content": "Completely forgot about this from https://old.reddit.com/r/DataHoarder/comments/18tlukv/cgsociety_is_closing_up_soon_decades_of_valuable/ https://archive.org/details/cgsocietyarchive It's about 300GB+ Images has JSON files with their metadata alongside it. Torrent needs to be regenerated I think, not sure if there's a way to do that."},
{"Title": "Home made, non GMO, cruelty free, off-site backup", "Author": "u/subtepass", "Content": "No content"},
{"Title": "How far does your email archive go back to?", "Author": "u/Commercial_Union_296", "Content": "My sent email archive goes back to 2013."},
{"Title": "Czkawka 3.1.0 - new version of my app to find duplicates, similar images, same music, broken files etc.", "Author": "u/krutkrutrar", "Content": "No content"},
{"Title": "Sorting tens of thousands of recovered video files by presence of audio?", "Author": "u/CyberpunkLover", "Content": "I've been collecting music video clips for about 15 years now, and collected something like 85k clips. I've been putting everything on hard drives, then after filling them up, moving everything to larger drive. The last drive I've used was some Western Digital Blue 8TB drive, and last week it filled up, so I purchased a new 20TB drive and was about to copy everything into it, but before I could do that, the WD drive failed. I've used few file restoration programs, and managed to salvage about 80% of everything that was on the drive, but the problem is, like 70% of what I've restored is either corrupted, or doesn't have audio. And all the files were put into ~75 different folders, with around 900 or so files each. I'd like to sort out the files with audio and without, in order to save the good ones and get rid of the bad ones. All files were renamed to random letters and numbers by recovery software, so the corrupted and muted files are basically completely useless to me, since I can't even use the file names to find out what files those are. All the files are either .mp4, .mkv, .avi, .mov or .wmv. Sorting through tens of thousands of files would take me months, and I just don't have the time or patience to do that. I've had the idea of importing files into video editing soft like Premiere and look at the generated waveform to find out good files,  but there's quite a lot of files with codecs unsupported by Premiere, like VP09 and such, so vast majority of files, even with audio present show up as flat lines on Premiere waveform, thus this method is completely useless to me. Like, importing a folder of restored files only generates maybe 20-30 waveforms, the rest are flat lines, and so manual sorting is still required. Anyone know of any software that can scan thousands of files and either mark or separate them by presence of audio, or have some other solution?"},
{"Title": "My Dad and Iâ€™s second home NAS build with a total of 80tb and plenty of streaming capability.", "Author": "u/benmilek", "Content": "No content"},
{"Title": "Hard Drive Formatting from Windows to Mac  Catch-22 Situation", "Author": "u/djensenteeken", "Content": "Hey everyone, so i have a situation i need help with. I recently switched over from my HP Omen laptop to a new MacBook Pro. Not knowing about the data formatting situation, I backed up all my data from my old laptop onto my Seagate Expansion Portable Drive 2TB. When using my MacBook, I noticed I couldn't upload files to this harddrive because the file formatting was different. I found on the internet that to switch the external harddrive from NTFS to APFS I can use Disk Utility to reformat the device. For this to happen however i have to backup the files that I have on my external harddrive, because to reformat it the harddrive has to be erased. The thing is, if i back-up the files to my old HP Omen, I can't reupload the files back to my external hard drive because it has been reformatted to APFS, and the files are NTFS. I also can't back-up the files to my new MacBook, because the size of my files is way to big for the internal storage of my Macbook! Is the only option for this really just to buy another external harddrive coded to APFS, or keep my files on my hard drive on my old dusty laptop that barely functions? This feels like a catch-22 situation to me. Sorry if this question has a) been answered already or b) is not relevant to this subreddit. Any help is appreciated!"},
{"Title": "I hit a bit of a milestone today", "Author": "u/Beaston02", "Content": "No content"},
{"Title": "NAS nvme build options", "Author": "u/AHappyGaijin", "Content": "Hello, i am currently building my first NAS using a Topton N100 board. Im currently deciding on which nvme to use as a system drive - currently leaning for a WD Blue SN580 2Tb. My question is if one should get 2 of the same drive to mirror the operating System? Using 2 nvme drives would limit the pcie expansion possibility of the board because the second nvme slot shares lanes with the nvme slot. Though currently i have no plans to use a pcie expansion card."},
{"Title": "Epic Games shuts down the Unreal Engine wiki, basically the only ressource for learning the C++ aspect of it, without any real warning", "Author": "u/WPLibrar2", "Content": "No content"},
{"Title": "NVMe SSD enclosure vs Portable external SSD?", "Author": "u/ProFalseIdol", "Content": "I am looking for at least 1TB to store videos and pictures. Travel a lot, so it has to be as convenient and compact as a Flash thumb drive. I saw plenty of recommendations of using NVMe + enclosure for portable consumer storage. Then I saw the SSK Portable options (while looking for their enclosure).. Also the Crucial X6.. https://www.amazon.com/SSK-Portable-External-MacBook-Laptops/dp/B0CL94LX9W?ref_=ast_sto_dp&th=1 What are the pros and cons? For 87 USD, you have a ready to use USB C or A portable solution. Got a new laptop with USB 4 and USB-C 3.2 Gen 2 ports (I assume I can get that sweet 2000 MB/s read/write).. Versus an M.2 NVMe + enclosure that I probably will never upgrade and will be permanently external as if I bought a Thumb drive.. TIA!"},
{"Title": "this is my new nas please like and subscribe", "Author": "u/jesuswhathaveidone", "Content": "No content"},
{"Title": "Is it worth archiving random pages aimlessly?", "Author": "u/peliciego", "Content": "Hi folks. I wonder if it is worth using the \"wayback machine\" and \"archive.ph\" addons for archiving random websites when you are navigating across the internet. Sometimes I focus on old websites (*index.html or other dork commands). Other times, it is local news in my surroundings. And from time to time, websites in minority languages. I don't save locally in large quantities. Do you think this strategy is worth it? Sometimes it is like planting a forest. You may never see the results in 100 years."},
{"Title": "Z-Library Website Is Alive Again", "Author": "u/vadhavaniyafaijan", "Content": "No content"},
{"Title": "Looking for a cloud backup service", "Author": "u/aradbe", "Content": "Hey all, so pretty simple, im looking for a cloud service where can i backup some of my important files so ill have them in case i lose them. i dont know much about this kinda stuff amd im looking for something simple and reliable, i was interested to see if thers a service where i can start off with purchasing a sub for a set amount of storage and if i want more later i can add a few dollars and expand the storage. like maybe start out with 100gb and expand it to 200gb if i need it later. If there nothing like then i would just like a recommendation for a good reliable backup service that will do the job. Thanks for any replies !"},
{"Title": "Unboxed my new NAS server just now and sheâ€™s already sitting on it", "Author": "u/blueskyn01se", "Content": "No content"},
{"Title": "What's your guys system for archiving entire youtube channels look like?", "Author": "u/glowcialist", "Content": "I tried TubeArchivist, and it worked great for maybe 500 vids, but now seems to be completely blocked, I'm assuming that any script utilizing yt-dlp would face similar issues? 4k tube seemed to work alright for individual playlists at first, but now seems to be consistently throttled to like 50 kb/s. Any ideas? Edit: Please forgive the missing apostrophe in the title. I won't do it again."},
{"Title": "Fitted out my old case with room enough for 20 HDD's of hoarding!", "Author": "u/lkashl", "Content": "No content"},
{"Title": "How do I download a certain video from the wayback machine?", "Author": "u/wiicrafttech", "Content": "This is the link I'm using https://web.archive.org/web/20230520032047/https://www.youtube.com/watch?v=Oy2c6Mt9KwY#"},
{"Title": "iFixit now has a How to guide on shucking drives", "Author": "u/Glenta3924", "Content": "No content"},
{"Title": "Looking for some critiquing of my plan.", "Author": "u/OnenonlyAl", "Content": "Thanks for taking the time to read and comment. I'm looking to backup a zpool of media. I had tried to setup a truenas for the data but I didn't love the UI and I have what I need on Ubuntu with docker. My plan is to destroy a zpool of the truenas made of 4x10tb raidz1 and move that pool into the primary server (add a pcie with more sata ports) then use zpool attach to mirror my current 4x8tb raidz1 (mnt) to the newly recreated zpool of the 4x10tb from the old truenas server (named mntbackup). So zpool attach mnt to mntbackup. Sounds like this automatically starts mirroring. I know this isn't ideal without being an off-site backup but I'm not the most literate at actually managing this and want something easy. Would I be able to just plug in backup, then unplug the backup pool and cold storage? I don't need perfect redundancy. Would I be able to plug the mntbackup raidz1 back in sometime in the future and the missing new data automatically sync or would I need scripts of rysnc or zfs send receive to add new data? Thanks for any insight and help!"},
{"Title": "This is why we exist - prime arguing you donâ€™t own what you pay for", "Author": "u/imajes", "Content": "No content"},
{"Title": "Suitcase for 3.5\" disks with screwed on HotSwap", "Author": "u/No-Balance-8038", "Content": "Need a way to protect my backup disks that are meant to be stored offsite. I already got cases, but they are too small to account for a 3.5\" HDD plus the mount for SilverStone Technology RM43-320-RS. What would you guys recommend? I live in Germany. From Amazon would be cool. I already have 3.5\" Protection cases but they are not long enough! The HDDs are 17cm long with it."},
{"Title": "A terabyte isnâ€™t what it used to be - 14% of Internet customers use more", "Author": "u/It_Is1-24PM", "Content": "No content"},
{"Title": "Software for finding all files with identical names on a drive", "Author": "u/MisterPenishead", "Content": "I have a hard drive with thousands of files spread across different folders and some of the files have the same name in spite of them having different content. Is there a software I can use to find all files with identical names so that I can rename them?"},
{"Title": "Who said they raise the price Before Black Friday?", "Author": "u/thenextbranson95", "Content": "No content"},
{"Title": "AWS deep freeze pricing", "Author": "u/KingRollos", "Content": "I'm thinking of moving most of my backup to Amazon's deep freeze service. I'm sure it's made complicated deliberately. I'm thinking of adding it in bits. Will this cost me more? Do I need to gather it all together and then transfer in 1 go?"},
{"Title": "Let's Say You Wanted to Back Up The Internet Archive", "Author": "u/textfiles", "Content": "So, you think you want to back up the Internet Archive. This is a gargantuan project and not something to be taken lightly. Definitely consider why you think you need to do this, and what exactly you hope to have at the end. There's thousands of subcollections at the Archive and maybe you actually want a smaller set of it. These instructions work for those smaller sets and you'll get it much faster. Or you're just curious as to what it would take to get everything . Well, first, bear in mind there's different classes of material in the Archive's 50+ petabytes of data storage. There's material that can be downloaded, material that can only be viewed/streamed, and material that is used internally like the wayback machine or database storage. We'll set aside the 20+ petabytes of material under the wayback for the purpose of this discussion other than you can get websites by directly downloading and mirroring as you would any web page. That leaves the many collections and items you can reach directly. They tend to be in the form of https://archive.org/details/identifier where identifier is the \"item identifier\", more like a directory scattered among dozens and dozens of racks that hold the items. By default, these are completely open to downloads, unless they're set to be a variety of \"stream/sample\" settings, at which point, for the sake of this tutorial, can't be downloaded at all - just viewed. To see the directory version of an item, switch details to download , like archive.org/download/identifier - this will show you all the files residing for an item, both Original, System, and Derived. Let's talk about those three. Original files are what were uploaded into the identifier by the user or script. They are never modifier or touched by the system. Unless something goes wrong, what you download of an original file is exactly what was uploaded. Derived files are then created by the scripts and handlers within the archive to make them easier to interact with. For example, PDF files are \"derived\" into EPUBs, jpeg-sets, OCR'd textfiles, and so on. System files are created by the processes of the Archive's scripts to either keep track of metadata, of information about the item, and so on. They are generally *.xml files, or thumbnails, or so on. In general, you only want the Original files as well as the metadata (from the *.xml files) to have the \"core\" of an item. This will save you a lot of disk space - the derived files can always be recreated later. So Anyway The best of the ways to download from Internet Archive is using the official client. I wrote an introduction to the IA client here: http://blog.archive.org/2019/06/05/the-ia-client-the-swiss-army-knife-of-internet-archive/ The direct link to the IA client is here: https://github.com/jjjake/internetarchive So, an initial experiment would be to download the entirety of a specific collection. To get a collection's items, do ia search collection:collection-name --itemlist Then, use ia download to download each individual item. You can do this with a script, and even do it in parallel. There's also the --retries command, in case systems hit load or other issues arise. (I advise checking the documentation and reading thoroughly - perhaps people can reply with recipes of what they have found. There are over 63,000,000 individual items at the Archive. Choose wisely. And good luck. Edit, Next Day: As is often the case when the Internet Archive's collections are discussed in this way, people are proposing the usual solutions, which I call the Big Three: Organize an ad-hoc/professional/simple/complicated shared storage scheme Go to a [corporate entity] and get some sort of discount/free service/hardware Send Over a Bunch of Hard Drives and Make a Copy I appreciate people giving thought to these solutions and will respond to them (or make new stand-along messages) in the thread. In the meantime, I will say that the Archive has endorsed and worked with a concept called The Distributed Web which has both included discussions and meetings as well as proposed technologies - at the very least, it's interesting and along the lines that people think of when they think of \"sharing\" the load. A FAQ: https://blog.archive.org/2018/07/21/decentralized-web-faq/"},
{"Title": "Im looking to transfer highest quality from Hi-Fi camera for 8mm Tapes.  Which is better: Mini-USB or FireWire?", "Author": "u/Throwaway173638o", "Content": "I have a Sony Hi-Fi camera for 8mm videos that has a FireWire, Mini USB, A/V, and S-video ports.  I want to get the highest quality as possible.  I narrowed down to FireWire or Mini USB transfer for better quality. Which of the two is better overall or are they the same? Edit: For context, the camera is a Sony DCR-TRV730 and the tapes are Sony Hi8 MP 8mm Video Cassette.  Cassette also lists it as Digital8 with \"60\" while the Hi8 is listed as \"120\""},
{"Title": "Donâ€™t lie, if they actually made it most of us would buy itâ€¦ RS-232 port and all.", "Author": "u/pdmcmahon", "Content": "No content"},
{"Title": "Sort backup raid 1", "Author": "u/sweetestpeach94", "Content": "Hi, I hope I can get some help. I have a Lacie 2big thunderbolt 2 set as Raid 1. Iâ€™ve just completed the backup of almost 6TB of data (mainly pics), however half of it are just the mirror copies. Now I would move the original half on another disk, the problem is that the backup isnâ€™t like sort out in different folders, itâ€™s just every file and its copy back to back in sequence all together (file A, file A-2 , file B, file B-2, file C, file C - 2 etc.). So here is my question, how I select just one half of the files to move it in the new disk without their duplicate?"},
{"Title": "PSA: It is unwise to 3D print your HDD holders out of PLA in this heatwave. Also, RAID is not a backup", "Author": "u/theartlav", "Content": "No content"},
{"Title": "SaveVideo bot is safe for now - the takedown notice was not sent by Reddit but an impersonator", "Author": "u/i_have_20_bucks", "Content": "No content"},
{"Title": "I just put these together at work. 48 7.68TB drives in 2 Dell 740xd servers with dual Xeon Platinum processors and 768GB of RAM each.", "Author": "u/Legionnaire1856", "Content": "No content"},
{"Title": "What happened to Pornhub is a sign of things to come. Be prepared for The Great Digitial Purge.", "Author": "u/Watchmen1986", "Content": "Transitional Justice is coming. Whether it is YouTube, instagram, facebook or whatever platform you are using, a wave of self-censorship is surging. Be smart enough to save things now. Like right now."},
{"Title": "Took a chance on buying 30 drives from eBay, USPS says they care, prognosis isn't good.", "Author": "u/the1337moderate", "Content": "No content"},
{"Title": "Wikia, known for deleting wikis that aren't active enough, has acquired gaming history including Gamefaqs, Gamespot, Metacritic and other sites - They are in critical danger of being purged", "Author": "u/CreationBlues", "Content": "No content"},
{"Title": "He gets it", "Author": "u/ThePandaMan1110", "Content": "No content"},
{"Title": "The entirety of Twitch has reportedly been leaked", "Author": "u/Megalan", "Content": "No content"},
{"Title": "Developer banned from Steam after using Steam Workshop of unreleased software as a porn stash. Which one of you did this?", "Author": "u/Metastasis3", "Content": "No content"},
{"Title": "Hard drives - from 5.25 inch to 1.0 inch", "Author": "u/HTWingNut", "Content": "No content"},
{"Title": "Home made, non GMO, cruelty free, off-site backup", "Author": "u/subtepass", "Content": "No content"},
{"Title": "Czkawka 3.1.0 - new version of my app to find duplicates, similar images, same music, broken files etc.", "Author": "u/krutkrutrar", "Content": "No content"},
{"Title": "My Dad and Iâ€™s second home NAS build with a total of 80tb and plenty of streaming capability.", "Author": "u/benmilek", "Content": "No content"},
{"Title": "I hit a bit of a milestone today", "Author": "u/Beaston02", "Content": "No content"},
{"Title": "Epic Games shuts down the Unreal Engine wiki, basically the only ressource for learning the C++ aspect of it, without any real warning", "Author": "u/WPLibrar2", "Content": "No content"},
{"Title": "this is my new nas please like and subscribe", "Author": "u/jesuswhathaveidone", "Content": "No content"},
{"Title": "Z-Library Website Is Alive Again", "Author": "u/vadhavaniyafaijan", "Content": "No content"},
{"Title": "Unboxed my new NAS server just now and sheâ€™s already sitting on it", "Author": "u/blueskyn01se", "Content": "No content"},
{"Title": "Fitted out my old case with room enough for 20 HDD's of hoarding!", "Author": "u/lkashl", "Content": "No content"},
{"Title": "iFixit now has a How to guide on shucking drives", "Author": "u/Glenta3924", "Content": "No content"},
{"Title": "This is why we exist - prime arguing you donâ€™t own what you pay for", "Author": "u/imajes", "Content": "No content"},
{"Title": "A terabyte isnâ€™t what it used to be - 14% of Internet customers use more", "Author": "u/It_Is1-24PM", "Content": "No content"},
{"Title": "Who said they raise the price Before Black Friday?", "Author": "u/thenextbranson95", "Content": "No content"},
{"Title": "Let's Say You Wanted to Back Up The Internet Archive", "Author": "u/textfiles", "Content": "So, you think you want to back up the Internet Archive. This is a gargantuan project and not something to be taken lightly. Definitely consider why you think you need to do this, and what exactly you hope to have at the end. There's thousands of subcollections at the Archive and maybe you actually want a smaller set of it. These instructions work for those smaller sets and you'll get it much faster. Or you're just curious as to what it would take to get everything . Well, first, bear in mind there's different classes of material in the Archive's 50+ petabytes of data storage. There's material that can be downloaded, material that can only be viewed/streamed, and material that is used internally like the wayback machine or database storage. We'll set aside the 20+ petabytes of material under the wayback for the purpose of this discussion other than you can get websites by directly downloading and mirroring as you would any web page. That leaves the many collections and items you can reach directly. They tend to be in the form of https://archive.org/details/identifier where identifier is the \"item identifier\", more like a directory scattered among dozens and dozens of racks that hold the items. By default, these are completely open to downloads, unless they're set to be a variety of \"stream/sample\" settings, at which point, for the sake of this tutorial, can't be downloaded at all - just viewed. To see the directory version of an item, switch details to download , like archive.org/download/identifier - this will show you all the files residing for an item, both Original, System, and Derived. Let's talk about those three. Original files are what were uploaded into the identifier by the user or script. They are never modifier or touched by the system. Unless something goes wrong, what you download of an original file is exactly what was uploaded. Derived files are then created by the scripts and handlers within the archive to make them easier to interact with. For example, PDF files are \"derived\" into EPUBs, jpeg-sets, OCR'd textfiles, and so on. System files are created by the processes of the Archive's scripts to either keep track of metadata, of information about the item, and so on. They are generally *.xml files, or thumbnails, or so on. In general, you only want the Original files as well as the metadata (from the *.xml files) to have the \"core\" of an item. This will save you a lot of disk space - the derived files can always be recreated later. So Anyway The best of the ways to download from Internet Archive is using the official client. I wrote an introduction to the IA client here: http://blog.archive.org/2019/06/05/the-ia-client-the-swiss-army-knife-of-internet-archive/ The direct link to the IA client is here: https://github.com/jjjake/internetarchive So, an initial experiment would be to download the entirety of a specific collection. To get a collection's items, do ia search collection:collection-name --itemlist Then, use ia download to download each individual item. You can do this with a script, and even do it in parallel. There's also the --retries command, in case systems hit load or other issues arise. (I advise checking the documentation and reading thoroughly - perhaps people can reply with recipes of what they have found. There are over 63,000,000 individual items at the Archive. Choose wisely. And good luck. Edit, Next Day: As is often the case when the Internet Archive's collections are discussed in this way, people are proposing the usual solutions, which I call the Big Three: Organize an ad-hoc/professional/simple/complicated shared storage scheme Go to a [corporate entity] and get some sort of discount/free service/hardware Send Over a Bunch of Hard Drives and Make a Copy I appreciate people giving thought to these solutions and will respond to them (or make new stand-along messages) in the thread. In the meantime, I will say that the Archive has endorsed and worked with a concept called The Distributed Web which has both included discussions and meetings as well as proposed technologies - at the very least, it's interesting and along the lines that people think of when they think of \"sharing\" the load. A FAQ: https://blog.archive.org/2018/07/21/decentralized-web-faq/"},
{"Title": "Donâ€™t lie, if they actually made it most of us would buy itâ€¦ RS-232 port and all.", "Author": "u/pdmcmahon", "Content": "No content"},
{"Title": "Some datahoarder porn :) one of my archive projects with a customer who ditched the tape towards disk/cloud archiving. 4PB here. Each orange jbod is 60 bay top loader", "Author": "u/kumits-u", "Content": "No content"},
{"Title": "ISOs are nice but sometimes you need to hoard the originals for the complete experience. (And also rip them to ISO)", "Author": "u/AshleyUncia", "Content": "No content"},
{"Title": "I collect countless documents and images and organize them by year. So satisfying.", "Author": "u/dirtypark", "Content": "No content"},
{"Title": "My offsite backup!", "Author": "u/pairofcrocs", "Content": "No content"},
{"Title": "X (formerly knows as Twitter) purged all media from posts from before 2014", "Author": "u/coasterghost", "Content": "No content"},
{"Title": "Internet forums are disappearing because now it's all Reddit and Discord. And that's worrying.", "Author": "u/Run_the_Line", "Content": "No content"},
{"Title": "Just found nearly every Playboy from January 1976 to September 2001 at the recycling drop", "Author": "u/frazzleb420", "Content": "No content"},
{"Title": "I Blame Each and Every One of You....", "Author": "u/MeatballB", "Content": "No content"},
{"Title": "Sad day at Warner Brothers", "Author": "u/imajes", "Content": "No content"},
{"Title": "My data hoarding project: scanning 15 years worth of journals & converting to PDF for archive and safekeeping", "Author": "u/thebilljim", "Content": "No content"},
{"Title": "Thanks Amazon I love dents", "Author": "u/ralphte", "Content": "No content"},
{"Title": "Lovely machine for digitalizing books", "Author": "u/Zloty_Diament", "Content": "No content"},
{"Title": "YouTube Vanced: speculation that profiting of the project with NFTs is what triggered the cease and desist", "Author": "u/EpsilonBlight", "Content": "https://arstechnica.com/gadgets/2022/03/google-shuts-down-youtube-vanced-a-popular-ad-blocking-android-app/ Just last month, Team Vanced pulled a provocative stunt involving minting a non-fungible token of the Vanced logo, and there's solid speculation that this action is what drew Google's ire. Google mostly tends to leave the Android modding community alone, but profiting off your legally dubious mod is sure to bring out the lawyers. Once again crypto is why we can't have nice things."},
{"Title": "Contrary to many posts here, at least second hand sellers know how pack things.", "Author": "u/Matti_Meikalainen", "Content": "No content"},
{"Title": "Looks like Amazon is pulling the plug on unlimited cloud storage.", "Author": "Unknown author", "Content": "No content"},
{"Title": "Do the data backup!", "Author": "u/Sarke1", "Content": "No content"},
{"Title": "Article: â€œ10 everyday things that will vanish in the next 10 yearsâ€... I wonder what they think cloud providers use to store all that data.", "Author": "u/Sp00ky777", "Content": "No content"},
{"Title": "Library Genesis Project update: 2.5 million books seeded with the world, 80 million scientific articles next", "Author": "u/shrine", "Content": "For the latest updates on the Library Genesis Seeding Project join r/libgen and r/scihub Last month volunteers on r/seedboxes , r/datahoarder , across reddit, and around the world joined together to secure and preserve 2.5 million scientific books for humanity- for students, for doctors, for scientists, for future generations. The outpour of support for the project still leaves me in total awe. Thousands of people around the world joined our seeding effort donating bandwidth, storage, and expertise. Today we announce that the final set of 1,000 books is now seeded, saved, and preserved. Stunning generosity and heart. But our volunteers couldnâ€™t stop at books. We have already started to secure and preserve a new library of 80 million scientific articles. And now thanks to the brave librarians at Library Genesis and SciHub and all the volunteer seeders the collections can never be taken away from humanity. Why are Library Genesis and SciHub vital to humanity? Library Genesis and SciHub set out to share every scientific article and every scientific book with every single person on Earth. Their initiative fulfills United Nations/UNESCO world development goals that mandate the removal of restrictions on access to science. Big publishing companies just want â€œopen access,â€ representing only about 28% of articles, and no books. They want the rest of humanityâ€™s accumulated scientific knowledge to remain locked up behind paywalled databases and unaffordable textbooks. We said fuck that. Limiting and delaying humanityâ€™s access to science isnâ€™t a business, itâ€™s a crime, one with an untold number of victims and preventable deaths. Doctors and scientists in the developing world already face unbelievable challenges in their jobs. Tearing down paywalls between them and the knowledge they need to fight for health and freedom in their homeland is the least we can do to help. How can I help? Redditâ€™s support has been huge. In December the projectâ€™s story was published in Vice, receiving 60,000 upvotes across r/technology , r/futurology , r/datahoarder , and r/seedboxes , and shared to readers around the world in international technology news. Thatâ€™s just for seeding the torrents! Imagine the stories of knowledge brought to doctors and scientists and students around the world. They hold an incredible story to tell. We need their stories next, and we can bring the crisis of access to knowledge into view with our upvotes. Our seeding project has been an incredible success thanks to literal 24/7 work of our volunteers over the last month. Seedbox.io and their provider NFOrce.nl donated a dedicated high-speed server to seed the full Library Genesis book collection. The-Eye.eu is both seeding and archiving the entirety of both library collections. Youâ€™re also welcome to join The-Eye.euâ€™s discord to learn how you can help seed (discord.gg/the-eye #books). Programmers are needed to help re-envision the web frontend, search engine, or distribution model ( https://gitlab.com/libgen1 ). The entirety of Library Genesis is open-source, so anyone is welcome to reimagine the project. Here's what else our communities accomplished in technical details: Swarm peers increased from 3,000 seeders to 30,000 seeders! Swarm speeds increased from about 60KB/s on most torrents to over 100MB/s, thanks to the joint Seedbox.io and NFOrce.nl dedicated server and everyone else seeding. Refreshed and indexed 2,400 .torrent files, replacing 100+ dead trackers with new, live announce URLs The-Eye.eu began to prepare and hash-check the collection for archiving, more to come on that (TBA) Endless thanks to everyone at the-eye.eu , all the volunteers, Seedbox.io/NFOrce.nl, and UltraSeedbox for coming together to make this project happen. We brought science around the world with our torrenting, one of the many big steps in permanently unchaining and preserving all of this knowledge for humanity. https://preview.redd.it/coz7hvkh3s541.png Relevant Links https://phillm.net/libgen-seeds-needed.php https://phillm.net/libgen-stats-table.php \"Archivists Are Trying to Make Sure a â€˜Pirate Bay of Scienceâ€™ Never Goes Down\" by Matthew Gault in Vice News TorrentFreak's coverage by Andy /r/DataHoarder: Let's talk about datahoarding that's actually important: distributing knowledge and the role of Libgen in educating the developing world. /r/Seedboxes Charity Drive /r/Seedboxes Update"},
{"Title": "Here's a simple 7 bay CD/DVD ripping machine I just made. Works great! Time to rip 2100 CDs and 300 DVDs", "Author": "u/TVSKS", "Content": "No content"},
{"Title": "Video Archival Rack - ~50TB of Uncompressed Video Digitized (and Counting)!", "Author": "u/VincentVazzo", "Content": "No content"},
{"Title": "Bought 2 old CCTV DVR units for $25, each one had a WD Purple 4tb Drive", "Author": "u/nickdrones", "Content": "No content"},
{"Title": "Just imagine what it would be like if it were still this size... An IBM 5MB hard drive back in 1956.", "Author": "u/TinderSubThrowAway", "Content": "No content"},
{"Title": "Data hoarder Flash Drive style", "Author": "u/forlotto", "Content": "No content"},
{"Title": "Stopped at a different Costco than normal and I may have found the holy grail. St Louis Park MN.", "Author": "u/twonuh", "Content": "No content"},
{"Title": "Data-mining reveals that 80% of books published 1924-63 never had their copyrights renewed and are now in the public domain", "Author": "u/no-mad", "Content": "No content"},
{"Title": "Some datahoarder porn :) one of my archive projects with a customer who ditched the tape towards disk/cloud archiving. 4PB here. Each orange jbod is 60 bay top loader", "Author": "u/kumits-u", "Content": "No content"},
{"Title": "Good Databasing software for digital media (movies in particular)?", "Author": "u/sbourwest", "Content": "My Dad has a pretty extensive collection of movies (close to 6,000), and his only real method for cataloguing is a private streaming platform (Emby), but he's interested in creating a database for all of his content, but doesn't want to have to populate the whole thing himself, he'd rather be able to just type in the media and have it pull from online databases like IMDB or the like. He would prefer PC software, no mobile apps or online-only (have to register an account) platforms, browser-based is fine though. I've tried doing a bit of looking myself but most personal film cataloguing software is focused on physical media (DVDs, Blu-Rays, etc.) and not a digital collection."},
{"Title": "ISOs are nice but sometimes you need to hoard the originals for the complete experience. (And also rip them to ISO)", "Author": "u/AshleyUncia", "Content": "No content"},
{"Title": "Best software to automatically catalog / search your data", "Author": "u/ripperdoc", "Content": "New to this subreddit. For long believed in manual cataloguing but Iâ€™m starting to feel I donâ€™t have time for that. Whatâ€™s the go to software for automatically cataloguing data, from documents to websites to media?"},
{"Title": "I collect countless documents and images and organize them by year. So satisfying.", "Author": "u/dirtypark", "Content": "No content"},
{"Title": "Uploading files to a shared folder consumes my storage space?", "Author": "u/IseeYouLater", "Content": "Hello, as mentioned in the headline I am slightly confused. A friend shared a google drive folder with me to upload our vacation pics - I created a Subfolder in his shared folder and now when I upload my pics to the subfolder that I created, the files count against my storage limit. Which is bad cuz I am already very close to max. Is this intended? Everywhere on the net it says subfolders should count against the parent folder's storage limit. What am I missing here?"},
{"Title": "My offsite backup!", "Author": "u/pairofcrocs", "Content": "No content"},
{"Title": "How often does the pCloud discount $890 10TB lifetime come around? I need to move my GSuite data soon to a new place.", "Author": "u/n4n4n4n4n", "Content": "(I will be using cryptomator to mask the data)"},
{"Title": "X (formerly knows as Twitter) purged all media from posts from before 2014", "Author": "u/coasterghost", "Content": "No content"},
{"Title": "Multipath aggregation aware file copy software?", "Author": "u/pally_nid", "Content": "I barely know how to ask this question... Would anyone know of a file copying software that can handle a source/destination like a NAS with multiple 1gb network interfaces. Maybe its best if I describe the effect I am looking for. Start a copy job, reach the avg-peak of 100MB/s and then the software becomes aware of this and then continues to enumerate folders as subtasks and starts these folders on the other IP addresses available from the NAS. So, if the NAS had 2 gb/s NICs, both would be saturated at 100MB/s, rather than only one granting higher thorough-put. Thank you for reading."},
{"Title": "Internet forums are disappearing because now it's all Reddit and Discord. And that's worrying.", "Author": "u/Run_the_Line", "Content": "No content"},
{"Title": "FLAC Server + Player that handles Ratings well, last played?", "Author": "u/th_teacher", "Content": "I believe nothing works well with \"standard\" tags, only stored in the database? I do not need multi-user handling. But I do want to be able to periodically export from the database and store the ratings in the FLAC tags for playlist creation hoping other servers/players will at least use them read-only Last played would be nice too, is played - count a thing?"},
{"Title": "Just found nearly every Playboy from January 1976 to September 2001 at the recycling drop", "Author": "u/frazzleb420", "Content": "No content"},
{"Title": "How to backup an entire website from a few days ago?", "Author": "u/TheGoodSir1", "Content": "Since vimm.net got in trouble with Nintendo I can't get new games for my GBC (I have a flash cart) and I was wondering if it would be possible to back up the entirety of vimm.net to archive.org   or a personal file/files, I know you can save individual webpages but since in some sections of the website there is near 20,000 pages, it would take forever. Are there any ways too?"},
{"Title": "I Blame Each and Every One of You....", "Author": "u/MeatballB", "Content": "No content"},
{"Title": "using gallery-dl for downloading sub. only content in deviantart", "Author": "u/South-Order2046", "Content": "I want to download a gallery of a creator that I have been subscribed to. I first tried download them with using \"WFDownloaderApp\" but it only downloaded the public images and gave me an error while trying to download the private images (subscribers only). Then, I tried my chances with gallery-dl. First, I had tried without adding any .conf file, but it gave me \"API responded with 429 Too Many Requests.\" error.  It doesn't work with any amounts of delay. Next, I added the .conf file, put my own client_idÂ andÂ client_secret values into that file. But it still give me the same error. How can I solve this problem? Thank you all for your answers."},
{"Title": "Sad day at Warner Brothers", "Author": "u/imajes", "Content": "No content"},
{"Title": "Does anyone know how to look into .rpk files? (Related to EA games)", "Author": "u/EUOS_the_cat", "Content": "Let me know if this isn't the right sub for this, and if it isn't, point me in the right direction please. I'm on a mission to extract models from old Littlest Pet Shop games (PC, Wii), and unfortunately they were made by EA. Me and another person have looked into it, and the models are stored in the .big format. These have ways to be opened, however they lead to the .rpk files that have no information that either of us could find on how to get into. The only info about them I see is that they're used as skins for RadLight, which obviously isn't what these files are. If anyone knows a way to get into these files, I'd be very thankful."},
{"Title": "My data hoarding project: scanning 15 years worth of journals & converting to PDF for archive and safekeeping", "Author": "u/thebilljim", "Content": "No content"},
{"Title": "CGSociety - Site Rip", "Author": "u/valdearg", "Content": "Completely forgot about this from https://old.reddit.com/r/DataHoarder/comments/18tlukv/cgsociety_is_closing_up_soon_decades_of_valuable/ https://archive.org/details/cgsocietyarchive It's about 300GB+ Images has JSON files with their metadata alongside it. Torrent needs to be regenerated I think, not sure if there's a way to do that."},
{"Title": "Thanks Amazon I love dents", "Author": "u/ralphte", "Content": "No content"},
{"Title": "How far does your email archive go back to?", "Author": "u/Commercial_Union_296", "Content": "My sent email archive goes back to 2013."},
{"Title": "Lovely machine for digitalizing books", "Author": "u/Zloty_Diament", "Content": "No content"},
{"Title": "Sorting tens of thousands of recovered video files by presence of audio?", "Author": "u/CyberpunkLover", "Content": "I've been collecting music video clips for about 15 years now, and collected something like 85k clips. I've been putting everything on hard drives, then after filling them up, moving everything to larger drive. The last drive I've used was some Western Digital Blue 8TB drive, and last week it filled up, so I purchased a new 20TB drive and was about to copy everything into it, but before I could do that, the WD drive failed. I've used few file restoration programs, and managed to salvage about 80% of everything that was on the drive, but the problem is, like 70% of what I've restored is either corrupted, or doesn't have audio. And all the files were put into ~75 different folders, with around 900 or so files each. I'd like to sort out the files with audio and without, in order to save the good ones and get rid of the bad ones. All files were renamed to random letters and numbers by recovery software, so the corrupted and muted files are basically completely useless to me, since I can't even use the file names to find out what files those are. All the files are either .mp4, .mkv, .avi, .mov or .wmv. Sorting through tens of thousands of files would take me months, and I just don't have the time or patience to do that. I've had the idea of importing files into video editing soft like Premiere and look at the generated waveform to find out good files,  but there's quite a lot of files with codecs unsupported by Premiere, like VP09 and such, so vast majority of files, even with audio present show up as flat lines on Premiere waveform, thus this method is completely useless to me. Like, importing a folder of restored files only generates maybe 20-30 waveforms, the rest are flat lines, and so manual sorting is still required. Anyone know of any software that can scan thousands of files and either mark or separate them by presence of audio, or have some other solution?"},
{"Title": "YouTube Vanced: speculation that profiting of the project with NFTs is what triggered the cease and desist", "Author": "u/EpsilonBlight", "Content": "https://arstechnica.com/gadgets/2022/03/google-shuts-down-youtube-vanced-a-popular-ad-blocking-android-app/ Just last month, Team Vanced pulled a provocative stunt involving minting a non-fungible token of the Vanced logo, and there's solid speculation that this action is what drew Google's ire. Google mostly tends to leave the Android modding community alone, but profiting off your legally dubious mod is sure to bring out the lawyers. Once again crypto is why we can't have nice things."},
{"Title": "Hard Drive Formatting from Windows to Mac  Catch-22 Situation", "Author": "u/djensenteeken", "Content": "Hey everyone, so i have a situation i need help with. I recently switched over from my HP Omen laptop to a new MacBook Pro. Not knowing about the data formatting situation, I backed up all my data from my old laptop onto my Seagate Expansion Portable Drive 2TB. When using my MacBook, I noticed I couldn't upload files to this harddrive because the file formatting was different. I found on the internet that to switch the external harddrive from NTFS to APFS I can use Disk Utility to reformat the device. For this to happen however i have to backup the files that I have on my external harddrive, because to reformat it the harddrive has to be erased. The thing is, if i back-up the files to my old HP Omen, I can't reupload the files back to my external hard drive because it has been reformatted to APFS, and the files are NTFS. I also can't back-up the files to my new MacBook, because the size of my files is way to big for the internal storage of my Macbook! Is the only option for this really just to buy another external harddrive coded to APFS, or keep my files on my hard drive on my old dusty laptop that barely functions? This feels like a catch-22 situation to me. Sorry if this question has a) been answered already or b) is not relevant to this subreddit. Any help is appreciated!"},
{"Title": "Contrary to many posts here, at least second hand sellers know how pack things.", "Author": "u/Matti_Meikalainen", "Content": "No content"},
{"Title": "NAS nvme build options", "Author": "u/AHappyGaijin", "Content": "Hello, i am currently building my first NAS using a Topton N100 board. Im currently deciding on which nvme to use as a system drive - currently leaning for a WD Blue SN580 2Tb. My question is if one should get 2 of the same drive to mirror the operating System? Using 2 nvme drives would limit the pcie expansion possibility of the board because the second nvme slot shares lanes with the nvme slot. Though currently i have no plans to use a pcie expansion card."},
{"Title": "Looks like Amazon is pulling the plug on unlimited cloud storage.", "Author": "Unknown author", "Content": "No content"},
{"Title": "NVMe SSD enclosure vs Portable external SSD?", "Author": "u/ProFalseIdol", "Content": "I am looking for at least 1TB to store videos and pictures. Travel a lot, so it has to be as convenient and compact as a Flash thumb drive. I saw plenty of recommendations of using NVMe + enclosure for portable consumer storage. Then I saw the SSK Portable options (while looking for their enclosure).. Also the Crucial X6.. https://www.amazon.com/SSK-Portable-External-MacBook-Laptops/dp/B0CL94LX9W?ref_=ast_sto_dp&th=1 What are the pros and cons? For 87 USD, you have a ready to use USB C or A portable solution. Got a new laptop with USB 4 and USB-C 3.2 Gen 2 ports (I assume I can get that sweet 2000 MB/s read/write).. Versus an M.2 NVMe + enclosure that I probably will never upgrade and will be permanently external as if I bought a Thumb drive.. TIA!"},
{"Title": "Do the data backup!", "Author": "u/Sarke1", "Content": "No content"},
{"Title": "Is it worth archiving random pages aimlessly?", "Author": "u/peliciego", "Content": "Hi folks. I wonder if it is worth using the \"wayback machine\" and \"archive.ph\" addons for archiving random websites when you are navigating across the internet. Sometimes I focus on old websites (*index.html or other dork commands). Other times, it is local news in my surroundings. And from time to time, websites in minority languages. I don't save locally in large quantities. Do you think this strategy is worth it? Sometimes it is like planting a forest. You may never see the results in 100 years."},
{"Title": "Article: â€œ10 everyday things that will vanish in the next 10 yearsâ€... I wonder what they think cloud providers use to store all that data.", "Author": "u/Sp00ky777", "Content": "No content"},
{"Title": "Looking for a cloud backup service", "Author": "u/aradbe", "Content": "Hey all, so pretty simple, im looking for a cloud service where can i backup some of my important files so ill have them in case i lose them. i dont know much about this kinda stuff amd im looking for something simple and reliable, i was interested to see if thers a service where i can start off with purchasing a sub for a set amount of storage and if i want more later i can add a few dollars and expand the storage. like maybe start out with 100gb and expand it to 200gb if i need it later. If there nothing like then i would just like a recommendation for a good reliable backup service that will do the job. Thanks for any replies !"},
{"Title": "Library Genesis Project update: 2.5 million books seeded with the world, 80 million scientific articles next", "Author": "u/shrine", "Content": "For the latest updates on the Library Genesis Seeding Project join r/libgen and r/scihub Last month volunteers on r/seedboxes , r/datahoarder , across reddit, and around the world joined together to secure and preserve 2.5 million scientific books for humanity- for students, for doctors, for scientists, for future generations. The outpour of support for the project still leaves me in total awe. Thousands of people around the world joined our seeding effort donating bandwidth, storage, and expertise. Today we announce that the final set of 1,000 books is now seeded, saved, and preserved. Stunning generosity and heart. But our volunteers couldnâ€™t stop at books. We have already started to secure and preserve a new library of 80 million scientific articles. And now thanks to the brave librarians at Library Genesis and SciHub and all the volunteer seeders the collections can never be taken away from humanity. Why are Library Genesis and SciHub vital to humanity? Library Genesis and SciHub set out to share every scientific article and every scientific book with every single person on Earth. Their initiative fulfills United Nations/UNESCO world development goals that mandate the removal of restrictions on access to science. Big publishing companies just want â€œopen access,â€ representing only about 28% of articles, and no books. They want the rest of humanityâ€™s accumulated scientific knowledge to remain locked up behind paywalled databases and unaffordable textbooks. We said fuck that. Limiting and delaying humanityâ€™s access to science isnâ€™t a business, itâ€™s a crime, one with an untold number of victims and preventable deaths. Doctors and scientists in the developing world already face unbelievable challenges in their jobs. Tearing down paywalls between them and the knowledge they need to fight for health and freedom in their homeland is the least we can do to help. How can I help? Redditâ€™s support has been huge. In December the projectâ€™s story was published in Vice, receiving 60,000 upvotes across r/technology , r/futurology , r/datahoarder , and r/seedboxes , and shared to readers around the world in international technology news. Thatâ€™s just for seeding the torrents! Imagine the stories of knowledge brought to doctors and scientists and students around the world. They hold an incredible story to tell. We need their stories next, and we can bring the crisis of access to knowledge into view with our upvotes. Our seeding project has been an incredible success thanks to literal 24/7 work of our volunteers over the last month. Seedbox.io and their provider NFOrce.nl donated a dedicated high-speed server to seed the full Library Genesis book collection. The-Eye.eu is both seeding and archiving the entirety of both library collections. Youâ€™re also welcome to join The-Eye.euâ€™s discord to learn how you can help seed (discord.gg/the-eye #books). Programmers are needed to help re-envision the web frontend, search engine, or distribution model ( https://gitlab.com/libgen1 ). The entirety of Library Genesis is open-source, so anyone is welcome to reimagine the project. Here's what else our communities accomplished in technical details: Swarm peers increased from 3,000 seeders to 30,000 seeders! Swarm speeds increased from about 60KB/s on most torrents to over 100MB/s, thanks to the joint Seedbox.io and NFOrce.nl dedicated server and everyone else seeding. Refreshed and indexed 2,400 .torrent files, replacing 100+ dead trackers with new, live announce URLs The-Eye.eu began to prepare and hash-check the collection for archiving, more to come on that (TBA) Endless thanks to everyone at the-eye.eu , all the volunteers, Seedbox.io/NFOrce.nl, and UltraSeedbox for coming together to make this project happen. We brought science around the world with our torrenting, one of the many big steps in permanently unchaining and preserving all of this knowledge for humanity. https://preview.redd.it/coz7hvkh3s541.png Relevant Links https://phillm.net/libgen-seeds-needed.php https://phillm.net/libgen-stats-table.php \"Archivists Are Trying to Make Sure a â€˜Pirate Bay of Scienceâ€™ Never Goes Down\" by Matthew Gault in Vice News TorrentFreak's coverage by Andy /r/DataHoarder: Let's talk about datahoarding that's actually important: distributing knowledge and the role of Libgen in educating the developing world. /r/Seedboxes Charity Drive /r/Seedboxes Update"},
{"Title": "What's your guys system for archiving entire youtube channels look like?", "Author": "u/glowcialist", "Content": "I tried TubeArchivist, and it worked great for maybe 500 vids, but now seems to be completely blocked, I'm assuming that any script utilizing yt-dlp would face similar issues? 4k tube seemed to work alright for individual playlists at first, but now seems to be consistently throttled to like 50 kb/s. Any ideas? Edit: Please forgive the missing apostrophe in the title. I won't do it again."},
{"Title": "Here's a simple 7 bay CD/DVD ripping machine I just made. Works great! Time to rip 2100 CDs and 300 DVDs", "Author": "u/TVSKS", "Content": "No content"},
{"Title": "How do I download a certain video from the wayback machine?", "Author": "u/wiicrafttech", "Content": "This is the link I'm using https://web.archive.org/web/20230520032047/https://www.youtube.com/watch?v=Oy2c6Mt9KwY#"},
{"Title": "Video Archival Rack - ~50TB of Uncompressed Video Digitized (and Counting)!", "Author": "u/VincentVazzo", "Content": "No content"},
{"Title": "Looking for some critiquing of my plan.", "Author": "u/OnenonlyAl", "Content": "Thanks for taking the time to read and comment. I'm looking to backup a zpool of media. I had tried to setup a truenas for the data but I didn't love the UI and I have what I need on Ubuntu with docker. My plan is to destroy a zpool of the truenas made of 4x10tb raidz1 and move that pool into the primary server (add a pcie with more sata ports) then use zpool attach to mirror my current 4x8tb raidz1 (mnt) to the newly recreated zpool of the 4x10tb from the old truenas server (named mntbackup). So zpool attach mnt to mntbackup. Sounds like this automatically starts mirroring. I know this isn't ideal without being an off-site backup but I'm not the most literate at actually managing this and want something easy. Would I be able to just plug in backup, then unplug the backup pool and cold storage? I don't need perfect redundancy. Would I be able to plug the mntbackup raidz1 back in sometime in the future and the missing new data automatically sync or would I need scripts of rysnc or zfs send receive to add new data? Thanks for any insight and help!"},
{"Title": "Bought 2 old CCTV DVR units for $25, each one had a WD Purple 4tb Drive", "Author": "u/nickdrones", "Content": "No content"},
{"Title": "Suitcase for 3.5\" disks with screwed on HotSwap", "Author": "u/No-Balance-8038", "Content": "Need a way to protect my backup disks that are meant to be stored offsite. I already got cases, but they are too small to account for a 3.5\" HDD plus the mount for SilverStone Technology RM43-320-RS. What would you guys recommend? I live in Germany. From Amazon would be cool. I already have 3.5\" Protection cases but they are not long enough! The HDDs are 17cm long with it."},
{"Title": "Just imagine what it would be like if it were still this size... An IBM 5MB hard drive back in 1956.", "Author": "u/TinderSubThrowAway", "Content": "No content"},
{"Title": "Software for finding all files with identical names on a drive", "Author": "u/MisterPenishead", "Content": "I have a hard drive with thousands of files spread across different folders and some of the files have the same name in spite of them having different content. Is there a software I can use to find all files with identical names so that I can rename them?"},
{"Title": "Data hoarder Flash Drive style", "Author": "u/forlotto", "Content": "No content"},
{"Title": "AWS deep freeze pricing", "Author": "u/KingRollos", "Content": "I'm thinking of moving most of my backup to Amazon's deep freeze service. I'm sure it's made complicated deliberately. I'm thinking of adding it in bits. Will this cost me more? Do I need to gather it all together and then transfer in 1 go?"},
{"Title": "Stopped at a different Costco than normal and I may have found the holy grail. St Louis Park MN.", "Author": "u/twonuh", "Content": "No content"},
{"Title": "Im looking to transfer highest quality from Hi-Fi camera for 8mm Tapes.  Which is better: Mini-USB or FireWire?", "Author": "u/Throwaway173638o", "Content": "I have a Sony Hi-Fi camera for 8mm videos that has a FireWire, Mini USB, A/V, and S-video ports.  I want to get the highest quality as possible.  I narrowed down to FireWire or Mini USB transfer for better quality. Which of the two is better overall or are they the same? Edit: For context, the camera is a Sony DCR-TRV730 and the tapes are Sony Hi8 MP 8mm Video Cassette.  Cassette also lists it as Digital8 with \"60\" while the Hi8 is listed as \"120\""},
{"Title": "Data-mining reveals that 80% of books published 1924-63 never had their copyrights renewed and are now in the public domain", "Author": "u/no-mad", "Content": "No content"},
{"Title": "Sort backup raid 1", "Author": "u/sweetestpeach94", "Content": "Hi, I hope I can get some help. I have a Lacie 2big thunderbolt 2 set as Raid 1. Iâ€™ve just completed the backup of almost 6TB of data (mainly pics), however half of it are just the mirror copies. Now I would move the original half on another disk, the problem is that the backup isnâ€™t like sort out in different folders, itâ€™s just every file and its copy back to back in sequence all together (file A, file A-2 , file B, file B-2, file C, file C - 2 etc.). So here is my question, how I select just one half of the files to move it in the new disk without their duplicate?"},
{"Title": "Alternatives to Epson V600", "Author": "u/ass-master-blaster", "Content": "The V600 is discontinued in my country (past prices are around $650) and the V850 is too expensive ($1600). Are there any alternatives around the same price and quality for a newer model scanner? I've looked at the FastFoto and don't like the quality of the photos and am happy to spend the time with a flatbed scanner."},
{"Title": "Ibms first commercially available 5mb ramac's disk storage 1956", "Author": "u/noideawhatimdoing444", "Content": "No content"},
{"Title": "are SSK USB flash drives safe?", "Author": "u/retrorays", "Content": "I read how USB drives can run malware to act as a USB keyboard and then compromise your system. I see the SSK drives are from China. Do we know if they are safe?"},
{"Title": "Need some help sanity checking my UnRAID 'Reamalgamation' project, specifically Disk Shelves", "Author": "u/AshleyUncia", "Content": "So here's the situation; I currently live with my spouse in a one bedroom apartment built in 1922.  For this reason there are some real issues with the loads on the electrical circuits between my network storage, gaming PC, HTPCs and so on.  Plus just 'physical space' and noise limitations. As such, when my 16 drive UnRAID server ran out of drive slots, the only solution was to build a second server which is in the Livingroom, in a 4U Rosewill case, sitting discretely in an Ikea Lack table with caster wheels.  The main server has a Ryzen 9 3950X and does all the dockers and stuff.  The secondary server has an Intel E5 2697v2 and sits there eating electricity for the sake of letting me run 12 more drives. But we're moving!  Three floors!  Gonna run ethernet in all the walls.  There will be a finished basement area for 'the gaming goodness' and I can finally set up network and storage in a real rack, on the unfinished side of the basement, with it's own 15amp circuit and where no one will care how much noise anything in there makes.  That means I an get disk shelf and make all of this way less stupid! The main plan is to retire the server inside the 4U rack case, then transplant the main tower server into that 4U rack case and expand it's drive capacity with a disk shelf.  So here's where I have questions: Firstly, things like the NetApp DS4246 and related seem to be what I'm looking at.  All my drives are SATA, to these disk shelves support SATA out of the box, is additional hardware required for SATA drives, or do I need to look for something alternate/specific? Secondly, these shelves offer up to four PSUs for redundancy, but how many are needed at minimum assuming 'up time' is not a major concern?  Also what kind of power consumption should I see beyond the drives it's powering?  I should def see an advantage over a whole 11 year old Xeon running, right? Thirdly, for the 'host server' to access this kind of disk shelf, I should only require something like an LSI 9201-16E and a quartet of 8088 to SFF-8088 cables, right?  From there on, the host device should just have an LSI controller which see's up to 24 drives on it, and it's all happy and 'just works'?"},
{"Title": "Any Software Recommendations for Folder Sync that Works on Top of Existing OS?", "Author": "u/avattz", "Content": "I almost had a data loss scare yesterday with a Windows machine, luckily I managed to restore it, but this has lead to figuring out a file sync system for my machines in case one has an issue with the boot drive or hardware failure. I currently have a small RAID 1 file server running Samba and while I manually copy files from my computers to this server, I wanted to see if there was software that could automated this. The goal is for this software to automatically copy a new completed file placed in the documents folder to a  network drive that is available on the same computer. Literally \"copy this file over there when it exists\". I looked into FreeFileSync and Syncthing but these appear to sync directly to a server instead to a local folder. One additional thing I an looking for is two-way syncing. This way, I can make a \"universal\" document folders where all my computers will have the same content, and update them if they are missing anything. This could count as additional backup since I would have the same files over many computers. Does anyone have recommendations for a software solution? Preferably: Open source \"Live\" syncing (runs when new or changed file detected instead of scheduled syncing) Flexible / Plenty of Options / Configurable Uses native Windows file commands Doesn't hurt, but works on Linux (I have \"better\" options for my Linux machines though) I appreciate any recommendations! Edit: I remember SyncToy which would be perfect, if anyone knows of an open source version of SyncToy, then that would be what I am looking for!"},
{"Title": "Video upscaling 480p to 1080 ffmpeg", "Author": "u/1michaelbrown", "Content": "The title pretty much explains what Iâ€™m asking. I want to know if itâ€™s worth it since most dvds only provide 480p. Would it be worth it upscaling it to 1080p. With FFmpeg. I plan on buying 100â€ screen and projector. If that matters What I am using to encode with. I can use either. Mac mini M1 Or Dell power edge r620 with Debian vm With two cpus with a total of 40 cores. I plan on using FFmpeg via command line. (Some inside thoughts I want to make a script that will kind of automate this process. Checking if upscaling is needed or not) I am new here I hope this post is the right place."},
{"Title": "UPS APC BX1200MI-MS a good choice with no fans?", "Author": "u/maguillo", "Content": "Hello , I am about to buy the APC BX1200MI-MS with 1200VA and 650W (for the price) to back my nas , but the thing is it does not have fans to cool the device like other models , so I dont know if is or not necessary as I dont want the place smell burnt plastic , or how it disipates the heat? Thanks https://preview.redd.it/ups-apc-bx1200mi-ms-a-good-choice-with-no-fans-v0-xbq80a528l5d1.png https://preview.redd.it/ups-apc-bx1200mi-ms-a-good-choice-with-no-fans-v0-s5alvwq38l5d1.png"},
{"Title": "Which type of external backup drives/systems should I get and use?", "Author": "u/BrinkleyPT", "Content": "Hi. I'm building a new PC with 1TB SSD nVME. What should I get for backup that's reliable? And if I get something how often should I replace the drive to ensure I don't lose data? Still trying to figure out what's the best method of image and file backup and what to buy and use, but also replace and when to replace it in order to avoid losing data. Thanks ðŸ‘"},
{"Title": "Is it good practice to leave free space on Optical Disks?", "Author": "u/finbarrgalloway", "Content": "I know for hard disk storage they often tell you to leave 1/3 to 1/4 free to not stress the drive, but does this hold true for a DVD? Or can I fill them up to my hearts content?"},
{"Title": "Does tubeup delete video file after upload?", "Author": "u/elgato123", "Content": "Once tubeup downloads a youtube video and uploads it to archive.org , does it then delete the file locally? I can see a hard drive getting full fast if not."},
{"Title": "Future Data Hoarder!", "Author": "u/allweretakenornot", "Content": "Hello! I long time lurker finally looking to break into data hoarding. My mom is looking to back up around a terabyte of photos and I was considering getting her a storage bay likely in RAID 1 to protect her data. Is there any recommended system? I was looking on Amazon and saw this 5 bay orico enclosure. Are these systems good? Should I try and find a cloud solution for her? PS: I read the rules and saw that they recommended tech support. If this post is not in the nature of the subreddit please let me know! Happy hoarding!"},
{"Title": "Does anyone use serverpartdeal drives as their main drives in a nas?", "Author": "u/DGU_kibb", "Content": "I'm planning my next NAS. Definitely going for an array of larger capacity drives. I bought new drives for my last build, but im going to have to buy several 12tb drives for my next one. I know if you have backups then it doesn't really matter (and I do have backups) but I'm curious, i know serverpartdeals is reputable around here, but are people using these in their main nas? Or are you using them purely as backups/cold storage or for unimportant data at the most?"},
{"Title": "Questions about File Integrity when and after transferring files", "Author": "u/gpspam", "Content": "Hello everyone, I'm a bit new to true data hoarding and I had made what is probably considered rookie mistakes. Some backstory: I had an unfortunate ssd failure which cost me about $1000 to recover the files. After some research I've decided to set up a NAS w/ RAID1 (and other back up methods). Now I have some questions to help me transition/transfer/migration from my old external hdd setup to my NAS solution. My first question mainly revolves around keeping file integrity when transferring files. What programs are best at doing this? I've done some research and I've currently chosen TeraCopy. It looks pretty good; other posts have suggested stuff like Robocopy but I couldn't find/get it to work (maybe it's command line stuff that, though I admittedly didn't look too hard since I don't trust inbuilt windows stuff that much). How good is TeraCopy and its file integrity verification? My second question is about checking files for corruption, mainly videos. This one is a bit of a shot in the dark, a hail mary hope of mine that I can fix this headache inducing rookie mistake of mine. Long story short, I had to reinitialize my NAS due to changing its setup. When doing this, I copied data I had on the NAS to a ext HDD (no verification done, I now know it was a stupid rookie mistake) totaling approx 3.5 TB of data, mainly video files (probably around 1k hours). Now I found at least 1 of them has a bit of corruption, where about 20 seconds got messed up. Is there a way or program that can find these kind of problems with files? I'm guessing probably not, but if there are potential solutions I'd love to find them. Otherwise, I guess those errors/problems will just exist and years later I'll find out and lament that if I knew years ago I could have replaced those files but not when I do. Thanks for your help and I'll reply if I have any follow up questions."},
{"Title": "Planning on storing txt, xlsx, and MKV files. It seems like Flash-Drives/Micro SD's aren't reliable, what is a good alternative?", "Author": "u/Sasutaschi", "Content": "So far I've mostly stored Data on the aforementioned methods, but browsing this Sub, it seems like a SSD + Enclosure would be the best way to go. This Enclosure was recommended. https://www.amazon.com/ineo-Aluminum-External-Enclosure-C2594-NVME/dp/B07MZQF1H6 For a SSD, I was thinking about the Samsung EVO 980. Are there better alternatives? Finally, would it be possible to watch MKV files, if plugged into a TV/PS4 from the enclosure? Will there be a delay? Thx. for the advice and have a nice day."},
{"Title": "SAS Expander with SATA power?", "Author": "u/emanknugsaeman", "Content": ""},
{"Title": "Question about Macrium's compression", "Author": "u/Revolutionary_Cod672", "Content": "Hi all, I've got a 12TB home server and I'm trying to use Macrium Reflect to run backups. In an effort to cut costs, I'm trying to backup onto smaller external HDs. I've got one media library that's about 6TB that I'm trying to put onto a drive that's got an actual capacity of 4.5TB, I'm trying to rely on compression to make that work. The issue is that I've tried with both medium and high compression but the backup always fails due to insufficient space. Does the compression happen after the backup takes place? Or am I doing something wrong? Wasn't sure if I needed to have 6TB of space available initially, then it does some compression afterwards. Thanks in advance."},
{"Title": "Is this a good deal listed in my local facebook marketplace â€œDell poweredge r710 md1220 READâ€", "Author": "u/makzero", "Content": "No content"},
{"Title": "Transcode Settings and Workflows for Archiving Video Game Clips", "Author": "u/HalluxTheGreat", "Content": "Looking for advice on how to compress some of the gameplay videos I make in some video games. I typically record in 3440 x 1440 30fps with a bitrate of 10000-15000 kbps. And at the end of the day or week I use handbrake to cut the size to about half. A lot of my clips arent for posting to streams but to just compare my early gameplay with myself later on to see how I changed with 0 hours in a game vs 100 or even 1000, or to just capture the moment when playing with friends. Does anyone have any workflows in capturing their own gameplay and storing it? Naming schemes, settings, scripts, etc... I've been looking at alternatives such as Shutter encoder as well."},
{"Title": "Need to rename thousands of files that were named with a bad template", "Author": "u/NighthawkE3", "Content": "Okay this is a potentially weird one, Iâ€™ll do my best Very quick summery: I have about 5000 files that must be renamed from: To Simply I have about 5000 of these incorrectly named files, and they can't be used in the current format. I have both Windows and Linux operating systems at my disposal, any help would be massively appreciated"},
{"Title": "Silverstone shows off CS383 at Computex", "Author": "u/Odrel", "Content": "GamersNexus is looking at the Silverstone CS383 at Computex. The video is super quick but some highlights include: Up to 12x3,5\" drives Option to add a second power supply by removing a HDD cage (-4 HDDs) 5,25\" bay at the top Support for large motherboards and GPUs Targeting $400 in Q3"},
{"Title": "TrackTalk.net, one of the biggest athletics forums of its time, on the verge of shutting down - Looking for a way to back it up.", "Author": "u/xd-Drewski13", "Content": "I'm somewhat of a fan of track and field fan and this was one of my favorite sites to go on back in the day. It seems that the owner is unsure if he can keep it up and is running out of time/energy to maintain it. I was wondering if there would be a simple way to back it up. It contains thousands of threads about information that would mostly be lost to time if it were to disappear. I have a fairly large server for movies and shows but don't have to experience scraping and hoarding data like this. Any ideas? I know this site is a certain type of forum template, but I don't know which one it is or the best way to go about this."},
{"Title": "Recovering webms and other video file formats (using R-Linux, or a recommended alternative)", "Author": "u/burning_torch", "Content": "I formatted a drive full of videos by accident on my Ubuntu 22.04 computer. Stupid me. It was a full format, not a quick format, but I stopped it only a second or two after it had started. I ran testdisk to check the damages and try to recover the files; it didn't really work all too well. I tried to run photorec, but the software is unusable - my input was randomized and would constantly change if it even accepted input at all; this wasn't a mapping issue, as the right arrow key would act as arrow up, enter, arrow down, or any other random input at any given time. Either way, I eventually found and ran R-Studio's R-Linux, which seemed to work fine, if a little unintuitive. I ran the software and seemingly recovered a lot of files; among others, I recovered a lot of mp4 files. This was expected, as the drive was filled almost exclusively with videos. However, R-Linux does not have built in capabilities to recover webms or other video file formats like qt, and I know there were a great deal of those files on the drive. Does anyone know of any good ways to recover those files as well? I know R-Linux has the ability to add custom file types by building an xml file for that type. I wouldn't know where to begin on getting the information for such a file, but does anyone have one that would work for the not included video file types (webm, qt, etc.)? Or does anyone know of a software that has the ability to recover those files?"},
{"Title": "How does position affect hdd life span?", "Author": "u/d3crypti0n", "Content": "Hello everybody, I found a cheap server chassis for my rack I wanted to buy but the thing that bothers me is the positing of the drives, they are mounted vertically. This brings me to my question - does the position affect the hdd lifespan? Do drives that stand life longer / shorter then the ones who are mounted horizontally due to gravity (because of the rotating platters) ? If yes, how big of difference does it make?"},
{"Title": "when your hdd order is delayed for the 3rd time you really learn what files are important to you", "Author": "u/d1ckpunch68", "Content": "No content"},
{"Title": "Decades since I was a teenager but sometimes 'You just don't understand me, Dad!' still applies.", "Author": "u/AshleyUncia", "Content": "No content"},
{"Title": "Some datahoarder porn :) one of my archive projects with a customer who ditched the tape towards disk/cloud archiving. 4PB here. Each orange jbod is 60 bay top loader", "Author": "u/kumits-u", "Content": "No content"},
{"Title": "ISOs are nice but sometimes you need to hoard the originals for the complete experience. (And also rip them to ISO)", "Author": "u/AshleyUncia", "Content": "No content"},
{"Title": "I collect countless documents and images and organize them by year. So satisfying.", "Author": "u/dirtypark", "Content": "No content"},
{"Title": "My offsite backup!", "Author": "u/pairofcrocs", "Content": "No content"},
{"Title": "X (formerly knows as Twitter) purged all media from posts from before 2014", "Author": "u/coasterghost", "Content": "No content"},
{"Title": "Internet forums are disappearing because now it's all Reddit and Discord. And that's worrying.", "Author": "u/Run_the_Line", "Content": "No content"},
{"Title": "Just found nearly every Playboy from January 1976 to September 2001 at the recycling drop", "Author": "u/frazzleb420", "Content": "No content"},
{"Title": "I Blame Each and Every One of You....", "Author": "u/MeatballB", "Content": "No content"},
{"Title": "Sad day at Warner Brothers", "Author": "u/imajes", "Content": "No content"},
{"Title": "My data hoarding project: scanning 15 years worth of journals & converting to PDF for archive and safekeeping", "Author": "u/thebilljim", "Content": "No content"},
{"Title": "Thanks Amazon I love dents", "Author": "u/ralphte", "Content": "No content"},
{"Title": "Lovely machine for digitalizing books", "Author": "u/Zloty_Diament", "Content": "No content"},
{"Title": "YouTube Vanced: speculation that profiting of the project with NFTs is what triggered the cease and desist", "Author": "u/EpsilonBlight", "Content": "https://arstechnica.com/gadgets/2022/03/google-shuts-down-youtube-vanced-a-popular-ad-blocking-android-app/ Just last month, Team Vanced pulled a provocative stunt involving minting a non-fungible token of the Vanced logo, and there's solid speculation that this action is what drew Google's ire. Google mostly tends to leave the Android modding community alone, but profiting off your legally dubious mod is sure to bring out the lawyers. Once again crypto is why we can't have nice things."},
{"Title": "Contrary to many posts here, at least second hand sellers know how pack things.", "Author": "u/Matti_Meikalainen", "Content": "No content"},
{"Title": "Looks like Amazon is pulling the plug on unlimited cloud storage.", "Author": "Unknown author", "Content": "No content"},
{"Title": "Do the data backup!", "Author": "u/Sarke1", "Content": "No content"},
{"Title": "Article: â€œ10 everyday things that will vanish in the next 10 yearsâ€... I wonder what they think cloud providers use to store all that data.", "Author": "u/Sp00ky777", "Content": "No content"},
{"Title": "Library Genesis Project update: 2.5 million books seeded with the world, 80 million scientific articles next", "Author": "u/shrine", "Content": "For the latest updates on the Library Genesis Seeding Project join r/libgen and r/scihub Last month volunteers on r/seedboxes , r/datahoarder , across reddit, and around the world joined together to secure and preserve 2.5 million scientific books for humanity- for students, for doctors, for scientists, for future generations. The outpour of support for the project still leaves me in total awe. Thousands of people around the world joined our seeding effort donating bandwidth, storage, and expertise. Today we announce that the final set of 1,000 books is now seeded, saved, and preserved. Stunning generosity and heart. But our volunteers couldnâ€™t stop at books. We have already started to secure and preserve a new library of 80 million scientific articles. And now thanks to the brave librarians at Library Genesis and SciHub and all the volunteer seeders the collections can never be taken away from humanity. Why are Library Genesis and SciHub vital to humanity? Library Genesis and SciHub set out to share every scientific article and every scientific book with every single person on Earth. Their initiative fulfills United Nations/UNESCO world development goals that mandate the removal of restrictions on access to science. Big publishing companies just want â€œopen access,â€ representing only about 28% of articles, and no books. They want the rest of humanityâ€™s accumulated scientific knowledge to remain locked up behind paywalled databases and unaffordable textbooks. We said fuck that. Limiting and delaying humanityâ€™s access to science isnâ€™t a business, itâ€™s a crime, one with an untold number of victims and preventable deaths. Doctors and scientists in the developing world already face unbelievable challenges in their jobs. Tearing down paywalls between them and the knowledge they need to fight for health and freedom in their homeland is the least we can do to help. How can I help? Redditâ€™s support has been huge. In December the projectâ€™s story was published in Vice, receiving 60,000 upvotes across r/technology , r/futurology , r/datahoarder , and r/seedboxes , and shared to readers around the world in international technology news. Thatâ€™s just for seeding the torrents! Imagine the stories of knowledge brought to doctors and scientists and students around the world. They hold an incredible story to tell. We need their stories next, and we can bring the crisis of access to knowledge into view with our upvotes. Our seeding project has been an incredible success thanks to literal 24/7 work of our volunteers over the last month. Seedbox.io and their provider NFOrce.nl donated a dedicated high-speed server to seed the full Library Genesis book collection. The-Eye.eu is both seeding and archiving the entirety of both library collections. Youâ€™re also welcome to join The-Eye.euâ€™s discord to learn how you can help seed (discord.gg/the-eye #books). Programmers are needed to help re-envision the web frontend, search engine, or distribution model ( https://gitlab.com/libgen1 ). The entirety of Library Genesis is open-source, so anyone is welcome to reimagine the project. Here's what else our communities accomplished in technical details: Swarm peers increased from 3,000 seeders to 30,000 seeders! Swarm speeds increased from about 60KB/s on most torrents to over 100MB/s, thanks to the joint Seedbox.io and NFOrce.nl dedicated server and everyone else seeding. Refreshed and indexed 2,400 .torrent files, replacing 100+ dead trackers with new, live announce URLs The-Eye.eu began to prepare and hash-check the collection for archiving, more to come on that (TBA) Endless thanks to everyone at the-eye.eu , all the volunteers, Seedbox.io/NFOrce.nl, and UltraSeedbox for coming together to make this project happen. We brought science around the world with our torrenting, one of the many big steps in permanently unchaining and preserving all of this knowledge for humanity. https://preview.redd.it/coz7hvkh3s541.png Relevant Links https://phillm.net/libgen-seeds-needed.php https://phillm.net/libgen-stats-table.php \"Archivists Are Trying to Make Sure a â€˜Pirate Bay of Scienceâ€™ Never Goes Down\" by Matthew Gault in Vice News TorrentFreak's coverage by Andy /r/DataHoarder: Let's talk about datahoarding that's actually important: distributing knowledge and the role of Libgen in educating the developing world. /r/Seedboxes Charity Drive /r/Seedboxes Update"},
{"Title": "Here's a simple 7 bay CD/DVD ripping machine I just made. Works great! Time to rip 2100 CDs and 300 DVDs", "Author": "u/TVSKS", "Content": "No content"},
{"Title": "Video Archival Rack - ~50TB of Uncompressed Video Digitized (and Counting)!", "Author": "u/VincentVazzo", "Content": "No content"},
{"Title": "Bought 2 old CCTV DVR units for $25, each one had a WD Purple 4tb Drive", "Author": "u/nickdrones", "Content": "No content"},
{"Title": "Just imagine what it would be like if it were still this size... An IBM 5MB hard drive back in 1956.", "Author": "u/TinderSubThrowAway", "Content": "No content"},
{"Title": "Data hoarder Flash Drive style", "Author": "u/forlotto", "Content": "No content"},
{"Title": "Stopped at a different Costco than normal and I may have found the holy grail. St Louis Park MN.", "Author": "u/twonuh", "Content": "No content"},
{"Title": "Data-mining reveals that 80% of books published 1924-63 never had their copyrights renewed and are now in the public domain", "Author": "u/no-mad", "Content": "No content"},
{"Title": "Some datahoarder porn :) one of my archive projects with a customer who ditched the tape towards disk/cloud archiving. 4PB here. Each orange jbod is 60 bay top loader", "Author": "u/kumits-u", "Content": "No content"},
{"Title": "ISOs are nice but sometimes you need to hoard the originals for the complete experience. (And also rip them to ISO)", "Author": "u/AshleyUncia", "Content": "No content"},
{"Title": "Alternatives to Epson V600", "Author": "u/ass-master-blaster", "Content": "The V600 is discontinued in my country (past prices are around $650) and the V850 is too expensive ($1600). Are there any alternatives around the same price and quality for a newer model scanner? I've looked at the FastFoto and don't like the quality of the photos and am happy to spend the time with a flatbed scanner."},
{"Title": "I collect countless documents and images and organize them by year. So satisfying.", "Author": "u/dirtypark", "Content": "No content"},
{"Title": "Ibms first commercially available 5mb ramac's disk storage 1956", "Author": "u/noideawhatimdoing444", "Content": "No content"},
{"Title": "Some datahoarder porn :) one of my archive projects with a customer who ditched the tape towards disk/cloud archiving. 4PB here. Each orange jbod is 60 bay top loader", "Author": "u/kumits-u", "Content": "No content"},
{"Title": "My offsite backup!", "Author": "u/pairofcrocs", "Content": "No content"},
{"Title": "are SSK USB flash drives safe?", "Author": "u/retrorays", "Content": "I read how USB drives can run malware to act as a USB keyboard and then compromise your system. I see the SSK drives are from China. Do we know if they are safe?"},
{"Title": "ISOs are nice but sometimes you need to hoard the originals for the complete experience. (And also rip them to ISO)", "Author": "u/AshleyUncia", "Content": "No content"},
{"Title": "X (formerly knows as Twitter) purged all media from posts from before 2014", "Author": "u/coasterghost", "Content": "No content"},
{"Title": "Need some help sanity checking my UnRAID 'Reamalgamation' project, specifically Disk Shelves", "Author": "u/AshleyUncia", "Content": "So here's the situation; I currently live with my spouse in a one bedroom apartment built in 1922.  For this reason there are some real issues with the loads on the electrical circuits between my network storage, gaming PC, HTPCs and so on.  Plus just 'physical space' and noise limitations. As such, when my 16 drive UnRAID server ran out of drive slots, the only solution was to build a second server which is in the Livingroom, in a 4U Rosewill case, sitting discretely in an Ikea Lack table with caster wheels.  The main server has a Ryzen 9 3950X and does all the dockers and stuff.  The secondary server has an Intel E5 2697v2 and sits there eating electricity for the sake of letting me run 12 more drives. But we're moving!  Three floors!  Gonna run ethernet in all the walls.  There will be a finished basement area for 'the gaming goodness' and I can finally set up network and storage in a real rack, on the unfinished side of the basement, with it's own 15amp circuit and where no one will care how much noise anything in there makes.  That means I an get disk shelf and make all of this way less stupid! The main plan is to retire the server inside the 4U rack case, then transplant the main tower server into that 4U rack case and expand it's drive capacity with a disk shelf.  So here's where I have questions: Firstly, things like the NetApp DS4246 and related seem to be what I'm looking at.  All my drives are SATA, to these disk shelves support SATA out of the box, is additional hardware required for SATA drives, or do I need to look for something alternate/specific? Secondly, these shelves offer up to four PSUs for redundancy, but how many are needed at minimum assuming 'up time' is not a major concern?  Also what kind of power consumption should I see beyond the drives it's powering?  I should def see an advantage over a whole 11 year old Xeon running, right? Thirdly, for the 'host server' to access this kind of disk shelf, I should only require something like an LSI 9201-16E and a quartet of 8088 to SFF-8088 cables, right?  From there on, the host device should just have an LSI controller which see's up to 24 drives on it, and it's all happy and 'just works'?"},
{"Title": "I collect countless documents and images and organize them by year. So satisfying.", "Author": "u/dirtypark", "Content": "No content"},
{"Title": "Internet forums are disappearing because now it's all Reddit and Discord. And that's worrying.", "Author": "u/Run_the_Line", "Content": "No content"},
{"Title": "Any Software Recommendations for Folder Sync that Works on Top of Existing OS?", "Author": "u/avattz", "Content": "I almost had a data loss scare yesterday with a Windows machine, luckily I managed to restore it, but this has lead to figuring out a file sync system for my machines in case one has an issue with the boot drive or hardware failure. I currently have a small RAID 1 file server running Samba and while I manually copy files from my computers to this server, I wanted to see if there was software that could automated this. The goal is for this software to automatically copy a new completed file placed in the documents folder to a  network drive that is available on the same computer. Literally \"copy this file over there when it exists\". I looked into FreeFileSync and Syncthing but these appear to sync directly to a server instead to a local folder. One additional thing I an looking for is two-way syncing. This way, I can make a \"universal\" document folders where all my computers will have the same content, and update them if they are missing anything. This could count as additional backup since I would have the same files over many computers. Does anyone have recommendations for a software solution? Preferably: Open source \"Live\" syncing (runs when new or changed file detected instead of scheduled syncing) Flexible / Plenty of Options / Configurable Uses native Windows file commands Doesn't hurt, but works on Linux (I have \"better\" options for my Linux machines though) I appreciate any recommendations! Edit: I remember SyncToy which would be perfect, if anyone knows of an open source version of SyncToy, then that would be what I am looking for!"},
{"Title": "My offsite backup!", "Author": "u/pairofcrocs", "Content": "No content"},
{"Title": "Just found nearly every Playboy from January 1976 to September 2001 at the recycling drop", "Author": "u/frazzleb420", "Content": "No content"},
{"Title": "Video upscaling 480p to 1080 ffmpeg", "Author": "u/1michaelbrown", "Content": "The title pretty much explains what Iâ€™m asking. I want to know if itâ€™s worth it since most dvds only provide 480p. Would it be worth it upscaling it to 1080p. With FFmpeg. I plan on buying 100â€ screen and projector. If that matters What I am using to encode with. I can use either. Mac mini M1 Or Dell power edge r620 with Debian vm With two cpus with a total of 40 cores. I plan on using FFmpeg via command line. (Some inside thoughts I want to make a script that will kind of automate this process. Checking if upscaling is needed or not) I am new here I hope this post is the right place."},
{"Title": "X (formerly knows as Twitter) purged all media from posts from before 2014", "Author": "u/coasterghost", "Content": "No content"},
{"Title": "I Blame Each and Every One of You....", "Author": "u/MeatballB", "Content": "No content"},
{"Title": "UPS APC BX1200MI-MS a good choice with no fans?", "Author": "u/maguillo", "Content": "Hello , I am about to buy the APC BX1200MI-MS with 1200VA and 650W (for the price) to back my nas , but the thing is it does not have fans to cool the device like other models , so I dont know if is or not necessary as I dont want the place smell burnt plastic , or how it disipates the heat? Thanks https://preview.redd.it/ups-apc-bx1200mi-ms-a-good-choice-with-no-fans-v0-xbq80a528l5d1.png https://preview.redd.it/ups-apc-bx1200mi-ms-a-good-choice-with-no-fans-v0-s5alvwq38l5d1.png"},
{"Title": "Internet forums are disappearing because now it's all Reddit and Discord. And that's worrying.", "Author": "u/Run_the_Line", "Content": "No content"},
{"Title": "Sad day at Warner Brothers", "Author": "u/imajes", "Content": "No content"},
{"Title": "Which type of external backup drives/systems should I get and use?", "Author": "u/BrinkleyPT", "Content": "Hi. I'm building a new PC with 1TB SSD nVME. What should I get for backup that's reliable? And if I get something how often should I replace the drive to ensure I don't lose data? Still trying to figure out what's the best method of image and file backup and what to buy and use, but also replace and when to replace it in order to avoid losing data. Thanks ðŸ‘"},
{"Title": "Just found nearly every Playboy from January 1976 to September 2001 at the recycling drop", "Author": "u/frazzleb420", "Content": "No content"},
{"Title": "My data hoarding project: scanning 15 years worth of journals & converting to PDF for archive and safekeeping", "Author": "u/thebilljim", "Content": "No content"},
{"Title": "Is it good practice to leave free space on Optical Disks?", "Author": "u/finbarrgalloway", "Content": "I know for hard disk storage they often tell you to leave 1/3 to 1/4 free to not stress the drive, but does this hold true for a DVD? Or can I fill them up to my hearts content?"},
{"Title": "I Blame Each and Every One of You....", "Author": "u/MeatballB", "Content": "No content"},
{"Title": "Thanks Amazon I love dents", "Author": "u/ralphte", "Content": "No content"},
{"Title": "Does tubeup delete video file after upload?", "Author": "u/elgato123", "Content": "Once tubeup downloads a youtube video and uploads it to archive.org , does it then delete the file locally? I can see a hard drive getting full fast if not."},
{"Title": "Sad day at Warner Brothers", "Author": "u/imajes", "Content": "No content"},
{"Title": "Lovely machine for digitalizing books", "Author": "u/Zloty_Diament", "Content": "No content"},
{"Title": "Future Data Hoarder!", "Author": "u/allweretakenornot", "Content": "Hello! I long time lurker finally looking to break into data hoarding. My mom is looking to back up around a terabyte of photos and I was considering getting her a storage bay likely in RAID 1 to protect her data. Is there any recommended system? I was looking on Amazon and saw this 5 bay orico enclosure. Are these systems good? Should I try and find a cloud solution for her? PS: I read the rules and saw that they recommended tech support. If this post is not in the nature of the subreddit please let me know! Happy hoarding!"},
{"Title": "My data hoarding project: scanning 15 years worth of journals & converting to PDF for archive and safekeeping", "Author": "u/thebilljim", "Content": "No content"},
{"Title": "YouTube Vanced: speculation that profiting of the project with NFTs is what triggered the cease and desist", "Author": "u/EpsilonBlight", "Content": "https://arstechnica.com/gadgets/2022/03/google-shuts-down-youtube-vanced-a-popular-ad-blocking-android-app/ Just last month, Team Vanced pulled a provocative stunt involving minting a non-fungible token of the Vanced logo, and there's solid speculation that this action is what drew Google's ire. Google mostly tends to leave the Android modding community alone, but profiting off your legally dubious mod is sure to bring out the lawyers. Once again crypto is why we can't have nice things."},
{"Title": "Does anyone use serverpartdeal drives as their main drives in a nas?", "Author": "u/DGU_kibb", "Content": "I'm planning my next NAS. Definitely going for an array of larger capacity drives. I bought new drives for my last build, but im going to have to buy several 12tb drives for my next one. I know if you have backups then it doesn't really matter (and I do have backups) but I'm curious, i know serverpartdeals is reputable around here, but are people using these in their main nas? Or are you using them purely as backups/cold storage or for unimportant data at the most?"},
{"Title": "Thanks Amazon I love dents", "Author": "u/ralphte", "Content": "No content"},
{"Title": "Contrary to many posts here, at least second hand sellers know how pack things.", "Author": "u/Matti_Meikalainen", "Content": "No content"},
{"Title": "Questions about File Integrity when and after transferring files", "Author": "u/gpspam", "Content": "Hello everyone, I'm a bit new to true data hoarding and I had made what is probably considered rookie mistakes. Some backstory: I had an unfortunate ssd failure which cost me about $1000 to recover the files. After some research I've decided to set up a NAS w/ RAID1 (and other back up methods). Now I have some questions to help me transition/transfer/migration from my old external hdd setup to my NAS solution. My first question mainly revolves around keeping file integrity when transferring files. What programs are best at doing this? I've done some research and I've currently chosen TeraCopy. It looks pretty good; other posts have suggested stuff like Robocopy but I couldn't find/get it to work (maybe it's command line stuff that, though I admittedly didn't look too hard since I don't trust inbuilt windows stuff that much). How good is TeraCopy and its file integrity verification? My second question is about checking files for corruption, mainly videos. This one is a bit of a shot in the dark, a hail mary hope of mine that I can fix this headache inducing rookie mistake of mine. Long story short, I had to reinitialize my NAS due to changing its setup. When doing this, I copied data I had on the NAS to a ext HDD (no verification done, I now know it was a stupid rookie mistake) totaling approx 3.5 TB of data, mainly video files (probably around 1k hours). Now I found at least 1 of them has a bit of corruption, where about 20 seconds got messed up. Is there a way or program that can find these kind of problems with files? I'm guessing probably not, but if there are potential solutions I'd love to find them. Otherwise, I guess those errors/problems will just exist and years later I'll find out and lament that if I knew years ago I could have replaced those files but not when I do. Thanks for your help and I'll reply if I have any follow up questions."},
{"Title": "Lovely machine for digitalizing books", "Author": "u/Zloty_Diament", "Content": "No content"},
{"Title": "Looks like Amazon is pulling the plug on unlimited cloud storage.", "Author": "Unknown author", "Content": "No content"},
{"Title": "Planning on storing txt, xlsx, and MKV files. It seems like Flash-Drives/Micro SD's aren't reliable, what is a good alternative?", "Author": "u/Sasutaschi", "Content": "So far I've mostly stored Data on the aforementioned methods, but browsing this Sub, it seems like a SSD + Enclosure would be the best way to go. This Enclosure was recommended. https://www.amazon.com/ineo-Aluminum-External-Enclosure-C2594-NVME/dp/B07MZQF1H6 For a SSD, I was thinking about the Samsung EVO 980. Are there better alternatives? Finally, would it be possible to watch MKV files, if plugged into a TV/PS4 from the enclosure? Will there be a delay? Thx. for the advice and have a nice day."},
{"Title": "YouTube Vanced: speculation that profiting of the project with NFTs is what triggered the cease and desist", "Author": "u/EpsilonBlight", "Content": "https://arstechnica.com/gadgets/2022/03/google-shuts-down-youtube-vanced-a-popular-ad-blocking-android-app/ Just last month, Team Vanced pulled a provocative stunt involving minting a non-fungible token of the Vanced logo, and there's solid speculation that this action is what drew Google's ire. Google mostly tends to leave the Android modding community alone, but profiting off your legally dubious mod is sure to bring out the lawyers. Once again crypto is why we can't have nice things."},
{"Title": "Do the data backup!", "Author": "u/Sarke1", "Content": "No content"},
{"Title": "SAS Expander with SATA power?", "Author": "u/emanknugsaeman", "Content": ""},
{"Title": "Contrary to many posts here, at least second hand sellers know how pack things.", "Author": "u/Matti_Meikalainen", "Content": "No content"},
{"Title": "Article: â€œ10 everyday things that will vanish in the next 10 yearsâ€... I wonder what they think cloud providers use to store all that data.", "Author": "u/Sp00ky777", "Content": "No content"},
{"Title": "Question about Macrium's compression", "Author": "u/Revolutionary_Cod672", "Content": "Hi all, I've got a 12TB home server and I'm trying to use Macrium Reflect to run backups. In an effort to cut costs, I'm trying to backup onto smaller external HDs. I've got one media library that's about 6TB that I'm trying to put onto a drive that's got an actual capacity of 4.5TB, I'm trying to rely on compression to make that work. The issue is that I've tried with both medium and high compression but the backup always fails due to insufficient space. Does the compression happen after the backup takes place? Or am I doing something wrong? Wasn't sure if I needed to have 6TB of space available initially, then it does some compression afterwards. Thanks in advance."},
{"Title": "Looks like Amazon is pulling the plug on unlimited cloud storage.", "Author": "Unknown author", "Content": "No content"},
{"Title": "Library Genesis Project update: 2.5 million books seeded with the world, 80 million scientific articles next", "Author": "u/shrine", "Content": "For the latest updates on the Library Genesis Seeding Project join r/libgen and r/scihub Last month volunteers on r/seedboxes , r/datahoarder , across reddit, and around the world joined together to secure and preserve 2.5 million scientific books for humanity- for students, for doctors, for scientists, for future generations. The outpour of support for the project still leaves me in total awe. Thousands of people around the world joined our seeding effort donating bandwidth, storage, and expertise. Today we announce that the final set of 1,000 books is now seeded, saved, and preserved. Stunning generosity and heart. But our volunteers couldnâ€™t stop at books. We have already started to secure and preserve a new library of 80 million scientific articles. And now thanks to the brave librarians at Library Genesis and SciHub and all the volunteer seeders the collections can never be taken away from humanity. Why are Library Genesis and SciHub vital to humanity? Library Genesis and SciHub set out to share every scientific article and every scientific book with every single person on Earth. Their initiative fulfills United Nations/UNESCO world development goals that mandate the removal of restrictions on access to science. Big publishing companies just want â€œopen access,â€ representing only about 28% of articles, and no books. They want the rest of humanityâ€™s accumulated scientific knowledge to remain locked up behind paywalled databases and unaffordable textbooks. We said fuck that. Limiting and delaying humanityâ€™s access to science isnâ€™t a business, itâ€™s a crime, one with an untold number of victims and preventable deaths. Doctors and scientists in the developing world already face unbelievable challenges in their jobs. Tearing down paywalls between them and the knowledge they need to fight for health and freedom in their homeland is the least we can do to help. How can I help? Redditâ€™s support has been huge. In December the projectâ€™s story was published in Vice, receiving 60,000 upvotes across r/technology , r/futurology , r/datahoarder , and r/seedboxes , and shared to readers around the world in international technology news. Thatâ€™s just for seeding the torrents! Imagine the stories of knowledge brought to doctors and scientists and students around the world. They hold an incredible story to tell. We need their stories next, and we can bring the crisis of access to knowledge into view with our upvotes. Our seeding project has been an incredible success thanks to literal 24/7 work of our volunteers over the last month. Seedbox.io and their provider NFOrce.nl donated a dedicated high-speed server to seed the full Library Genesis book collection. The-Eye.eu is both seeding and archiving the entirety of both library collections. Youâ€™re also welcome to join The-Eye.euâ€™s discord to learn how you can help seed (discord.gg/the-eye #books). Programmers are needed to help re-envision the web frontend, search engine, or distribution model ( https://gitlab.com/libgen1 ). The entirety of Library Genesis is open-source, so anyone is welcome to reimagine the project. Here's what else our communities accomplished in technical details: Swarm peers increased from 3,000 seeders to 30,000 seeders! Swarm speeds increased from about 60KB/s on most torrents to over 100MB/s, thanks to the joint Seedbox.io and NFOrce.nl dedicated server and everyone else seeding. Refreshed and indexed 2,400 .torrent files, replacing 100+ dead trackers with new, live announce URLs The-Eye.eu began to prepare and hash-check the collection for archiving, more to come on that (TBA) Endless thanks to everyone at the-eye.eu , all the volunteers, Seedbox.io/NFOrce.nl, and UltraSeedbox for coming together to make this project happen. We brought science around the world with our torrenting, one of the many big steps in permanently unchaining and preserving all of this knowledge for humanity. https://preview.redd.it/coz7hvkh3s541.png Relevant Links https://phillm.net/libgen-seeds-needed.php https://phillm.net/libgen-stats-table.php \"Archivists Are Trying to Make Sure a â€˜Pirate Bay of Scienceâ€™ Never Goes Down\" by Matthew Gault in Vice News TorrentFreak's coverage by Andy /r/DataHoarder: Let's talk about datahoarding that's actually important: distributing knowledge and the role of Libgen in educating the developing world. /r/Seedboxes Charity Drive /r/Seedboxes Update"},
{"Title": "Is this a good deal listed in my local facebook marketplace â€œDell poweredge r710 md1220 READâ€", "Author": "u/makzero", "Content": "No content"},
{"Title": "Do the data backup!", "Author": "u/Sarke1", "Content": "No content"},
{"Title": "Here's a simple 7 bay CD/DVD ripping machine I just made. Works great! Time to rip 2100 CDs and 300 DVDs", "Author": "u/TVSKS", "Content": "No content"},
{"Title": "Transcode Settings and Workflows for Archiving Video Game Clips", "Author": "u/HalluxTheGreat", "Content": "Looking for advice on how to compress some of the gameplay videos I make in some video games. I typically record in 3440 x 1440 30fps with a bitrate of 10000-15000 kbps. And at the end of the day or week I use handbrake to cut the size to about half. A lot of my clips arent for posting to streams but to just compare my early gameplay with myself later on to see how I changed with 0 hours in a game vs 100 or even 1000, or to just capture the moment when playing with friends. Does anyone have any workflows in capturing their own gameplay and storing it? Naming schemes, settings, scripts, etc... I've been looking at alternatives such as Shutter encoder as well."},
{"Title": "Article: â€œ10 everyday things that will vanish in the next 10 yearsâ€... I wonder what they think cloud providers use to store all that data.", "Author": "u/Sp00ky777", "Content": "No content"},
{"Title": "Video Archival Rack - ~50TB of Uncompressed Video Digitized (and Counting)!", "Author": "u/VincentVazzo", "Content": "No content"},
{"Title": "Need to rename thousands of files that were named with a bad template", "Author": "u/NighthawkE3", "Content": "Okay this is a potentially weird one, Iâ€™ll do my best Very quick summery: I have about 5000 files that must be renamed from: To Simply I have about 5000 of these incorrectly named files, and they can't be used in the current format. I have both Windows and Linux operating systems at my disposal, any help would be massively appreciated"},
{"Title": "Library Genesis Project update: 2.5 million books seeded with the world, 80 million scientific articles next", "Author": "u/shrine", "Content": "For the latest updates on the Library Genesis Seeding Project join r/libgen and r/scihub Last month volunteers on r/seedboxes , r/datahoarder , across reddit, and around the world joined together to secure and preserve 2.5 million scientific books for humanity- for students, for doctors, for scientists, for future generations. The outpour of support for the project still leaves me in total awe. Thousands of people around the world joined our seeding effort donating bandwidth, storage, and expertise. Today we announce that the final set of 1,000 books is now seeded, saved, and preserved. Stunning generosity and heart. But our volunteers couldnâ€™t stop at books. We have already started to secure and preserve a new library of 80 million scientific articles. And now thanks to the brave librarians at Library Genesis and SciHub and all the volunteer seeders the collections can never be taken away from humanity. Why are Library Genesis and SciHub vital to humanity? Library Genesis and SciHub set out to share every scientific article and every scientific book with every single person on Earth. Their initiative fulfills United Nations/UNESCO world development goals that mandate the removal of restrictions on access to science. Big publishing companies just want â€œopen access,â€ representing only about 28% of articles, and no books. They want the rest of humanityâ€™s accumulated scientific knowledge to remain locked up behind paywalled databases and unaffordable textbooks. We said fuck that. Limiting and delaying humanityâ€™s access to science isnâ€™t a business, itâ€™s a crime, one with an untold number of victims and preventable deaths. Doctors and scientists in the developing world already face unbelievable challenges in their jobs. Tearing down paywalls between them and the knowledge they need to fight for health and freedom in their homeland is the least we can do to help. How can I help? Redditâ€™s support has been huge. In December the projectâ€™s story was published in Vice, receiving 60,000 upvotes across r/technology , r/futurology , r/datahoarder , and r/seedboxes , and shared to readers around the world in international technology news. Thatâ€™s just for seeding the torrents! Imagine the stories of knowledge brought to doctors and scientists and students around the world. They hold an incredible story to tell. We need their stories next, and we can bring the crisis of access to knowledge into view with our upvotes. Our seeding project has been an incredible success thanks to literal 24/7 work of our volunteers over the last month. Seedbox.io and their provider NFOrce.nl donated a dedicated high-speed server to seed the full Library Genesis book collection. The-Eye.eu is both seeding and archiving the entirety of both library collections. Youâ€™re also welcome to join The-Eye.euâ€™s discord to learn how you can help seed (discord.gg/the-eye #books). Programmers are needed to help re-envision the web frontend, search engine, or distribution model ( https://gitlab.com/libgen1 ). The entirety of Library Genesis is open-source, so anyone is welcome to reimagine the project. Here's what else our communities accomplished in technical details: Swarm peers increased from 3,000 seeders to 30,000 seeders! Swarm speeds increased from about 60KB/s on most torrents to over 100MB/s, thanks to the joint Seedbox.io and NFOrce.nl dedicated server and everyone else seeding. Refreshed and indexed 2,400 .torrent files, replacing 100+ dead trackers with new, live announce URLs The-Eye.eu began to prepare and hash-check the collection for archiving, more to come on that (TBA) Endless thanks to everyone at the-eye.eu , all the volunteers, Seedbox.io/NFOrce.nl, and UltraSeedbox for coming together to make this project happen. We brought science around the world with our torrenting, one of the many big steps in permanently unchaining and preserving all of this knowledge for humanity. https://preview.redd.it/coz7hvkh3s541.png Relevant Links https://phillm.net/libgen-seeds-needed.php https://phillm.net/libgen-stats-table.php \"Archivists Are Trying to Make Sure a â€˜Pirate Bay of Scienceâ€™ Never Goes Down\" by Matthew Gault in Vice News TorrentFreak's coverage by Andy /r/DataHoarder: Let's talk about datahoarding that's actually important: distributing knowledge and the role of Libgen in educating the developing world. /r/Seedboxes Charity Drive /r/Seedboxes Update"},
{"Title": "Bought 2 old CCTV DVR units for $25, each one had a WD Purple 4tb Drive", "Author": "u/nickdrones", "Content": "No content"},
{"Title": "Silverstone shows off CS383 at Computex", "Author": "u/Odrel", "Content": "GamersNexus is looking at the Silverstone CS383 at Computex. The video is super quick but some highlights include: Up to 12x3,5\" drives Option to add a second power supply by removing a HDD cage (-4 HDDs) 5,25\" bay at the top Support for large motherboards and GPUs Targeting $400 in Q3"},
{"Title": "Here's a simple 7 bay CD/DVD ripping machine I just made. Works great! Time to rip 2100 CDs and 300 DVDs", "Author": "u/TVSKS", "Content": "No content"},
{"Title": "Video Archival Rack - ~50TB of Uncompressed Video Digitized (and Counting)!", "Author": "u/VincentVazzo", "Content": "No content"},
{"Title": "TrackTalk.net, one of the biggest athletics forums of its time, on the verge of shutting down - Looking for a way to back it up.", "Author": "u/xd-Drewski13", "Content": "I'm somewhat of a fan of track and field fan and this was one of my favorite sites to go on back in the day. It seems that the owner is unsure if he can keep it up and is running out of time/energy to maintain it. I was wondering if there would be a simple way to back it up. It contains thousands of threads about information that would mostly be lost to time if it were to disappear. I have a fairly large server for movies and shows but don't have to experience scraping and hoarding data like this. Any ideas? I know this site is a certain type of forum template, but I don't know which one it is or the best way to go about this."},
{"Title": "Just imagine what it would be like if it were still this size... An IBM 5MB hard drive back in 1956.", "Author": "u/TinderSubThrowAway", "Content": "No content"},
{"Title": "Bought 2 old CCTV DVR units for $25, each one had a WD Purple 4tb Drive", "Author": "u/nickdrones", "Content": "No content"},
{"Title": "Recovering webms and other video file formats (using R-Linux, or a recommended alternative)", "Author": "u/burning_torch", "Content": "I formatted a drive full of videos by accident on my Ubuntu 22.04 computer. Stupid me. It was a full format, not a quick format, but I stopped it only a second or two after it had started. I ran testdisk to check the damages and try to recover the files; it didn't really work all too well. I tried to run photorec, but the software is unusable - my input was randomized and would constantly change if it even accepted input at all; this wasn't a mapping issue, as the right arrow key would act as arrow up, enter, arrow down, or any other random input at any given time. Either way, I eventually found and ran R-Studio's R-Linux, which seemed to work fine, if a little unintuitive. I ran the software and seemingly recovered a lot of files; among others, I recovered a lot of mp4 files. This was expected, as the drive was filled almost exclusively with videos. However, R-Linux does not have built in capabilities to recover webms or other video file formats like qt, and I know there were a great deal of those files on the drive. Does anyone know of any good ways to recover those files as well? I know R-Linux has the ability to add custom file types by building an xml file for that type. I wouldn't know where to begin on getting the information for such a file, but does anyone have one that would work for the not included video file types (webm, qt, etc.)? Or does anyone know of a software that has the ability to recover those files?"},
{"Title": "Data hoarder Flash Drive style", "Author": "u/forlotto", "Content": "No content"},
{"Title": "Just imagine what it would be like if it were still this size... An IBM 5MB hard drive back in 1956.", "Author": "u/TinderSubThrowAway", "Content": "No content"},
{"Title": "How does position affect hdd life span?", "Author": "u/d3crypti0n", "Content": "Hello everybody, I found a cheap server chassis for my rack I wanted to buy but the thing that bothers me is the positing of the drives, they are mounted vertically. This brings me to my question - does the position affect the hdd lifespan? Do drives that stand life longer / shorter then the ones who are mounted horizontally due to gravity (because of the rotating platters) ? If yes, how big of difference does it make?"},
{"Title": "Stopped at a different Costco than normal and I may have found the holy grail. St Louis Park MN.", "Author": "u/twonuh", "Content": "No content"},
{"Title": "Data hoarder Flash Drive style", "Author": "u/forlotto", "Content": "No content"},
{"Title": "when your hdd order is delayed for the 3rd time you really learn what files are important to you", "Author": "u/d1ckpunch68", "Content": "No content"},
{"Title": "Data-mining reveals that 80% of books published 1924-63 never had their copyrights renewed and are now in the public domain", "Author": "u/no-mad", "Content": "No content"},
{"Title": "Stopped at a different Costco than normal and I may have found the holy grail. St Louis Park MN.", "Author": "u/twonuh", "Content": "No content"},
{"Title": "Decades since I was a teenager but sometimes 'You just don't understand me, Dad!' still applies.", "Author": "u/AshleyUncia", "Content": "No content"},
{"Title": "PSA: The US copyright Office is taking public comments on automated copyright filters till TONIGHT, and The Senate Judiciary Committee is considering the EARN-IT act THIS WEEK, which would create liability for user created content, and encourage/force file scanning-Encryption bans. Take action!", "Author": "u/jabberwockxeno", "Content": "This isn't strictly Datahoarding related, but obviously automated copyright filters lead to lost data and online content, and anything like the EARN-IT act which increases liability for user created content will lead to sites shutting down and lost content as well. I have cleared this with the Mod team. As stated, the US copyright office is taking information, feedback, and input on automated filters and detection systems on Copyright infringement, likely to suggest and support the proliferation of those systems. It will be holding a session on February 22nd, followed by consultation with industry groups. However,** it is also taking comments and input from the general public up till 11:59PM EST on February 8th (TODAY), with a online form, and is explicitly also open to hearing the downsides of such systems.** More information as well as links to the comment form (direct link here can be found here: https://www.eff.org/deeplinks/2022/02/tell-copyright-office-who-really-affected-filters . If you're not sure what exactly to write or aren't familar with how much of a trashfire these filters tend to be, [this[() article is a good starting overview, also from the EFF, and I have compiled some other examples here . The EARN-IT act is also being considered. Remember FOSTA-SESTA from a few years back, the legislation that was osteinbly to go after sex trafficking but really just led to dozens of major websites to shut down their legal adult content and actually made it harder for law enforcement, by their own subsequent admission, to go after actual abusers and traffickers; and which was decried by basically every Digital civil liberty and sex worker group? This is that but worse. It will remove Section 230 protection for wide swathes of websites tangentially connected to adult material, opening it up to liability over user created content, as well as creates liability for using encryption and \"advises\" websites to scan all uploaded content. More info can be found firstly here: https://www.eff.org/deeplinks/2022/02/its-back-senators-want-earn-it-bill-scan-all-online-messages and secondly here: https://www.techdirt.com/articles/20220203/18143448411/how-earn-it-act-is-significantly-more-dangerous-than-fosta.shtml and thirdly here: http://cyberlaw.stanford.edu/blog/2022/02/earn-it-act-back-and-it%E2%80%99s-more-dangerous-ever The EFF links I have provided include and link to tools to actually leave comments for the former, and contact your representatives in the Senate for the latter. If you aren't a US citizen, I encourage you to spread the word to those who are, and I technically don't see anything requiring you're a citizen for the Copyright Office form if you pick \"Anonymous\" If you're seeing this on the 9th or later, it is too late to comment on the Copyright Office stuff (timezone shenngians aside) but there IS still time to contact your senators about EARN IT! EDIT: EARN-IT is allegedly going up for vote on Thursday, the 10th, though this can change, so please still contact your represenatives! Please also follow the EFF and Fight for the Future, both regularly do advocacy and legal lobbying for digital rights and online privacy and against copyright maximalist (I am not affilated with either group, I am just a nerd who cares about this stuff way too much and both have consistently been the sources to follow for this sort of stuff)"},
{"Title": "Data-mining reveals that 80% of books published 1924-63 never had their copyrights renewed and are now in the public domain", "Author": "u/no-mad", "Content": "No content"},
{"Title": "My favorite hard drive tracker + when will 30+ TB drives hit retail?", "Author": "u/chuckremes", "Content": "I will share some information and then ask a question too. I like this site for tracking hard drive prices. Last September I bought a bunch of 16 TB drives for $10.6 per terabyte and then I saw the price go as low as $9.40. For the 16+ TB sizes it's now been hovering at around $14/TB for months. Very concerning. https://diskprices.com/?locale=us&condition=new&disk_types=external_hdd,internal_hdd,external_ssd,internal_ssd So when can we expect to see those 30+ TB drives hit retail? The older articles I read all indicated 2024Q1 but we're about to exit 2024Q2 and I still haven't seen any. What's the latest buzz?"},
{"Title": "GitHub Archive in Svalbard", "Author": "u/FreneticFrench", "Content": "No content"},
{"Title": "PSA: The US copyright Office is taking public comments on automated copyright filters till TONIGHT, and The Senate Judiciary Committee is considering the EARN-IT act THIS WEEK, which would create liability for user created content, and encourage/force file scanning-Encryption bans. Take action!", "Author": "u/jabberwockxeno", "Content": "This isn't strictly Datahoarding related, but obviously automated copyright filters lead to lost data and online content, and anything like the EARN-IT act which increases liability for user created content will lead to sites shutting down and lost content as well. I have cleared this with the Mod team. As stated, the US copyright office is taking information, feedback, and input on automated filters and detection systems on Copyright infringement, likely to suggest and support the proliferation of those systems. It will be holding a session on February 22nd, followed by consultation with industry groups. However,** it is also taking comments and input from the general public up till 11:59PM EST on February 8th (TODAY), with a online form, and is explicitly also open to hearing the downsides of such systems.** More information as well as links to the comment form (direct link here can be found here: https://www.eff.org/deeplinks/2022/02/tell-copyright-office-who-really-affected-filters . If you're not sure what exactly to write or aren't familar with how much of a trashfire these filters tend to be, [this[() article is a good starting overview, also from the EFF, and I have compiled some other examples here . The EARN-IT act is also being considered. Remember FOSTA-SESTA from a few years back, the legislation that was osteinbly to go after sex trafficking but really just led to dozens of major websites to shut down their legal adult content and actually made it harder for law enforcement, by their own subsequent admission, to go after actual abusers and traffickers; and which was decried by basically every Digital civil liberty and sex worker group? This is that but worse. It will remove Section 230 protection for wide swathes of websites tangentially connected to adult material, opening it up to liability over user created content, as well as creates liability for using encryption and \"advises\" websites to scan all uploaded content. More info can be found firstly here: https://www.eff.org/deeplinks/2022/02/its-back-senators-want-earn-it-bill-scan-all-online-messages and secondly here: https://www.techdirt.com/articles/20220203/18143448411/how-earn-it-act-is-significantly-more-dangerous-than-fosta.shtml and thirdly here: http://cyberlaw.stanford.edu/blog/2022/02/earn-it-act-back-and-it%E2%80%99s-more-dangerous-ever The EFF links I have provided include and link to tools to actually leave comments for the former, and contact your representatives in the Senate for the latter. If you aren't a US citizen, I encourage you to spread the word to those who are, and I technically don't see anything requiring you're a citizen for the Copyright Office form if you pick \"Anonymous\" If you're seeing this on the 9th or later, it is too late to comment on the Copyright Office stuff (timezone shenngians aside) but there IS still time to contact your senators about EARN IT! EDIT: EARN-IT is allegedly going up for vote on Thursday, the 10th, though this can change, so please still contact your represenatives! Please also follow the EFF and Fight for the Future, both regularly do advocacy and legal lobbying for digital rights and online privacy and against copyright maximalist (I am not affilated with either group, I am just a nerd who cares about this stuff way too much and both have consistently been the sources to follow for this sort of stuff)"},
{"Title": "40Gb/s fiber in Finland is o so tempting for Torrents and Usenet...", "Author": "u/Rugta", "Content": "No content"},
{"Title": "I thought you guys would appreciate my messy, 10x DVD ripping machine I made out of an old DVD duplicator!", "Author": "u/pairofcrocs", "Content": "No content"},
{"Title": "GitHub Archive in Svalbard", "Author": "u/FreneticFrench", "Content": "No content"},
{"Title": "Anyway to grab the name of a mega folder in the terminal? Megatools doesnt preserve the folder name", "Author": "u/NotAnADC", "Content": "Thank you for any help! Been trying to download mega files but hate that I can't save the folder names"},
{"Title": "Well, Iâ€™m no mathematician but I think Iâ€™ll go with the 14TB. Best Buy Canada", "Author": "u/animatedhockeyfan", "Content": "No content"},
{"Title": "I thought you guys would appreciate my messy, 10x DVD ripping machine I made out of an old DVD duplicator!", "Author": "u/pairofcrocs", "Content": "No content"},
{"Title": "Does anyone have a used harddrive to give for free?", "Author": "u/Alexander_Alexis", "Content": "Hello, i need to put emergently  my phone photos to my pc but my pc is full(200gb ssd) And i dont have moneys or a place near to buy a hard drive, does anyone have a very used hard drive? i need one only for the photos so not running games or big stuff like that"},
{"Title": "52% of YouTube videos live in 2010 have been deleted", "Author": "u/themadprogramer", "Content": "No content"},
{"Title": "Well, Iâ€™m no mathematician but I think Iâ€™ll go with the 14TB. Best Buy Canada", "Author": "u/animatedhockeyfan", "Content": "No content"},
{"Title": "Starting data hording journey", "Author": "u/nlj1978", "Content": "After the help I got here on ripping my cd collection I'm jumping in the deep end. I'm putting together a media server. Have a HP z1 G5 tower workstation with i5-9500 on the way but still have decisions to make in terms of hardware and software. I'm finding mixed data on ECC ram support. Apparently the motherboards come in a couple variations. Once I confirm I believe I believe ECC is a better choice for this application. Still need to verify but I believe the board supports either 64 or 128gig.  That said how much ram should I use? Mobo has 2 m2 slots and 4 sata ports. Computer comes with a 256g nvme. I've read a little about using m2 Intel Optane drives for improved server performance. Anyone have any experience or input on this? Drives I'm still pondering. On the software side I am leaning towards TrueNas and  Jellyfin. Both seem relatively user friendly to someone with limited server knowledge. Thoughts?"},
{"Title": "BOUNTY: $10 000USD. Remember me? Yes, I still have not found it. Please help me find the Whole videotape of Donald Trump on The Oprah Winfrey Show, 25th of April, 1988. Season 3, Episode 5 (a 60 min. episode with a few ads).", "Author": "u/trycoconutoil", "Content": "The Discord group for anyone interested: https://discord.gg/JD8Je3v (ongoing discussions) Saw this tape on Facebook back in 2015. Then, it vanished. I have been searching on and off since then. OFFICIAL BOUNTY : $1000USD TRANSCRIPTS: $1000USD Must be a copy of the original Journal Graphics transcript (Harpo productions bought their inventory) Sources that are relevant for the investigation: https://thetvdb.com/series/the-oprah-winfrey-show/episodes/6650339 (air date) Michael Cohen (trumps former lawyer) talked about the \"Catch and Kill\" method that they utilized several times. This is most likely why it's difficult (not impossible) to find the tape. Explained in short at 1:37:35 https://www.youtube.com/watch?v=v0e62bOKm8o&t=5855s&ab_channel=TheNewYorkTimes - \"[Cohen] was often described by the media as Trump's 'fixer'.\" - Wiki 3 min. part from the original video (there are more short videos out there and almost the full video in the discord group. unfortunately, the broadcasting got interrupted for about 10 min. about half a year after the original tape). NOTE! the video is no longer up but can be put up again if requested. https://www.reddit.com/r/politicalfactchecking/comments/4nt6iw/donald_trump_republicans_are_dumb_why_did_this/ (Frustrated Redditors) https://www.youtube.com/watch?v=AZtfSIgsH1o (some YouTube video that may not be important, but the comments on it are very relevant) http://www.oprah.com/own-oprahshow/what-donald-trump-told-oprah-about-his-presidential-hopes-video (an old link that might have shown the full episode in the past) interesting article (Interesting article on the disappearance of the video.) Another Reddit post by someone else asking for the same episode. A Reddit post about the missing episode and what OP of this post thinks about it . I guess this TikToker deleted her account but found a tweet about her talking about it. (had over 1 million views). An old website and      its resource page image I will give the person who can find it and send it to me $10 000USD (if you want). I'll give it through cryptocurrency, anonymous PayPal system, product(s), or other means. Several Redditors also support the bounty. That will also be included if they stick to their guns. NOTE! Ignore the images and fact-checker sites. Those are red herrings and so point towards something different from the year 1998. I want the entire video from The Oprah Winfrey Show, 25th of April, 1988, season 3, Episode 5, 60 minutes of the show. DM me here on Reddit or post it publically. No short vids or re-recordings that include interruptions. Simple. All of it. The show used to be quite popular; the entire content should be out there, internet, or stored. **Since some ask. Why do I want it?** I want it cause it has been suppressed and repressed on the internet, and various sites and people tell me what I saw and heard didn't happen, and it bothers me. And have found this exploration quite intriguing. I do not have any political intentions; I want the truth, sweetheart, for truth's sake. Great References: $1310 bounty post from last year (the second post) $300 bounty post from last year (the first post) Subreddit for this: https://www.reddit.com/r/BountyFindThisEpisode/ Had a website for this in the past that is no longer up, but viaprize made something similar If someone here wants to use it for broadcast or commerce's sake. DM me, and I can give you the contact info of the person responsible for the original OWN archive. Unfortunately, we could not get it because they don't distribute for \"Personal viewing.\" And even if you are big, they seem to be quite protective of this episode in particular. We have communicated with countless people. The journalist who first rediscovered and published the tape in 2015, multiple archivists, mindless scammers and haters that help bring energy into this, stubborn and helpful companies, Lost Media YouTubers (reluctant to make a video because of potential strike), and many more. Through the years. Traces of what was will naturally disappear. Forum posts, Reddit posts, articles, websites, comments, notes, talks, even TikToks, fade away as the ripples surrounding the question loses energy. Sometimes you have to throw another stone in the pond to make it heard again. Some say you can't erase stuff from the internet. But you can most definitely hide it. So, can you find the easter egg? Or if you have it. Well, an easy prize for you, amigo/a. Buhuu to the others. Let the Streisand Effect unfold. Hopefully, it will be found this time. Best of luck. The Discord group for anyone interested: https://discord.gg/JD8Je3v (ongoing discussions)"},
{"Title": "52% of YouTube videos live in 2010 have been deleted", "Author": "u/themadprogramer", "Content": "No content"},
{"Title": "I just need a a quiet always on network disk without all the usual NAS apps/features.", "Author": "u/largePenisLover", "Content": "The only thing missing from my setup is a simple NAS like 4tb volume that I can split into two partitions or shares, running on a very quiet always on device. I've been looking at NAS solutions for this but these do wayyyy to much feature wise and often have a weird walled gardenish approaches to things. Like this device that looks perfect, synology BeeDrive, apparently needs to be controlled VIA a synology run web service and has undeletable folders related to photo and video apps. Less idiot proof devices don't have this nonsense but are weird about backing them up. It all feels very bloatwary to a home-NAS newbie I JUST need a dumb networked file box WITHOUT all the apps and features QNAP and Synolgoy bring to the table. I need it to be JUST a network share that I can access from any device on my home network using the file browsers on those devices. No video managers, DSM's, BSM's, photo managers, or any of the usual stuff. Just files, in a file browser, that I can doubleclick to open. Jut a dumb, quiet , reliable, file box. I want to be able to backup this dumb file box by mapping it to a drive letter on the backup pc and then run my backup script. No bullshit with installing third party synology/qnap/whatever software, I just want to integrate it in my existing backup solutions and ignore what vendors want me to do. Does anyone know if such a dumb file box is available as an off the shelf product?"},
{"Title": "Apparently you CAN be banned by Lego", "Author": "u/xiyatumerica", "Content": "So, I've been downloading lego instructions every day for the past few months and usually it goes fine. The script I use has a timeout so I don't overload the server and usually it works. Pro tip: don't use \"replace all\" when editing your scripts. My working timeout is 150ms. Well, I did a replace all and that accidentally changed the timeout to 1ms (I was checking I/O on my drives). It ran fine for about an hour and then I started receiving a 403 error. Apparently Lego thought I was trying to DDOS the instructions site and ip blocked me. Interestingly, shop.lego.com still worked. I was blocked over the weekend, but on Monday I was able to download again. So now I can safely download lego instructions. TLDR; Accidentally changed the timeout from 150ms to 1ms in my script which attempted to DDOS Lego's servers"},
{"Title": "BOUNTY: $10 000USD. Remember me? Yes, I still have not found it. Please help me find the Whole videotape of Donald Trump on The Oprah Winfrey Show, 25th of April, 1988. Season 3, Episode 5 (a 60 min. episode with a few ads).", "Author": "u/trycoconutoil", "Content": "The Discord group for anyone interested: https://discord.gg/JD8Je3v (ongoing discussions) Saw this tape on Facebook back in 2015. Then, it vanished. I have been searching on and off since then. OFFICIAL BOUNTY : $1000USD TRANSCRIPTS: $1000USD Must be a copy of the original Journal Graphics transcript (Harpo productions bought their inventory) Sources that are relevant for the investigation: https://thetvdb.com/series/the-oprah-winfrey-show/episodes/6650339 (air date) Michael Cohen (trumps former lawyer) talked about the \"Catch and Kill\" method that they utilized several times. This is most likely why it's difficult (not impossible) to find the tape. Explained in short at 1:37:35 https://www.youtube.com/watch?v=v0e62bOKm8o&t=5855s&ab_channel=TheNewYorkTimes - \"[Cohen] was often described by the media as Trump's 'fixer'.\" - Wiki 3 min. part from the original video (there are more short videos out there and almost the full video in the discord group. unfortunately, the broadcasting got interrupted for about 10 min. about half a year after the original tape). NOTE! the video is no longer up but can be put up again if requested. https://www.reddit.com/r/politicalfactchecking/comments/4nt6iw/donald_trump_republicans_are_dumb_why_did_this/ (Frustrated Redditors) https://www.youtube.com/watch?v=AZtfSIgsH1o (some YouTube video that may not be important, but the comments on it are very relevant) http://www.oprah.com/own-oprahshow/what-donald-trump-told-oprah-about-his-presidential-hopes-video (an old link that might have shown the full episode in the past) interesting article (Interesting article on the disappearance of the video.) Another Reddit post by someone else asking for the same episode. A Reddit post about the missing episode and what OP of this post thinks about it . I guess this TikToker deleted her account but found a tweet about her talking about it. (had over 1 million views). An old website and      its resource page image I will give the person who can find it and send it to me $10 000USD (if you want). I'll give it through cryptocurrency, anonymous PayPal system, product(s), or other means. Several Redditors also support the bounty. That will also be included if they stick to their guns. NOTE! Ignore the images and fact-checker sites. Those are red herrings and so point towards something different from the year 1998. I want the entire video from The Oprah Winfrey Show, 25th of April, 1988, season 3, Episode 5, 60 minutes of the show. DM me here on Reddit or post it publically. No short vids or re-recordings that include interruptions. Simple. All of it. The show used to be quite popular; the entire content should be out there, internet, or stored. **Since some ask. Why do I want it?** I want it cause it has been suppressed and repressed on the internet, and various sites and people tell me what I saw and heard didn't happen, and it bothers me. And have found this exploration quite intriguing. I do not have any political intentions; I want the truth, sweetheart, for truth's sake. Great References: $1310 bounty post from last year (the second post) $300 bounty post from last year (the first post) Subreddit for this: https://www.reddit.com/r/BountyFindThisEpisode/ Had a website for this in the past that is no longer up, but viaprize made something similar If someone here wants to use it for broadcast or commerce's sake. DM me, and I can give you the contact info of the person responsible for the original OWN archive. Unfortunately, we could not get it because they don't distribute for \"Personal viewing.\" And even if you are big, they seem to be quite protective of this episode in particular. We have communicated with countless people. The journalist who first rediscovered and published the tape in 2015, multiple archivists, mindless scammers and haters that help bring energy into this, stubborn and helpful companies, Lost Media YouTubers (reluctant to make a video because of potential strike), and many more. Through the years. Traces of what was will naturally disappear. Forum posts, Reddit posts, articles, websites, comments, notes, talks, even TikToks, fade away as the ripples surrounding the question loses energy. Sometimes you have to throw another stone in the pond to make it heard again. Some say you can't erase stuff from the internet. But you can most definitely hide it. So, can you find the easter egg? Or if you have it. Well, an easy prize for you, amigo/a. Buhuu to the others. Let the Streisand Effect unfold. Hopefully, it will be found this time. Best of luck. The Discord group for anyone interested: https://discord.gg/JD8Je3v (ongoing discussions)"},
{"Title": "DAS Box - Can it be restarted remotely after a blackout?", "Author": "u/jfromeo", "Content": "I am in the search of a USB-C DAS to upgrade my 3,5\" HDD hoarding, as I do not have the space to accommodate a rack solution at the moment. I have found the MB-X10U31 from Fantec, which ticks all the features (10 bays, hot-swap, USB-C, etc). Quite similiar to the Mediasonic, IcyBox and Sabrent ones. https://fantecshop.de/p/fantec-mb-x10u31 https://preview.redd.it/das-box-can-it-be-restarted-remotely-after-a-blackout-v0-npzcdqysbb5d1.jpg But I have one feature I depend on, and I cannot deduce how it will behave, and the brand has not answered me either. I need to find out if the box can somehow be restarted remotely after a blackout , as I live 1.000km away from the server and there is no one to operate it. I have an UPS which can hold around 15 minutes, but sometimes the blackout are longer and I have a routine via NUT to power off all the devices if power is not restored within that period of time. I can WoL the main machine, but I do not know how could I power up the DAS box. Thanks in advance."},
{"Title": "YouTube Vanced has been discontinued", "Author": "u/FamousM1", "Content": "No content"},
{"Title": "Apparently you CAN be banned by Lego", "Author": "u/xiyatumerica", "Content": "So, I've been downloading lego instructions every day for the past few months and usually it goes fine. The script I use has a timeout so I don't overload the server and usually it works. Pro tip: don't use \"replace all\" when editing your scripts. My working timeout is 150ms. Well, I did a replace all and that accidentally changed the timeout to 1ms (I was checking I/O on my drives). It ran fine for about an hour and then I started receiving a 403 error. Apparently Lego thought I was trying to DDOS the instructions site and ip blocked me. Interestingly, shop.lego.com still worked. I was blocked over the weekend, but on Monday I was able to download again. So now I can safely download lego instructions. TLDR; Accidentally changed the timeout from 150ms to 1ms in my script which attempted to DDOS Lego's servers"},
{"Title": "Merging folders and files", "Author": "u/cl326", "Content": "I have a Windows 11 system with many folders, sub-folders, and files. I want to keep only one copy of each file based on its MD5 hash, including its filename and extension. In the end I want just one folder with all the unique files. Is there any easy way to do this? If not a specific app, I'm willing to write an app or script in Python, Ruby, or PowerShell. Any thoughts or suggestions?"},
{"Title": "Adobe Uses DMCA to Nuke Project That Keeps Flash Alive, Secure & Adware Free", "Author": "u/retrac1324", "Content": "No content"},
{"Title": "YouTube Vanced has been discontinued", "Author": "u/FamousM1", "Content": "No content"},
{"Title": "WD Ultrastar DC SN655 7.68 Tb for 196â‚¬ (211$) at a French retailer", "Author": "u/Nayko93", "Content": "I just found this deal, a 7.68TB NAS SSD for 196â‚¬, that's 0.02 cents per GB !I was thinking it could interest a few people here https://fr.shopping.rakuten.com/offer/buy/11626286226/disque-dur-interne-7-68to-wd-ultrastar-dc-sn655-2-5-u-3-pcie-4-0-nvme-wus5ea176esp7e3.html I'm not even sure it's real but it look like it, seller have a good reputation and the website is legitMaybe it's a price error, I don't know, but I've seen other posts talking about huge deal on those SSD so it seem this kind of deal is not impossible I tried to place a order to see if they ship outside of France and unfortunately they don't, so you either need to be in France, or use a friend or package forwarding service I'm almost tempted to buy 2 myself, since I'm looking to make my first home NASThat's a lot of money but for this much storage it would be dumb to pass it... but I know nothing of those SSD or NAS SSD in general, They need a PCI adapter to plug on a normal MB and I think I've read somewhere they heat up a lot ?I would be more comfortable dealing with normal sata or m.2 SSD... but such a deal.. I'm really tempted Update : It was legit, the seller confirmed my order and the price Unfortunately I had to cancel my order, because I can't find any U.3 pci card that can hold more than 1 SSD, and I need 3 off them, but I don't have 3 PCI left in my NAS"},
{"Title": "I thought this sub would appreciate this", "Author": "u/Needleroozer", "Content": "No content"},
{"Title": "Adobe Uses DMCA to Nuke Project That Keeps Flash Alive, Secure & Adware Free", "Author": "u/retrac1324", "Content": "No content"},
{"Title": "WD MyCloud Home once again screws me over", "Author": "u/techek", "Content": "My MyCloud Home has been updated to version 4.23.0 from December 4th 2023. I now know not become all excited and such, because almost every update since my purchase, has introduced one or more downgrades. Why remains unanswered. This time is no exception although the information about version 4.23.0 starts of with \"We constantly evaluate the customer experience ... wish to improve ... bring new and exciting features\" and so on. In reality it's a load of b......t, because this time they remove support for backing up videos and photos from Facebook. Previous update removed feature for editing photos in the app, before that remote backup with ElephantDrive disappeared, less connectivity and so on. The list of downgrades is impressive! And I feel used. Thanks for nothing, Western Digital! I cannot recommend their products for anyone anymore - those days are definitely over now."},
{"Title": "Archival Suggestion - Rooster Teeth/affiliated videos", "Author": "u/FairLadyVivi", "Content": "hello everyone! It has been recently announced that Rooster Teeth (but not their Roost podcast network) will be being shuttered by Warner Bros. No information has been made yet about what will happen to content produced/owned/hosted by RT. In the past during some smaller video purges I know that members on this sub were working on archiving RT content, so I wanted to raise a bit more awareness that more of their content may disappear in the impending days/months, to ensure that decades of their productions donâ€™t end up completely gone form the internet. I recall similar issues happening when Machinima shuttered and would hate to see the same with RT! :( My apologies if this isnâ€™t quite right for the sub, as more of a call to action than explicit discussion post, but I canâ€™t imagine Iâ€™m the only RT fan around wanting to make sure stuff doesnâ€™t disappear. I just donâ€™t have the setup to archive and hoard it all!"},
{"Title": "I thought this sub would appreciate this", "Author": "u/Needleroozer", "Content": "No content"},
{"Title": "irc.hackint.org", "Author": "u/GuillermoBotonio", "Content": "Anyone know why I would suddenly not be able to access irc.hackint.org? I was able to go on there for about 3 days then its not loading. The guy I talked to from the page says the server is loading for him. I can get on Hackint.org ok but not the IRC."},
{"Title": "Obsession and anxiety in one picture.", "Author": "u/string97bean", "Content": "No content"},
{"Title": "Archival Suggestion - Rooster Teeth/affiliated videos", "Author": "u/FairLadyVivi", "Content": "hello everyone! It has been recently announced that Rooster Teeth (but not their Roost podcast network) will be being shuttered by Warner Bros. No information has been made yet about what will happen to content produced/owned/hosted by RT. In the past during some smaller video purges I know that members on this sub were working on archiving RT content, so I wanted to raise a bit more awareness that more of their content may disappear in the impending days/months, to ensure that decades of their productions donâ€™t end up completely gone form the internet. I recall similar issues happening when Machinima shuttered and would hate to see the same with RT! :( My apologies if this isnâ€™t quite right for the sub, as more of a call to action than explicit discussion post, but I canâ€™t imagine Iâ€™m the only RT fan around wanting to make sure stuff doesnâ€™t disappear. I just donâ€™t have the setup to archive and hoard it all!"},
{"Title": "Are there any risks of using a USB Switch with a power supply on my PC and my DAS ?", "Author": "u/Yukinoooo", "Content": "My goal is to use my DAS on my PC and my media player (like a kind of Nvidia Shield, not Android or IOS, just the KODI interface under GNU/Linux) safely. I'm wondering if it's a good idea to use the two ports (PC and my media player) of a usb switch + a power supply and one port for my DAS ? If not, which solution should I use ?"},
{"Title": "API Clusterfuck! ~ Reddit said 'Fuck you, we don't care.' so here's where we stand.", "Author": "u/-Archivist", "Content": "Here's the bottom line.... Reddit exists to serve you ads, farm and sell your data. Reddit doesn't like or support you data hoarding. Reddit only cares if you're making them money. Reddit says one thing and does another. Reddit will strip and ban mods that aren't willing to bend over. We could go on, but you get the point... You have no say here, you lick the boots or fuck you. So the API is about to be shafted, many apps/bots will die, other things will change, you know what's up. But the more important thing directly related to the DataHoarding community is that Reddit has now very effectively killed Pushshift from a data hoarding perspective which was the only place you could get the most complete up-to-date Reddit data in bulk. Reddit has now taken control of Pushshift, had them delete bulk data downloads, prevents them releasing new dumps and limits PS API access to only mods Reddit approves of. r/DataHoarder moving forward.... We will continue to exist and operate as we have for as long as Reddit allows us to. We will promote alternatives for those of you who wish leave finding DataHoarder communities elsewhere. We will promote every project, tool and download that seeks to keep Reddit data available to both DataHoarders and researchers. We will continue to hoard. We will not hit any fucking delete buttons. New rule. 9. r/techsupport exists. We see a lot of basic vaguely dh related tech support questions here, we're going to be more actively removing these posts. Many of these also clearly break rule 1 as they're asked every other week. Sidebar updates. Historic Reddit Archives & Download Tools, Etc. #datahoarders @ The-Eye Discord (tag a helper, #support exists also) c/datahoarder @ lemmy.ml (experimental, we we're invited there) r/DataHorader 2013-2023 Searchable Archives Happy Hoarding."},
{"Title": "Obsession and anxiety in one picture.", "Author": "u/string97bean", "Content": "No content"},
{"Title": "RAID card no longer working, gives firmware error on POST", "Author": "u/Cyber_Akuma", "Content": "I have a LSI 9260-8i, this was actually reflashed from the original IRM ServeRAID M5014 it arrived as. It has since also been upgraded to the latest firmware from LSI/Broadcom, which is 12.15.0-0239 (also the card at boot identifies as BIOS version \"3.30.02.2 (Build June 17, 2014)\"). I am just simply using it in my Windows 10 system, not as part of a server or NAS. This card had been disconnected from my system for about 6-12 months, as well as it's drives, I recently reconnected everything in a new system. I have four HDDs in a RAID5, but had recently acquired the license key to enable RAID6 and had installed a 5th HDD in preparation for that. I had a few errors at first, but many things in the system were at first giving me errors so I didn't think much of it as I had performed several upgrades and changes at once. I eventually got everything booting properly and then opened up the RAID Windows management software (Version 17.05.02.01 at the time) and it seemed to be going ok. It was performing a Patrol Read on all of my drives and was recharging the battery (Though it claimed the battery was bad, but it was new, so I figured it needed to do a recharge cycle and re-learn to see it as good again). It said the Patrol Read was only going to take 10 minutes but I knew it was going to take hours. Halfway in, at about the 3-4 hour mark, the software completely stopped responding. I restarted it, and now the card was showing that absolutely nothing was installed to it. I performed a reboot and now I kept getting an error message during the card's initialization during post: LSI MegaRAID SAS-MFI BIOS Version 3.30.02.2 (Build June 17, 2014) Copyright(c) 2014 LSI Corporation Host Adapter Bus 5 Dev 0: F/W is in Fault State MFI Register State 0xF0010002 Adapter at Baseport is not responding No MegaRAID Adapter Installed The card then could be seen as installed, but the drives were not showing up, and the management software could not even tell a card was installed anymore. I tried updating to the latest management software (17.05.06.00) in case it might at least see there is a card installed, with that being it's own can of worms of expecting me to manually install OpenJDK and set the environment paths myself, it also just gets stuck loading the application. I tried MegaCLI, StorCLI, and MegaSCU (I admit I am not too familiar with managing this card through a CLI) but -v and -AdpAllInfo -aALL but they all returned nothing. I tried disconnecting the drives in case one had somehow become so fault during storage that it was crashing it, and no difference. I tried the only other PCI socket in my motherboard and it would not even boot then, guess that socket isn't even working with everything else installed in my system. I have no idea what to do now. The card refuses to work, claiming it's suffering some kind of firmware fault on POST, none of the software seems to even detect the presence of the card at all despite it physically showing up in Device Manager (although with an \"An I/O adapter hardware error has occurred.\" error) and HWiNFO, and I am not aware of any way I can attempt a force-reflash of the firmware in case it's a software issue (although I somehow doubt it) or of what else to try. And yes, I have a backup of my data."},
{"Title": "Google Photos Removing Unlimited Storage Option", "Author": "u/ThisAsYou", "Content": "No content"},
{"Title": "API Clusterfuck! ~ Reddit said 'Fuck you, we don't care.' so here's where we stand.", "Author": "u/-Archivist", "Content": "Here's the bottom line.... Reddit exists to serve you ads, farm and sell your data. Reddit doesn't like or support you data hoarding. Reddit only cares if you're making them money. Reddit says one thing and does another. Reddit will strip and ban mods that aren't willing to bend over. We could go on, but you get the point... You have no say here, you lick the boots or fuck you. So the API is about to be shafted, many apps/bots will die, other things will change, you know what's up. But the more important thing directly related to the DataHoarding community is that Reddit has now very effectively killed Pushshift from a data hoarding perspective which was the only place you could get the most complete up-to-date Reddit data in bulk. Reddit has now taken control of Pushshift, had them delete bulk data downloads, prevents them releasing new dumps and limits PS API access to only mods Reddit approves of. r/DataHoarder moving forward.... We will continue to exist and operate as we have for as long as Reddit allows us to. We will promote alternatives for those of you who wish leave finding DataHoarder communities elsewhere. We will promote every project, tool and download that seeks to keep Reddit data available to both DataHoarders and researchers. We will continue to hoard. We will not hit any fucking delete buttons. New rule. 9. r/techsupport exists. We see a lot of basic vaguely dh related tech support questions here, we're going to be more actively removing these posts. Many of these also clearly break rule 1 as they're asked every other week. Sidebar updates. Historic Reddit Archives & Download Tools, Etc. #datahoarders @ The-Eye Discord (tag a helper, #support exists also) c/datahoarder @ lemmy.ml (experimental, we we're invited there) r/DataHorader 2013-2023 Searchable Archives Happy Hoarding."},
{"Title": "IT BEGINS!", "Author": "u/pele4096", "Content": "No content"},
{"Title": "My off-site backup solution is finally coming to life thanks to the knowledge I got here", "Author": "u/olivercer", "Content": "No content"},
{"Title": "Google Photos Removing Unlimited Storage Option", "Author": "u/ThisAsYou", "Content": "No content"},
{"Title": "Just started VHS digitizing. Any tips for cable management?", "Author": "u/Applewoood", "Content": "No content"},
{"Title": "my rarbg magnet backup (268k)", "Author": "u/trilionaire07", "Content": "hey guys, i've been working on a rarbg scraping project for a few weeks now and i humbly offer the incompleted result of my labors. i think i have almost every show, but i have zero movies that aren't rarbg. https://github.com/2004content/rarbg/ edit: i'm trying to focus on this one. https://www.reddit.com/r/Piracy/comments/13wn554/my_rarbg_magnet_backup_268k/"},
{"Title": "My off-site backup solution is finally coming to life thanks to the knowledge I got here", "Author": "u/olivercer", "Content": "No content"},
{"Title": "Jonsbo announces the N5 case", "Author": "u/Ben4425", "Content": "Nascompares just posted a description of the new Jonsbo N5 case . It's a beast with space for up to 12 3.5\" drives and motherboard support for mITX, mATX, ATX, and E-ATX all with full-height PCIe cards."},
{"Title": "Perfect example of why I hoard", "Author": "u/nickdrones", "Content": "No content"},
{"Title": "my rarbg magnet backup (268k)", "Author": "u/trilionaire07", "Content": "hey guys, i've been working on a rarbg scraping project for a few weeks now and i humbly offer the incompleted result of my labors. i think i have almost every show, but i have zero movies that aren't rarbg. https://github.com/2004content/rarbg/ edit: i'm trying to focus on this one. https://www.reddit.com/r/Piracy/comments/13wn554/my_rarbg_magnet_backup_268k/"},
{"Title": "Newbie needing help", "Author": "u/Lochness_al", "Content": "I have 5 portable SSD and HDD plugged into my computer making my desk a mess I was wonding what would be a good external bay and HDD to buy. I want to run it with some kind of redundancy  (raid 1 most likely) It has my audio books, photos, and movies on it that I access through jelly fin I have thought about a nas before but I'm not that good with computers and just run jelly fin off my main PC."},
{"Title": "Dang it... barely bumped it. [Gore]", "Author": "u/Mahcks", "Content": "No content"},
{"Title": "Perfect example of why I hoard", "Author": "u/nickdrones", "Content": "No content"},
{"Title": "Need Help Setting Up Android and iPhone Backup Over WiFi to Windows PC", "Author": "u/BrokenScorp", "Content": "Hey everyone, Iâ€™m looking for some advice on setting up a reliable photo backup solution for my Android and iPhone over WiFi to my Windows PC. Currently, Iâ€™m using Resilio Sync, but it doesnâ€™t seem to work well with iOS. Does anyone have any recommendations for tools or software that can handle this more effectively? Ideally, Iâ€™d like something that can manage both platforms seamlessly and ensure my data is safely backed up to my PC. Thanks in advance for your suggestions!"},
{"Title": "RIP Samsung spinning rust. 12 of them, 12 years old, not a single bad sector and being retired to free up bays.", "Author": "u/cantanko", "Content": "No content"},
{"Title": "Dang it... barely bumped it. [Gore]", "Author": "u/Mahcks", "Content": "No content"},
{"Title": "Teracopy Cut/Paste", "Author": "u/rickydumpling", "Content": "Does anyone know how Teracopy handles cutting and pasting? I interrupted it while moving multiple folders with verify enabled and I canâ€™t find one of the original folders. Just wondering if this means that Teracopy finished pasting, verified, then deleted the original folder or something else."},
{"Title": "Michaels has a \"Photo and Craft Keeper\" box on sale for $12.59 that's the perfect size for HDDs", "Author": "u/Kontakr", "Content": "No content"},
{"Title": "RIP Samsung spinning rust. 12 of them, 12 years old, not a single bad sector and being retired to free up bays.", "Author": "u/cantanko", "Content": "No content"},
{"Title": "Powered down some HSGT workhorses today for not quite the last time - SMART stats are insane.", "Author": "u/platformterrestial", "Content": "We powered down an old storage server today and I took a few drives home partially to re-use and partially to check the SMART data. 94568 power on hours, 23 spin ups. Still cranking away happily. https://preview.redd.it/powered-down-some-hsgt-workhorses-today-for-not-quite-the-v0-8hl946jqa85d1.png"},
{"Title": "Shoutout to the University of Waterloo, for hosting 40TB of literal Linux ISOs", "Author": "u/thismustbetemporary", "Content": "Got a chuckle out of that. I discovered that the University of Waterloo is one of the few mirrors in Canada for Linux packages of various distros, and found their cool page showing all the content they're hosting. http://mirror.csclub.uwaterloo.ca/ I wish I could add my server onto Linux mirror lists like these, but it's nowhere near stable enough. Would do more harm than good for my server to be on that list. Thanks to the CS club for doing important work!!"},
{"Title": "Michaels has a \"Photo and Craft Keeper\" box on sale for $12.59 that's the perfect size for HDDs", "Author": "u/Kontakr", "Content": "No content"},
{"Title": "Portable (no-install) backup software for Windows?", "Author": "u/nefarious_bumpps", "Content": "I'm looking for a portable backup utility for Windows 10/11 desktops/laptops that can run from a USB drive without installation and perform disk image and full volume/partition backups and restores quickly to a USB external hdd/ssd. Free or reasonably-priced (under $150) commercial license. Any suggestions?"},
{"Title": "Studio Ghibli releases 400 images from eight movies free to download online", "Author": "u/hinghenry", "Content": "No content"},
{"Title": "Shoutout to the University of Waterloo, for hosting 40TB of literal Linux ISOs", "Author": "u/thismustbetemporary", "Content": "Got a chuckle out of that. I discovered that the University of Waterloo is one of the few mirrors in Canada for Linux packages of various distros, and found their cool page showing all the content they're hosting. http://mirror.csclub.uwaterloo.ca/ I wish I could add my server onto Linux mirror lists like these, but it's nowhere near stable enough. Would do more harm than good for my server to be on that list. Thanks to the CS club for doing important work!!"},
{"Title": "External Hard Drives", "Author": "u/Cravendale3", "Content": "Can someone explain why curryâ€™s sells a 1tb external hard drive for Â£143 and then some random online shop sells 128tb for Â£130? How can there but such a tb difference but price is practically the same, Iâ€™m having such a difficult time trying to pick the right drive because thereâ€™s so much variety in price and tb storage, all I want is around 2tbs for a decent price, confused ðŸ¤”"},
{"Title": "was not aware google scans all your private files for hate speech violations... Is this true and does this apply to all of google one storage?", "Author": "u/cooqieslayer", "Content": "No content"},
{"Title": "Studio Ghibli releases 400 images from eight movies free to download online", "Author": "u/hinghenry", "Content": "No content"},
{"Title": "Is there any news about Redfox AnyDVD being taken down?", "Author": "u/brandonyoung", "Content": "I use AnyDVD  as an easy way to make backup iso images of my DVDs and blurays.  I just noticed their forums now come up with an error page, and their main website  domain name no longer resolves to a site. http://www.redfox.bz/ Has their been any news of them going out of business or being taken down?  I tried Google search, but I haven't found anything. As an alternative, I can use makemkv to copy the folders, or just the video files. but it isn't the same.   I find backing up to iso image files easier for me to manage and organize my backups.  I tried using imgburn to copy the files to  an iso.  But VLC wouldn't play it.  Maybe I messed up somewhere in the settings  when creating the iso image from the backed up files?"},
{"Title": "40-year-old Data Hoarder is suing his parents after they threw out his cherished $29,000 porn collection", "Author": "u/therourke", "Content": "No content"},
{"Title": "was not aware google scans all your private files for hate speech violations... Is this true and does this apply to all of google one storage?", "Author": "u/cooqieslayer", "Content": "No content"},
{"Title": "Online NAS custom builders?", "Author": "u/Creepy_Finish1497", "Content": "Are there any online retailers that build NAS' for consumers?"},
{"Title": "Got about all of these for 50$ (Roughy 40tb in differnet sizes)", "Author": "Unknown author", "Content": "No content"},
{"Title": "40-year-old Data Hoarder is suing his parents after they threw out his cherished $29,000 porn collection", "Author": "u/therourke", "Content": "No content"},
{"Title": "Still not over the fact after close to 20 years Apple/iTunes/Apple TV lowers their movie trailer encode rate not just for new encodes/trailers but all prexisting encodes/trailers too", "Author": "u/ekos_640", "Content": "You can still download trailers from Apple though it's just a step or two more now - but they went from 9mbps on their movie trailer encodes since 2005-2006 until about 2022 when they redid everything on their backend and killed the old iTunes movie trailers site and moved it to Apple TV down to 6mbps Guess they wanted to start saving on storage/bandwith costs over the cost of the old files This wasn't just for new trailers from then on, but all preexisting trailers too :( Already have some 'older' movies I added to Plex I could have grabbed better trailers for then but didn't know I wanted the movies yet or changed my mind since I initially saw them :( We used to live in an age of opulence :("},
{"Title": "Got about all of these for 50$ (Roughy 40tb in differnet sizes)", "Author": "Unknown author", "Content": "No content"},
{"Title": "My favorite hard drive tracker + when will 30+ TB drives hit retail?", "Author": "u/chuckremes", "Content": "I will share some information and then ask a question too. I like this site for tracking hard drive prices. Last September I bought a bunch of 16 TB drives for $10.6 per terabyte and then I saw the price go as low as $9.40. For the 16+ TB sizes it's now been hovering at around $14/TB for months. Very concerning. https://diskprices.com/?locale=us&condition=new&disk_types=external_hdd,internal_hdd,external_ssd,internal_ssd So when can we expect to see those 30+ TB drives hit retail? The older articles I read all indicated 2024Q1 but we're about to exit 2024Q2 and I still haven't seen any. What's the latest buzz?"},
{"Title": "40Gb/s fiber in Finland is o so tempting for Torrents and Usenet...", "Author": "u/Rugta", "Content": "No content"},
{"Title": "Anyway to grab the name of a mega folder in the terminal? Megatools doesnt preserve the folder name", "Author": "u/NotAnADC", "Content": "Thank you for any help! Been trying to download mega files but hate that I can't save the folder names"},
{"Title": "Does anyone have a used harddrive to give for free?", "Author": "u/Alexander_Alexis", "Content": "Hello, i need to put emergently  my phone photos to my pc but my pc is full(200gb ssd) And i dont have moneys or a place near to buy a hard drive, does anyone have a very used hard drive? i need one only for the photos so not running games or big stuff like that"},
{"Title": "Starting data hording journey", "Author": "u/nlj1978", "Content": "After the help I got here on ripping my cd collection I'm jumping in the deep end. I'm putting together a media server. Have a HP z1 G5 tower workstation with i5-9500 on the way but still have decisions to make in terms of hardware and software. I'm finding mixed data on ECC ram support. Apparently the motherboards come in a couple variations. Once I confirm I believe I believe ECC is a better choice for this application. Still need to verify but I believe the board supports either 64 or 128gig.  That said how much ram should I use? Mobo has 2 m2 slots and 4 sata ports. Computer comes with a 256g nvme. I've read a little about using m2 Intel Optane drives for improved server performance. Anyone have any experience or input on this? Drives I'm still pondering. On the software side I am leaning towards TrueNas and  Jellyfin. Both seem relatively user friendly to someone with limited server knowledge. Thoughts?"},
{"Title": "I just need a a quiet always on network disk without all the usual NAS apps/features.", "Author": "u/largePenisLover", "Content": "The only thing missing from my setup is a simple NAS like 4tb volume that I can split into two partitions or shares, running on a very quiet always on device. I've been looking at NAS solutions for this but these do wayyyy to much feature wise and often have a weird walled gardenish approaches to things. Like this device that looks perfect, synology BeeDrive, apparently needs to be controlled VIA a synology run web service and has undeletable folders related to photo and video apps. Less idiot proof devices don't have this nonsense but are weird about backing them up. It all feels very bloatwary to a home-NAS newbie I JUST need a dumb networked file box WITHOUT all the apps and features QNAP and Synolgoy bring to the table. I need it to be JUST a network share that I can access from any device on my home network using the file browsers on those devices. No video managers, DSM's, BSM's, photo managers, or any of the usual stuff. Just files, in a file browser, that I can doubleclick to open. Jut a dumb, quiet , reliable, file box. I want to be able to backup this dumb file box by mapping it to a drive letter on the backup pc and then run my backup script. No bullshit with installing third party synology/qnap/whatever software, I just want to integrate it in my existing backup solutions and ignore what vendors want me to do. Does anyone know if such a dumb file box is available as an off the shelf product?"},
{"Title": "DAS Box - Can it be restarted remotely after a blackout?", "Author": "u/jfromeo", "Content": "I am in the search of a USB-C DAS to upgrade my 3,5\" HDD hoarding, as I do not have the space to accommodate a rack solution at the moment. I have found the MB-X10U31 from Fantec, which ticks all the features (10 bays, hot-swap, USB-C, etc). Quite similiar to the Mediasonic, IcyBox and Sabrent ones. https://fantecshop.de/p/fantec-mb-x10u31 https://preview.redd.it/das-box-can-it-be-restarted-remotely-after-a-blackout-v0-npzcdqysbb5d1.jpg But I have one feature I depend on, and I cannot deduce how it will behave, and the brand has not answered me either. I need to find out if the box can somehow be restarted remotely after a blackout , as I live 1.000km away from the server and there is no one to operate it. I have an UPS which can hold around 15 minutes, but sometimes the blackout are longer and I have a routine via NUT to power off all the devices if power is not restored within that period of time. I can WoL the main machine, but I do not know how could I power up the DAS box. Thanks in advance."},
{"Title": "Merging folders and files", "Author": "u/cl326", "Content": "I have a Windows 11 system with many folders, sub-folders, and files. I want to keep only one copy of each file based on its MD5 hash, including its filename and extension. In the end I want just one folder with all the unique files. Is there any easy way to do this? If not a specific app, I'm willing to write an app or script in Python, Ruby, or PowerShell. Any thoughts or suggestions?"},
{"Title": "WD Ultrastar DC SN655 7.68 Tb for 196â‚¬ (211$) at a French retailer", "Author": "u/Nayko93", "Content": "I just found this deal, a 7.68TB NAS SSD for 196â‚¬, that's 0.02 cents per GB !I was thinking it could interest a few people here https://fr.shopping.rakuten.com/offer/buy/11626286226/disque-dur-interne-7-68to-wd-ultrastar-dc-sn655-2-5-u-3-pcie-4-0-nvme-wus5ea176esp7e3.html I'm not even sure it's real but it look like it, seller have a good reputation and the website is legitMaybe it's a price error, I don't know, but I've seen other posts talking about huge deal on those SSD so it seem this kind of deal is not impossible I tried to place a order to see if they ship outside of France and unfortunately they don't, so you either need to be in France, or use a friend or package forwarding service I'm almost tempted to buy 2 myself, since I'm looking to make my first home NASThat's a lot of money but for this much storage it would be dumb to pass it... but I know nothing of those SSD or NAS SSD in general, They need a PCI adapter to plug on a normal MB and I think I've read somewhere they heat up a lot ?I would be more comfortable dealing with normal sata or m.2 SSD... but such a deal.. I'm really tempted Update : It was legit, the seller confirmed my order and the price Unfortunately I had to cancel my order, because I can't find any U.3 pci card that can hold more than 1 SSD, and I need 3 off them, but I don't have 3 PCI left in my NAS"},
{"Title": "WD MyCloud Home once again screws me over", "Author": "u/techek", "Content": "My MyCloud Home has been updated to version 4.23.0 from December 4th 2023. I now know not become all excited and such, because almost every update since my purchase, has introduced one or more downgrades. Why remains unanswered. This time is no exception although the information about version 4.23.0 starts of with \"We constantly evaluate the customer experience ... wish to improve ... bring new and exciting features\" and so on. In reality it's a load of b......t, because this time they remove support for backing up videos and photos from Facebook. Previous update removed feature for editing photos in the app, before that remote backup with ElephantDrive disappeared, less connectivity and so on. The list of downgrades is impressive! And I feel used. Thanks for nothing, Western Digital! I cannot recommend their products for anyone anymore - those days are definitely over now."},
{"Title": "irc.hackint.org", "Author": "u/GuillermoBotonio", "Content": "Anyone know why I would suddenly not be able to access irc.hackint.org? I was able to go on there for about 3 days then its not loading. The guy I talked to from the page says the server is loading for him. I can get on Hackint.org ok but not the IRC."},
{"Title": "Are there any risks of using a USB Switch with a power supply on my PC and my DAS ?", "Author": "u/Yukinoooo", "Content": "My goal is to use my DAS on my PC and my media player (like a kind of Nvidia Shield, not Android or IOS, just the KODI interface under GNU/Linux) safely. I'm wondering if it's a good idea to use the two ports (PC and my media player) of a usb switch + a power supply and one port for my DAS ? If not, which solution should I use ?"},
{"Title": "RAID card no longer working, gives firmware error on POST", "Author": "u/Cyber_Akuma", "Content": "I have a LSI 9260-8i, this was actually reflashed from the original IRM ServeRAID M5014 it arrived as. It has since also been upgraded to the latest firmware from LSI/Broadcom, which is 12.15.0-0239 (also the card at boot identifies as BIOS version \"3.30.02.2 (Build June 17, 2014)\"). I am just simply using it in my Windows 10 system, not as part of a server or NAS. This card had been disconnected from my system for about 6-12 months, as well as it's drives, I recently reconnected everything in a new system. I have four HDDs in a RAID5, but had recently acquired the license key to enable RAID6 and had installed a 5th HDD in preparation for that. I had a few errors at first, but many things in the system were at first giving me errors so I didn't think much of it as I had performed several upgrades and changes at once. I eventually got everything booting properly and then opened up the RAID Windows management software (Version 17.05.02.01 at the time) and it seemed to be going ok. It was performing a Patrol Read on all of my drives and was recharging the battery (Though it claimed the battery was bad, but it was new, so I figured it needed to do a recharge cycle and re-learn to see it as good again). It said the Patrol Read was only going to take 10 minutes but I knew it was going to take hours. Halfway in, at about the 3-4 hour mark, the software completely stopped responding. I restarted it, and now the card was showing that absolutely nothing was installed to it. I performed a reboot and now I kept getting an error message during the card's initialization during post: LSI MegaRAID SAS-MFI BIOS Version 3.30.02.2 (Build June 17, 2014) Copyright(c) 2014 LSI Corporation Host Adapter Bus 5 Dev 0: F/W is in Fault State MFI Register State 0xF0010002 Adapter at Baseport is not responding No MegaRAID Adapter Installed The card then could be seen as installed, but the drives were not showing up, and the management software could not even tell a card was installed anymore. I tried updating to the latest management software (17.05.06.00) in case it might at least see there is a card installed, with that being it's own can of worms of expecting me to manually install OpenJDK and set the environment paths myself, it also just gets stuck loading the application. I tried MegaCLI, StorCLI, and MegaSCU (I admit I am not too familiar with managing this card through a CLI) but -v and -AdpAllInfo -aALL but they all returned nothing. I tried disconnecting the drives in case one had somehow become so fault during storage that it was crashing it, and no difference. I tried the only other PCI socket in my motherboard and it would not even boot then, guess that socket isn't even working with everything else installed in my system. I have no idea what to do now. The card refuses to work, claiming it's suffering some kind of firmware fault on POST, none of the software seems to even detect the presence of the card at all despite it physically showing up in Device Manager (although with an \"An I/O adapter hardware error has occurred.\" error) and HWiNFO, and I am not aware of any way I can attempt a force-reflash of the firmware in case it's a software issue (although I somehow doubt it) or of what else to try. And yes, I have a backup of my data."},
{"Title": "IT BEGINS!", "Author": "u/pele4096", "Content": "No content"},
{"Title": "Just started VHS digitizing. Any tips for cable management?", "Author": "u/Applewoood", "Content": "No content"},
{"Title": "Jonsbo announces the N5 case", "Author": "u/Ben4425", "Content": "Nascompares just posted a description of the new Jonsbo N5 case . It's a beast with space for up to 12 3.5\" drives and motherboard support for mITX, mATX, ATX, and E-ATX all with full-height PCIe cards."},
{"Title": "Newbie needing help", "Author": "u/Lochness_al", "Content": "I have 5 portable SSD and HDD plugged into my computer making my desk a mess I was wonding what would be a good external bay and HDD to buy. I want to run it with some kind of redundancy  (raid 1 most likely) It has my audio books, photos, and movies on it that I access through jelly fin I have thought about a nas before but I'm not that good with computers and just run jelly fin off my main PC."},
{"Title": "Need Help Setting Up Android and iPhone Backup Over WiFi to Windows PC", "Author": "u/BrokenScorp", "Content": "Hey everyone, Iâ€™m looking for some advice on setting up a reliable photo backup solution for my Android and iPhone over WiFi to my Windows PC. Currently, Iâ€™m using Resilio Sync, but it doesnâ€™t seem to work well with iOS. Does anyone have any recommendations for tools or software that can handle this more effectively? Ideally, Iâ€™d like something that can manage both platforms seamlessly and ensure my data is safely backed up to my PC. Thanks in advance for your suggestions!"},
{"Title": "Teracopy Cut/Paste", "Author": "u/rickydumpling", "Content": "Does anyone know how Teracopy handles cutting and pasting? I interrupted it while moving multiple folders with verify enabled and I canâ€™t find one of the original folders. Just wondering if this means that Teracopy finished pasting, verified, then deleted the original folder or something else."},
{"Title": "Powered down some HSGT workhorses today for not quite the last time - SMART stats are insane.", "Author": "u/platformterrestial", "Content": "We powered down an old storage server today and I took a few drives home partially to re-use and partially to check the SMART data. 94568 power on hours, 23 spin ups. Still cranking away happily. https://preview.redd.it/powered-down-some-hsgt-workhorses-today-for-not-quite-the-v0-8hl946jqa85d1.png"},
{"Title": "Portable (no-install) backup software for Windows?", "Author": "u/nefarious_bumpps", "Content": "I'm looking for a portable backup utility for Windows 10/11 desktops/laptops that can run from a USB drive without installation and perform disk image and full volume/partition backups and restores quickly to a USB external hdd/ssd. Free or reasonably-priced (under $150) commercial license. Any suggestions?"},
{"Title": "External Hard Drives", "Author": "u/Cravendale3", "Content": "Can someone explain why curryâ€™s sells a 1tb external hard drive for Â£143 and then some random online shop sells 128tb for Â£130? How can there but such a tb difference but price is practically the same, Iâ€™m having such a difficult time trying to pick the right drive because thereâ€™s so much variety in price and tb storage, all I want is around 2tbs for a decent price, confused ðŸ¤”"},
{"Title": "Is there any news about Redfox AnyDVD being taken down?", "Author": "u/brandonyoung", "Content": "I use AnyDVD  as an easy way to make backup iso images of my DVDs and blurays.  I just noticed their forums now come up with an error page, and their main website  domain name no longer resolves to a site. http://www.redfox.bz/ Has their been any news of them going out of business or being taken down?  I tried Google search, but I haven't found anything. As an alternative, I can use makemkv to copy the folders, or just the video files. but it isn't the same.   I find backing up to iso image files easier for me to manage and organize my backups.  I tried using imgburn to copy the files to  an iso.  But VLC wouldn't play it.  Maybe I messed up somewhere in the settings  when creating the iso image from the backed up files?"},
{"Title": "Online NAS custom builders?", "Author": "u/Creepy_Finish1497", "Content": "Are there any online retailers that build NAS' for consumers?"},
{"Title": "Still not over the fact after close to 20 years Apple/iTunes/Apple TV lowers their movie trailer encode rate not just for new encodes/trailers but all prexisting encodes/trailers too", "Author": "u/ekos_640", "Content": "You can still download trailers from Apple though it's just a step or two more now - but they went from 9mbps on their movie trailer encodes since 2005-2006 until about 2022 when they redid everything on their backend and killed the old iTunes movie trailers site and moved it to Apple TV down to 6mbps Guess they wanted to start saving on storage/bandwith costs over the cost of the old files This wasn't just for new trailers from then on, but all preexisting trailers too :( Already have some 'older' movies I added to Plex I could have grabbed better trailers for then but didn't know I wanted the movies yet or changed my mind since I initially saw them :( We used to live in an age of opulence :("},
{"Title": "Need Honest Advice", "Author": "u/Substantial-Big8229", "Content": "For context, I left my gaming PC in a storage unit in a desert environment for nearly 2-3 years without ventilation, I'm concerned about potential data corruption or loss, especially regarding the SSD. I haven't powered it on, cleaned it, or updated its firmware during this time. What are the realistic chances of data corruption or loss, and how feasible is data recovery at this stage? (NOTE: Its consumer grade TLC SSD if that makes a difference.)"},
{"Title": "11 years ago I built a computer", "Author": "u/BinaryPatrickDev", "Content": "No content"},
{"Title": "Jumper pins on exos x18 14tb", "Author": "u/ctles", "Content": "Figured i might try here in that probably lots of people would have the 14tb exos here: What's the 4 jumper pins next to the SATA data for, see image below. I've looked through the main document i could find and asked a rep in seagate chat but he could only provide me what jumper pins may be for and his doc said the drive didn't have any jumper pins. But clearly they're there: https://www.kitguru.net/wp-content/uploads/2018/11/Seagate-Exos-X14-14TB-Hard-Drive-Review-on-KitGuru-SATA-Connectors.png"},
{"Title": "Big off-line database for recipes?", "Author": "u/CamT86", "Content": "I'm about to start compiling some recipes for offline use(ideally in a way that'll be easy to view on a tablet, either in its memory or from a home server) and i was wondering if maybe there was already something similar thats already been set up, that i could work from rather than start from scratch. I dont really have any bright ideas on how to do this. In 2 weeks i'll need to go to a remote cabin with really poor internet connection(old style satellite shared between 8 households, thats barely better than 56k... not the elon musk new stuff) and cook for a few families that are also staying out there for summer. I'm sure this time I could literally just bring a cookbook and maybe a few printouts for what i need, but id like to set something up for future use as well. They already have a small server set up with i think Kodi or plex that gets dragged back to civilization every few months for updates, so i figure i could just add whatever database file/system to that, to also be accessible to the person doing the cooking."},
{"Title": "Easy store external hard drive not mounting", "Author": "u/JessSerrano", "Content": "Hello! I have an Apple MacBook and I have the 2TB EasyStore external hard drive. I plug it in and sometimes it mounts, sometimes it doesnâ€™t. I have another external hard drive (Seagate) that mounts with no issues. I changed cords too and my EasyStore isnâ€™t mounting. Sometimes it does, though, which is odd. The light always turns on when I plug the cord into my Mac. Any suggestions on how to fix this? Thank you!"},
{"Title": "No media on window help.", "Author": "u/MisakaMisakaS100", "Content": "No content"},
{"Title": "Fast write speed small capacity USB Flash Drive", "Author": "u/Eidbanger", "Content": "Searching for a small capacity (due to expecting lower price) flash drive which has fast write speeds. I found a $32 Kingston DataTraveler Max 256 GB with up to 900 MB/s write but curious to hear if there's anything from 500 MB/s write for cheaper (~$10-20)? I'll be using this for creating bootable ISO images."},
{"Title": "Twitter is silently deleting some suspended accounts which has not been logged in for some time.", "Author": "u/cyberanakinvader", "Content": "I think I've made a disturbing discovery regarding Twitter where they have silently deleted some suspended accounts which has not been logged in for some time, including some of mine. Has anyone else encountered this issue recently?"},
{"Title": "Lib-gen question", "Author": "u/Hungry-Sentence-6722", "Content": "Iâ€™ve seen the direct download page for lib-gen sci-mag articles. I would like to understand how to convert all those files to the correct file name and extension. Libgen desktop is having issues with sourcing from mirrors.  I also donâ€™t see the point in one by one files.. any advice? The metadata must be in the sql file but many pdfâ€™s are just a few pages of a large book, I want to merge them all automatically."},
{"Title": "I don't trust raid 5", "Author": "u/obalobadik", "Content": "I have a 4-bay Asustor nas and 4x3TB HDD's. As for now I do not require more than 3TB raw storage and run the nas in raid 1 with only two drives. When I eventually want to increase my storage, I am sceptical to convert my volume to raid 5. Seeing the struggles involved with recovering data from an raid 5 array, I can't trust it as an option. I am more comfortable running two 2x3TB raid 1 array, since then I know that the data is formatted such that I can fetch it without rebuilding.  Even asustor themselves does not have a method of recovering the data from an raid 5 array. I have been using snapraid before, and that is has been feeling more safe, as  when a drive has failed, only the data on that drive has potentially been lost. Am I paranoid and recovering from raid 5 data loss not that troublesome? can you recover data from a single drive that is from a raid 5 array?"},
{"Title": "I'm running low on space in my custom built tower, what internal HDD's would you recommend?", "Author": "u/AVoraciousLatias", "Content": "Hi there, I have a 1 tb HDD and a 1 tb SDD. I'm looking to upgrade my HDD so I can store more games and programs on it, I'm looking for possibly a 4 tb or bigger so I won't have to worry about it any time soon. Do you have any suggestions for reliable HDDs that will last for a while? I'm looking for an internal that can be hooked up to the motherboard."},
{"Title": "Building a DIY JBOD", "Author": "u/OverlyBurntToast", "Content": "I have a bunch of old hard drives, and was wondering the best approach to putting them in some sort of DIY enclosure. My first thought was to just buy a bunch of the super cheap SATA to USB converters and plug that into a USB hub (speed is a low priority), but that wont be able to power the 3.5 inch drives I have. I was wondering if I could power the hard drives using a spare PC PSU I have, and then connect the hard drives via some sort of SATA DATA to USB or something. Does anyone have any ideas or hints, I would appreciate it greatly, as I am not too experienced in this field."},
{"Title": "Mid tower case vs 4U chassis for DIY NAS?", "Author": "u/Neurrone", "Content": "Hi, I'm currently looking to upgrade from an O11 air mini that only holds 4 drives because I realized it would be nice to have more drive slots and inadequate airflow causing some drives to run hot (ambient 28 C, idle 45, 51 under load). I made the mistake of choosing a case that only has as many drives that I had, so wanted to do it right this time. Use case is TrueNas with some apps, running on Ryzen 5700G to optimize for idle power consumption. I don't yet have a rack, but will be getting one when I move. Hence, I shortlisted some 4U rackmounts to place vertically first on a shelf: Logic case sc-4316 : US$350, 16 drives Sliger CX4712 : US$399, 12 drives. I'm leaning towards this These 4u cases aren't available for me locally - so estimated shipping to get either of those is an extra US$200, though I'm trying to find out if I can reduce that somehow. On the other hand,, a tower like the Fractal Meshify 2 (US$150) is much cheaper, and available to me locally, so I don't have to pay much for shipping. I could get 3 of these with the money needed to get the 4U. In this situation, are there enough advantages of those 4U cases to justify the extra expense? If I get a rack, I think I could place the Meshify 2 on its side on a shelf so that it only takes 6U. I'm new to rackmounted chassis, so these are the pros and cons I'm weighing, let me know if I missed anything. Pros: Support for redundant PSUs, hot swappable fans and drives for less uptime in case of a failure Support for larger motherboards: I currently have an ATX, not sure how useful support for larger form factors would be if I upgrade to lower end server boards Designed for rack mounting, smaller footprint Cons: Expensive Requires fan swaps for silence Higher drive temperatures? I assume drive temps would be worse compared to e.g, the meshify Won't fit my 160 mm cooler Thanks for reading!"},
{"Title": "My new Factory Recertified 22TB Seagate hard drives", "Author": "u/dropswisdom", "Content": "Sorry, had to re-post due to my previous post lacking in details. So here: got the drives here: https://www.ebay.com.sg/itm/204806952151 It was slightly cheaper when I got it. But it's one of the cheapest options you can find anywhere even now. The next step is 24TB drives which are not sold factory recertified as of yet, so they would cost.. about 50% more than this. I was concerned that some of the disks will arrive damaged, but as you can see it comes very well packaged, and they all passed testing. I put them into a Xpenology home made NAS. which is basically a itx motherboard inside a NAS case (jonsbo N1). It works like a charm. with one disk redundancy, I got about 80TB of fast storage. They cleared the hours on it when they recertified, but not the rest I think. But all the disks passed testing and are not making any suspicious noises or giving any issues. https://preview.redd.it/my-new-factory-recertified-22tb-seagate-hard-drives-v0-j8twcye0lx4d1.jpg Seagate 22TB Sata Hard Drives"},
{"Title": "Do you trust your backup enough to not use parity?", "Author": "u/19wolf", "Content": "If downtime isn't an issue, is there a reason to \"waste\"  space if you have a valid 3-2-1 backup? I'm using unraid so if a drive dies it's not like I need to restore my whole pool. How does everyone else feel about this?"},
{"Title": "Best Way to Convert Podcasts on Spotify?", "Author": "u/ShyGuyGaming76", "Content": "I listen to a lot of audio dramas, and with about two exceptions (The Magnus Archives and Kakos Industries) most of them are only on Spotify, so I'm not really able to archive them conveniently. What I'm doing right now is just screen recording them and converting it to an audio file. However, this is ungodly slow. Any better ways?"},
{"Title": "Still trying to get the hand of Virtual Dub for transcribing my tapes", "Author": "u/KWalthersArt", "Content": "I am posting this so I can get answers while I sleep, I hope I'm not breaking any rules about redundent or stupid questions. So the problems I have are. Langrith compression seems to follow a high instance of dropped frames if I record to long audio and video goes out of sync. I don't know how to import my files into davinci for recompression in better lossless codec. I don't know how to install a better codec in VDub Some tapes have a whine sound on them. Set up. Gaming PC Intel Core i7 Diamond VC500 usb card in a high level USB slot. Internal Hard drive then to external. Zenith XBR716 DVD-R VCR for play back. Most tapes are at SLP speed, I was a stupid teenager."},
{"Title": "Repairing drivesâ€¦", "Author": "u/TwoRight9509", "Content": "A decade ago youâ€™d send a broken drive to a place in California to get a repair / copy. How do we handle that now? Is there a best company to sent to?"},
{"Title": "Retrieving Photos From iPhoto Library In Time Machine Backup", "Author": "u/Akura_Awesome", "Content": "I have about a decadeâ€™s worth of Time Machine back ups across a dozen external drives, all with disparate photo libraries on them. Is there any sure fire way to pull these libraries out with metadata to throw on my NAS? Iâ€™ve tried to pull them in before but I get notifications about version and key mismatches, not to mention they can be a pain to locate. Any ideas are appreciated!"},
{"Title": "Help! I erased one of four RAID volumes set up with Disk Utilities", "Author": "u/peterinjapan", "Content": "Hi, I'm having an issue with a RAID with a volume I accidentally erased. My Yottamaster RAID started becoming unstable and was beeping all the time, so after giving up on it, I bought a Logitech enclosure and put the drives in, creating a RAID 5 volume with SoftRaid 8. After I got everything set up, of my disks disappeared from the new RAID, and as I was verifying the other three, another one gave me issues. So I decided to format the \"new\" disks and try again. I seem to have had one of the volumes in my old (backup) RAID selected, and erased it. The backup RAID was, I think, set up as two striped RAID volumes, which were then striped together to create a raid of that. This might not be the best way to do a RAID, but there was no option other than striped or mirrored disks, and at the time SoftRaid was not working with Apple Silicon at all. I have bought a large external disk to serve as a working disk to hopefully save my data. Can anyone suggest what will happen when I use the Restore option in Disk Utilities? If I can save the files and hopefully the folder structure, that will be good.  If there are any other tools besides Disk Utility I should be exploring, please let me know. Sending the disks to a data service isn't an option as I'm in rural Japan."},
{"Title": "Saw this and thought it belonged here.", "Author": "u/Mist17", "Content": "No content"},
{"Title": "Document Scanning Services", "Author": "u/JL4575", "Content": "Iâ€™m looking to scan a small collection of magazines, more than I want to manage manually. Each is stapled through the middle, not bound, and I would really like to not destroy the collection in getting them scanned. Can anyone recommend mail order services theyâ€™ve used?"},
{"Title": "G-RAID - Use built in HW RAID, Disk Utility RAID or something else?", "Author": "u/SuperChooch", "Content": "I have an older G-RAID device that is not supported by WD, attached to a Mac mini.    It is currently running G-RAID's hardware RAID at RAID 0 and I want to redeploy it for another purpose and make it RAID 1.  It is not mission critical and I'll be backing it up to another local location and to the cloud so if it craps out, it won't be the end of the world.  Should I continue to use G-RAID's RAID, should I use the Mac's Disk Utility RAID assistant or should I use something else?"},
{"Title": "2 part question.", "Author": "u/GrayManTech", "Content": "Hi. I have a bunch of DVDs that I want to convert to mp4 or whatever. I know I need a DVD player of some sort and software. I've seen here there are a couple of dvd players that don't need to be modified to use for this, but the posts I found were pretty old, have the available drives changed and if so which ones work out of the box? 2nd part, storage. Is it possible to setup a Nas or something to use wifi so everyone in my house can access it? My router has a USB port but from what I understand it's disabled by xfinity. Basically I'm looking for something that can hold both 2.5 and 3.5 hdd since I already have both."},
{"Title": "CD ripping for dummies", "Author": "u/nlj1978", "Content": "While spring cleaning I stumbled across a long forgotten CD collection. Rummaging through it nostalgia is kicking hard. So of course I'm going to have to rip it all so I can enjoy. Have ~200 CDs I've located to rip.  Also have a previous stash of ripped CDs on one of my home computers Googling a bit I'm reading about FLAC files being best to get the best rip and gather the music information. Exact Audio Copy along with a few others get mentioned alot for ripping. What's the most user friendly and effective ripping software for someone new to this? Doesn't have to be freeware, will pay for solid software. Is the FLAC file format relatively usable? For example I have a networked Onkyo receiver, would it be able to play those files from the network?"},
{"Title": "PSA: The US copyright Office is taking public comments on automated copyright filters till TONIGHT, and The Senate Judiciary Committee is considering the EARN-IT act THIS WEEK, which would create liability for user created content, and encourage/force file scanning-Encryption bans. Take action!", "Author": "u/jabberwockxeno", "Content": "This isn't strictly Datahoarding related, but obviously automated copyright filters lead to lost data and online content, and anything like the EARN-IT act which increases liability for user created content will lead to sites shutting down and lost content as well. I have cleared this with the Mod team. As stated, the US copyright office is taking information, feedback, and input on automated filters and detection systems on Copyright infringement, likely to suggest and support the proliferation of those systems. It will be holding a session on February 22nd, followed by consultation with industry groups. However,** it is also taking comments and input from the general public up till 11:59PM EST on February 8th (TODAY), with a online form, and is explicitly also open to hearing the downsides of such systems.** More information as well as links to the comment form (direct link here can be found here: https://www.eff.org/deeplinks/2022/02/tell-copyright-office-who-really-affected-filters . If you're not sure what exactly to write or aren't familar with how much of a trashfire these filters tend to be, [this[() article is a good starting overview, also from the EFF, and I have compiled some other examples here . The EARN-IT act is also being considered. Remember FOSTA-SESTA from a few years back, the legislation that was osteinbly to go after sex trafficking but really just led to dozens of major websites to shut down their legal adult content and actually made it harder for law enforcement, by their own subsequent admission, to go after actual abusers and traffickers; and which was decried by basically every Digital civil liberty and sex worker group? This is that but worse. It will remove Section 230 protection for wide swathes of websites tangentially connected to adult material, opening it up to liability over user created content, as well as creates liability for using encryption and \"advises\" websites to scan all uploaded content. More info can be found firstly here: https://www.eff.org/deeplinks/2022/02/its-back-senators-want-earn-it-bill-scan-all-online-messages and secondly here: https://www.techdirt.com/articles/20220203/18143448411/how-earn-it-act-is-significantly-more-dangerous-than-fosta.shtml and thirdly here: http://cyberlaw.stanford.edu/blog/2022/02/earn-it-act-back-and-it%E2%80%99s-more-dangerous-ever The EFF links I have provided include and link to tools to actually leave comments for the former, and contact your representatives in the Senate for the latter. If you aren't a US citizen, I encourage you to spread the word to those who are, and I technically don't see anything requiring you're a citizen for the Copyright Office form if you pick \"Anonymous\" If you're seeing this on the 9th or later, it is too late to comment on the Copyright Office stuff (timezone shenngians aside) but there IS still time to contact your senators about EARN IT! EDIT: EARN-IT is allegedly going up for vote on Thursday, the 10th, though this can change, so please still contact your represenatives! Please also follow the EFF and Fight for the Future, both regularly do advocacy and legal lobbying for digital rights and online privacy and against copyright maximalist (I am not affilated with either group, I am just a nerd who cares about this stuff way too much and both have consistently been the sources to follow for this sort of stuff)"},
{"Title": "GitHub Archive in Svalbard", "Author": "u/FreneticFrench", "Content": "No content"},
{"Title": "PSA: The US copyright Office is taking public comments on automated copyright filters till TONIGHT, and The Senate Judiciary Committee is considering the EARN-IT act THIS WEEK, which would create liability for user created content, and encourage/force file scanning-Encryption bans. Take action!", "Author": "u/jabberwockxeno", "Content": "This isn't strictly Datahoarding related, but obviously automated copyright filters lead to lost data and online content, and anything like the EARN-IT act which increases liability for user created content will lead to sites shutting down and lost content as well. I have cleared this with the Mod team. As stated, the US copyright office is taking information, feedback, and input on automated filters and detection systems on Copyright infringement, likely to suggest and support the proliferation of those systems. It will be holding a session on February 22nd, followed by consultation with industry groups. However,** it is also taking comments and input from the general public up till 11:59PM EST on February 8th (TODAY), with a online form, and is explicitly also open to hearing the downsides of such systems.** More information as well as links to the comment form (direct link here can be found here: https://www.eff.org/deeplinks/2022/02/tell-copyright-office-who-really-affected-filters . If you're not sure what exactly to write or aren't familar with how much of a trashfire these filters tend to be, [this[() article is a good starting overview, also from the EFF, and I have compiled some other examples here . The EARN-IT act is also being considered. Remember FOSTA-SESTA from a few years back, the legislation that was osteinbly to go after sex trafficking but really just led to dozens of major websites to shut down their legal adult content and actually made it harder for law enforcement, by their own subsequent admission, to go after actual abusers and traffickers; and which was decried by basically every Digital civil liberty and sex worker group? This is that but worse. It will remove Section 230 protection for wide swathes of websites tangentially connected to adult material, opening it up to liability over user created content, as well as creates liability for using encryption and \"advises\" websites to scan all uploaded content. More info can be found firstly here: https://www.eff.org/deeplinks/2022/02/its-back-senators-want-earn-it-bill-scan-all-online-messages and secondly here: https://www.techdirt.com/articles/20220203/18143448411/how-earn-it-act-is-significantly-more-dangerous-than-fosta.shtml and thirdly here: http://cyberlaw.stanford.edu/blog/2022/02/earn-it-act-back-and-it%E2%80%99s-more-dangerous-ever The EFF links I have provided include and link to tools to actually leave comments for the former, and contact your representatives in the Senate for the latter. If you aren't a US citizen, I encourage you to spread the word to those who are, and I technically don't see anything requiring you're a citizen for the Copyright Office form if you pick \"Anonymous\" If you're seeing this on the 9th or later, it is too late to comment on the Copyright Office stuff (timezone shenngians aside) but there IS still time to contact your senators about EARN IT! EDIT: EARN-IT is allegedly going up for vote on Thursday, the 10th, though this can change, so please still contact your represenatives! Please also follow the EFF and Fight for the Future, both regularly do advocacy and legal lobbying for digital rights and online privacy and against copyright maximalist (I am not affilated with either group, I am just a nerd who cares about this stuff way too much and both have consistently been the sources to follow for this sort of stuff)"},
{"Title": "I thought you guys would appreciate my messy, 10x DVD ripping machine I made out of an old DVD duplicator!", "Author": "u/pairofcrocs", "Content": "No content"},
{"Title": "GitHub Archive in Svalbard", "Author": "u/FreneticFrench", "Content": "No content"},
{"Title": "Well, Iâ€™m no mathematician but I think Iâ€™ll go with the 14TB. Best Buy Canada", "Author": "u/animatedhockeyfan", "Content": "No content"},
{"Title": "I thought you guys would appreciate my messy, 10x DVD ripping machine I made out of an old DVD duplicator!", "Author": "u/pairofcrocs", "Content": "No content"},
{"Title": "52% of YouTube videos live in 2010 have been deleted", "Author": "u/themadprogramer", "Content": "No content"},
{"Title": "Well, Iâ€™m no mathematician but I think Iâ€™ll go with the 14TB. Best Buy Canada", "Author": "u/animatedhockeyfan", "Content": "No content"},
{"Title": "BOUNTY: $10 000USD. Remember me? Yes, I still have not found it. Please help me find the Whole videotape of Donald Trump on The Oprah Winfrey Show, 25th of April, 1988. Season 3, Episode 5 (a 60 min. episode with a few ads).", "Author": "u/trycoconutoil", "Content": "The Discord group for anyone interested: https://discord.gg/JD8Je3v (ongoing discussions) Saw this tape on Facebook back in 2015. Then, it vanished. I have been searching on and off since then. OFFICIAL BOUNTY : $1000USD TRANSCRIPTS: $1000USD Must be a copy of the original Journal Graphics transcript (Harpo productions bought their inventory) Sources that are relevant for the investigation: https://thetvdb.com/series/the-oprah-winfrey-show/episodes/6650339 (air date) Michael Cohen (trumps former lawyer) talked about the \"Catch and Kill\" method that they utilized several times. This is most likely why it's difficult (not impossible) to find the tape. Explained in short at 1:37:35 https://www.youtube.com/watch?v=v0e62bOKm8o&t=5855s&ab_channel=TheNewYorkTimes - \"[Cohen] was often described by the media as Trump's 'fixer'.\" - Wiki 3 min. part from the original video (there are more short videos out there and almost the full video in the discord group. unfortunately, the broadcasting got interrupted for about 10 min. about half a year after the original tape). NOTE! the video is no longer up but can be put up again if requested. https://www.reddit.com/r/politicalfactchecking/comments/4nt6iw/donald_trump_republicans_are_dumb_why_did_this/ (Frustrated Redditors) https://www.youtube.com/watch?v=AZtfSIgsH1o (some YouTube video that may not be important, but the comments on it are very relevant) http://www.oprah.com/own-oprahshow/what-donald-trump-told-oprah-about-his-presidential-hopes-video (an old link that might have shown the full episode in the past) interesting article (Interesting article on the disappearance of the video.) Another Reddit post by someone else asking for the same episode. A Reddit post about the missing episode and what OP of this post thinks about it . I guess this TikToker deleted her account but found a tweet about her talking about it. (had over 1 million views). An old website and      its resource page image I will give the person who can find it and send it to me $10 000USD (if you want). I'll give it through cryptocurrency, anonymous PayPal system, product(s), or other means. Several Redditors also support the bounty. That will also be included if they stick to their guns. NOTE! Ignore the images and fact-checker sites. Those are red herrings and so point towards something different from the year 1998. I want the entire video from The Oprah Winfrey Show, 25th of April, 1988, season 3, Episode 5, 60 minutes of the show. DM me here on Reddit or post it publically. No short vids or re-recordings that include interruptions. Simple. All of it. The show used to be quite popular; the entire content should be out there, internet, or stored. **Since some ask. Why do I want it?** I want it cause it has been suppressed and repressed on the internet, and various sites and people tell me what I saw and heard didn't happen, and it bothers me. And have found this exploration quite intriguing. I do not have any political intentions; I want the truth, sweetheart, for truth's sake. Great References: $1310 bounty post from last year (the second post) $300 bounty post from last year (the first post) Subreddit for this: https://www.reddit.com/r/BountyFindThisEpisode/ Had a website for this in the past that is no longer up, but viaprize made something similar If someone here wants to use it for broadcast or commerce's sake. DM me, and I can give you the contact info of the person responsible for the original OWN archive. Unfortunately, we could not get it because they don't distribute for \"Personal viewing.\" And even if you are big, they seem to be quite protective of this episode in particular. We have communicated with countless people. The journalist who first rediscovered and published the tape in 2015, multiple archivists, mindless scammers and haters that help bring energy into this, stubborn and helpful companies, Lost Media YouTubers (reluctant to make a video because of potential strike), and many more. Through the years. Traces of what was will naturally disappear. Forum posts, Reddit posts, articles, websites, comments, notes, talks, even TikToks, fade away as the ripples surrounding the question loses energy. Sometimes you have to throw another stone in the pond to make it heard again. Some say you can't erase stuff from the internet. But you can most definitely hide it. So, can you find the easter egg? Or if you have it. Well, an easy prize for you, amigo/a. Buhuu to the others. Let the Streisand Effect unfold. Hopefully, it will be found this time. Best of luck. The Discord group for anyone interested: https://discord.gg/JD8Je3v (ongoing discussions)"},
{"Title": "52% of YouTube videos live in 2010 have been deleted", "Author": "u/themadprogramer", "Content": "No content"},
{"Title": "Apparently you CAN be banned by Lego", "Author": "u/xiyatumerica", "Content": "So, I've been downloading lego instructions every day for the past few months and usually it goes fine. The script I use has a timeout so I don't overload the server and usually it works. Pro tip: don't use \"replace all\" when editing your scripts. My working timeout is 150ms. Well, I did a replace all and that accidentally changed the timeout to 1ms (I was checking I/O on my drives). It ran fine for about an hour and then I started receiving a 403 error. Apparently Lego thought I was trying to DDOS the instructions site and ip blocked me. Interestingly, shop.lego.com still worked. I was blocked over the weekend, but on Monday I was able to download again. So now I can safely download lego instructions. TLDR; Accidentally changed the timeout from 150ms to 1ms in my script which attempted to DDOS Lego's servers"},
{"Title": "BOUNTY: $10 000USD. Remember me? Yes, I still have not found it. Please help me find the Whole videotape of Donald Trump on The Oprah Winfrey Show, 25th of April, 1988. Season 3, Episode 5 (a 60 min. episode with a few ads).", "Author": "u/trycoconutoil", "Content": "The Discord group for anyone interested: https://discord.gg/JD8Je3v (ongoing discussions) Saw this tape on Facebook back in 2015. Then, it vanished. I have been searching on and off since then. OFFICIAL BOUNTY : $1000USD TRANSCRIPTS: $1000USD Must be a copy of the original Journal Graphics transcript (Harpo productions bought their inventory) Sources that are relevant for the investigation: https://thetvdb.com/series/the-oprah-winfrey-show/episodes/6650339 (air date) Michael Cohen (trumps former lawyer) talked about the \"Catch and Kill\" method that they utilized several times. This is most likely why it's difficult (not impossible) to find the tape. Explained in short at 1:37:35 https://www.youtube.com/watch?v=v0e62bOKm8o&t=5855s&ab_channel=TheNewYorkTimes - \"[Cohen] was often described by the media as Trump's 'fixer'.\" - Wiki 3 min. part from the original video (there are more short videos out there and almost the full video in the discord group. unfortunately, the broadcasting got interrupted for about 10 min. about half a year after the original tape). NOTE! the video is no longer up but can be put up again if requested. https://www.reddit.com/r/politicalfactchecking/comments/4nt6iw/donald_trump_republicans_are_dumb_why_did_this/ (Frustrated Redditors) https://www.youtube.com/watch?v=AZtfSIgsH1o (some YouTube video that may not be important, but the comments on it are very relevant) http://www.oprah.com/own-oprahshow/what-donald-trump-told-oprah-about-his-presidential-hopes-video (an old link that might have shown the full episode in the past) interesting article (Interesting article on the disappearance of the video.) Another Reddit post by someone else asking for the same episode. A Reddit post about the missing episode and what OP of this post thinks about it . I guess this TikToker deleted her account but found a tweet about her talking about it. (had over 1 million views). An old website and      its resource page image I will give the person who can find it and send it to me $10 000USD (if you want). I'll give it through cryptocurrency, anonymous PayPal system, product(s), or other means. Several Redditors also support the bounty. That will also be included if they stick to their guns. NOTE! Ignore the images and fact-checker sites. Those are red herrings and so point towards something different from the year 1998. I want the entire video from The Oprah Winfrey Show, 25th of April, 1988, season 3, Episode 5, 60 minutes of the show. DM me here on Reddit or post it publically. No short vids or re-recordings that include interruptions. Simple. All of it. The show used to be quite popular; the entire content should be out there, internet, or stored. **Since some ask. Why do I want it?** I want it cause it has been suppressed and repressed on the internet, and various sites and people tell me what I saw and heard didn't happen, and it bothers me. And have found this exploration quite intriguing. I do not have any political intentions; I want the truth, sweetheart, for truth's sake. Great References: $1310 bounty post from last year (the second post) $300 bounty post from last year (the first post) Subreddit for this: https://www.reddit.com/r/BountyFindThisEpisode/ Had a website for this in the past that is no longer up, but viaprize made something similar If someone here wants to use it for broadcast or commerce's sake. DM me, and I can give you the contact info of the person responsible for the original OWN archive. Unfortunately, we could not get it because they don't distribute for \"Personal viewing.\" And even if you are big, they seem to be quite protective of this episode in particular. We have communicated with countless people. The journalist who first rediscovered and published the tape in 2015, multiple archivists, mindless scammers and haters that help bring energy into this, stubborn and helpful companies, Lost Media YouTubers (reluctant to make a video because of potential strike), and many more. Through the years. Traces of what was will naturally disappear. Forum posts, Reddit posts, articles, websites, comments, notes, talks, even TikToks, fade away as the ripples surrounding the question loses energy. Sometimes you have to throw another stone in the pond to make it heard again. Some say you can't erase stuff from the internet. But you can most definitely hide it. So, can you find the easter egg? Or if you have it. Well, an easy prize for you, amigo/a. Buhuu to the others. Let the Streisand Effect unfold. Hopefully, it will be found this time. Best of luck. The Discord group for anyone interested: https://discord.gg/JD8Je3v (ongoing discussions)"},
{"Title": "YouTube Vanced has been discontinued", "Author": "u/FamousM1", "Content": "No content"},
{"Title": "Apparently you CAN be banned by Lego", "Author": "u/xiyatumerica", "Content": "So, I've been downloading lego instructions every day for the past few months and usually it goes fine. The script I use has a timeout so I don't overload the server and usually it works. Pro tip: don't use \"replace all\" when editing your scripts. My working timeout is 150ms. Well, I did a replace all and that accidentally changed the timeout to 1ms (I was checking I/O on my drives). It ran fine for about an hour and then I started receiving a 403 error. Apparently Lego thought I was trying to DDOS the instructions site and ip blocked me. Interestingly, shop.lego.com still worked. I was blocked over the weekend, but on Monday I was able to download again. So now I can safely download lego instructions. TLDR; Accidentally changed the timeout from 150ms to 1ms in my script which attempted to DDOS Lego's servers"},
{"Title": "Adobe Uses DMCA to Nuke Project That Keeps Flash Alive, Secure & Adware Free", "Author": "u/retrac1324", "Content": "No content"},
{"Title": "YouTube Vanced has been discontinued", "Author": "u/FamousM1", "Content": "No content"},
{"Title": "I thought this sub would appreciate this", "Author": "u/Needleroozer", "Content": "No content"},
{"Title": "Adobe Uses DMCA to Nuke Project That Keeps Flash Alive, Secure & Adware Free", "Author": "u/retrac1324", "Content": "No content"},
{"Title": "Archival Suggestion - Rooster Teeth/affiliated videos", "Author": "u/FairLadyVivi", "Content": "hello everyone! It has been recently announced that Rooster Teeth (but not their Roost podcast network) will be being shuttered by Warner Bros. No information has been made yet about what will happen to content produced/owned/hosted by RT. In the past during some smaller video purges I know that members on this sub were working on archiving RT content, so I wanted to raise a bit more awareness that more of their content may disappear in the impending days/months, to ensure that decades of their productions donâ€™t end up completely gone form the internet. I recall similar issues happening when Machinima shuttered and would hate to see the same with RT! :( My apologies if this isnâ€™t quite right for the sub, as more of a call to action than explicit discussion post, but I canâ€™t imagine Iâ€™m the only RT fan around wanting to make sure stuff doesnâ€™t disappear. I just donâ€™t have the setup to archive and hoard it all!"},
{"Title": "I thought this sub would appreciate this", "Author": "u/Needleroozer", "Content": "No content"},
{"Title": "Obsession and anxiety in one picture.", "Author": "u/string97bean", "Content": "No content"},
{"Title": "Archival Suggestion - Rooster Teeth/affiliated videos", "Author": "u/FairLadyVivi", "Content": "hello everyone! It has been recently announced that Rooster Teeth (but not their Roost podcast network) will be being shuttered by Warner Bros. No information has been made yet about what will happen to content produced/owned/hosted by RT. In the past during some smaller video purges I know that members on this sub were working on archiving RT content, so I wanted to raise a bit more awareness that more of their content may disappear in the impending days/months, to ensure that decades of their productions donâ€™t end up completely gone form the internet. I recall similar issues happening when Machinima shuttered and would hate to see the same with RT! :( My apologies if this isnâ€™t quite right for the sub, as more of a call to action than explicit discussion post, but I canâ€™t imagine Iâ€™m the only RT fan around wanting to make sure stuff doesnâ€™t disappear. I just donâ€™t have the setup to archive and hoard it all!"},
{"Title": "API Clusterfuck! ~ Reddit said 'Fuck you, we don't care.' so here's where we stand.", "Author": "u/-Archivist", "Content": "Here's the bottom line.... Reddit exists to serve you ads, farm and sell your data. Reddit doesn't like or support you data hoarding. Reddit only cares if you're making them money. Reddit says one thing and does another. Reddit will strip and ban mods that aren't willing to bend over. We could go on, but you get the point... You have no say here, you lick the boots or fuck you. So the API is about to be shafted, many apps/bots will die, other things will change, you know what's up. But the more important thing directly related to the DataHoarding community is that Reddit has now very effectively killed Pushshift from a data hoarding perspective which was the only place you could get the most complete up-to-date Reddit data in bulk. Reddit has now taken control of Pushshift, had them delete bulk data downloads, prevents them releasing new dumps and limits PS API access to only mods Reddit approves of. r/DataHoarder moving forward.... We will continue to exist and operate as we have for as long as Reddit allows us to. We will promote alternatives for those of you who wish leave finding DataHoarder communities elsewhere. We will promote every project, tool and download that seeks to keep Reddit data available to both DataHoarders and researchers. We will continue to hoard. We will not hit any fucking delete buttons. New rule. 9. r/techsupport exists. We see a lot of basic vaguely dh related tech support questions here, we're going to be more actively removing these posts. Many of these also clearly break rule 1 as they're asked every other week. Sidebar updates. Historic Reddit Archives & Download Tools, Etc. #datahoarders @ The-Eye Discord (tag a helper, #support exists also) c/datahoarder @ lemmy.ml (experimental, we we're invited there) r/DataHorader 2013-2023 Searchable Archives Happy Hoarding."},
{"Title": "Obsession and anxiety in one picture.", "Author": "u/string97bean", "Content": "No content"},
{"Title": "Google Photos Removing Unlimited Storage Option", "Author": "u/ThisAsYou", "Content": "No content"},
{"Title": "API Clusterfuck! ~ Reddit said 'Fuck you, we don't care.' so here's where we stand.", "Author": "u/-Archivist", "Content": "Here's the bottom line.... Reddit exists to serve you ads, farm and sell your data. Reddit doesn't like or support you data hoarding. Reddit only cares if you're making them money. Reddit says one thing and does another. Reddit will strip and ban mods that aren't willing to bend over. We could go on, but you get the point... You have no say here, you lick the boots or fuck you. So the API is about to be shafted, many apps/bots will die, other things will change, you know what's up. But the more important thing directly related to the DataHoarding community is that Reddit has now very effectively killed Pushshift from a data hoarding perspective which was the only place you could get the most complete up-to-date Reddit data in bulk. Reddit has now taken control of Pushshift, had them delete bulk data downloads, prevents them releasing new dumps and limits PS API access to only mods Reddit approves of. r/DataHoarder moving forward.... We will continue to exist and operate as we have for as long as Reddit allows us to. We will promote alternatives for those of you who wish leave finding DataHoarder communities elsewhere. We will promote every project, tool and download that seeks to keep Reddit data available to both DataHoarders and researchers. We will continue to hoard. We will not hit any fucking delete buttons. New rule. 9. r/techsupport exists. We see a lot of basic vaguely dh related tech support questions here, we're going to be more actively removing these posts. Many of these also clearly break rule 1 as they're asked every other week. Sidebar updates. Historic Reddit Archives & Download Tools, Etc. #datahoarders @ The-Eye Discord (tag a helper, #support exists also) c/datahoarder @ lemmy.ml (experimental, we we're invited there) r/DataHorader 2013-2023 Searchable Archives Happy Hoarding."},
{"Title": "My off-site backup solution is finally coming to life thanks to the knowledge I got here", "Author": "u/olivercer", "Content": "No content"},
{"Title": "Google Photos Removing Unlimited Storage Option", "Author": "u/ThisAsYou", "Content": "No content"},
{"Title": "my rarbg magnet backup (268k)", "Author": "u/trilionaire07", "Content": "hey guys, i've been working on a rarbg scraping project for a few weeks now and i humbly offer the incompleted result of my labors. i think i have almost every show, but i have zero movies that aren't rarbg. https://github.com/2004content/rarbg/ edit: i'm trying to focus on this one. https://www.reddit.com/r/Piracy/comments/13wn554/my_rarbg_magnet_backup_268k/"},
{"Title": "My off-site backup solution is finally coming to life thanks to the knowledge I got here", "Author": "u/olivercer", "Content": "No content"},
{"Title": "Perfect example of why I hoard", "Author": "u/nickdrones", "Content": "No content"},
{"Title": "my rarbg magnet backup (268k)", "Author": "u/trilionaire07", "Content": "hey guys, i've been working on a rarbg scraping project for a few weeks now and i humbly offer the incompleted result of my labors. i think i have almost every show, but i have zero movies that aren't rarbg. https://github.com/2004content/rarbg/ edit: i'm trying to focus on this one. https://www.reddit.com/r/Piracy/comments/13wn554/my_rarbg_magnet_backup_268k/"},
{"Title": "Dang it... barely bumped it. [Gore]", "Author": "u/Mahcks", "Content": "No content"},
{"Title": "Perfect example of why I hoard", "Author": "u/nickdrones", "Content": "No content"},
{"Title": "RIP Samsung spinning rust. 12 of them, 12 years old, not a single bad sector and being retired to free up bays.", "Author": "u/cantanko", "Content": "No content"},
{"Title": "Dang it... barely bumped it. [Gore]", "Author": "u/Mahcks", "Content": "No content"},
{"Title": "Michaels has a \"Photo and Craft Keeper\" box on sale for $12.59 that's the perfect size for HDDs", "Author": "u/Kontakr", "Content": "No content"},
{"Title": "RIP Samsung spinning rust. 12 of them, 12 years old, not a single bad sector and being retired to free up bays.", "Author": "u/cantanko", "Content": "No content"},
{"Title": "Shoutout to the University of Waterloo, for hosting 40TB of literal Linux ISOs", "Author": "u/thismustbetemporary", "Content": "Got a chuckle out of that. I discovered that the University of Waterloo is one of the few mirrors in Canada for Linux packages of various distros, and found their cool page showing all the content they're hosting. http://mirror.csclub.uwaterloo.ca/ I wish I could add my server onto Linux mirror lists like these, but it's nowhere near stable enough. Would do more harm than good for my server to be on that list. Thanks to the CS club for doing important work!!"},
{"Title": "Michaels has a \"Photo and Craft Keeper\" box on sale for $12.59 that's the perfect size for HDDs", "Author": "u/Kontakr", "Content": "No content"},
{"Title": "Studio Ghibli releases 400 images from eight movies free to download online", "Author": "u/hinghenry", "Content": "No content"},
{"Title": "Shoutout to the University of Waterloo, for hosting 40TB of literal Linux ISOs", "Author": "u/thismustbetemporary", "Content": "Got a chuckle out of that. I discovered that the University of Waterloo is one of the few mirrors in Canada for Linux packages of various distros, and found their cool page showing all the content they're hosting. http://mirror.csclub.uwaterloo.ca/ I wish I could add my server onto Linux mirror lists like these, but it's nowhere near stable enough. Would do more harm than good for my server to be on that list. Thanks to the CS club for doing important work!!"},
{"Title": "was not aware google scans all your private files for hate speech violations... Is this true and does this apply to all of google one storage?", "Author": "u/cooqieslayer", "Content": "No content"},
{"Title": "Studio Ghibli releases 400 images from eight movies free to download online", "Author": "u/hinghenry", "Content": "No content"},
{"Title": "40-year-old Data Hoarder is suing his parents after they threw out his cherished $29,000 porn collection", "Author": "u/therourke", "Content": "No content"},
{"Title": "was not aware google scans all your private files for hate speech violations... Is this true and does this apply to all of google one storage?", "Author": "u/cooqieslayer", "Content": "No content"},
{"Title": "Got about all of these for 50$ (Roughy 40tb in differnet sizes)", "Author": "Unknown author", "Content": "No content"},
{"Title": "40-year-old Data Hoarder is suing his parents after they threw out his cherished $29,000 porn collection", "Author": "u/therourke", "Content": "No content"},
{"Title": "Had to add a second sata card and upgrade to a 1600 watt power supply because spinning up 17 drives was too much for my poor 900 watt...", "Author": "u/timekillerjay", "Content": "No content"},
{"Title": "Got about all of these for 50$ (Roughy 40tb in differnet sizes)", "Author": "Unknown author", "Content": "No content"},
{"Title": "Lana Del Rey says laptop containing her new album was stolen. This is why you back up your data peopleâ€¦", "Author": "Unknown author", "Content": "No content"},
{"Title": "Had to add a second sata card and upgrade to a 1600 watt power supply because spinning up 17 drives was too much for my poor 900 watt...", "Author": "u/timekillerjay", "Content": "No content"},
{"Title": "Thought you guys would enjoy, this is a 20TB ssd we are building at work. All in a 2.5\" formfactor. There's so many NAND chips that we have to make a pcb that folds in half.", "Author": "u/isonotlikethat", "Content": "No content"},
{"Title": "Lana Del Rey says laptop containing her new album was stolen. This is why you back up your data peopleâ€¦", "Author": "Unknown author", "Content": "No content"},
{"Title": "Is $92 per lb a good price?", "Author": "u/FragileRasputin", "Content": "No content"},
{"Title": "Thought you guys would enjoy, this is a 20TB ssd we are building at work. All in a 2.5\" formfactor. There's so many NAND chips that we have to make a pcb that folds in half.", "Author": "u/isonotlikethat", "Content": "No content"},
{"Title": "Protect yourselves out there - NEW Drive from Amazon with 15257 Power On Hours", "Author": "u/TheWoodser", "Content": "No content"},
{"Title": "Is $92 per lb a good price?", "Author": "u/FragileRasputin", "Content": "No content"},
{"Title": "Data hoarding also means having bigger hard drives, right?", "Author": "u/VulturE", "Content": "No content"},
{"Title": "Protect yourselves out there - NEW Drive from Amazon with 15257 Power On Hours", "Author": "u/TheWoodser", "Content": "No content"},
{"Title": "I did the math, realized that this 200 Disc CD player I got for the office can only store 130GB of audio, and I'm sad now.", "Author": "u/AshleyUncia", "Content": "No content"},
{"Title": "Data hoarding also means having bigger hard drives, right?", "Author": "u/VulturE", "Content": "No content"},
{"Title": "Which of you fucker's did this.", "Author": "u/linuxcommunist", "Content": "No content"},
{"Title": "I did the math, realized that this 200 Disc CD player I got for the office can only store 130GB of audio, and I'm sad now.", "Author": "u/AshleyUncia", "Content": "No content"},
{"Title": "Inventor of cassette tape Lou Ottens passes away at 94", "Author": "u/themadprogramer", "Content": "No content"},
{"Title": "Which of you fucker's did this.", "Author": "u/linuxcommunist", "Content": "No content"},
{"Title": "100TB! Just got to 100tb of storage on my server!", "Author": "u/mbailey5", "Content": "No content"},
{"Title": "Inventor of cassette tape Lou Ottens passes away at 94", "Author": "u/themadprogramer", "Content": "No content"},
{"Title": "Saying goodbye to a few fallen soldiers", "Author": "u/djlspider", "Content": "No content"},
{"Title": "100TB! Just got to 100tb of storage on my server!", "Author": "u/mbailey5", "Content": "No content"},
{"Title": "Was ticketing clearance items at work and this was down to $14AUD. Snagged it instantly - perks of the job!", "Author": "u/StackProne", "Content": "No content"},
{"Title": "Saying goodbye to a few fallen soldiers", "Author": "u/djlspider", "Content": "No content"},
{"Title": "Your content belongs to you, not Reddit: A thread.", "Author": "u/themadprogramer", "Content": "Welcome to the Post-API dystopia! So unless you have been living under a rock, Reddit has decided to begin pay-tiering its API following the footsteps of Facebook, Google and very recently Twitter. And people are MAD ! Given that here at Reddit we are a more tech-competent audience, protest has been very interesting . We have seen Subreddit black-outs, user mass-deletions.. I think the funniest suggestion I heard came from u/IkePAnderson who suggested overwriting posts with gibberish instead. Except there's a problem: I think this general attitude will not only fail to bring change, it will give the company exactly what it wants. I mean, is there any form of dissent better than self-destruction? All the complaints being filed and the rage and vitriol are cleaning after themselves. Once the new pay-tiers come into effect, the evidence of people not welcoming the change will vanish as has already happened in the case of Facebook and Twitter whose API changes failed to attract much attention from the press. Reddit, for better or worse, is a company that derives its revenue from band-waggoning trends. The top subreddits on this site include r/funny , r/AskReddit , r/worldnews ; things that capture the here and now and are not so much concerned with posteriority. Might I remind you that just until a few months ago, threads older than 6 months would be locked not allowing further edits or comments. Reddit's revenue stream does not benefit from retaining history beyond a certain point and is only retained as a gesture for brand-loyalty. So if everyone who now despises Reddit removes their history, that's okay, those who are indifferent will get to keep the same benefits and it won't cost Reddit any more or less. I'm saying all of this to make a point that mass-deletion only hurts individuals. It hurts you, it hurts me; it hurts the dissent towards Reddit because the community becomes invisible.. Your content is yours. It's not property of Reddit. And therefore, if you so wish, you can move it to another platform. As a dissenter of the API overhaul, I think it is in our interest to do so. The fact that our content is portable in this way is a thing that scares companies, because it is dangerous. Just look at YouTube and Twitch to see how they force their big streamers into exclusivity contracts. I might be u/themadprogramer on Reddit, and my words might be attributed to that name. But I can also exist as @madpro on other platforms; whether on YouTube or Discord, or something fediversy like Mastodon or Pleroma. So I believe the best way we can petition our redress is not through mass-deletion, but rather mass-action. You're a data hoarder, just download a bulk of your comments and post to a blog. If you're not camera shy record yourself talking about the API changes and why you left Reddit and put it on YouTube or TikTok. Do you want to know the best part? Reddit can't do anything about it, even the skeptics who have suggested the possibility of the company to revert changes must concede that the company cannot suppress what is happening outside of their platform. If nothing else, I just think it's good practice to cross-post because redundancy means retention. Every one of us has a personal history and that is personal not Redditorial . That personal history is split across mediums, as it should be, because we move in the world. Reddit is merely the context, it is neither the object nor subject. The best form of protest can only be reclaiming our content instead of destroying it!"},
{"Title": "Was ticketing clearance items at work and this was down to $14AUD. Snagged it instantly - perks of the job!", "Author": "u/StackProne", "Content": "No content"},
{"Title": "Spent hours prying these out... RIP fingernails but worth it for 128T's", "Author": "u/igob8a", "Content": "No content"},
{"Title": "Your content belongs to you, not Reddit: A thread.", "Author": "u/themadprogramer", "Content": "Welcome to the Post-API dystopia! So unless you have been living under a rock, Reddit has decided to begin pay-tiering its API following the footsteps of Facebook, Google and very recently Twitter. And people are MAD ! Given that here at Reddit we are a more tech-competent audience, protest has been very interesting . We have seen Subreddit black-outs, user mass-deletions.. I think the funniest suggestion I heard came from u/IkePAnderson who suggested overwriting posts with gibberish instead. Except there's a problem: I think this general attitude will not only fail to bring change, it will give the company exactly what it wants. I mean, is there any form of dissent better than self-destruction? All the complaints being filed and the rage and vitriol are cleaning after themselves. Once the new pay-tiers come into effect, the evidence of people not welcoming the change will vanish as has already happened in the case of Facebook and Twitter whose API changes failed to attract much attention from the press. Reddit, for better or worse, is a company that derives its revenue from band-waggoning trends. The top subreddits on this site include r/funny , r/AskReddit , r/worldnews ; things that capture the here and now and are not so much concerned with posteriority. Might I remind you that just until a few months ago, threads older than 6 months would be locked not allowing further edits or comments. Reddit's revenue stream does not benefit from retaining history beyond a certain point and is only retained as a gesture for brand-loyalty. So if everyone who now despises Reddit removes their history, that's okay, those who are indifferent will get to keep the same benefits and it won't cost Reddit any more or less. I'm saying all of this to make a point that mass-deletion only hurts individuals. It hurts you, it hurts me; it hurts the dissent towards Reddit because the community becomes invisible.. Your content is yours. It's not property of Reddit. And therefore, if you so wish, you can move it to another platform. As a dissenter of the API overhaul, I think it is in our interest to do so. The fact that our content is portable in this way is a thing that scares companies, because it is dangerous. Just look at YouTube and Twitch to see how they force their big streamers into exclusivity contracts. I might be u/themadprogramer on Reddit, and my words might be attributed to that name. But I can also exist as @madpro on other platforms; whether on YouTube or Discord, or something fediversy like Mastodon or Pleroma. So I believe the best way we can petition our redress is not through mass-deletion, but rather mass-action. You're a data hoarder, just download a bulk of your comments and post to a blog. If you're not camera shy record yourself talking about the API changes and why you left Reddit and put it on YouTube or TikTok. Do you want to know the best part? Reddit can't do anything about it, even the skeptics who have suggested the possibility of the company to revert changes must concede that the company cannot suppress what is happening outside of their platform. If nothing else, I just think it's good practice to cross-post because redundancy means retention. Every one of us has a personal history and that is personal not Redditorial . That personal history is split across mediums, as it should be, because we move in the world. Reddit is merely the context, it is neither the object nor subject. The best form of protest can only be reclaiming our content instead of destroying it!"},
{"Title": "How book publishers are trying to turn the age-old library system into a â€œreading as a serviceâ€ through their lawsuit against The Internet Archive", "Author": "u/retrac1324", "Content": "No content"},
{"Title": "Spent hours prying these out... RIP fingernails but worth it for 128T's", "Author": "u/igob8a", "Content": "No content"},
{"Title": "The end of an era. The last of 12 drive set of Dooms. Memes aside. Only 3 of the 16 Doom drives I originally purchased (heavily reduced price) actually died prematurely. Lab inspector is verifying the results.", "Author": "Unknown author", "Content": "No content"},
{"Title": "How book publishers are trying to turn the age-old library system into a â€œreading as a serviceâ€ through their lawsuit against The Internet Archive", "Author": "u/retrac1324", "Content": "No content"},
{"Title": "The end of an era. The last of 12 drive set of Dooms. Memes aside. Only 3 of the 16 Doom drives I originally purchased (heavily reduced price) actually died prematurely. Lab inspector is verifying the results.", "Author": "Unknown author", "Content": "No content"},
{"Title": "Need Honest Advice", "Author": "u/Substantial-Big8229", "Content": "For context, I left my gaming PC in a storage unit in a desert environment for nearly 2-3 years without ventilation, I'm concerned about potential data corruption or loss, especially regarding the SSD. I haven't powered it on, cleaned it, or updated its firmware during this time. What are the realistic chances of data corruption or loss, and how feasible is data recovery at this stage? (NOTE: Its consumer grade TLC SSD if that makes a difference.)"},
{"Title": "11 years ago I built a computer", "Author": "u/BinaryPatrickDev", "Content": "No content"},
{"Title": "Jumper pins on exos x18 14tb", "Author": "u/ctles", "Content": "Figured i might try here in that probably lots of people would have the 14tb exos here: What's the 4 jumper pins next to the SATA data for, see image below. I've looked through the main document i could find and asked a rep in seagate chat but he could only provide me what jumper pins may be for and his doc said the drive didn't have any jumper pins. But clearly they're there: https://www.kitguru.net/wp-content/uploads/2018/11/Seagate-Exos-X14-14TB-Hard-Drive-Review-on-KitGuru-SATA-Connectors.png"},
{"Title": "Big off-line database for recipes?", "Author": "u/CamT86", "Content": "I'm about to start compiling some recipes for offline use(ideally in a way that'll be easy to view on a tablet, either in its memory or from a home server) and i was wondering if maybe there was already something similar thats already been set up, that i could work from rather than start from scratch. I dont really have any bright ideas on how to do this. In 2 weeks i'll need to go to a remote cabin with really poor internet connection(old style satellite shared between 8 households, thats barely better than 56k... not the elon musk new stuff) and cook for a few families that are also staying out there for summer. I'm sure this time I could literally just bring a cookbook and maybe a few printouts for what i need, but id like to set something up for future use as well. They already have a small server set up with i think Kodi or plex that gets dragged back to civilization every few months for updates, so i figure i could just add whatever database file/system to that, to also be accessible to the person doing the cooking."},
{"Title": "Easy store external hard drive not mounting", "Author": "u/JessSerrano", "Content": "Hello! I have an Apple MacBook and I have the 2TB EasyStore external hard drive. I plug it in and sometimes it mounts, sometimes it doesnâ€™t. I have another external hard drive (Seagate) that mounts with no issues. I changed cords too and my EasyStore isnâ€™t mounting. Sometimes it does, though, which is odd. The light always turns on when I plug the cord into my Mac. Any suggestions on how to fix this? Thank you!"},
{"Title": "No media on window help.", "Author": "u/MisakaMisakaS100", "Content": "No content"},
{"Title": "Fast write speed small capacity USB Flash Drive", "Author": "u/Eidbanger", "Content": "Searching for a small capacity (due to expecting lower price) flash drive which has fast write speeds. I found a $32 Kingston DataTraveler Max 256 GB with up to 900 MB/s write but curious to hear if there's anything from 500 MB/s write for cheaper (~$10-20)? I'll be using this for creating bootable ISO images."},
{"Title": "Twitter is silently deleting some suspended accounts which has not been logged in for some time.", "Author": "u/cyberanakinvader", "Content": "I think I've made a disturbing discovery regarding Twitter where they have silently deleted some suspended accounts which has not been logged in for some time, including some of mine. Has anyone else encountered this issue recently?"},
{"Title": "Lib-gen question", "Author": "u/Hungry-Sentence-6722", "Content": "Iâ€™ve seen the direct download page for lib-gen sci-mag articles. I would like to understand how to convert all those files to the correct file name and extension. Libgen desktop is having issues with sourcing from mirrors.  I also donâ€™t see the point in one by one files.. any advice? The metadata must be in the sql file but many pdfâ€™s are just a few pages of a large book, I want to merge them all automatically."},
{"Title": "I don't trust raid 5", "Author": "u/obalobadik", "Content": "I have a 4-bay Asustor nas and 4x3TB HDD's. As for now I do not require more than 3TB raw storage and run the nas in raid 1 with only two drives. When I eventually want to increase my storage, I am sceptical to convert my volume to raid 5. Seeing the struggles involved with recovering data from an raid 5 array, I can't trust it as an option. I am more comfortable running two 2x3TB raid 1 array, since then I know that the data is formatted such that I can fetch it without rebuilding.  Even asustor themselves does not have a method of recovering the data from an raid 5 array. I have been using snapraid before, and that is has been feeling more safe, as  when a drive has failed, only the data on that drive has potentially been lost. Am I paranoid and recovering from raid 5 data loss not that troublesome? can you recover data from a single drive that is from a raid 5 array?"},
{"Title": "I'm running low on space in my custom built tower, what internal HDD's would you recommend?", "Author": "u/AVoraciousLatias", "Content": "Hi there, I have a 1 tb HDD and a 1 tb SDD. I'm looking to upgrade my HDD so I can store more games and programs on it, I'm looking for possibly a 4 tb or bigger so I won't have to worry about it any time soon. Do you have any suggestions for reliable HDDs that will last for a while? I'm looking for an internal that can be hooked up to the motherboard."},
{"Title": "Building a DIY JBOD", "Author": "u/OverlyBurntToast", "Content": "I have a bunch of old hard drives, and was wondering the best approach to putting them in some sort of DIY enclosure. My first thought was to just buy a bunch of the super cheap SATA to USB converters and plug that into a USB hub (speed is a low priority), but that wont be able to power the 3.5 inch drives I have. I was wondering if I could power the hard drives using a spare PC PSU I have, and then connect the hard drives via some sort of SATA DATA to USB or something. Does anyone have any ideas or hints, I would appreciate it greatly, as I am not too experienced in this field."},
{"Title": "Mid tower case vs 4U chassis for DIY NAS?", "Author": "u/Neurrone", "Content": "Hi, I'm currently looking to upgrade from an O11 air mini that only holds 4 drives because I realized it would be nice to have more drive slots and inadequate airflow causing some drives to run hot (ambient 28 C, idle 45, 51 under load). I made the mistake of choosing a case that only has as many drives that I had, so wanted to do it right this time. Use case is TrueNas with some apps, running on Ryzen 5700G to optimize for idle power consumption. I don't yet have a rack, but will be getting one when I move. Hence, I shortlisted some 4U rackmounts to place vertically first on a shelf: Logic case sc-4316 : US$350, 16 drives Sliger CX4712 : US$399, 12 drives. I'm leaning towards this These 4u cases aren't available for me locally - so estimated shipping to get either of those is an extra US$200, though I'm trying to find out if I can reduce that somehow. On the other hand,, a tower like the Fractal Meshify 2 (US$150) is much cheaper, and available to me locally, so I don't have to pay much for shipping. I could get 3 of these with the money needed to get the 4U. In this situation, are there enough advantages of those 4U cases to justify the extra expense? If I get a rack, I think I could place the Meshify 2 on its side on a shelf so that it only takes 6U. I'm new to rackmounted chassis, so these are the pros and cons I'm weighing, let me know if I missed anything. Pros: Support for redundant PSUs, hot swappable fans and drives for less uptime in case of a failure Support for larger motherboards: I currently have an ATX, not sure how useful support for larger form factors would be if I upgrade to lower end server boards Designed for rack mounting, smaller footprint Cons: Expensive Requires fan swaps for silence Higher drive temperatures? I assume drive temps would be worse compared to e.g, the meshify Won't fit my 160 mm cooler Thanks for reading!"},
{"Title": "My new Factory Recertified 22TB Seagate hard drives", "Author": "u/dropswisdom", "Content": "Sorry, had to re-post due to my previous post lacking in details. So here: got the drives here: https://www.ebay.com.sg/itm/204806952151 It was slightly cheaper when I got it. But it's one of the cheapest options you can find anywhere even now. The next step is 24TB drives which are not sold factory recertified as of yet, so they would cost.. about 50% more than this. I was concerned that some of the disks will arrive damaged, but as you can see it comes very well packaged, and they all passed testing. I put them into a Xpenology home made NAS. which is basically a itx motherboard inside a NAS case (jonsbo N1). It works like a charm. with one disk redundancy, I got about 80TB of fast storage. They cleared the hours on it when they recertified, but not the rest I think. But all the disks passed testing and are not making any suspicious noises or giving any issues. https://preview.redd.it/my-new-factory-recertified-22tb-seagate-hard-drives-v0-j8twcye0lx4d1.jpg Seagate 22TB Sata Hard Drives"},
{"Title": "Do you trust your backup enough to not use parity?", "Author": "u/19wolf", "Content": "If downtime isn't an issue, is there a reason to \"waste\"  space if you have a valid 3-2-1 backup? I'm using unraid so if a drive dies it's not like I need to restore my whole pool. How does everyone else feel about this?"},
{"Title": "Best Way to Convert Podcasts on Spotify?", "Author": "u/ShyGuyGaming76", "Content": "I listen to a lot of audio dramas, and with about two exceptions (The Magnus Archives and Kakos Industries) most of them are only on Spotify, so I'm not really able to archive them conveniently. What I'm doing right now is just screen recording them and converting it to an audio file. However, this is ungodly slow. Any better ways?"},
{"Title": "Still trying to get the hand of Virtual Dub for transcribing my tapes", "Author": "u/KWalthersArt", "Content": "I am posting this so I can get answers while I sleep, I hope I'm not breaking any rules about redundent or stupid questions. So the problems I have are. Langrith compression seems to follow a high instance of dropped frames if I record to long audio and video goes out of sync. I don't know how to import my files into davinci for recompression in better lossless codec. I don't know how to install a better codec in VDub Some tapes have a whine sound on them. Set up. Gaming PC Intel Core i7 Diamond VC500 usb card in a high level USB slot. Internal Hard drive then to external. Zenith XBR716 DVD-R VCR for play back. Most tapes are at SLP speed, I was a stupid teenager."},
{"Title": "Repairing drivesâ€¦", "Author": "u/TwoRight9509", "Content": "A decade ago youâ€™d send a broken drive to a place in California to get a repair / copy. How do we handle that now? Is there a best company to sent to?"},
{"Title": "Retrieving Photos From iPhoto Library In Time Machine Backup", "Author": "u/Akura_Awesome", "Content": "I have about a decadeâ€™s worth of Time Machine back ups across a dozen external drives, all with disparate photo libraries on them. Is there any sure fire way to pull these libraries out with metadata to throw on my NAS? Iâ€™ve tried to pull them in before but I get notifications about version and key mismatches, not to mention they can be a pain to locate. Any ideas are appreciated!"},
{"Title": "Help! I erased one of four RAID volumes set up with Disk Utilities", "Author": "u/peterinjapan", "Content": "Hi, I'm having an issue with a RAID with a volume I accidentally erased. My Yottamaster RAID started becoming unstable and was beeping all the time, so after giving up on it, I bought a Logitech enclosure and put the drives in, creating a RAID 5 volume with SoftRaid 8. After I got everything set up, of my disks disappeared from the new RAID, and as I was verifying the other three, another one gave me issues. So I decided to format the \"new\" disks and try again. I seem to have had one of the volumes in my old (backup) RAID selected, and erased it. The backup RAID was, I think, set up as two striped RAID volumes, which were then striped together to create a raid of that. This might not be the best way to do a RAID, but there was no option other than striped or mirrored disks, and at the time SoftRaid was not working with Apple Silicon at all. I have bought a large external disk to serve as a working disk to hopefully save my data. Can anyone suggest what will happen when I use the Restore option in Disk Utilities? If I can save the files and hopefully the folder structure, that will be good.  If there are any other tools besides Disk Utility I should be exploring, please let me know. Sending the disks to a data service isn't an option as I'm in rural Japan."},
{"Title": "Saw this and thought it belonged here.", "Author": "u/Mist17", "Content": "No content"},
{"Title": "Document Scanning Services", "Author": "u/JL4575", "Content": "Iâ€™m looking to scan a small collection of magazines, more than I want to manage manually. Each is stapled through the middle, not bound, and I would really like to not destroy the collection in getting them scanned. Can anyone recommend mail order services theyâ€™ve used?"},
{"Title": "G-RAID - Use built in HW RAID, Disk Utility RAID or something else?", "Author": "u/SuperChooch", "Content": "I have an older G-RAID device that is not supported by WD, attached to a Mac mini.    It is currently running G-RAID's hardware RAID at RAID 0 and I want to redeploy it for another purpose and make it RAID 1.  It is not mission critical and I'll be backing it up to another local location and to the cloud so if it craps out, it won't be the end of the world.  Should I continue to use G-RAID's RAID, should I use the Mac's Disk Utility RAID assistant or should I use something else?"},
{"Title": "2 part question.", "Author": "u/GrayManTech", "Content": "Hi. I have a bunch of DVDs that I want to convert to mp4 or whatever. I know I need a DVD player of some sort and software. I've seen here there are a couple of dvd players that don't need to be modified to use for this, but the posts I found were pretty old, have the available drives changed and if so which ones work out of the box? 2nd part, storage. Is it possible to setup a Nas or something to use wifi so everyone in my house can access it? My router has a USB port but from what I understand it's disabled by xfinity. Basically I'm looking for something that can hold both 2.5 and 3.5 hdd since I already have both."},
{"Title": "CD ripping for dummies", "Author": "u/nlj1978", "Content": "While spring cleaning I stumbled across a long forgotten CD collection. Rummaging through it nostalgia is kicking hard. So of course I'm going to have to rip it all so I can enjoy. Have ~200 CDs I've located to rip.  Also have a previous stash of ripped CDs on one of my home computers Googling a bit I'm reading about FLAC files being best to get the best rip and gather the music information. Exact Audio Copy along with a few others get mentioned alot for ripping. What's the most user friendly and effective ripping software for someone new to this? Doesn't have to be freeware, will pay for solid software. Is the FLAC file format relatively usable? For example I have a networked Onkyo receiver, would it be able to play those files from the network?"},
{"Title": "Which would be better for a small NAS, using a RPi 4?", "Author": "u/NaAlSiO6", "Content": "I'm currently looking at two external HDD/SSD enclosures and can't decide which one to go with. The ADSA-ST SUPERSPEED USB DUAL HDD DOCK The Asus TUF Gaming A1 The first one supports two SSDs/HDDs, but it's somewhat difficult to find reasonable priced (when compared to a same capacity M.2 drive) 2.5 inch SATA SSDs. Still, it's not impossible and it could give me a lot more room to expand in the future. However, I also wonder about the performance of the drives, as there will be no cooling The second one however only supports one NVME SSD. Which couples with the USB 3.2 Gen 2X1 interface would give me better speed and thermals, due to the metal construction."},
{"Title": "[Noob] Using LTO5 for archival purposes, tips?", "Author": "u/mactep66", "Content": "I just got myself a FC LTO5 drive along with a few tapes (haven't arrived yet) for the purposes of archiving old data and freeing up my HDDs. What im looking for is a tool/tools that will keep a (hopefully) searchable index of all the files on all my tapes on my main SSD, one that would allow me to add/remove files from a tape at will and one for archiving large amounts of data as reliably and fast as possible. Ive head of Uranium backup, Veeam CE and my 5 should also be compatible with LTFS, but i haven't heard the best things about it, how good is it?"},
{"Title": "Needing Help Managing Duplicates", "Author": "u/klnadler", "Content": "Hi everyone, I'm on a long journey consolidating years of external drives to an Unraid server. I've been having a hard time managing duplicates, I am using Czkawka which is great at finding the duplicates but I'm having trouble with selecting the correct file to remove. I'll provide a screenshot of an example, after searching for duplicates I chose to select the newer file to delete and most of the files with -1 in the name are the newer file and it's from using exiftool to pool the files together. In the screenshot the one selected isn't the newer file according to any of the tags. Photo Another issue I'm having is finding thumbnails that were accidentally sorted into the folders along the way. Some of the thumbnails are of the whole image and others are of faces from iPhotos face detection feature. Any guidance is appreciated TIA!"},
{"Title": "How hard is it to switch between RAID configurations?", "Author": "u/Austinitered", "Content": "I have a CM3588 Nas with 4 x 4TB M.2 SSDs I'm trying to setup and I'm stuck between the RAID configurations. It sounds like RAID-5 isn't the best since they SSDs are over 3TB; haven't fully confirmed this, just something I've read (LTT used it on this setup though). RAID-Z sounds like the best option so far, but only allows for 1 drive failure and I don't really see me utilizing >8TB anytime in the near future. Because of this, I'm wondering if I should start with RAID-10, and adjust/recreate the configuration to RAID-6 and then/or RAID-Z later on? Just started looking into RAID configurations, so this is all new to me. I'm guessing the biggest hurdle is migrating the data to temporary disks if I need to switch to a different configuration? Edit: Using Open Media Vault"},
{"Title": "Software recommendations for monitoring disk speeds (including NAS drives) in a nice graph format", "Author": "u/ThatGuyFilms", "Content": "I have a homelab setup that I do heaps of data transfers on, as I work in the video space a bunch of media coming in off external hard drives, NAS drives, an Unraid server and LTO tape. I'm after a software (ideally that works on both mac and windows) that can give me realtime data into transfers speeds from attached drives, to LTO tape and bandwidth across the machines and switch. I currently use the iStat menus ( https://bjango.com/mac/istatmenus/ ) which I love, however you can only view it in a small popup window and can't see combined usage (i.e. network traffic as well as drive speeds) so would love an all in one solution if one exists, that displays in a nice graph format that shows the speeds of everything. Any recommendations?"},
{"Title": "How to download video from mediasite.com", "Author": "u/pavoganso", "Content": "Hi, I can't figure out how to download video. It doesn't seem to have a m3u8 stream."},
{"Title": "LTO5 tape drive with usbc thunderbolt", "Author": "u/mc_louis", "Content": "Hi everyone. I'm in the middle of some home renovation, and I decided that I want to reduce the space occupied by my all my stuff. At the moment I have a whole tower pc with windows 11 that I use just to run my external sas hp lto5 drive, through THIS an lsi9211, which is a pcie 2.0 x8 lanes, everything working perfectly fine. I would like to remove the tower pc, if only I can connect the tape drive to a generic usb to use it with just the laptop when I need. I found few adapters like this and I pulled the trigger. But it's not working, as I understand because my hba card is a x8 lanes, but all this adapters looks like they're just x4 lanes. I tried just covering the contacts on the card that correspond to the 5,6,7,8 lanes, as seen somewhere, but it's not working. Does anybody ever worked something like this out? For what I understand, there are not pcie x8 adapters that are affordable... are there any hba card that are x4 lanes? I found something on ebay, like this , somewhere in the description says x8, but pictures and specifics says x4. Would this card work with my drive or am I missing something else? Damn it would be a game changer for me a usbc tape drive..."},
{"Title": "Should I buy a 1TB external SSD, or a Blu-ray burner + a 50-pack BD-R spindle + 50 jewel cases?", "Author": "u/TheresThisOtherThing", "Content": "Hello, I am new here and to data hoarding in general, so please give me the for-dummies version. I need to back up some data, a little less than 1TB. I have a couple copies in the cloud with different providers, and I'd like to make a physical copy that I can put in a box and store at a relative's house. I don't expect to update or modify this copy at all, ever, just copy the files in it to my laptop if the copies in the cloud and my laptop somehow all fail at the same time. I've been poking around on Amazon and it looks like at time of writing, a 1 TB external SSD or a combination of a Blu-ray burner, a 50-pack of BD-Rs, and 50 jewel cases, will each cost around $80. Which option is better for my case, and could you point me to better options? I'm on a budget, so maybe keep it below $100 if possible. Thanks!"},
{"Title": "Using an external drive to store photos question.", "Author": "u/shdujssnensisishs", "Content": "https://support.apple.com/guide/iphone/import-and-export-photos-and-videos-iph480caa1f3/ios The link above is to appleâ€™s website that shows u how to move photos to an external device. More specifically the â€œExport photos and videos to an external storage deviceâ€ section. If I choose to do this, will my iPhone move the photos or just copy the photos and videos over and leave the originals on my phone? How would restoring the photos work? If I import it back, would the dates and location and everything still be there? Has anyone done this and can I get insight on this? Thank you!"},
{"Title": "any way to download entire galleries of art like gallery-dl?", "Author": "u/dietgilroy", "Content": "i tried using that but it didn't work at all, so i might use wfdownloader for that. i am considering trying to archive art from deviantart."},
{"Title": "The infuriating things about communicating the importance of data hoarding to the average internet user", "Author": "u/volthunter", "Content": "This is inspired by a conversation I've had which often feels cyclical about how we are losing access to most of the data on the internet, Yuzu the switch emulator was taken down and with it many clones and spin off's, Vimm's lair has had it's Nintendo roms forcefully removed, internet archive is being sued and getting cease and desists, some from Nintendo some from other companies, we've lost a multitude of pirate websites as of late, but people think piracy \" cant be defeated\". My point being that in the past 2 years more damage has been done to the current scene of the internet and preservation than all the years prior, it's not just about piracy or emulation, it's more than that, google search was a vital core of the internet and it's been ripped out. It's like the adage that there is a man in Czechoslovakia maintaining a piece of software that is crucial to the internet's existence and multiple times that has proven to be true, and now google has fallen and the internet is worse for it. A big loss that most people don't know if is that visa through their various connections to right wing christian organisations was actively campaigned by mormon extremists to revoke funding from most of the main xxx websites such as pornhub and xhamster, this seems funny, but an incredible amount of material was lost and i don't think it wise to dismiss something like this as just some minor occurrence. This may have been one of the largest data losses ever, the loss of amateur content on pornhub represented billions of hours of video that is lost to time, and PornHub conclusively proved that it was a sham case set up by a mormon church that aims to ban all adult material and even then, they were forced to shut down a retaliatory case and to shut down the amateur section of the site because of pressure from visa. The government itself has made material that many corporations would prefer you don't have access to such as fixing old factory equipment and booklets on home electric repair, with the american governments currently starting the fight to launch right to repair, you must realise that the companies would prefer if you did not have that right or information, ifixit and it's guides would be eliminated. Even the people testing products, tech channels like linus and level 1 techs would all be eliminated in time, too much finds itself at odds with these companies and you think it's hypothetical but if you upload a video tearing down an iphone and repairing it, apple may take it down, they are known to do it, same for john deer tractor hacks and lg washing machine repair videos, this is a fight, and we are losing. Anyways, i just see things getting worse and frankly it seems like we aren't bouncing back how the internet seems to think we are, the torrent scenes are no where near as alive as they used to be, even with private trackers, the scene is a shadow of what it once was, and i only see it getting worse, so hoard, data hoard it all."},
{"Title": "Upgrading from a USB JBOD", "Author": "u/doubledundercoder", "Content": "Hey all. Iâ€™ve got a usb 3.1 jbod attached to a decent laptop with 16gb of ram with a roughly 16tb zfs RAID. Works fine most of the time. I get 100mb/s reads if only one process is accessing the volume. If more than one process tryâ€™s it just tanks. Upgrading to a SAS enclosure or just getting a desktop case where I plug in the drives directly to the sata controller, what kind of performance increase can I expect for multiple processes on the volume? Am I dreaming thinking it will make a huge difference? Itâ€™s mostly for hosting Plex. Anyone gone from this setup to a direct attached on zfs? 4 7.2k sata drives"},
{"Title": "Best way to transcode entire library of videos on Google Drive preferably though Google cloud", "Author": "u/cewong2", "Content": "Iâ€™ve been searing around for a day and a half trying to find a way to transcode lots of videos I have stored in my Google drive. I havenâ€™t been able to find anyone ever having a solution (lots of what Iâ€™ve found isnâ€™t gets downloaded and reuploaded after transcoding). Hopefully this is posted in the right place otherwise please suggest a subReddit for me to post to. Lots of my videos are in x264 format and I want to transcode them into x265 as I read thereâ€™s possibly a saving of at least 20-30% but itâ€™s nearly 60-70TB which would take probably a long time compared just using Googleâ€™s free cloud trial. Can anyone recommend me a guide or tool that I can use to get this done?"},
{"Title": "NVME Speeds:  PCIe x1 , and USB3.0", "Author": "u/Anarcho_Christian", "Content": "I've got an extra Kingston NVME Drive lying around, and a lot of extra usb 3.0 ports. Would the speeds be comparable on a USB 3.0 drive vs a PCIe x1? Not \"theoretical\" speeds mind you, but like, practical speeds?"},
{"Title": "Macrium Reflect vs Arq Backup", "Author": "u/Gazumbo", "Content": "I've been using Arq for a few years. No majors issues, although I haven't had to test a full restore. I've been tempted to switch to Macrium as I already have a disk image using Macrium and switching would then mean I'm only using one piece of software instead of two. But, with regards to file and folder backup, what would I miss by switching to Macrium? I'm a home user and not knowledgeable on the more technical side of backup software. I believe Arq does a full backup and incremental from then on. I know Macrium has this option also. My main need is software that will never delete a file from a backup, no matter if I delete the source file or move it. Retaining a few versions. Bonus would be the ability to mirror this backup to the cloud in some form. Currently, I have local backup to an external drive using Arq, and cloud backup to backblaze using their personal computer license. So it's two different pieces of software but that's not a problem."},
{"Title": "Power Outage Froze my TrueNAS OS. How to Monitor Offsite TrueNAS Replication?", "Author": "u/Luz3r", "Content": "Hey everyone, I recently had a power outage at my backup location, and it seems to have messed with my TrueNAS box. It got stuck in a weird state, but thankfully a reboot fixed it. The problem is, I don't check my offsite server that often since it's a secondary location. This time, I only found out because I got an alert about my ZFS replication failing. My question is: How can I best check if my replication is actually working properly? I was thinking about setting up Uptime Kuma to monitor the offsite TrueNAS web interface. While this wouldn't necessarily tell me if replication is failing, it might catch issues like SSH being unresponsive. Ideally, I'd like to get Telegram alerts for any problems. Anyone have any suggestions for a better or cool way to monitor my TrueNAS replication health?"},
{"Title": "Can 3.5â€ external enclosure setup drive", "Author": "u/ChillCaptain", "Content": "https://www.amazon.com/SSK-External-Docking-Enclosure-Supports/dp/B08P1539VD/ref=asc_df_B08P1539VD/?hvadid=693495256271&hvpos=&hvnetw=g&hvrand=2570617204095985210&hvpone=&hvptwo=&hvqmt=&hvdev=m&hvdvcmdl=&hvlocint=&hvlocphy=9057119&hvtargid=pla-1626594922711&psc=1&mcid=086ce8faf4d5351490f73a899c982a27&gad_source=1 Iâ€™m looking to get this enclosure to help with backups. Can I insert a brand new drive and set up the drive in windows? Usually this is done in disk management where you select gpt vs mbr and select the drive letter. After setting it up can I take the drive and insert it internally through sata and use the drive? Iâ€™m wondering if the usb connection limits this?"},
{"Title": "MixesDB - A Site Archiving Mixes And Live Shows Is Shutting Down At The End Of The Month. Data Hoarders, How Can We Fully Archive The Site?", "Author": "u/FinleyGomez", "Content": "No content"},
{"Title": "What is the best way to back-up 1 PB?", "Author": "u/Early-Mechanic6508", "Content": "My job has about 900 TB worth of storage. I'm looking for a cloud-based backup. Google Drive seems to have increments of 5 TB. Should I just get 200 users on drive? Since Dropbox unlimited is dead, that does not seem to be an option. Does anyone know a cheaper way? Does anyone have any suggestions?"},
{"Title": "DAS with 2x Raid1 for working and backup storage - Could use some guidance", "Author": "u/Final_Wedding_5634", "Content": "Hobbyist Photographer and videographer looking for DAS to expand my 2019 Macbook Pro's working space and archival backup space. Cost is certainly a consideration. I have two needs: Backups - I presently have 2 x 3TB 5.4k rpm HDDs for storage in a desktop nearby. This PC needs to be replaced as its old and dying slowly. The HDDs work great - used for cold storage. the PC is off most of the time. More working space. Presently, I just keep my MBP clear of working document - i only load one project at a time and work off the internal 500GB SSD. This works but is getting tedious. I typically edit in either iMovie or Davinci Resolve (depending on project needs) and would like to move this to a larger DAS. I'm thinking I'd like 1 DAS with 4 bays. But from who? OWC? MediaSonic? Configure the two HDDs into a single Raid 1 for local backups with redundancy. I'll copy files to cold storage as needed. This data is backed up to an external drive in a fire-safe and critical docs are backedup to the cloud as well. Configure two SSDs into a single Raid 1 for faster working space redundancy. I'll use these as working space for editing two or three streams of 1080.60 video. Once projects are finished, the final result and maybe the raw footage is saved to the HDDs for archival. Questions: General thoughts for this approach? I'm sure i'm not the only one doing this but I couldn't find anything. Maybe I should just go RAID 10 or 5 or something :( Is there a specific bus type to handle 2x 1 Raid? How do i know if a DAS can handle this 2x Raid 1 configuration? is any JBOD good enough? Suggestions for affordable DAS? Raid controllers? Do i use Disk Utility or the DAS's softwareraid? Who's got the best notification of a failed drive? Speed - ideally i'd like NVMe as it's fast and future proof (i.e. I'll eventually get into 4k filming, maybe). Are there boxes that have NVMe and HDD? I couldn't find any. Would SSDs suffice (the 2.5\" variety such as evos). edited for clarity"},
{"Title": "Is this Rot?", "Author": "u/MDCasino21", "Content": "No content"},
{"Title": "Synology vs Ebay PC", "Author": "u/Suspicious_Dig_5684", "Content": "So I have a server; what I am looking for is something to put 2 18tb drives in and be a dedicated photo backup for both IPhone and Android. Would love a way to pull the pictures from ICloud and get them local as well. The wife has more pictures in the cloud then she has space on the phone and I don't have a easy way to pull those down. Is the ds223 worth it? Are there better options. Biggest concern is a simple way to backup the iPhone that doesn't require user input. We tired nextcloud works great on android but with iphone I have to constantly remind her to open the app before it would backup anything. Thanks for any help."},
{"Title": "Any DataHoarders from 3rd world countries?", "Author": "u/kkgmgfn", "Content": "How do you backup TBs of data to cloud considering that we dont have unlimited home plans even on shelling money? I am from Bangalore,India I have multiple TBs to upload, thats collected over 15yrs. Later in incremental backup is fine but initial is hard."},
{"Title": "PSA: Blocking the blank PSU slots on NetApp disk shelves is non-optional, i.e. don't screw up like I did!", "Author": "u/xxpor", "Content": "My DS4246 came from ebay with the standard 2 PSU/2 Blank slot setup. I have the left side of the shelf totally full, and the right side partially full, but mostly empty. The drives on the right side were sitting around 45c, but the drives on the left side hit 67c (!!!!). I thought the drive sensors might be lying, since they were so high. I took a closer look at the empty slots and realized that the airflow that the PSUs were moving were likely coming mostly from the back of the unit since that's a path of less resistance than flowing over the drives themselves. I printed 2 of these guys: https://www.thingiverse.com/thing:6514452 . I stuck them in the slots, and within 10 seconds the fans spun way up and started moving serious air. Now the drives are all around 35-42c, totally normal! So if you have a NetApp disk shelf, check your drive temps and make sure you block every open port. There's also an STL available to blank off IOM slots if you need it under the netapp category. Of course, they're available to buy as well."},
{"Title": "Currently have a desktop PC with a 1-bay NAS hosting my video and audio.  If I get this 12u rack with UPS and power strip with 2 vented racks, 1 to sit my PC on, other for my modem and wifi.  My next purchase will be a rackmount Synology 4-bay nas.  is this the way to go for me?", "Author": "u/Kevalemig", "Content": "No content"},
{"Title": "Had to add a second sata card and upgrade to a 1600 watt power supply because spinning up 17 drives was too much for my poor 900 watt...", "Author": "u/timekillerjay", "Content": "No content"},
{"Title": "Lana Del Rey says laptop containing her new album was stolen. This is why you back up your data peopleâ€¦", "Author": "Unknown author", "Content": "No content"},
{"Title": "Thought you guys would enjoy, this is a 20TB ssd we are building at work. All in a 2.5\" formfactor. There's so many NAND chips that we have to make a pcb that folds in half.", "Author": "u/isonotlikethat", "Content": "No content"},
{"Title": "Had to add a second sata card and upgrade to a 1600 watt power supply because spinning up 17 drives was too much for my poor 900 watt...", "Author": "u/timekillerjay", "Content": "No content"},
{"Title": "Is $92 per lb a good price?", "Author": "u/FragileRasputin", "Content": "No content"},
{"Title": "Lana Del Rey says laptop containing her new album was stolen. This is why you back up your data peopleâ€¦", "Author": "Unknown author", "Content": "No content"},
{"Title": "Protect yourselves out there - NEW Drive from Amazon with 15257 Power On Hours", "Author": "u/TheWoodser", "Content": "No content"},
{"Title": "Thought you guys would enjoy, this is a 20TB ssd we are building at work. All in a 2.5\" formfactor. There's so many NAND chips that we have to make a pcb that folds in half.", "Author": "u/isonotlikethat", "Content": "No content"},
{"Title": "Data hoarding also means having bigger hard drives, right?", "Author": "u/VulturE", "Content": "No content"},
{"Title": "Is $92 per lb a good price?", "Author": "u/FragileRasputin", "Content": "No content"},
{"Title": "I did the math, realized that this 200 Disc CD player I got for the office can only store 130GB of audio, and I'm sad now.", "Author": "u/AshleyUncia", "Content": "No content"},
{"Title": "Protect yourselves out there - NEW Drive from Amazon with 15257 Power On Hours", "Author": "u/TheWoodser", "Content": "No content"},
{"Title": "Which of you fucker's did this.", "Author": "u/linuxcommunist", "Content": "No content"},
{"Title": "Data hoarding also means having bigger hard drives, right?", "Author": "u/VulturE", "Content": "No content"},
{"Title": "Inventor of cassette tape Lou Ottens passes away at 94", "Author": "u/themadprogramer", "Content": "No content"},
{"Title": "I did the math, realized that this 200 Disc CD player I got for the office can only store 130GB of audio, and I'm sad now.", "Author": "u/AshleyUncia", "Content": "No content"},
{"Title": "100TB! Just got to 100tb of storage on my server!", "Author": "u/mbailey5", "Content": "No content"},
{"Title": "Which of you fucker's did this.", "Author": "u/linuxcommunist", "Content": "No content"},
{"Title": "Saying goodbye to a few fallen soldiers", "Author": "u/djlspider", "Content": "No content"},
{"Title": "Inventor of cassette tape Lou Ottens passes away at 94", "Author": "u/themadprogramer", "Content": "No content"},
{"Title": "Was ticketing clearance items at work and this was down to $14AUD. Snagged it instantly - perks of the job!", "Author": "u/StackProne", "Content": "No content"},
{"Title": "100TB! Just got to 100tb of storage on my server!", "Author": "u/mbailey5", "Content": "No content"},
{"Title": "Your content belongs to you, not Reddit: A thread.", "Author": "u/themadprogramer", "Content": "Welcome to the Post-API dystopia! So unless you have been living under a rock, Reddit has decided to begin pay-tiering its API following the footsteps of Facebook, Google and very recently Twitter. And people are MAD ! Given that here at Reddit we are a more tech-competent audience, protest has been very interesting . We have seen Subreddit black-outs, user mass-deletions.. I think the funniest suggestion I heard came from u/IkePAnderson who suggested overwriting posts with gibberish instead. Except there's a problem: I think this general attitude will not only fail to bring change, it will give the company exactly what it wants. I mean, is there any form of dissent better than self-destruction? All the complaints being filed and the rage and vitriol are cleaning after themselves. Once the new pay-tiers come into effect, the evidence of people not welcoming the change will vanish as has already happened in the case of Facebook and Twitter whose API changes failed to attract much attention from the press. Reddit, for better or worse, is a company that derives its revenue from band-waggoning trends. The top subreddits on this site include r/funny , r/AskReddit , r/worldnews ; things that capture the here and now and are not so much concerned with posteriority. Might I remind you that just until a few months ago, threads older than 6 months would be locked not allowing further edits or comments. Reddit's revenue stream does not benefit from retaining history beyond a certain point and is only retained as a gesture for brand-loyalty. So if everyone who now despises Reddit removes their history, that's okay, those who are indifferent will get to keep the same benefits and it won't cost Reddit any more or less. I'm saying all of this to make a point that mass-deletion only hurts individuals. It hurts you, it hurts me; it hurts the dissent towards Reddit because the community becomes invisible.. Your content is yours. It's not property of Reddit. And therefore, if you so wish, you can move it to another platform. As a dissenter of the API overhaul, I think it is in our interest to do so. The fact that our content is portable in this way is a thing that scares companies, because it is dangerous. Just look at YouTube and Twitch to see how they force their big streamers into exclusivity contracts. I might be u/themadprogramer on Reddit, and my words might be attributed to that name. But I can also exist as @madpro on other platforms; whether on YouTube or Discord, or something fediversy like Mastodon or Pleroma. So I believe the best way we can petition our redress is not through mass-deletion, but rather mass-action. You're a data hoarder, just download a bulk of your comments and post to a blog. If you're not camera shy record yourself talking about the API changes and why you left Reddit and put it on YouTube or TikTok. Do you want to know the best part? Reddit can't do anything about it, even the skeptics who have suggested the possibility of the company to revert changes must concede that the company cannot suppress what is happening outside of their platform. If nothing else, I just think it's good practice to cross-post because redundancy means retention. Every one of us has a personal history and that is personal not Redditorial . That personal history is split across mediums, as it should be, because we move in the world. Reddit is merely the context, it is neither the object nor subject. The best form of protest can only be reclaiming our content instead of destroying it!"},
{"Title": "Saying goodbye to a few fallen soldiers", "Author": "u/djlspider", "Content": "No content"},
{"Title": "Spent hours prying these out... RIP fingernails but worth it for 128T's", "Author": "u/igob8a", "Content": "No content"},
{"Title": "Was ticketing clearance items at work and this was down to $14AUD. Snagged it instantly - perks of the job!", "Author": "u/StackProne", "Content": "No content"},
{"Title": "How book publishers are trying to turn the age-old library system into a â€œreading as a serviceâ€ through their lawsuit against The Internet Archive", "Author": "u/retrac1324", "Content": "No content"},
{"Title": "Your content belongs to you, not Reddit: A thread.", "Author": "u/themadprogramer", "Content": "Welcome to the Post-API dystopia! So unless you have been living under a rock, Reddit has decided to begin pay-tiering its API following the footsteps of Facebook, Google and very recently Twitter. And people are MAD ! Given that here at Reddit we are a more tech-competent audience, protest has been very interesting . We have seen Subreddit black-outs, user mass-deletions.. I think the funniest suggestion I heard came from u/IkePAnderson who suggested overwriting posts with gibberish instead. Except there's a problem: I think this general attitude will not only fail to bring change, it will give the company exactly what it wants. I mean, is there any form of dissent better than self-destruction? All the complaints being filed and the rage and vitriol are cleaning after themselves. Once the new pay-tiers come into effect, the evidence of people not welcoming the change will vanish as has already happened in the case of Facebook and Twitter whose API changes failed to attract much attention from the press. Reddit, for better or worse, is a company that derives its revenue from band-waggoning trends. The top subreddits on this site include r/funny , r/AskReddit , r/worldnews ; things that capture the here and now and are not so much concerned with posteriority. Might I remind you that just until a few months ago, threads older than 6 months would be locked not allowing further edits or comments. Reddit's revenue stream does not benefit from retaining history beyond a certain point and is only retained as a gesture for brand-loyalty. So if everyone who now despises Reddit removes their history, that's okay, those who are indifferent will get to keep the same benefits and it won't cost Reddit any more or less. I'm saying all of this to make a point that mass-deletion only hurts individuals. It hurts you, it hurts me; it hurts the dissent towards Reddit because the community becomes invisible.. Your content is yours. It's not property of Reddit. And therefore, if you so wish, you can move it to another platform. As a dissenter of the API overhaul, I think it is in our interest to do so. The fact that our content is portable in this way is a thing that scares companies, because it is dangerous. Just look at YouTube and Twitch to see how they force their big streamers into exclusivity contracts. I might be u/themadprogramer on Reddit, and my words might be attributed to that name. But I can also exist as @madpro on other platforms; whether on YouTube or Discord, or something fediversy like Mastodon or Pleroma. So I believe the best way we can petition our redress is not through mass-deletion, but rather mass-action. You're a data hoarder, just download a bulk of your comments and post to a blog. If you're not camera shy record yourself talking about the API changes and why you left Reddit and put it on YouTube or TikTok. Do you want to know the best part? Reddit can't do anything about it, even the skeptics who have suggested the possibility of the company to revert changes must concede that the company cannot suppress what is happening outside of their platform. If nothing else, I just think it's good practice to cross-post because redundancy means retention. Every one of us has a personal history and that is personal not Redditorial . That personal history is split across mediums, as it should be, because we move in the world. Reddit is merely the context, it is neither the object nor subject. The best form of protest can only be reclaiming our content instead of destroying it!"},
{"Title": "The end of an era. The last of 12 drive set of Dooms. Memes aside. Only 3 of the 16 Doom drives I originally purchased (heavily reduced price) actually died prematurely. Lab inspector is verifying the results.", "Author": "Unknown author", "Content": "No content"},
{"Title": "Spent hours prying these out... RIP fingernails but worth it for 128T's", "Author": "u/igob8a", "Content": "No content"},
{"Title": "How book publishers are trying to turn the age-old library system into a â€œreading as a serviceâ€ through their lawsuit against The Internet Archive", "Author": "u/retrac1324", "Content": "No content"},
{"Title": "The end of an era. The last of 12 drive set of Dooms. Memes aside. Only 3 of the 16 Doom drives I originally purchased (heavily reduced price) actually died prematurely. Lab inspector is verifying the results.", "Author": "Unknown author", "Content": "No content"},
{"Title": "Which would be better for a small NAS, using a RPi 4?", "Author": "u/NaAlSiO6", "Content": "I'm currently looking at two external HDD/SSD enclosures and can't decide which one to go with. The ADSA-ST SUPERSPEED USB DUAL HDD DOCK The Asus TUF Gaming A1 The first one supports two SSDs/HDDs, but it's somewhat difficult to find reasonable priced (when compared to a same capacity M.2 drive) 2.5 inch SATA SSDs. Still, it's not impossible and it could give me a lot more room to expand in the future. However, I also wonder about the performance of the drives, as there will be no cooling The second one however only supports one NVME SSD. Which couples with the USB 3.2 Gen 2X1 interface would give me better speed and thermals, due to the metal construction."},
{"Title": "[Noob] Using LTO5 for archival purposes, tips?", "Author": "u/mactep66", "Content": "I just got myself a FC LTO5 drive along with a few tapes (haven't arrived yet) for the purposes of archiving old data and freeing up my HDDs. What im looking for is a tool/tools that will keep a (hopefully) searchable index of all the files on all my tapes on my main SSD, one that would allow me to add/remove files from a tape at will and one for archiving large amounts of data as reliably and fast as possible. Ive head of Uranium backup, Veeam CE and my 5 should also be compatible with LTFS, but i haven't heard the best things about it, how good is it?"},
{"Title": "Needing Help Managing Duplicates", "Author": "u/klnadler", "Content": "Hi everyone, I'm on a long journey consolidating years of external drives to an Unraid server. I've been having a hard time managing duplicates, I am using Czkawka which is great at finding the duplicates but I'm having trouble with selecting the correct file to remove. I'll provide a screenshot of an example, after searching for duplicates I chose to select the newer file to delete and most of the files with -1 in the name are the newer file and it's from using exiftool to pool the files together. In the screenshot the one selected isn't the newer file according to any of the tags. Photo Another issue I'm having is finding thumbnails that were accidentally sorted into the folders along the way. Some of the thumbnails are of the whole image and others are of faces from iPhotos face detection feature. Any guidance is appreciated TIA!"},
{"Title": "How hard is it to switch between RAID configurations?", "Author": "u/Austinitered", "Content": "I have a CM3588 Nas with 4 x 4TB M.2 SSDs I'm trying to setup and I'm stuck between the RAID configurations. It sounds like RAID-5 isn't the best since they SSDs are over 3TB; haven't fully confirmed this, just something I've read (LTT used it on this setup though). RAID-Z sounds like the best option so far, but only allows for 1 drive failure and I don't really see me utilizing >8TB anytime in the near future. Because of this, I'm wondering if I should start with RAID-10, and adjust/recreate the configuration to RAID-6 and then/or RAID-Z later on? Just started looking into RAID configurations, so this is all new to me. I'm guessing the biggest hurdle is migrating the data to temporary disks if I need to switch to a different configuration? Edit: Using Open Media Vault"},
{"Title": "Software recommendations for monitoring disk speeds (including NAS drives) in a nice graph format", "Author": "u/ThatGuyFilms", "Content": "I have a homelab setup that I do heaps of data transfers on, as I work in the video space a bunch of media coming in off external hard drives, NAS drives, an Unraid server and LTO tape. I'm after a software (ideally that works on both mac and windows) that can give me realtime data into transfers speeds from attached drives, to LTO tape and bandwidth across the machines and switch. I currently use the iStat menus ( https://bjango.com/mac/istatmenus/ ) which I love, however you can only view it in a small popup window and can't see combined usage (i.e. network traffic as well as drive speeds) so would love an all in one solution if one exists, that displays in a nice graph format that shows the speeds of everything. Any recommendations?"},
{"Title": "How to download video from mediasite.com", "Author": "u/pavoganso", "Content": "Hi, I can't figure out how to download video. It doesn't seem to have a m3u8 stream."},
{"Title": "LTO5 tape drive with usbc thunderbolt", "Author": "u/mc_louis", "Content": "Hi everyone. I'm in the middle of some home renovation, and I decided that I want to reduce the space occupied by my all my stuff. At the moment I have a whole tower pc with windows 11 that I use just to run my external sas hp lto5 drive, through THIS an lsi9211, which is a pcie 2.0 x8 lanes, everything working perfectly fine. I would like to remove the tower pc, if only I can connect the tape drive to a generic usb to use it with just the laptop when I need. I found few adapters like this and I pulled the trigger. But it's not working, as I understand because my hba card is a x8 lanes, but all this adapters looks like they're just x4 lanes. I tried just covering the contacts on the card that correspond to the 5,6,7,8 lanes, as seen somewhere, but it's not working. Does anybody ever worked something like this out? For what I understand, there are not pcie x8 adapters that are affordable... are there any hba card that are x4 lanes? I found something on ebay, like this , somewhere in the description says x8, but pictures and specifics says x4. Would this card work with my drive or am I missing something else? Damn it would be a game changer for me a usbc tape drive..."},
{"Title": "Should I buy a 1TB external SSD, or a Blu-ray burner + a 50-pack BD-R spindle + 50 jewel cases?", "Author": "u/TheresThisOtherThing", "Content": "Hello, I am new here and to data hoarding in general, so please give me the for-dummies version. I need to back up some data, a little less than 1TB. I have a couple copies in the cloud with different providers, and I'd like to make a physical copy that I can put in a box and store at a relative's house. I don't expect to update or modify this copy at all, ever, just copy the files in it to my laptop if the copies in the cloud and my laptop somehow all fail at the same time. I've been poking around on Amazon and it looks like at time of writing, a 1 TB external SSD or a combination of a Blu-ray burner, a 50-pack of BD-Rs, and 50 jewel cases, will each cost around $80. Which option is better for my case, and could you point me to better options? I'm on a budget, so maybe keep it below $100 if possible. Thanks!"},
{"Title": "Using an external drive to store photos question.", "Author": "u/shdujssnensisishs", "Content": "https://support.apple.com/guide/iphone/import-and-export-photos-and-videos-iph480caa1f3/ios The link above is to appleâ€™s website that shows u how to move photos to an external device. More specifically the â€œExport photos and videos to an external storage deviceâ€ section. If I choose to do this, will my iPhone move the photos or just copy the photos and videos over and leave the originals on my phone? How would restoring the photos work? If I import it back, would the dates and location and everything still be there? Has anyone done this and can I get insight on this? Thank you!"},
{"Title": "any way to download entire galleries of art like gallery-dl?", "Author": "u/dietgilroy", "Content": "i tried using that but it didn't work at all, so i might use wfdownloader for that. i am considering trying to archive art from deviantart."},
{"Title": "The infuriating things about communicating the importance of data hoarding to the average internet user", "Author": "u/volthunter", "Content": "This is inspired by a conversation I've had which often feels cyclical about how we are losing access to most of the data on the internet, Yuzu the switch emulator was taken down and with it many clones and spin off's, Vimm's lair has had it's Nintendo roms forcefully removed, internet archive is being sued and getting cease and desists, some from Nintendo some from other companies, we've lost a multitude of pirate websites as of late, but people think piracy \" cant be defeated\". My point being that in the past 2 years more damage has been done to the current scene of the internet and preservation than all the years prior, it's not just about piracy or emulation, it's more than that, google search was a vital core of the internet and it's been ripped out. It's like the adage that there is a man in Czechoslovakia maintaining a piece of software that is crucial to the internet's existence and multiple times that has proven to be true, and now google has fallen and the internet is worse for it. A big loss that most people don't know if is that visa through their various connections to right wing christian organisations was actively campaigned by mormon extremists to revoke funding from most of the main xxx websites such as pornhub and xhamster, this seems funny, but an incredible amount of material was lost and i don't think it wise to dismiss something like this as just some minor occurrence. This may have been one of the largest data losses ever, the loss of amateur content on pornhub represented billions of hours of video that is lost to time, and PornHub conclusively proved that it was a sham case set up by a mormon church that aims to ban all adult material and even then, they were forced to shut down a retaliatory case and to shut down the amateur section of the site because of pressure from visa. The government itself has made material that many corporations would prefer you don't have access to such as fixing old factory equipment and booklets on home electric repair, with the american governments currently starting the fight to launch right to repair, you must realise that the companies would prefer if you did not have that right or information, ifixit and it's guides would be eliminated. Even the people testing products, tech channels like linus and level 1 techs would all be eliminated in time, too much finds itself at odds with these companies and you think it's hypothetical but if you upload a video tearing down an iphone and repairing it, apple may take it down, they are known to do it, same for john deer tractor hacks and lg washing machine repair videos, this is a fight, and we are losing. Anyways, i just see things getting worse and frankly it seems like we aren't bouncing back how the internet seems to think we are, the torrent scenes are no where near as alive as they used to be, even with private trackers, the scene is a shadow of what it once was, and i only see it getting worse, so hoard, data hoard it all."},
{"Title": "Upgrading from a USB JBOD", "Author": "u/doubledundercoder", "Content": "Hey all. Iâ€™ve got a usb 3.1 jbod attached to a decent laptop with 16gb of ram with a roughly 16tb zfs RAID. Works fine most of the time. I get 100mb/s reads if only one process is accessing the volume. If more than one process tryâ€™s it just tanks. Upgrading to a SAS enclosure or just getting a desktop case where I plug in the drives directly to the sata controller, what kind of performance increase can I expect for multiple processes on the volume? Am I dreaming thinking it will make a huge difference? Itâ€™s mostly for hosting Plex. Anyone gone from this setup to a direct attached on zfs? 4 7.2k sata drives"},
{"Title": "Best way to transcode entire library of videos on Google Drive preferably though Google cloud", "Author": "u/cewong2", "Content": "Iâ€™ve been searing around for a day and a half trying to find a way to transcode lots of videos I have stored in my Google drive. I havenâ€™t been able to find anyone ever having a solution (lots of what Iâ€™ve found isnâ€™t gets downloaded and reuploaded after transcoding). Hopefully this is posted in the right place otherwise please suggest a subReddit for me to post to. Lots of my videos are in x264 format and I want to transcode them into x265 as I read thereâ€™s possibly a saving of at least 20-30% but itâ€™s nearly 60-70TB which would take probably a long time compared just using Googleâ€™s free cloud trial. Can anyone recommend me a guide or tool that I can use to get this done?"},
{"Title": "NVME Speeds:  PCIe x1 , and USB3.0", "Author": "u/Anarcho_Christian", "Content": "I've got an extra Kingston NVME Drive lying around, and a lot of extra usb 3.0 ports. Would the speeds be comparable on a USB 3.0 drive vs a PCIe x1? Not \"theoretical\" speeds mind you, but like, practical speeds?"},
{"Title": "Macrium Reflect vs Arq Backup", "Author": "u/Gazumbo", "Content": "I've been using Arq for a few years. No majors issues, although I haven't had to test a full restore. I've been tempted to switch to Macrium as I already have a disk image using Macrium and switching would then mean I'm only using one piece of software instead of two. But, with regards to file and folder backup, what would I miss by switching to Macrium? I'm a home user and not knowledgeable on the more technical side of backup software. I believe Arq does a full backup and incremental from then on. I know Macrium has this option also. My main need is software that will never delete a file from a backup, no matter if I delete the source file or move it. Retaining a few versions. Bonus would be the ability to mirror this backup to the cloud in some form. Currently, I have local backup to an external drive using Arq, and cloud backup to backblaze using their personal computer license. So it's two different pieces of software but that's not a problem."},
{"Title": "Power Outage Froze my TrueNAS OS. How to Monitor Offsite TrueNAS Replication?", "Author": "u/Luz3r", "Content": "Hey everyone, I recently had a power outage at my backup location, and it seems to have messed with my TrueNAS box. It got stuck in a weird state, but thankfully a reboot fixed it. The problem is, I don't check my offsite server that often since it's a secondary location. This time, I only found out because I got an alert about my ZFS replication failing. My question is: How can I best check if my replication is actually working properly? I was thinking about setting up Uptime Kuma to monitor the offsite TrueNAS web interface. While this wouldn't necessarily tell me if replication is failing, it might catch issues like SSH being unresponsive. Ideally, I'd like to get Telegram alerts for any problems. Anyone have any suggestions for a better or cool way to monitor my TrueNAS replication health?"},
{"Title": "Can 3.5â€ external enclosure setup drive", "Author": "u/ChillCaptain", "Content": "https://www.amazon.com/SSK-External-Docking-Enclosure-Supports/dp/B08P1539VD/ref=asc_df_B08P1539VD/?hvadid=693495256271&hvpos=&hvnetw=g&hvrand=2570617204095985210&hvpone=&hvptwo=&hvqmt=&hvdev=m&hvdvcmdl=&hvlocint=&hvlocphy=9057119&hvtargid=pla-1626594922711&psc=1&mcid=086ce8faf4d5351490f73a899c982a27&gad_source=1 Iâ€™m looking to get this enclosure to help with backups. Can I insert a brand new drive and set up the drive in windows? Usually this is done in disk management where you select gpt vs mbr and select the drive letter. After setting it up can I take the drive and insert it internally through sata and use the drive? Iâ€™m wondering if the usb connection limits this?"},
{"Title": "MixesDB - A Site Archiving Mixes And Live Shows Is Shutting Down At The End Of The Month. Data Hoarders, How Can We Fully Archive The Site?", "Author": "u/FinleyGomez", "Content": "No content"},
{"Title": "What is the best way to back-up 1 PB?", "Author": "u/Early-Mechanic6508", "Content": "My job has about 900 TB worth of storage. I'm looking for a cloud-based backup. Google Drive seems to have increments of 5 TB. Should I just get 200 users on drive? Since Dropbox unlimited is dead, that does not seem to be an option. Does anyone know a cheaper way? Does anyone have any suggestions?"},
{"Title": "DAS with 2x Raid1 for working and backup storage - Could use some guidance", "Author": "u/Final_Wedding_5634", "Content": "Hobbyist Photographer and videographer looking for DAS to expand my 2019 Macbook Pro's working space and archival backup space. Cost is certainly a consideration. I have two needs: Backups - I presently have 2 x 3TB 5.4k rpm HDDs for storage in a desktop nearby. This PC needs to be replaced as its old and dying slowly. The HDDs work great - used for cold storage. the PC is off most of the time. More working space. Presently, I just keep my MBP clear of working document - i only load one project at a time and work off the internal 500GB SSD. This works but is getting tedious. I typically edit in either iMovie or Davinci Resolve (depending on project needs) and would like to move this to a larger DAS. I'm thinking I'd like 1 DAS with 4 bays. But from who? OWC? MediaSonic? Configure the two HDDs into a single Raid 1 for local backups with redundancy. I'll copy files to cold storage as needed. This data is backed up to an external drive in a fire-safe and critical docs are backedup to the cloud as well. Configure two SSDs into a single Raid 1 for faster working space redundancy. I'll use these as working space for editing two or three streams of 1080.60 video. Once projects are finished, the final result and maybe the raw footage is saved to the HDDs for archival. Questions: General thoughts for this approach? I'm sure i'm not the only one doing this but I couldn't find anything. Maybe I should just go RAID 10 or 5 or something :( Is there a specific bus type to handle 2x 1 Raid? How do i know if a DAS can handle this 2x Raid 1 configuration? is any JBOD good enough? Suggestions for affordable DAS? Raid controllers? Do i use Disk Utility or the DAS's softwareraid? Who's got the best notification of a failed drive? Speed - ideally i'd like NVMe as it's fast and future proof (i.e. I'll eventually get into 4k filming, maybe). Are there boxes that have NVMe and HDD? I couldn't find any. Would SSDs suffice (the 2.5\" variety such as evos). edited for clarity"},
{"Title": "Is this Rot?", "Author": "u/MDCasino21", "Content": "No content"},
{"Title": "Synology vs Ebay PC", "Author": "u/Suspicious_Dig_5684", "Content": "So I have a server; what I am looking for is something to put 2 18tb drives in and be a dedicated photo backup for both IPhone and Android. Would love a way to pull the pictures from ICloud and get them local as well. The wife has more pictures in the cloud then she has space on the phone and I don't have a easy way to pull those down. Is the ds223 worth it? Are there better options. Biggest concern is a simple way to backup the iPhone that doesn't require user input. We tired nextcloud works great on android but with iphone I have to constantly remind her to open the app before it would backup anything. Thanks for any help."},
{"Title": "Any DataHoarders from 3rd world countries?", "Author": "u/kkgmgfn", "Content": "How do you backup TBs of data to cloud considering that we dont have unlimited home plans even on shelling money? I am from Bangalore,India I have multiple TBs to upload, thats collected over 15yrs. Later in incremental backup is fine but initial is hard."},
{"Title": "PSA: Blocking the blank PSU slots on NetApp disk shelves is non-optional, i.e. don't screw up like I did!", "Author": "u/xxpor", "Content": "My DS4246 came from ebay with the standard 2 PSU/2 Blank slot setup. I have the left side of the shelf totally full, and the right side partially full, but mostly empty. The drives on the right side were sitting around 45c, but the drives on the left side hit 67c (!!!!). I thought the drive sensors might be lying, since they were so high. I took a closer look at the empty slots and realized that the airflow that the PSUs were moving were likely coming mostly from the back of the unit since that's a path of less resistance than flowing over the drives themselves. I printed 2 of these guys: https://www.thingiverse.com/thing:6514452 . I stuck them in the slots, and within 10 seconds the fans spun way up and started moving serious air. Now the drives are all around 35-42c, totally normal! So if you have a NetApp disk shelf, check your drive temps and make sure you block every open port. There's also an STL available to blank off IOM slots if you need it under the netapp category. Of course, they're available to buy as well."},
{"Title": "Currently have a desktop PC with a 1-bay NAS hosting my video and audio.  If I get this 12u rack with UPS and power strip with 2 vented racks, 1 to sit my PC on, other for my modem and wifi.  My next purchase will be a rackmount Synology 4-bay nas.  is this the way to go for me?", "Author": "u/Kevalemig", "Content": "No content"},
{"Title": "How can I add subtitles to my okru videos?", "Author": "u/irinaz165", "Content": "Hi! I've been trying to upload some videos to okru, I have the .srt files for the subtitles but I can't find the option to add them to my videos. I know it's possible to do it because I've seen videos with the option of turning the subtitles on and even with several languages to choose from. Thanks in advance!"},
{"Title": "What NAS Software and Mobile Apps work well for cleaning up a mess of photos and files", "Author": "u/SP4CEBAR-YT", "Content": "I have many storage devices with photos, documents, and backups of other storage devices, so there are a lot of duplicate files and photos. To fix this, I probably need a NAS with a good mobile app that can: find duplicate files and photos delete local photos after transferring them to the NAS handle errors during Photo transfers  (and avoid partial transfers that can't be resumed) optionally find if a photo is a photograph or a screenshot based on its metadata What NAS Manufacturers do you recommend based on their mobile apps and NAS software?"},
{"Title": "Storage solutions for photographers (and beyond).", "Author": "u/Uhuhgurl", "Content": "Hello all, Iâ€™m seeking advice from the experts here on finding the right storage solution for my wife and I. To give you some background, my wife is a professional photographer primarily working in digital. For many years, she has been using an external drive for her work. I, on the other hand, shoot film but recently started digitizing my photos. We had a Synology SD215 for a few years, which met many of our needs but was ultimately too slow for my wife's requirements. Additionally, we didn't fully utilize its capabilities. Now though, we believe integrating a NAS back into our workflow is exactly what weâ€™re looking for. Some requirements: A storage solution that allows multiple users to interact with it simultaneously. Large storage capacity with fast read and write speeds. RAID capability for data loss protection. Versatility for various uses, including working storage, archival storage for digital assets (primarily images), potential Plex use, and security camera support. A \"set it and forget it\" solution that won't require frequent upgrades or replacements. Until this morning I was considering the OWC Jupiter MiniÂ but I have some concerns about its network requirements (though Iâ€™m not opposed to upgrading our network if necessary). Iâ€™ve also considered a DIY approach but would prefer a solution with customer support in caseÂ we encounterÂ any issues. Any recommendations you have would be greatly appreciated. Thank you!"},
{"Title": "Indexing of images with text recognition?", "Author": "u/asheritess", "Content": "TLDR: Is there any program that will analyze my images for text and attach what it finds to its metadata or create an external index, such that I can search by text in finder/file explorer? I'm currently using Apple's Photos to store nearly all my photos, in conjunction with periodically exporting my images from iCloud to hard drives. I have a good degree of organization of my exported files, able to export them with dates attached. This is useful for finding photos from trips I know the dates of or walking down memory lane. I also backed up the actual Photo Libraries that the app loads back when they were manageable sizes that could be stored on my computer. When I did work for distance learning, downloaded images, etc, the amount of images I generated skyrocketed. Since then iCloud's image recognition and character recognition abilities have become exceptional in my opinion, even processing frames of videos. However, this is not available on the desktop app. My efficiency has increased, as on my phone I can search my indexed library going back years by date, location, subject, and text. As of late I have been delighted to see that I can also search by text for images in folders on my Mac. I want to reap these benefits on an external storage drive. I've seen multiple image library applications with OCR capabilities. I understand that they are not able to handle such a large volume of images. I do like the convenience of scrolling through a library and am interested in hearing suggestions for these, although I have done some research myself. An issue that it poses is (in my experience) libraries doubling the amount of space my images take up by making their own copies for the library, and if I attempted to put all my photos into libraries that would take up so much space. I wonder if an idea I have has been executed by anyone other than Apple: is there any program that will alter the metadata, or create an index independent of a library app, such that I could search by text within Finder(/file explorer)? I want to use this feature on my backup drives and have all my images processed like what my laptop is currently doing locally. Or alternatively, is there a way to use Apple's feature on an external drive?"},
{"Title": "Searching for a decent 5+ bay JBOD with UASP", "Author": "u/ShapeShifter499", "Content": "Is there a decent 5 or more bay JBOD enclosure that supports UASP? My primary use is with a Linux machine via usb 3.0. I tried a terramaster D5-300C but I'm getting a load of I/O errors that seem related to UASP capping out. I may return it unless there's a software fix. Should I just resort to not having UASP on JBOD?"},
{"Title": "QNAP DAS for Plex server", "Author": "u/mxz117", "Content": "I'm looking at needing a storage upgrade for my plex server and a DAS seems to be the best option. It's already running on a champ of a little mini pc, already have an external SSD but I need more storage. I've found the QNAP TR-004 which looks like it would work well for me I've just got some questions about it. That I can't really seem to find answers about. Do I HAVE to use server drives? They seem to be way more expensive, logically thinking the drives would only need to be active when somebody is watching something, which isn't constantly as there's only about 3 users. Do I have to fill all 4 slots? Or could I fill 2, then add another 2 in the future when needed. (And do they all have to be the same drive/capacity/speed) And how would this go for future proofing vs a NAS? Would it be viable to just buy another of these DAS's to add more storage and plug into the computer or would a more expensive NAS be a better option?"},
{"Title": "Can you download long videos from VK.com", "Author": "u/raisedbyowls", "Content": "I know itâ€™s an account only thing, but itâ€™s easy to reg and thereâ€™s so much stuff, but I discovered that nothing can help me to get the video lasting over an hour. Itâ€™s a legal video, just in case."},
{"Title": "Any way to do an automatic \"on-demand\" setup involving an SSD and an HDD, where the SSD has only my recent active files, and everything else gets offloaded to the HDD until I try to access it again?", "Author": "u/OliveBranchMLP", "Content": "Howdy, I'm a film editor looking for an \"active\" solution to my problem. I have tried Googling this with terms like \"cold storage\" and \"file retrieval\" but those are giving me results that aren't applicable to my use-case scenario. I'm sure this setup exists, I just have no idea what it's called or what words to search for it. My current setup: I have two drives: an \"active\" 2TB NVMe SSD, where I put all the stuff I'm actively working on an \"offline\" 24TB SATA HDD, where I put all the stuff I'm not actively working on The folder structure is identical between the drives, but to optimize space, no actual data is duplicated between them. (Yes, I have both drives backed up to a third drive.) The problem: It's getting cumbersome having to manually move files to and from these drives as I need them, especially while preserving the file structure. I always have to dig into every folder to move the individual files because I don't need EVERYTHING in that folder. And it's hard to keep them in sync or avoid file conflicts because every syncing program I've found tries to sync EVERYTHING. What I want: Is there a way to automate this process, so that I only ever look at my SSD, but my SSD shows me everything that's on the HDD too as if they're one drive, and when I open a file on the SSD, it transfers it over from my HDD? Any help would be appreciated!"},
{"Title": "Using Photorec to scrape an old HDD", "Author": "u/Vast-Avocado-6321", "Content": "Hey all, I'm using \"Photorec\" that's included in \"TestDisk\" (i.e. free software that searches for deleted photos on storage mediums) to scrape some drives for .jpgs that used to be in an old PC of mine. Here's my conundrum: I initially scraped this drive for all .jpg files and it pulled about 11,000 files (this was a year ago). I'm now running the same program again, and searching for .jpg and it's only pulling about 3,000 files. I initially thought that maybe the data search was interrupted so I ran it again, and it pulled about the same amount. What's going on here? Is this software just not as robust as I thought? Should I try a paid software? Did the jpgs disappear from the sectors during the initial scrape and now, even though the drive has not been in use? Thanks."},
{"Title": "Good used document scanner", "Author": "u/OkBackground5843", "Content": "I'm shopping around on eBay for a document scanner. I'm hoping to get something good for under $150 which is why I'm shopping used. I see quite a few Canon DR-M160's within my price range. It seems like this is/was a fairly high end scanner, will it hold up well for home use?"},
{"Title": "How would you manage data storage on a pre-failing drive? Ensuring integrity is a risky context", "Author": "u/I-need-a-proper-nick", "Content": "Hi all, While the usual practice is to trash (pre-)failing external USB-powered drives, I tend to like to keep them as additional backup storage (like a 3rd copy) and some served me surprisingly well over the years. First, I simply copied the data in there and leave it be. Then I wondered how to make the thing more robust and I used checksums over the whole content which were periodically ran in order to ensure integrity Now, I'd like to know if you had other suggestions on how to store data in risky situations such as failing or pre-failing drives? Ideally, I'd like to be able to have a way to check integrity or maybe have self-healing ability (I looked into Multipar PAR2 but there're too many files to do so) inside the drive itself Thank you for your inputs."},
{"Title": "JMCD 12S4 12-bay case with backplane", "Author": "u/pavoganso", "Content": "This looks like a Jonsbo-killer with a decent amount of drive bays. Looks like my dream case based on the form factor. https://preview.redd.it/jmcd-12s4-12-bay-case-with-backplane-v0-948ajvjeek4d1.jpg https://preview.redd.it/jmcd-12s4-12-bay-case-with-backplane-v0-2kf5zl9gek4d1.jpg https://preview.redd.it/jmcd-12s4-12-bay-case-with-backplane-v0-6e3kutphek4d1.jpg Any thoughts or anyone tried it yet?"},
{"Title": "My ideal setup (theoretical)", "Author": "u/Balnian", "Content": "Hi fellow Homelaber! Recently I had to part way with my modest homelab setup (2x R710) because I'm moving to Japan (I don't think I need to explain why R710 and Japan don't work together). I've been looking for a replacement setup (mostly a NAS) that would sip power and leaning toward a one box solution to save on CPU power overhead. Some of the requirements I've come up with: Must have a NAS Component for backups/storage Must have enough compute power for simple services (Jellyfin, VPN, Hass) Should be able to do some basic transcoding (Jellyfin) preferably AV1 Nice to have would be a NPU for basic AI task (Frigate/Hass) Inspired by the Asustor Flashstor ( gen 1 & gen 2 ), Minisforum MS-01 and some perusing of the intel spec sheets I found this \"gem\": Intel Core Ultra 5 125H . What's interesting with this processor are the 28 PCIe lanes which would theoretically allow for 12 M.2 SSD with   2 PCIe lanes each and still have 4 PCIe lanes free (can add NIC or other). It also has hardware support for AV1 transcoding and an NPU. Finally, since it's a 5 series processor it should have plenty of power for my needs. System configuration The configuration I came up with is interesting because it has 4 drives with a higher throughput (Gen5 x2) if we use SSDs like the Samsung 990 EVO which support PCIeÂ® 4.0 x4 or 5.0 x2 interface. N.B.: An alternative config. would be a 6 drives config with all drives connected to a x4 interface Finally, this is just a theoretical configuration, a bunch of stuff could go wrong like improper support for the PCIe bifurcation for the x1 interface of the CPU. But I'm also bullish on this and would love to see an equivalent spec'd CPU from the newly announced Lunar Lake architecture generation (better energy efficiency, more powerful CPU, GPU and NPU with better power management) . Some discussion points for the community: What are your thoughts? What would you change to better fit your needs? Any improvement suggestions? Which manufacturer would you like to see try to bring this to market? What would you run on such a device?"},
{"Title": "My oldest Mp3s turn 25 this year!", "Author": "u/Over_Contact_5032", "Content": "No content"},
{"Title": "NVME SSD Enclosures/Docks Recommendations?", "Author": "u/Jasonwj322a", "Content": "Desperately need new enclosures as my current sabrent ones keeps disconnecting randomly. I've tried new cables so I don't believe that is the issue. Maybe the connector is worn? Either way, I checked out many options, but am having a hard time deciding since the well reviewed ones can cost upwards of $150. Sabrent offers a no frills enclosure for $30, but they also have a docking station which supports 2 SSD for $120. The latter also has a fan, which is neat since I have to point a cooling fan on my current ones. Though I am aware that it will be another point of failure. Curious what you guys are using? I know ZikeDrive and Orico are also well reviewed as well."},
{"Title": "Microfiche without equipment, or Microfiche alternatives?", "Author": "u/busybeeworking", "Content": "I want to archive a large number of documents, but don't want to do it on something that requires equipment that is expensive or could break. I heard using a flashlight and magnifying glass would be possible but annoying. I also heard using photographic slides but I only saw information for that for images, not written documents. Thoughts? Edit: I'm going to buy the archived material, the thing us I need a way to view it without equipment"},
{"Title": "80TB across 6 drives (with backup)", "Author": "u/19wolf", "Content": "I'm migrating my giant 8-bay CS381 into a 6-bay Ugreen, mostly for physical size reduction (and partly because the 8-bay model was $300 more expensive for some reason?). I bought two of them so I'll have an active+backup and I'm trying to decide how to set everything up. I absolutely do not want to use UGreen's OS. Right now I'm using UnRAID, but previously used Debian/MergerFS. I'm thinking maybe I'll try out TrueNAS? Or else stick with Unraid. I have 4x16tb drives (plus some 12/14tb), and am torn between buying 8 more to fill out the 6x2=12 bays, or going all in and getting 12x20tb drives. I'm not sure how I want to lay everything out. Currently I'm using 68 of 80TB, and I think it grows slowly enough to still last me a bit. I'll have the backup system and don't really mind downtime so I'm wondering if RAIDz/1 parity is enough for my purposes? Or does it really make the difference to do RAIDz2 with 20tb drives? Or maybe I should just skip RAID/parity altogether? What would you do?"},
{"Title": "Good NAS for video storage in 2024?", "Author": "u/DieserCoookie", "Content": "So atm im using a external harddrive connected to my Fritz!Router but im starting to run out of space on that one so i was wondering if maybe it's time to invest into a NAS and some huge harddrives. I stream the videos over network if that is a point to consider. So any suggestions?"},
{"Title": "Best small long-term storage medium", "Author": "u/ondsinet", "Content": "Hello. This might be a bit of an uncommon use case, so i haven't been able to find much info on the internet. I'd like to make a backup copy of my (videogame) cd collection to a more modern form factor (my computer doesn't have a cd reader). Instead of one big hard disk, I would like to dedicate an individual \"device\" to each game/game series, so storage space of around 1GB to 100GB, possibly in an easily stackble form factor. I know that CDs are perfect for this, i know i should just get an usb cd reader (i will), that's what all the search results said. I was wondering if there is any other medium that's suitable for this. USB sticks and SD cards fit every need, except they apparently won't keep data reliably when unpowered for years. Sorry if this is a stupid question, or this isn't the right place, but i haven't had to deal with a similar issue before ( i just copy all my other stuff to normal hard drives, but i wanted to keep the collection feel for these games), so feel free to kick me towards the right direction if I'm missing something obvious."},
{"Title": "Anyone using VueScan with a Fujitsu ADF scanner?", "Author": "u/Primary_Season7533", "Content": "I got a Fujitsu fi-7160 to digitize my family's photos. Had really high hopes for this scanner since Fujitsu seems very reliable and it has CCD sensors. I've been very disappointed with the image quality from their own software, Paperstream Capture, so I turned to VueScan. However, there seem to be something that's fundamentally wrong with this software or the combination of the software and scanner. I get really unpredictable results, cropping some photos but not others, settings not updating when I press the buttons â€“ then updating next time I scan, duplex only works sometimes (not set to skip blank pages)... the list of bugs goes on. Yes, I wrote to Ed Hamrick who asked me for a bug report. Did my best to write one but never heard back. Saw an April update (VueScan 9.8.32) was supposed to address Fujitsu scanner issues but seems to only be worse for me compared to earlier. Have tried NAPS2 and it seems to work super well so hardware should be fine. Anyone who got Fujitsu ADF-scanners to work with VueScan?"},
{"Title": "Is ext4/XFS really better than ZFS/BTRFS for reliability?", "Author": "u/No-Balance-8038", "Content": "I have had a btrfs raid1c3 out of 3x20TB disks myself. It was really unhelpful when I used AOSP (via USB) as it didnt tell me why it did make it read only... not using AOSP would have helped Recently I switched over to a real server, with a HBA and immediate connection. Then I googled, and the following points were made: btrfs: Super bad because silently is overfilled and getting NOENT - all disk used, but regular rebalance is completely not recommended to avoid this issue altogether. Apparently misrepresents actual disk space. ZFS: Super bad because will never be in Linux tree, and is hard to maintain, apparently \"just hype\" https://www.reddit.com/r/zfs/comments/sfo1tq/linus_tech_tips_fails_at_using_zfs_properly_loses/ https://storytime.ivysaur.me/posts/why-not-zfs/ https://github.com/openzfs/zfs/labels/Type%3A%20Defect And then we had that corruption bug in ZFS. Backups are most important! But then people in ZFS IRC tell me to instead use multiple different filesystems and just hope to have one that doesnt break, and to start with ext4, cause its the easiest to repair... [...] So in the end I read, use ext4 or XFS on a single disk but have 2 offline backups offsite. Whats your verdict? My server is as follows: 64GB registered ECC RAM (single bit correction) Intel i3-9100 10x20TB HDD installed - 10 slots empty 2x2TB SSD SuperMicro X11SCL-F Motherboard My Data is unique in the sense of that I put lots of time in it, and that I would not want to re-do all that stuff at any given point."},
{"Title": "WD passport transfer speed accurate?", "Author": "u/bananamuffinsareyum", "Content": "Hi, Iâ€™m transferring files from a Samsung T7 ssd to my WD passport HDD through davinci resolve clone tool. 1 TB of 6k Blackmagic video footage. Itâ€™s taking about 10-12 hours estimated. Is this normal? I feel like my Lacie did transfers faster back when I used it. Wondering if anyone else has been in same situation."},
{"Title": "Anyone have a clean workflow for automating youtube channels to emby?", "Author": "u/theresmorethan42", "Content": "There are bits and kinda jumbled together things all over the place, bur does anyone know of a tool that just does the thing? ie. I feed it a list of channels/playlists and it dumps the right metadata/.nfo to feed emby properly?"},
{"Title": "Can I Use a Surveillance HDD as a nas hdd?", "Author": "u/osrott", "Content": "Hey, I'm looking for a new nas hdds for my  ds215+. Can I safely use a Surveillance HDD?"},
{"Title": "Is DiskGenius a good tool to check refurbished HDDs?", "Author": "u/Ok_Fish285", "Content": "No content"},
{"Title": "How can I add subtitles to my okru videos?", "Author": "u/irinaz165", "Content": "Hi! I've been trying to upload some videos to okru, I have the .srt files for the subtitles but I can't find the option to add them to my videos. I know it's possible to do it because I've seen videos with the option of turning the subtitles on and even with several languages to choose from. Thanks in advance!"},
{"Title": "What NAS Software and Mobile Apps work well for cleaning up a mess of photos and files", "Author": "u/SP4CEBAR-YT", "Content": "I have many storage devices with photos, documents, and backups of other storage devices, so there are a lot of duplicate files and photos. To fix this, I probably need a NAS with a good mobile app that can: find duplicate files and photos delete local photos after transferring them to the NAS handle errors during Photo transfers  (and avoid partial transfers that can't be resumed) optionally find if a photo is a photograph or a screenshot based on its metadata What NAS Manufacturers do you recommend based on their mobile apps and NAS software?"},
{"Title": "Storage solutions for photographers (and beyond).", "Author": "u/Uhuhgurl", "Content": "Hello all, Iâ€™m seeking advice from the experts here on finding the right storage solution for my wife and I. To give you some background, my wife is a professional photographer primarily working in digital. For many years, she has been using an external drive for her work. I, on the other hand, shoot film but recently started digitizing my photos. We had a Synology SD215 for a few years, which met many of our needs but was ultimately too slow for my wife's requirements. Additionally, we didn't fully utilize its capabilities. Now though, we believe integrating a NAS back into our workflow is exactly what weâ€™re looking for. Some requirements: A storage solution that allows multiple users to interact with it simultaneously. Large storage capacity with fast read and write speeds. RAID capability for data loss protection. Versatility for various uses, including working storage, archival storage for digital assets (primarily images), potential Plex use, and security camera support. A \"set it and forget it\" solution that won't require frequent upgrades or replacements. Until this morning I was considering the OWC Jupiter MiniÂ but I have some concerns about its network requirements (though Iâ€™m not opposed to upgrading our network if necessary). Iâ€™ve also considered a DIY approach but would prefer a solution with customer support in caseÂ we encounterÂ any issues. Any recommendations you have would be greatly appreciated. Thank you!"},
{"Title": "Indexing of images with text recognition?", "Author": "u/asheritess", "Content": "TLDR: Is there any program that will analyze my images for text and attach what it finds to its metadata or create an external index, such that I can search by text in finder/file explorer? I'm currently using Apple's Photos to store nearly all my photos, in conjunction with periodically exporting my images from iCloud to hard drives. I have a good degree of organization of my exported files, able to export them with dates attached. This is useful for finding photos from trips I know the dates of or walking down memory lane. I also backed up the actual Photo Libraries that the app loads back when they were manageable sizes that could be stored on my computer. When I did work for distance learning, downloaded images, etc, the amount of images I generated skyrocketed. Since then iCloud's image recognition and character recognition abilities have become exceptional in my opinion, even processing frames of videos. However, this is not available on the desktop app. My efficiency has increased, as on my phone I can search my indexed library going back years by date, location, subject, and text. As of late I have been delighted to see that I can also search by text for images in folders on my Mac. I want to reap these benefits on an external storage drive. I've seen multiple image library applications with OCR capabilities. I understand that they are not able to handle such a large volume of images. I do like the convenience of scrolling through a library and am interested in hearing suggestions for these, although I have done some research myself. An issue that it poses is (in my experience) libraries doubling the amount of space my images take up by making their own copies for the library, and if I attempted to put all my photos into libraries that would take up so much space. I wonder if an idea I have has been executed by anyone other than Apple: is there any program that will alter the metadata, or create an index independent of a library app, such that I could search by text within Finder(/file explorer)? I want to use this feature on my backup drives and have all my images processed like what my laptop is currently doing locally. Or alternatively, is there a way to use Apple's feature on an external drive?"},
{"Title": "Searching for a decent 5+ bay JBOD with UASP", "Author": "u/ShapeShifter499", "Content": "Is there a decent 5 or more bay JBOD enclosure that supports UASP? My primary use is with a Linux machine via usb 3.0. I tried a terramaster D5-300C but I'm getting a load of I/O errors that seem related to UASP capping out. I may return it unless there's a software fix. Should I just resort to not having UASP on JBOD?"},
{"Title": "QNAP DAS for Plex server", "Author": "u/mxz117", "Content": "I'm looking at needing a storage upgrade for my plex server and a DAS seems to be the best option. It's already running on a champ of a little mini pc, already have an external SSD but I need more storage. I've found the QNAP TR-004 which looks like it would work well for me I've just got some questions about it. That I can't really seem to find answers about. Do I HAVE to use server drives? They seem to be way more expensive, logically thinking the drives would only need to be active when somebody is watching something, which isn't constantly as there's only about 3 users. Do I have to fill all 4 slots? Or could I fill 2, then add another 2 in the future when needed. (And do they all have to be the same drive/capacity/speed) And how would this go for future proofing vs a NAS? Would it be viable to just buy another of these DAS's to add more storage and plug into the computer or would a more expensive NAS be a better option?"},
{"Title": "Can you download long videos from VK.com", "Author": "u/raisedbyowls", "Content": "I know itâ€™s an account only thing, but itâ€™s easy to reg and thereâ€™s so much stuff, but I discovered that nothing can help me to get the video lasting over an hour. Itâ€™s a legal video, just in case."},
{"Title": "Any way to do an automatic \"on-demand\" setup involving an SSD and an HDD, where the SSD has only my recent active files, and everything else gets offloaded to the HDD until I try to access it again?", "Author": "u/OliveBranchMLP", "Content": "Howdy, I'm a film editor looking for an \"active\" solution to my problem. I have tried Googling this with terms like \"cold storage\" and \"file retrieval\" but those are giving me results that aren't applicable to my use-case scenario. I'm sure this setup exists, I just have no idea what it's called or what words to search for it. My current setup: I have two drives: an \"active\" 2TB NVMe SSD, where I put all the stuff I'm actively working on an \"offline\" 24TB SATA HDD, where I put all the stuff I'm not actively working on The folder structure is identical between the drives, but to optimize space, no actual data is duplicated between them. (Yes, I have both drives backed up to a third drive.) The problem: It's getting cumbersome having to manually move files to and from these drives as I need them, especially while preserving the file structure. I always have to dig into every folder to move the individual files because I don't need EVERYTHING in that folder. And it's hard to keep them in sync or avoid file conflicts because every syncing program I've found tries to sync EVERYTHING. What I want: Is there a way to automate this process, so that I only ever look at my SSD, but my SSD shows me everything that's on the HDD too as if they're one drive, and when I open a file on the SSD, it transfers it over from my HDD? Any help would be appreciated!"},
{"Title": "Using Photorec to scrape an old HDD", "Author": "u/Vast-Avocado-6321", "Content": "Hey all, I'm using \"Photorec\" that's included in \"TestDisk\" (i.e. free software that searches for deleted photos on storage mediums) to scrape some drives for .jpgs that used to be in an old PC of mine. Here's my conundrum: I initially scraped this drive for all .jpg files and it pulled about 11,000 files (this was a year ago). I'm now running the same program again, and searching for .jpg and it's only pulling about 3,000 files. I initially thought that maybe the data search was interrupted so I ran it again, and it pulled about the same amount. What's going on here? Is this software just not as robust as I thought? Should I try a paid software? Did the jpgs disappear from the sectors during the initial scrape and now, even though the drive has not been in use? Thanks."},
{"Title": "Good used document scanner", "Author": "u/OkBackground5843", "Content": "I'm shopping around on eBay for a document scanner. I'm hoping to get something good for under $150 which is why I'm shopping used. I see quite a few Canon DR-M160's within my price range. It seems like this is/was a fairly high end scanner, will it hold up well for home use?"},
{"Title": "How would you manage data storage on a pre-failing drive? Ensuring integrity is a risky context", "Author": "u/I-need-a-proper-nick", "Content": "Hi all, While the usual practice is to trash (pre-)failing external USB-powered drives, I tend to like to keep them as additional backup storage (like a 3rd copy) and some served me surprisingly well over the years. First, I simply copied the data in there and leave it be. Then I wondered how to make the thing more robust and I used checksums over the whole content which were periodically ran in order to ensure integrity Now, I'd like to know if you had other suggestions on how to store data in risky situations such as failing or pre-failing drives? Ideally, I'd like to be able to have a way to check integrity or maybe have self-healing ability (I looked into Multipar PAR2 but there're too many files to do so) inside the drive itself Thank you for your inputs."},
{"Title": "JMCD 12S4 12-bay case with backplane", "Author": "u/pavoganso", "Content": "This looks like a Jonsbo-killer with a decent amount of drive bays. Looks like my dream case based on the form factor. https://preview.redd.it/jmcd-12s4-12-bay-case-with-backplane-v0-948ajvjeek4d1.jpg https://preview.redd.it/jmcd-12s4-12-bay-case-with-backplane-v0-2kf5zl9gek4d1.jpg https://preview.redd.it/jmcd-12s4-12-bay-case-with-backplane-v0-6e3kutphek4d1.jpg Any thoughts or anyone tried it yet?"},
{"Title": "My ideal setup (theoretical)", "Author": "u/Balnian", "Content": "Hi fellow Homelaber! Recently I had to part way with my modest homelab setup (2x R710) because I'm moving to Japan (I don't think I need to explain why R710 and Japan don't work together). I've been looking for a replacement setup (mostly a NAS) that would sip power and leaning toward a one box solution to save on CPU power overhead. Some of the requirements I've come up with: Must have a NAS Component for backups/storage Must have enough compute power for simple services (Jellyfin, VPN, Hass) Should be able to do some basic transcoding (Jellyfin) preferably AV1 Nice to have would be a NPU for basic AI task (Frigate/Hass) Inspired by the Asustor Flashstor ( gen 1 & gen 2 ), Minisforum MS-01 and some perusing of the intel spec sheets I found this \"gem\": Intel Core Ultra 5 125H . What's interesting with this processor are the 28 PCIe lanes which would theoretically allow for 12 M.2 SSD with   2 PCIe lanes each and still have 4 PCIe lanes free (can add NIC or other). It also has hardware support for AV1 transcoding and an NPU. Finally, since it's a 5 series processor it should have plenty of power for my needs. System configuration The configuration I came up with is interesting because it has 4 drives with a higher throughput (Gen5 x2) if we use SSDs like the Samsung 990 EVO which support PCIeÂ® 4.0 x4 or 5.0 x2 interface. N.B.: An alternative config. would be a 6 drives config with all drives connected to a x4 interface Finally, this is just a theoretical configuration, a bunch of stuff could go wrong like improper support for the PCIe bifurcation for the x1 interface of the CPU. But I'm also bullish on this and would love to see an equivalent spec'd CPU from the newly announced Lunar Lake architecture generation (better energy efficiency, more powerful CPU, GPU and NPU with better power management) . Some discussion points for the community: What are your thoughts? What would you change to better fit your needs? Any improvement suggestions? Which manufacturer would you like to see try to bring this to market? What would you run on such a device?"},
{"Title": "My oldest Mp3s turn 25 this year!", "Author": "u/Over_Contact_5032", "Content": "No content"},
{"Title": "NVME SSD Enclosures/Docks Recommendations?", "Author": "u/Jasonwj322a", "Content": "Desperately need new enclosures as my current sabrent ones keeps disconnecting randomly. I've tried new cables so I don't believe that is the issue. Maybe the connector is worn? Either way, I checked out many options, but am having a hard time deciding since the well reviewed ones can cost upwards of $150. Sabrent offers a no frills enclosure for $30, but they also have a docking station which supports 2 SSD for $120. The latter also has a fan, which is neat since I have to point a cooling fan on my current ones. Though I am aware that it will be another point of failure. Curious what you guys are using? I know ZikeDrive and Orico are also well reviewed as well."},
{"Title": "Microfiche without equipment, or Microfiche alternatives?", "Author": "u/busybeeworking", "Content": "I want to archive a large number of documents, but don't want to do it on something that requires equipment that is expensive or could break. I heard using a flashlight and magnifying glass would be possible but annoying. I also heard using photographic slides but I only saw information for that for images, not written documents. Thoughts? Edit: I'm going to buy the archived material, the thing us I need a way to view it without equipment"},
{"Title": "80TB across 6 drives (with backup)", "Author": "u/19wolf", "Content": "I'm migrating my giant 8-bay CS381 into a 6-bay Ugreen, mostly for physical size reduction (and partly because the 8-bay model was $300 more expensive for some reason?). I bought two of them so I'll have an active+backup and I'm trying to decide how to set everything up. I absolutely do not want to use UGreen's OS. Right now I'm using UnRAID, but previously used Debian/MergerFS. I'm thinking maybe I'll try out TrueNAS? Or else stick with Unraid. I have 4x16tb drives (plus some 12/14tb), and am torn between buying 8 more to fill out the 6x2=12 bays, or going all in and getting 12x20tb drives. I'm not sure how I want to lay everything out. Currently I'm using 68 of 80TB, and I think it grows slowly enough to still last me a bit. I'll have the backup system and don't really mind downtime so I'm wondering if RAIDz/1 parity is enough for my purposes? Or does it really make the difference to do RAIDz2 with 20tb drives? Or maybe I should just skip RAID/parity altogether? What would you do?"},
{"Title": "Good NAS for video storage in 2024?", "Author": "u/DieserCoookie", "Content": "So atm im using a external harddrive connected to my Fritz!Router but im starting to run out of space on that one so i was wondering if maybe it's time to invest into a NAS and some huge harddrives. I stream the videos over network if that is a point to consider. So any suggestions?"},
{"Title": "Best small long-term storage medium", "Author": "u/ondsinet", "Content": "Hello. This might be a bit of an uncommon use case, so i haven't been able to find much info on the internet. I'd like to make a backup copy of my (videogame) cd collection to a more modern form factor (my computer doesn't have a cd reader). Instead of one big hard disk, I would like to dedicate an individual \"device\" to each game/game series, so storage space of around 1GB to 100GB, possibly in an easily stackble form factor. I know that CDs are perfect for this, i know i should just get an usb cd reader (i will), that's what all the search results said. I was wondering if there is any other medium that's suitable for this. USB sticks and SD cards fit every need, except they apparently won't keep data reliably when unpowered for years. Sorry if this is a stupid question, or this isn't the right place, but i haven't had to deal with a similar issue before ( i just copy all my other stuff to normal hard drives, but i wanted to keep the collection feel for these games), so feel free to kick me towards the right direction if I'm missing something obvious."},
{"Title": "Anyone using VueScan with a Fujitsu ADF scanner?", "Author": "u/Primary_Season7533", "Content": "I got a Fujitsu fi-7160 to digitize my family's photos. Had really high hopes for this scanner since Fujitsu seems very reliable and it has CCD sensors. I've been very disappointed with the image quality from their own software, Paperstream Capture, so I turned to VueScan. However, there seem to be something that's fundamentally wrong with this software or the combination of the software and scanner. I get really unpredictable results, cropping some photos but not others, settings not updating when I press the buttons â€“ then updating next time I scan, duplex only works sometimes (not set to skip blank pages)... the list of bugs goes on. Yes, I wrote to Ed Hamrick who asked me for a bug report. Did my best to write one but never heard back. Saw an April update (VueScan 9.8.32) was supposed to address Fujitsu scanner issues but seems to only be worse for me compared to earlier. Have tried NAPS2 and it seems to work super well so hardware should be fine. Anyone who got Fujitsu ADF-scanners to work with VueScan?"},
{"Title": "Is ext4/XFS really better than ZFS/BTRFS for reliability?", "Author": "u/No-Balance-8038", "Content": "I have had a btrfs raid1c3 out of 3x20TB disks myself. It was really unhelpful when I used AOSP (via USB) as it didnt tell me why it did make it read only... not using AOSP would have helped Recently I switched over to a real server, with a HBA and immediate connection. Then I googled, and the following points were made: btrfs: Super bad because silently is overfilled and getting NOENT - all disk used, but regular rebalance is completely not recommended to avoid this issue altogether. Apparently misrepresents actual disk space. ZFS: Super bad because will never be in Linux tree, and is hard to maintain, apparently \"just hype\" https://www.reddit.com/r/zfs/comments/sfo1tq/linus_tech_tips_fails_at_using_zfs_properly_loses/ https://storytime.ivysaur.me/posts/why-not-zfs/ https://github.com/openzfs/zfs/labels/Type%3A%20Defect And then we had that corruption bug in ZFS. Backups are most important! But then people in ZFS IRC tell me to instead use multiple different filesystems and just hope to have one that doesnt break, and to start with ext4, cause its the easiest to repair... [...] So in the end I read, use ext4 or XFS on a single disk but have 2 offline backups offsite. Whats your verdict? My server is as follows: 64GB registered ECC RAM (single bit correction) Intel i3-9100 10x20TB HDD installed - 10 slots empty 2x2TB SSD SuperMicro X11SCL-F Motherboard My Data is unique in the sense of that I put lots of time in it, and that I would not want to re-do all that stuff at any given point."},
{"Title": "WD passport transfer speed accurate?", "Author": "u/bananamuffinsareyum", "Content": "Hi, Iâ€™m transferring files from a Samsung T7 ssd to my WD passport HDD through davinci resolve clone tool. 1 TB of 6k Blackmagic video footage. Itâ€™s taking about 10-12 hours estimated. Is this normal? I feel like my Lacie did transfers faster back when I used it. Wondering if anyone else has been in same situation."},
{"Title": "Anyone have a clean workflow for automating youtube channels to emby?", "Author": "u/theresmorethan42", "Content": "There are bits and kinda jumbled together things all over the place, bur does anyone know of a tool that just does the thing? ie. I feed it a list of channels/playlists and it dumps the right metadata/.nfo to feed emby properly?"},
{"Title": "Can I Use a Surveillance HDD as a nas hdd?", "Author": "u/osrott", "Content": "Hey, I'm looking for a new nas hdds for my  ds215+. Can I safely use a Surveillance HDD?"},
{"Title": "Is DiskGenius a good tool to check refurbished HDDs?", "Author": "u/Ok_Fish285", "Content": "No content"},
{"Title": "MEGA sending data while downloading", "Author": "u/CorgiFun1874", "Content": "Hi everyone ! I'm making a local backup of files I store on MEGA servers, using their MEGAsync software. But turns out after downloading 7 GB of data, it uploaded 350 MB, and it keeps growing. I couldn't find any information about it. What does MEGA send when downloading ? Signatures and tracking files are no way that heavy !"},
{"Title": "I *feel* like I've been adding less and less to my server as the 'old media I want to collect runs out, and it becomes only a trickle of new media', but my spreadsheet tells me that it's all in my head...", "Author": "u/AshleyUncia", "Content": "No content"},
{"Title": "Questions on magnetic tape for storage (Now that prices have changed, apparently)", "Author": "u/Alemismun", "Content": "Yesterday I saw some article about how magnetic tape was much cheaper per TB than hard disks (and you could drop them like SSDs, which is always nice), and this has made me consider if I should perhaps switch... But its really hard to get any reliable info on the matter, which is why Im making this post. Is magnetic tape actually cheaper? Some older reddit posts say that this is not the case (and indeed, the latest LTO tape is much more expensive per TB than a hard drive), but at the same time, random websites on the internet seem to pitch some wild prices, such as this one (or the same one from another site ) (or the same one from amazon , wait wtf why are all the cheap ones the same model), which comes at about 6 bucks per terabyte of data. This is way better than even a used hard drive. Are there any hidden costs or technical issues that one could expect? Or does it just behave like a hard drive but with much lower speeds? Do they wear out or are they expected to last more than hard drives? Is there any issue with buying used? Is it even possible to encrypt this data (in particular with Veracrypt)? This is kind of a dealbreaker for me otherwise, as I greatly value data security. I generally store lots of random data across many folders, in the form of a digital library. Generally books/films by category. Plus random family pictures and work documents here and there. Speed is not much of a concern, nor is having the storage cold. My hard drives usually take all night to sync, then they stay disconnected until needed again."},
{"Title": "Are there any legit m-disks currently for sale?", "Author": "u/Gwyn777", "Content": "Of course I see the ones on amazon, but they look sketchy. After hearing of disks being labeled as m-disc but not truly them, I am skeptical."},
{"Title": "S3 Glacier Deep Archive VS Backblaze B2", "Author": "u/Ryhaph99", "Content": "I chose B2 because, although I might be able to save some money with glacier deep archive, AWS pricing is so confusing that I feel like I'm gonna fuck it up and owe them a ton of money. B2 was much easier to understand and say let's go. Is AWS intentionally making it confusing or is that just how they are able to make it cheap in specific use cases? Am I just dense? B2 has been great so far but ultimately I'd rather be paying $0.6/TB/month than $6/TB/month for my offsite backups. I'm also considering the \"at a friend's house\" solution. Maybe there are even co-lo facilities for this kinda thing that could be competitive in my area. I'm more curious about cloud solutions than guerilla solutions though, so please let me know if you have any insight. TL;DR: most cost effective cloud solution for data hoarding is desired, do you have it?"},
{"Title": "Scanning trifolded statements", "Author": "u/briko3", "Content": "I have many sets of client statements that run about 20 pages front and back and were trifolded into an envelope. I have an Epson 410 scanner and no matter how much I try to flatten these statements, it can never reliably feed one page at a time. It always ends up clumping several together throughout the scan, which has been frustrating. So my question is, does anyone know of a scanner that does well in these kinds of circumstances? Thanks"},
{"Title": "Soundcloud Database", "Author": "u/yonah_xy", "Content": "hello, new here. Iâ€™m a HUGE fan of this soundcloud artist 9TAILS. Iâ€™m looking into archiving him just in case and iâ€™m aiming at OG files, so the wavs. I was hoping someome here could help me out or someone who has access to it. Itâ€™s a lot but iâ€™m just a fan!"},
{"Title": "Recommended fans for Norco 4220 (4U, 20 HDD bays) case? (re: 80mm fans)", "Author": "u/sofakng", "Content": "I've been using a Norco 4220 case for quite a while but I think it's time to replace the fans. Behind the 20-bay HDDs are 4x Delta AFB0812H fans.  They are loud, which is fine for now but it would be nice to get something quieter if I need to move the server. There are also two empty rear fans (80mm) that I would like to populate. I was thinking about buying 4x Artic P8 Max (HDD bays) and 2x Artic P8 PWM (rear case).  Would this be a good choice or does anybody have any other recommendations?"},
{"Title": "M3 Pro Mac External SSD pauses/freezes", "Author": "u/mishrah10", "Content": "I recently purchased an internal ssd and combined it with an enclosure to make it external ssd. Now on my personal M3 Pro Macbook the ssd just pauses, the light blinks on enclosure but no read or write. On BlackMagic the speed comes as 0. But after couple of minutes it starts writing at full speed. So basically it is 0 then 950 MBPS then again 0. Mostly its 0. But on my office Intel Mac it is constantly 950MBPS I donâ€™t know what settings to change. Note the ssd is formatted to AFPS. Thanks Upvote1Downvote0comments0 awardsShare"},
{"Title": "Building a fileserver", "Author": "u/MrMaxxExcaliber", "Content": "I just bought a Poweredge T320 and plan to add 6 disks for data storage (photos) probably in RAID5. Specs: Dell PowerEdge T320 Xeon E5-1410 2.80GHz 16GB RAM Tower Server No HDD My thought was to use two x 3Tb disks for the OS, in RAID1. The server (allegedly) has the RAID controller included.. Is this doable? Can I have two separate RAID filesystems?"},
{"Title": "Stop LSI sleeping Win11", "Author": "u/cowrevengeJP", "Content": "Is there a way to stop the LSI card from sleeping with the rest of the OS? I don't want the drives to keep spinning, but the PC crashes after sleeping. Doesn't happen if I use the onboard raid instead only, as it just waits a few seconds to spin up again. I have 8 drives, so I can't use board only. The card is a Dell/HP LSI I bought on Mercari and flashed to unlock RAID."},
{"Title": "backup formats to increasing recovery chances from hardware corruption?", "Author": "u/IcyCheetah3568", "Content": "What type of password protected file/folder backup format, or method, is good to increase recovery chances (not breaking the whole backup) from hardware corruption or from file transfer corruption (from one storage to another or copying/moving backups within an OS). I understand that the condition of the hardware is very important but I want to see what we can do on software level to at least have some better chances at recovery when something happens. Using RAR archives with recovery records? Does compression settings matter? Store, Fastest, Best, etc. Whole disk encryption on the backup drive? (Or a partition to also have easy access to other files) Is there any reason to use a backup program for this purpose? They also put all files into one single file, with or without compression, is it any different than a rar/7zip/zip file? Maybe something about processing metadata and chances of recovery?"},
{"Title": "New HDDs running +8Â° acceptable?", "Author": "u/BigFatRusski", "Content": "So I swapped my 2 bay synology drives from WD red to WD101EDBZ, an upgrade of 2tb on raid1. My WD red was running at cool 37Â°, new drive is at 45Â°. This is the newer model EDBZ, and not helium filled. Also, my nas warned me that this model drive is not on compatibility listâ€¦ message received and ignored. Iâ€™ve had very good experience with Red drives, is +8Â° still considered reliable long run? I usually swap drives every 2-3 years. Edit: second EDBZ drive with same firmware is running at 41Â°ðŸ¤·â€â™‚ï¸"},
{"Title": "Toshiba PC P300 usage failure", "Author": "u/Ghosteen_18", "Content": "No content"},
{"Title": "Checksum verification - Blake2B / 3 or SHA512?", "Author": "u/manzurfahim", "Content": "Hello everyone, Hope you are well. I am thinking of doing a checksum verification between the source files and the files on backup drives.  Quickhash seems like a nice little utility to do that. It has multiple algorithms: MD5, SHA-1, SHA-3, SHA256, SHA512, xxHash64, Blake2B, Blake3 and CRC32. Did a little googling and it seems like SHA512 and Blake2B / 3 are the best ones, but I'm only going to choose one and wondering if you use any of them or checksum and which should I use. I could use some advise. Many thanks in advance. https://preview.redd.it/checksum-verification-blake2b-3-or-sha512-v0-495hfvgnj34d1.jpg"},
{"Title": "Most efficient way of converting terabytes of h.264 to h.265?", "Author": "u/X2ytUniverse", "Content": "Over the last few years I've done quite a bit of wedding photography and videography, and have quite a lot of footage. As a rule of thumb, I keep footage for 5 years, in case people need some additonal stuff, photos or videos later (happened only like 3 times ever, but still). For quite some time i've been using OM-D E-M5 Mark III, which as far as I know can only record with h.264. (at least thats what we've always recorded in), and only switched to h.265/hevc camera quite recently. Problem is, I've got terabytes of old h.264 files left over, and space is becoming an issue., there's only so many drives I can store safely and/or connect to computer. What I'd like is to convert h.264 files to h.265, which would save me terabytes of space, but all the solutions I've found by researching so far include very small amount of files being converted, and even then it takes quite some time. What I've got is ~3520 video files in h.264, around 9 terabytes total space. What would be the best way to convert all of that into h.265?"},
{"Title": "Next Step from WD Black 4TB?", "Author": "u/Hawthm_the_Coward", "Content": "Hello! I've been using a home setup with a 1 TB SSD and an older model WD Black 4 TB HDD for some time now. It's done a good job, but lately, I've been annoyed with the HDD's capabilities... It's fairly noisy despite being in a quieting enclosure, and the 4 TB capacity is pretty close to being filled up. So, here are some considerations I have. The replacement absolutely must be 7200 RPM, and a similar performance. Applications regularly load from this drive so a 5400 RPM drive is not an option. No matter what, this has to hold true. A quieter drive without losing capacity would be very nice. I'd be alright swapping for something like a 4 TB Blue if it was comparable. Alternatively, a size upgrade would also be great. I've heard the Exos X18 16 TB is good and also pretty fast. How much worse would the noise be compared to the Black? If possible, a slight upgrade to both would be wonderful. Would an HGST He8 6TB be quieter, too? Just trying to weigh my options, and any and all insights would be much appreciated!"},
{"Title": "Mirroring and also Frequency of Cold Storage Backups", "Author": "u/ngs428", "Content": "In my current situation I have about 4.5TB of media I am backing up on my home PC.  The contents of the existing media changes daily as I add, change and delete content. I have been doing a cold storage backup, but that only seems to happen every 3 months or so at best.  Over those 3 months there was a lot of time spent on revising the original media, I would hate to lose that. I understand that Mirroring is not considered a true backup, but it certainly is a backup in the case of a drive failure, which is what I am most concerned about. Thoughts or comments on mirroring and frequency of cold storage backups?  I feel like employing both is a good strategy."},
{"Title": "Archiving Jobs on a Mac. Any alternatives to Retrospect?", "Author": "u/blunderbot", "Content": "Just wondering if there's another way to archive my largish projects on a Mac. I'm about to upgrade the computer that handles this work (probably to a Mac Mini M2) so now is a good time to weigh my options. Jobs range from 5-20TB, each job is in its own folder that is then archived to a collective Media Set. At the moment, the archives are kept on hard drives that I keep offline. The best feature for me is that I can search the Media Set for the offline files I want to restore and easily grab just those files. I need to do this for less than 5% of my jobs so it's great when it works, but not worth spending $ on solutions with ongoing costs. But retrospect occasionally needs to rebuild the catalog which significantly slows down the process. I also have a mild concern that the archive format is not plainly readable."},
{"Title": "RAID 0 on 980 NVME 4x4 Drive?", "Author": "u/Administrative-Air73", "Content": "Simple problem - I got a 2TB SAMSUNG 980 PRO Gen 4 NVME SDD for my C-Drive filled with programs and Unreal Environments that I need to expand. My options are to buy a 4TB Drive for $320 or buy another 2TB for $169; considering my bank is already breaking along side my PC I was considering using RAID 0 as a solution, temporarily backing my OS to another drive in the meantime. Is this practical?"},
{"Title": "Issues Finding Duplicates with Czkawka on Unraid", "Author": "u/klnadler", "Content": "Iâ€™m in the process of cleaning up my massive photo library and have been using exiftool to sort everything. I have a small folder of photos that a lot are found within a much larger folder of photos Iâ€™m trying to delete out. I used Czkawka to find the duplicates by having both folders as a source and it found a lot of the matches but itâ€™s still missing many and none of the other options (different hashes, file size, name) donâ€™t produce the correct results. Any help on what to do?"},
{"Title": "any way to fix my HDD?", "Author": "u/greetings__mortal", "Content": "I have this old Toshiba 1TB hdd that I had salvaged from a laptop a few years back. It travels with me a lot and I often share it with others, which means it has been through quite some unintentional abuse. But recently it stated acting up. it would keep freezing on windows. I managed to recover the important data using Linux where it froze once in a while but worked significantly better and after the recovery i formatted the HDD in hopes of fixing it. But It was of no use since now it just freezes after a few seconds of me trying to transfer a large file to it. I ran HDDscan and it is filled with bad blocks. Is there any way to fix it? or is it dead?"},
{"Title": "How do i get back files i changed it a folder because they had the same name?", "Author": "u/NokiaTheReuploader", "Content": "i needed to send something some archives to my friend and when i downloaded more of them i forgot to change the old files name and when i clicked ''change it the folder'' the old files dissapeared, how can i retrieve them?"},
{"Title": "What DIY DAS Case", "Author": "u/BurgerQuester", "Content": "Iâ€™m looking to build a DAS to add storage to my unraid server. Iâ€™ve got an optiplex 5050 as the â€˜serverâ€™ for now with an m2 drive, and 2 sata connection ports. The plan is to get a pcie expansion card to add more sata ports to the optiplex and add drives as and when I need them. What case would you use for this? Or is my idea a bad one?"},
{"Title": "H.264 vs MPEG2", "Author": "u/mro2352", "Content": "I have seen that my Plex server has been transcoding my mkv files which have mpeg2 encoding to h.264. What are the pros and cons of h.264 over mpeg2? I want to preserve the subtitles as a general rule and want to keep my collection in a single data format."},
{"Title": "MEGA sending data while downloading", "Author": "u/CorgiFun1874", "Content": "Hi everyone ! I'm making a local backup of files I store on MEGA servers, using their MEGAsync software. But turns out after downloading 7 GB of data, it uploaded 350 MB, and it keeps growing. I couldn't find any information about it. What does MEGA send when downloading ? Signatures and tracking files are no way that heavy !"},
{"Title": "I *feel* like I've been adding less and less to my server as the 'old media I want to collect runs out, and it becomes only a trickle of new media', but my spreadsheet tells me that it's all in my head...", "Author": "u/AshleyUncia", "Content": "No content"},
{"Title": "Questions on magnetic tape for storage (Now that prices have changed, apparently)", "Author": "u/Alemismun", "Content": "Yesterday I saw some article about how magnetic tape was much cheaper per TB than hard disks (and you could drop them like SSDs, which is always nice), and this has made me consider if I should perhaps switch... But its really hard to get any reliable info on the matter, which is why Im making this post. Is magnetic tape actually cheaper? Some older reddit posts say that this is not the case (and indeed, the latest LTO tape is much more expensive per TB than a hard drive), but at the same time, random websites on the internet seem to pitch some wild prices, such as this one (or the same one from another site ) (or the same one from amazon , wait wtf why are all the cheap ones the same model), which comes at about 6 bucks per terabyte of data. This is way better than even a used hard drive. Are there any hidden costs or technical issues that one could expect? Or does it just behave like a hard drive but with much lower speeds? Do they wear out or are they expected to last more than hard drives? Is there any issue with buying used? Is it even possible to encrypt this data (in particular with Veracrypt)? This is kind of a dealbreaker for me otherwise, as I greatly value data security. I generally store lots of random data across many folders, in the form of a digital library. Generally books/films by category. Plus random family pictures and work documents here and there. Speed is not much of a concern, nor is having the storage cold. My hard drives usually take all night to sync, then they stay disconnected until needed again."},
{"Title": "Are there any legit m-disks currently for sale?", "Author": "u/Gwyn777", "Content": "Of course I see the ones on amazon, but they look sketchy. After hearing of disks being labeled as m-disc but not truly them, I am skeptical."},
{"Title": "S3 Glacier Deep Archive VS Backblaze B2", "Author": "u/Ryhaph99", "Content": "I chose B2 because, although I might be able to save some money with glacier deep archive, AWS pricing is so confusing that I feel like I'm gonna fuck it up and owe them a ton of money. B2 was much easier to understand and say let's go. Is AWS intentionally making it confusing or is that just how they are able to make it cheap in specific use cases? Am I just dense? B2 has been great so far but ultimately I'd rather be paying $0.6/TB/month than $6/TB/month for my offsite backups. I'm also considering the \"at a friend's house\" solution. Maybe there are even co-lo facilities for this kinda thing that could be competitive in my area. I'm more curious about cloud solutions than guerilla solutions though, so please let me know if you have any insight. TL;DR: most cost effective cloud solution for data hoarding is desired, do you have it?"},
{"Title": "Scanning trifolded statements", "Author": "u/briko3", "Content": "I have many sets of client statements that run about 20 pages front and back and were trifolded into an envelope. I have an Epson 410 scanner and no matter how much I try to flatten these statements, it can never reliably feed one page at a time. It always ends up clumping several together throughout the scan, which has been frustrating. So my question is, does anyone know of a scanner that does well in these kinds of circumstances? Thanks"},
{"Title": "Soundcloud Database", "Author": "u/yonah_xy", "Content": "hello, new here. Iâ€™m a HUGE fan of this soundcloud artist 9TAILS. Iâ€™m looking into archiving him just in case and iâ€™m aiming at OG files, so the wavs. I was hoping someome here could help me out or someone who has access to it. Itâ€™s a lot but iâ€™m just a fan!"},
{"Title": "Recommended fans for Norco 4220 (4U, 20 HDD bays) case? (re: 80mm fans)", "Author": "u/sofakng", "Content": "I've been using a Norco 4220 case for quite a while but I think it's time to replace the fans. Behind the 20-bay HDDs are 4x Delta AFB0812H fans.  They are loud, which is fine for now but it would be nice to get something quieter if I need to move the server. There are also two empty rear fans (80mm) that I would like to populate. I was thinking about buying 4x Artic P8 Max (HDD bays) and 2x Artic P8 PWM (rear case).  Would this be a good choice or does anybody have any other recommendations?"},
{"Title": "M3 Pro Mac External SSD pauses/freezes", "Author": "u/mishrah10", "Content": "I recently purchased an internal ssd and combined it with an enclosure to make it external ssd. Now on my personal M3 Pro Macbook the ssd just pauses, the light blinks on enclosure but no read or write. On BlackMagic the speed comes as 0. But after couple of minutes it starts writing at full speed. So basically it is 0 then 950 MBPS then again 0. Mostly its 0. But on my office Intel Mac it is constantly 950MBPS I donâ€™t know what settings to change. Note the ssd is formatted to AFPS. Thanks Upvote1Downvote0comments0 awardsShare"},
{"Title": "Building a fileserver", "Author": "u/MrMaxxExcaliber", "Content": "I just bought a Poweredge T320 and plan to add 6 disks for data storage (photos) probably in RAID5. Specs: Dell PowerEdge T320 Xeon E5-1410 2.80GHz 16GB RAM Tower Server No HDD My thought was to use two x 3Tb disks for the OS, in RAID1. The server (allegedly) has the RAID controller included.. Is this doable? Can I have two separate RAID filesystems?"},
{"Title": "Stop LSI sleeping Win11", "Author": "u/cowrevengeJP", "Content": "Is there a way to stop the LSI card from sleeping with the rest of the OS? I don't want the drives to keep spinning, but the PC crashes after sleeping. Doesn't happen if I use the onboard raid instead only, as it just waits a few seconds to spin up again. I have 8 drives, so I can't use board only. The card is a Dell/HP LSI I bought on Mercari and flashed to unlock RAID."},
{"Title": "backup formats to increasing recovery chances from hardware corruption?", "Author": "u/IcyCheetah3568", "Content": "What type of password protected file/folder backup format, or method, is good to increase recovery chances (not breaking the whole backup) from hardware corruption or from file transfer corruption (from one storage to another or copying/moving backups within an OS). I understand that the condition of the hardware is very important but I want to see what we can do on software level to at least have some better chances at recovery when something happens. Using RAR archives with recovery records? Does compression settings matter? Store, Fastest, Best, etc. Whole disk encryption on the backup drive? (Or a partition to also have easy access to other files) Is there any reason to use a backup program for this purpose? They also put all files into one single file, with or without compression, is it any different than a rar/7zip/zip file? Maybe something about processing metadata and chances of recovery?"},
{"Title": "New HDDs running +8Â° acceptable?", "Author": "u/BigFatRusski", "Content": "So I swapped my 2 bay synology drives from WD red to WD101EDBZ, an upgrade of 2tb on raid1. My WD red was running at cool 37Â°, new drive is at 45Â°. This is the newer model EDBZ, and not helium filled. Also, my nas warned me that this model drive is not on compatibility listâ€¦ message received and ignored. Iâ€™ve had very good experience with Red drives, is +8Â° still considered reliable long run? I usually swap drives every 2-3 years. Edit: second EDBZ drive with same firmware is running at 41Â°ðŸ¤·â€â™‚ï¸"},
{"Title": "Toshiba PC P300 usage failure", "Author": "u/Ghosteen_18", "Content": "No content"},
{"Title": "Checksum verification - Blake2B / 3 or SHA512?", "Author": "u/manzurfahim", "Content": "Hello everyone, Hope you are well. I am thinking of doing a checksum verification between the source files and the files on backup drives.  Quickhash seems like a nice little utility to do that. It has multiple algorithms: MD5, SHA-1, SHA-3, SHA256, SHA512, xxHash64, Blake2B, Blake3 and CRC32. Did a little googling and it seems like SHA512 and Blake2B / 3 are the best ones, but I'm only going to choose one and wondering if you use any of them or checksum and which should I use. I could use some advise. Many thanks in advance. https://preview.redd.it/checksum-verification-blake2b-3-or-sha512-v0-495hfvgnj34d1.jpg"},
{"Title": "Most efficient way of converting terabytes of h.264 to h.265?", "Author": "u/X2ytUniverse", "Content": "Over the last few years I've done quite a bit of wedding photography and videography, and have quite a lot of footage. As a rule of thumb, I keep footage for 5 years, in case people need some additonal stuff, photos or videos later (happened only like 3 times ever, but still). For quite some time i've been using OM-D E-M5 Mark III, which as far as I know can only record with h.264. (at least thats what we've always recorded in), and only switched to h.265/hevc camera quite recently. Problem is, I've got terabytes of old h.264 files left over, and space is becoming an issue., there's only so many drives I can store safely and/or connect to computer. What I'd like is to convert h.264 files to h.265, which would save me terabytes of space, but all the solutions I've found by researching so far include very small amount of files being converted, and even then it takes quite some time. What I've got is ~3520 video files in h.264, around 9 terabytes total space. What would be the best way to convert all of that into h.265?"},
{"Title": "Next Step from WD Black 4TB?", "Author": "u/Hawthm_the_Coward", "Content": "Hello! I've been using a home setup with a 1 TB SSD and an older model WD Black 4 TB HDD for some time now. It's done a good job, but lately, I've been annoyed with the HDD's capabilities... It's fairly noisy despite being in a quieting enclosure, and the 4 TB capacity is pretty close to being filled up. So, here are some considerations I have. The replacement absolutely must be 7200 RPM, and a similar performance. Applications regularly load from this drive so a 5400 RPM drive is not an option. No matter what, this has to hold true. A quieter drive without losing capacity would be very nice. I'd be alright swapping for something like a 4 TB Blue if it was comparable. Alternatively, a size upgrade would also be great. I've heard the Exos X18 16 TB is good and also pretty fast. How much worse would the noise be compared to the Black? If possible, a slight upgrade to both would be wonderful. Would an HGST He8 6TB be quieter, too? Just trying to weigh my options, and any and all insights would be much appreciated!"},
{"Title": "Mirroring and also Frequency of Cold Storage Backups", "Author": "u/ngs428", "Content": "In my current situation I have about 4.5TB of media I am backing up on my home PC.  The contents of the existing media changes daily as I add, change and delete content. I have been doing a cold storage backup, but that only seems to happen every 3 months or so at best.  Over those 3 months there was a lot of time spent on revising the original media, I would hate to lose that. I understand that Mirroring is not considered a true backup, but it certainly is a backup in the case of a drive failure, which is what I am most concerned about. Thoughts or comments on mirroring and frequency of cold storage backups?  I feel like employing both is a good strategy."},
{"Title": "Archiving Jobs on a Mac. Any alternatives to Retrospect?", "Author": "u/blunderbot", "Content": "Just wondering if there's another way to archive my largish projects on a Mac. I'm about to upgrade the computer that handles this work (probably to a Mac Mini M2) so now is a good time to weigh my options. Jobs range from 5-20TB, each job is in its own folder that is then archived to a collective Media Set. At the moment, the archives are kept on hard drives that I keep offline. The best feature for me is that I can search the Media Set for the offline files I want to restore and easily grab just those files. I need to do this for less than 5% of my jobs so it's great when it works, but not worth spending $ on solutions with ongoing costs. But retrospect occasionally needs to rebuild the catalog which significantly slows down the process. I also have a mild concern that the archive format is not plainly readable."},
{"Title": "RAID 0 on 980 NVME 4x4 Drive?", "Author": "u/Administrative-Air73", "Content": "Simple problem - I got a 2TB SAMSUNG 980 PRO Gen 4 NVME SDD for my C-Drive filled with programs and Unreal Environments that I need to expand. My options are to buy a 4TB Drive for $320 or buy another 2TB for $169; considering my bank is already breaking along side my PC I was considering using RAID 0 as a solution, temporarily backing my OS to another drive in the meantime. Is this practical?"},
{"Title": "Issues Finding Duplicates with Czkawka on Unraid", "Author": "u/klnadler", "Content": "Iâ€™m in the process of cleaning up my massive photo library and have been using exiftool to sort everything. I have a small folder of photos that a lot are found within a much larger folder of photos Iâ€™m trying to delete out. I used Czkawka to find the duplicates by having both folders as a source and it found a lot of the matches but itâ€™s still missing many and none of the other options (different hashes, file size, name) donâ€™t produce the correct results. Any help on what to do?"},
{"Title": "any way to fix my HDD?", "Author": "u/greetings__mortal", "Content": "I have this old Toshiba 1TB hdd that I had salvaged from a laptop a few years back. It travels with me a lot and I often share it with others, which means it has been through quite some unintentional abuse. But recently it stated acting up. it would keep freezing on windows. I managed to recover the important data using Linux where it froze once in a while but worked significantly better and after the recovery i formatted the HDD in hopes of fixing it. But It was of no use since now it just freezes after a few seconds of me trying to transfer a large file to it. I ran HDDscan and it is filled with bad blocks. Is there any way to fix it? or is it dead?"},
{"Title": "How do i get back files i changed it a folder because they had the same name?", "Author": "u/NokiaTheReuploader", "Content": "i needed to send something some archives to my friend and when i downloaded more of them i forgot to change the old files name and when i clicked ''change it the folder'' the old files dissapeared, how can i retrieve them?"},
{"Title": "What DIY DAS Case", "Author": "u/BurgerQuester", "Content": "Iâ€™m looking to build a DAS to add storage to my unraid server. Iâ€™ve got an optiplex 5050 as the â€˜serverâ€™ for now with an m2 drive, and 2 sata connection ports. The plan is to get a pcie expansion card to add more sata ports to the optiplex and add drives as and when I need them. What case would you use for this? Or is my idea a bad one?"},
{"Title": "H.264 vs MPEG2", "Author": "u/mro2352", "Content": "I have seen that my Plex server has been transcoding my mkv files which have mpeg2 encoding to h.264. What are the pros and cons of h.264 over mpeg2? I want to preserve the subtitles as a general rule and want to keep my collection in a single data format."},
{"Title": "Does 9400-16i HBA require PCIe 3.1?", "Author": "u/sofakng", "Content": "I'm looking to upgrade my possibly defective 9211-8i (x2) to a 9400-16i but the 9400-16i lists PCIe 3.1. My motherboard is a SuperMicro X9DR3-F which only supports PCIe 3.0. Will this be compatible? Â For some reason the 9305-16i cards are more expensive (but they only require PCIe 3.0) ..."},
{"Title": "Best way to grab FB videos in highest quality in June 2024?", "Author": "u/rubbishfairy", "Content": "Hey guys I know there is a lot of info about this out there already but it seems to go out of date quickly and a lot of it just seems to be wrong. So I'm hoping there's some proper experts in this sub. Firstly I know there are various browser plugins. I got one called \"video download helper\" especially for the task but it just doesn't work with FB. I'm also a fan of Jdownloader but again that doesn't work with FB. Secondly I know you can switch \"www\" for \"mbasic\" and find the actual URLs of videos. You can then download them but ONLY it seems in the lowest possible quality. I suspect the URL just needs to be changed slightly to give me the high quality but I don't know how. What do you guys suggest?"},
{"Title": "\"Autocare Depot\"?", "Author": "u/joetaxpayer", "Content": "I've remarked that drives seem to have crept back up in price. The Toshiba Enterprise 16TB I bought for $220 2 years ago is far higher today. I just got a camel alert for this drive at $250, which is far better than the $290 I've seen. But. the vendor is \"Autocare Depot\". I look them up and it says \"Autocaredepot, a subsidiary of e-commerce giant Newegg.com , is a fast growing online retailer.\" Looking for feedback on this company before i make the purchase. Newegg, but they seem to have an odd mix of stuff. Not sure what i'm dealing with here."},
{"Title": "Internet Archive", "Author": "u/Specialist-Pause9394", "Content": "There needs to be a indexed search engine at the Internet Archive. It's time consuming to put in the URLs of a webpage. I wish there was a way for the Internet Archive to search every webpage in the archives and indexed it like Google does for the live web."},
{"Title": "Do I need to worry about the data safety?", "Author": "u/johnqhu", "Content": "Hi Guys, A newbie question here. I have a 1618+ with raid6 array and 6 disks. One of them was broken today. In fact it's quite weird. The broken one is less than 1 month old. It's a WD red pro 18TB. I bought it from Amazon. Amazon agreed to send me a brand new replacement. But I need to return the broken one. So my question is: do I need to worry about the data safety? I know raid6 can only tolerate 2 disks failure. But some software claim be able to recover some information when even 3 disk failed. So I just wondering whether anyone can get some useful data from just 1 disk? Whether is it 0 risk or still some risk to send a disk of raid6 to others without erasing data? On my side, I cannot erase it because it even cannot be found when i put it into my PC. Regards,"},
{"Title": "Downloading Instagram Likes?", "Author": "u/Monitichello", "Content": "Anyone know how I can download my likes from instagram? Looking to store full videos and images rather than just links to posts."},
{"Title": "My ears are bleeding", "Author": "u/ElonTastical", "Content": "No content"},
{"Title": "Incremental M-DISK backups and drive choice", "Author": "u/ReactCereals", "Content": "Hello, so I have decided to revise my current backup strategy and opt for using off site M-DISK backups. However, there are still two things I can't figure out (or find recent opinions about at least). My first issue is choosing an internal burner to buy. Currently I only have an array of old DVD drives to batch rip old DVDs and no experience regarding bluray/M-DISK. I feel like I can only find product recommendations that are quite a few years old and it feels like one year people tend to recommend pioneer over everything else and another year its only LG and the year after pioneer again and so on... On top I can't figure out what to even watch out for. For example the currently available Pioneer drives BDR-S12XLT, BDR-213EBK, BDR-S13EBK, etc. all offer the same compatibility, same write speeds, same read speeds, and according as to how I read their product page the exact same features. Is there even something in a technical data sheet I can use to judge whether one drive might be more reliable for my use case than the other? What drives do you use/recommend in 2024 and why? My base line requirement is just burning M-DISKs (single/dual layer) and occasionally ripping a Blu-ray. Linux support would be highly appreciated but I can fall back to a windows system I have to keep running for other legacy scanning software anyways. For backing up old collections of media which won't change I guess how to do it is pretty straight forward. However, I'd also like to backup folders that like to change and grow like my document folders. These changing folders by far do not even come close to fill up the capacity of any M-DISK as of now. So I'd like to regularly keep burning info about recent changes/new files only on the same disk to not waste e.g. a 25 GB disk every time I backup a 3 GB folder. I originally intended to write a few small scripts that would keep track of changes/new files since last backup and manage versioning for writing/restoring those. However, I feel like reinventing the wheel here but can't really find the software I am looking for. Any simple and open source software recommendations there? Thanks for your input!"},
{"Title": "Linus Media Group is working on digitizing the Reboot master tapes; they already have the DCR-300/DCR-500 VTR machines to play them back, but they need documentation and/or experts to sort out hardware issues", "Author": "u/Alt230s", "Content": "No content"},
{"Title": "12 years worth of posts getting deleted...", "Author": "u/moonronic", "Content": "Hi there, I am not massively tech literate nor sure of where to go or who to ask about this, but I have this website I've been using for 12 years called Quotev, recently, they took out the social feed where you could see other people's posts, now it's hard to find your friends on there and such from years passed. It says they are deleting all messages on July 1st, and I was hoping if anybody knows an easy method to archive all my posts on the website? the activity page is the main part i'd want to save; it's https://www.quotev.com/(username)/activity"},
{"Title": "VHSC To digital backup", "Author": "u/EngineeringGlum5318", "Content": "Hello everyone, as my title suggests I have a bunch of old VHS compact that Iâ€™m looking to digitize. Just trying to figure out how to best go about digitizing with the right equipment for decent quality. Any suggestions / help is welcome!"},
{"Title": "Syncing To Sd Cards using FreeFileSync", "Author": "u/MachineThatGoesP1ng", "Content": "I'm assuming some of you guys use Freefilesync and i was wondering if anyone can help me with syncing files on automation to a specific Sd card. The problem I'm having is that when i put in the Sd card as the destination folder FFS only recognizes the D: drive and doesn't consider the Sd unique, so if i put in a different Sd card it still tried to sync into that drive. Im sure the Sd has a unique ID somewhere, is there a way for FFS to recognize and use that as a destination and how do i obtain the unique ID?"},
{"Title": "Thoughts on digitizing strangersâ€™ home videos?", "Author": "u/RelaxRelapse", "Content": "I often buy used VHS-C and Video8 tapes from Japan, both because they tend to be cheap and also because often times people in Japan used it to make personal copies of TV shows using those formats. Plus itâ€™s cheaper than shipping full sized VHS tapes. Unsurprisingly Iâ€™ll find home videos in the lots I buy. A part of me feels obligated to make digital copies since theyâ€™re someoneâ€™s memories, and also a time capsule of that period of time. Another part of me feels I shouldnâ€™t because they are someoneâ€™s personal recording that I assume they had no plans to show outside of their homes. And then again, what do I even do with the digital copies if I do make them besides hoard them? I legally couldnâ€™t share them most likely. I wouldnâ€™t mind digitizing a copy and sending it to the original owner, but since itâ€™s just a random lot online from a resale shop, thereâ€™s nothing really to go off of. Iâ€™ve found NSFW content as well, but the choice with that stuff is obvious and I just erase them. The normal, everyday stuff though Iâ€™m unsure about. I think I just want to get a second opinion before I proceed."},
{"Title": "Data Strategy Review: Is my approach good?", "Author": "u/Distinct-Yoghurt5665", "Content": "I really hope it's ok to ask this. From the rules and the wiki it does seem ok, but I'm new to this sub. So currently I have two external SSDs for my home server. I'm using SSD_A to save all my stuff to it and SSD_B as a backup. Now in the near future my SSD_A will be full. So I want to do the following: I want to buy a very large HDD use that as a backup and then use the two SSDs as my standard mounts for the home server to save stuff to. Ideas I could use mergerfs to combine both external SSDs to one mount point. I need to to this for some of my applications. I could then use rsync to create my backups from the SSD-mergerfs-mount to the HDD. Hopefully there is an rsync flag to indicate that I do not want to sync already existing data again (even if it changed). This ensures that broken files will not overwrite healthy files on the HDD. Yes, I know that this means that changes to files won't be backed up but this is ok for my use case. I could use ext4 for all drives, cause there is no reason not to and I do not know anything else. What do the experts think? Does that sound like a good approach? Could anything go wrong? I do not care about write speed to the SSDs they are fast enough I do not need striping. I also do not care if some data gets lost between the back up cycles so I do not need RAID. Is mergerfs the best there is and is it easy to set up? Should I use anything else then ext4? ext4 has always been my go to and I do not really see anything wrong with it. File hashes seem to be unnecessary cause I can just not overwrite existing files, that seems to do the trick for my use case. Very much appreciate any feedback on my plan."},
{"Title": "Do you guys think there's interest in things like this being scanned and preserved? 70s Pamphlets (gardening, cooking, random health stuff like self breast exams and mongolism)", "Author": "u/Quirky_Ad_69", "Content": "No content"},
{"Title": "Is anyone happy with iDrive?", "Author": "u/Jack15911", "Content": "This is an honest question because most people who are happy with their service don't write about it. I currently use external drives to swap between the home safe and the safety deposit box, but want to add  personal backup data into the cloud so I can cut down on external backup drives (individual drives, not NAS). Most reviews on the web rank iDrive pretty highly, while mostly I see unhappiness on Reddit. Does iDrive work mostly or does it not?"},
{"Title": "Best deal on 20tb or 22tb drives from serverpartdeals?", "Author": "u/thegameksk", "Content": "Looking to upgrade my NAS storage. They will he used mainly for Plex, transmission, calibre and comics reading. Also how do I tell when serverpartdeals has a sale? Is it advertised somewhere?"},
{"Title": "Is SyncBack 11 Free the best free option for my situation?", "Author": "u/ngs428", "Content": "I have a W11 desktop PC with about 3TB of data consisting of family videos and pictures.  I currently have it on an 8TB internal HDD and once every 3 months do a copy to my external HDD and delete the old backup. Moving forward I am looking to have the external HDD connected to the PC 24/7 and monitor for changes in the source folders on the internal HDD and mirror those changes to the external HDD.  So if I delete something on the internal, it deletes from the external.  Add to the internal, it adds to the external. After some research of looks like SyncBack 11 will do this and is free.  I see Veeam mentioned quite a bit.  What is the best option out there for free or low cost? Edit: I see freefilesync may be an option. https://freefilesync.org/download.php Thank you!"},
{"Title": "A data hoarding tool I really like for moving files around, backups, copying, etc", "Author": "u/LuckyRide", "Content": "As always, I don't work for them, but I really like the product. \"Synchronize Files and Folders\" => https://freefilesync.org It's really just a copy program - pretty typical and standard. Except it does SUCH a good job. You can set up sessions that copies when you run them - so update, mirror, things like that, to arbitrarily complex sets of disks and files. I use it extensively for backups to always know my files are backed up. It needs to be kicked off manually, but it just works so well and I like monitoring the steps. Most especially, I move files around between diff folders (think picture sorting, etc) and it will figure that out and \"move\" the files versus \"always re-copy\". So it really speeds everything up. So really what I use it for is \"make sure every file is backed up properly\" - and it does it really well - it \"never gives up until success\" no matter what happens. You can just re-run and make sure it's \"all up to date\". It supports Windows, Mac, and I think linux - so same on all platforms. It's donation-ware - and I think there is some more features if you donate - but I don't remember since I donated since the very beginning. No other tool really does it as well - no Windows copy or anything like that gives me any confidence like this does."},
{"Title": "WD Passport Ultra on Mac: HFS+ vs NTFS with WD Driver", "Author": "u/DatasTemporalLobe", "Content": "Trying to get a WD Passport Ultra to work on a Mac in order to back up an iPhone.  I see the Western Digital app has an option to download a driver (Paragon) for NTFS. Is it better to download the driver and use NTFS or reformat to HFS+? I like that I'd be able to use the hard drive on a pc if I go with NTFS with the Paragon driver but am I losing any functionality on the Mac?"},
{"Title": "Most efficient method of syncing a primary drive to a back-up drive (MacOS)", "Author": "u/pipmike", "Content": "I could use advise on which syncing software to use for a Mac and two external hard drives. Scenario: I have one external hard drive as my primary \"hoard\" drive to which I actively add large files (1+ GB), move files to different folders, change file tags, etc. I have a second external hard drive that I use as my first-level disaster recovery in case my primary drive fails. The second drive is currently one-way synced to the primary hard drive using a rysnc script i wrote. I don't need to store changes or previous file versions. Just want to mirror one drive to another. While rsync gets the job done, it doesn't handle the above efficiently because any changes are seen as new files, which means it's constantly writing/re-writing large files to the secondary hard drive. If I change the name of a directory including 100+ GB of files, rather than just similarly rename the directory of the secondary hard drive, rsync will delete all of the files, create a new directory, and re-sync all of those files. If I rename a file, rysnc will delete the file and then rewrite it. If I add a MacOS file tag to the file, rysnc will delete the file and then rewrite it. Thus, if I make any directory changes or tags, rsync can take hours (even days) to re-sync. I can imagine that's not healthy on either drive. It would be great if MacOS had a native function that said \"if I do anything to Drive A, do the same thing to Drive B\", but I haven't found it if it exists. Thus, I'm looking at other options (Chronosync, Carbon Copy Cloner, etc.) that might be a better solution for my use case. Open for all suggestions or feedback."},
{"Title": "How to avoid wasting time encrypting twice? Need to encrypt eHDD and want to encrypt cloud storage. Can I encrypt in place?", "Author": "u/Snowblind45", "Content": "Hello, Iâ€™ve new this this stuff. Iâ€™m trying to improve my data hoarding habits. Currently I have 2 external HDD identical to each other (5 TB filled). I plan on also storing a copy on Blackblaze Cloud storage. Iâ€™ve heard that rclone can encrypt the files as it uploads them to the cloud, amazing! But what about encrypting the drive itself? Do I spend time using rclone to encrypt and upload, then encrypt the drive using maybe veracrypt? Is it possible to encrypt first using veracrypt then simply rclone upload those contents to the cloud? Iâ€™m feeling a bit confused about veracrypt since a tutorial makes it seem like I need to have free space to allocate a virtual partition to encrypt but I donâ€™t have the extra spaceâ€¦is there a way to encrypt in place?"},
{"Title": "Diff Checker - to check whether backup folder contents match source folder?", "Author": "u/redexposure", "Content": "I'm wondering if anyone can recommend a type of \"Diff Checker\", for files and folders, to see whether backup drive/folder contents match the source folder (and highlight files which haven't been duplicated)? Essentially, the inverse of a Duplicate Finder. Does anyone know of any tools like this?"},
{"Title": "How to connect an LTO tape drive to a Synology NAS (DS1621+)", "Author": "u/CyclicRedundancyMach", "Content": "All, I am looking to cold-store about 25tb of video along with the normal user data.  We are full-time RVrs and the continuous movement increases the risk of a full crash, risking all of my data.  My wife would be less than pleased.  In an attempt for a happy marriage, I wanted to purchase a tape drive for simple cold backups.  I would do a full backup followed by an incremental every month or so, rotating them.  I'd also do a full data backup and mail the tape(s) to a friend for safe keeping. I am just not sure how to connect an LTO.  Suggestions? The NAS has USB, gigabit RJ45s (CAT6), and a USB, a PCIE expansion slot. All of the drives seem to have SAS connections. I honestly thought that this would be 15 minute no-brainer.   would have thought that this illustrious group of data-centric technocrats would have 15 fully documented methods.  Instead, there is nothing at all. I would appreciate your advice."},
{"Title": "RE: Tools for Bit-for-Bit / 1:1 Optical Media Preservation", "Author": "u/Archivist_Goals", "Content": "I wanted to create a separate post that further elaborates on what was mentioned in this post's comments from others. And, I will tag the original post's author u/KeptinGL6 : Just to clarify for everyone, MPF is the overall GUI. There are 3 main utilities from different people that are used in the Media Preservation Frontend program: Redumper AARU Disc Image Creator (DIC) DIC is actively being phased out. Redumper is pretty much the gold standard right now. AARU is also very well developed. DIC's original developer sort-of abandoned it and it's regarded as a legacy dumper utility and will be phased out at some point. Edit: Some clarification. The original developer behind DIC didn't abandon it. Redumper is being chosen over DIC where applicable and/or possible. These programs' inception originate to the Redump.org , Video Game Preservation Collective, Gaming Alexandria and Hit Save! organizations and communities where, through Discord, a ton of collaboration has taken hold for proactive media preservation, the world over. It's a very sizable, but niche community of dedicated people very much akin to the VHS Decode and Domesday Laser Disc duplicator projects. Links https://www.preservegames.org/ https://www.gamingalexandria.com/wp/ https://hitsave.org/ Discords https://discord.com/invite/AHTfxQV https://discord.com/invite/dQhd6d7 https://discord.com/invite/TCKT6uA The Video Game History Foundation is also an invaluable resource https://gamehistory.org/ These utilities are not solely aimed at preserving video/PC games. They can be used to preserve all forms of media. EAC, Exact Audio Copy, is a bit dated because it can not grab, for example, compact disc subcode https://en.wikipedia.org/wiki/Compact_Disc_subcode Redumper can be found here: https://github.com/superg/redumper BOS (Binary Object Scanner) is an integral part of MPF for detection of copy protection mechanisms and can be found here: https://github.com/SabreTools/BinaryObjectScanner While EAC is a decent utility, it lacks the ability to grab all possible information written to a an optical disc. Redumper/AARU have the ability to grab even more."},
{"Title": "Does 9400-16i HBA require PCIe 3.1?", "Author": "u/sofakng", "Content": "I'm looking to upgrade my possibly defective 9211-8i (x2) to a 9400-16i but the 9400-16i lists PCIe 3.1. My motherboard is a SuperMicro X9DR3-F which only supports PCIe 3.0. Will this be compatible? Â For some reason the 9305-16i cards are more expensive (but they only require PCIe 3.0) ..."},
{"Title": "Best way to grab FB videos in highest quality in June 2024?", "Author": "u/rubbishfairy", "Content": "Hey guys I know there is a lot of info about this out there already but it seems to go out of date quickly and a lot of it just seems to be wrong. So I'm hoping there's some proper experts in this sub. Firstly I know there are various browser plugins. I got one called \"video download helper\" especially for the task but it just doesn't work with FB. I'm also a fan of Jdownloader but again that doesn't work with FB. Secondly I know you can switch \"www\" for \"mbasic\" and find the actual URLs of videos. You can then download them but ONLY it seems in the lowest possible quality. I suspect the URL just needs to be changed slightly to give me the high quality but I don't know how. What do you guys suggest?"},
{"Title": "\"Autocare Depot\"?", "Author": "u/joetaxpayer", "Content": "I've remarked that drives seem to have crept back up in price. The Toshiba Enterprise 16TB I bought for $220 2 years ago is far higher today. I just got a camel alert for this drive at $250, which is far better than the $290 I've seen. But. the vendor is \"Autocare Depot\". I look them up and it says \"Autocaredepot, a subsidiary of e-commerce giant Newegg.com , is a fast growing online retailer.\" Looking for feedback on this company before i make the purchase. Newegg, but they seem to have an odd mix of stuff. Not sure what i'm dealing with here."},
{"Title": "Internet Archive", "Author": "u/Specialist-Pause9394", "Content": "There needs to be a indexed search engine at the Internet Archive. It's time consuming to put in the URLs of a webpage. I wish there was a way for the Internet Archive to search every webpage in the archives and indexed it like Google does for the live web."},
{"Title": "Do I need to worry about the data safety?", "Author": "u/johnqhu", "Content": "Hi Guys, A newbie question here. I have a 1618+ with raid6 array and 6 disks. One of them was broken today. In fact it's quite weird. The broken one is less than 1 month old. It's a WD red pro 18TB. I bought it from Amazon. Amazon agreed to send me a brand new replacement. But I need to return the broken one. So my question is: do I need to worry about the data safety? I know raid6 can only tolerate 2 disks failure. But some software claim be able to recover some information when even 3 disk failed. So I just wondering whether anyone can get some useful data from just 1 disk? Whether is it 0 risk or still some risk to send a disk of raid6 to others without erasing data? On my side, I cannot erase it because it even cannot be found when i put it into my PC. Regards,"},
{"Title": "Downloading Instagram Likes?", "Author": "u/Monitichello", "Content": "Anyone know how I can download my likes from instagram? Looking to store full videos and images rather than just links to posts."},
{"Title": "My ears are bleeding", "Author": "u/ElonTastical", "Content": "No content"},
{"Title": "Incremental M-DISK backups and drive choice", "Author": "u/ReactCereals", "Content": "Hello, so I have decided to revise my current backup strategy and opt for using off site M-DISK backups. However, there are still two things I can't figure out (or find recent opinions about at least). My first issue is choosing an internal burner to buy. Currently I only have an array of old DVD drives to batch rip old DVDs and no experience regarding bluray/M-DISK. I feel like I can only find product recommendations that are quite a few years old and it feels like one year people tend to recommend pioneer over everything else and another year its only LG and the year after pioneer again and so on... On top I can't figure out what to even watch out for. For example the currently available Pioneer drives BDR-S12XLT, BDR-213EBK, BDR-S13EBK, etc. all offer the same compatibility, same write speeds, same read speeds, and according as to how I read their product page the exact same features. Is there even something in a technical data sheet I can use to judge whether one drive might be more reliable for my use case than the other? What drives do you use/recommend in 2024 and why? My base line requirement is just burning M-DISKs (single/dual layer) and occasionally ripping a Blu-ray. Linux support would be highly appreciated but I can fall back to a windows system I have to keep running for other legacy scanning software anyways. For backing up old collections of media which won't change I guess how to do it is pretty straight forward. However, I'd also like to backup folders that like to change and grow like my document folders. These changing folders by far do not even come close to fill up the capacity of any M-DISK as of now. So I'd like to regularly keep burning info about recent changes/new files only on the same disk to not waste e.g. a 25 GB disk every time I backup a 3 GB folder. I originally intended to write a few small scripts that would keep track of changes/new files since last backup and manage versioning for writing/restoring those. However, I feel like reinventing the wheel here but can't really find the software I am looking for. Any simple and open source software recommendations there? Thanks for your input!"},
{"Title": "Linus Media Group is working on digitizing the Reboot master tapes; they already have the DCR-300/DCR-500 VTR machines to play them back, but they need documentation and/or experts to sort out hardware issues", "Author": "u/Alt230s", "Content": "No content"},
{"Title": "12 years worth of posts getting deleted...", "Author": "u/moonronic", "Content": "Hi there, I am not massively tech literate nor sure of where to go or who to ask about this, but I have this website I've been using for 12 years called Quotev, recently, they took out the social feed where you could see other people's posts, now it's hard to find your friends on there and such from years passed. It says they are deleting all messages on July 1st, and I was hoping if anybody knows an easy method to archive all my posts on the website? the activity page is the main part i'd want to save; it's https://www.quotev.com/(username)/activity"},
{"Title": "VHSC To digital backup", "Author": "u/EngineeringGlum5318", "Content": "Hello everyone, as my title suggests I have a bunch of old VHS compact that Iâ€™m looking to digitize. Just trying to figure out how to best go about digitizing with the right equipment for decent quality. Any suggestions / help is welcome!"},
{"Title": "Syncing To Sd Cards using FreeFileSync", "Author": "u/MachineThatGoesP1ng", "Content": "I'm assuming some of you guys use Freefilesync and i was wondering if anyone can help me with syncing files on automation to a specific Sd card. The problem I'm having is that when i put in the Sd card as the destination folder FFS only recognizes the D: drive and doesn't consider the Sd unique, so if i put in a different Sd card it still tried to sync into that drive. Im sure the Sd has a unique ID somewhere, is there a way for FFS to recognize and use that as a destination and how do i obtain the unique ID?"},
{"Title": "Thoughts on digitizing strangersâ€™ home videos?", "Author": "u/RelaxRelapse", "Content": "I often buy used VHS-C and Video8 tapes from Japan, both because they tend to be cheap and also because often times people in Japan used it to make personal copies of TV shows using those formats. Plus itâ€™s cheaper than shipping full sized VHS tapes. Unsurprisingly Iâ€™ll find home videos in the lots I buy. A part of me feels obligated to make digital copies since theyâ€™re someoneâ€™s memories, and also a time capsule of that period of time. Another part of me feels I shouldnâ€™t because they are someoneâ€™s personal recording that I assume they had no plans to show outside of their homes. And then again, what do I even do with the digital copies if I do make them besides hoard them? I legally couldnâ€™t share them most likely. I wouldnâ€™t mind digitizing a copy and sending it to the original owner, but since itâ€™s just a random lot online from a resale shop, thereâ€™s nothing really to go off of. Iâ€™ve found NSFW content as well, but the choice with that stuff is obvious and I just erase them. The normal, everyday stuff though Iâ€™m unsure about. I think I just want to get a second opinion before I proceed."},
{"Title": "Data Strategy Review: Is my approach good?", "Author": "u/Distinct-Yoghurt5665", "Content": "I really hope it's ok to ask this. From the rules and the wiki it does seem ok, but I'm new to this sub. So currently I have two external SSDs for my home server. I'm using SSD_A to save all my stuff to it and SSD_B as a backup. Now in the near future my SSD_A will be full. So I want to do the following: I want to buy a very large HDD use that as a backup and then use the two SSDs as my standard mounts for the home server to save stuff to. Ideas I could use mergerfs to combine both external SSDs to one mount point. I need to to this for some of my applications. I could then use rsync to create my backups from the SSD-mergerfs-mount to the HDD. Hopefully there is an rsync flag to indicate that I do not want to sync already existing data again (even if it changed). This ensures that broken files will not overwrite healthy files on the HDD. Yes, I know that this means that changes to files won't be backed up but this is ok for my use case. I could use ext4 for all drives, cause there is no reason not to and I do not know anything else. What do the experts think? Does that sound like a good approach? Could anything go wrong? I do not care about write speed to the SSDs they are fast enough I do not need striping. I also do not care if some data gets lost between the back up cycles so I do not need RAID. Is mergerfs the best there is and is it easy to set up? Should I use anything else then ext4? ext4 has always been my go to and I do not really see anything wrong with it. File hashes seem to be unnecessary cause I can just not overwrite existing files, that seems to do the trick for my use case. Very much appreciate any feedback on my plan."},
{"Title": "Do you guys think there's interest in things like this being scanned and preserved? 70s Pamphlets (gardening, cooking, random health stuff like self breast exams and mongolism)", "Author": "u/Quirky_Ad_69", "Content": "No content"},
{"Title": "Is anyone happy with iDrive?", "Author": "u/Jack15911", "Content": "This is an honest question because most people who are happy with their service don't write about it. I currently use external drives to swap between the home safe and the safety deposit box, but want to add  personal backup data into the cloud so I can cut down on external backup drives (individual drives, not NAS). Most reviews on the web rank iDrive pretty highly, while mostly I see unhappiness on Reddit. Does iDrive work mostly or does it not?"},
{"Title": "Best deal on 20tb or 22tb drives from serverpartdeals?", "Author": "u/thegameksk", "Content": "Looking to upgrade my NAS storage. They will he used mainly for Plex, transmission, calibre and comics reading. Also how do I tell when serverpartdeals has a sale? Is it advertised somewhere?"},
{"Title": "Is SyncBack 11 Free the best free option for my situation?", "Author": "u/ngs428", "Content": "I have a W11 desktop PC with about 3TB of data consisting of family videos and pictures.  I currently have it on an 8TB internal HDD and once every 3 months do a copy to my external HDD and delete the old backup. Moving forward I am looking to have the external HDD connected to the PC 24/7 and monitor for changes in the source folders on the internal HDD and mirror those changes to the external HDD.  So if I delete something on the internal, it deletes from the external.  Add to the internal, it adds to the external. After some research of looks like SyncBack 11 will do this and is free.  I see Veeam mentioned quite a bit.  What is the best option out there for free or low cost? Edit: I see freefilesync may be an option. https://freefilesync.org/download.php Thank you!"},
{"Title": "A data hoarding tool I really like for moving files around, backups, copying, etc", "Author": "u/LuckyRide", "Content": "As always, I don't work for them, but I really like the product. \"Synchronize Files and Folders\" => https://freefilesync.org It's really just a copy program - pretty typical and standard. Except it does SUCH a good job. You can set up sessions that copies when you run them - so update, mirror, things like that, to arbitrarily complex sets of disks and files. I use it extensively for backups to always know my files are backed up. It needs to be kicked off manually, but it just works so well and I like monitoring the steps. Most especially, I move files around between diff folders (think picture sorting, etc) and it will figure that out and \"move\" the files versus \"always re-copy\". So it really speeds everything up. So really what I use it for is \"make sure every file is backed up properly\" - and it does it really well - it \"never gives up until success\" no matter what happens. You can just re-run and make sure it's \"all up to date\". It supports Windows, Mac, and I think linux - so same on all platforms. It's donation-ware - and I think there is some more features if you donate - but I don't remember since I donated since the very beginning. No other tool really does it as well - no Windows copy or anything like that gives me any confidence like this does."},
{"Title": "WD Passport Ultra on Mac: HFS+ vs NTFS with WD Driver", "Author": "u/DatasTemporalLobe", "Content": "Trying to get a WD Passport Ultra to work on a Mac in order to back up an iPhone.  I see the Western Digital app has an option to download a driver (Paragon) for NTFS. Is it better to download the driver and use NTFS or reformat to HFS+? I like that I'd be able to use the hard drive on a pc if I go with NTFS with the Paragon driver but am I losing any functionality on the Mac?"},
{"Title": "Most efficient method of syncing a primary drive to a back-up drive (MacOS)", "Author": "u/pipmike", "Content": "I could use advise on which syncing software to use for a Mac and two external hard drives. Scenario: I have one external hard drive as my primary \"hoard\" drive to which I actively add large files (1+ GB), move files to different folders, change file tags, etc. I have a second external hard drive that I use as my first-level disaster recovery in case my primary drive fails. The second drive is currently one-way synced to the primary hard drive using a rysnc script i wrote. I don't need to store changes or previous file versions. Just want to mirror one drive to another. While rsync gets the job done, it doesn't handle the above efficiently because any changes are seen as new files, which means it's constantly writing/re-writing large files to the secondary hard drive. If I change the name of a directory including 100+ GB of files, rather than just similarly rename the directory of the secondary hard drive, rsync will delete all of the files, create a new directory, and re-sync all of those files. If I rename a file, rysnc will delete the file and then rewrite it. If I add a MacOS file tag to the file, rysnc will delete the file and then rewrite it. Thus, if I make any directory changes or tags, rsync can take hours (even days) to re-sync. I can imagine that's not healthy on either drive. It would be great if MacOS had a native function that said \"if I do anything to Drive A, do the same thing to Drive B\", but I haven't found it if it exists. Thus, I'm looking at other options (Chronosync, Carbon Copy Cloner, etc.) that might be a better solution for my use case. Open for all suggestions or feedback."},
{"Title": "How to avoid wasting time encrypting twice? Need to encrypt eHDD and want to encrypt cloud storage. Can I encrypt in place?", "Author": "u/Snowblind45", "Content": "Hello, Iâ€™ve new this this stuff. Iâ€™m trying to improve my data hoarding habits. Currently I have 2 external HDD identical to each other (5 TB filled). I plan on also storing a copy on Blackblaze Cloud storage. Iâ€™ve heard that rclone can encrypt the files as it uploads them to the cloud, amazing! But what about encrypting the drive itself? Do I spend time using rclone to encrypt and upload, then encrypt the drive using maybe veracrypt? Is it possible to encrypt first using veracrypt then simply rclone upload those contents to the cloud? Iâ€™m feeling a bit confused about veracrypt since a tutorial makes it seem like I need to have free space to allocate a virtual partition to encrypt but I donâ€™t have the extra spaceâ€¦is there a way to encrypt in place?"},
{"Title": "Diff Checker - to check whether backup folder contents match source folder?", "Author": "u/redexposure", "Content": "I'm wondering if anyone can recommend a type of \"Diff Checker\", for files and folders, to see whether backup drive/folder contents match the source folder (and highlight files which haven't been duplicated)? Essentially, the inverse of a Duplicate Finder. Does anyone know of any tools like this?"},
{"Title": "How to connect an LTO tape drive to a Synology NAS (DS1621+)", "Author": "u/CyclicRedundancyMach", "Content": "All, I am looking to cold-store about 25tb of video along with the normal user data.  We are full-time RVrs and the continuous movement increases the risk of a full crash, risking all of my data.  My wife would be less than pleased.  In an attempt for a happy marriage, I wanted to purchase a tape drive for simple cold backups.  I would do a full backup followed by an incremental every month or so, rotating them.  I'd also do a full data backup and mail the tape(s) to a friend for safe keeping. I am just not sure how to connect an LTO.  Suggestions? The NAS has USB, gigabit RJ45s (CAT6), and a USB, a PCIE expansion slot. All of the drives seem to have SAS connections. I honestly thought that this would be 15 minute no-brainer.   would have thought that this illustrious group of data-centric technocrats would have 15 fully documented methods.  Instead, there is nothing at all. I would appreciate your advice."},
{"Title": "RE: Tools for Bit-for-Bit / 1:1 Optical Media Preservation", "Author": "u/Archivist_Goals", "Content": "I wanted to create a separate post that further elaborates on what was mentioned in this post's comments from others. And, I will tag the original post's author u/KeptinGL6 : Just to clarify for everyone, MPF is the overall GUI. There are 3 main utilities from different people that are used in the Media Preservation Frontend program: Redumper AARU Disc Image Creator (DIC) DIC is actively being phased out. Redumper is pretty much the gold standard right now. AARU is also very well developed. DIC's original developer sort-of abandoned it and it's regarded as a legacy dumper utility and will be phased out at some point. Edit: Some clarification. The original developer behind DIC didn't abandon it. Redumper is being chosen over DIC where applicable and/or possible. These programs' inception originate to the Redump.org , Video Game Preservation Collective, Gaming Alexandria and Hit Save! organizations and communities where, through Discord, a ton of collaboration has taken hold for proactive media preservation, the world over. It's a very sizable, but niche community of dedicated people very much akin to the VHS Decode and Domesday Laser Disc duplicator projects. Links https://www.preservegames.org/ https://www.gamingalexandria.com/wp/ https://hitsave.org/ Discords https://discord.com/invite/AHTfxQV https://discord.com/invite/dQhd6d7 https://discord.com/invite/TCKT6uA The Video Game History Foundation is also an invaluable resource https://gamehistory.org/ These utilities are not solely aimed at preserving video/PC games. They can be used to preserve all forms of media. EAC, Exact Audio Copy, is a bit dated because it can not grab, for example, compact disc subcode https://en.wikipedia.org/wiki/Compact_Disc_subcode Redumper can be found here: https://github.com/superg/redumper BOS (Binary Object Scanner) is an integral part of MPF for detection of copy protection mechanisms and can be found here: https://github.com/SabreTools/BinaryObjectScanner While EAC is a decent utility, it lacks the ability to grab all possible information written to a an optical disc. Redumper/AARU have the ability to grab even more."},
{"Title": "LSi 9300 able to connect 7x HDDs and 1x SDD?", "Author": "u/fenderbender8", "Content": "I am new to the diy NAS scene, and have decided on creating a N100 mini PC NAS as it meets my needs for low power consumption and enough performance for my use case. However, seeing as the mini PC I have only has 1 NVMe slot, I was wondering if it would be fine to hook up 1x Sata SSD over the LSI card to act as the boot drive and 7x HDDs in Raid for storage? Would this work?"},
{"Title": "Downloading from Shootproof.com", "Author": "u/Bringback-T_D", "Content": "A shootproof.com gallery was sent out to attendees of an event I just went to... However, I can't figure out how to download everything... It's all watermarked, so I wouldn't consider it 'stealing' (I also plan to purchase some of the pictures), but I just want to have a good-enough archive, before it's taken down.... I attempted to use https://github.com/ShootProof/shootproof-cli , however it's broken and out-of-date. What other tools/commands should I try out?"},
{"Title": "Canopus ADVC-55?", "Author": "u/LandRoverMedic", "Content": "Hi all, trying to build a setup for transfer of home movies etc, anyone having luck with ADVC-55â€™s? I see a few on ebay at a decent price. I hope to run with FireWire card and win11, not set on capture program yet. Was going to run with an old SHARP VC-684 although doesnâ€™t have s-video so may look for another. Appreciate any help. ðŸ‘ðŸ»"},
{"Title": "External HDD Reader", "Author": "u/alex_5506", "Content": "Hi there- I have an older laptop with a broken monitor so I canâ€™t just boot it up (also canâ€™t locate cords so I canâ€™t use an external monitor). Is there any kind of external hdd reader that will work without another laptop/desktop? These days Iâ€™m strictly iPhone/ipad and donâ€™t own a windows based machine. Any suggestions for easily accessing my old laptop hdd would be greatly appreciated."},
{"Title": "2 issues while digitizing VHS tapes", "Author": "u/desperado491", "Content": "My setup is: Playing VHS tapes on a Sony VHS/DVD combo with component jacks running into a Sony DCR-TRV 340 Camcorder and then via firewire into my Windows 7, capturing into WinDV. The biggest issue I've noticed is dropped frames- 33 frames dropped over the course of a 2 hour tape, another tape dropped 14 frames. This causes it to go out of sync. Is this the result of my computer? The tapes? I'm not sure how these dropped frames can be avoided and any help would be greatly appreciated. Another issue are these moving lines going up and down the screen. I've attached a video below that shows it. Adjusting the tracking doesn't seem to solve the issue. I first saw this issue in OBS but it's still happening in WinDV. Can anyone help identify these lines so I know what's causing them? Thanks so much!! https://youtu.be/xtHPgzeOorc"},
{"Title": "Personal Backups: what are the recommendations?", "Author": "u/ligerzeronz", "Content": "So currently, I have 4 portable hard drives which i am not using. These have family photos/videos/documents. I am wanting to use these drives in the next 2 months but has to be completely empty. I am also now at a phase where i need stuff backed up just in case (3TB on one NAS, same data as above) I am not worried about it being cold storage as I have now uploaded these to Youtube and Google Photos. What would you guys recommend? A few I've heard is Backblaze and crashplan but i am completely new in this realm"},
{"Title": "Which file system(s) should I choose?", "Author": "u/jyssys", "Content": "Context: I'm going to run Linux file server VMs on VmWare ESXi. Each server will have two virtual disks connected: one for live data, and one for backup (on separate physical hard drives). The plan is to use Rsnapshot to backup the live data onto the backup data disk. In the past, I've had troubles where files has somehow gotten corrupted, and then the backup of the working files has rolled out of scope in the backup scheme, losing me those files. I'm told that there are file systems that can help me avoid that sort of thing. But which one? For the backups, I'm thinking simply ext4 since it's just rock solid. But for the live data, something like btrfs or zfs sounds good, but I cannot make my mind up about which one, or even if either is what I want. Ideally, I could run some tool once in a while, and if the tool spots corruption, I could just restore the file from backup. Which is best for this out of btrfs or zfs? Or is there a better option?"},
{"Title": "I Resurrected Subscene from the Subscene_V2 dump", "Author": "u/UltraNigatelo1911", "Content": "https://resubscene.vercel.app/ A subtitles database website using all the data that was dumped before subscene closure (Only extracted Arabic & English subtitle) website screenshot The dump was massive with over 2 million extracted subtitle files (deduped & counting only english & arabic) With over 75 GB of extracted files and 1.2 GB of just the metadata The whole goal of this project was to provide a website to access this vast amount of subtitles accumulated over the years of subscene operation and also an opportunity to improve the horrible user experience the website suffered from, and the slow and inaccurate search, inability to download individual .srt; .ass; files directly. I plan on adding the missing languages and open sourcing the whole project alongside the processed data Huge thanks to the Subscene dump: Subscene.com full Dump : r/DataHoarder (reddit.com)"},
{"Title": "Backup a GitHub repository's Wiki and Discussions", "Author": "u/pea_gravel", "Content": "There's this old GitHub repository that I'd like to have its Wiki and Discussions backed up. Do you guys know if it's possible to have a copy of it in markdown or any other format? Thanks"},
{"Title": "Need advice for creating a data hoarding setup - pretty lost", "Author": "u/CommercialDue1397", "Content": "Hi, I have a budget of around $5,000 CAD and would like to try to back up my data as well as I can. I would like to have 2 backups, 1 onsite 1 offsite. I have around 35 TB of data that I need to back up, and another 32 TB that I'd like to dedicate to new data. I'm looking into making a NAS build with RAID1 and creating the same build twice (once offsite, once where I live) but I really don't think that's cost-effective. In terms of drives - eyeing WD Red at the moment. I'm pretty sure this is a bad idea. I just don't know what would be considered a good idea. I don't know much about this sort of stuff and I'm tired of having 20 hard drives that could fail at any time. I've been lucky that my drives haven't failed in 10 years+ (besides one) but I don't want to take that risk anymore. Budget could increase theoreticaly to $8,000 CAD if needed but I'm trying to stay under $5,000 as much as I can. Please help suggest any solutions that I should look into. I don't have a good local network where I live right now, but I am going to move soon and will be able to get 10gbit inside the new place + 1gbit symmetrical offsite. Thank you!"},
{"Title": "How to recover old and faulty HDD?", "Author": "u/LauraAmerica", "Content": "I recently went back to my parents' (overseas) and found my backup ( WD 3TB My Book ). I got all hyped, it enclosures the treasures from my golden years. Sadly, it doesn't seem to work properly anymore â€”even when it was stored in a dry fresh dark place (for over a decade). When I plugged it in the computer it was able to read it without issue. Knowing the fragility of the situation, I started to copy the whole drive instead of browsing it. After 10% or so it stopped working. I re-plugged it in a different port and retried with similar results (with a progress equally low). After that it became hard to read it so I stopped â€”I didn't want to damage it by forcing a huge transfer in an unstable condition. My current plan is to try to access it from some Linux distro and, if I can read it, copy by folders (everything is organized in main categories). I don't think my old friend has much time left for trial and errors, so I'd like to check with you. I'm sure you'll have better ideas than my current barbaric plan. Thank you, this is important for me, and I highly appreciate and advise."},
{"Title": "Mobile game Rips?", "Author": "u/Wise_Leather_9159", "Content": "Okay I've been banging my head on every wall and surface of the Internet trying to solve a problem, I want to extract some 3D models from a mobile game so I can do some renders with em in SFM but everything I've tried just seems to come up empty or just simply not work. Does anyone have any advice, how to's etc etc???"},
{"Title": "I bought one of those fake flash drives accidently... Now half my videos are unplayable, is there any way to recover them or are they gone forever? Thanks.", "Author": "u/Valuable-Chance5370", "Content": "No content"},
{"Title": "BetaBagels: A briefing with the MTA Open Data Team https://us02web.zoom.us/meeting/register/tZEscuihpjwvGdT4RvNn7xPQbc0KsnpLHCGT#/registration", "Author": "u/Gabba_Rama", "Content": "Taking place online, may be of interest. https://us02web.zoom.us/meeting/register/tZEscuihpjwvGdT4RvNn7xPQbc0KsnpLHCGT#/registration"},
{"Title": "What do you do with your old hdd that are â€œstill goodâ€?", "Author": "u/Strange_Advisor_", "Content": "So as things grow I find myself replacing disks with bigger ones, replacing enclosures with Nas and das and now im at the point of what do I do with the obsolete stuff ? I have about 6 x 2 TB hdd and 1 x 3 TB hdd and one old ProBox that I am retiring with my upgrade ... do I just leave them on a shelf? Do I yeet them? There is nothing really important on them, just movies and tv shows, but it seems like a waste to toss them and seems like a waste to keep them. Thus i have no idea what do so ... so what does everyone else do? Thanks fam !"},
{"Title": "Efficient way to bulk download from Live Music Archive?", "Author": "u/jplank1983", "Content": "I'm interested in downloading all shows from a particular musician from the Live Music Archive on archive.org in FLAC format. Is there an efficient way to do this? The way I've been doing this so far is to navigate to each show's page and download the flac files manually. But, this is tedious as there are many, many shows. I'm wondering if there's some tool that would allow me to do the same thing more easily. I am on a Windows 11 machine, in case that's relevant."},
{"Title": "How to archive mixed data types? Structured + unstructured...", "Author": "u/rnourse", "Content": "Looking for some ideas from the hive mind here. The department I support is retiring a number of applications and wants to archive the data for regulatory and compliance reasons. But these apps are a mixture of structured and unstructured data and since at least one of them is a SaaS app we wont have the ability to simply leave the old system running in RO mode. I'm trying to develop a shortlist of commercial products that can handle both the db tables+schemas along with pdf files, emails and documents in a single tool, ideally via a single pane of glass. Has anyone here had a similar challenge and if so, what types of tools did you consider?  Cloud as a target is fine."},
{"Title": "Better option for in site backup ?", "Author": "u/Ihavefinancialissues", "Content": "Storage option for on-site backup ? This is part of a backup plan with off site and cloud backup."},
{"Title": "Internal power error with new HDD", "Author": "Unknown author", "Content": "Currently running into an issue with adding another drive to my hoard. Any time I connect a new drive I get a BSOD Internal power error. I remove the new HDD and the error goes away. I have tried other driver other power connectors but nothing beside removing the newly connected HDD fixes it. Currently running: Os: Windows Server 2022 Standard v 21H2 HDD count 10 SSd count 2 PSU 850watt Any idea what I am doing wrong ?"},
{"Title": "Segate exos", "Author": "u/sezayesh", "Content": "Hello guys. Noob here. I just bought a seagate exos 10TB and i think i made a mistake. I read someone saying about iron wolf drives that you should reduce start/stop events on these drives because they are build for 24/7 operation. Does it mean they are not suitable for pc use?(Since you keep turning them on and off) And is it the same with exos dar drives? Thank you"},
{"Title": "Long Lasting External Hard Drive Recommendations?", "Author": "u/Ispeakforthelorax", "Content": "Just discovered this sub, and wanted recommendations on external hard drives that will last! For a bit of background, my laptop is currently filled with all my stuff (personal documents, stuff from my undergrad, photos, videos, phone back ups, etc.). I currently just have a back up on a WD 1 TB HDD (I know I'm a bad boy for not following the 3-2-1 method, but I am trying to now!). I've been using this HDD for the past 8 years (and it saved my ass when my laptop died on me twice), and I am recently starting to hear sounds from it when it's running and it got me worried to find a replacement. All my data is ~250 GB, and I am looking for recommendations on what are some good external hard drives I can use! Although I am considering online cloud services, but I'm not sure if I want to pay a yearly subscription which has roughly the same annual cost similar to an external hard drive. I would prefer to pay for 2 hard drives and have it last for couple of years instead, rinse and repeat. I am looking to buy 2 hard drives, but am having a hard time decide which ones. I don't care about transfer speeds, and am looking for 500GB to 1 TB memory. The thing I care most about is that it lasts. I know there isn't any guarantee a that a external drive will last and could die any moment, but I would prefer it to last at least 5 years (hopefully 10 years) before I find a replacement. I generally do a back up 3-4 times a year, and let it collect dust on my shelf for the rest of the year. I was thinking of buying a HDD since I thought those last longer than SSDs, but apparently a bunch of websites on Google is telling me otherwise? That today's SSDs now last as long as HDDs or longer? Has SSDs surpassed HDDs in the past decade? Right now, I got my eyes on: Samsung T7 1 TB Portable SSD LaCie Rugged Mini 1 TB (SSD) Seagate STHN1000400 1TB Backup Plus Slim Portable Drive WD - Easystore 1TB External USB 3.0 Portable Drive I'm thinking I may do 1 HDD and 1 SSD cause why not lmao. I would be open to any suggestions on how to go about this! I would love to hear your thoughts and recommendations from your personal experiences!"},
{"Title": "I am a non-techy person. Is there an easy way to calculate the size of a website?", "Author": "u/ColdDijon", "Content": "I am non-techy and I would like to download all the images from a specific website for personal archiving. Is there a way I could know exactly just how much storage/size a website has before I download the whole website/images, and is there a way to just download the images, and have them downloaded automatically categorized like it does on the site? The site is a photo gallery. Thanks to whoever can help!"},
{"Title": "File Integrity checking for offline storage - RapidCRC still sufficient?", "Author": "u/Individual-Many-5784", "Content": "Just started properly getting into the rabbit hole that is datahoarding; for the past couple years I have only been storing my relevant files on external storage devices and using RapidCRC to generate CRC32 hashes for the files within each directory (such that I can pick up on any file corruption issues during quarterly checks of my data). While I have since decided to build myself a proper NAS to serve as my primary storage (with external storage and a cloud service serving as the backups), a couple questions still remain in regards to my current practices with external storage: To my understanding, there are far better hashing algorithms than CRC32 but would they provide any tangible benefits over CRC32 solely from a data corruption perspective (e.g. a lower chance of hash remaining the same in the event of corruption, even if unlikely to begin with)? (I'm probably overthinking this one, but) does it matter if I have one checksum file per directory (that contains all the file hashes in said directory) as opposed to an individual file per item? Lastly, would there be a more efficient method of checking the directories (opening checksum files to verify file integrity) than doing so manually? I don't have much practical knowledge with running scripts and the like, but am willing to learn if necessary. Thanks for reading and appreciate the help! :)"},
{"Title": "Curate your dataset before the internet gets spammed by ai generated content.", "Author": "u/YouWide5985", "Content": "I've spent the past few weeks doing this, and only now do I realize that I have the data hoarding disease."},
{"Title": "Growing my RAID1 with larger disks or creating a new RAID1 and copying?", "Author": "u/bloepz", "Content": "I currently have 2*3TB in RAID1 (HDD) and have two new disks (6TB HDD) which I want to use instead. It is encrypted and looks like this: /dev/sd(d,e)1 (partitions) -> /dev/md1 (raid1) -> /dev/mapper/raid1 (crypt) -> /files/raid1 (ext4) Is it better/faster to: Create a new RAID1 on the two 6TB, encrypt it, create filesystem and copy the data from the old RAID1 to this new RAID1 or Add disks to the existing RAID1, grow it and resize the crypt and fs like this: mdadm /dev/md1 --add /dev/sdf1 /dev/sdh1 --replace /dev/sdd1 /dev/sde1 --with /dev/sdf1 /dev/sdh1 mdadm /dev/md1 --remove /dev/sdd1 /dev/sde1 cryptsetup --resize /dev/mapper/raid1 resize2fs /dev/mapper/raid1 Availability is not an issue - it can be offline for the whole time. I'm assuming growing the existing would take more time but that's purely a guess. And does one of those models do more wear on the new disks?"},
{"Title": "LSi 9300 able to connect 7x HDDs and 1x SDD?", "Author": "u/fenderbender8", "Content": "I am new to the diy NAS scene, and have decided on creating a N100 mini PC NAS as it meets my needs for low power consumption and enough performance for my use case. However, seeing as the mini PC I have only has 1 NVMe slot, I was wondering if it would be fine to hook up 1x Sata SSD over the LSI card to act as the boot drive and 7x HDDs in Raid for storage? Would this work?"},
{"Title": "Downloading from Shootproof.com", "Author": "u/Bringback-T_D", "Content": "A shootproof.com gallery was sent out to attendees of an event I just went to... However, I can't figure out how to download everything... It's all watermarked, so I wouldn't consider it 'stealing' (I also plan to purchase some of the pictures), but I just want to have a good-enough archive, before it's taken down.... I attempted to use https://github.com/ShootProof/shootproof-cli , however it's broken and out-of-date. What other tools/commands should I try out?"},
{"Title": "Canopus ADVC-55?", "Author": "u/LandRoverMedic", "Content": "Hi all, trying to build a setup for transfer of home movies etc, anyone having luck with ADVC-55â€™s? I see a few on ebay at a decent price. I hope to run with FireWire card and win11, not set on capture program yet. Was going to run with an old SHARP VC-684 although doesnâ€™t have s-video so may look for another. Appreciate any help. ðŸ‘ðŸ»"},
{"Title": "External HDD Reader", "Author": "u/alex_5506", "Content": "Hi there- I have an older laptop with a broken monitor so I canâ€™t just boot it up (also canâ€™t locate cords so I canâ€™t use an external monitor). Is there any kind of external hdd reader that will work without another laptop/desktop? These days Iâ€™m strictly iPhone/ipad and donâ€™t own a windows based machine. Any suggestions for easily accessing my old laptop hdd would be greatly appreciated."},
{"Title": "2 issues while digitizing VHS tapes", "Author": "u/desperado491", "Content": "My setup is: Playing VHS tapes on a Sony VHS/DVD combo with component jacks running into a Sony DCR-TRV 340 Camcorder and then via firewire into my Windows 7, capturing into WinDV. The biggest issue I've noticed is dropped frames- 33 frames dropped over the course of a 2 hour tape, another tape dropped 14 frames. This causes it to go out of sync. Is this the result of my computer? The tapes? I'm not sure how these dropped frames can be avoided and any help would be greatly appreciated. Another issue are these moving lines going up and down the screen. I've attached a video below that shows it. Adjusting the tracking doesn't seem to solve the issue. I first saw this issue in OBS but it's still happening in WinDV. Can anyone help identify these lines so I know what's causing them? Thanks so much!! https://youtu.be/xtHPgzeOorc"},
{"Title": "Personal Backups: what are the recommendations?", "Author": "u/ligerzeronz", "Content": "So currently, I have 4 portable hard drives which i am not using. These have family photos/videos/documents. I am wanting to use these drives in the next 2 months but has to be completely empty. I am also now at a phase where i need stuff backed up just in case (3TB on one NAS, same data as above) I am not worried about it being cold storage as I have now uploaded these to Youtube and Google Photos. What would you guys recommend? A few I've heard is Backblaze and crashplan but i am completely new in this realm"},
{"Title": "Which file system(s) should I choose?", "Author": "u/jyssys", "Content": "Context: I'm going to run Linux file server VMs on VmWare ESXi. Each server will have two virtual disks connected: one for live data, and one for backup (on separate physical hard drives). The plan is to use Rsnapshot to backup the live data onto the backup data disk. In the past, I've had troubles where files has somehow gotten corrupted, and then the backup of the working files has rolled out of scope in the backup scheme, losing me those files. I'm told that there are file systems that can help me avoid that sort of thing. But which one? For the backups, I'm thinking simply ext4 since it's just rock solid. But for the live data, something like btrfs or zfs sounds good, but I cannot make my mind up about which one, or even if either is what I want. Ideally, I could run some tool once in a while, and if the tool spots corruption, I could just restore the file from backup. Which is best for this out of btrfs or zfs? Or is there a better option?"},
{"Title": "I Resurrected Subscene from the Subscene_V2 dump", "Author": "u/UltraNigatelo1911", "Content": "https://resubscene.vercel.app/ A subtitles database website using all the data that was dumped before subscene closure (Only extracted Arabic & English subtitle) website screenshot The dump was massive with over 2 million extracted subtitle files (deduped & counting only english & arabic) With over 75 GB of extracted files and 1.2 GB of just the metadata The whole goal of this project was to provide a website to access this vast amount of subtitles accumulated over the years of subscene operation and also an opportunity to improve the horrible user experience the website suffered from, and the slow and inaccurate search, inability to download individual .srt; .ass; files directly. I plan on adding the missing languages and open sourcing the whole project alongside the processed data Huge thanks to the Subscene dump: Subscene.com full Dump : r/DataHoarder (reddit.com)"},
{"Title": "Backup a GitHub repository's Wiki and Discussions", "Author": "u/pea_gravel", "Content": "There's this old GitHub repository that I'd like to have its Wiki and Discussions backed up. Do you guys know if it's possible to have a copy of it in markdown or any other format? Thanks"},
{"Title": "Need advice for creating a data hoarding setup - pretty lost", "Author": "u/CommercialDue1397", "Content": "Hi, I have a budget of around $5,000 CAD and would like to try to back up my data as well as I can. I would like to have 2 backups, 1 onsite 1 offsite. I have around 35 TB of data that I need to back up, and another 32 TB that I'd like to dedicate to new data. I'm looking into making a NAS build with RAID1 and creating the same build twice (once offsite, once where I live) but I really don't think that's cost-effective. In terms of drives - eyeing WD Red at the moment. I'm pretty sure this is a bad idea. I just don't know what would be considered a good idea. I don't know much about this sort of stuff and I'm tired of having 20 hard drives that could fail at any time. I've been lucky that my drives haven't failed in 10 years+ (besides one) but I don't want to take that risk anymore. Budget could increase theoreticaly to $8,000 CAD if needed but I'm trying to stay under $5,000 as much as I can. Please help suggest any solutions that I should look into. I don't have a good local network where I live right now, but I am going to move soon and will be able to get 10gbit inside the new place + 1gbit symmetrical offsite. Thank you!"},
{"Title": "How to recover old and faulty HDD?", "Author": "u/LauraAmerica", "Content": "I recently went back to my parents' (overseas) and found my backup ( WD 3TB My Book ). I got all hyped, it enclosures the treasures from my golden years. Sadly, it doesn't seem to work properly anymore â€”even when it was stored in a dry fresh dark place (for over a decade). When I plugged it in the computer it was able to read it without issue. Knowing the fragility of the situation, I started to copy the whole drive instead of browsing it. After 10% or so it stopped working. I re-plugged it in a different port and retried with similar results (with a progress equally low). After that it became hard to read it so I stopped â€”I didn't want to damage it by forcing a huge transfer in an unstable condition. My current plan is to try to access it from some Linux distro and, if I can read it, copy by folders (everything is organized in main categories). I don't think my old friend has much time left for trial and errors, so I'd like to check with you. I'm sure you'll have better ideas than my current barbaric plan. Thank you, this is important for me, and I highly appreciate and advise."},
{"Title": "Mobile game Rips?", "Author": "u/Wise_Leather_9159", "Content": "Okay I've been banging my head on every wall and surface of the Internet trying to solve a problem, I want to extract some 3D models from a mobile game so I can do some renders with em in SFM but everything I've tried just seems to come up empty or just simply not work. Does anyone have any advice, how to's etc etc???"},
{"Title": "I bought one of those fake flash drives accidently... Now half my videos are unplayable, is there any way to recover them or are they gone forever? Thanks.", "Author": "u/Valuable-Chance5370", "Content": "No content"},
{"Title": "BetaBagels: A briefing with the MTA Open Data Team https://us02web.zoom.us/meeting/register/tZEscuihpjwvGdT4RvNn7xPQbc0KsnpLHCGT#/registration", "Author": "u/Gabba_Rama", "Content": "Taking place online, may be of interest. https://us02web.zoom.us/meeting/register/tZEscuihpjwvGdT4RvNn7xPQbc0KsnpLHCGT#/registration"},
{"Title": "What do you do with your old hdd that are â€œstill goodâ€?", "Author": "u/Strange_Advisor_", "Content": "So as things grow I find myself replacing disks with bigger ones, replacing enclosures with Nas and das and now im at the point of what do I do with the obsolete stuff ? I have about 6 x 2 TB hdd and 1 x 3 TB hdd and one old ProBox that I am retiring with my upgrade ... do I just leave them on a shelf? Do I yeet them? There is nothing really important on them, just movies and tv shows, but it seems like a waste to toss them and seems like a waste to keep them. Thus i have no idea what do so ... so what does everyone else do? Thanks fam !"},
{"Title": "Efficient way to bulk download from Live Music Archive?", "Author": "u/jplank1983", "Content": "I'm interested in downloading all shows from a particular musician from the Live Music Archive on archive.org in FLAC format. Is there an efficient way to do this? The way I've been doing this so far is to navigate to each show's page and download the flac files manually. But, this is tedious as there are many, many shows. I'm wondering if there's some tool that would allow me to do the same thing more easily. I am on a Windows 11 machine, in case that's relevant."},
{"Title": "How to archive mixed data types? Structured + unstructured...", "Author": "u/rnourse", "Content": "Looking for some ideas from the hive mind here. The department I support is retiring a number of applications and wants to archive the data for regulatory and compliance reasons. But these apps are a mixture of structured and unstructured data and since at least one of them is a SaaS app we wont have the ability to simply leave the old system running in RO mode. I'm trying to develop a shortlist of commercial products that can handle both the db tables+schemas along with pdf files, emails and documents in a single tool, ideally via a single pane of glass. Has anyone here had a similar challenge and if so, what types of tools did you consider?  Cloud as a target is fine."},
{"Title": "Better option for in site backup ?", "Author": "u/Ihavefinancialissues", "Content": "Storage option for on-site backup ? This is part of a backup plan with off site and cloud backup."},
{"Title": "Internal power error with new HDD", "Author": "Unknown author", "Content": "Currently running into an issue with adding another drive to my hoard. Any time I connect a new drive I get a BSOD Internal power error. I remove the new HDD and the error goes away. I have tried other driver other power connectors but nothing beside removing the newly connected HDD fixes it. Currently running: Os: Windows Server 2022 Standard v 21H2 HDD count 10 SSd count 2 PSU 850watt Any idea what I am doing wrong ?"},
{"Title": "Segate exos", "Author": "u/sezayesh", "Content": "Hello guys. Noob here. I just bought a seagate exos 10TB and i think i made a mistake. I read someone saying about iron wolf drives that you should reduce start/stop events on these drives because they are build for 24/7 operation. Does it mean they are not suitable for pc use?(Since you keep turning them on and off) And is it the same with exos dar drives? Thank you"},
{"Title": "Long Lasting External Hard Drive Recommendations?", "Author": "u/Ispeakforthelorax", "Content": "Just discovered this sub, and wanted recommendations on external hard drives that will last! For a bit of background, my laptop is currently filled with all my stuff (personal documents, stuff from my undergrad, photos, videos, phone back ups, etc.). I currently just have a back up on a WD 1 TB HDD (I know I'm a bad boy for not following the 3-2-1 method, but I am trying to now!). I've been using this HDD for the past 8 years (and it saved my ass when my laptop died on me twice), and I am recently starting to hear sounds from it when it's running and it got me worried to find a replacement. All my data is ~250 GB, and I am looking for recommendations on what are some good external hard drives I can use! Although I am considering online cloud services, but I'm not sure if I want to pay a yearly subscription which has roughly the same annual cost similar to an external hard drive. I would prefer to pay for 2 hard drives and have it last for couple of years instead, rinse and repeat. I am looking to buy 2 hard drives, but am having a hard time decide which ones. I don't care about transfer speeds, and am looking for 500GB to 1 TB memory. The thing I care most about is that it lasts. I know there isn't any guarantee a that a external drive will last and could die any moment, but I would prefer it to last at least 5 years (hopefully 10 years) before I find a replacement. I generally do a back up 3-4 times a year, and let it collect dust on my shelf for the rest of the year. I was thinking of buying a HDD since I thought those last longer than SSDs, but apparently a bunch of websites on Google is telling me otherwise? That today's SSDs now last as long as HDDs or longer? Has SSDs surpassed HDDs in the past decade? Right now, I got my eyes on: Samsung T7 1 TB Portable SSD LaCie Rugged Mini 1 TB (SSD) Seagate STHN1000400 1TB Backup Plus Slim Portable Drive WD - Easystore 1TB External USB 3.0 Portable Drive I'm thinking I may do 1 HDD and 1 SSD cause why not lmao. I would be open to any suggestions on how to go about this! I would love to hear your thoughts and recommendations from your personal experiences!"},
{"Title": "I am a non-techy person. Is there an easy way to calculate the size of a website?", "Author": "u/ColdDijon", "Content": "I am non-techy and I would like to download all the images from a specific website for personal archiving. Is there a way I could know exactly just how much storage/size a website has before I download the whole website/images, and is there a way to just download the images, and have them downloaded automatically categorized like it does on the site? The site is a photo gallery. Thanks to whoever can help!"},
{"Title": "File Integrity checking for offline storage - RapidCRC still sufficient?", "Author": "u/Individual-Many-5784", "Content": "Just started properly getting into the rabbit hole that is datahoarding; for the past couple years I have only been storing my relevant files on external storage devices and using RapidCRC to generate CRC32 hashes for the files within each directory (such that I can pick up on any file corruption issues during quarterly checks of my data). While I have since decided to build myself a proper NAS to serve as my primary storage (with external storage and a cloud service serving as the backups), a couple questions still remain in regards to my current practices with external storage: To my understanding, there are far better hashing algorithms than CRC32 but would they provide any tangible benefits over CRC32 solely from a data corruption perspective (e.g. a lower chance of hash remaining the same in the event of corruption, even if unlikely to begin with)? (I'm probably overthinking this one, but) does it matter if I have one checksum file per directory (that contains all the file hashes in said directory) as opposed to an individual file per item? Lastly, would there be a more efficient method of checking the directories (opening checksum files to verify file integrity) than doing so manually? I don't have much practical knowledge with running scripts and the like, but am willing to learn if necessary. Thanks for reading and appreciate the help! :)"},
{"Title": "Curate your dataset before the internet gets spammed by ai generated content.", "Author": "u/YouWide5985", "Content": "I've spent the past few weeks doing this, and only now do I realize that I have the data hoarding disease."},
{"Title": "Growing my RAID1 with larger disks or creating a new RAID1 and copying?", "Author": "u/bloepz", "Content": "I currently have 2*3TB in RAID1 (HDD) and have two new disks (6TB HDD) which I want to use instead. It is encrypted and looks like this: /dev/sd(d,e)1 (partitions) -> /dev/md1 (raid1) -> /dev/mapper/raid1 (crypt) -> /files/raid1 (ext4) Is it better/faster to: Create a new RAID1 on the two 6TB, encrypt it, create filesystem and copy the data from the old RAID1 to this new RAID1 or Add disks to the existing RAID1, grow it and resize the crypt and fs like this: mdadm /dev/md1 --add /dev/sdf1 /dev/sdh1 --replace /dev/sdd1 /dev/sde1 --with /dev/sdf1 /dev/sdh1 mdadm /dev/md1 --remove /dev/sdd1 /dev/sde1 cryptsetup --resize /dev/mapper/raid1 resize2fs /dev/mapper/raid1 Availability is not an issue - it can be offline for the whole time. I'm assuming growing the existing would take more time but that's purely a guess. And does one of those models do more wear on the new disks?"},
{"Title": "Organizing Photos & Videos + Deleting Duplicates", "Author": "u/Misclickable", "Content": "Hi. I would like to organize my backup of pictures and videos of my iPhone. I've been importing the pictures and videos on my iPhones to my PC for the last decade. Sometimes the import function didn't work as expected and I had to import everything all over again. Long story short, I now have 3 folders: Folder 1: Old pictures that are in DICM folders. It branches out quite a bit and their names are IMG_0001. Folder 2: A folder with pictures and videos split into two folders. Folder 3: Main folder that I want everything to be in. Most of it is right, but I'm sure it is missing the older pictures and videos in Folder 1. What I'm looking for: I want the pictures and videos to be chronologically placed so I can go through them without interruption. Preferably a program that would look at the 3 folders and create a new folder with all the files organized like 2024-05-28_IMG_0001. Remove duplicates so they don't occupy space unnecessarily. It could check if the file was created at the same time, instead of comparing the files one by one. What I've done so far: Tried using AntiDupl.NET but it would take too long to sort them one by one. Tried using dupeGuru but I can't verify the files actually being duplicates. A slight problem could be that some images are in .HEIC and some videos are in .HEVC format. I don't mind them being in that format as I use ImageGlass to view the pictures and VLC to view the videos. Extra: I'd appreciate if anybody have a good method of backing up their pictures and videos on their iPhone."},
{"Title": "6 bay USB Raid enclosure", "Author": "u/boglim_destroyer", "Content": "I just set up my DS224+ with mirrored 16tb drives and am currently backing up to a 5tb external, as my data can fit on that currently. However this will not always be the case and I am planning for when that time comes. I have 6 x 4tb drives that are not in use and I would like to find a USB RAID enclosure for these that I can run RAID 5 with a hot spare or RAID 6. Does anyone have any recommendations on an enclosure?"},
{"Title": "Prefer To Use Drives As Drives, Not Combined Storage (Noob Question)", "Author": "u/shadowoflight", "Content": "Hello, Totally new to NAS. I typically just add drives to my desktop, so in that sense I'm already super used to treating each drive individually. Been considering an NAS because sometimes the random drive spin-up causing the whole system to slow down or pause. Can be annoying, esp when gaming, that's why I'm considering shifting all my data storage off-pc. I thought it was just a matter of getting a NAS and plugging the drives in, but it seems like in a NAS, it is considered a single storage drive regardless of the number of drives it has? I'm assuming you can split up the storage, but that's still not using the drives individually yes? Or is there a way/setting you can set so that you don't have to use any kind of RAID and just have the system treat each drive as, well, a drive?"},
{"Title": "Is there an easyish app to auto-save and organize images and videos that meet a certain criteria?", "Author": "u/drupadoo", "Content": "Like everytime I watch a youtube video, just download it and back it up. Same with every reddit image I look at. Etc."},
{"Title": "Program to View/Change in Mass Encoding Types of Text (.lrc) files", "Author": "u/ngs428", "Content": "I have about 13,000 text (song lyric .lrc) files saved music file folders.  The are several hundred folders in this structure. I am looking for a program that can scan the folder and give me a view of the encoding types on each of the .lrc files.  Some are saved as UTF-8-BOM and I need to remove the BOM, saving them as UTF-8 only. Something like the document list in Notepad++ but having an â€œencodingâ€ column. Or maybe there is a better way to accomplish what I am trying to do?  Any help would be appreciated!  Thanks! Edit: I just found EncodingChecker at: https://github.com/amrali-eg/EncodingChecker This is what I am looking for.  I will leave the post up for anyone with the same question in the future."},
{"Title": "Can a Focus Enhancement FS-H200 digitize DV tapes?", "Author": "u/AriFeblowitzVFX", "Content": "I just bought one of these things on Ebay assuming that if it hooks up to your camera via firewire and records direct from camera to flash card, that it could handle recording tapes to a flash card right? Well, the manual only talks about recording with the camera and has nothing about playblack/digitizing,. why would it not have that feature? Does anyone know if there's a way to use these to digitize?"},
{"Title": "back up a larger drive into a smaller drive using compression", "Author": "u/8yp00o19pB14Ic", "Content": "atm i have a sff hp thin client running linux, set up as a samba file server. got all my data on a 1 tb toshiba usb hdd. about 700 gb of photos and documents, music and movies.... i bought a 2 tb  seagate usb hdd for a great price this week because i want to back up the data on the toshiba hdd, i dont trust all my data being on a single drive jic it fails..... my plan was to copy all the data from the toshiba to the seagate, set up the seagate as the new file server drive, and then i was going to set up the file server to do a weekly incremental backup on the toshiba drive. basically set the file server to back up the seagate drive to the toshiba drive weekly... i have done backups before, backing up my laptop's 256 gb ssd to the toshiba drive, no issues there. part of me wonders if its possible to compress down a backup of a drive and fit it into a smaller drive, say theoretically i had 1.2 tb worth on the seagate drive and compress it and store the backup on the 1 tb toshiba drive. if i could back up a larger drive into a smaller drive using compression, id be able to take advantage of that 2 tb instead of the opposite way around, where i only have 1 tb to play with and im backing up to a 2 tb drive. what am i missing, is this possible to do?"},
{"Title": "Archive down", "Author": "u/Legendary_Player", "Content": "Updates: https://x.com/internetarchive/status/1794793738482659453 [May 26th, 6:12 pm UTC] https://x.com/internetarchive/status/1795117949499445554 (DDOS Attack) [May 27th, 3:40 pm UTC] https://x.com/internetarchive/status/1795140710624039286 (Up again, then down, then Up again) [May 27th, 5:11 pm UTC] It's up â¬†ï¸ for now ðŸ¥³ [from around May 27th, 7:29 pm UTC] https://x.com/internetarchive/status/1795451463465845141 (Another New DDOS Attack) [May 28th, 1:46 pm UTC] \"We are continuing to experience service disruptions due to a recurrence of a ddos attack. Weâ€™ll post updates in this thread.\" https://x.com/internetarchive/status/1795536130160390567 (Another Update) [May 28, 7:22 pm UTC] https://x.com/internetarchive/status/1795559880423534731 (Another Update) [May 28, 8:56 pm UTC] Some attacks are occurring today ðŸ˜Ÿ [June 5, 3:30 pm UTC]"},
{"Title": "Dvd drive", "Author": "u/mro2352", "Content": "Iâ€™m wondering when firmware from dvd drives didnâ€™t have a flash chip but actual rom."},
{"Title": "Linux Executables to Shrink your Video Horde?", "Author": "Unknown author", "Content": "I was looking for any kind of script or docker image that can help. My horde has gotten very bloated again. I have tried Fileflows(Docker image is broke on version 24.05+ for me), Tdarr just doesn't provide enough customization, and I don't feel like learning how to code my own plugin, & Handbrake docker doesn't have permissions to launch NGIX for me. I was thinking of a script with find , a txt file for processed files, ffmpeg , and watch .  Because I would like to do everything in AV1 and I have  an A380 available in my server for transcodes. As any one done a script like this or know of one? I can reinvent the wheel but I would prefer to have a starting point. Thanks"},
{"Title": "Anyone in Minnesota repair Digibeta decks?", "Author": "u/AriFeblowitzVFX", "Content": "Here are the 3 problem machines: VTR NTSC - Thomson TTV 3450 N Sony DVW-A510 Betamax EDV-7000 TTV 3450- Error code 009, eject button doesnâ€™t work, no video signal output, not even for menu or time codes DVW-A510- everything works except for the Channel condition doesnâ€™t light up and the video signal from the tapes are blank gray with no audio, but the menu, time code, and color bars all output and the tape plays/rewinds/ejects fine. EDV-7000- This one a tape got stuck and none of the tape buttons like play or eject are working I only need one of the digibeta decks repaired and I should be good, but it would be good to at least get the tape out of the Betamax deck as well. Anyone know of anyone in Minnesota who fixes these things?Â  As you could guess Iâ€™ve been having bad luck trying to get a working digibeta deck on EBay haha."},
{"Title": "How do you error correct between multiple backup copies?", "Author": "u/Auralisme", "Content": "Letâ€™s say you have 1TB worth of files that you donâ€™t need to access frequently (twice a year.) You have 3 different 1TB hard drives available so you make 3 copies of the files. Scenario 1: Some sections on one of the drives are no longer correct. Scenario 2: Some sections on two of the drives are no longer correct, but they donâ€™t overlap. What should you do in each of these scenarios? In my mind a raid 4 would makes sense for this application but Iâ€™ve read that using raid for long term backup strategies is not a good idea, so Iâ€™m trying to figure out how to make this work with 3 separate identical copies. My guess would be to simply overwrite the data on the damaged copy with one of the correct ones in scenario 1, but is there a good way to recover from scenario 2? What preparations do I need to do beforehand to ensure scenario 2 is recoverable? Is there any software I can use for such situations? Thank you for taking the time to answer this."},
{"Title": "GHTorrent used to be largest github archive. It was stopped updating couple of years ago and now whole site is down. Any alternatives?", "Author": "u/real_life_ironman", "Content": "As title, ghtorrent.org is not working anymore. Someone hacked/bought the domain. And mongo files from http://ghtorrent-downloads.ewi.tudelft.nl/ are not accessible anymore. I just ordered ~100TB of harddrives and wanted to start a project. Any alternatives to access this so I can store it?"},
{"Title": "automatically move/sort music artists into folders", "Author": "u/vanderzee", "Content": "im not so tech savy, i tried finding a solution but it only got me more confused i would like to move all my music artist folers into genre folders is there some whay to do this with software assistance? thanks!"},
{"Title": "How do I store NVME drives while itâ€™s not in use?", "Author": "u/Teeeeze", "Content": "I have a few nvme drives that are not currently used lying around in my house. But I think thereâ€™s a certain way to store and preserve it but not sure. I searched on the internet and YouTube but no clue."},
{"Title": "Looking for a scanner that can handle a 40-year collection of documents according to my needs.", "Author": "u/SergeantYoshi", "Content": "Hello dear community, I have spent the last few weeks extensively researching scanners and now I have my second one here. I first had a ScanSnap ix1600 and now I have the Epson ES-580W. Both are great scanners, although I like the ES-580W a bit better because it has a web interface. But now I have reached the point where I don't want to bring every existing scanner into my house just to see if it has the functions I want, simply because the internet doesn't provide enough information. Therefore, I am reaching out to you for help. I need a scanner that allows me to create a large number of different profiles, in which I can then scan to a network folder. The reason is that my family and I have a wide variety of different documents that all need to be scanned directly into the corresponding folders (around 60+ profiles/shortcuts). The ScanSnap only has a handful of profiles and even though the Epson has 300 contacts, I can ultimately only create 48 presets. And I would like to be able to customize the file prefixes for each profile. I am hoping the community can help me find a suitable scanner. It should have what the previous two had, but additionally, it is important to me that it: Has a web server where I can make my settings (create profiles, etc.) Has a lot of Network Folder Shortcuts. Or alot of Shortcuts in General. Can scan receipts well Can handle no less than 30 pages The Epson was pleasantly quiet, so it would be great if it wasn't too loud (not a main criterion) The cheaper, the better, but given the price range I am currently considering, I expect to find something around â‚¬800 or less with the desired features. I look forward to your responses and hope that people can help me out."},
{"Title": "Help with portable music storage", "Author": "u/tejasdharmabum", "Content": "Hello! I have a large music collection (mostly CDs) that Iâ€™m backing up, and Iâ€™ve been putting some on a USB drive to shuffle on my commuting drive, I live in a rural area so streaming is not an option. However the Texas heat seems to be getting to my flash drives and I have to replace them about twice a year. Is there a better quality drive that wonâ€™t break the bank too much, can hold about 3000 songs and doesnâ€™t eff-up that often? I have to leave it in my truck, I work on construction sites so nowhere to store it and not really safe to bring it with me.. Thanks yâ€™all!"},
{"Title": "nas motherboard with sff 8643 to 4* nvme Pcie adapter", "Author": "u/Hilarsky", "Content": "So, just after I bought 1165g7 nas from topton I found this from CWWK, which on paper looks amazing to me. In theory it supports up to 6x nvme with this odd extension board you can buy with this bundle, however they're stating on product page that it doesn't support pcie 4.0 nvme drives (which sounds like bs to me ) My question is , does anyone has this board ? Works ok ? Anyone seen any information regarding sff 8643 -> to pcie ? google produces only garbage results ... https://www.aliexpress.com/item/1005007029671776.html?pdp_npi=4%40dis%21GBP%21%EF%BF%A1321.66%21%EF%BF%A1160.83%21%21%21398.00%21199.00%21%40210384cc17168245681252058e0b43%2112000039151333112%21sh%21UK%210%21"},
{"Title": "Can I use SATA drives directly without enclosure for data back ups?", "Author": "u/tealbull", "Content": "I'm currently in need of around 30TB of storage to archive completed project data and free up my RAID volumes, which I use for active projects. I've looked into external desktop drive options from LaCie, WD, and Seagate, but they are quite pricey. I'm considering buying 3.5inch SATA drives and using a SATA to USB connector (which I already have) to offload the data. I've found that 18TB SATA drives are significantly cheaper (2-3 times) compared to external desktop drive solutions from LaCie, WD, or Seagate. Is this a good idea? What might be some disadvantages of doing this?"},
{"Title": "What is the best (safe, but priority on space efficient) way to store a large quantity (around 1.2k) optical discs", "Author": "u/A_Big_Igloo", "Content": "Title says it. I have just shy of 1.2K discs that I have digitized for my media server, but I'd like to keep the originals in case I discover an error in digitization or server failure. I currently have four 3d printed spindles that hold around 300 each but they're uncovered and the design doesn't allow for a cover. Is anyone aware of either retail spindles that have covers that I can buy without having to buy a bunch of blank discs that I'd just throw out in a 300ish capacity, or some other space efficient storage system? I don't anticipate the need to access them any time soon, hence the spindles. I have avoided books because they are so space inefficient, I can fit twice the amount of discs in spindles that I can in the same space of books. Are there other methods that I haven't even considered?"},
{"Title": "Anyone know how to read this old disk?", "Author": "u/GloomyAction", "Content": "No content"},
{"Title": "5 TB drive went from 100 mb/s transfers to now 60", "Author": "u/Soolidus", "Content": "WD HDD. Got slower and slower the closer it got to full capacity, which I thought was normal, but I recently deleted 3 TBs worth and it's still the same. Any way to fix it? Is this normal?"},
{"Title": "Win10, how can I have more drive letters for backblaze?", "Author": "u/covered1028", "Content": "Right now I am plugging each drive 1 by 1 and have backblaze back up each drive while I figure out my setup but I will run out of drive letters soon. One solution I thought about was that when I run out of drive letters, I will reuse a drive letter and for that drive make a new folder moving all data in there so I would know what data I lost and I can restore it by going through the time machine thing."},
{"Title": "Is there a noticeable difference between 150 and 400 MB/s?", "Author": "u/cannotperson", "Content": "I'm looking at https://www.amazon.com/SanDisk-Ultra-Dual-Drive-Type-C/dp/B0842P5GG5 and there are two options, one for 150 and another for 400 MB/s read speed. The main purpose will be storing backup files and downloaded files from the web. For a $30 USD price increase, is the 400 MB/s flash drive worth it?"},
{"Title": "What is a good program to download a neocities website?", "Author": "u/Acceptable_Cook_8112", "Content": "hey i have been struggling to download a neo cities website i want because i tried doing it with htttrack and it didnt work because it didnt give me alll the sources /; idk how to download it with the full sources. im downloading it because i wanna edit the code and modify it so it can fit my aesthetic lol"},
{"Title": "Organizing Photos & Videos + Deleting Duplicates", "Author": "u/Misclickable", "Content": "Hi. I would like to organize my backup of pictures and videos of my iPhone. I've been importing the pictures and videos on my iPhones to my PC for the last decade. Sometimes the import function didn't work as expected and I had to import everything all over again. Long story short, I now have 3 folders: Folder 1: Old pictures that are in DICM folders. It branches out quite a bit and their names are IMG_0001. Folder 2: A folder with pictures and videos split into two folders. Folder 3: Main folder that I want everything to be in. Most of it is right, but I'm sure it is missing the older pictures and videos in Folder 1. What I'm looking for: I want the pictures and videos to be chronologically placed so I can go through them without interruption. Preferably a program that would look at the 3 folders and create a new folder with all the files organized like 2024-05-28_IMG_0001. Remove duplicates so they don't occupy space unnecessarily. It could check if the file was created at the same time, instead of comparing the files one by one. What I've done so far: Tried using AntiDupl.NET but it would take too long to sort them one by one. Tried using dupeGuru but I can't verify the files actually being duplicates. A slight problem could be that some images are in .HEIC and some videos are in .HEVC format. I don't mind them being in that format as I use ImageGlass to view the pictures and VLC to view the videos. Extra: I'd appreciate if anybody have a good method of backing up their pictures and videos on their iPhone."},
{"Title": "6 bay USB Raid enclosure", "Author": "u/boglim_destroyer", "Content": "I just set up my DS224+ with mirrored 16tb drives and am currently backing up to a 5tb external, as my data can fit on that currently. However this will not always be the case and I am planning for when that time comes. I have 6 x 4tb drives that are not in use and I would like to find a USB RAID enclosure for these that I can run RAID 5 with a hot spare or RAID 6. Does anyone have any recommendations on an enclosure?"},
{"Title": "Prefer To Use Drives As Drives, Not Combined Storage (Noob Question)", "Author": "u/shadowoflight", "Content": "Hello, Totally new to NAS. I typically just add drives to my desktop, so in that sense I'm already super used to treating each drive individually. Been considering an NAS because sometimes the random drive spin-up causing the whole system to slow down or pause. Can be annoying, esp when gaming, that's why I'm considering shifting all my data storage off-pc. I thought it was just a matter of getting a NAS and plugging the drives in, but it seems like in a NAS, it is considered a single storage drive regardless of the number of drives it has? I'm assuming you can split up the storage, but that's still not using the drives individually yes? Or is there a way/setting you can set so that you don't have to use any kind of RAID and just have the system treat each drive as, well, a drive?"},
{"Title": "Is there an easyish app to auto-save and organize images and videos that meet a certain criteria?", "Author": "u/drupadoo", "Content": "Like everytime I watch a youtube video, just download it and back it up. Same with every reddit image I look at. Etc."},
{"Title": "Program to View/Change in Mass Encoding Types of Text (.lrc) files", "Author": "u/ngs428", "Content": "I have about 13,000 text (song lyric .lrc) files saved music file folders.  The are several hundred folders in this structure. I am looking for a program that can scan the folder and give me a view of the encoding types on each of the .lrc files.  Some are saved as UTF-8-BOM and I need to remove the BOM, saving them as UTF-8 only. Something like the document list in Notepad++ but having an â€œencodingâ€ column. Or maybe there is a better way to accomplish what I am trying to do?  Any help would be appreciated!  Thanks! Edit: I just found EncodingChecker at: https://github.com/amrali-eg/EncodingChecker This is what I am looking for.  I will leave the post up for anyone with the same question in the future."},
{"Title": "Can a Focus Enhancement FS-H200 digitize DV tapes?", "Author": "u/AriFeblowitzVFX", "Content": "I just bought one of these things on Ebay assuming that if it hooks up to your camera via firewire and records direct from camera to flash card, that it could handle recording tapes to a flash card right? Well, the manual only talks about recording with the camera and has nothing about playblack/digitizing,. why would it not have that feature? Does anyone know if there's a way to use these to digitize?"},
{"Title": "back up a larger drive into a smaller drive using compression", "Author": "u/8yp00o19pB14Ic", "Content": "atm i have a sff hp thin client running linux, set up as a samba file server. got all my data on a 1 tb toshiba usb hdd. about 700 gb of photos and documents, music and movies.... i bought a 2 tb  seagate usb hdd for a great price this week because i want to back up the data on the toshiba hdd, i dont trust all my data being on a single drive jic it fails..... my plan was to copy all the data from the toshiba to the seagate, set up the seagate as the new file server drive, and then i was going to set up the file server to do a weekly incremental backup on the toshiba drive. basically set the file server to back up the seagate drive to the toshiba drive weekly... i have done backups before, backing up my laptop's 256 gb ssd to the toshiba drive, no issues there. part of me wonders if its possible to compress down a backup of a drive and fit it into a smaller drive, say theoretically i had 1.2 tb worth on the seagate drive and compress it and store the backup on the 1 tb toshiba drive. if i could back up a larger drive into a smaller drive using compression, id be able to take advantage of that 2 tb instead of the opposite way around, where i only have 1 tb to play with and im backing up to a 2 tb drive. what am i missing, is this possible to do?"},
{"Title": "Archive down", "Author": "u/Legendary_Player", "Content": "Updates: https://x.com/internetarchive/status/1794793738482659453 [May 26th, 6:12 pm UTC] https://x.com/internetarchive/status/1795117949499445554 (DDOS Attack) [May 27th, 3:40 pm UTC] https://x.com/internetarchive/status/1795140710624039286 (Up again, then down, then Up again) [May 27th, 5:11 pm UTC] It's up â¬†ï¸ for now ðŸ¥³ [from around May 27th, 7:29 pm UTC] https://x.com/internetarchive/status/1795451463465845141 (Another New DDOS Attack) [May 28th, 1:46 pm UTC] \"We are continuing to experience service disruptions due to a recurrence of a ddos attack. Weâ€™ll post updates in this thread.\" https://x.com/internetarchive/status/1795536130160390567 (Another Update) [May 28, 7:22 pm UTC] https://x.com/internetarchive/status/1795559880423534731 (Another Update) [May 28, 8:56 pm UTC] Some attacks are occurring today ðŸ˜Ÿ [June 5, 3:30 pm UTC]"},
{"Title": "Dvd drive", "Author": "u/mro2352", "Content": "Iâ€™m wondering when firmware from dvd drives didnâ€™t have a flash chip but actual rom."},
{"Title": "Linux Executables to Shrink your Video Horde?", "Author": "Unknown author", "Content": "I was looking for any kind of script or docker image that can help. My horde has gotten very bloated again. I have tried Fileflows(Docker image is broke on version 24.05+ for me), Tdarr just doesn't provide enough customization, and I don't feel like learning how to code my own plugin, & Handbrake docker doesn't have permissions to launch NGIX for me. I was thinking of a script with find , a txt file for processed files, ffmpeg , and watch .  Because I would like to do everything in AV1 and I have  an A380 available in my server for transcodes. As any one done a script like this or know of one? I can reinvent the wheel but I would prefer to have a starting point. Thanks"},
{"Title": "Anyone in Minnesota repair Digibeta decks?", "Author": "u/AriFeblowitzVFX", "Content": "Here are the 3 problem machines: VTR NTSC - Thomson TTV 3450 N Sony DVW-A510 Betamax EDV-7000 TTV 3450- Error code 009, eject button doesnâ€™t work, no video signal output, not even for menu or time codes DVW-A510- everything works except for the Channel condition doesnâ€™t light up and the video signal from the tapes are blank gray with no audio, but the menu, time code, and color bars all output and the tape plays/rewinds/ejects fine. EDV-7000- This one a tape got stuck and none of the tape buttons like play or eject are working I only need one of the digibeta decks repaired and I should be good, but it would be good to at least get the tape out of the Betamax deck as well. Anyone know of anyone in Minnesota who fixes these things?Â  As you could guess Iâ€™ve been having bad luck trying to get a working digibeta deck on EBay haha."},
{"Title": "How do you error correct between multiple backup copies?", "Author": "u/Auralisme", "Content": "Letâ€™s say you have 1TB worth of files that you donâ€™t need to access frequently (twice a year.) You have 3 different 1TB hard drives available so you make 3 copies of the files. Scenario 1: Some sections on one of the drives are no longer correct. Scenario 2: Some sections on two of the drives are no longer correct, but they donâ€™t overlap. What should you do in each of these scenarios? In my mind a raid 4 would makes sense for this application but Iâ€™ve read that using raid for long term backup strategies is not a good idea, so Iâ€™m trying to figure out how to make this work with 3 separate identical copies. My guess would be to simply overwrite the data on the damaged copy with one of the correct ones in scenario 1, but is there a good way to recover from scenario 2? What preparations do I need to do beforehand to ensure scenario 2 is recoverable? Is there any software I can use for such situations? Thank you for taking the time to answer this."},
{"Title": "GHTorrent used to be largest github archive. It was stopped updating couple of years ago and now whole site is down. Any alternatives?", "Author": "u/real_life_ironman", "Content": "As title, ghtorrent.org is not working anymore. Someone hacked/bought the domain. And mongo files from http://ghtorrent-downloads.ewi.tudelft.nl/ are not accessible anymore. I just ordered ~100TB of harddrives and wanted to start a project. Any alternatives to access this so I can store it?"},
{"Title": "automatically move/sort music artists into folders", "Author": "u/vanderzee", "Content": "im not so tech savy, i tried finding a solution but it only got me more confused i would like to move all my music artist folers into genre folders is there some whay to do this with software assistance? thanks!"},
{"Title": "How do I store NVME drives while itâ€™s not in use?", "Author": "u/Teeeeze", "Content": "I have a few nvme drives that are not currently used lying around in my house. But I think thereâ€™s a certain way to store and preserve it but not sure. I searched on the internet and YouTube but no clue."},
{"Title": "Looking for a scanner that can handle a 40-year collection of documents according to my needs.", "Author": "u/SergeantYoshi", "Content": "Hello dear community, I have spent the last few weeks extensively researching scanners and now I have my second one here. I first had a ScanSnap ix1600 and now I have the Epson ES-580W. Both are great scanners, although I like the ES-580W a bit better because it has a web interface. But now I have reached the point where I don't want to bring every existing scanner into my house just to see if it has the functions I want, simply because the internet doesn't provide enough information. Therefore, I am reaching out to you for help. I need a scanner that allows me to create a large number of different profiles, in which I can then scan to a network folder. The reason is that my family and I have a wide variety of different documents that all need to be scanned directly into the corresponding folders (around 60+ profiles/shortcuts). The ScanSnap only has a handful of profiles and even though the Epson has 300 contacts, I can ultimately only create 48 presets. And I would like to be able to customize the file prefixes for each profile. I am hoping the community can help me find a suitable scanner. It should have what the previous two had, but additionally, it is important to me that it: Has a web server where I can make my settings (create profiles, etc.) Has a lot of Network Folder Shortcuts. Or alot of Shortcuts in General. Can scan receipts well Can handle no less than 30 pages The Epson was pleasantly quiet, so it would be great if it wasn't too loud (not a main criterion) The cheaper, the better, but given the price range I am currently considering, I expect to find something around â‚¬800 or less with the desired features. I look forward to your responses and hope that people can help me out."},
{"Title": "Help with portable music storage", "Author": "u/tejasdharmabum", "Content": "Hello! I have a large music collection (mostly CDs) that Iâ€™m backing up, and Iâ€™ve been putting some on a USB drive to shuffle on my commuting drive, I live in a rural area so streaming is not an option. However the Texas heat seems to be getting to my flash drives and I have to replace them about twice a year. Is there a better quality drive that wonâ€™t break the bank too much, can hold about 3000 songs and doesnâ€™t eff-up that often? I have to leave it in my truck, I work on construction sites so nowhere to store it and not really safe to bring it with me.. Thanks yâ€™all!"},
{"Title": "nas motherboard with sff 8643 to 4* nvme Pcie adapter", "Author": "u/Hilarsky", "Content": "So, just after I bought 1165g7 nas from topton I found this from CWWK, which on paper looks amazing to me. In theory it supports up to 6x nvme with this odd extension board you can buy with this bundle, however they're stating on product page that it doesn't support pcie 4.0 nvme drives (which sounds like bs to me ) My question is , does anyone has this board ? Works ok ? Anyone seen any information regarding sff 8643 -> to pcie ? google produces only garbage results ... https://www.aliexpress.com/item/1005007029671776.html?pdp_npi=4%40dis%21GBP%21%EF%BF%A1321.66%21%EF%BF%A1160.83%21%21%21398.00%21199.00%21%40210384cc17168245681252058e0b43%2112000039151333112%21sh%21UK%210%21"},
{"Title": "Can I use SATA drives directly without enclosure for data back ups?", "Author": "u/tealbull", "Content": "I'm currently in need of around 30TB of storage to archive completed project data and free up my RAID volumes, which I use for active projects. I've looked into external desktop drive options from LaCie, WD, and Seagate, but they are quite pricey. I'm considering buying 3.5inch SATA drives and using a SATA to USB connector (which I already have) to offload the data. I've found that 18TB SATA drives are significantly cheaper (2-3 times) compared to external desktop drive solutions from LaCie, WD, or Seagate. Is this a good idea? What might be some disadvantages of doing this?"},
{"Title": "What is the best (safe, but priority on space efficient) way to store a large quantity (around 1.2k) optical discs", "Author": "u/A_Big_Igloo", "Content": "Title says it. I have just shy of 1.2K discs that I have digitized for my media server, but I'd like to keep the originals in case I discover an error in digitization or server failure. I currently have four 3d printed spindles that hold around 300 each but they're uncovered and the design doesn't allow for a cover. Is anyone aware of either retail spindles that have covers that I can buy without having to buy a bunch of blank discs that I'd just throw out in a 300ish capacity, or some other space efficient storage system? I don't anticipate the need to access them any time soon, hence the spindles. I have avoided books because they are so space inefficient, I can fit twice the amount of discs in spindles that I can in the same space of books. Are there other methods that I haven't even considered?"},
{"Title": "Anyone know how to read this old disk?", "Author": "u/GloomyAction", "Content": "No content"},
{"Title": "5 TB drive went from 100 mb/s transfers to now 60", "Author": "u/Soolidus", "Content": "WD HDD. Got slower and slower the closer it got to full capacity, which I thought was normal, but I recently deleted 3 TBs worth and it's still the same. Any way to fix it? Is this normal?"},
{"Title": "Win10, how can I have more drive letters for backblaze?", "Author": "u/covered1028", "Content": "Right now I am plugging each drive 1 by 1 and have backblaze back up each drive while I figure out my setup but I will run out of drive letters soon. One solution I thought about was that when I run out of drive letters, I will reuse a drive letter and for that drive make a new folder moving all data in there so I would know what data I lost and I can restore it by going through the time machine thing."},
{"Title": "Is there a noticeable difference between 150 and 400 MB/s?", "Author": "u/cannotperson", "Content": "I'm looking at https://www.amazon.com/SanDisk-Ultra-Dual-Drive-Type-C/dp/B0842P5GG5 and there are two options, one for 150 and another for 400 MB/s read speed. The main purpose will be storing backup files and downloaded files from the web. For a $30 USD price increase, is the 400 MB/s flash drive worth it?"},
{"Title": "What is a good program to download a neocities website?", "Author": "u/Acceptable_Cook_8112", "Content": "hey i have been struggling to download a neo cities website i want because i tried doing it with htttrack and it didnt work because it didnt give me alll the sources /; idk how to download it with the full sources. im downloading it because i wanna edit the code and modify it so it can fit my aesthetic lol"},
{"Title": "Finally built a desktop. Stuffed it with all my old laptop drives. 32TB, I may have a problem.", "Author": "u/DR650SE", "Content": "No content"},
{"Title": "Preserve online games", "Author": "u/Occhako", "Content": "Hi so a game I've been playing for 7 years, Love Live SIF, got shut down. And I lost everything that got me scared about this happening to other games i play like genshin, obey me, Honkai: Star Rail and love and deepspace. I want to be able to make a copy of the game that can run locally so that if the game ever closes it's servers i can keep playing content that i already own. So like i don't mind if there's no updates, i just don't want to lose my hardwork because the company can't afford to keep the servers or they just shut down with no explanation. How would i go about doing that or learning how to do that. If someone could point me in the right direction that would be great. I should probably stop playing these kind of games and spending money on them, but all the good mobile and PC games these days keep requiring an internet to play the game even if it's a single player."},
{"Title": "Can the \"WD_BLACK D10 Game Drive for Xbox 12TB\" be used as a normal external drive?", "Author": "u/Pinguinina", "Content": "With a PC or Mac for regular files, etc. It has Xbox branding and is bigger than the \"PC/Console version\" so I don't know if it has some sort of formatting or block built into it that prevents it from just being used as an external. I've had the 8TB for a while but I need more space. It's reasonably portable considering the size which is something I need and as far as I'm aware it includes a good drive. I'm open to alternative suggestions though but it needs to be portable and not really more expensive."},
{"Title": "Data access: SSD vs HDD once plugged in â€“ thoughts?", "Author": "u/Future-Cod-7565", "Content": "Hello fellow hoarders, Please could someone help me with this: I have a 2TB SanDisk SSD and a 5TB Seagate HDD. Both full of data (both almost filled up). When plugged into my laptop (MacBook Pro 2017, 16GB RAM, 4 USB-C ports) the access time to the data significantly varies: while for both disks the root is readily available the moment you click the disk icon and open the drive, for the SSD drive it takes a while (sometimes rather long while, I'd say) to show the contents of a folder, while for the HDD it takes no time at all. This surprises me greatly, for what I used to think (mechanic-wise) the HDD having moving parts, spindle and platters and head to position, it should take longer to locate the data, when the SSD having no need to position the head and all that it should be a split second to give me access to the data in a folder. Can this be related to how my OS (macOS Monterey 12.7.5) accesses the drives? Anyone with the same \"issue\" (can't name it a real issue since I do have access to the data)? Please advise. Thank you!"},
{"Title": "Anyone know why my DigiBeta DVW-A510 deck is just giving blank gray for the video signal?", "Author": "u/AriFeblowitzVFX", "Content": "No content"},
{"Title": "Cheapest short term 100TB storage with good bandwidth", "Author": "u/CategoryHoliday9210", "Content": "I need to have preferably SSD of 100TB -200TB storage. Can you share where I can get the best bang for my buck? I am not planning to buy it as I don't think I can manage it for a long time. I would need it for 2-3 months and My connection speed is not great down 300mbps. So looking for online solution. Edit: since a lot people are curious about SSD, it would be various databases so speed is kind of important , I wish I could do it in RAM but SSD would do."},
{"Title": "1fichier to store 100-200gb of data in the cloud for cheap", "Author": "u/greyfeather9", "Content": "Hey, after looking at multiple services I think 1fichier seems fit. I just want to store a cloud backup of some stuff, the limiting factor is a 50gb zip with some sites having limitations on filesize, but fichier limit is 300gb. the zip will stay as is but I will be doing daily/weekly uploads of 2gb backups or so. just wanted to ask your opinion on their service, thanks."},
{"Title": "LTO 5 Class Tape Drive or Hot Swapping a bunch of 1.2TB 2.5\" 10k SAS Drives?", "Author": "u/Empire_Fable", "Content": "Good day every one.  Thank you for letting me in the community.  I just started dabbling with archiving data I'm currently archiving some rather big compressed files 200+ GB.  Weighing some options.  I considered buying a HP LTO 5 class tape drive(Its more in my price range then the newer ones) .  Looking at the Tape media cost, I'm seeing maybe a better option to just get a bunch of hotswappable 2.5\"  1.2Tb SAS drives for a older HP dl360.  Then just hot swapping them out like tapes.  I really did want a tape drive though.  Any one know of a good benefit to a tape drive other then them being super cool?"},
{"Title": "Sagittarius NAS Case Review and Build Tips", "Author": "u/Ben4425", "Content": "I recently rebuilt my NAS by moving it from a Fractal Node 804 case into the Sagittarius NAS case available from AliExpress. The Node 804 was a good case, with great temps, but swapping hard drives around was a pain. The 804 is also ginormous. So, why the Sagittarius? It met my requirements for MATX, eight externally accessible drive bays, and what appeared to be good drive cooling. I also considered: Audheid K7. Only had two 92mm fans and some reviews reported high drive temps. Also required buying a Flex PSU. Audheid 8-Bay 2023 Edition. Provides better cooling with two 120mm fans but still required a Flex PSU if you wanted all 8 drive bays. Jonsbo N4. Only 4 bays were externally accessible and it only has one 120mm fan. Overall, I'm happy with the Sagittarius case. Its very compact yet it holds 8 drives, an MATX motherboard, and four 120mm fans. My drive and CPU temps are excellent. But, you really need to plan your build because there's no documentation, no cable management, and because some connectors are hidden by other components. If you don't plug in your cables as you build then you'll never get to them later after the build is complete. You also need think about air flow which I'll discuss after documenting my build. Time for some photos, starting with the empty case. Empty Case The two small rectangular holes in the upper and bottom left are all you have for routing cables from this, the motherboard side, to the hard drives on the other side. I ran 4 SATA cables through each of these holes. My motherboard mounts 4 of its SATA Ports along the edge so I had to plug those in before installing the motherboard itself. Otherwise, those connectors would have been practically inaccessible: Motherboard Edge Connector Issues The case supports two 2.5 SSD drives that are screwed to the bottom of the case. But, if you do, they will be flush to the case so plugging in cables will be near impossible. I purchased some 1/4\" nylon standoffs and longer M3-10 screws to elevate the SSDs a bit. It was still a pain to plug in the cables (because they are toward the bottom of this photo) but it worked: https://preview.redd.it/sagittarius-nas-case-review-and-build-tips-v0-31oe40fbxq2d1.jpg I routed all my SATA and fan cables next. I have 10 SATA ports total, two for SSDs and 8 for HDDs. Four of those interfaces are on an ASM-1064 PCIe add-on board and the rest are on the motherboard. Then, it was time for the power supply. I strongly suggest using a modular SFX power supply that typically comes with shorter cables. Long, or unnecessary, cables will be an issue because there's no place to put them. Also note you should plug in the EPS power cable before you install the power supply because you'll never get to it afterward: EPS Power Connector Also make sure you route the SATA power cable before installing the power supply. Last, install the fans. Standard 25mm thickness fans just barely clear the main motherboard power cable at the bottom of this picture. Also note I installed fan grills on all my fans otherwise (for my airflow) the cables would have hit the fan blades: Finished Interior Now, about the \"drive sleds\". This case only provides rubber bushings and screws to fasten those bushings to the sides of your hard drives. They also provide a metal plate with a bend that acts as the handle to pull the drive from the case: \"Drive Sled\" This is really basic but I found it works well. Wrapping up, here's a photo of the finished product. You can see the slots on the right that hold the rubber bushings that are attached to the hard drives. Final Result w/o Drive Bay Cover I installed four 120mm Phanteks fans (from my old Node 804) into this case and all of them are configured to exhaust air from the case. There are two behind the grill on the left of this picture and you can see that the fan screws just go through the grating holes. Air for the left side of the case is pulled in through holes in the rear and a large grating on the left side of the case (not visible here). So, on the left, air is pulled from the side and down towards the CPU and motherboard before exhausting out the front. On the right, there are two fans behind the hard drive cage. They too exhaust air that is pulled from the front of the case, past the hard drives, and then blown out the rear. There's maybe 5mm space between the drives so airflow is unimpeded. At 22c ambient, my idle drive temps vary from 24c to 27c. Not bad! As I said earlier, I'm happy. The case is very compact (about 300x260x265 mm), holds eight 3.5\" drives, two 2.5\" SSDs, and runs cool. For about $180, which included shipping to Massachusetts, I think it was a good purchase. That said, it isn't perfect: No cable management features. No fans are included, you must provide your own. Standard ATX PSU are supported but IMHO are impractical due to the larger PSU size and longer cables. Cable management would be a mess . FYI, the case has one USB 3.0 Type A port and one USB-C port on the front. Both of these are wired to the same USB 3.0 motherboard cable so the USB-C port will be limited to USB 3.0 speeds (5 Gbps). I.e. the USB-C port is wired to a USB 3.0 port on the motherboard."},
{"Title": "What are expected SMART values for Seagate Recertified Drives?", "Author": "u/ELO_Space", "Content": "I recently bought a Seagate Ironwolf Pro Recertified drive (ST18000nt001), and the SMART results are worrying me a bit.. In particular with the seek error rate. Is this normal for a Seagate drive and how it reports raw values, or should I return it? (2 year warranty, plus I still am in the return period) https://preview.redd.it/what-are-expected-smart-values-for-seagate-recertified-v0-l756opbzlt2d1.png"},
{"Title": "Using NIB but older SSD / HDD manufactured in 2021 / 2022 ?", "Author": "u/instagigated", "Content": "Thoughts on using drives manufactured some years ago? Is there greater risk of data corruption/loss vs. using drives manufactured in the past 12 months? I received two drives recently: Sandisk 1 TB portable SSD (2021) WD 16 TB external HDD (2022) Should I return and get something manufactured more recently?"},
{"Title": "LTO5 Drive Repeatedly Clicks?", "Author": "u/1823alex", "Content": "No content"},
{"Title": "SingleFile vs Save Page WE?", "Author": "u/eldomtom2", "Content": "I'm using Save Page WE at the moment, is there a good reason to switch?"},
{"Title": "2 x Raid5 in mirror or in seperate servers and weekly backups", "Author": "u/Adorok", "Content": "i currently have 2x 16tb hdds in a mirror + a seperate disk for weekly backups (full backups, i dont want incremental backups), and the space is slowly running out. Buying bigger drives would be more expensive, than just buying 3 additional drives. I have a spare server, splitting between servers would only increase the power bill. By putting them in a mirror, i could increase read speed and have a lower electricity consumption. On the other hand, if my psu fails, all drives could theorethicaly fail at once. What do you guys think / would you do?"},
{"Title": "RAID 6 + 3, 2, 1 rule. Check my understanding?", "Author": "u/tddammo1", "Content": "Hi all, learning *much* more about RAID etc now that I'm upgrading from essentially raid 0 across 3 pi's into a new system that has the potential for 13 drive bays (at some point). As I'm learning, I want to verify I understand (some) of how the interaction of RAID 6 and the 3,2,1 rule can be applied. I'm perfectly okay with if this idea is wrong, and that's fine. Assumptions: All raid drives are 1TB, running RAID 6, using 4 drives 3 Copies Obviously the first one is RAID 6 1TB, allowing for 2 TB storage, 2TB protection. (HDD) Second backup: Should this be *another* RAID 6 node? Or can this be a single 2TB drive (SSD) Offsite backup: Similarly, can this be a 2TB drive Essentially when using 3, 2, 1 rule should we be mimicking RAID? Or is it safe to have these drives be single drives (yes, now we've gone from double redundancy to single point of failure, which is why I'm asking) Thanks!"},
{"Title": "What are the best free tools for creating a perfect bit-for-bit copy of a CD, including copy protection?", "Author": "u/KeptinGL6", "Content": "?"},
{"Title": "my solution to hoarding music", "Author": "u/admin_NLboy", "Content": "https://github.com/nlboy1/.album-filetype meet a tinyy new project of mine: .album, pretty simple, its just a renamed zip file which vlc can open, i think this can help some people out there who are juggling with 100+gb of flacs/mp3s or even wavs (me lol). so uuh please ggive feedback"},
{"Title": "Personal cloud storage", "Author": "u/Temporary-Fennel-785", "Content": "I have a few other friends that are fellow data hoarders, and we had a paid for google drive that held hundreds of GB of stuff that we would share back and forth. However it was apparently terminated by google (I don't have the full story, I wasn't the owner) Is there a way we could create our own server that would provide the same service? Something that could store all our files and allow our group spread across the states to access it."},
{"Title": "Mini-DV: Transcode or keep native codec?", "Author": "u/vanillapenguins", "Content": "The native codec of the video is DV25, which is neither lossless or open. For preservation purposes it would be better to transcode the video to FFV1 or another codec that is lossless, open source and commonly used. However, I have read that transcoding from DV25 does not guarantee that the technical metadata is preserved. That is data of about the camera, recording date and time, and video settings. I guess this depends on the settings - but since transcoding is more about the pixel data (?) it might not cover this extra metadata? Would you recommend me to archive the video in the native codec or transcode to a more preservation-friendly format?"},
{"Title": "Compatibility between SATA Add-On Cards & NAS OSes (TrueNAS, Unraid, etc)?", "Author": "u/McFlyParadox", "Content": "I am just starting work on my first DIY NAS. It's going to either run Plex directly, or host files for Plex (still figuring that part out). I selected the Silverstone CS382 for the case, because I liked it size; that it had 8x hot swap drive bays; has a slot for a slim ODD so that I can do some limited BDR rips for cold storage; and that the motherboard tray is inverted, potentially allowing for full-length GPUs if I decide to run Plex directly (I end up transcoding a lot because I need subtitles on). The only thing I don't really care for is that the largest motherboard it can accept is mATX, which has less to do with the size and more to do with current trends that exist on modern mATX boards. Basically, most modern mATX boards seem to sacrifice SATA port count in favor of dedicating the PCIe lanes to NVMe slots instead. I get it: between their speed and lack of cables, NVMe are pretty neat (*rim shot*), but not really what I need for this build. It seems that most mATX boards top out around 4x SATA ports (LGA 1700 Intel mATX boards seem to top out at 4x; some AM5 AMD mATX boards will go up to 6x, but most are 4x), and instead offer 2-3x NVMe slots instead. So what I am wondering is this: Has anyone tried using SATA add-on cards in a situation like this? Not a RAID card, just \"here is some SATA ports and a chip a tie them into the PCIe lanes\". I've found a couple of items like the Vantec M.2 PCIe Gen3x2 to 5 ports SATA III expansion card that look like they might get me up to 9x SATA ports, which will be enough to get me my 8x hotswap bays operational and the slim ODD drive. But what I am concerned about is that 4x of the SATA ports will be \"proper\" ones on the motherboard and 5x will be \"add-ons\", and I am not sure how that might affect a RAID 5 or 6 array. Side note that may change people's answers: there is a non-zero chance that if I decide to run Plex directly on this box, I may just make it a server running Ubuntu or Windows instead, and figure out either a software RAID 5 or 6, JBOD, or just plain drives and remote backup."},
{"Title": "Backup to Google drive", "Author": "u/Low_Start_3087", "Content": "Trying to sync 5Tb 500k files to Google drive with Win10 desktop apps. It works for about 250k files but after that it seems like the sync app is broken down and nothing happens. It only utilize a lot of memory and CPU. I use the backup feature and not sync with GD. Iâ€™m aware of the 750GB a day limit and this is something else. Doing same thing with Dropbox works fine even if they have a soft 250k limit and Google drive 500k. Is there any good way to sync/backup files to Google drive? I have tried Goodsync but it constantly think most of the files has change every time I do an analyze even if they donâ€™t. Please help! Since I have GD it would be nice to be able to use it as an extra clone of my files."},
{"Title": "New Backup Solution (Time to go LTO? Offsite)", "Author": "u/cyong", "Content": "Its time to re-evaluate the backup situation. My previous offsite backup (relatives place remote server) is no longer an option. So its time to evolve. I have about 250TB Unraid that I think I should take to a LTO setup. (Somehow...) And what I am thinking I want to do is to have some tape that I backup to once (or twice) a year and store offsite... and then another set of tapes that does more of a monthly backup and stays onsite. Given the amount of data I think LTO makes the most sense. But I honestly have no idea what to look for hardware, or software wise, or how to approach the matter in order to accomplish my goal. Do i need a desktop drive connected to PC running software there? Or a tape library in a rack? or????"},
{"Title": "Why do new PC cases have both 3.5inch and 2.5inch HDD drive mounts?", "Author": "u/SpatulaFlip", "Content": "May be a dumb question but Iâ€™m in the process of planning out my build for a home server/NAS and was looking at PC cases that can hold a decent number of drives. Why do PC cases have slots for both 3.5inch and 2.5inch drive mounts? Do people generally use both kinds in their systems? Or is it more of a space thing? Thanks ðŸ˜…"},
{"Title": "Is this speed normal? I am running a Write test on HDSentinel on WD My Passport", "Author": "u/sammorrison9800", "Content": "No content"},
{"Title": "Family Archivists: Best storage for 321 backups for posterity?", "Author": "u/CharlesGoo20201210", "Content": "I'm organizing my family archives for several branches of my family tree. This is really just files and directories organized in a loose way such that different branches of the family can read or write to their \"sub-trees\". This is mainly photos, pdfs, and plain text or markdown with stories, history, and metadata. I'm primarily using Google drive (and groups and contacts) to manage all this. I realized I need to trust google less and create a 321-backup as in https://www.veeam.com/blog/321-backup-rule.html \"three copies, two media types, one off-site\".  Note that \"for two media types\" here means more than one provider service (i.e. not just google). Questions: Do you use a storage service that looks similar, with plain old: files, folders, users, groups, permissions (read, write, admin) that you're happy with?  That covers this requirement Do you have a good automated way to copy from google drive to create backups periodically?  That preserves security data (possibly versioning too). This could be to my own RAID/NAS at home. Is there any service in the cloud that backs up directly from google drive for you (for a price) that works well? Encryption: do you know of a seamless service that allows one to automatically add a layer of zero-trust security to particular folders on google drive (or elsewhere). E.g. Some documents are financial and sensitive enough that we'd like extra security, but it's a big pain to encrypt/decrypt these locally then upload/download to the shared area. Thanks for any insights!"},
{"Title": "Finally built a desktop. Stuffed it with all my old laptop drives. 32TB, I may have a problem.", "Author": "u/DR650SE", "Content": "No content"},
{"Title": "Preserve online games", "Author": "u/Occhako", "Content": "Hi so a game I've been playing for 7 years, Love Live SIF, got shut down. And I lost everything that got me scared about this happening to other games i play like genshin, obey me, Honkai: Star Rail and love and deepspace. I want to be able to make a copy of the game that can run locally so that if the game ever closes it's servers i can keep playing content that i already own. So like i don't mind if there's no updates, i just don't want to lose my hardwork because the company can't afford to keep the servers or they just shut down with no explanation. How would i go about doing that or learning how to do that. If someone could point me in the right direction that would be great. I should probably stop playing these kind of games and spending money on them, but all the good mobile and PC games these days keep requiring an internet to play the game even if it's a single player."},
{"Title": "Can the \"WD_BLACK D10 Game Drive for Xbox 12TB\" be used as a normal external drive?", "Author": "u/Pinguinina", "Content": "With a PC or Mac for regular files, etc. It has Xbox branding and is bigger than the \"PC/Console version\" so I don't know if it has some sort of formatting or block built into it that prevents it from just being used as an external. I've had the 8TB for a while but I need more space. It's reasonably portable considering the size which is something I need and as far as I'm aware it includes a good drive. I'm open to alternative suggestions though but it needs to be portable and not really more expensive."},
{"Title": "Data access: SSD vs HDD once plugged in â€“ thoughts?", "Author": "u/Future-Cod-7565", "Content": "Hello fellow hoarders, Please could someone help me with this: I have a 2TB SanDisk SSD and a 5TB Seagate HDD. Both full of data (both almost filled up). When plugged into my laptop (MacBook Pro 2017, 16GB RAM, 4 USB-C ports) the access time to the data significantly varies: while for both disks the root is readily available the moment you click the disk icon and open the drive, for the SSD drive it takes a while (sometimes rather long while, I'd say) to show the contents of a folder, while for the HDD it takes no time at all. This surprises me greatly, for what I used to think (mechanic-wise) the HDD having moving parts, spindle and platters and head to position, it should take longer to locate the data, when the SSD having no need to position the head and all that it should be a split second to give me access to the data in a folder. Can this be related to how my OS (macOS Monterey 12.7.5) accesses the drives? Anyone with the same \"issue\" (can't name it a real issue since I do have access to the data)? Please advise. Thank you!"},
{"Title": "Anyone know why my DigiBeta DVW-A510 deck is just giving blank gray for the video signal?", "Author": "u/AriFeblowitzVFX", "Content": "No content"},
{"Title": "Cheapest short term 100TB storage with good bandwidth", "Author": "u/CategoryHoliday9210", "Content": "I need to have preferably SSD of 100TB -200TB storage. Can you share where I can get the best bang for my buck? I am not planning to buy it as I don't think I can manage it for a long time. I would need it for 2-3 months and My connection speed is not great down 300mbps. So looking for online solution. Edit: since a lot people are curious about SSD, it would be various databases so speed is kind of important , I wish I could do it in RAM but SSD would do."},
{"Title": "1fichier to store 100-200gb of data in the cloud for cheap", "Author": "u/greyfeather9", "Content": "Hey, after looking at multiple services I think 1fichier seems fit. I just want to store a cloud backup of some stuff, the limiting factor is a 50gb zip with some sites having limitations on filesize, but fichier limit is 300gb. the zip will stay as is but I will be doing daily/weekly uploads of 2gb backups or so. just wanted to ask your opinion on their service, thanks."},
{"Title": "LTO 5 Class Tape Drive or Hot Swapping a bunch of 1.2TB 2.5\" 10k SAS Drives?", "Author": "u/Empire_Fable", "Content": "Good day every one.  Thank you for letting me in the community.  I just started dabbling with archiving data I'm currently archiving some rather big compressed files 200+ GB.  Weighing some options.  I considered buying a HP LTO 5 class tape drive(Its more in my price range then the newer ones) .  Looking at the Tape media cost, I'm seeing maybe a better option to just get a bunch of hotswappable 2.5\"  1.2Tb SAS drives for a older HP dl360.  Then just hot swapping them out like tapes.  I really did want a tape drive though.  Any one know of a good benefit to a tape drive other then them being super cool?"},
{"Title": "Sagittarius NAS Case Review and Build Tips", "Author": "u/Ben4425", "Content": "I recently rebuilt my NAS by moving it from a Fractal Node 804 case into the Sagittarius NAS case available from AliExpress. The Node 804 was a good case, with great temps, but swapping hard drives around was a pain. The 804 is also ginormous. So, why the Sagittarius? It met my requirements for MATX, eight externally accessible drive bays, and what appeared to be good drive cooling. I also considered: Audheid K7. Only had two 92mm fans and some reviews reported high drive temps. Also required buying a Flex PSU. Audheid 8-Bay 2023 Edition. Provides better cooling with two 120mm fans but still required a Flex PSU if you wanted all 8 drive bays. Jonsbo N4. Only 4 bays were externally accessible and it only has one 120mm fan. Overall, I'm happy with the Sagittarius case. Its very compact yet it holds 8 drives, an MATX motherboard, and four 120mm fans. My drive and CPU temps are excellent. But, you really need to plan your build because there's no documentation, no cable management, and because some connectors are hidden by other components. If you don't plug in your cables as you build then you'll never get to them later after the build is complete. You also need think about air flow which I'll discuss after documenting my build. Time for some photos, starting with the empty case. Empty Case The two small rectangular holes in the upper and bottom left are all you have for routing cables from this, the motherboard side, to the hard drives on the other side. I ran 4 SATA cables through each of these holes. My motherboard mounts 4 of its SATA Ports along the edge so I had to plug those in before installing the motherboard itself. Otherwise, those connectors would have been practically inaccessible: Motherboard Edge Connector Issues The case supports two 2.5 SSD drives that are screwed to the bottom of the case. But, if you do, they will be flush to the case so plugging in cables will be near impossible. I purchased some 1/4\" nylon standoffs and longer M3-10 screws to elevate the SSDs a bit. It was still a pain to plug in the cables (because they are toward the bottom of this photo) but it worked: https://preview.redd.it/sagittarius-nas-case-review-and-build-tips-v0-31oe40fbxq2d1.jpg I routed all my SATA and fan cables next. I have 10 SATA ports total, two for SSDs and 8 for HDDs. Four of those interfaces are on an ASM-1064 PCIe add-on board and the rest are on the motherboard. Then, it was time for the power supply. I strongly suggest using a modular SFX power supply that typically comes with shorter cables. Long, or unnecessary, cables will be an issue because there's no place to put them. Also note you should plug in the EPS power cable before you install the power supply because you'll never get to it afterward: EPS Power Connector Also make sure you route the SATA power cable before installing the power supply. Last, install the fans. Standard 25mm thickness fans just barely clear the main motherboard power cable at the bottom of this picture. Also note I installed fan grills on all my fans otherwise (for my airflow) the cables would have hit the fan blades: Finished Interior Now, about the \"drive sleds\". This case only provides rubber bushings and screws to fasten those bushings to the sides of your hard drives. They also provide a metal plate with a bend that acts as the handle to pull the drive from the case: \"Drive Sled\" This is really basic but I found it works well. Wrapping up, here's a photo of the finished product. You can see the slots on the right that hold the rubber bushings that are attached to the hard drives. Final Result w/o Drive Bay Cover I installed four 120mm Phanteks fans (from my old Node 804) into this case and all of them are configured to exhaust air from the case. There are two behind the grill on the left of this picture and you can see that the fan screws just go through the grating holes. Air for the left side of the case is pulled in through holes in the rear and a large grating on the left side of the case (not visible here). So, on the left, air is pulled from the side and down towards the CPU and motherboard before exhausting out the front. On the right, there are two fans behind the hard drive cage. They too exhaust air that is pulled from the front of the case, past the hard drives, and then blown out the rear. There's maybe 5mm space between the drives so airflow is unimpeded. At 22c ambient, my idle drive temps vary from 24c to 27c. Not bad! As I said earlier, I'm happy. The case is very compact (about 300x260x265 mm), holds eight 3.5\" drives, two 2.5\" SSDs, and runs cool. For about $180, which included shipping to Massachusetts, I think it was a good purchase. That said, it isn't perfect: No cable management features. No fans are included, you must provide your own. Standard ATX PSU are supported but IMHO are impractical due to the larger PSU size and longer cables. Cable management would be a mess . FYI, the case has one USB 3.0 Type A port and one USB-C port on the front. Both of these are wired to the same USB 3.0 motherboard cable so the USB-C port will be limited to USB 3.0 speeds (5 Gbps). I.e. the USB-C port is wired to a USB 3.0 port on the motherboard."},
{"Title": "What are expected SMART values for Seagate Recertified Drives?", "Author": "u/ELO_Space", "Content": "I recently bought a Seagate Ironwolf Pro Recertified drive (ST18000nt001), and the SMART results are worrying me a bit.. In particular with the seek error rate. Is this normal for a Seagate drive and how it reports raw values, or should I return it? (2 year warranty, plus I still am in the return period) https://preview.redd.it/what-are-expected-smart-values-for-seagate-recertified-v0-l756opbzlt2d1.png"},
{"Title": "Using NIB but older SSD / HDD manufactured in 2021 / 2022 ?", "Author": "u/instagigated", "Content": "Thoughts on using drives manufactured some years ago? Is there greater risk of data corruption/loss vs. using drives manufactured in the past 12 months? I received two drives recently: Sandisk 1 TB portable SSD (2021) WD 16 TB external HDD (2022) Should I return and get something manufactured more recently?"},
{"Title": "LTO5 Drive Repeatedly Clicks?", "Author": "u/1823alex", "Content": "No content"},
{"Title": "SingleFile vs Save Page WE?", "Author": "u/eldomtom2", "Content": "I'm using Save Page WE at the moment, is there a good reason to switch?"},
{"Title": "2 x Raid5 in mirror or in seperate servers and weekly backups", "Author": "u/Adorok", "Content": "i currently have 2x 16tb hdds in a mirror + a seperate disk for weekly backups (full backups, i dont want incremental backups), and the space is slowly running out. Buying bigger drives would be more expensive, than just buying 3 additional drives. I have a spare server, splitting between servers would only increase the power bill. By putting them in a mirror, i could increase read speed and have a lower electricity consumption. On the other hand, if my psu fails, all drives could theorethicaly fail at once. What do you guys think / would you do?"},
{"Title": "RAID 6 + 3, 2, 1 rule. Check my understanding?", "Author": "u/tddammo1", "Content": "Hi all, learning *much* more about RAID etc now that I'm upgrading from essentially raid 0 across 3 pi's into a new system that has the potential for 13 drive bays (at some point). As I'm learning, I want to verify I understand (some) of how the interaction of RAID 6 and the 3,2,1 rule can be applied. I'm perfectly okay with if this idea is wrong, and that's fine. Assumptions: All raid drives are 1TB, running RAID 6, using 4 drives 3 Copies Obviously the first one is RAID 6 1TB, allowing for 2 TB storage, 2TB protection. (HDD) Second backup: Should this be *another* RAID 6 node? Or can this be a single 2TB drive (SSD) Offsite backup: Similarly, can this be a 2TB drive Essentially when using 3, 2, 1 rule should we be mimicking RAID? Or is it safe to have these drives be single drives (yes, now we've gone from double redundancy to single point of failure, which is why I'm asking) Thanks!"},
{"Title": "What are the best free tools for creating a perfect bit-for-bit copy of a CD, including copy protection?", "Author": "u/KeptinGL6", "Content": "?"},
{"Title": "my solution to hoarding music", "Author": "u/admin_NLboy", "Content": "https://github.com/nlboy1/.album-filetype meet a tinyy new project of mine: .album, pretty simple, its just a renamed zip file which vlc can open, i think this can help some people out there who are juggling with 100+gb of flacs/mp3s or even wavs (me lol). so uuh please ggive feedback"},
{"Title": "Personal cloud storage", "Author": "u/Temporary-Fennel-785", "Content": "I have a few other friends that are fellow data hoarders, and we had a paid for google drive that held hundreds of GB of stuff that we would share back and forth. However it was apparently terminated by google (I don't have the full story, I wasn't the owner) Is there a way we could create our own server that would provide the same service? Something that could store all our files and allow our group spread across the states to access it."},
{"Title": "Mini-DV: Transcode or keep native codec?", "Author": "u/vanillapenguins", "Content": "The native codec of the video is DV25, which is neither lossless or open. For preservation purposes it would be better to transcode the video to FFV1 or another codec that is lossless, open source and commonly used. However, I have read that transcoding from DV25 does not guarantee that the technical metadata is preserved. That is data of about the camera, recording date and time, and video settings. I guess this depends on the settings - but since transcoding is more about the pixel data (?) it might not cover this extra metadata? Would you recommend me to archive the video in the native codec or transcode to a more preservation-friendly format?"},
{"Title": "Compatibility between SATA Add-On Cards & NAS OSes (TrueNAS, Unraid, etc)?", "Author": "u/McFlyParadox", "Content": "I am just starting work on my first DIY NAS. It's going to either run Plex directly, or host files for Plex (still figuring that part out). I selected the Silverstone CS382 for the case, because I liked it size; that it had 8x hot swap drive bays; has a slot for a slim ODD so that I can do some limited BDR rips for cold storage; and that the motherboard tray is inverted, potentially allowing for full-length GPUs if I decide to run Plex directly (I end up transcoding a lot because I need subtitles on). The only thing I don't really care for is that the largest motherboard it can accept is mATX, which has less to do with the size and more to do with current trends that exist on modern mATX boards. Basically, most modern mATX boards seem to sacrifice SATA port count in favor of dedicating the PCIe lanes to NVMe slots instead. I get it: between their speed and lack of cables, NVMe are pretty neat (*rim shot*), but not really what I need for this build. It seems that most mATX boards top out around 4x SATA ports (LGA 1700 Intel mATX boards seem to top out at 4x; some AM5 AMD mATX boards will go up to 6x, but most are 4x), and instead offer 2-3x NVMe slots instead. So what I am wondering is this: Has anyone tried using SATA add-on cards in a situation like this? Not a RAID card, just \"here is some SATA ports and a chip a tie them into the PCIe lanes\". I've found a couple of items like the Vantec M.2 PCIe Gen3x2 to 5 ports SATA III expansion card that look like they might get me up to 9x SATA ports, which will be enough to get me my 8x hotswap bays operational and the slim ODD drive. But what I am concerned about is that 4x of the SATA ports will be \"proper\" ones on the motherboard and 5x will be \"add-ons\", and I am not sure how that might affect a RAID 5 or 6 array. Side note that may change people's answers: there is a non-zero chance that if I decide to run Plex directly on this box, I may just make it a server running Ubuntu or Windows instead, and figure out either a software RAID 5 or 6, JBOD, or just plain drives and remote backup."},
{"Title": "Backup to Google drive", "Author": "u/Low_Start_3087", "Content": "Trying to sync 5Tb 500k files to Google drive with Win10 desktop apps. It works for about 250k files but after that it seems like the sync app is broken down and nothing happens. It only utilize a lot of memory and CPU. I use the backup feature and not sync with GD. Iâ€™m aware of the 750GB a day limit and this is something else. Doing same thing with Dropbox works fine even if they have a soft 250k limit and Google drive 500k. Is there any good way to sync/backup files to Google drive? I have tried Goodsync but it constantly think most of the files has change every time I do an analyze even if they donâ€™t. Please help! Since I have GD it would be nice to be able to use it as an extra clone of my files."},
{"Title": "New Backup Solution (Time to go LTO? Offsite)", "Author": "u/cyong", "Content": "Its time to re-evaluate the backup situation. My previous offsite backup (relatives place remote server) is no longer an option. So its time to evolve. I have about 250TB Unraid that I think I should take to a LTO setup. (Somehow...) And what I am thinking I want to do is to have some tape that I backup to once (or twice) a year and store offsite... and then another set of tapes that does more of a monthly backup and stays onsite. Given the amount of data I think LTO makes the most sense. But I honestly have no idea what to look for hardware, or software wise, or how to approach the matter in order to accomplish my goal. Do i need a desktop drive connected to PC running software there? Or a tape library in a rack? or????"},
{"Title": "Why do new PC cases have both 3.5inch and 2.5inch HDD drive mounts?", "Author": "u/SpatulaFlip", "Content": "May be a dumb question but Iâ€™m in the process of planning out my build for a home server/NAS and was looking at PC cases that can hold a decent number of drives. Why do PC cases have slots for both 3.5inch and 2.5inch drive mounts? Do people generally use both kinds in their systems? Or is it more of a space thing? Thanks ðŸ˜…"},
{"Title": "Is this speed normal? I am running a Write test on HDSentinel on WD My Passport", "Author": "u/sammorrison9800", "Content": "No content"},
{"Title": "Family Archivists: Best storage for 321 backups for posterity?", "Author": "u/CharlesGoo20201210", "Content": "I'm organizing my family archives for several branches of my family tree. This is really just files and directories organized in a loose way such that different branches of the family can read or write to their \"sub-trees\". This is mainly photos, pdfs, and plain text or markdown with stories, history, and metadata. I'm primarily using Google drive (and groups and contacts) to manage all this. I realized I need to trust google less and create a 321-backup as in https://www.veeam.com/blog/321-backup-rule.html \"three copies, two media types, one off-site\".  Note that \"for two media types\" here means more than one provider service (i.e. not just google). Questions: Do you use a storage service that looks similar, with plain old: files, folders, users, groups, permissions (read, write, admin) that you're happy with?  That covers this requirement Do you have a good automated way to copy from google drive to create backups periodically?  That preserves security data (possibly versioning too). This could be to my own RAID/NAS at home. Is there any service in the cloud that backs up directly from google drive for you (for a price) that works well? Encryption: do you know of a seamless service that allows one to automatically add a layer of zero-trust security to particular folders on google drive (or elsewhere). E.g. Some documents are financial and sensitive enough that we'd like extra security, but it's a big pain to encrypt/decrypt these locally then upload/download to the shared area. Thanks for any insights!"},
{"Title": "A PWL solution that worked for me, and alternative drives", "Author": "u/flossy_cake", "Content": "I have the 12TB WD Red Plus drive.  It thumps every 5 seconds due to PWL.  It is annoying.  I was able to shut it up with this specific type of anti shock bracket which suspends the drive on rubber posts from all 4 sides.   You will need a 5.25\" bay in your PC case which are getting rarer these days. Available at the usual places like ebay, amazon, aliexpress etc. WARNING:  do not necessarily expect it to be a miracle solution for you straight away.  I have two 5.25\" bays and if I mount it in the top one I can still hear the thump.  If I use the bottom bay it is completely silent.   I don't know why.  I think it is something to do with resonation inside the case.   In the bottom silent bay I can put my ear right up to it and I hear no thump at all.  I can't explain it.  There is some trick to getting the drive suspended in just the right way that makes it not transfer any vibration.  I have read on some silent PC review site that people were suspending their drives in mid air inside the case with elastic bands.  Maybe it's like that.   When you are mounting the drive on the posts, inspect each post to make sure each one is making a perfect circular maximal contact patch on all 4 sides.  If one of the posts is bending slightly its contact patch will be smaller. It is easy to be slightly off and I think that affects how much vibration gets damped. Now for the alternatives:  I have two Western Digital 8TB blues (current model) which seem to not have PWL thump that I can tell.  Maybe they do, but I cannot hear it.   Or maybe it does it so rarely and quietly that I cannot tell it is happening.   But the 8TB blues have more platter spinning whoosh noise like \"hhhhhhhhhh\" so the 12TB red plus with silenced PWL is actually quieter. I will keep playing around with my top bay to get to the bottom of why it's still thumping despite the rubber mount.  Maybe one of the rubber posts went out of alignment or something.  I have it in the bottom bay right now and I can tell you there is no thump at all, none.  The drive has not gone to sleep. I was going to buy a second 12TB WD Red Plus but then I thought if they both thump every 5 seconds there is no guarantee both their thumps would be in sync so it could actually result in a thump every 2.5 seconds!   Imagine having four of them, it would be a thump every 1.25 seconds, oh my goodness!"},
{"Title": "(cross-post from HN) When would 1GB HDD have cost $100?", "Author": "u/stevecondy123", "Content": "Someone's trying to track down when a 1GB HDD would have cost $100. Figured this community may know. https://news.ycombinator.com/item?id=40446887"},
{"Title": "Expanding my Nas", "Author": "u/lgittens", "Content": "A couple years ago I miss judged my storage space needs. I should have bought a 8 bay nas but only purchased a four bay. I currently run plex off a ds418. 2x8tb and 2x18tb hybrid raid. I also have a 10tb currently not being used. Not sure what my next step should be Buy a two bay raid enclosure and a 10tb drive. This should last me a couple years. However I'm not sure what to get and if this may cause some lag while streeming. Buy 2x18tb drives or larger. Then I will have 3 drives not doing anything. This seems like a big waste until I upgrade my Nas which should be in a few years 2.Replace my Nas with a 8bay nas and buy another 10tb drive. This is not my favorite option as this will be the most expensive. My current Nas I would have to find a good home for. Somthing else to note is that my pc already has plenty of hd space. My goal is to use my pc for sorting and move all data to the Nas for storage. Any thoughts or suggestions would be appreciated."},
{"Title": "How to save a redgifs GIF", "Author": "u/Dragonkindren1", "Content": "Just change the watch in the url to ifr and then right click and save video"},
{"Title": "I have a horrible confession to make", "Author": "u/ByteArchivist", "Content": "I love deleting things. After I download an archive of data I love going through it and choosing what is interesting to me, or might be interesting, and what is not. Seeing the amount of free space on my drives go up as I delete things is such a fun feeling. If I ever must stand trial my only defense will be that everything I decide to keep gets saved forever in triplicate. But serial killers have their own twisted justifications for their heinous actions too. I understand this is a mortal sin here, so I will excuse myself now before being detained."},
{"Title": "How can I rip this walkthrough from this site?", "Author": "u/molitar", "Content": "https://www.yountl.xyz/biniku/walkthrough I have never seen anyone go through such a nightmare to stop someone from copying a walkthrough."},
{"Title": "How do I merge old backups?", "Author": "u/Teh_Original", "Content": "I have maintained backups of my old computers over the years, and they are on one of my current PC's hard drives. Sometimes I was using multiple computers at the same time and they were never synchronized (and therefore have different folder structures sometimes), but had some of the same data on each computer. I'd like to merge all of my backups together (probably with my current PC's file system) and eliminate the duplicates. What's the best methodology for doing this? Are there tools for this?"},
{"Title": "Data-hoarding for the struggling college student?", "Author": "u/BigTigerM", "Content": "Hi! My nameâ€™s Tiger, and Iâ€™m struggling to keep my data stored. Every external storage device Iâ€™ve purchased has eventually had to be sent to repair services and be promptly replaced within a two to four year span - Iâ€™m now on my third harddrive, and have decided that I need a change to my approach. I am a college student with tons of personal digital memorabilia, from movies to OSTs and historical documents. After a period of time, the drives Iâ€™ve bought have consistently taken ages to mount & unmount, face a myriad of widely-spanning technical problems, and then promptly refuse to work. This has led to the decision that they will never, ever leave my areas of residence for fear of some whimsically-inclined failure. So Iâ€™m down on my luck, and thought Iâ€™d ask yâ€™all for help since you folk seem dedicated enough to have some tips!! :D I suppose a list of what Iâ€™m generally looking for in a storage device would be helpful: Can be safely relocated, but incredibly rarely Can be accessed with immediacy (ie direct cables and not network shenanigans) Loads files at an incredibly accessible speed Preferably functions out of the box, some set-up is fine if it needs updates or a wizard, hesitant if it requires additional purchases Doesnâ€™t require a 24/7 connection to a computer Is not OS-dependant, can be formatted to be read by anything Offers 5TB of storage or more Thank yâ€™all so much for reading!! TwT If I sound douchey, do let me know."},
{"Title": "Question about usable space after raid5", "Author": "u/jchocolate99", "Content": "I'm trying to set up my new 6 bay NAS. I have four 20TB and two 4TB. When I set it up for raid 5 it says my usable space is only 20TB. Why wouldn't I have 64TB of useable space? I'm really new to all this and just trying to get an understanding"},
{"Title": "Warning: Internxt removing features and offer no refund on lifetime plans (cloud storage scam)", "Author": "u/hi-pi", "Content": "The automatic photo upload from your phone feature was the primary selling point when I chose internxt, plus what I thought was a low price for a lifetime 10TB. This feature is now removed entirely with no plan to return after I have only had the \"lifetime\" plan for 2 months. I understand things change, so I reached out to politely request a refund and they said it was out of the 30 day window for a refund. So I am 60 days into a \"lifetime\" subscription and they will not do even a partial refund, after they removed the most important feature they sold me. Amazing business plan, offer many wonderful features and then remove them once you have customers money. I would like to point out that the photo upload worked terribly and inconsistently to begin with, and I really should have canceled much sooner, but I wanted to give a new company the benefit of the doubt. Also they recently released webdav, but it requires running your own server, which kind of defeats the purpose of webdav. Practically every other service lets you upload many different ways and deal with your own encryption. I made a terrible mistake and am out $1000. I should have listened to people's warnings that Internxt was not good, and that it was a waste of money. I really was foolish and Internxt did an evil bait and switch. Do not give Internxt your money! pcloud, icecloud, koofr are all much more qualified companies"},
{"Title": "How susceptible are stored, unpowered hard drives to vibrations from a subwoofer?", "Author": "u/Gamba_Kufu_of_Huru", "Content": "So I have a little home theater with a 12\" subwoofer, and in the same room, around 3 meters away is where my hard drives are stored, in a drawer. For security reasons they need to be in this room so they can't simply be moved for now. I never play any music or movies or anything when they're plugged in and in use of course. I just want to know if they are safe from the vibrations when stored away. I listen at pretty reasonable volumes I think but ofc some movies can get very loud and bassy. They work and I have no issues or anything, it's just me being the big worrier I am am wondering if they somehow may be getting some subtle damage over time that may cause issues down the line or reduce the lifespan etc."},
{"Title": "People with really big music collections, how do you even load them in your music player?", "Author": "u/inhalingsounds", "Content": "I was thinking about this. If you have, say, 1M+ songs, even if they are mp3, how do they even load in any player? Even for foobar2000 which is very slim and optimized, wouldn't it take forever to load that amount of 10+ TB, process the metadata and have it in memory? How about players like MediaMonkey that also digest all metatags? Doesn't this volume just crash the program completely?"},
{"Title": "How to change drive enclosure but preserve the SoftRAID Volume?", "Author": "u/tealbull", "Content": "I have two U.2 SSDs in RAID 0 in my OWC Thunderbay Flex 8 using SoftRaid. I recently got an OWC U.2 Mercury Dual enclosure. I want to move these U.2 SSDs from the thunderbay to the mercury. But I want to ensure the RAID 0 volume and data remains intact. Since it is a SoftRaid volume, Iâ€™m guessing the info will carry over when I change enclosures. What are some precautions I should take? Any steps I should follow? I will definitely keep a back up of the data on the RAID 0 volume"},
{"Title": "Is getting a 4 bay NAS, using 2 slots, and then filling the 2 when I need more space possible?", "Author": "u/_cant_talk", "Content": "To save money, Iâ€™ll only buy drives for 2 slots, use raid 1, and when I need more space Iâ€™ll add more drives and use raid 5 Is that possible or will changing raid configurations screw everything up? I donâ€™t need a ton of storage right now, maybe 10TB, but I know Iâ€™ll need 40tb in the next 2 years or so"},
{"Title": "Dedicated network transfer tool?", "Author": "u/Darwinmate", "Content": "Several times a week machine(s) generate between 20 to 500gig of data that needs to be transferred to a large NAS. Currently I have an rsync script that runs continuosly monitoring for changes looped in a bash script with --partial to pick up where it left off. However this is more of a hack. rsync is more a syrnconisation tool than a dedicated network transfer tool. So while it works great, it has issues resulting from frequent dropouts. It works okay but I'm after something more robust that can withtstand network dropouts. To complicate matters, the NAS is running (ReFS)[ https://en.wikipedia.org/wiki/ReFS] , the machines which generate the data run window10, others run weird Centos DE and finally the last one is ubuntu. All transfer to the ReFS. Ideally the tool would work on Windows/Linux/MacOS. I have only one contender: https://github.com/fast-data-transfer/fdt Speed is not priority but stability and tracbility is. Any recommendations?"},
{"Title": "Any archive format that is particularly faster for searching/selective extraction?", "Author": "u/evolution2015", "Content": "Is there an archive format that is faster than ZIP for selectively extracting a few files or search the archive for file names when the archive is large (100GB+) and contains tens of thousands of files?"},
{"Title": "It's been nine years since this post.  What do people think of this idea now?", "Author": "u/Canttalkwhatsapponly", "Content": "Ex: If you buy a 64GB iPhone, you should get 64GB of space on iCloud for free. This would encourage people to buy higher capacity phones meaning Apple will make more profit which they can spend on cloud maintenance and expansion etc.. Its a win-win for everyone"},
{"Title": "Help needed for Datahoarding via NAS", "Author": "u/i_xm_nxsh", "Content": "Hey everyone! I did my research and narrowed it down to Synology 923+ to start my data hoarding journey however I recently came across a PC built to use as NAS. Now, because of this, I am a bit confused as to which one should I go for since the PC helps me save a lot of money. However, I am not that knowledgeable when it comes to using NAS via TrueNAS, maintaining my NAS-related PC setup etc. I have used a Windows PC Tower (with monitor) in the past but I only used it for 7 years (without any major issues) and changed to a laptop. It will be really helpful to know if the PC setup given below is a good choice for its price, good for my use case and if it will consume too much power as a NAS or not. This person is looking for $240 (probably can give the system to me for $220). Below are the system specs with my comments in (brackets): CPU: AMD Phenom II X6 1055T 6 Core Processor Memory: 16GB (4x4GB) DDR3 (brand not mentioned) HBA: LSI SAS[hidden information]i Flashed to IT Mode and acted as a HBA (no idea what this is) Supports 8x SATA/SAS 6gb/s Drives. Two SFF_8087 Breakout cables are included. Motherboard: Gigabyte GA-890GPA-UD3H with Tower CPU Cooler Case: Fractal Design Define R3 with 8 bays for HDDs Power Supply: Seasonic 550W PSU Thank you so much for reading this far and if you want to read further to know my use cases then here it is: My wife is a beginner in photography. She started 2 years back and now has over ~400 GB of photos. I know it's not much but we know that she will need more space soon. I have been using Plex (with Plex Pass) through an old laptop which has 2 TB HDD+SSD combined. Sadly, I have to delete videos to make space for new ones. I am planning on getting 24TB (8x3) drives to tackle both the use cases mentioned above. Thanks in advance for your responses!"},
{"Title": "LTO Tape Capacity - Do tapes lose capacity with use and age?", "Author": "u/fgt67cam", "Content": "I recently purchased a used LTO-5 tape drive and some used tapes. LTO-5 has a rated capacity of 1.5TB (1.36TiB). When I've been writing data to these tapes (uncompressed) and they seem to cap at 1300 GiB which seems to be ~90GiB short. I write the tapes using the command: tar cvf - /files-to-write 2> >(tee /Tape.log >&2) | mbuffer -m 14G -L -P 80 > /dev/st0 I recorded how much data was written to each different tape before I got the \"tar: Exiting with failure status due to previous errors\" message which usually means it has reached the end of the tape and can no longer write anything. Tape 1: 1391776710636 bytes (1296 GiB) Tape 2: 1424535198256 bytes (1326 GiB) Tape 3: 1334993518029 bytes (1243 GiB) Tape 4: 1383193282217 bytes (1292 GiB) The tape drive I'm using is heavily used as well as the tapes. This is the sg_log for one of them: HP        Ultrium 5-SCSI    I6CZ Supported log pages  [0x0]: 0x00        Supported log pages [sp] 0x02        Write error [we] 0x03        Read error [re] 0x0c        Sequential access device [sad] 0x0d        Temperature [temp] 0x11        DT Device status [dtds] 0x12        Tape alert response [tar] 0x13        Requested recovery [rr] 0x14        Device statistics [ds] 0x15        Service buffers information [sbi] 0x16        Tape diagnostic data [tdd] 0x17        Volume statistics [vs] 0x18        Protocol specific port [psp] 0x1b        Data compression [dc] 0x2e        Tape alert [ta] 0x30        Tape usage (lto-5, 6) [tu_] 0x31        Tape capacity (lto-5, 6) [tc_] 0x32        Data compression (lto-5) [dc_] 0x34        Read forward errors (lto-5) [rfe_] 0x35        DT Device Error (lto-5, 6) [dtde_] 0x3e        Device Status (lto-5, 6) [ds_] Write error counter page  [0x2] Errors corrected without substantial delay = 0 Errors corrected with possible delays = 0 Total rewrites or rereads = 0 Total errors corrected = 0 Total times correction algorithm processed = 0 Total bytes processed = 0 Total uncorrected errors = 0 Read error counter page  [0x3] Errors corrected without substantial delay = 0 Errors corrected with possible delays = 0 Total rewrites or rereads = 0 Total errors corrected = 0 Total times correction algorithm processed = 0 Total bytes processed = 0 Total uncorrected errors = 0 Sequential access device page (ssc-3) Data bytes received with WRITE commands: 0 GB Data bytes written to media by WRITE commands: 0 GB Data bytes read from media by READ commands: 0 GB Data bytes transferred by READ commands: 0 GB Native capacity from BOP to EOD: 1527775 MB Native capacity from BOP to EW of current partition: 1517690 MB Minimum native capacity from EW to EOP of current partition: 12239 MB Native capacity from BOP to current position: 17654 MB Maximum native capacity in device object buffer: 206 MB Cleaning action not required (or completed) Temperature page  [0xd] Current temperature = 41 C Reference temperature = <not available> DT device status page (ssc-3, adc-3) [0x11] Very high frequency data: PAMR=0 HUI=0 MACC=1 CMPR=1 WRTP=0 CRQST=0 CRQRD=0 DINIT=1 INXTN=0 RAA=0 MPRSNT=1 MSTD=1 MTHRD=1 MOUNTED=1 DT device activity: No DT device activity VS=0 TDDEC=0 EPP=0 ESR=0 RRQST=0 INTFC=0 TAFC=0 Very high frequency polling delay:  16 milliseconds DT device ADC data encryption control status (hex only now): 00     00 00 00 00 00 00 00 00 Key management error data (hex only now): 00     00 00 00 00 00 00 00 00  00 00 00 00 Primary port 1 status: non-SAS transport, in hex: 00     00 00 00 00 00 00 00 00  50 01 10 a0 01 4a a5 9c    ........P....J.. 10     50 01 10 a0 01 4a a5 9e                             P....J.. Primary port 2 status: non-SAS transport, in hex: 00     3b 00 00 01 00 00 00 01  50 01 10 a0 01 4a a5 9d    ;.......P....J.. 10     50 01 10 a0 01 4a a5 9e                             P....J.. Primary port 3 status: non-SAS transport, in hex: 00     00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00    ................ 10     00 00 00 00 00 00 00 00                             ........ Primary port 4 status: non-SAS transport, in hex: 00     02 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00    ................ 10     00 00 00 00 00 00 00 00                             ........ Vendor specific [parameter_code=0x8000]: 00     80 00 43 06 31 17 00 00  00 02                      ..C.1..... Vendor specific [parameter_code=0x8003]: 00     80 03 43 08 00 00 00 00  00 00 00 00                ..C......... Vendor specific [parameter_code=0x8010]: 00     80 10 43 08 00 00 00 00  00 00 00 00                ..C......... Vendor specific [parameter_code=0x8020]: 00     80 20 43 12 00 00 00 00  00 00 00 00 00 00 00 00    . C............. 10     00 00 00 00 00 00                                   ...... Vendor specific [parameter_code=0xa101]: 00     a1 01 43 04 00 00 00 00                             ..C..... Vendor specific [parameter_code=0xa102]: 00     a1 02 43 04 00 00 00 00                             ..C..... TapeAlert response page (ssc-3, adc-3) [0x12] Flag01h: 0  02h: 0  03h: 0  04h: 0  05h: 0  06h: 0  07h: 0  08h: 0 Flag09h: 0  0Ah: 0  0Bh: 0  0Ch: 0  0Dh: 0  0Eh: 0  0Fh: 0  10h: 0 Flag11h: 0  12h: 0  13h: 0  14h: 0  15h: 0  16h: 0  17h: 0  18h: 0 Flag19h: 0  1Ah: 0  1Bh: 0  1Ch: 0  1Dh: 0  1Eh: 0  1Fh: 0  20h: 0 Flag21h: 0  22h: 0  23h: 0  24h: 0  25h: 0  26h: 0  27h: 0  28h: 0 Flag29h: 0  2Ah: 0  2Bh: 0  2Ch: 0  2Dh: 0  2Eh: 0  2Fh: 0  30h: 0 Flag31h: 0  32h: 0  33h: 0  34h: 0  35h: 0  36h: 0  37h: 0  38h: 0 Flag39h: 0  3Ah: 0  3Bh: 0  3Ch: 0  3Dh: 0  3Eh: 0  3Fh: 0  40h: 0 Requested recovery page (ssc-3) [0x13] Recovery procedures: Recovery not requested Device statistics page (ssc-3 and adc) Lifetime media loads: 5288 Lifetime cleaning operations: 195 Lifetime power on hours: 48480 Lifetime media motion (head) hours: 12747 Lifetime metres of tape processed: 187049835 Lifetime media motion (head) hours when incompatible media last loaded: 0 Lifetime power on hours when last temperature condition occurred: 0 Lifetime power on hours when last power consumption condition occurred: 0 Media motion (head) hours since last successful cleaning operation: 66 Media motion (head) hours since 2nd to last successful cleaning: 126 Media motion (head) hours since 3rd to last successful cleaning: 187 Lifetime power on hours when last operator initiated forced reset and/or emergency eject occurred: 20048 Lifetime power cycles: 77 Volume loads since last parameter reset: 5288 Hard write errors: 2 Hard read errors: 0 Duty cycle sample time (ms): 27276000 Read duty cycle: 0 Write duty cycle: 0 Activity duty cycle: 76 Volume not present duty cycle: 15 Drive manufacturer's serial number: 0 Drive serial number: 0 Medium removal prevented: 0 Maximum recommended mechanism temperature exceeded: 0 Media motion (head) hours for each medium type: Density code: 0x44, Medium type: 0x0 Medium motion hours: 0 Density code: 0x44, Medium type: 0x1 Medium motion hours: 0 Density code: 0x46, Medium type: 0x0 Medium motion hours: 0 Density code: 0x46, Medium type: 0x1 Medium motion hours: 0 Density code: 0x58, Medium type: 0x0 Medium motion hours: 12747 Density code: 0x58, Medium type: 0x1 Medium motion hours: 0 Service buffer information page (adc-3) [0x15] Service buffer identifier: 0x0 Buffer id: 0x41, tu=0, nmp=0, nmm=0, offline=0 pd=0, code_set: Binary, Service buffer title: DT Device Error Log Tape diagnostics data page (ssc-3) [0x16] Parameter code: 0 Density code: 0x58 Medium type: 0x0 Lifetime media motion hours: 12739 Repeat: 0 Sense key: 0x3 [Medium Error] Additional sense code: 0x0 Additional sense code qualifier: 0x2 [Additional sense: End-of-partition/medium detected] Vendor specific code qualifier: 0x5098 Product revision level: 1228292954 Hours since last clean: 58 Operation code: 0x1d Service action: 0x0 Medium id number (in hex): 00     49 43 32 47 4f 42 6e 36  38 30 00 00 00 00 00 00    IC2GOBn680...... 10     00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00    ................ Timestamp origin: 0x0 Timestamp: 00     00 00 00 09 6f 65 Parameter code: 1 Density code: 0x58 Medium type: 0x0 Lifetime media motion hours: 11121 Repeat: 0 Sense key: 0x3 [Medium Error] Additional sense code: 0xc Additional sense code qualifier: 0x0 [Additional sense: Write error] Vendor specific code qualifier: 0x5083 Product revision level: 1228292954 Hours since last clean: 67 Operation code: 0x0 Service action: 0x0 Medium id number (in hex): 00     41 44 37 48 52 56 4e 55  58 4e 00 00 00 00 00 00    AD7HRVNUXN...... 10     00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00    ................ Timestamp origin: 0x0 Timestamp: 00     00 00 fb 62 2c 3e Parameter code: 2 Density code: 0x58 Medium type: 0x0 Lifetime media motion hours: 1261 Repeat: 0 Sense key: 0x3 [Medium Error] Additional sense code: 0x14 Additional sense code qualifier: 0x0 [Additional sense: Recorded entity not found] Vendor specific code qualifier: 0x5090 Product revision level: 1228290394 Hours since last clean: 167 Operation code: 0x11 Service action: 0x0 Medium id number (in hex): 00     41 44 37 48 52 56 4e 55  32 4d 00 00 00 00 00 00    AD7HRVNU2M...... 10     00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00    ................ Timestamp origin: 0x0 Timestamp: 00     00 00 00 c9 22 70 Volume statistics page (ssc-4), subpage=0 Page valid: 1 Thread count: 162 Total data sets written: 2916336 Total write retries: 20 Total unrecovered write errors: 0 Total suspended writes: 11 Total fatal suspended writes: 0 Total data sets read: 1579246 Total read retries: 154 Total unrecovered read errors: 0 Last mount unrecovered write errors: 0 Last mount unrecovered read errors: 0 Last mount megabytes written: 0 Last mount megabytes read: 0 Lifetime megabytes written: 7209299 Lifetime megabytes read: 3903959 Last load write compression ratio: 0 Last load read compression ratio: 0 Medium mount time: 136669 Medium ready time: 136669 Total native capacity [MB]: 1529930 Total used native capacity [MB]: 1527775 Volume serial number: E111229096 Tape lot identifier: H1080121 Volume barcode: SN3018L5 Volume manufacturer: SONY Volume license code: U109 Volume personality: Ultrium-5 Write protect: 0 WORM: 0 Maximum recommended tape path temperature exceeded: 0 Beginning of medium passes: 1231 Middle of medium passes: 929 Logical position of first encrypted logical object: partition number: 0, partition record data counter: 0xffffffffffff Logical position of first unencrypted logical object after first encrypted logical object: partition number: 0, partition record data counter: 0xffffffffffff Native capacity partition(s) [MB]: partition number: 0, partition record data counter: 1529930 Used native capacity partition(s) [MB]: partition number: 0, partition record data counter: 1527775 Vendor specific parameter code (0xf000), payload in hex 00     00 01                                               .. Data compression page  (ssc-4) [0x1b] Read compression ratio x100: 0 Write compression ratio x100: 0 Megabytes transferred to server: 0 Bytes transferred to server: 0 Megabytes read from tape: 0 Bytes read from tape: 0 Megabytes transferred from server: 0 Bytes transferred from server: 0 Megabytes written to tape: 0 Bytes written to tape: 0 Data compression enabled: 0x1 Tape alert page (ssc-3) [0x2e] Read warning: 0 Write warning: 0 Hard error: 0 Media: 0 Read failure: 0 Write failure: 0 Media life: 0 Not data grade: 0 Write protect: 0 No removal: 0 Cleaning media: 0 Unsupported format: 0 Recoverable mechanical cartridge failure: 0 Unrecoverable mechanical cartridge failure: 0 Memory chip in cartridge failure: 0 Forced eject: 0 Read only format: 0 Tape directory corrupted on load: 0 Nearing media life: 0 Cleaning required: 0 Cleaning requested: 0 Expired cleaning media: 0 Invalid cleaning tape: 0 Retension requested: 0 Dual port interface error: 0 Cooling fan failing: 0 Power supply failure: 0 Power consumption: 0 Drive maintenance: 0 Hardware A: 0 Hardware B: 0 Interface: 0 Eject media: 0 Microcode update fail: 0 Drive humidity: 0 Drive temperature: 0 Drive voltage: 0 Predictive failure: 0 Diagnostics required: 0 Obsolete (28h): 0 Obsolete (29h): 0 Obsolete (2Ah): 0 Obsolete (2Bh): 0 Obsolete (2Ch): 0 Obsolete (2Dh): 0 Obsolete (2Eh): 0 Reserved (2Fh): 0 Reserved (30h): 0 Reserved (31h): 0 Lost statistics: 0 Tape directory invalid at unload: 0 Tape system area write failure: 0 Tape system area read failure: 0 No start of data: 0 Loading failure: 0 Unrecoverable unload failure: 0 Automation interface failure: 0 Firmware failure: 0 WORM medium - integrity check failed: 0 WORM medium - overwrite attempted: 0 Reserved parameter code 0x3d, flag: 0 Reserved parameter code 0x3e, flag: 0 Reserved parameter code 0x3f, flag: 0 Reserved parameter code 0x40, flag: 0 Tape usage page  (LTO-5 and LTO-6 specific) [0x30] Thread count: 162 Total data sets written: 2916336 Total write retries: 20 Total unrecovered write errors: 0 Total suspended writes: 11 Total fatal suspended writes: 0 Total data sets read: 1579246 Total read retries: 154 Total unrecovered read errors: 0 Tape capacity page  (LTO-5 and LTO-6 specific) [0x31] Main partition remaining capacity (in MiB): 2056 Alternate partition remaining capacity (in MiB): 0 Main partition maximum capacity (in MiB): 1459056 Alternate partition maximum capacity (in MiB): 0 Data compression page  (LTO-5 specific) [0x32] Read compression ratio x100: 0 Write compression ratio x100: 0 Megabytes transferred to server: 0 Bytes transferred to server: 0 Megabytes read from tape: 0 Bytes read from tape: 0 Megabytes transferred from server: 0 Bytes transferred from server: 0 Megabytes written to tape: 0 Bytes written to tape: 0 Unable to decode page = 0x34, here is hex: 00     34 00 00 1e 00 00 60 02  00 00 00 01 60 02 00 00 10     00 02 60 02 00 00 00 03  60 02 00 00 00 04 40 02 20     05 78 Unable to decode page = 0x35, here is hex: 00     35 00 00 74 00 00 43 16  00 00 00 00 00 00 00 00 10     00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 01 20     43 56 00 00 00 00 00 00  00 00 00 00 00 00 00 00 30     00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00 40     00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00 50     00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00 60     00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00 70     00 00 00 00 00 00 00 00 Unable to decode page = 0x3e, here is hex: 00     3e 00 00 28 00 00 40 04  00 00 00 00 00 01 40 04 10     00 05 01 00 00 02 60 04  00 00 14 a8 00 03 40 04 20     ff ff ff ff 00 04 40 04  01 09 02 23 The tapes write speed is sporadic throughout with writes speeds ranging from 140/MBs all the way down to 40/MBs with the mbuffer 100% remaining full so it's not being starved for data. I don't have any brand new tapes to use or a second tape drive so is it normal for used tapes to lose a lot of capacity and I will just have to account for this with a safer space margin?"},
{"Title": "Loudest HDD available?", "Author": "u/Western_Bass_1491", "Content": "I am giving a iMac a massive storage boost, however all the HDDâ€™s I have are decently quiet. I am looking for one to give it the classic HDD sound, and that actually still exists and isnâ€™t impossible to find. I donâ€™t care about speed or rpm, just the sound and one that wonâ€™t make me broke lol"},
{"Title": "New Hard drives not detecting (Not usual issues)", "Author": "u/may9899999", "Content": "I have a PCIE to sata card that supports 8 drives. I had 5 installed on the card that all worked fine. I bought 3 new drives but they aren't being detected. I've tried changed cables with no effect. I've tried adding one at a time, nothing. I'm only seeing the original 5 no matter what I do. I thought maybe unplugging and old one and trying a new one could potentially let me know if I had too many, but then only the 4 drives showed up. They don't show up with fdisk -l, the don't show up in disks or gparted. I tried a USB enclosure to initialize them to see if that would help, and they did initialize, but once connected back to SATA, they don't show up. I'm just really at a loss of what's going on at this point. Below is the link to my previous post that I made trying to fix this issue. OS is Ubuntu 22.04 https://www.reddit.com/r/Ubuntu/comments/1cz2jlb/new_hard_drive_not_showing_up/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button Edit Issue solved below, TLDR: I needed an adapter power cable (which came in the box but looked like just an extension)"},
{"Title": "Anyone experimented with p2p storage for backup purposes?", "Author": "u/The_B0rg", "Content": "I'm mostly talking about IPFS and systems built on top of it like filecoin and hivenet, both commercial solutions that provide file storage with guarantees over IPFS. You can pay in money but lower your costs by providing storage in return in a P2P fashion. I truly believe these kinds of services will only get better and are a big part of the future of the internet and data storage. The question is if anyone already has any experience with them and if they are already good enough or not?"},
{"Title": "WD BLACK Hard Drive Failure", "Author": "u/Majestic-Owl-5801", "Content": "I was transferring files from one hard drive to another and had to end the process, but now I cant open up the hard drive I was transferring files from. I am sure I corrupted something by ending the process, but it ended on one of the windows that asked to clear space to continue. So it was on a single file rather than in the middle of a file. I clicked skip for all remaining and it froze, it was quite a lot of files. I closed the window because it was frozen, and now it wont show me the files on the drive. It will load, show me the remaining space taken up, but wont load any files when I actually open the drive in explorer."},
{"Title": "Best practices with Terracopy? Verify and checksum?", "Author": "u/Bern_Down_the_DNC", "Content": "First time using Teracopy. I was recommended to use this since I can check \"verify files\" and it will do that after copying files on drive 1 and syncing to drive 2. Nobody said anything about checksum files. Not sure if it would be beneficial. I will rename files or add more files to the directory later on drive 1, then sync all the files to drive 2 again. If I need to do checksum, would I create a checksum for folders on drive 1, then start copy and sync to drive 2 but select \"do checksum at the end\" so it generates a second checksum on drive 2 that I can compare to the checksum from drive 1? Also do I need to choose \"store/verify checksum on ADS\"? (And should I use xx3-64 instead of MD5 now?) Thank you!"},
{"Title": "I want to expand my zpool in both size and efficiency, could you please help me?", "Author": "u/mathscasual", "Content": "Right  now I have two 16 TB hdds mirrored in raid 1. I want to change my raid configuration to increase speed and size. Give me an excuse to buy 6 more drives(or more) so I have both increased size and speed. What raid configuration? How many more drives?(can they be larger than 16Tb(was looking at 22TBs) cacching? Many pleases and thank yous"},
{"Title": "A PWL solution that worked for me, and alternative drives", "Author": "u/flossy_cake", "Content": "I have the 12TB WD Red Plus drive.  It thumps every 5 seconds due to PWL.  It is annoying.  I was able to shut it up with this specific type of anti shock bracket which suspends the drive on rubber posts from all 4 sides.   You will need a 5.25\" bay in your PC case which are getting rarer these days. Available at the usual places like ebay, amazon, aliexpress etc. WARNING:  do not necessarily expect it to be a miracle solution for you straight away.  I have two 5.25\" bays and if I mount it in the top one I can still hear the thump.  If I use the bottom bay it is completely silent.   I don't know why.  I think it is something to do with resonation inside the case.   In the bottom silent bay I can put my ear right up to it and I hear no thump at all.  I can't explain it.  There is some trick to getting the drive suspended in just the right way that makes it not transfer any vibration.  I have read on some silent PC review site that people were suspending their drives in mid air inside the case with elastic bands.  Maybe it's like that.   When you are mounting the drive on the posts, inspect each post to make sure each one is making a perfect circular maximal contact patch on all 4 sides.  If one of the posts is bending slightly its contact patch will be smaller. It is easy to be slightly off and I think that affects how much vibration gets damped. Now for the alternatives:  I have two Western Digital 8TB blues (current model) which seem to not have PWL thump that I can tell.  Maybe they do, but I cannot hear it.   Or maybe it does it so rarely and quietly that I cannot tell it is happening.   But the 8TB blues have more platter spinning whoosh noise like \"hhhhhhhhhh\" so the 12TB red plus with silenced PWL is actually quieter. I will keep playing around with my top bay to get to the bottom of why it's still thumping despite the rubber mount.  Maybe one of the rubber posts went out of alignment or something.  I have it in the bottom bay right now and I can tell you there is no thump at all, none.  The drive has not gone to sleep. I was going to buy a second 12TB WD Red Plus but then I thought if they both thump every 5 seconds there is no guarantee both their thumps would be in sync so it could actually result in a thump every 2.5 seconds!   Imagine having four of them, it would be a thump every 1.25 seconds, oh my goodness!"},
{"Title": "(cross-post from HN) When would 1GB HDD have cost $100?", "Author": "u/stevecondy123", "Content": "Someone's trying to track down when a 1GB HDD would have cost $100. Figured this community may know. https://news.ycombinator.com/item?id=40446887"},
{"Title": "Expanding my Nas", "Author": "u/lgittens", "Content": "A couple years ago I miss judged my storage space needs. I should have bought a 8 bay nas but only purchased a four bay. I currently run plex off a ds418. 2x8tb and 2x18tb hybrid raid. I also have a 10tb currently not being used. Not sure what my next step should be Buy a two bay raid enclosure and a 10tb drive. This should last me a couple years. However I'm not sure what to get and if this may cause some lag while streeming. Buy 2x18tb drives or larger. Then I will have 3 drives not doing anything. This seems like a big waste until I upgrade my Nas which should be in a few years 2.Replace my Nas with a 8bay nas and buy another 10tb drive. This is not my favorite option as this will be the most expensive. My current Nas I would have to find a good home for. Somthing else to note is that my pc already has plenty of hd space. My goal is to use my pc for sorting and move all data to the Nas for storage. Any thoughts or suggestions would be appreciated."},
{"Title": "How to save a redgifs GIF", "Author": "u/Dragonkindren1", "Content": "Just change the watch in the url to ifr and then right click and save video"},
{"Title": "I have a horrible confession to make", "Author": "u/ByteArchivist", "Content": "I love deleting things. After I download an archive of data I love going through it and choosing what is interesting to me, or might be interesting, and what is not. Seeing the amount of free space on my drives go up as I delete things is such a fun feeling. If I ever must stand trial my only defense will be that everything I decide to keep gets saved forever in triplicate. But serial killers have their own twisted justifications for their heinous actions too. I understand this is a mortal sin here, so I will excuse myself now before being detained."},
{"Title": "How can I rip this walkthrough from this site?", "Author": "u/molitar", "Content": "https://www.yountl.xyz/biniku/walkthrough I have never seen anyone go through such a nightmare to stop someone from copying a walkthrough."},
{"Title": "How do I merge old backups?", "Author": "u/Teh_Original", "Content": "I have maintained backups of my old computers over the years, and they are on one of my current PC's hard drives. Sometimes I was using multiple computers at the same time and they were never synchronized (and therefore have different folder structures sometimes), but had some of the same data on each computer. I'd like to merge all of my backups together (probably with my current PC's file system) and eliminate the duplicates. What's the best methodology for doing this? Are there tools for this?"},
{"Title": "Data-hoarding for the struggling college student?", "Author": "u/BigTigerM", "Content": "Hi! My nameâ€™s Tiger, and Iâ€™m struggling to keep my data stored. Every external storage device Iâ€™ve purchased has eventually had to be sent to repair services and be promptly replaced within a two to four year span - Iâ€™m now on my third harddrive, and have decided that I need a change to my approach. I am a college student with tons of personal digital memorabilia, from movies to OSTs and historical documents. After a period of time, the drives Iâ€™ve bought have consistently taken ages to mount & unmount, face a myriad of widely-spanning technical problems, and then promptly refuse to work. This has led to the decision that they will never, ever leave my areas of residence for fear of some whimsically-inclined failure. So Iâ€™m down on my luck, and thought Iâ€™d ask yâ€™all for help since you folk seem dedicated enough to have some tips!! :D I suppose a list of what Iâ€™m generally looking for in a storage device would be helpful: Can be safely relocated, but incredibly rarely Can be accessed with immediacy (ie direct cables and not network shenanigans) Loads files at an incredibly accessible speed Preferably functions out of the box, some set-up is fine if it needs updates or a wizard, hesitant if it requires additional purchases Doesnâ€™t require a 24/7 connection to a computer Is not OS-dependant, can be formatted to be read by anything Offers 5TB of storage or more Thank yâ€™all so much for reading!! TwT If I sound douchey, do let me know."},
{"Title": "Question about usable space after raid5", "Author": "u/jchocolate99", "Content": "I'm trying to set up my new 6 bay NAS. I have four 20TB and two 4TB. When I set it up for raid 5 it says my usable space is only 20TB. Why wouldn't I have 64TB of useable space? I'm really new to all this and just trying to get an understanding"},
{"Title": "Warning: Internxt removing features and offer no refund on lifetime plans (cloud storage scam)", "Author": "u/hi-pi", "Content": "The automatic photo upload from your phone feature was the primary selling point when I chose internxt, plus what I thought was a low price for a lifetime 10TB. This feature is now removed entirely with no plan to return after I have only had the \"lifetime\" plan for 2 months. I understand things change, so I reached out to politely request a refund and they said it was out of the 30 day window for a refund. So I am 60 days into a \"lifetime\" subscription and they will not do even a partial refund, after they removed the most important feature they sold me. Amazing business plan, offer many wonderful features and then remove them once you have customers money. I would like to point out that the photo upload worked terribly and inconsistently to begin with, and I really should have canceled much sooner, but I wanted to give a new company the benefit of the doubt. Also they recently released webdav, but it requires running your own server, which kind of defeats the purpose of webdav. Practically every other service lets you upload many different ways and deal with your own encryption. I made a terrible mistake and am out $1000. I should have listened to people's warnings that Internxt was not good, and that it was a waste of money. I really was foolish and Internxt did an evil bait and switch. Do not give Internxt your money! pcloud, icecloud, koofr are all much more qualified companies"},
{"Title": "How susceptible are stored, unpowered hard drives to vibrations from a subwoofer?", "Author": "u/Gamba_Kufu_of_Huru", "Content": "So I have a little home theater with a 12\" subwoofer, and in the same room, around 3 meters away is where my hard drives are stored, in a drawer. For security reasons they need to be in this room so they can't simply be moved for now. I never play any music or movies or anything when they're plugged in and in use of course. I just want to know if they are safe from the vibrations when stored away. I listen at pretty reasonable volumes I think but ofc some movies can get very loud and bassy. They work and I have no issues or anything, it's just me being the big worrier I am am wondering if they somehow may be getting some subtle damage over time that may cause issues down the line or reduce the lifespan etc."},
{"Title": "People with really big music collections, how do you even load them in your music player?", "Author": "u/inhalingsounds", "Content": "I was thinking about this. If you have, say, 1M+ songs, even if they are mp3, how do they even load in any player? Even for foobar2000 which is very slim and optimized, wouldn't it take forever to load that amount of 10+ TB, process the metadata and have it in memory? How about players like MediaMonkey that also digest all metatags? Doesn't this volume just crash the program completely?"},
{"Title": "How to change drive enclosure but preserve the SoftRAID Volume?", "Author": "u/tealbull", "Content": "I have two U.2 SSDs in RAID 0 in my OWC Thunderbay Flex 8 using SoftRaid. I recently got an OWC U.2 Mercury Dual enclosure. I want to move these U.2 SSDs from the thunderbay to the mercury. But I want to ensure the RAID 0 volume and data remains intact. Since it is a SoftRaid volume, Iâ€™m guessing the info will carry over when I change enclosures. What are some precautions I should take? Any steps I should follow? I will definitely keep a back up of the data on the RAID 0 volume"},
{"Title": "Is getting a 4 bay NAS, using 2 slots, and then filling the 2 when I need more space possible?", "Author": "u/_cant_talk", "Content": "To save money, Iâ€™ll only buy drives for 2 slots, use raid 1, and when I need more space Iâ€™ll add more drives and use raid 5 Is that possible or will changing raid configurations screw everything up? I donâ€™t need a ton of storage right now, maybe 10TB, but I know Iâ€™ll need 40tb in the next 2 years or so"},
{"Title": "Dedicated network transfer tool?", "Author": "u/Darwinmate", "Content": "Several times a week machine(s) generate between 20 to 500gig of data that needs to be transferred to a large NAS. Currently I have an rsync script that runs continuosly monitoring for changes looped in a bash script with --partial to pick up where it left off. However this is more of a hack. rsync is more a syrnconisation tool than a dedicated network transfer tool. So while it works great, it has issues resulting from frequent dropouts. It works okay but I'm after something more robust that can withtstand network dropouts. To complicate matters, the NAS is running (ReFS)[ https://en.wikipedia.org/wiki/ReFS] , the machines which generate the data run window10, others run weird Centos DE and finally the last one is ubuntu. All transfer to the ReFS. Ideally the tool would work on Windows/Linux/MacOS. I have only one contender: https://github.com/fast-data-transfer/fdt Speed is not priority but stability and tracbility is. Any recommendations?"},
{"Title": "Any archive format that is particularly faster for searching/selective extraction?", "Author": "u/evolution2015", "Content": "Is there an archive format that is faster than ZIP for selectively extracting a few files or search the archive for file names when the archive is large (100GB+) and contains tens of thousands of files?"},
{"Title": "It's been nine years since this post.  What do people think of this idea now?", "Author": "u/Canttalkwhatsapponly", "Content": "Ex: If you buy a 64GB iPhone, you should get 64GB of space on iCloud for free. This would encourage people to buy higher capacity phones meaning Apple will make more profit which they can spend on cloud maintenance and expansion etc.. Its a win-win for everyone"},
{"Title": "Help needed for Datahoarding via NAS", "Author": "u/i_xm_nxsh", "Content": "Hey everyone! I did my research and narrowed it down to Synology 923+ to start my data hoarding journey however I recently came across a PC built to use as NAS. Now, because of this, I am a bit confused as to which one should I go for since the PC helps me save a lot of money. However, I am not that knowledgeable when it comes to using NAS via TrueNAS, maintaining my NAS-related PC setup etc. I have used a Windows PC Tower (with monitor) in the past but I only used it for 7 years (without any major issues) and changed to a laptop. It will be really helpful to know if the PC setup given below is a good choice for its price, good for my use case and if it will consume too much power as a NAS or not. This person is looking for $240 (probably can give the system to me for $220). Below are the system specs with my comments in (brackets): CPU: AMD Phenom II X6 1055T 6 Core Processor Memory: 16GB (4x4GB) DDR3 (brand not mentioned) HBA: LSI SAS[hidden information]i Flashed to IT Mode and acted as a HBA (no idea what this is) Supports 8x SATA/SAS 6gb/s Drives. Two SFF_8087 Breakout cables are included. Motherboard: Gigabyte GA-890GPA-UD3H with Tower CPU Cooler Case: Fractal Design Define R3 with 8 bays for HDDs Power Supply: Seasonic 550W PSU Thank you so much for reading this far and if you want to read further to know my use cases then here it is: My wife is a beginner in photography. She started 2 years back and now has over ~400 GB of photos. I know it's not much but we know that she will need more space soon. I have been using Plex (with Plex Pass) through an old laptop which has 2 TB HDD+SSD combined. Sadly, I have to delete videos to make space for new ones. I am planning on getting 24TB (8x3) drives to tackle both the use cases mentioned above. Thanks in advance for your responses!"},
{"Title": "LTO Tape Capacity - Do tapes lose capacity with use and age?", "Author": "u/fgt67cam", "Content": "I recently purchased a used LTO-5 tape drive and some used tapes. LTO-5 has a rated capacity of 1.5TB (1.36TiB). When I've been writing data to these tapes (uncompressed) and they seem to cap at 1300 GiB which seems to be ~90GiB short. I write the tapes using the command: tar cvf - /files-to-write 2> >(tee /Tape.log >&2) | mbuffer -m 14G -L -P 80 > /dev/st0 I recorded how much data was written to each different tape before I got the \"tar: Exiting with failure status due to previous errors\" message which usually means it has reached the end of the tape and can no longer write anything. Tape 1: 1391776710636 bytes (1296 GiB) Tape 2: 1424535198256 bytes (1326 GiB) Tape 3: 1334993518029 bytes (1243 GiB) Tape 4: 1383193282217 bytes (1292 GiB) The tape drive I'm using is heavily used as well as the tapes. This is the sg_log for one of them: HP        Ultrium 5-SCSI    I6CZ Supported log pages  [0x0]: 0x00        Supported log pages [sp] 0x02        Write error [we] 0x03        Read error [re] 0x0c        Sequential access device [sad] 0x0d        Temperature [temp] 0x11        DT Device status [dtds] 0x12        Tape alert response [tar] 0x13        Requested recovery [rr] 0x14        Device statistics [ds] 0x15        Service buffers information [sbi] 0x16        Tape diagnostic data [tdd] 0x17        Volume statistics [vs] 0x18        Protocol specific port [psp] 0x1b        Data compression [dc] 0x2e        Tape alert [ta] 0x30        Tape usage (lto-5, 6) [tu_] 0x31        Tape capacity (lto-5, 6) [tc_] 0x32        Data compression (lto-5) [dc_] 0x34        Read forward errors (lto-5) [rfe_] 0x35        DT Device Error (lto-5, 6) [dtde_] 0x3e        Device Status (lto-5, 6) [ds_] Write error counter page  [0x2] Errors corrected without substantial delay = 0 Errors corrected with possible delays = 0 Total rewrites or rereads = 0 Total errors corrected = 0 Total times correction algorithm processed = 0 Total bytes processed = 0 Total uncorrected errors = 0 Read error counter page  [0x3] Errors corrected without substantial delay = 0 Errors corrected with possible delays = 0 Total rewrites or rereads = 0 Total errors corrected = 0 Total times correction algorithm processed = 0 Total bytes processed = 0 Total uncorrected errors = 0 Sequential access device page (ssc-3) Data bytes received with WRITE commands: 0 GB Data bytes written to media by WRITE commands: 0 GB Data bytes read from media by READ commands: 0 GB Data bytes transferred by READ commands: 0 GB Native capacity from BOP to EOD: 1527775 MB Native capacity from BOP to EW of current partition: 1517690 MB Minimum native capacity from EW to EOP of current partition: 12239 MB Native capacity from BOP to current position: 17654 MB Maximum native capacity in device object buffer: 206 MB Cleaning action not required (or completed) Temperature page  [0xd] Current temperature = 41 C Reference temperature = <not available> DT device status page (ssc-3, adc-3) [0x11] Very high frequency data: PAMR=0 HUI=0 MACC=1 CMPR=1 WRTP=0 CRQST=0 CRQRD=0 DINIT=1 INXTN=0 RAA=0 MPRSNT=1 MSTD=1 MTHRD=1 MOUNTED=1 DT device activity: No DT device activity VS=0 TDDEC=0 EPP=0 ESR=0 RRQST=0 INTFC=0 TAFC=0 Very high frequency polling delay:  16 milliseconds DT device ADC data encryption control status (hex only now): 00     00 00 00 00 00 00 00 00 Key management error data (hex only now): 00     00 00 00 00 00 00 00 00  00 00 00 00 Primary port 1 status: non-SAS transport, in hex: 00     00 00 00 00 00 00 00 00  50 01 10 a0 01 4a a5 9c    ........P....J.. 10     50 01 10 a0 01 4a a5 9e                             P....J.. Primary port 2 status: non-SAS transport, in hex: 00     3b 00 00 01 00 00 00 01  50 01 10 a0 01 4a a5 9d    ;.......P....J.. 10     50 01 10 a0 01 4a a5 9e                             P....J.. Primary port 3 status: non-SAS transport, in hex: 00     00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00    ................ 10     00 00 00 00 00 00 00 00                             ........ Primary port 4 status: non-SAS transport, in hex: 00     02 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00    ................ 10     00 00 00 00 00 00 00 00                             ........ Vendor specific [parameter_code=0x8000]: 00     80 00 43 06 31 17 00 00  00 02                      ..C.1..... Vendor specific [parameter_code=0x8003]: 00     80 03 43 08 00 00 00 00  00 00 00 00                ..C......... Vendor specific [parameter_code=0x8010]: 00     80 10 43 08 00 00 00 00  00 00 00 00                ..C......... Vendor specific [parameter_code=0x8020]: 00     80 20 43 12 00 00 00 00  00 00 00 00 00 00 00 00    . C............. 10     00 00 00 00 00 00                                   ...... Vendor specific [parameter_code=0xa101]: 00     a1 01 43 04 00 00 00 00                             ..C..... Vendor specific [parameter_code=0xa102]: 00     a1 02 43 04 00 00 00 00                             ..C..... TapeAlert response page (ssc-3, adc-3) [0x12] Flag01h: 0  02h: 0  03h: 0  04h: 0  05h: 0  06h: 0  07h: 0  08h: 0 Flag09h: 0  0Ah: 0  0Bh: 0  0Ch: 0  0Dh: 0  0Eh: 0  0Fh: 0  10h: 0 Flag11h: 0  12h: 0  13h: 0  14h: 0  15h: 0  16h: 0  17h: 0  18h: 0 Flag19h: 0  1Ah: 0  1Bh: 0  1Ch: 0  1Dh: 0  1Eh: 0  1Fh: 0  20h: 0 Flag21h: 0  22h: 0  23h: 0  24h: 0  25h: 0  26h: 0  27h: 0  28h: 0 Flag29h: 0  2Ah: 0  2Bh: 0  2Ch: 0  2Dh: 0  2Eh: 0  2Fh: 0  30h: 0 Flag31h: 0  32h: 0  33h: 0  34h: 0  35h: 0  36h: 0  37h: 0  38h: 0 Flag39h: 0  3Ah: 0  3Bh: 0  3Ch: 0  3Dh: 0  3Eh: 0  3Fh: 0  40h: 0 Requested recovery page (ssc-3) [0x13] Recovery procedures: Recovery not requested Device statistics page (ssc-3 and adc) Lifetime media loads: 5288 Lifetime cleaning operations: 195 Lifetime power on hours: 48480 Lifetime media motion (head) hours: 12747 Lifetime metres of tape processed: 187049835 Lifetime media motion (head) hours when incompatible media last loaded: 0 Lifetime power on hours when last temperature condition occurred: 0 Lifetime power on hours when last power consumption condition occurred: 0 Media motion (head) hours since last successful cleaning operation: 66 Media motion (head) hours since 2nd to last successful cleaning: 126 Media motion (head) hours since 3rd to last successful cleaning: 187 Lifetime power on hours when last operator initiated forced reset and/or emergency eject occurred: 20048 Lifetime power cycles: 77 Volume loads since last parameter reset: 5288 Hard write errors: 2 Hard read errors: 0 Duty cycle sample time (ms): 27276000 Read duty cycle: 0 Write duty cycle: 0 Activity duty cycle: 76 Volume not present duty cycle: 15 Drive manufacturer's serial number: 0 Drive serial number: 0 Medium removal prevented: 0 Maximum recommended mechanism temperature exceeded: 0 Media motion (head) hours for each medium type: Density code: 0x44, Medium type: 0x0 Medium motion hours: 0 Density code: 0x44, Medium type: 0x1 Medium motion hours: 0 Density code: 0x46, Medium type: 0x0 Medium motion hours: 0 Density code: 0x46, Medium type: 0x1 Medium motion hours: 0 Density code: 0x58, Medium type: 0x0 Medium motion hours: 12747 Density code: 0x58, Medium type: 0x1 Medium motion hours: 0 Service buffer information page (adc-3) [0x15] Service buffer identifier: 0x0 Buffer id: 0x41, tu=0, nmp=0, nmm=0, offline=0 pd=0, code_set: Binary, Service buffer title: DT Device Error Log Tape diagnostics data page (ssc-3) [0x16] Parameter code: 0 Density code: 0x58 Medium type: 0x0 Lifetime media motion hours: 12739 Repeat: 0 Sense key: 0x3 [Medium Error] Additional sense code: 0x0 Additional sense code qualifier: 0x2 [Additional sense: End-of-partition/medium detected] Vendor specific code qualifier: 0x5098 Product revision level: 1228292954 Hours since last clean: 58 Operation code: 0x1d Service action: 0x0 Medium id number (in hex): 00     49 43 32 47 4f 42 6e 36  38 30 00 00 00 00 00 00    IC2GOBn680...... 10     00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00    ................ Timestamp origin: 0x0 Timestamp: 00     00 00 00 09 6f 65 Parameter code: 1 Density code: 0x58 Medium type: 0x0 Lifetime media motion hours: 11121 Repeat: 0 Sense key: 0x3 [Medium Error] Additional sense code: 0xc Additional sense code qualifier: 0x0 [Additional sense: Write error] Vendor specific code qualifier: 0x5083 Product revision level: 1228292954 Hours since last clean: 67 Operation code: 0x0 Service action: 0x0 Medium id number (in hex): 00     41 44 37 48 52 56 4e 55  58 4e 00 00 00 00 00 00    AD7HRVNUXN...... 10     00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00    ................ Timestamp origin: 0x0 Timestamp: 00     00 00 fb 62 2c 3e Parameter code: 2 Density code: 0x58 Medium type: 0x0 Lifetime media motion hours: 1261 Repeat: 0 Sense key: 0x3 [Medium Error] Additional sense code: 0x14 Additional sense code qualifier: 0x0 [Additional sense: Recorded entity not found] Vendor specific code qualifier: 0x5090 Product revision level: 1228290394 Hours since last clean: 167 Operation code: 0x11 Service action: 0x0 Medium id number (in hex): 00     41 44 37 48 52 56 4e 55  32 4d 00 00 00 00 00 00    AD7HRVNU2M...... 10     00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00    ................ Timestamp origin: 0x0 Timestamp: 00     00 00 00 c9 22 70 Volume statistics page (ssc-4), subpage=0 Page valid: 1 Thread count: 162 Total data sets written: 2916336 Total write retries: 20 Total unrecovered write errors: 0 Total suspended writes: 11 Total fatal suspended writes: 0 Total data sets read: 1579246 Total read retries: 154 Total unrecovered read errors: 0 Last mount unrecovered write errors: 0 Last mount unrecovered read errors: 0 Last mount megabytes written: 0 Last mount megabytes read: 0 Lifetime megabytes written: 7209299 Lifetime megabytes read: 3903959 Last load write compression ratio: 0 Last load read compression ratio: 0 Medium mount time: 136669 Medium ready time: 136669 Total native capacity [MB]: 1529930 Total used native capacity [MB]: 1527775 Volume serial number: E111229096 Tape lot identifier: H1080121 Volume barcode: SN3018L5 Volume manufacturer: SONY Volume license code: U109 Volume personality: Ultrium-5 Write protect: 0 WORM: 0 Maximum recommended tape path temperature exceeded: 0 Beginning of medium passes: 1231 Middle of medium passes: 929 Logical position of first encrypted logical object: partition number: 0, partition record data counter: 0xffffffffffff Logical position of first unencrypted logical object after first encrypted logical object: partition number: 0, partition record data counter: 0xffffffffffff Native capacity partition(s) [MB]: partition number: 0, partition record data counter: 1529930 Used native capacity partition(s) [MB]: partition number: 0, partition record data counter: 1527775 Vendor specific parameter code (0xf000), payload in hex 00     00 01                                               .. Data compression page  (ssc-4) [0x1b] Read compression ratio x100: 0 Write compression ratio x100: 0 Megabytes transferred to server: 0 Bytes transferred to server: 0 Megabytes read from tape: 0 Bytes read from tape: 0 Megabytes transferred from server: 0 Bytes transferred from server: 0 Megabytes written to tape: 0 Bytes written to tape: 0 Data compression enabled: 0x1 Tape alert page (ssc-3) [0x2e] Read warning: 0 Write warning: 0 Hard error: 0 Media: 0 Read failure: 0 Write failure: 0 Media life: 0 Not data grade: 0 Write protect: 0 No removal: 0 Cleaning media: 0 Unsupported format: 0 Recoverable mechanical cartridge failure: 0 Unrecoverable mechanical cartridge failure: 0 Memory chip in cartridge failure: 0 Forced eject: 0 Read only format: 0 Tape directory corrupted on load: 0 Nearing media life: 0 Cleaning required: 0 Cleaning requested: 0 Expired cleaning media: 0 Invalid cleaning tape: 0 Retension requested: 0 Dual port interface error: 0 Cooling fan failing: 0 Power supply failure: 0 Power consumption: 0 Drive maintenance: 0 Hardware A: 0 Hardware B: 0 Interface: 0 Eject media: 0 Microcode update fail: 0 Drive humidity: 0 Drive temperature: 0 Drive voltage: 0 Predictive failure: 0 Diagnostics required: 0 Obsolete (28h): 0 Obsolete (29h): 0 Obsolete (2Ah): 0 Obsolete (2Bh): 0 Obsolete (2Ch): 0 Obsolete (2Dh): 0 Obsolete (2Eh): 0 Reserved (2Fh): 0 Reserved (30h): 0 Reserved (31h): 0 Lost statistics: 0 Tape directory invalid at unload: 0 Tape system area write failure: 0 Tape system area read failure: 0 No start of data: 0 Loading failure: 0 Unrecoverable unload failure: 0 Automation interface failure: 0 Firmware failure: 0 WORM medium - integrity check failed: 0 WORM medium - overwrite attempted: 0 Reserved parameter code 0x3d, flag: 0 Reserved parameter code 0x3e, flag: 0 Reserved parameter code 0x3f, flag: 0 Reserved parameter code 0x40, flag: 0 Tape usage page  (LTO-5 and LTO-6 specific) [0x30] Thread count: 162 Total data sets written: 2916336 Total write retries: 20 Total unrecovered write errors: 0 Total suspended writes: 11 Total fatal suspended writes: 0 Total data sets read: 1579246 Total read retries: 154 Total unrecovered read errors: 0 Tape capacity page  (LTO-5 and LTO-6 specific) [0x31] Main partition remaining capacity (in MiB): 2056 Alternate partition remaining capacity (in MiB): 0 Main partition maximum capacity (in MiB): 1459056 Alternate partition maximum capacity (in MiB): 0 Data compression page  (LTO-5 specific) [0x32] Read compression ratio x100: 0 Write compression ratio x100: 0 Megabytes transferred to server: 0 Bytes transferred to server: 0 Megabytes read from tape: 0 Bytes read from tape: 0 Megabytes transferred from server: 0 Bytes transferred from server: 0 Megabytes written to tape: 0 Bytes written to tape: 0 Unable to decode page = 0x34, here is hex: 00     34 00 00 1e 00 00 60 02  00 00 00 01 60 02 00 00 10     00 02 60 02 00 00 00 03  60 02 00 00 00 04 40 02 20     05 78 Unable to decode page = 0x35, here is hex: 00     35 00 00 74 00 00 43 16  00 00 00 00 00 00 00 00 10     00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 01 20     43 56 00 00 00 00 00 00  00 00 00 00 00 00 00 00 30     00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00 40     00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00 50     00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00 60     00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00 70     00 00 00 00 00 00 00 00 Unable to decode page = 0x3e, here is hex: 00     3e 00 00 28 00 00 40 04  00 00 00 00 00 01 40 04 10     00 05 01 00 00 02 60 04  00 00 14 a8 00 03 40 04 20     ff ff ff ff 00 04 40 04  01 09 02 23 The tapes write speed is sporadic throughout with writes speeds ranging from 140/MBs all the way down to 40/MBs with the mbuffer 100% remaining full so it's not being starved for data. I don't have any brand new tapes to use or a second tape drive so is it normal for used tapes to lose a lot of capacity and I will just have to account for this with a safer space margin?"},
{"Title": "Loudest HDD available?", "Author": "u/Western_Bass_1491", "Content": "I am giving a iMac a massive storage boost, however all the HDDâ€™s I have are decently quiet. I am looking for one to give it the classic HDD sound, and that actually still exists and isnâ€™t impossible to find. I donâ€™t care about speed or rpm, just the sound and one that wonâ€™t make me broke lol"},
{"Title": "New Hard drives not detecting (Not usual issues)", "Author": "u/may9899999", "Content": "I have a PCIE to sata card that supports 8 drives. I had 5 installed on the card that all worked fine. I bought 3 new drives but they aren't being detected. I've tried changed cables with no effect. I've tried adding one at a time, nothing. I'm only seeing the original 5 no matter what I do. I thought maybe unplugging and old one and trying a new one could potentially let me know if I had too many, but then only the 4 drives showed up. They don't show up with fdisk -l, the don't show up in disks or gparted. I tried a USB enclosure to initialize them to see if that would help, and they did initialize, but once connected back to SATA, they don't show up. I'm just really at a loss of what's going on at this point. Below is the link to my previous post that I made trying to fix this issue. OS is Ubuntu 22.04 https://www.reddit.com/r/Ubuntu/comments/1cz2jlb/new_hard_drive_not_showing_up/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button Edit Issue solved below, TLDR: I needed an adapter power cable (which came in the box but looked like just an extension)"},
{"Title": "Anyone experimented with p2p storage for backup purposes?", "Author": "u/The_B0rg", "Content": "I'm mostly talking about IPFS and systems built on top of it like filecoin and hivenet, both commercial solutions that provide file storage with guarantees over IPFS. You can pay in money but lower your costs by providing storage in return in a P2P fashion. I truly believe these kinds of services will only get better and are a big part of the future of the internet and data storage. The question is if anyone already has any experience with them and if they are already good enough or not?"},
{"Title": "WD BLACK Hard Drive Failure", "Author": "u/Majestic-Owl-5801", "Content": "I was transferring files from one hard drive to another and had to end the process, but now I cant open up the hard drive I was transferring files from. I am sure I corrupted something by ending the process, but it ended on one of the windows that asked to clear space to continue. So it was on a single file rather than in the middle of a file. I clicked skip for all remaining and it froze, it was quite a lot of files. I closed the window because it was frozen, and now it wont show me the files on the drive. It will load, show me the remaining space taken up, but wont load any files when I actually open the drive in explorer."},
{"Title": "Best practices with Terracopy? Verify and checksum?", "Author": "u/Bern_Down_the_DNC", "Content": "First time using Teracopy. I was recommended to use this since I can check \"verify files\" and it will do that after copying files on drive 1 and syncing to drive 2. Nobody said anything about checksum files. Not sure if it would be beneficial. I will rename files or add more files to the directory later on drive 1, then sync all the files to drive 2 again. If I need to do checksum, would I create a checksum for folders on drive 1, then start copy and sync to drive 2 but select \"do checksum at the end\" so it generates a second checksum on drive 2 that I can compare to the checksum from drive 1? Also do I need to choose \"store/verify checksum on ADS\"? (And should I use xx3-64 instead of MD5 now?) Thank you!"},
{"Title": "I want to expand my zpool in both size and efficiency, could you please help me?", "Author": "u/mathscasual", "Content": "Right  now I have two 16 TB hdds mirrored in raid 1. I want to change my raid configuration to increase speed and size. Give me an excuse to buy 6 more drives(or more) so I have both increased size and speed. What raid configuration? How many more drives?(can they be larger than 16Tb(was looking at 22TBs) cacching? Many pleases and thank yous"},
{"Title": "Can you still get Yoyotta with a lifetime license or is it all subscriptions?", "Author": "u/DevilryAscended", "Content": "I saw this post from 6 years ago, https://www.reddit.com/r/DataHoarder/comments/a2k5y4/hoarding_to_lto_tape_primer_all_you_wanted_to/ Where it listed a price that is definitely higher than their current subscription price. Does anyone know when they made the change? I'm currently doing research into LTO backups/archiving as an option for system we're building out and a key thing I would like to avoid is any subscriptions but I did like how well documented using Yoyotta seemed."},
{"Title": "(UK) 18TB WD Elements @ Â£14.3 per TB on WD Website", "Author": "u/mrnngbgs", "Content": "You get Â£50 off if you spend over Â£300 on WD website. If you pair WD Elements 18TB (currently discounted at Â£297.99) with their 2 year data recovery plan (Â£10), you will only pay Â£258 which equals to Â£14.33 per TB. Finally a good deal! (UK) Ends 27th of May"},
{"Title": "How much data duplication do you have? Do you really follow 3-2-1 at home?", "Author": "u/pm_me_xenomorphs", "Content": "For example, i have 3 copies of all my drives that i care about. I have 1 copy on my NAS, 1 copy offline for regular backups and another copy offsite for infrequent backups. In total i have 3 hard drives for every one data. Do you guys actually do this?"},
{"Title": "Tape drives and cartridges - the ultimate endgame?", "Author": "u/the_rodent_incident", "Content": "I'm watching my pile of old hard drives with centuries worth of books, comics, music and movies, which are all gathering dust. And now the intrusive thoughts of replacing them with a somewhat modern solution are starting to appear. Just started researching about tape drives, and the first quarter of rabbit's hole is promising. They do seem like the best backup solution ever. I can find brand new LTO-4 1.6TB tapes for like $12. Drive goes for about $200. There are better tapes which go from 5 to 10TB, but their drives are also more expensive. I love the concept, though. Tapes are pure media, like DVD-RW drives. No electronics on them. They won't die because a spindle motor or a controller chip failed, or a bit flipped in the flash chip holding the firmware. Tapes are ancient retro tech thriving in the solid state age. What do you think? Am I deluding myself?"},
{"Title": "How can I fix this error (caused by power cable not being in when rebooted) (Truenas core)", "Author": "u/edwardK1231", "Content": "https://preview.redd.it/how-can-i-fix-this-error-caused-by-power-cable-not-being-in-v0-pdt9i82r8c2d1.png There was an error before saying one drive had a failure or something but the whole pool is fine. (makes sense as running Raid Z1) The drive is fine I just didn't connect the power after I had added/replaced some fans. How can I get rid of the error? I have scrubbed the pool but nothing changed. I am a bit of a noob with truenas sorry. Thanks in advance for any help."},
{"Title": "Looking for the Wikiread software that could read MediaWiki wikis from JDC Creations", "Author": "u/Mewto17", "Content": "The website is down now. Here is a Archive.org link . The zip file for the software is corrupt. Does anyone know where I can get it from? Thanks."},
{"Title": "What kind of system is best for low data transfer for a longer time", "Author": "u/BiggieChezes", "Content": "I want to make a setup that lasts for a long time. I believe it would be best not to use a raid setup since it seems silly to have multiple drives running for basically USB 2.0 transfer speeds (Just fast enough to play Blue Rays), and I already have a backup of that data. Preferably some external solution, a power cable, and a USB A output so it would be easy to plug into my media box For now, I've been using a few USB thumb drives, which did the job. They were slow to transfer too and one of them died (tho it was easy to get it replaced via the manufacturer). So I'm basically thinking about some smaller ssd, nvme setup (2tb each drive), but I'm having a hard time finding one that isn't just a single ssd or single nvme enclosure. External hard drives also didn't sound so bad. Still, if I wanted a bigger capacity I would be stuck with Seagate One Touch Hubs and the question of how long will it last. In contrast, external SSDs seemed bad since they cost like 100 euros per 1tb while a decent 2tb nvme costs around 120 euros (150 euros if where count the enclosure)."},
{"Title": "Writing family photos to BD-R directly with file explorer in Windows 10? Or use burning software?", "Author": "u/almondbutter4", "Content": "Hi all, I have some 25GB BD-R m-discs and a blu-ray burner that I want to use to back up family photos. Has anyone ever used Windows's native ability to burn files to blu-ray? Is it recommended to use software like ImgBurn instead? Also, if I do it through file explorer, it seems like I can burn a few files at a time. Has anyone had a problem with burning some files. Ending the session. Then adding additional files to the disc later? Thanks!"},
{"Title": "How to set ctbrec to auto record?", "Author": "u/QueenAng429", "Content": "Can someone please explain how to set ctbrec to auto record? There's zero documentation online, and all the posts here of people recommending it are years old without anything other than a recommendation. I can't figure out how to make it auto record, just a manual recording."},
{"Title": "seeing different \"available\" space on ZFS pools depending on whether the host is proxmox or TrueNAS CORE. both pools are 3 drives, PM shows 73% of space available TNC shows 65%. what's up ?", "Author": "u/ImaginaryCheetah", "Content": "good evening, title gets my whole question... i have two machines, each with 3 drives in a raidZ ZFS pool, but i'm seeing 8% difference in how much space is \"available\", relative to the drive capacity, between a PM or TNC host. any ideas what could cause the difference ?"},
{"Title": "Why does the LotR:FotR Extended Edition DVD have two copies of the movie on the disc?", "Author": "u/TrekChris", "Content": "I just got the extended trilogy on DVD, and I wanted to rip them to my computer.  I discovered that there are two copies of the movie stored on the DVD.  Same length, the only difference is that one had more audio streams than the other.  Why would they do this?  They could probably fit the entire extended movie on one disc if they hadn't done this.  I'm using MakeMKV to do the ripping, and this is the first time I've come across this."},
{"Title": "DAS or NAS? Using existing drives in external storage without reformatting.", "Author": "u/PyrolyticCarbon", "Content": "Hi all. I've been researching and asking questions regarding NAS drives, and I thought I'd chosen a NAS to purchase. Then, just before I buy, I see that it formats the drives you're installing and there's no way round it, something I can't do. I have an 512Gb SSD and two 1TB HDDs filled with data in my old PC. I have a new mini PC and I had hoped to move these into a NAS and access them as is, no reformatting, no RAID - although I may buy another drive and drop it in a 4-bay solution and mirror to that, I have no need or desire for reformatting these drives currently, nor to move to a RAID solution. All the NAS units I look at say I have to reformat my drives first. I can't just plug them in and use them. It's a shame as my really old 2-bay Buffalo NAS allowed just that. DAS drives look attractive, but mixing an SSD with HDDs seems harder there and I haven't found one to suit. Any thoughts or solutions for the above? Am I missing a NAS that could mix these drives and not require me to reformat? Is there a DAS that would allow SSD and HDD together? Do I have to purchase a NAS with new disks and copy everything across and just throw my current disks out? Thank you!"},
{"Title": "New to SAS. Have a lot of them. What is your recommendation for cards to interface them with a desktop PC?", "Author": "u/Sketchy_Uncle", "Content": "No content"},
{"Title": "I copied a hard drive without Terracopy, so now there are two drives with all the same data. Is there any way to verify the data after the fact?", "Author": "u/Bern_Down_the_DNC", "Content": "I forgot to download Terracopy before doing the transfer. Is there a way to easily verify the data hashes for everything at this point? Thank you."},
{"Title": "Automatically dump all telegram files", "Author": "u/not-the-real-chopin", "Content": "There are few Telegram groups that are sharing a lot of linux iso I'd like to automatically download them for my iso collection. Can you suggest a tool to install on linux for this purpose?"},
{"Title": "Storage advice for home server", "Author": "u/Negative-Main7422", "Content": "Hi, I've just bought a HP Prodesk 400 G5 MT with Intel Core i5-9400T and 16GB RAM. It just shipped. I've seen that it's motherboard has 3 sata connectors on which I hope to connect 3 HDDs in zfs raidz. Now being a hardware newbie I don't know know which HDDs to buy and what should I keep in mind. Do 3.5\" HDDs need to be connected to the power supply? And if so, do you know where could I find out if this HP i bought can have 3 HDDs connected to the power supply? And if it can't, do you know any way around this problem? Also in the product specifications manual, it lists supported HDDs but only lists up to 2TB 7200rpm 3.5 inch drive. Is there any reason why it shouldn't support a 3TB HDD? Since I've found used 3TB WD reds for 30 euros and I'm looking to buy several."},
{"Title": "What would drag-and-drop copying errors result in for personal photos and videos? Is it necessary to copy them with an extra integrity software? Best such software for Mac OS?", "Author": "u/AntarcticNightingale", "Content": "What percentage of drag-and-drop copy result in undetected errors on a MacOS? (Discounting detected errors like sometimes if there are issues with not enough storage, I get a pop up telling me the problem, so that doesnâ€™t count.) Iâ€™m just copying decades worth of personal photos and videos, and some documents. What would an undetected error be like? Just a single pixel of a picture damaged, a portion of the picture, entire picture, or entire batches of pictures and media? If the error rate is something to be concerned about, what is the MacOS equivalent of TetaCopy?"},
{"Title": "Optimal external drive distribution?", "Author": "u/CautiousXperimentor", "Content": "Hello. Historically, Iâ€™ve had the following USB spinning hard drives (all 2.5â€): - WD My Passport: 1 TB: personal stuff and old files, as this was my first external drive; this includes photos, videos, class recordings, old images and screenshots, music, software from the 90s and early 2000â€¦ stuff that I donâ€™t want to get rid of. Itâ€™s at 80% full. - Toshiba Canvio Basics (1TB): Movie backups, classified by genre, and game backups. 96% capacity. - Toshiba Canvio Basics (yes, I love how good and reliable this model is) (2TB): Series, documentaries and anime backups, as well as other content from the internet such as podcasts or video-podcasts. Itâ€™s at 60% of its capacity. - Re-used Hitachi drive (256GB): itâ€™s health is low, but it still works, I use it as the only ExFAT drive if I need to use it on a machine that doesnâ€™t support my current files system (APFS). It has a lot of random stuff. Now, those are the traditional spinning external drives Iâ€™ve always had. On the other hand, during the recent years, thanks to different sales, Iâ€™ve managed to get the following Samsung T5 and T7 SSDs. My plan when I got this new T5 and T7 Samsung SSDs was to be able to carry with me the same information but with much less weight, and keep the older 2.5â€ hard drives as a long term storage/backup, without actually using them unless when I want to back up something very important. My new portable SSDs are the following: -T5 500GB: I intend to use it to carry it with me to the university, store class recordings (including video) that usually take 1GB each, and my device has only 128GB on board. I want to use it as a high performance pen-drive. -T5 1TB: here, Iâ€™d thought to store the same personal data, such as photos or videos or old files that I already have on my WD My Passport (1TB) - T7 2TB: intended to use as a media SSD, because of itâ€™s big size. However, Iâ€™m not sure if I should use it to store my academical recordings (remember, ~1GB per file), or just put there my favorite movie and series backups. Or maybe both? But having already 1TB 2.5â€ HDD just for movies, and another 2TB 2.5â€ HDD for series+anime backups and other Internet video content such as videopodcasts, Iâ€™m not sure if putting here my favorite content (duplicate) is wasting this 2TB SSD. -T7 Shield 4TB: And when I thought I had enough SSDs to fulfill my needsâ€¦ I saw a sale on the T7 Shield 4TB SSD, at less than half the price. Obviously, I had to get it. So now I have this 4TB SSD that I can carry with me with EVERYTHING I have on the other SSDs, in just one drive. However, when I go to the Uni, the SSD I want to carry with me is the T5 500GB. Itâ€™s the cheapest and everything on it will -hopefully- be backed up. My main doubt is (option A) wether to use the T5 1TB SSD for personal stuff that I already have on the WD My PassPort, but thatâ€™s not data I use too much. In this case I would use the 2TB SSD for my classes (long term) AND my favorite media content such as movies and shows. The alternative (option B) is to use the T5 1TB for storing my classes long term (short term is what the 500GB is for), and leave the 2TB SSD just for non-personal media. In this case (option B), among the personal data from the WD drive, I could make a selection, i.e. all my personal photos and memories and PDFs and documents, and put them on the 4TB drive along with the media stuff and my uni stuff to have everything in one drive. I could also use this 4TB SSD to sort all the duplicates I have on every hard drive, which I suspect they are a lot. I think itâ€™s easier having all in one drive. Donâ€™t worry about privacy, as Iâ€™m formatting all the T5 and T7 with encryption and a 16-20 character long password, a different one for each drive. How would you use the 4 SSDs if you were me? You can also comment on my hardware if you want. The Toshiba Canvio Basic were cheap and extraordinarily reliable and silent hard drives that still work perfectly after more than 10 years."},
{"Title": "Any way to make a cheap docking station or something similar?", "Author": "u/BiggieChezes", "Content": "I have a few smaller 256GB, and 512GB scrap hard drives. And I wanted to turn them into home theater storage. I thought about a Plex server but decided against it since I only need it for 1 device and the regular PC motherboard (just an old crappy pc) usually didn't have enough sata ports. So I'm looking for something (or to make something) that powers the multiple hard drives and sends the storage signal via a single USB A (don't care if it's janky as hell) tho I don't care for redundancy. Did think about just buying a few HHD enclosures but tbh I don't like that since every one of them would need a different power cable and they cost a bit. A 256 GB USB would cost the same as getting an enclosure for an old 256GB hard drive. And I do mean a cheap option since if it costs nearly 100 euros it's way more price-effective just to buy a 4 TB hard drive with an enclosure."},
{"Title": "Flash NAS help", "Author": "u/Oblec", "Content": "I love to build a NAS with only flash storage. But looking at prices, m2 storage seems to be the way to go 4x 8tb nvme 15-16tb should be enough for me. I also gonna add two 10tb for my surveillance. Im concerned the lifetime of it wonâ€™t be that good. Is that an issue?"},
{"Title": "How do you guys keep your files organized?", "Author": "u/ramy_chaos", "Content": "Hi everyone! So ive got a bunch of external and internal drives and my pc just feels like a cluttered mess in terms of storage. Ive got files all over the place, some of which are duplicates, and it's driving me nuts. If anyone has any advice, id be super grateful!"},
{"Title": "I thought what I wanted was simple, then I read on here and now I'm confused. Can I get some ELI5 advice on expanding my storage?", "Author": "u/LoliSukhoi", "Content": "All I want is additional storage space for archiving files off of my PC that has some protection against drive failure. What is the simplest method of achieving this? Until recently I used a Synology 4 bay NAS but I've outgrown it and since I never used 99% of the features of a NAS, I decided to get a 10 bay IcyBox DAS (I think it's a DAS? It connects via USBC) that I saw on sale along with 4 24TB Seagate drives. My idea was to then use Windows Storage Spaces to create a pool and mirror the drives to add redundancy. But then I saw a lot of people on here really don't like Storage Spaces which sent me down a rabbit hole of Googling and reading threads which involved a million acronyms and other words and names I don't understand and now I'm thoroughly confused. Like what's the difference between a DAS and a JBOD? What's a Home Lab? What's ZFS? (I'm just asking these to show the kind of research mess I've ended up in, don't feel the need to spend paragraphs answering them.) Is Storage Spaces good enough for what I want? Will it easily allow me to add more drives to the pool down the line? Did I buy the wrong thing? Should I have done something else?"},
{"Title": "how to check are files good or damaged?", "Author": "u/oO0_", "Content": "i do simple backups. But how can i check if current files are good? Is there way to check (hash or what) that file should have unless i change file by myself or by some software? Most files on Windows/NTFS"},
{"Title": "For a folder of media files, is it possible to get a directory list  with media encoded type? AV1 H264 etc.?", "Author": "u/danuser8", "Content": "Looking for solution in windows"},
{"Title": "i've got a couple HP 4k13c5 HBAs from salvaged machines, which include the onboard cache. but i only see folks praising LSI HBAs on here. should i spend the $35 on a LSI ?", "Author": "u/ImaginaryCheetah", "Content": "\"when in rome\", as they say. for someone generally ignorant on good verses bad HBAs, i assume there's a reason for the cult status of LSI, but is there any reason to not use the free HP ones i've got ? having onboard cache seems like a useful feature, but might not be worth the potential issue with HP drivers."},
{"Title": "Can you still get Yoyotta with a lifetime license or is it all subscriptions?", "Author": "u/DevilryAscended", "Content": "I saw this post from 6 years ago, https://www.reddit.com/r/DataHoarder/comments/a2k5y4/hoarding_to_lto_tape_primer_all_you_wanted_to/ Where it listed a price that is definitely higher than their current subscription price. Does anyone know when they made the change? I'm currently doing research into LTO backups/archiving as an option for system we're building out and a key thing I would like to avoid is any subscriptions but I did like how well documented using Yoyotta seemed."},
{"Title": "(UK) 18TB WD Elements @ Â£14.3 per TB on WD Website", "Author": "u/mrnngbgs", "Content": "You get Â£50 off if you spend over Â£300 on WD website. If you pair WD Elements 18TB (currently discounted at Â£297.99) with their 2 year data recovery plan (Â£10), you will only pay Â£258 which equals to Â£14.33 per TB. Finally a good deal! (UK) Ends 27th of May"},
{"Title": "How much data duplication do you have? Do you really follow 3-2-1 at home?", "Author": "u/pm_me_xenomorphs", "Content": "For example, i have 3 copies of all my drives that i care about. I have 1 copy on my NAS, 1 copy offline for regular backups and another copy offsite for infrequent backups. In total i have 3 hard drives for every one data. Do you guys actually do this?"},
{"Title": "Tape drives and cartridges - the ultimate endgame?", "Author": "u/the_rodent_incident", "Content": "I'm watching my pile of old hard drives with centuries worth of books, comics, music and movies, which are all gathering dust. And now the intrusive thoughts of replacing them with a somewhat modern solution are starting to appear. Just started researching about tape drives, and the first quarter of rabbit's hole is promising. They do seem like the best backup solution ever. I can find brand new LTO-4 1.6TB tapes for like $12. Drive goes for about $200. There are better tapes which go from 5 to 10TB, but their drives are also more expensive. I love the concept, though. Tapes are pure media, like DVD-RW drives. No electronics on them. They won't die because a spindle motor or a controller chip failed, or a bit flipped in the flash chip holding the firmware. Tapes are ancient retro tech thriving in the solid state age. What do you think? Am I deluding myself?"},
{"Title": "How can I fix this error (caused by power cable not being in when rebooted) (Truenas core)", "Author": "u/edwardK1231", "Content": "https://preview.redd.it/how-can-i-fix-this-error-caused-by-power-cable-not-being-in-v0-pdt9i82r8c2d1.png There was an error before saying one drive had a failure or something but the whole pool is fine. (makes sense as running Raid Z1) The drive is fine I just didn't connect the power after I had added/replaced some fans. How can I get rid of the error? I have scrubbed the pool but nothing changed. I am a bit of a noob with truenas sorry. Thanks in advance for any help."},
{"Title": "Looking for the Wikiread software that could read MediaWiki wikis from JDC Creations", "Author": "u/Mewto17", "Content": "The website is down now. Here is a Archive.org link . The zip file for the software is corrupt. Does anyone know where I can get it from? Thanks."},
{"Title": "What kind of system is best for low data transfer for a longer time", "Author": "u/BiggieChezes", "Content": "I want to make a setup that lasts for a long time. I believe it would be best not to use a raid setup since it seems silly to have multiple drives running for basically USB 2.0 transfer speeds (Just fast enough to play Blue Rays), and I already have a backup of that data. Preferably some external solution, a power cable, and a USB A output so it would be easy to plug into my media box For now, I've been using a few USB thumb drives, which did the job. They were slow to transfer too and one of them died (tho it was easy to get it replaced via the manufacturer). So I'm basically thinking about some smaller ssd, nvme setup (2tb each drive), but I'm having a hard time finding one that isn't just a single ssd or single nvme enclosure. External hard drives also didn't sound so bad. Still, if I wanted a bigger capacity I would be stuck with Seagate One Touch Hubs and the question of how long will it last. In contrast, external SSDs seemed bad since they cost like 100 euros per 1tb while a decent 2tb nvme costs around 120 euros (150 euros if where count the enclosure)."},
{"Title": "Writing family photos to BD-R directly with file explorer in Windows 10? Or use burning software?", "Author": "u/almondbutter4", "Content": "Hi all, I have some 25GB BD-R m-discs and a blu-ray burner that I want to use to back up family photos. Has anyone ever used Windows's native ability to burn files to blu-ray? Is it recommended to use software like ImgBurn instead? Also, if I do it through file explorer, it seems like I can burn a few files at a time. Has anyone had a problem with burning some files. Ending the session. Then adding additional files to the disc later? Thanks!"},
{"Title": "How to set ctbrec to auto record?", "Author": "u/QueenAng429", "Content": "Can someone please explain how to set ctbrec to auto record? There's zero documentation online, and all the posts here of people recommending it are years old without anything other than a recommendation. I can't figure out how to make it auto record, just a manual recording."},
{"Title": "seeing different \"available\" space on ZFS pools depending on whether the host is proxmox or TrueNAS CORE. both pools are 3 drives, PM shows 73% of space available TNC shows 65%. what's up ?", "Author": "u/ImaginaryCheetah", "Content": "good evening, title gets my whole question... i have two machines, each with 3 drives in a raidZ ZFS pool, but i'm seeing 8% difference in how much space is \"available\", relative to the drive capacity, between a PM or TNC host. any ideas what could cause the difference ?"},
{"Title": "Why does the LotR:FotR Extended Edition DVD have two copies of the movie on the disc?", "Author": "u/TrekChris", "Content": "I just got the extended trilogy on DVD, and I wanted to rip them to my computer.  I discovered that there are two copies of the movie stored on the DVD.  Same length, the only difference is that one had more audio streams than the other.  Why would they do this?  They could probably fit the entire extended movie on one disc if they hadn't done this.  I'm using MakeMKV to do the ripping, and this is the first time I've come across this."},
{"Title": "DAS or NAS? Using existing drives in external storage without reformatting.", "Author": "u/PyrolyticCarbon", "Content": "Hi all. I've been researching and asking questions regarding NAS drives, and I thought I'd chosen a NAS to purchase. Then, just before I buy, I see that it formats the drives you're installing and there's no way round it, something I can't do. I have an 512Gb SSD and two 1TB HDDs filled with data in my old PC. I have a new mini PC and I had hoped to move these into a NAS and access them as is, no reformatting, no RAID - although I may buy another drive and drop it in a 4-bay solution and mirror to that, I have no need or desire for reformatting these drives currently, nor to move to a RAID solution. All the NAS units I look at say I have to reformat my drives first. I can't just plug them in and use them. It's a shame as my really old 2-bay Buffalo NAS allowed just that. DAS drives look attractive, but mixing an SSD with HDDs seems harder there and I haven't found one to suit. Any thoughts or solutions for the above? Am I missing a NAS that could mix these drives and not require me to reformat? Is there a DAS that would allow SSD and HDD together? Do I have to purchase a NAS with new disks and copy everything across and just throw my current disks out? Thank you!"},
{"Title": "New to SAS. Have a lot of them. What is your recommendation for cards to interface them with a desktop PC?", "Author": "u/Sketchy_Uncle", "Content": "No content"},
{"Title": "I copied a hard drive without Terracopy, so now there are two drives with all the same data. Is there any way to verify the data after the fact?", "Author": "u/Bern_Down_the_DNC", "Content": "I forgot to download Terracopy before doing the transfer. Is there a way to easily verify the data hashes for everything at this point? Thank you."},
{"Title": "Automatically dump all telegram files", "Author": "u/not-the-real-chopin", "Content": "There are few Telegram groups that are sharing a lot of linux iso I'd like to automatically download them for my iso collection. Can you suggest a tool to install on linux for this purpose?"},
{"Title": "Storage advice for home server", "Author": "u/Negative-Main7422", "Content": "Hi, I've just bought a HP Prodesk 400 G5 MT with Intel Core i5-9400T and 16GB RAM. It just shipped. I've seen that it's motherboard has 3 sata connectors on which I hope to connect 3 HDDs in zfs raidz. Now being a hardware newbie I don't know know which HDDs to buy and what should I keep in mind. Do 3.5\" HDDs need to be connected to the power supply? And if so, do you know where could I find out if this HP i bought can have 3 HDDs connected to the power supply? And if it can't, do you know any way around this problem? Also in the product specifications manual, it lists supported HDDs but only lists up to 2TB 7200rpm 3.5 inch drive. Is there any reason why it shouldn't support a 3TB HDD? Since I've found used 3TB WD reds for 30 euros and I'm looking to buy several."},
{"Title": "What would drag-and-drop copying errors result in for personal photos and videos? Is it necessary to copy them with an extra integrity software? Best such software for Mac OS?", "Author": "u/AntarcticNightingale", "Content": "What percentage of drag-and-drop copy result in undetected errors on a MacOS? (Discounting detected errors like sometimes if there are issues with not enough storage, I get a pop up telling me the problem, so that doesnâ€™t count.) Iâ€™m just copying decades worth of personal photos and videos, and some documents. What would an undetected error be like? Just a single pixel of a picture damaged, a portion of the picture, entire picture, or entire batches of pictures and media? If the error rate is something to be concerned about, what is the MacOS equivalent of TetaCopy?"},
{"Title": "Optimal external drive distribution?", "Author": "u/CautiousXperimentor", "Content": "Hello. Historically, Iâ€™ve had the following USB spinning hard drives (all 2.5â€): - WD My Passport: 1 TB: personal stuff and old files, as this was my first external drive; this includes photos, videos, class recordings, old images and screenshots, music, software from the 90s and early 2000â€¦ stuff that I donâ€™t want to get rid of. Itâ€™s at 80% full. - Toshiba Canvio Basics (1TB): Movie backups, classified by genre, and game backups. 96% capacity. - Toshiba Canvio Basics (yes, I love how good and reliable this model is) (2TB): Series, documentaries and anime backups, as well as other content from the internet such as podcasts or video-podcasts. Itâ€™s at 60% of its capacity. - Re-used Hitachi drive (256GB): itâ€™s health is low, but it still works, I use it as the only ExFAT drive if I need to use it on a machine that doesnâ€™t support my current files system (APFS). It has a lot of random stuff. Now, those are the traditional spinning external drives Iâ€™ve always had. On the other hand, during the recent years, thanks to different sales, Iâ€™ve managed to get the following Samsung T5 and T7 SSDs. My plan when I got this new T5 and T7 Samsung SSDs was to be able to carry with me the same information but with much less weight, and keep the older 2.5â€ hard drives as a long term storage/backup, without actually using them unless when I want to back up something very important. My new portable SSDs are the following: -T5 500GB: I intend to use it to carry it with me to the university, store class recordings (including video) that usually take 1GB each, and my device has only 128GB on board. I want to use it as a high performance pen-drive. -T5 1TB: here, Iâ€™d thought to store the same personal data, such as photos or videos or old files that I already have on my WD My Passport (1TB) - T7 2TB: intended to use as a media SSD, because of itâ€™s big size. However, Iâ€™m not sure if I should use it to store my academical recordings (remember, ~1GB per file), or just put there my favorite movie and series backups. Or maybe both? But having already 1TB 2.5â€ HDD just for movies, and another 2TB 2.5â€ HDD for series+anime backups and other Internet video content such as videopodcasts, Iâ€™m not sure if putting here my favorite content (duplicate) is wasting this 2TB SSD. -T7 Shield 4TB: And when I thought I had enough SSDs to fulfill my needsâ€¦ I saw a sale on the T7 Shield 4TB SSD, at less than half the price. Obviously, I had to get it. So now I have this 4TB SSD that I can carry with me with EVERYTHING I have on the other SSDs, in just one drive. However, when I go to the Uni, the SSD I want to carry with me is the T5 500GB. Itâ€™s the cheapest and everything on it will -hopefully- be backed up. My main doubt is (option A) wether to use the T5 1TB SSD for personal stuff that I already have on the WD My PassPort, but thatâ€™s not data I use too much. In this case I would use the 2TB SSD for my classes (long term) AND my favorite media content such as movies and shows. The alternative (option B) is to use the T5 1TB for storing my classes long term (short term is what the 500GB is for), and leave the 2TB SSD just for non-personal media. In this case (option B), among the personal data from the WD drive, I could make a selection, i.e. all my personal photos and memories and PDFs and documents, and put them on the 4TB drive along with the media stuff and my uni stuff to have everything in one drive. I could also use this 4TB SSD to sort all the duplicates I have on every hard drive, which I suspect they are a lot. I think itâ€™s easier having all in one drive. Donâ€™t worry about privacy, as Iâ€™m formatting all the T5 and T7 with encryption and a 16-20 character long password, a different one for each drive. How would you use the 4 SSDs if you were me? You can also comment on my hardware if you want. The Toshiba Canvio Basic were cheap and extraordinarily reliable and silent hard drives that still work perfectly after more than 10 years."},
{"Title": "Any way to make a cheap docking station or something similar?", "Author": "u/BiggieChezes", "Content": "I have a few smaller 256GB, and 512GB scrap hard drives. And I wanted to turn them into home theater storage. I thought about a Plex server but decided against it since I only need it for 1 device and the regular PC motherboard (just an old crappy pc) usually didn't have enough sata ports. So I'm looking for something (or to make something) that powers the multiple hard drives and sends the storage signal via a single USB A (don't care if it's janky as hell) tho I don't care for redundancy. Did think about just buying a few HHD enclosures but tbh I don't like that since every one of them would need a different power cable and they cost a bit. A 256 GB USB would cost the same as getting an enclosure for an old 256GB hard drive. And I do mean a cheap option since if it costs nearly 100 euros it's way more price-effective just to buy a 4 TB hard drive with an enclosure."},
{"Title": "Flash NAS help", "Author": "u/Oblec", "Content": "I love to build a NAS with only flash storage. But looking at prices, m2 storage seems to be the way to go 4x 8tb nvme 15-16tb should be enough for me. I also gonna add two 10tb for my surveillance. Im concerned the lifetime of it wonâ€™t be that good. Is that an issue?"},
{"Title": "How do you guys keep your files organized?", "Author": "u/ramy_chaos", "Content": "Hi everyone! So ive got a bunch of external and internal drives and my pc just feels like a cluttered mess in terms of storage. Ive got files all over the place, some of which are duplicates, and it's driving me nuts. If anyone has any advice, id be super grateful!"},
{"Title": "I thought what I wanted was simple, then I read on here and now I'm confused. Can I get some ELI5 advice on expanding my storage?", "Author": "u/LoliSukhoi", "Content": "All I want is additional storage space for archiving files off of my PC that has some protection against drive failure. What is the simplest method of achieving this? Until recently I used a Synology 4 bay NAS but I've outgrown it and since I never used 99% of the features of a NAS, I decided to get a 10 bay IcyBox DAS (I think it's a DAS? It connects via USBC) that I saw on sale along with 4 24TB Seagate drives. My idea was to then use Windows Storage Spaces to create a pool and mirror the drives to add redundancy. But then I saw a lot of people on here really don't like Storage Spaces which sent me down a rabbit hole of Googling and reading threads which involved a million acronyms and other words and names I don't understand and now I'm thoroughly confused. Like what's the difference between a DAS and a JBOD? What's a Home Lab? What's ZFS? (I'm just asking these to show the kind of research mess I've ended up in, don't feel the need to spend paragraphs answering them.) Is Storage Spaces good enough for what I want? Will it easily allow me to add more drives to the pool down the line? Did I buy the wrong thing? Should I have done something else?"},
{"Title": "how to check are files good or damaged?", "Author": "u/oO0_", "Content": "i do simple backups. But how can i check if current files are good? Is there way to check (hash or what) that file should have unless i change file by myself or by some software? Most files on Windows/NTFS"},
{"Title": "For a folder of media files, is it possible to get a directory list  with media encoded type? AV1 H264 etc.?", "Author": "u/danuser8", "Content": "Looking for solution in windows"},
{"Title": "i've got a couple HP 4k13c5 HBAs from salvaged machines, which include the onboard cache. but i only see folks praising LSI HBAs on here. should i spend the $35 on a LSI ?", "Author": "u/ImaginaryCheetah", "Content": "\"when in rome\", as they say. for someone generally ignorant on good verses bad HBAs, i assume there's a reason for the cult status of LSI, but is there any reason to not use the free HP ones i've got ? having onboard cache seems like a useful feature, but might not be worth the potential issue with HP drivers."},
{"Title": "Encryption tool that could output smaller chunks of encrypted blocks?", "Author": "u/SuperElephantX", "Content": "Long story short, need some tools that could securely do file/folder encryption and output as smaller chunks of encrypted blocks. Benefits: Easier to track small changes without hashing 1TB of data. Easier to sync to the cloud. Had been using VeraCrypt for ages and worked fine every time (Container's too large). Also heard of Cryptomator , but the reputation of the software made me stepped away from it (Corruption issues) Any solutions that you guys could recommend would fit the need of mine? Backed by secure algo like AES Auto splitting chunks into smaller file sizes. Content change only affects the necessary encrypted blocks and not all of them."},
{"Title": "Migrating Drobo user saying thanks for the help, and one more question", "Author": "u/Splitsurround", "Content": "Thanks for the guidance towards my new DAS, friends. It took a while for me to really understand the options, software raid vs hardware, how many bays I needed, the limitations of  it, etc. So I've settled on the Terramaster 6 bay DAS  D6-320. I will put 6 16TB drives in it. I'll be using Softraid with it, thus my next question: since softraid doesn't do raid 6 yet, what flavor would you recommend for me? I need at least one disk failure protection, but wouldn't hate two. I don't understand what some of the raid options mean by \"at least one disk protection\" fwiw. Looking for the most usable space with the most comprehensive \"realistic\" protection Thanks again, and looking forward to getting moved to something stable"},
{"Title": "PSA: 14TB WD Red Plus (WD140EFGX) back in stock at Western Digital", "Author": "u/SimplyDown", "Content": "https://www.westerndigital.com/products/internal-drives/wd-red-plus-sata-3-5-hdd?sku=WD140EFGX Just posting in case anyone else has been trying to get their hands on one since they've all but disappeared over the past year.  If you're wondering why it's anything special, it's the largest reasonably quiet HD if you're looking for a low noise solution compared to the typical enterprise HD. I logged into my WD account and added 5 to my cart.  Checked back a couple of times over a few hours and WD ended up emailing me a 15% off one time discount code to finish checking out.  I'm also receiving 18% rewards on Western Digital through Capital One Shopping (after window shopping Western Digital site a few times on the app).  Not sure if this is normal or some special push for Memorial Day Weekend.   But final price for me ends up at $220.15 + tax plus $39.63 in rewards per HD."},
{"Title": "Is cloud storage/sync and online backup redundant?", "Author": "u/AreaMuppet", "Content": "I've used Google Drive sparingly for years but recently upgraded to a Google One 2TB account to give me and my family more storage and syncing capability across devices, as well as allowing me to consolidate all of the files on my laptop - which includes all of the \"family management\" documents, photos, videos, etc. - in one place rather than split up so I could stay under my 15GB free Google account limit. For years I have also used online backup. I've used IDrive in recent years (I've seen some complaints here, but I've found it to be fine, if a bit clunky) and Carbonite before that. But with my IDrive renewal coming up, I have wondered if having both Google Drive and IDrive are redundant. Years ago, I recall reading that online backup was still necessary because cloud storage wasn't reliable. But it seems like cloud storage has come a long way in terms of reliability and features, like file versioning and retrieval in case of loss. I've looked extensively and been surprised not to find much information on that question. Is there a consensus? (I should add that I also back up to an external hard drive so I do maintain multiple copies.)"},
{"Title": "Just bought a 1TB Samsung 980 PRO. Should I return it?", "Author": "u/greencalx8", "Content": "So basically I read about all the firmware issues and kinda scared me. I did my research inside this subreddit before buying it, but now I have doubts, even though I shouldn't have any problems since it was manufactured in October 2023. What are your thoughts? Should I return it? I didn't have any more options available (cost me 88â‚¬). Forgot to say that I'm using it with Immich to back up all my pictures, and it will be on 24/7. Thanks in advance."},
{"Title": "Transferring iCloud to PC not going very well", "Author": "u/NegativeNiek", "Content": "So my iCloud has been full for a while, and I wanted to transfer everything to my local pc storage, so I requested my data as rar files, and eventually got those, unzipped them and everything, but now I just have these 6 25gb folders with images and videos from random dates (because the dates didn't save and I don't know how to sort on EXIF data) and I don't even know how to find the most recent saved image inbetween them all.. Could someone maybe help me figure out how to get all these dates back into place?"},
{"Title": "Alternative to paperless-ngx for archiving magazines?", "Author": "u/GibtNixZuSehen", "Content": "Is there any good alternative to paperless-ngx for archiving >5000 magazines and books in pdf format? Would be nice to have full text search over all documents. I'm running an paperless-ngx container on my proxmox server but several pdfs take ages to ocr and indexing. Still have >4500 files to go and the files I added so far took several days to complete."},
{"Title": "Why is SSD price increasing from last year?", "Author": "u/AntarcticNightingale", "Content": "I was looking at the SSD I bought last year and found them generally increased from the past year by almost $100. What caused this? Just curious. https://camelcamelcamel.com/product/B09VLJ7VBM https://camelcamelcamel.com/product/B0BHZQGN26"},
{"Title": "30+ usb hard drives, 20+ years of hoarding.", "Author": "u/password_too_short", "Content": "so i've amassed just over 30 usb 2.5\" hard drives. i'm in my mid 30's and i use them to store basically every tv show and move i've ever watched. and yep, i do re-watch stuff. none of them have failed yet. except my music drive that makes a high pitched whine sometimes and lots of beeps...yeah i might replace that...but haven't yet. for some reason i don't hoard games i've played though.  i seem to value movies and tv and music more. anyone else with a shelf of drives? what do you store?"},
{"Title": "The WD PWL water torture THUMP!", "Author": "u/Prime_Objective666", "Content": "I was shocked at how loud the WD Pre-emptive Wear Levelling thump was! My 18Tb Elements got sent back. I then got in touch with Seagate and asked how they compere - I thought I would share my experience here in May 2024 and hope it helps: Me: Hi - I just got shocked about a WD 18Tb PWL thump every 5 seconds which I cannot cope with soundwise. So I am looking to Seagate to see if you offer 14+TB drives that do not have regular PWL thump sounds. I dont care about access noise nor motor whirr noise but PWL every 5 secs is like Chinese Water Torture! The ONLY issue is regular Pre-emptive Wear Levelling noises - and NOT access or drive noise Seagate Agent: Upon checking with the concern technical team, our technical team requested to inform you that, you will get a minimum sound with a lite vibration in our drives. As this sound will occur in all the hard drives, It will not be unusual like WD and this sound will not be disturbing to you. Other than that you can buy any Seagate drives according to the capacity and usage. PWL sounds will not occur in Seagate. But you can hear some minimum sort of sound and vibration can be heard slightly when the drive is in a silent room. You need not worry, you will not face any PWL sound issue in Seagate. Me: Second question: what is the best large drive that copes with regular power cycles? In my usage, the drive will be attached to a USB external dock cabinet which powers down the drive after 20 mins of non-use. This will occur 20 times a day, Out of the SG range, what copes best? Seagate Agent: Upon checking, Seagate does not recommend to use the drives in the USB dock or using splitters or any other external connectors. We recommend and request you to use the drives directly to the host device."},
{"Title": "Having trouble copying video files to flash drive.", "Author": "u/Fantastic-Notice-756", "Content": "So, a while back I got a 2TB flash drive from a friend. I needed a new one because I was having corruption trouble with the 1TB flash drive that I'd been using. Anything I backed up to the 1TB drive would be corrupted upon reconnecting the drive to my laptop  There wasn't much on the 1TB drive, but I did have a lot of videos downloaded from YT that were clean. I thought the corruption might've been carried over, so I decided to alphabetically re download all the videos onto the 2 TB drive. Re downloading the videos going fine until I hit a bit of a snag. I'd gotten to the R and S part of the video title alphabet. After I pasted some video files onto the 2TB drive I noticed there was external corruption being caused. I know this because I tried to paste a rar of the pc version of neversoft's spider-man game onto the drive (It was the lightest game I could think of) and the archive ended up being corrupt. I can tell when it's a clean paste, because the rar will paste slowly and the green meter resembles a set of waves, but when it's a corrupted paste, it'll look more like a straight line. I took those files off the drive, re pasted the archive and the rar was just fine. I re downloaded the files, some of them were copied to the drive and didn't cause any corruption, but others would still cause corruption. I even pasted some video files from the 1TB drive and some of them would be fine, but others would have the same corruption result. I don't know if this is malware related or if it has something to do with the allocation unit size, But I have no idea how to fix it. And what's worse is that I have no way of knowing which files the 2TB drive will accept or not. I can't back anything up until this is fixed and my desktop doesn't have enough space. Any help at all will be appreciated."},
{"Title": "Is it better to sleep drives or leave them spinning in between infrequent uses?", "Author": "u/Streetamp_Lamoose", "Content": "I'm pretty new to using larger drive arrays and this seemed like the kind of place where people have things figured out. My apologies if this is basic knowledge, but my internet searching didn't seem to reveal a consensus. I have a Mac Studio that I use as my main machine with 14+ Tb (out of 36Tb total) of mostly RAW photos stored on a 4-Bay OWC Thunderbay enclosure (Raid 5 via SoftRaid). This enclosure is directly connected to the mac via a Thunderbolt 3 (type-C connector) to the Mac. What drives me crazy is that, for reasons unknown to me, all day everyday, the drives seem to be spinning up, spinning down, spinning up, spinning down. Rest for 2 minutes, then spin up again and down again. I had been sleeping the Mac and allowing it to sleep my disks, but no matter what settings I tried it always woke from sleep about every 5 minutes, and would spin up the drive array, only to immediately go back to sleep and deactivate them. After about 2 years of this, My Hitachi drives eventually had catastrophic failures, and corrupted HFS+ volumes. I was able to recover most of the data from various backup schemes. I decided to replace the 4 drives with Segate Datacenter Exos Enterprise drives, thinking that I would change my approach and never allow the mac to sleep (just the displays). I hoped this would mean that the drives would stay powered up and always spinning, so hopefully avoiding the massive wear and tear I was getting before through constant power cycling and mechanical acceleration. My plan seems to be failing. I never sleep the Mac, and I've tried every setting I can find in the UI menus. Even if I haven't touched the machine in days, it is still spinning up, spinning down, spinning up, spinning down every minute or so. And to make matters worse, these Enterprise drives are LOUD. My poor daughter, who tries to sleep in the room next to my computer can hear the drives all night \"Growling\" at her, crunching, and spasming and spinning loudly, even when I haven't asked to access the volumes in days. I know modern OS'es run things in the background, like indexing, Time Machine, search optimization, caching stuff, etc. But is it really so constant? And why can't I seem to prevent the drives from ever turning off, which I assume (power waste aside) would be better for the health of the drives? I just want to have a large Raid 5 drive directly connected to my desktop machine, with a fast connection for things like 8k video editing, and not feel like the drives are constantly power cycling themselves into an early grave. I've got to be missing something obvious here..."},
{"Title": "I want to be able to host my own personal â€œshort videoâ€ collection and be able to scroll like TikTok.", "Author": "u/Unlikelyusername3", "Content": "New to this. Iâ€™m looking for a program that allows me to stream videos from my pc similar to Plex, but with scrollable videos more similar to TikTok for some shortâ€¦.clips. I havenâ€™t seen what Iâ€™m looking for on google so I figured Iâ€™d ask the experts here. :)"},
{"Title": "DVD Case Storage", "Author": "u/Parking-Mirror3283", "Content": "Hi guys, might be a little bit strange and not sure if there's anywhere better to ask this, but i was wondering if anybody had an idea of where to get a solid storage box for full DVD sized cases as opposed to just the discs themselves? Have a bunch of PS2/Xbox/360 games collected and i want to do something better than just having them loose in a box but have had no luck finding anything."},
{"Title": "Whatâ€™s your drive health checklist?", "Author": "u/Educational_Tap4663", "Content": "Iâ€™m planning on shucking a group of external hard drives to use in a homelab server Iâ€™m building. When you get a new, used, or refurbished hard drive, what are you go-to commands, tools, or processes to assess its health and performance? Iâ€™ve found several software applications, but I thought the experts of DataHoarder would know best. Cheers"},
{"Title": "Help me understand size of Pool and Volume", "Author": "u/Hakihiko", "Content": "Premise: early I tried to post in the qnap subreddit, but there isn't much movement there in this moment. So I take a chance to ask here, hoping someone can help me. Hi, I have a qnap TS-653A with a the firmware version 5.1.5. Recently I switched from 5x6TB to 5x18TB using RAID 5. Following guides online, I expanded the pool and now in the specifications it tells me that I have a Capacity of 65.45TB, Allocated 64.21TB and Unallocated 1.24TB. My DataVol1 (I have just a single volume), after the resize to increase the volume to the maximum possible size, has a Capacity of 62.99TB. Is it normal to have 1.24TB more on the pool that I cannot use to increase further the capacity of my volume? Can I use it in any way? Also, since today I used always a Thick volume. If I'll change to Thin, will I be able to use more space (aka that unallocated portion of the pool)? Thank you"},
{"Title": "Finding duplicate folders and partially-duplicate folders", "Author": "u/big-sugoi", "Content": "I've got 18 years of unorganized backups that I'm trying to sift through (deleting 95+% of it all). A lot of folders have 2-4 duplicates in bizarre completely different directories, usually hastily backed up without care when reinstalling windows every few years. Sometimes 1 duplicate was partially sifted through to clean up. Please god what do I do to make this easier. I don't want automated deletion I wanna do that part myself."},
{"Title": "Risks by using a old pc as a Nas", "Author": "u/Juaniesteban", "Content": "I have a lot of old labtops i dont use and some very old computers. Can i posibly use them as a nas? Is there a possibility that the computer dies?"},
{"Title": "Program to renew magnetic strength of bits on HDD (to counteract datarot)", "Author": "u/unable_To_Username", "Content": "Bing Chat always comes up with programs that are for data recovery or whiping without traces... but I just want a program that: Read binary... and Write binary at the exact same spot you've red it. (to newly write the magnetic information, to counteract loss of magentic field strength over years)"},
{"Title": "End of en era, Google Drive", "Author": "u/Boogertwilliams", "Content": "My old unlimited Google Drive went read only about a year ago. Been keeping the 100TB plex library still active as read only. Now finally, the message came it will be deleted in 30 days. Was a good run. But so many options now anyway, I wont really miss it. Plex Debrid, Stremio + Torrentio. Only the rarer stuff I had found over the years I put somewhere else. So long and thanks for all the fish. I have learned that I dont HAVE to be a datahoarder, lol. Just \"get what I need when I need it\""},
{"Title": "Tired of Dropbox, need something cheaper (long term)", "Author": "u/Archmaxy", "Content": "Hi guys, I've been paying monthly for Dropbox for several years now, and it's becoming a bit annoying. I am also buying a house soon and I don't mind to make a single purchase for something and be cheaper off in the long, as opposed to keeping my monthly outgoing higher because of a subscription like this. I would very much appreciate any advice to migrate to a NAS for example, as I've heard that's the most comon alternative. But I really don't know. Thanks in advance!"},
{"Title": "Help with Hard drive expansion SAS expander or new HBA card", "Author": "u/Cody112233X", "Content": "So I currently have the HBA LSI Logic SAS9211-8I  and have decided to buy more drives 16 in total I was wondering should i just buy an SAS expander like the Intel RES2SV240 or should I just buy the HBA LSI 9300-16I The HBA card is suppose to come in It mode already I think I would need to update the firmware of  the SAS expander. What would be the better option most of the drives are exos 20 tb on Truenas scale I dont know if that makes a difference. Thank for the Input."},
{"Title": "New vs used hard drive", "Author": "u/linbeg", "Content": "How do you guys select the terabyte HDD Storage that you guys use? Do you guys always buy new or are you OK with used? And if used are you worried about the driving breaking or what are the steps that you guys take to make sure that they used one will be ok and reliable? Would love to get yâ€™allâ€™s input !"},
{"Title": "To 'stress test' a new Sandisk Extreme PRO 2TB portable SSD to check whether it's defective, before actually using it?", "Author": "u/SaarN", "Content": "I bought this drive last year because it was on sale and came with 5 years of warranty (and they're way cheaper now due to the bad rep), and I've just found out it has HW issues. Although WD website states that my drive's firmware doesn't need to be updated, I've seen people complaining about losing data while using up-to-date drives. How should I tackle this? It's supposed to be a portable backup drive - but it's totally worthless if I can't trust it to keep my files safe. I thought of testing the drive before putting it into actual use. Like fill it up a few times and then formatting - over and over. I don't want to kill it by exceeding the writes\\reads limits, but to make it sweat a bit and check whether it's trustworthy to be used as a.. portable drive. What do you guys think? Any suggestions?"},
{"Title": "Question: Backing up a Drivepool Pool", "Author": "u/Tsusai", "Content": "Since a Drivepool pool doesn't support VSS, how are you backing up your data? I've currently spun up a Veeam B&R VM (for synthetic full), backing up the individual volumes (can't do file backup), but the incrementals in the past 3 days since it was setup have been oddly high for not adding any data to the volumes (30-60GB). I've pondered using my licensed copy of macrium to perform synthetic file-based incrementals (complains about VSS but it carries on), but I wonder what everyone else with Drivepool does Edit: I currently have Crashplan as a online backup.  This would be for an offsite in the same city setup (currently using Truenas & Tailscale)"},
{"Title": "Encryption tool that could output smaller chunks of encrypted blocks?", "Author": "u/SuperElephantX", "Content": "Long story short, need some tools that could securely do file/folder encryption and output as smaller chunks of encrypted blocks. Benefits: Easier to track small changes without hashing 1TB of data. Easier to sync to the cloud. Had been using VeraCrypt for ages and worked fine every time (Container's too large). Also heard of Cryptomator , but the reputation of the software made me stepped away from it (Corruption issues) Any solutions that you guys could recommend would fit the need of mine? Backed by secure algo like AES Auto splitting chunks into smaller file sizes. Content change only affects the necessary encrypted blocks and not all of them."},
{"Title": "Migrating Drobo user saying thanks for the help, and one more question", "Author": "u/Splitsurround", "Content": "Thanks for the guidance towards my new DAS, friends. It took a while for me to really understand the options, software raid vs hardware, how many bays I needed, the limitations of  it, etc. So I've settled on the Terramaster 6 bay DAS  D6-320. I will put 6 16TB drives in it. I'll be using Softraid with it, thus my next question: since softraid doesn't do raid 6 yet, what flavor would you recommend for me? I need at least one disk failure protection, but wouldn't hate two. I don't understand what some of the raid options mean by \"at least one disk protection\" fwiw. Looking for the most usable space with the most comprehensive \"realistic\" protection Thanks again, and looking forward to getting moved to something stable"},
{"Title": "PSA: 14TB WD Red Plus (WD140EFGX) back in stock at Western Digital", "Author": "u/SimplyDown", "Content": "https://www.westerndigital.com/products/internal-drives/wd-red-plus-sata-3-5-hdd?sku=WD140EFGX Just posting in case anyone else has been trying to get their hands on one since they've all but disappeared over the past year.  If you're wondering why it's anything special, it's the largest reasonably quiet HD if you're looking for a low noise solution compared to the typical enterprise HD. I logged into my WD account and added 5 to my cart.  Checked back a couple of times over a few hours and WD ended up emailing me a 15% off one time discount code to finish checking out.  I'm also receiving 18% rewards on Western Digital through Capital One Shopping (after window shopping Western Digital site a few times on the app).  Not sure if this is normal or some special push for Memorial Day Weekend.   But final price for me ends up at $220.15 + tax plus $39.63 in rewards per HD."},
{"Title": "Is cloud storage/sync and online backup redundant?", "Author": "u/AreaMuppet", "Content": "I've used Google Drive sparingly for years but recently upgraded to a Google One 2TB account to give me and my family more storage and syncing capability across devices, as well as allowing me to consolidate all of the files on my laptop - which includes all of the \"family management\" documents, photos, videos, etc. - in one place rather than split up so I could stay under my 15GB free Google account limit. For years I have also used online backup. I've used IDrive in recent years (I've seen some complaints here, but I've found it to be fine, if a bit clunky) and Carbonite before that. But with my IDrive renewal coming up, I have wondered if having both Google Drive and IDrive are redundant. Years ago, I recall reading that online backup was still necessary because cloud storage wasn't reliable. But it seems like cloud storage has come a long way in terms of reliability and features, like file versioning and retrieval in case of loss. I've looked extensively and been surprised not to find much information on that question. Is there a consensus? (I should add that I also back up to an external hard drive so I do maintain multiple copies.)"},
{"Title": "Just bought a 1TB Samsung 980 PRO. Should I return it?", "Author": "u/greencalx8", "Content": "So basically I read about all the firmware issues and kinda scared me. I did my research inside this subreddit before buying it, but now I have doubts, even though I shouldn't have any problems since it was manufactured in October 2023. What are your thoughts? Should I return it? I didn't have any more options available (cost me 88â‚¬). Forgot to say that I'm using it with Immich to back up all my pictures, and it will be on 24/7. Thanks in advance."},
{"Title": "Transferring iCloud to PC not going very well", "Author": "u/NegativeNiek", "Content": "So my iCloud has been full for a while, and I wanted to transfer everything to my local pc storage, so I requested my data as rar files, and eventually got those, unzipped them and everything, but now I just have these 6 25gb folders with images and videos from random dates (because the dates didn't save and I don't know how to sort on EXIF data) and I don't even know how to find the most recent saved image inbetween them all.. Could someone maybe help me figure out how to get all these dates back into place?"},
{"Title": "Alternative to paperless-ngx for archiving magazines?", "Author": "u/GibtNixZuSehen", "Content": "Is there any good alternative to paperless-ngx for archiving >5000 magazines and books in pdf format? Would be nice to have full text search over all documents. I'm running an paperless-ngx container on my proxmox server but several pdfs take ages to ocr and indexing. Still have >4500 files to go and the files I added so far took several days to complete."},
{"Title": "Why is SSD price increasing from last year?", "Author": "u/AntarcticNightingale", "Content": "I was looking at the SSD I bought last year and found them generally increased from the past year by almost $100. What caused this? Just curious. https://camelcamelcamel.com/product/B09VLJ7VBM https://camelcamelcamel.com/product/B0BHZQGN26"},
{"Title": "30+ usb hard drives, 20+ years of hoarding.", "Author": "u/password_too_short", "Content": "so i've amassed just over 30 usb 2.5\" hard drives. i'm in my mid 30's and i use them to store basically every tv show and move i've ever watched. and yep, i do re-watch stuff. none of them have failed yet. except my music drive that makes a high pitched whine sometimes and lots of beeps...yeah i might replace that...but haven't yet. for some reason i don't hoard games i've played though.  i seem to value movies and tv and music more. anyone else with a shelf of drives? what do you store?"},
{"Title": "The WD PWL water torture THUMP!", "Author": "u/Prime_Objective666", "Content": "I was shocked at how loud the WD Pre-emptive Wear Levelling thump was! My 18Tb Elements got sent back. I then got in touch with Seagate and asked how they compere - I thought I would share my experience here in May 2024 and hope it helps: Me: Hi - I just got shocked about a WD 18Tb PWL thump every 5 seconds which I cannot cope with soundwise. So I am looking to Seagate to see if you offer 14+TB drives that do not have regular PWL thump sounds. I dont care about access noise nor motor whirr noise but PWL every 5 secs is like Chinese Water Torture! The ONLY issue is regular Pre-emptive Wear Levelling noises - and NOT access or drive noise Seagate Agent: Upon checking with the concern technical team, our technical team requested to inform you that, you will get a minimum sound with a lite vibration in our drives. As this sound will occur in all the hard drives, It will not be unusual like WD and this sound will not be disturbing to you. Other than that you can buy any Seagate drives according to the capacity and usage. PWL sounds will not occur in Seagate. But you can hear some minimum sort of sound and vibration can be heard slightly when the drive is in a silent room. You need not worry, you will not face any PWL sound issue in Seagate. Me: Second question: what is the best large drive that copes with regular power cycles? In my usage, the drive will be attached to a USB external dock cabinet which powers down the drive after 20 mins of non-use. This will occur 20 times a day, Out of the SG range, what copes best? Seagate Agent: Upon checking, Seagate does not recommend to use the drives in the USB dock or using splitters or any other external connectors. We recommend and request you to use the drives directly to the host device."},
{"Title": "Having trouble copying video files to flash drive.", "Author": "u/Fantastic-Notice-756", "Content": "So, a while back I got a 2TB flash drive from a friend. I needed a new one because I was having corruption trouble with the 1TB flash drive that I'd been using. Anything I backed up to the 1TB drive would be corrupted upon reconnecting the drive to my laptop  There wasn't much on the 1TB drive, but I did have a lot of videos downloaded from YT that were clean. I thought the corruption might've been carried over, so I decided to alphabetically re download all the videos onto the 2 TB drive. Re downloading the videos going fine until I hit a bit of a snag. I'd gotten to the R and S part of the video title alphabet. After I pasted some video files onto the 2TB drive I noticed there was external corruption being caused. I know this because I tried to paste a rar of the pc version of neversoft's spider-man game onto the drive (It was the lightest game I could think of) and the archive ended up being corrupt. I can tell when it's a clean paste, because the rar will paste slowly and the green meter resembles a set of waves, but when it's a corrupted paste, it'll look more like a straight line. I took those files off the drive, re pasted the archive and the rar was just fine. I re downloaded the files, some of them were copied to the drive and didn't cause any corruption, but others would still cause corruption. I even pasted some video files from the 1TB drive and some of them would be fine, but others would have the same corruption result. I don't know if this is malware related or if it has something to do with the allocation unit size, But I have no idea how to fix it. And what's worse is that I have no way of knowing which files the 2TB drive will accept or not. I can't back anything up until this is fixed and my desktop doesn't have enough space. Any help at all will be appreciated."},
{"Title": "Is it better to sleep drives or leave them spinning in between infrequent uses?", "Author": "u/Streetamp_Lamoose", "Content": "I'm pretty new to using larger drive arrays and this seemed like the kind of place where people have things figured out. My apologies if this is basic knowledge, but my internet searching didn't seem to reveal a consensus. I have a Mac Studio that I use as my main machine with 14+ Tb (out of 36Tb total) of mostly RAW photos stored on a 4-Bay OWC Thunderbay enclosure (Raid 5 via SoftRaid). This enclosure is directly connected to the mac via a Thunderbolt 3 (type-C connector) to the Mac. What drives me crazy is that, for reasons unknown to me, all day everyday, the drives seem to be spinning up, spinning down, spinning up, spinning down. Rest for 2 minutes, then spin up again and down again. I had been sleeping the Mac and allowing it to sleep my disks, but no matter what settings I tried it always woke from sleep about every 5 minutes, and would spin up the drive array, only to immediately go back to sleep and deactivate them. After about 2 years of this, My Hitachi drives eventually had catastrophic failures, and corrupted HFS+ volumes. I was able to recover most of the data from various backup schemes. I decided to replace the 4 drives with Segate Datacenter Exos Enterprise drives, thinking that I would change my approach and never allow the mac to sleep (just the displays). I hoped this would mean that the drives would stay powered up and always spinning, so hopefully avoiding the massive wear and tear I was getting before through constant power cycling and mechanical acceleration. My plan seems to be failing. I never sleep the Mac, and I've tried every setting I can find in the UI menus. Even if I haven't touched the machine in days, it is still spinning up, spinning down, spinning up, spinning down every minute or so. And to make matters worse, these Enterprise drives are LOUD. My poor daughter, who tries to sleep in the room next to my computer can hear the drives all night \"Growling\" at her, crunching, and spasming and spinning loudly, even when I haven't asked to access the volumes in days. I know modern OS'es run things in the background, like indexing, Time Machine, search optimization, caching stuff, etc. But is it really so constant? And why can't I seem to prevent the drives from ever turning off, which I assume (power waste aside) would be better for the health of the drives? I just want to have a large Raid 5 drive directly connected to my desktop machine, with a fast connection for things like 8k video editing, and not feel like the drives are constantly power cycling themselves into an early grave. I've got to be missing something obvious here..."},
{"Title": "I want to be able to host my own personal â€œshort videoâ€ collection and be able to scroll like TikTok.", "Author": "u/Unlikelyusername3", "Content": "New to this. Iâ€™m looking for a program that allows me to stream videos from my pc similar to Plex, but with scrollable videos more similar to TikTok for some shortâ€¦.clips. I havenâ€™t seen what Iâ€™m looking for on google so I figured Iâ€™d ask the experts here. :)"},
{"Title": "DVD Case Storage", "Author": "u/Parking-Mirror3283", "Content": "Hi guys, might be a little bit strange and not sure if there's anywhere better to ask this, but i was wondering if anybody had an idea of where to get a solid storage box for full DVD sized cases as opposed to just the discs themselves? Have a bunch of PS2/Xbox/360 games collected and i want to do something better than just having them loose in a box but have had no luck finding anything."},
{"Title": "Whatâ€™s your drive health checklist?", "Author": "u/Educational_Tap4663", "Content": "Iâ€™m planning on shucking a group of external hard drives to use in a homelab server Iâ€™m building. When you get a new, used, or refurbished hard drive, what are you go-to commands, tools, or processes to assess its health and performance? Iâ€™ve found several software applications, but I thought the experts of DataHoarder would know best. Cheers"},
{"Title": "Help me understand size of Pool and Volume", "Author": "u/Hakihiko", "Content": "Premise: early I tried to post in the qnap subreddit, but there isn't much movement there in this moment. So I take a chance to ask here, hoping someone can help me. Hi, I have a qnap TS-653A with a the firmware version 5.1.5. Recently I switched from 5x6TB to 5x18TB using RAID 5. Following guides online, I expanded the pool and now in the specifications it tells me that I have a Capacity of 65.45TB, Allocated 64.21TB and Unallocated 1.24TB. My DataVol1 (I have just a single volume), after the resize to increase the volume to the maximum possible size, has a Capacity of 62.99TB. Is it normal to have 1.24TB more on the pool that I cannot use to increase further the capacity of my volume? Can I use it in any way? Also, since today I used always a Thick volume. If I'll change to Thin, will I be able to use more space (aka that unallocated portion of the pool)? Thank you"},
{"Title": "Finding duplicate folders and partially-duplicate folders", "Author": "u/big-sugoi", "Content": "I've got 18 years of unorganized backups that I'm trying to sift through (deleting 95+% of it all). A lot of folders have 2-4 duplicates in bizarre completely different directories, usually hastily backed up without care when reinstalling windows every few years. Sometimes 1 duplicate was partially sifted through to clean up. Please god what do I do to make this easier. I don't want automated deletion I wanna do that part myself."},
{"Title": "Risks by using a old pc as a Nas", "Author": "u/Juaniesteban", "Content": "I have a lot of old labtops i dont use and some very old computers. Can i posibly use them as a nas? Is there a possibility that the computer dies?"},
{"Title": "Program to renew magnetic strength of bits on HDD (to counteract datarot)", "Author": "u/unable_To_Username", "Content": "Bing Chat always comes up with programs that are for data recovery or whiping without traces... but I just want a program that: Read binary... and Write binary at the exact same spot you've red it. (to newly write the magnetic information, to counteract loss of magentic field strength over years)"},
{"Title": "End of en era, Google Drive", "Author": "u/Boogertwilliams", "Content": "My old unlimited Google Drive went read only about a year ago. Been keeping the 100TB plex library still active as read only. Now finally, the message came it will be deleted in 30 days. Was a good run. But so many options now anyway, I wont really miss it. Plex Debrid, Stremio + Torrentio. Only the rarer stuff I had found over the years I put somewhere else. So long and thanks for all the fish. I have learned that I dont HAVE to be a datahoarder, lol. Just \"get what I need when I need it\""},
{"Title": "Tired of Dropbox, need something cheaper (long term)", "Author": "u/Archmaxy", "Content": "Hi guys, I've been paying monthly for Dropbox for several years now, and it's becoming a bit annoying. I am also buying a house soon and I don't mind to make a single purchase for something and be cheaper off in the long, as opposed to keeping my monthly outgoing higher because of a subscription like this. I would very much appreciate any advice to migrate to a NAS for example, as I've heard that's the most comon alternative. But I really don't know. Thanks in advance!"},
{"Title": "Help with Hard drive expansion SAS expander or new HBA card", "Author": "u/Cody112233X", "Content": "So I currently have the HBA LSI Logic SAS9211-8I  and have decided to buy more drives 16 in total I was wondering should i just buy an SAS expander like the Intel RES2SV240 or should I just buy the HBA LSI 9300-16I The HBA card is suppose to come in It mode already I think I would need to update the firmware of  the SAS expander. What would be the better option most of the drives are exos 20 tb on Truenas scale I dont know if that makes a difference. Thank for the Input."},
{"Title": "New vs used hard drive", "Author": "u/linbeg", "Content": "How do you guys select the terabyte HDD Storage that you guys use? Do you guys always buy new or are you OK with used? And if used are you worried about the driving breaking or what are the steps that you guys take to make sure that they used one will be ok and reliable? Would love to get yâ€™allâ€™s input !"},
{"Title": "To 'stress test' a new Sandisk Extreme PRO 2TB portable SSD to check whether it's defective, before actually using it?", "Author": "u/SaarN", "Content": "I bought this drive last year because it was on sale and came with 5 years of warranty (and they're way cheaper now due to the bad rep), and I've just found out it has HW issues. Although WD website states that my drive's firmware doesn't need to be updated, I've seen people complaining about losing data while using up-to-date drives. How should I tackle this? It's supposed to be a portable backup drive - but it's totally worthless if I can't trust it to keep my files safe. I thought of testing the drive before putting it into actual use. Like fill it up a few times and then formatting - over and over. I don't want to kill it by exceeding the writes\\reads limits, but to make it sweat a bit and check whether it's trustworthy to be used as a.. portable drive. What do you guys think? Any suggestions?"},
{"Title": "Question: Backing up a Drivepool Pool", "Author": "u/Tsusai", "Content": "Since a Drivepool pool doesn't support VSS, how are you backing up your data? I've currently spun up a Veeam B&R VM (for synthetic full), backing up the individual volumes (can't do file backup), but the incrementals in the past 3 days since it was setup have been oddly high for not adding any data to the volumes (30-60GB). I've pondered using my licensed copy of macrium to perform synthetic file-based incrementals (complains about VSS but it carries on), but I wonder what everyone else with Drivepool does Edit: I currently have Crashplan as a online backup.  This would be for an offsite in the same city setup (currently using Truenas & Tailscale)"},
{"Title": "[DIY DAS USB-C enclosure] SATA to USB adapter with no power?", "Author": "u/jfromeo", "Content": "I am planning to build a DIY DAS with external USB-C connection to host, just like some solutions out there, for example the IcyBox IB-3810-C31 https://preview.redd.it/diy-das-usb-c-enclosure-sata-to-usb-adapter-with-no-power-v0-1vx9qmg1jy1d1.jpg I know the way to go for data integrity is SATA/SAS controller/HBA, but USB-C is enough for my needs (JBOD with 1-2 simultaneous read access to mkv files, no writes at all). I have a spare SilverStone DS380B with its 8-bay hotswap backplane, along with a SilverStone SFX 500W Gold power supply. I plan to power on the system with a Supermicro CSE-PTJBOD card, with power output for the 3x120mm fans I have installed in the case. https://preview.redd.it/diy-das-usb-c-enclosure-sata-to-usb-adapter-with-no-power-v0-18q935qyjy1d1.jpg But I cannot find a way to transform the SATA outputs of the backplane to USB inputs in a USB hub which I plan to install inside the case. All the cables and interfaces SATA-to-USB I find, include the power adaptor, which does not fit the backplane SATA connections of the backplane. I would need 8 x cable adapter like the Startech one, but without the SATA power part, does it exist? https://preview.redd.it/diy-das-usb-c-enclosure-sata-to-usb-adapter-with-no-power-v0-z2zpg6mbky1d1.jpg Thanks in advance."},
{"Title": "Update Synology OS if you're using it. Several CVE's", "Author": "u/PaganLinuxGeek", "Content": "If you're using a Synology NAS, update. Several CVE posted with some issues on the OS."},
{"Title": "Migrating from Windows based NAS to Unraid", "Author": "u/CodeJBDA", "Content": "Hi All, I am moving my whole server from a windows 11 base to unRaid. I am currently using the program call drivepool to pool my 8hdds together so that Plex can look at one location. My worry is that when had to reinstall windows I had a ton of issues with ownership of the files. I am hoping to avoid that when going to unraid. Any thoughts?"},
{"Title": "What is the best method to download all saved media on Reddit in 2024?", "Author": "u/Fxxxk2023", "Content": "Hello, I want to backup all media I saved on my Reddit account. Sadly it seems as a lot of options died with Reddits crackdown on API use. What is the best way to automatically download media from all saved posts on my account?"},
{"Title": "This USB flash drive can only store 8KB of data, but will last you 200 years", "Author": "u/tzfld", "Content": "No content"},
{"Title": "best array type for me?", "Author": "u/FearlessENT33", "Content": "i currently have a server running omv, with a zfs pool with a 4tb and 2x 2tb, for a total of 8tb however there is no parity or backup so if a drive fails it all goes kaput. which means replacing- i want to upgrade my storage and have a better solution for potential drive failures, while potentially being expandable in the future. most of my data is movies / tv / music that can be reacquired, critical data is small and is backed up. my budget isnâ€™t massive, iâ€™ve only just started working full time, so have potentially been looking at 3x 8tb drives in a raid-z1 array, for 16tb useable. being able to add in another drive in the future would also be good, and i think raidz1 would allow this, but hopefully by that point i can just afford a completely new server. is this a good plan? or would there be a better array i could do? any advice would be much appreciated, thanks"},
{"Title": "My community didnâ€™t quite appreciate my new data hoarder case. I geeked out over 14 modular drive bays. Lol", "Author": "u/MikeTheTech", "Content": "No content"},
{"Title": "Will burning a 1080p video on a DVD-R still make it autoplay when opened on PC?", "Author": "u/Skidbladmir", "Content": "So I know that in order for it to be played on common DVD players the video needs to be encoded in a specific format (with a max 720p resolution) but if I just want the video to be played on normal PCs will that allow me to burn a high resolution video onto the disk?"},
{"Title": "Explain Enterprise SSDs", "Author": "u/Culbrelai", "Content": "Alright so I donâ€™t understand how this works. 8tb consumer nvme pcie4 m.2 ssds run the gamut from $850 to about $1100. Sabrent Rocket, Team, and so on. They are all about 7000mbs sequential write, 6600 read. 8tb is the max size as of 2024 it seems. There are also SATA ssds which also top out at 8tb and are massively slower than the NVME, but about $620 for a Samsung QVO 8tb. Meanwhile https://www.amazon.com/SOLIDIGM-D5-P5430-15-36-TB-Intensive/dp/B0C6BR89C2/ref=mp_s_a_1_8?crid=BBQOQ2PIYQDC&dib=eyJ2IjoiMSJ9.ylEfhDF9TJGI_8EwgLEBHUXoDzf17amHeqJMISzMk2x7tf7hXg1PgheTCstANtyVpEAD28qRj_TYx1CvOTh1hZbsgSteo1rgvqG8UQugG6-wnRrhgmph4dr-4AxHVAP6ebCn0J4ZQ83fvJcw2ZFnLBcaagyvJ_amnEjwjrB_Jszlx1cPmEOZmVoXaHjaUgaY839fH40SQWInIifesfAEag.o2h7SYjhTQoKgyvfpAGEtn7Mu2zDsfs4Gn3XrXFZKfQ&dib_tag=se&keywords=Solidigm&qid=1716304607&sprefix=solidigm%2Caps%2C115&sr=8-8 This enterprise drive is double the capacity of the sabrent, for $1500 instead of the $2200 youâ€™d need in m.2 to match. Half as fast in writes. Would take one less m.2 slot than the dual m.2 solution and therefore more lanes could be used elsewhere. Would need a relatively cheap m.2/u.2 adapter. Has higher durability ratings and longer warranties than the consumer options. Why are high capacity SSDs only offered to enterprises? I donâ€™t get it. Is there no consumer demand for SSDs greater than 8tb? Whatâ€™s the catch? Iâ€™m close to just buying one of these and replacing 3 m.2 steam game drives with it."},
{"Title": "Which software RAIDs allow triple parity?", "Author": "u/reddit_faa7777", "Content": "Out of all the software raids, which ones allow having 3 (or maybe 4) parity drives, amongst like 16+ drives? I'm thinking of doing this on Windows 11. I won't be using Linux as it's easier installing my VPN on Windows. I'm not a huge fan of Snapraid because..... when doing important tasks I like to use a GUI."},
{"Title": "Long-term storage and organization of CD/DVD/B BLU-RAY.", "Author": "u/KaleMercer", "Content": "I have a massive collection of movies, music, TV shows, and assorted disc-based media. There is little to no resale values so I'm not even interested in trying, But at the same time I'm not just going to chuck them in the trash is that would just be wasteful And I paid for these. Looking for suggestions, opinions, and or recommendations for organization/ storage of the discs of themselves. I want to get them out of their cases as they take up a massive amount of space. I was thinking about just getting one of the large disks cases but was wondering if there was anything more interesting?"},
{"Title": "Kiwix wikipedia zim file ends up being corrupted no matter what", "Author": "u/kaczynski_machine", "Content": "Hi guys, I have recently gained an interest to download alal of wikipedia just in case. So i chose to use Kiwix using the .zim file from: https://download.kiwix.org/zim/wikipedia/ ,  en_all_maxi_2024-01.zim. Downloading it is all fine and great, And in fact once it is downloaded on either of my computers (Yes, I tried on both my laptop and on my desktop.) I am able to open it using Kiwix and read it. But obviously I want to keep the zim file safe so I can have it ready at a moments notice without needing to download it every time... So I've tried to put it on my SSD. SO I guess the issue I'm having is when I try to copy the .zim file onto my disk, it seemingly went fine with no obvious mistakes. But then I disconnect the external ssd from my Desktop and connect it to my laptop to open it there (my way of testing if it is being kept safe on the disk.) and everytime some specific new error shows up when I try to open it in Kiwix. Sometimes its outright corrupted, sometimes it's missing articles. Like putting it on the SSD corrupted the .zim file somehow. Which would be a bummer because this external ssd  is new and works like a charm usually, its just somehow not working for this specific file. A few notes: I have already tried compressing the file into .7r or .zip. Same type of corruption occurs after interacting with the SSD. The SSD: 1TB, connected to my computer using a usb-c to USB cable. it is external. I already ran \"chkdsk D: /f\"  for the disk to check for any bad sectors, but the program reported zero bad sectors and said everything is fine. I have also tried to reformat the drive to both NTFS and exFat. Nothing works. I'm so dissapointed. And feel stupid. Please help me, thank you! Have a pleasant day."},
{"Title": "Mechanical Engineer looking for other DataHoarders for Engineering Knowledge", "Author": "u/Liizam", "Content": "Hi there, Iâ€™m new to data hoarding. Iâ€™ve noticed some of the design guides and other useful info disappearing from the internet and being places behind paid pdfs sites. I would love to connect to other engineers on here to see what we can do. I have a pile of knowledge Iâ€™ve been collection throughout the years."},
{"Title": "High capacity HDD for workstation- surveillance drive? Enterprise? ~20+ tb", "Author": "u/theseawoof", "Content": "Noticed that these drives tend to be the ones that easily have up to 24tb or so. I need a large HDD of around 20-24tb to hold my music and video software, VSTs, samples, video footage and assets, etc. Right now my case is maxed out with HDDs and I've had two consumer Seagate drives fail within recent years. I am willing to spend the ~$500 for one drive of high capacity if there is no downside. Someone recommended WD Purple, saying that surveillance drives hold up to constant usage, but someone else mentioned that they have a slightly lower read speed. What would you choose for something if high capacity and longevity that is going to hold up to usage and not just general storage?"},
{"Title": "How to extract raw files from multiple DVDs at the same time?", "Author": "u/TRUE_BIT", "Content": "I have approx 6k DVDs that need the raw files extracted from them. These aren't movies, just acting as storage media. They will be transferred locally or to a network share. I was thinking about getting multiple DVD to USB devices (5-8, depending on what would be most performant). I feel like I would be at the mercy of the USB transfer speeds. I'm not sure if USB-C will be available. Is there a better way to do this? Is there an optimal way to get the most out of my transfer speeds? A coworker suggested looking into leveraging VMs but I don't see how that would help."},
{"Title": "Any 16TB SSD Options Out There?", "Author": "u/ShiitakeTheMushroom", "Content": "I'm looking for a 16TB or larger SSD, but haven't been able to find anything larger than 8TB. Any good options out there that folks have been using?"},
{"Title": "WD My Book Duo eating drives :(", "Author": "u/No_Reflection1510", "Content": "I have had this 16TB My Book Duo (2x 8TB drives mirrored) for about 4 years. I only use it to store old photos and video that I don't want to lose but also for Time Machine backups. A few months ago one ofthe drives died so I got a replacement (see below). The raid rebuilt itself in a few days and I thought it was golden again. A week or two goes by and the other drive dies. So I get another of the same drive replace/rebuild and figured NOW I'm golden. Fast fwd to now, I've got another drive failure. This is getting pretty old.  So I guess I'm just looking for some advice. Should I keep trusting this WD enclosure and get yet another replacement drive? Should I abandon it and switch to something else? I really just don't want to lose this data. I'd like to just shove all this data into Google Drive and be done with external enclosures for this but ~8TB is a lot to upload and would take my little uplink weeks or months to do so and it seems likely that my one remaining drive would fail in that timeframe :-( Replacement drives were both: Western Digital 8TB WD Red Plus NAS Internal Hard Drive HDD - 5640 RPM, SATA 6 Gb/s, CMR, 256 MB Cache, 3.5\" - WD80EFPX"},
{"Title": "I want to automatically back up the contents of one external hard drive to another in a different house. Which software would let me do this?", "Author": "u/PradleyBitts", "Content": "I don't really want sync, i.e I don't want something that gets deleted on one drive to get deleted on another. Just want all the content added to one hard drive backed up to another in a different house automatically."},
{"Title": "backing up folders to S3 (compatible software), weekly, with terminal", "Author": "u/leurs247", "Content": "Hi all, I have some important files (not much atm, about 5GB) I want to upload to a (secure) online storage (cheap if possible). I already have the data on my local machine (macbook pro) and an external harddrive, but I want an online storage solution as well. I have used aws S3 before (for my work) so I have some knowledge. I could make a bucket to store the data. Is there any interesting (free) software I can use (on a mac) to backup some folders to s3. I have mountainduck on my laptop but only in trial. Another possible solution is to create a bash script to upload specific folders to s3 (not necessarily aws, could also be digitalocean) on an regular basis. I could possibly make a script with rclone, is this a good usecase for this? I could add a cronjob (like every sunday at 01AM) but my laptop is probably in sleep mode at that time, how can I create a script that runs at sunday 01AM OR when I first start my laptop after that time? Thanks!"},
{"Title": "Data recovery on failed drives - is it ever practical or reasonably priced for a regular joe?", "Author": "u/riftwave77", "Content": "I have two old drives that failed years ago that i've been keeping around on the off chance that HDD recovery ever gets cheap enough for me to consider it. am I just wasting my time?"},
{"Title": "i've got a J4025 w/8GB memory i want to use for a backup NAS. is TrueNAS CORE viable, or do i need lighter weight freeBSD/openZFS combo ? no other services planned, but wanting compression & encryption", "Author": "u/ImaginaryCheetah", "Content": "title says it all :) any of you folks running TrueNAS on a J4025, or does the little guy not have enough juice ?"},
{"Title": "Help: Scaling Up My Storage Rig: From Synology DS920+ to Custom Build with Hot Swap", "Author": "u/Doh_facepalm_admin", "Content": "I've hit a bit of a snag in my data hoarding journey and need some advice. I started off with a Synology DS920+ in 2020, equipped it with 4x6 TB drives, and soon found myself upgrading to 4x10 TB drives as my media and container needs grew. I've been heavily using PLEX, SONARR, RADARR, among other containers, and it's been fantastic so far. However, I'm now planning to dive into 4K transcoding, enhance my setup with more RAM for network labs, and maybe even host a gaming server. The demand on my current system is skyrocketing, and once again, I'm running out of space. I'm contemplating building a custom PC since the price seems comparable to getting an 8-bay Synology unit, but I'm really hung up on finding the right case. I'd prefer something that isn't a tower due to space constraints and aesthetics, but most importantly, I need it to support hot swapping. As my data hoarding isn't slowing down any time soon, I want to easily add more drives and replace any that fail without shutting down the system. Has anyone here tackled a similar upgrade or built their own high-capacity, hot-swappable storage system? I'd love to hear about your experiences, particularly about your choice of cases and any setup tips you might have for someone looking to transition from a NAS to a custom build. Thanks in advance for any suggestions or insights!"},
{"Title": "Need desperate help recovering an old photo", "Author": "u/dmc1155", "Content": "First of all i don't know if this is the correct subreddit to ask my question, but i've been desperately searching for my old motorola razer for more than a year now and i've finally found it. This razer phone of mine has a picture of my dog that i am very desperate to recover ever since she passed. The little information i remember about the picture is that it was set as a wallpaper and i wanted to know if it is possible to extract it from the phone someway. The phone turns on and i get greeted with an unskippable \"insert sim\" pop up. Is there any program where i can download the phone's contents or maybe something like the phone's nand where i can then find the wallpaper and save it?"},
{"Title": "38% of webpages that existed in 2013 are no longer accessible a decade later", "Author": "u/retrac1324", "Content": "No content"},
{"Title": "Did I make a mistake getting the WUH721816ALE6L1 not the ALE04 or is the drive from ServerPartDeals bad?", "Author": "u/mlgSD", "Content": "I'm posting this here as you all seem to know about these Ultrastar datacenter drives. Did I make a mistake getting the WUH721816ALE6L1 not the ALE04 or is the drive from ServerPartDeals bad? The L1 is a \"self-encrypting drive\". It's supposed to be \"refurbished\" by WDC and has a new WDC label that says \"Recertified 12 NOV 2023\" and P/N 0F24861  FW:870. I previously bought two 14TB  WUH721414ALE604 drives from Amazon before I learned of ServerPartDeals.com here (thanks guys!). I ran Spinrite 6.1 Level 5 on both. Each took about almost a week to do. Level 5 checks every sector, recovers (if possible) unreadable data, inverts it, writes it, reads it, verifies it, then rewrites and re-verifies each sector. One of them was fine and the other had some bad sectors at the very end so I returned it. I was thinking I should have bought a 16TB one, so after returning the 14TB to Amazon, I bought the WUH721816ALE6L1 from ServerPartDeals on Friday and got it today, Monday. Free 2nd Day Air. Very cool! I especially liked that they say it's \"Manufacturer Recertified\". Spinrite can see the drive and reads it's configuration from the drive electronics. It knows it's 16TB and how many bytes and sectors it has. It tests every drive it can recognize to be sure it can read and write sectors on each drive. However when it tests the 16TB drive to verify it can read and write the drive's sectors, that fails. I've tried this on two different systems with the same result. I did a quick test in Linux trying to 'dd' another drive to it, but it complained it can't access the drive saying that maybe it is DRM'd. The same 'dd' command works just fine with the 14TB drive. Is there something I need to do to the drive to make it work or is it just bad and needs to be returned? My apologies if I should post this elsewhere. WDC Recertified HC550 16TB WUH721816A:EL1 fails Spinrite 6.1 data transfer verification"},
{"Title": "[DIY DAS USB-C enclosure] SATA to USB adapter with no power?", "Author": "u/jfromeo", "Content": "I am planning to build a DIY DAS with external USB-C connection to host, just like some solutions out there, for example the IcyBox IB-3810-C31 https://preview.redd.it/diy-das-usb-c-enclosure-sata-to-usb-adapter-with-no-power-v0-1vx9qmg1jy1d1.jpg I know the way to go for data integrity is SATA/SAS controller/HBA, but USB-C is enough for my needs (JBOD with 1-2 simultaneous read access to mkv files, no writes at all). I have a spare SilverStone DS380B with its 8-bay hotswap backplane, along with a SilverStone SFX 500W Gold power supply. I plan to power on the system with a Supermicro CSE-PTJBOD card, with power output for the 3x120mm fans I have installed in the case. https://preview.redd.it/diy-das-usb-c-enclosure-sata-to-usb-adapter-with-no-power-v0-18q935qyjy1d1.jpg But I cannot find a way to transform the SATA outputs of the backplane to USB inputs in a USB hub which I plan to install inside the case. All the cables and interfaces SATA-to-USB I find, include the power adaptor, which does not fit the backplane SATA connections of the backplane. I would need 8 x cable adapter like the Startech one, but without the SATA power part, does it exist? https://preview.redd.it/diy-das-usb-c-enclosure-sata-to-usb-adapter-with-no-power-v0-z2zpg6mbky1d1.jpg Thanks in advance."},
{"Title": "Update Synology OS if you're using it. Several CVE's", "Author": "u/PaganLinuxGeek", "Content": "If you're using a Synology NAS, update. Several CVE posted with some issues on the OS."},
{"Title": "Migrating from Windows based NAS to Unraid", "Author": "u/CodeJBDA", "Content": "Hi All, I am moving my whole server from a windows 11 base to unRaid. I am currently using the program call drivepool to pool my 8hdds together so that Plex can look at one location. My worry is that when had to reinstall windows I had a ton of issues with ownership of the files. I am hoping to avoid that when going to unraid. Any thoughts?"},
{"Title": "What is the best method to download all saved media on Reddit in 2024?", "Author": "u/Fxxxk2023", "Content": "Hello, I want to backup all media I saved on my Reddit account. Sadly it seems as a lot of options died with Reddits crackdown on API use. What is the best way to automatically download media from all saved posts on my account?"},
{"Title": "This USB flash drive can only store 8KB of data, but will last you 200 years", "Author": "u/tzfld", "Content": "No content"},
{"Title": "best array type for me?", "Author": "u/FearlessENT33", "Content": "i currently have a server running omv, with a zfs pool with a 4tb and 2x 2tb, for a total of 8tb however there is no parity or backup so if a drive fails it all goes kaput. which means replacing- i want to upgrade my storage and have a better solution for potential drive failures, while potentially being expandable in the future. most of my data is movies / tv / music that can be reacquired, critical data is small and is backed up. my budget isnâ€™t massive, iâ€™ve only just started working full time, so have potentially been looking at 3x 8tb drives in a raid-z1 array, for 16tb useable. being able to add in another drive in the future would also be good, and i think raidz1 would allow this, but hopefully by that point i can just afford a completely new server. is this a good plan? or would there be a better array i could do? any advice would be much appreciated, thanks"},
{"Title": "My community didnâ€™t quite appreciate my new data hoarder case. I geeked out over 14 modular drive bays. Lol", "Author": "u/MikeTheTech", "Content": "No content"},
{"Title": "Will burning a 1080p video on a DVD-R still make it autoplay when opened on PC?", "Author": "u/Skidbladmir", "Content": "So I know that in order for it to be played on common DVD players the video needs to be encoded in a specific format (with a max 720p resolution) but if I just want the video to be played on normal PCs will that allow me to burn a high resolution video onto the disk?"},
{"Title": "Explain Enterprise SSDs", "Author": "u/Culbrelai", "Content": "Alright so I donâ€™t understand how this works. 8tb consumer nvme pcie4 m.2 ssds run the gamut from $850 to about $1100. Sabrent Rocket, Team, and so on. They are all about 7000mbs sequential write, 6600 read. 8tb is the max size as of 2024 it seems. There are also SATA ssds which also top out at 8tb and are massively slower than the NVME, but about $620 for a Samsung QVO 8tb. Meanwhile https://www.amazon.com/SOLIDIGM-D5-P5430-15-36-TB-Intensive/dp/B0C6BR89C2/ref=mp_s_a_1_8?crid=BBQOQ2PIYQDC&dib=eyJ2IjoiMSJ9.ylEfhDF9TJGI_8EwgLEBHUXoDzf17amHeqJMISzMk2x7tf7hXg1PgheTCstANtyVpEAD28qRj_TYx1CvOTh1hZbsgSteo1rgvqG8UQugG6-wnRrhgmph4dr-4AxHVAP6ebCn0J4ZQ83fvJcw2ZFnLBcaagyvJ_amnEjwjrB_Jszlx1cPmEOZmVoXaHjaUgaY839fH40SQWInIifesfAEag.o2h7SYjhTQoKgyvfpAGEtn7Mu2zDsfs4Gn3XrXFZKfQ&dib_tag=se&keywords=Solidigm&qid=1716304607&sprefix=solidigm%2Caps%2C115&sr=8-8 This enterprise drive is double the capacity of the sabrent, for $1500 instead of the $2200 youâ€™d need in m.2 to match. Half as fast in writes. Would take one less m.2 slot than the dual m.2 solution and therefore more lanes could be used elsewhere. Would need a relatively cheap m.2/u.2 adapter. Has higher durability ratings and longer warranties than the consumer options. Why are high capacity SSDs only offered to enterprises? I donâ€™t get it. Is there no consumer demand for SSDs greater than 8tb? Whatâ€™s the catch? Iâ€™m close to just buying one of these and replacing 3 m.2 steam game drives with it."},
{"Title": "Which software RAIDs allow triple parity?", "Author": "u/reddit_faa7777", "Content": "Out of all the software raids, which ones allow having 3 (or maybe 4) parity drives, amongst like 16+ drives? I'm thinking of doing this on Windows 11. I won't be using Linux as it's easier installing my VPN on Windows. I'm not a huge fan of Snapraid because..... when doing important tasks I like to use a GUI."},
{"Title": "Long-term storage and organization of CD/DVD/B BLU-RAY.", "Author": "u/KaleMercer", "Content": "I have a massive collection of movies, music, TV shows, and assorted disc-based media. There is little to no resale values so I'm not even interested in trying, But at the same time I'm not just going to chuck them in the trash is that would just be wasteful And I paid for these. Looking for suggestions, opinions, and or recommendations for organization/ storage of the discs of themselves. I want to get them out of their cases as they take up a massive amount of space. I was thinking about just getting one of the large disks cases but was wondering if there was anything more interesting?"},
{"Title": "Kiwix wikipedia zim file ends up being corrupted no matter what", "Author": "u/kaczynski_machine", "Content": "Hi guys, I have recently gained an interest to download alal of wikipedia just in case. So i chose to use Kiwix using the .zim file from: https://download.kiwix.org/zim/wikipedia/ ,  en_all_maxi_2024-01.zim. Downloading it is all fine and great, And in fact once it is downloaded on either of my computers (Yes, I tried on both my laptop and on my desktop.) I am able to open it using Kiwix and read it. But obviously I want to keep the zim file safe so I can have it ready at a moments notice without needing to download it every time... So I've tried to put it on my SSD. SO I guess the issue I'm having is when I try to copy the .zim file onto my disk, it seemingly went fine with no obvious mistakes. But then I disconnect the external ssd from my Desktop and connect it to my laptop to open it there (my way of testing if it is being kept safe on the disk.) and everytime some specific new error shows up when I try to open it in Kiwix. Sometimes its outright corrupted, sometimes it's missing articles. Like putting it on the SSD corrupted the .zim file somehow. Which would be a bummer because this external ssd  is new and works like a charm usually, its just somehow not working for this specific file. A few notes: I have already tried compressing the file into .7r or .zip. Same type of corruption occurs after interacting with the SSD. The SSD: 1TB, connected to my computer using a usb-c to USB cable. it is external. I already ran \"chkdsk D: /f\"  for the disk to check for any bad sectors, but the program reported zero bad sectors and said everything is fine. I have also tried to reformat the drive to both NTFS and exFat. Nothing works. I'm so dissapointed. And feel stupid. Please help me, thank you! Have a pleasant day."},
{"Title": "Mechanical Engineer looking for other DataHoarders for Engineering Knowledge", "Author": "u/Liizam", "Content": "Hi there, Iâ€™m new to data hoarding. Iâ€™ve noticed some of the design guides and other useful info disappearing from the internet and being places behind paid pdfs sites. I would love to connect to other engineers on here to see what we can do. I have a pile of knowledge Iâ€™ve been collection throughout the years."},
{"Title": "High capacity HDD for workstation- surveillance drive? Enterprise? ~20+ tb", "Author": "u/theseawoof", "Content": "Noticed that these drives tend to be the ones that easily have up to 24tb or so. I need a large HDD of around 20-24tb to hold my music and video software, VSTs, samples, video footage and assets, etc. Right now my case is maxed out with HDDs and I've had two consumer Seagate drives fail within recent years. I am willing to spend the ~$500 for one drive of high capacity if there is no downside. Someone recommended WD Purple, saying that surveillance drives hold up to constant usage, but someone else mentioned that they have a slightly lower read speed. What would you choose for something if high capacity and longevity that is going to hold up to usage and not just general storage?"},
{"Title": "How to extract raw files from multiple DVDs at the same time?", "Author": "u/TRUE_BIT", "Content": "I have approx 6k DVDs that need the raw files extracted from them. These aren't movies, just acting as storage media. They will be transferred locally or to a network share. I was thinking about getting multiple DVD to USB devices (5-8, depending on what would be most performant). I feel like I would be at the mercy of the USB transfer speeds. I'm not sure if USB-C will be available. Is there a better way to do this? Is there an optimal way to get the most out of my transfer speeds? A coworker suggested looking into leveraging VMs but I don't see how that would help."},
{"Title": "Any 16TB SSD Options Out There?", "Author": "u/ShiitakeTheMushroom", "Content": "I'm looking for a 16TB or larger SSD, but haven't been able to find anything larger than 8TB. Any good options out there that folks have been using?"},
{"Title": "WD My Book Duo eating drives :(", "Author": "u/No_Reflection1510", "Content": "I have had this 16TB My Book Duo (2x 8TB drives mirrored) for about 4 years. I only use it to store old photos and video that I don't want to lose but also for Time Machine backups. A few months ago one ofthe drives died so I got a replacement (see below). The raid rebuilt itself in a few days and I thought it was golden again. A week or two goes by and the other drive dies. So I get another of the same drive replace/rebuild and figured NOW I'm golden. Fast fwd to now, I've got another drive failure. This is getting pretty old.  So I guess I'm just looking for some advice. Should I keep trusting this WD enclosure and get yet another replacement drive? Should I abandon it and switch to something else? I really just don't want to lose this data. I'd like to just shove all this data into Google Drive and be done with external enclosures for this but ~8TB is a lot to upload and would take my little uplink weeks or months to do so and it seems likely that my one remaining drive would fail in that timeframe :-( Replacement drives were both: Western Digital 8TB WD Red Plus NAS Internal Hard Drive HDD - 5640 RPM, SATA 6 Gb/s, CMR, 256 MB Cache, 3.5\" - WD80EFPX"},
{"Title": "I want to automatically back up the contents of one external hard drive to another in a different house. Which software would let me do this?", "Author": "u/PradleyBitts", "Content": "I don't really want sync, i.e I don't want something that gets deleted on one drive to get deleted on another. Just want all the content added to one hard drive backed up to another in a different house automatically."},
{"Title": "backing up folders to S3 (compatible software), weekly, with terminal", "Author": "u/leurs247", "Content": "Hi all, I have some important files (not much atm, about 5GB) I want to upload to a (secure) online storage (cheap if possible). I already have the data on my local machine (macbook pro) and an external harddrive, but I want an online storage solution as well. I have used aws S3 before (for my work) so I have some knowledge. I could make a bucket to store the data. Is there any interesting (free) software I can use (on a mac) to backup some folders to s3. I have mountainduck on my laptop but only in trial. Another possible solution is to create a bash script to upload specific folders to s3 (not necessarily aws, could also be digitalocean) on an regular basis. I could possibly make a script with rclone, is this a good usecase for this? I could add a cronjob (like every sunday at 01AM) but my laptop is probably in sleep mode at that time, how can I create a script that runs at sunday 01AM OR when I first start my laptop after that time? Thanks!"},
{"Title": "Data recovery on failed drives - is it ever practical or reasonably priced for a regular joe?", "Author": "u/riftwave77", "Content": "I have two old drives that failed years ago that i've been keeping around on the off chance that HDD recovery ever gets cheap enough for me to consider it. am I just wasting my time?"},
{"Title": "i've got a J4025 w/8GB memory i want to use for a backup NAS. is TrueNAS CORE viable, or do i need lighter weight freeBSD/openZFS combo ? no other services planned, but wanting compression & encryption", "Author": "u/ImaginaryCheetah", "Content": "title says it all :) any of you folks running TrueNAS on a J4025, or does the little guy not have enough juice ?"},
{"Title": "Help: Scaling Up My Storage Rig: From Synology DS920+ to Custom Build with Hot Swap", "Author": "u/Doh_facepalm_admin", "Content": "I've hit a bit of a snag in my data hoarding journey and need some advice. I started off with a Synology DS920+ in 2020, equipped it with 4x6 TB drives, and soon found myself upgrading to 4x10 TB drives as my media and container needs grew. I've been heavily using PLEX, SONARR, RADARR, among other containers, and it's been fantastic so far. However, I'm now planning to dive into 4K transcoding, enhance my setup with more RAM for network labs, and maybe even host a gaming server. The demand on my current system is skyrocketing, and once again, I'm running out of space. I'm contemplating building a custom PC since the price seems comparable to getting an 8-bay Synology unit, but I'm really hung up on finding the right case. I'd prefer something that isn't a tower due to space constraints and aesthetics, but most importantly, I need it to support hot swapping. As my data hoarding isn't slowing down any time soon, I want to easily add more drives and replace any that fail without shutting down the system. Has anyone here tackled a similar upgrade or built their own high-capacity, hot-swappable storage system? I'd love to hear about your experiences, particularly about your choice of cases and any setup tips you might have for someone looking to transition from a NAS to a custom build. Thanks in advance for any suggestions or insights!"},
{"Title": "Need desperate help recovering an old photo", "Author": "u/dmc1155", "Content": "First of all i don't know if this is the correct subreddit to ask my question, but i've been desperately searching for my old motorola razer for more than a year now and i've finally found it. This razer phone of mine has a picture of my dog that i am very desperate to recover ever since she passed. The little information i remember about the picture is that it was set as a wallpaper and i wanted to know if it is possible to extract it from the phone someway. The phone turns on and i get greeted with an unskippable \"insert sim\" pop up. Is there any program where i can download the phone's contents or maybe something like the phone's nand where i can then find the wallpaper and save it?"},
{"Title": "38% of webpages that existed in 2013 are no longer accessible a decade later", "Author": "u/retrac1324", "Content": "No content"},
{"Title": "Did I make a mistake getting the WUH721816ALE6L1 not the ALE04 or is the drive from ServerPartDeals bad?", "Author": "u/mlgSD", "Content": "I'm posting this here as you all seem to know about these Ultrastar datacenter drives. Did I make a mistake getting the WUH721816ALE6L1 not the ALE04 or is the drive from ServerPartDeals bad? The L1 is a \"self-encrypting drive\". It's supposed to be \"refurbished\" by WDC and has a new WDC label that says \"Recertified 12 NOV 2023\" and P/N 0F24861  FW:870. I previously bought two 14TB  WUH721414ALE604 drives from Amazon before I learned of ServerPartDeals.com here (thanks guys!). I ran Spinrite 6.1 Level 5 on both. Each took about almost a week to do. Level 5 checks every sector, recovers (if possible) unreadable data, inverts it, writes it, reads it, verifies it, then rewrites and re-verifies each sector. One of them was fine and the other had some bad sectors at the very end so I returned it. I was thinking I should have bought a 16TB one, so after returning the 14TB to Amazon, I bought the WUH721816ALE6L1 from ServerPartDeals on Friday and got it today, Monday. Free 2nd Day Air. Very cool! I especially liked that they say it's \"Manufacturer Recertified\". Spinrite can see the drive and reads it's configuration from the drive electronics. It knows it's 16TB and how many bytes and sectors it has. It tests every drive it can recognize to be sure it can read and write sectors on each drive. However when it tests the 16TB drive to verify it can read and write the drive's sectors, that fails. I've tried this on two different systems with the same result. I did a quick test in Linux trying to 'dd' another drive to it, but it complained it can't access the drive saying that maybe it is DRM'd. The same 'dd' command works just fine with the 14TB drive. Is there something I need to do to the drive to make it work or is it just bad and needs to be returned? My apologies if I should post this elsewhere. WDC Recertified HC550 16TB WUH721816A:EL1 fails Spinrite 6.1 data transfer verification"},
{"Title": "2U 4x3.5 hdd expansion 14\" depth?", "Author": "u/chinzw", "Content": "Does this exist? I've been searching everywhere, can't find anything that fits the bill. I need to add more storage to my server, my rack is 15\" deep, with plug i get about 14\" usable. Im looking to spend no more than 1000$ all in (no drives)."},
{"Title": "Samsung T7 help", "Author": "u/Creative_Emu3851", "Content": "No content"},
{"Title": "I built a self hosted version of AWS S3 using only open source technology and Raspberry Pis thats compatible with the official AWS S3 SDK", "Author": "u/Anthonyb-s3", "Content": "No content"},
{"Title": "Need advice about a DIY NAS", "Author": "u/-empty-head", "Content": "https://preview.redd.it/need-advice-about-a-diy-nas-v0-i1wla3ypmm1d1.jpg A couple months ago a friend of mine gave me his TERRAMASTER D4-300 drive enclosure, I was thinking about getting a SBC / mini pc, or something similar and turning it into a jankie DIY NAS. The hardware I currently have for the project is, 4TB HDD Ã— 4 and the drive enclosure My use case is pretty simple I just want mass networked bulk storage, with basic resilience In case of loss of a single drive. But I have a couple of questions about this Idea I've been having, befor I do this. I want to do this on a bit of a tight budget, so I would like some hardware suggestions. Is it actually going to work and be reasonably reliable? And any other thoughts or ideas would be very much appreciated?"},
{"Title": "Is the 2TB Seagate Barracuda 3.5 fine for my case?", "Author": "u/typcalthowawayacount", "Content": "I wanted to give a DIY Nas a try and it seems it's recommended to have a NAS specific hard drives like WD or Seagate RED, but that's if you're using it 24/7 which isn't my case. I'll be using to rarely to store videos and images on the fly, and my MOBO does support power on by PME which allows me to turn the PC on by PCI/PCIE LAn or modem card. So with that, a the 2TB Seagate Barracude should be fine then? The other hardware on my PC: MOBO: F1A55-M LX3 R2.0 CPU: A4-3400 RAM: 12 GB of DDR3 Storage: 500GB HDD (OS)"},
{"Title": "How does p2p connection actually works on CGNAT network.", "Author": "u/Mukun00", "Content": "Hello guys, I have recently found a p2p file transfer website https://send-anywhere.com/ . In Webrtc we can't able to transfer the files through p2p while connected with CGNAT connection, but this site allows p2p even with CGNAT connection, I wonder what protocol they been used to work with CGNAT connections. Do you guys any Idea which protocol or method they used to work on CGNAT connections. Thank in advance."},
{"Title": "Is this any good? WD Red WE30EFRX + Synology DS213j", "Author": "u/Germasiansensation", "Content": "No content"},
{"Title": "Old DIY Media Server is Kaput. Looking for Advice!", "Author": "u/HightopNinja", "Content": "Hi all, Last week, my beast of an old machine decided to give up the ghost. This machine served as both a media server (DVD/Blu-ray rips and music) and as a HTPC in my living room (used for watching media on the server and watching streaming services). I managed to pull the data off the drives and am currently running some tests on them to make sure there are no issues. (They are older drives, and quite small by today's standards). Thankfully, no data lost! I'm trying to figure out the best solution for myself going forward. Should I go NAS for storage and grab a mini-pc (N100 Beelink or similar) for the living room? Should I build a new HTPC/Server? (it would need to be somewhat cost effective) The main uses I need are: Sharing of media over our home network (in house only) I'd like to run a Jellyfin or Plex server to serve that media Data Storage (pictures, family movies, work projects) I'm not looking to share files remotely and there would be a max of 2 users accessing the data on our home network. Unfortunately, I wasn't able to salvage anything but the mechanical drives from the system. (MB was fried and video card was ancient). Supplies I already have but am not married to: Radeon 6800 XT (given from a friend's old system) 1TB NVME Drive - Western Digital Blue 3x 1TB Mechanical Drives - Western Digital Black -- Want to ditch these for something more robust 3TB Mechanical Drive - Western Digital Black -- Same as above Any thoughts/ideas would be greatly appreciated, as all my data is currently living on my current system and it's backed up on various external drives (not ideal). Thanks!"},
{"Title": "Which WD Elements External Desktop HDD should I buy for longevity?", "Author": "u/AntarcticNightingale", "Content": "[SOLVED!! Better buy an internal drive. Also here is my new question about which internal drive to pick .] Buying a large externally powered Desktop HDD will be for making a backup, before I invest a ton of time, money, and energy into a longtime NAS solution, it will be a quick solution for now.  (Yes I know the general advice that the best brand is having another copy regardless of brand. But with that said ...) Is it true for externally powered HDD, that Western Digital is more reliable in general than Seagate or any other brands? Is it true that it's better to get a WD Elements Desktop than a WD My Book because My Book had encryption which is kinda of a headache if we need to ever do data recovery? (Since I don't need my data to be encrypted.) So there are the following choices for the WD Elements Desktop : 6, 8, 10, 12, 14, 16, 18, 20, 22 TB. Given price is not a concern, which size would be the best for quality and longevity? Because I have read that if the data is too dense, it's more likely to break or something? I can buy multiple of the smaller size if smaller sizes are more reliable. Thanks!"},
{"Title": "NAS bay vs just build a new system?", "Author": "u/tddammo1", "Content": "Hi all, My current method is I have a pi 4 hooked up via USB 3.0 to an external 2TB seagate HDD. Eventually I know I want to expand, and my question is should I get a big (low power) PC and go that route? (E.g. something with 4-6 bays) or would something like a dedicated external NAS bay be better value for my $$$. Which do people generally recommend and why? Note: I do have a dedicated ML rig that does all my processing etc when converting and compressing videos, this system is STRICTLY hosting jellyfin"},
{"Title": "Transforming a QLC SSD to SLC for dramatically increased endurance", "Author": "u/johnklos", "Content": "No content"},
{"Title": "How to get the URL to this video? Nothing seems to be able to pick it up", "Author": "u/airkuroko", "Content": "Link: https://watchreplay.net/suns-vs-clippers-west-1st-round-game-1-april-16-2023/19471/ I'm trying to download this video of an NBA game. On this site is an embedded video with 3 sources to choose from on the top right. Source 1 and source 3 work. But I can't find the URL to the video for either source 1 or 3. I've played the video while having enabled multiple extensions on Chrome and Firefox, and none of these extensions can detect the link to the video. Does anyone know how to get the URL to this video?"},
{"Title": "Tool for renaming files/movies with common schema", "Author": "u/BlossomingPsyche", "Content": "Let's say you regularly have files with extraneous tags that you might want to remove (ie renaming something from MyDVDMovieRip1981/MyDVDMovieRip1981.x264.mkv and with perhaps a script /  command (I can do this on Os X with an application called Alfred) rename the folder, file and categorize it in the way I want (ie Horror, Scifi) so my Alien1994Bluray/AlienRip1994.mkv would get stripped of the tags and then renamed with a naming scheme like if I wanted to do a scifi rename on my mac I would type \"shift-command space to activate alfred then \"sort Alien 1984 1080p Scifi\" which then renames the selected folder to Alien [1080P] [SCI-FI] [USA] [1984] and the file to Alien 1984 1080P.mkv which then gets dropped into plex. Is there something that will let me rename files/folders with a 'format' like this for PC ? Kind of like automator or alfred or raycast for Os X ?"},
{"Title": "How can I extract and download ALL images from websites gallery?", "Author": "u/atribecallednet", "Content": "How can I extract and download ALL images from these WW2 sites below? https://albumwar2.com/ https://ww2db.com/photo.php"},
{"Title": "List of ALL COLORS of WD internal HDD drives: which ones have the best long term reliability? Is CMR or SMR more reliable? How about best RMP and cache for reliability?", "Author": "u/AntarcticNightingale", "Content": "I'm looking to buy a large (or several smaller) internal HDD for backups, before I invest a ton of time and money for a NAS solution, it will be a quick solution for now. (Yes I know the general advice that the best brand is having another copy regardless of brand. But with that said ...) I have no idea what all the colors of WD internal drives mean. Sometimes I see a color being trashed on this subreddit, but I don't remember. Is it true for externally powered HDD, that Western Digital is more reliable in general than other brands? If not, what is the most reliable brand and which internal drive should I get from it? Is it true that WD internal drives don't have encryption? Which is good for me because encryption is a headache if we ever need data recovery and I don't have any sensitive stuff that needs encryption. Is a faster RPM (7200) more reliable than a slower RPM (5400 or 5640)? Does cache size matter? Is it true that CMR is more reliable than SMR? I don't care about speed or cost and size is a minor concern (because I can always buy multiples of a good small drive). I just want to have the highest chance of my data not go poof. What are the best Western Digital internal drive colors in terms of long-term reliability? Do storage capacity and cache size (64, 128, or 256MB) affects reliability too? I have made the following list based on Amazon's best selling WD drives: WD Blue PC 3.5\" HDD 54,500 ratings, 4.6 average, 80% 5 star, 4% 1 star 8 TB with 5640 RMP and 256 MB cache WD80EAAZ 3-6 TB with 5400 RMP and 256 MB cache WD30EZAX, WD40EZAX, WD60EZAX, respectively 2 TB with 7200 RMP and 256 MB cache WD20EZBX WD Blue PC 3.5\" HDD I don't know why there is a second different link for WD Blue 23,600 ratings, 4.6 average, 81% 5 star, 4% 1 star 1, 3, 4 or 6 TB with 5400 RMP and 64 MB cache WD10EZRZ, WD30EZRZ, WD40EZRZ, WD60EZRZ 2 TB with 5400 RMP and 256 MB cache WD20EZAZ WD Black Performance Gaming 3.5\" HDD 14,700 ratings, 4.6 average, 80% 5 star, 6% 1 star 4, 6, or 10 TB with 7200 RPM and 256 MB cache WD4005FZBX, WD6003FZBX, WD101FZBX 6 or 8 TB with 7200 RPM and 128 MB cache WD6004FZWX, WD8002FZWX 1, or 2 TB with 7200 RPM and 64 MB cache WD1003FZEX, WD2003FZEX WD Purple Surveillance 3.5\" HDD 13,500 ratings, 4.6 average, 80% 5 star, 5% 1 star 14 TB with 7200 RMP and 512 MB cache 10 TB with 7200 RMP and 256 MB cache 8 TB with 5400 RMP and 256 MB cache 6 TB with 5400 RMP and 256 MB cache or 5640 RMP and 128 MB cache 1-4 TB with 5400 RMP and 64 or 256 MB cache WD Red Plus 3.5\" HDD 9,100 ratings, 4.6 average, 80% 5 star, 5% 1 star 12 TB with 7200 RMP and 512 MB cache WD120EFBX 10 TB with 7200 RMP and 256 MB cache 8 TB with 5640 RMP and 256 MB cache 6 TB with 5400 RMP and 256 MB cache or 5640 RMP and 128 MB cache 4 TB with 5640 RMP and 128 or 256 MB cache 2 or 3 TB with 5400 RMP and 128 MB cache 1 or 2 TB with 5400 RMP and 64 MB cache WD Red Pro 3.5\" HDD CMR 1,700 ratings, 4.5 average, 78% 5 star, 8% 1 star 14, 20, 22, or 24 TB with 7200 RMP and 512 MB cache WD240KFGX 4-16, or 18 TB with 7200 RMP and 256 MB cache 2 TB with 7200 RMP and 64 MB cache WD Purple Pro Surveillance 3.5\" HDD 630 ratings, 4.6 average, 81% 5 star, 7% 1 star 14 - 22 TB with 7200 RMP and 512 MB cache WD221PURP 8 - 12 TB with 7200 RMP and 256 MB cache WD Gold Enterprise 3.5\" HDD NO ratings! 8 TB with 7200 RMP and 256 MB cache WD8005FRYZ let me know if I missed any other reliable drives to compare, especially if it's better than any listed here, regardless of brand. I have no idea the purpose of so many colors is. I have no idea why some are marketed only for surveillance or gaming, or enterprise. As you can see, the ratings are very similar. Please let me know which one(s) is/are the most reliable! Thank you so much!!"},
{"Title": "Have you ever thought \"is this worth it?\"", "Author": "u/banisheduser", "Content": "I have some media files saved, but I am very unlikely to ever watch them. However, I know they're not available from the official site any more. I sort of feel a responsibility to keep them archived but at the same time, I do wonder why I am bothering. Have you ever felt this way about some data you've hoarded and what was the outcome?"},
{"Title": "Best way to â€œrefreshâ€ 750 TB of HDDs?", "Author": "u/Nautricity", "Content": "Recently got ahold of A LOT of FREE server equipment, with drives, resulting in me now having nearly a petabyte of total storage combined. Iâ€™m very new to RAIDs and server setups in general. My question for now is, whatâ€™s the best way to refresh these drives to achieve optimal performance before deploying them? Most of these drives have multiple 10s of thousands of hours on them. I have a mix of Apple Xserve RAID, Dell PowerEdge, HP ProLiant, Cisco UCS, etc branded servers and RAID arrays, ranging from 250 GB HDDs all the way up to 8 TB HDDs. Is there any sort of specific software thatâ€™s recommended to â€œrefurbishâ€ these drives in mass? Bonus points for macOS (64 bit) and OS X (32 bit) compatibility! Thank you all for any help as Iâ€™m very new to this!"},
{"Title": "How do you deal with replaced drives?", "Author": "u/DazedWithCoffee", "Content": "I've got a 4TB disk that just degraded my RAIDZ2 pool with 16 read errors. I've got a replacement ready and the appropriate backups already in place, so there's no risk of data loss. My question is this: what to do with this drive? I'd hate to toss it, but I don't particularly trust it anymore. Has anyone found a low-waste strategy for dealing with these things?"},
{"Title": "More info about the height of the new 6TB 2.5\" WD drive", "Author": "u/Far_Marsupial6303", "Content": "Posting for visibility as an addendum to this thread. https://www.reddit.com/r/DataHoarder/comments/1cu2301/finally_after_seven_years_a_6tb_25inch_external/ I was looking at the WD site and noticed they have other models beyond the WD Black external and there are spec sheets for them. They are not listed To those who wonder why the height is important, max 15mm height for 2.5\" drives has been the standard for may years and is the largest height allowed in 2.5\" enclosures. The 5TB MyPassport case is 19.2mm and the drive is 15mm. The 6TB is 20.6mm so the drive is probably 16 or 16.5mm to accommodate the additional platter. I suspect this may allow them to fit snuggly in some 15mm cases. For reference, the lowest height 3.5\" drive is 19.5mm. https://documents.westerndigital.com/content/dam/doc-library/en_us/assets/public/wd/product/external-storage/my_passport/my-passport-phdd-usb-c/data-sheet-my-passport-phdd-usb-c.pdf https://documents.westerndigital.com/content/dam/doc-library/en_us/assets/public/wd/product/external-storage/wd-black-p10-game-drive-for-xbox-usb-3-2-hdd/data-sheet-wd-black-p10-game-drive-for-xbox-usb-3-2-hdd.pdf Pricewise, the \"bargain\" ;-p, is the Elements SE at 169.99MSRP, vs $129.99 for the 5TB."},
{"Title": "Powering down archive pools when not in use.", "Author": "u/dtf_0", "Content": "I was wondering if any NAS hardware/software allows a user to configure powering-down drives/enclosures for storage pools designed for archiving. I am a robotics engineer. I have a lot of old sensor data that is used very infrequently. Whenever we release a new release, we run a simulation against old data to ensure there are no regressions. However, this data is only touched at max 30 days per year. Currently, I manually turn on my archive NAS when need, run my test, move any new data to the archive, and power down the NAS manually. I am curious if there is any hardware or software that can do this automatically. I am thinking of something like a disk enclosure or disk shelf that contains an 'archive pool.' When I access the archive pool, the enclosure powers up, attaches the pool to the NAS, and works as normal. Then, after some delay, the pool detaches and powers down. This is more about power consumption than wear and tear. I currently average about ten years of life out of my drives. They start their lives as constantly spinning drives in my primary NAS, and then when I replace those drives with larger ones, they move to the archive NAS, which runs infrequently."},
{"Title": "hgst 12tb refurbished not going standby", "Author": "u/zannare", "Content": "I recently purchased a refurbished HGST 12tb hard disk (HUH721212ALE601) to use as an additional parity disk in my Snapraid setup it seems to work well, the smart is ok and completed the sync without problems (about 23 hours) but I was unable to make it go to standby automatically I tried all the APM values â€‹â€‹from 1 to 127 (default was 254) but nothing changes I tried various settings of the time in which it should go to standy with smartctl, but nothing manual standy works with both smartctl and hdparm the hard disk makes a \"tick\" every 5 seconds https://vocaroo.com/148gEPXcUo36"},
{"Title": "Any Advice - Sabrent 5 bay docking station upgrade", "Author": "u/SCDUDELEX", "Content": "Current status:  Using the 5 Bay station... been working very well (and yes have a 6 Bay NAS as well, different functions in my system): I need to expand the bay capacity and assuming others have gotten to this point.  Question is, pros and cons of adding another Sabrent 5 bay and using the second USB C to connect the two.   Versus, retiring the 5 bay and swapping it out for the 10 bay model. Obviously it's nearly twice the cost to grab the 10 Bay, but that choice would leave me with the \"old 5 bay\"; to maybe do something with.   But is the single usb connection an advantage over the daisy chain of two 5 bays? Anything else I should consider? Thanks in advance..."},
{"Title": "Whatsapp ICloud Backup to PC?", "Author": "u/Lj_theoneandonly", "Content": "I've followed the guides on this sub for extracting whatsapp backup on android to pc with success. But i haven't seen as much useful discussion around trying to do the same with iPhone. I currently have a couple gb of whatsapp data in my icloud that I encrypted using the 64 digit key. I was looking for ways to take this encrypted backup out of my old iphone but the process doesn't seem to be straightforward. All i've found online were a bunch of third party sites like \"Dr Fone\" which several posts here have vouched for as a scam. So uhhh is there anything else I can do? I wouldn't mind jailbreaking the phone since it's an old one anyway. Thanks in advance for any help!"},
{"Title": "Plex server/build to run arrs", "Author": "Unknown author", "Content": "https://www.amazon.com/gp/product/B01NCESRJX/ref=ox_sc_saved_image_4?smid=A1ZYJ2SSCYE072&psc=1 https://www.amazon.com/gp/product/B07LBXP8KZ/ref=ox_sc_saved_image_7?smid=ADT2TM5FCS88L&psc=1 https://www.amazon.com/gp/product/B0BRQSWSFQ/ref=ox_sc_saved_image_3?smid=ATVPDKIKX0DER&th=1 https://www.amazon.com/Expansion-Controller-Adapter-Splitter-Windows/dp/B0CPC2YLBJ/ref=sr_1_5?crid=1743QI3GMOT9A&dib=eyJ2IjoiMSJ9.cwPeFFKR905GlWuzCZZ90fDEDW0gmLnNiz32tX2qxVe8tZk1FsE4pBGCKj6qeO6tJY7hPucioGRtIPanw0zE0gi5RtMrhvFxX_JO2Kas2r15AMg92x8Gxd-Dk2VoFGZR4S3qaY0eT8qEhvGRhMEPeihGtUY-SgqrSMbwC0HXDvLRvriQqdaXo5ZM624jCN9FzQG-EPAjqZQvu20tzyohQLYNaVmRnY8X6f7NXo1DuBuylBdl_tEeKaW9ih7Eb8qYtWbVANh1NDTPwdEMut_TgI7lizZptl1c0IrnWzthZe0.TlGNxlwUwl0RczFhkZsZcm0kIEEWIT7CccQg4wPB3FQ&dib_tag=se&keywords=sata%2Bexpansion%2Bcard%2Basm&qid=1716180089&sprefix=sata%2Bexpansion%2Bcard%2Basm%2Caps%2C133&sr=8-5&th=1 Already have the ram and psu and an m.2 drive, that cpu is more than plenty right? I will probably end up installing Linux to take advantage of proper tone mapping. any other recommendations?"},
{"Title": "SeaGate Expansion 2 Tb 12$ are it worth and what risk ?", "Author": "u/Merchant_Lawrence", "Content": "Hello everyone, i currently try set up my own nas for archive stuff and on tight budget around 55$ i found on local online shop offer seagate expansion (without mentioning what number or series) 2 tb for just 12 $, it have good rating, are it good choice for long term or it not good ? thanks. link on that local online shop (it say tosiba but option available are seagate) : https://shopee.co.id/Hardisk-Eksternal-Toshiba-2TB-1TB-HDD-USB-3.0-Portable-External-Hard-Drive-Hard-disk-i.997456609.22981243111?sp_atk=abc600c5-5d08-4044-ba64-5c73626f8ba5&xptdk=abc600c5-5d08-4044-ba64-5c73626f8ba5"},
{"Title": "2U 4x3.5 hdd expansion 14\" depth?", "Author": "u/chinzw", "Content": "Does this exist? I've been searching everywhere, can't find anything that fits the bill. I need to add more storage to my server, my rack is 15\" deep, with plug i get about 14\" usable. Im looking to spend no more than 1000$ all in (no drives)."},
{"Title": "Samsung T7 help", "Author": "u/Creative_Emu3851", "Content": "No content"},
{"Title": "I built a self hosted version of AWS S3 using only open source technology and Raspberry Pis thats compatible with the official AWS S3 SDK", "Author": "u/Anthonyb-s3", "Content": "No content"},
{"Title": "Need advice about a DIY NAS", "Author": "u/-empty-head", "Content": "https://preview.redd.it/need-advice-about-a-diy-nas-v0-i1wla3ypmm1d1.jpg A couple months ago a friend of mine gave me his TERRAMASTER D4-300 drive enclosure, I was thinking about getting a SBC / mini pc, or something similar and turning it into a jankie DIY NAS. The hardware I currently have for the project is, 4TB HDD Ã— 4 and the drive enclosure My use case is pretty simple I just want mass networked bulk storage, with basic resilience In case of loss of a single drive. But I have a couple of questions about this Idea I've been having, befor I do this. I want to do this on a bit of a tight budget, so I would like some hardware suggestions. Is it actually going to work and be reasonably reliable? And any other thoughts or ideas would be very much appreciated?"},
{"Title": "Is the 2TB Seagate Barracuda 3.5 fine for my case?", "Author": "u/typcalthowawayacount", "Content": "I wanted to give a DIY Nas a try and it seems it's recommended to have a NAS specific hard drives like WD or Seagate RED, but that's if you're using it 24/7 which isn't my case. I'll be using to rarely to store videos and images on the fly, and my MOBO does support power on by PME which allows me to turn the PC on by PCI/PCIE LAn or modem card. So with that, a the 2TB Seagate Barracude should be fine then? The other hardware on my PC: MOBO: F1A55-M LX3 R2.0 CPU: A4-3400 RAM: 12 GB of DDR3 Storage: 500GB HDD (OS)"},
{"Title": "How does p2p connection actually works on CGNAT network.", "Author": "u/Mukun00", "Content": "Hello guys, I have recently found a p2p file transfer website https://send-anywhere.com/ . In Webrtc we can't able to transfer the files through p2p while connected with CGNAT connection, but this site allows p2p even with CGNAT connection, I wonder what protocol they been used to work with CGNAT connections. Do you guys any Idea which protocol or method they used to work on CGNAT connections. Thank in advance."},
{"Title": "Is this any good? WD Red WE30EFRX + Synology DS213j", "Author": "u/Germasiansensation", "Content": "No content"},
{"Title": "Old DIY Media Server is Kaput. Looking for Advice!", "Author": "u/HightopNinja", "Content": "Hi all, Last week, my beast of an old machine decided to give up the ghost. This machine served as both a media server (DVD/Blu-ray rips and music) and as a HTPC in my living room (used for watching media on the server and watching streaming services). I managed to pull the data off the drives and am currently running some tests on them to make sure there are no issues. (They are older drives, and quite small by today's standards). Thankfully, no data lost! I'm trying to figure out the best solution for myself going forward. Should I go NAS for storage and grab a mini-pc (N100 Beelink or similar) for the living room? Should I build a new HTPC/Server? (it would need to be somewhat cost effective) The main uses I need are: Sharing of media over our home network (in house only) I'd like to run a Jellyfin or Plex server to serve that media Data Storage (pictures, family movies, work projects) I'm not looking to share files remotely and there would be a max of 2 users accessing the data on our home network. Unfortunately, I wasn't able to salvage anything but the mechanical drives from the system. (MB was fried and video card was ancient). Supplies I already have but am not married to: Radeon 6800 XT (given from a friend's old system) 1TB NVME Drive - Western Digital Blue 3x 1TB Mechanical Drives - Western Digital Black -- Want to ditch these for something more robust 3TB Mechanical Drive - Western Digital Black -- Same as above Any thoughts/ideas would be greatly appreciated, as all my data is currently living on my current system and it's backed up on various external drives (not ideal). Thanks!"},
{"Title": "Which WD Elements External Desktop HDD should I buy for longevity?", "Author": "u/AntarcticNightingale", "Content": "[SOLVED!! Better buy an internal drive. Also here is my new question about which internal drive to pick .] Buying a large externally powered Desktop HDD will be for making a backup, before I invest a ton of time, money, and energy into a longtime NAS solution, it will be a quick solution for now.  (Yes I know the general advice that the best brand is having another copy regardless of brand. But with that said ...) Is it true for externally powered HDD, that Western Digital is more reliable in general than Seagate or any other brands? Is it true that it's better to get a WD Elements Desktop than a WD My Book because My Book had encryption which is kinda of a headache if we need to ever do data recovery? (Since I don't need my data to be encrypted.) So there are the following choices for the WD Elements Desktop : 6, 8, 10, 12, 14, 16, 18, 20, 22 TB. Given price is not a concern, which size would be the best for quality and longevity? Because I have read that if the data is too dense, it's more likely to break or something? I can buy multiple of the smaller size if smaller sizes are more reliable. Thanks!"},
{"Title": "NAS bay vs just build a new system?", "Author": "u/tddammo1", "Content": "Hi all, My current method is I have a pi 4 hooked up via USB 3.0 to an external 2TB seagate HDD. Eventually I know I want to expand, and my question is should I get a big (low power) PC and go that route? (E.g. something with 4-6 bays) or would something like a dedicated external NAS bay be better value for my $$$. Which do people generally recommend and why? Note: I do have a dedicated ML rig that does all my processing etc when converting and compressing videos, this system is STRICTLY hosting jellyfin"},
{"Title": "Transforming a QLC SSD to SLC for dramatically increased endurance", "Author": "u/johnklos", "Content": "No content"},
{"Title": "How to get the URL to this video? Nothing seems to be able to pick it up", "Author": "u/airkuroko", "Content": "Link: https://watchreplay.net/suns-vs-clippers-west-1st-round-game-1-april-16-2023/19471/ I'm trying to download this video of an NBA game. On this site is an embedded video with 3 sources to choose from on the top right. Source 1 and source 3 work. But I can't find the URL to the video for either source 1 or 3. I've played the video while having enabled multiple extensions on Chrome and Firefox, and none of these extensions can detect the link to the video. Does anyone know how to get the URL to this video?"},
{"Title": "Tool for renaming files/movies with common schema", "Author": "u/BlossomingPsyche", "Content": "Let's say you regularly have files with extraneous tags that you might want to remove (ie renaming something from MyDVDMovieRip1981/MyDVDMovieRip1981.x264.mkv and with perhaps a script /  command (I can do this on Os X with an application called Alfred) rename the folder, file and categorize it in the way I want (ie Horror, Scifi) so my Alien1994Bluray/AlienRip1994.mkv would get stripped of the tags and then renamed with a naming scheme like if I wanted to do a scifi rename on my mac I would type \"shift-command space to activate alfred then \"sort Alien 1984 1080p Scifi\" which then renames the selected folder to Alien [1080P] [SCI-FI] [USA] [1984] and the file to Alien 1984 1080P.mkv which then gets dropped into plex. Is there something that will let me rename files/folders with a 'format' like this for PC ? Kind of like automator or alfred or raycast for Os X ?"},
{"Title": "How can I extract and download ALL images from websites gallery?", "Author": "u/atribecallednet", "Content": "How can I extract and download ALL images from these WW2 sites below? https://albumwar2.com/ https://ww2db.com/photo.php"},
{"Title": "List of ALL COLORS of WD internal HDD drives: which ones have the best long term reliability? Is CMR or SMR more reliable? How about best RMP and cache for reliability?", "Author": "u/AntarcticNightingale", "Content": "I'm looking to buy a large (or several smaller) internal HDD for backups, before I invest a ton of time and money for a NAS solution, it will be a quick solution for now. (Yes I know the general advice that the best brand is having another copy regardless of brand. But with that said ...) I have no idea what all the colors of WD internal drives mean. Sometimes I see a color being trashed on this subreddit, but I don't remember. Is it true for externally powered HDD, that Western Digital is more reliable in general than other brands? If not, what is the most reliable brand and which internal drive should I get from it? Is it true that WD internal drives don't have encryption? Which is good for me because encryption is a headache if we ever need data recovery and I don't have any sensitive stuff that needs encryption. Is a faster RPM (7200) more reliable than a slower RPM (5400 or 5640)? Does cache size matter? Is it true that CMR is more reliable than SMR? I don't care about speed or cost and size is a minor concern (because I can always buy multiples of a good small drive). I just want to have the highest chance of my data not go poof. What are the best Western Digital internal drive colors in terms of long-term reliability? Do storage capacity and cache size (64, 128, or 256MB) affects reliability too? I have made the following list based on Amazon's best selling WD drives: WD Blue PC 3.5\" HDD 54,500 ratings, 4.6 average, 80% 5 star, 4% 1 star 8 TB with 5640 RMP and 256 MB cache WD80EAAZ 3-6 TB with 5400 RMP and 256 MB cache WD30EZAX, WD40EZAX, WD60EZAX, respectively 2 TB with 7200 RMP and 256 MB cache WD20EZBX WD Blue PC 3.5\" HDD I don't know why there is a second different link for WD Blue 23,600 ratings, 4.6 average, 81% 5 star, 4% 1 star 1, 3, 4 or 6 TB with 5400 RMP and 64 MB cache WD10EZRZ, WD30EZRZ, WD40EZRZ, WD60EZRZ 2 TB with 5400 RMP and 256 MB cache WD20EZAZ WD Black Performance Gaming 3.5\" HDD 14,700 ratings, 4.6 average, 80% 5 star, 6% 1 star 4, 6, or 10 TB with 7200 RPM and 256 MB cache WD4005FZBX, WD6003FZBX, WD101FZBX 6 or 8 TB with 7200 RPM and 128 MB cache WD6004FZWX, WD8002FZWX 1, or 2 TB with 7200 RPM and 64 MB cache WD1003FZEX, WD2003FZEX WD Purple Surveillance 3.5\" HDD 13,500 ratings, 4.6 average, 80% 5 star, 5% 1 star 14 TB with 7200 RMP and 512 MB cache 10 TB with 7200 RMP and 256 MB cache 8 TB with 5400 RMP and 256 MB cache 6 TB with 5400 RMP and 256 MB cache or 5640 RMP and 128 MB cache 1-4 TB with 5400 RMP and 64 or 256 MB cache WD Red Plus 3.5\" HDD 9,100 ratings, 4.6 average, 80% 5 star, 5% 1 star 12 TB with 7200 RMP and 512 MB cache WD120EFBX 10 TB with 7200 RMP and 256 MB cache 8 TB with 5640 RMP and 256 MB cache 6 TB with 5400 RMP and 256 MB cache or 5640 RMP and 128 MB cache 4 TB with 5640 RMP and 128 or 256 MB cache 2 or 3 TB with 5400 RMP and 128 MB cache 1 or 2 TB with 5400 RMP and 64 MB cache WD Red Pro 3.5\" HDD CMR 1,700 ratings, 4.5 average, 78% 5 star, 8% 1 star 14, 20, 22, or 24 TB with 7200 RMP and 512 MB cache WD240KFGX 4-16, or 18 TB with 7200 RMP and 256 MB cache 2 TB with 7200 RMP and 64 MB cache WD Purple Pro Surveillance 3.5\" HDD 630 ratings, 4.6 average, 81% 5 star, 7% 1 star 14 - 22 TB with 7200 RMP and 512 MB cache WD221PURP 8 - 12 TB with 7200 RMP and 256 MB cache WD Gold Enterprise 3.5\" HDD NO ratings! 8 TB with 7200 RMP and 256 MB cache WD8005FRYZ let me know if I missed any other reliable drives to compare, especially if it's better than any listed here, regardless of brand. I have no idea the purpose of so many colors is. I have no idea why some are marketed only for surveillance or gaming, or enterprise. As you can see, the ratings are very similar. Please let me know which one(s) is/are the most reliable! Thank you so much!!"},
{"Title": "Have you ever thought \"is this worth it?\"", "Author": "u/banisheduser", "Content": "I have some media files saved, but I am very unlikely to ever watch them. However, I know they're not available from the official site any more. I sort of feel a responsibility to keep them archived but at the same time, I do wonder why I am bothering. Have you ever felt this way about some data you've hoarded and what was the outcome?"},
{"Title": "Best way to â€œrefreshâ€ 750 TB of HDDs?", "Author": "u/Nautricity", "Content": "Recently got ahold of A LOT of FREE server equipment, with drives, resulting in me now having nearly a petabyte of total storage combined. Iâ€™m very new to RAIDs and server setups in general. My question for now is, whatâ€™s the best way to refresh these drives to achieve optimal performance before deploying them? Most of these drives have multiple 10s of thousands of hours on them. I have a mix of Apple Xserve RAID, Dell PowerEdge, HP ProLiant, Cisco UCS, etc branded servers and RAID arrays, ranging from 250 GB HDDs all the way up to 8 TB HDDs. Is there any sort of specific software thatâ€™s recommended to â€œrefurbishâ€ these drives in mass? Bonus points for macOS (64 bit) and OS X (32 bit) compatibility! Thank you all for any help as Iâ€™m very new to this!"},
{"Title": "How do you deal with replaced drives?", "Author": "u/DazedWithCoffee", "Content": "I've got a 4TB disk that just degraded my RAIDZ2 pool with 16 read errors. I've got a replacement ready and the appropriate backups already in place, so there's no risk of data loss. My question is this: what to do with this drive? I'd hate to toss it, but I don't particularly trust it anymore. Has anyone found a low-waste strategy for dealing with these things?"},
{"Title": "More info about the height of the new 6TB 2.5\" WD drive", "Author": "u/Far_Marsupial6303", "Content": "Posting for visibility as an addendum to this thread. https://www.reddit.com/r/DataHoarder/comments/1cu2301/finally_after_seven_years_a_6tb_25inch_external/ I was looking at the WD site and noticed they have other models beyond the WD Black external and there are spec sheets for them. They are not listed To those who wonder why the height is important, max 15mm height for 2.5\" drives has been the standard for may years and is the largest height allowed in 2.5\" enclosures. The 5TB MyPassport case is 19.2mm and the drive is 15mm. The 6TB is 20.6mm so the drive is probably 16 or 16.5mm to accommodate the additional platter. I suspect this may allow them to fit snuggly in some 15mm cases. For reference, the lowest height 3.5\" drive is 19.5mm. https://documents.westerndigital.com/content/dam/doc-library/en_us/assets/public/wd/product/external-storage/my_passport/my-passport-phdd-usb-c/data-sheet-my-passport-phdd-usb-c.pdf https://documents.westerndigital.com/content/dam/doc-library/en_us/assets/public/wd/product/external-storage/wd-black-p10-game-drive-for-xbox-usb-3-2-hdd/data-sheet-wd-black-p10-game-drive-for-xbox-usb-3-2-hdd.pdf Pricewise, the \"bargain\" ;-p, is the Elements SE at 169.99MSRP, vs $129.99 for the 5TB."},
{"Title": "Powering down archive pools when not in use.", "Author": "u/dtf_0", "Content": "I was wondering if any NAS hardware/software allows a user to configure powering-down drives/enclosures for storage pools designed for archiving. I am a robotics engineer. I have a lot of old sensor data that is used very infrequently. Whenever we release a new release, we run a simulation against old data to ensure there are no regressions. However, this data is only touched at max 30 days per year. Currently, I manually turn on my archive NAS when need, run my test, move any new data to the archive, and power down the NAS manually. I am curious if there is any hardware or software that can do this automatically. I am thinking of something like a disk enclosure or disk shelf that contains an 'archive pool.' When I access the archive pool, the enclosure powers up, attaches the pool to the NAS, and works as normal. Then, after some delay, the pool detaches and powers down. This is more about power consumption than wear and tear. I currently average about ten years of life out of my drives. They start their lives as constantly spinning drives in my primary NAS, and then when I replace those drives with larger ones, they move to the archive NAS, which runs infrequently."},
{"Title": "hgst 12tb refurbished not going standby", "Author": "u/zannare", "Content": "I recently purchased a refurbished HGST 12tb hard disk (HUH721212ALE601) to use as an additional parity disk in my Snapraid setup it seems to work well, the smart is ok and completed the sync without problems (about 23 hours) but I was unable to make it go to standby automatically I tried all the APM values â€‹â€‹from 1 to 127 (default was 254) but nothing changes I tried various settings of the time in which it should go to standy with smartctl, but nothing manual standy works with both smartctl and hdparm the hard disk makes a \"tick\" every 5 seconds https://vocaroo.com/148gEPXcUo36"},
{"Title": "Any Advice - Sabrent 5 bay docking station upgrade", "Author": "u/SCDUDELEX", "Content": "Current status:  Using the 5 Bay station... been working very well (and yes have a 6 Bay NAS as well, different functions in my system): I need to expand the bay capacity and assuming others have gotten to this point.  Question is, pros and cons of adding another Sabrent 5 bay and using the second USB C to connect the two.   Versus, retiring the 5 bay and swapping it out for the 10 bay model. Obviously it's nearly twice the cost to grab the 10 Bay, but that choice would leave me with the \"old 5 bay\"; to maybe do something with.   But is the single usb connection an advantage over the daisy chain of two 5 bays? Anything else I should consider? Thanks in advance..."},
{"Title": "Whatsapp ICloud Backup to PC?", "Author": "u/Lj_theoneandonly", "Content": "I've followed the guides on this sub for extracting whatsapp backup on android to pc with success. But i haven't seen as much useful discussion around trying to do the same with iPhone. I currently have a couple gb of whatsapp data in my icloud that I encrypted using the 64 digit key. I was looking for ways to take this encrypted backup out of my old iphone but the process doesn't seem to be straightforward. All i've found online were a bunch of third party sites like \"Dr Fone\" which several posts here have vouched for as a scam. So uhhh is there anything else I can do? I wouldn't mind jailbreaking the phone since it's an old one anyway. Thanks in advance for any help!"},
{"Title": "Plex server/build to run arrs", "Author": "Unknown author", "Content": "https://www.amazon.com/gp/product/B01NCESRJX/ref=ox_sc_saved_image_4?smid=A1ZYJ2SSCYE072&psc=1 https://www.amazon.com/gp/product/B07LBXP8KZ/ref=ox_sc_saved_image_7?smid=ADT2TM5FCS88L&psc=1 https://www.amazon.com/gp/product/B0BRQSWSFQ/ref=ox_sc_saved_image_3?smid=ATVPDKIKX0DER&th=1 https://www.amazon.com/Expansion-Controller-Adapter-Splitter-Windows/dp/B0CPC2YLBJ/ref=sr_1_5?crid=1743QI3GMOT9A&dib=eyJ2IjoiMSJ9.cwPeFFKR905GlWuzCZZ90fDEDW0gmLnNiz32tX2qxVe8tZk1FsE4pBGCKj6qeO6tJY7hPucioGRtIPanw0zE0gi5RtMrhvFxX_JO2Kas2r15AMg92x8Gxd-Dk2VoFGZR4S3qaY0eT8qEhvGRhMEPeihGtUY-SgqrSMbwC0HXDvLRvriQqdaXo5ZM624jCN9FzQG-EPAjqZQvu20tzyohQLYNaVmRnY8X6f7NXo1DuBuylBdl_tEeKaW9ih7Eb8qYtWbVANh1NDTPwdEMut_TgI7lizZptl1c0IrnWzthZe0.TlGNxlwUwl0RczFhkZsZcm0kIEEWIT7CccQg4wPB3FQ&dib_tag=se&keywords=sata%2Bexpansion%2Bcard%2Basm&qid=1716180089&sprefix=sata%2Bexpansion%2Bcard%2Basm%2Caps%2C133&sr=8-5&th=1 Already have the ram and psu and an m.2 drive, that cpu is more than plenty right? I will probably end up installing Linux to take advantage of proper tone mapping. any other recommendations?"},
{"Title": "SeaGate Expansion 2 Tb 12$ are it worth and what risk ?", "Author": "u/Merchant_Lawrence", "Content": "Hello everyone, i currently try set up my own nas for archive stuff and on tight budget around 55$ i found on local online shop offer seagate expansion (without mentioning what number or series) 2 tb for just 12 $, it have good rating, are it good choice for long term or it not good ? thanks. link on that local online shop (it say tosiba but option available are seagate) : https://shopee.co.id/Hardisk-Eksternal-Toshiba-2TB-1TB-HDD-USB-3.0-Portable-External-Hard-Drive-Hard-disk-i.997456609.22981243111?sp_atk=abc600c5-5d08-4044-ba64-5c73626f8ba5&xptdk=abc600c5-5d08-4044-ba64-5c73626f8ba5"},
{"Title": "Preserving SketchFab Viewer Experience Offline, with Annotations", "Author": "u/wellnowholdon", "Content": "Hello hello, If this is in the wrong place let me know. Recently, I subscribed to a website with some embedded SketchFab models available for viewing and interaction. Resultingly, it appears that I have been granted (for an expensive and limited time) indirect access to a very rich repository of engineering models that feature various diagrams, cutaways, animations, and annotations. This access allows for interacting with a variety of models, viewing associated educational annotations, and watching demonstrative animations that illustrate the working principles of the modeled object. Using Chrome, via the Network tab, I am able to see some line items of interest. These are firstly, *.binz files, alongside what appear to my eye to be associated texture maps, normal files, and so on as a variety of .jpeg files. There are also links to the sketchfab.com domain and similarly structured API calls. I am aware and have investigated the path leading to .binz extraction and conversion. That is not what I'm interested in. It is not a raw 3D model that I am seeking, it is this comprehensive experience. Having access to the base model is all well and good, but what I would like to do is preserve this \"SketchFab Viewer\" experience even after my membership ends with this website. So, I would like to ask assistance. If possible, could pointers be provided to a way for me to preserve offline the material I currently have access to? Alternatively, is there a method to somehow extract a SketchFab link that will persist after my membership expires? Thank you."},
{"Title": "Is it bad for an external HDD to spin while idling?", "Author": "u/BobbythebreinHeenan", "Content": "Iâ€™ve used external hard drives (almost always western digital) most of my life. One thing Iâ€™ve always noticed is that even when a drive isnâ€™t in use, it will be spinning. Is a drive thatâ€™s spinning while not in use bad? Is it better to disconnect it from the computer when not in use? Would just unplugging it from the computer sufficient? Or do i need to also disconnect it from power? Iâ€™ve never had a hard drive fail on me btw."},
{"Title": "Most effective way of using multiple storage devices", "Author": "u/csyst", "Content": "I am looking forward a solution so i can combine multiple harddrives and access them as one drive/mount. Of course something like RAID 0 or LVM would solve this requirement, but I donâ€™t want to loose all the data of all drives in case one drive fails, only the data of the failed drive. But since both variants use striping both ways donâ€™t work, i guess? Also I want to be able to extend the â€žvirtualâ€œ drive later and I donâ€™t want to loose space for redundancy, since I can get all the files again. Perhaps someone has an idea? Is there anything like a filesystem or mechanism that merges the drives on the fly?"},
{"Title": "Is a used 71% health 1tb Transcend m.2 nvme worth it for $26?", "Author": "u/ahmadmughal0", "Content": "What the title says, also it's in a third world country so it's not soo cheap for me, but cheaper than a new one. Is it a good catch? A 256gb ssd will be fine for me since I have a 1tb hdd buts it's slow/5400 rpm, in 2017 hp omen) but I'm wondering should i get this cheap with 71%health or a 256gb one new?"},
{"Title": "Trashy setups, mine.", "Author": "u/laggyservice", "Content": "No content"},
{"Title": "how to i set up a regular backup in WIN 11 of a folder.", "Author": "u/BatteryAcidEnj0yer", "Content": "I have a folder of \"family pictures\" and would like to have i back up automatically every few days from an m.2 ssd to a HDD (in the same PC) . The folder is about 200Gb+ so I'd be best if It would only copy new files. Preferably using built in tools or opensource solutions I'm using windows built in solution ( backup and restore (windows 7)) but I'm unsure if there is a better, more efficient and/or more reliable solution"},
{"Title": "Can anyone help me with some ideas on how to condense my storage into one place? As well, as recommendations of cloud providers", "Author": "u/Coloradozonian", "Content": "Hey there! I'm trying to simplify my digital storage, use one provide,  and cut down on subscriptions. As a photographer, mom, and blogger, So naturally 've accumulated a ton of photos, files, and albums, but they're all over the place between iCloud & Google One, I pay 2TB with each provider! Sometimes my photos do not upload to both clouds, and i go crazy trying to find it. I'd love to consolidate everything into one service, but I'm not sure how to transfer my entire Google account (about 290 GB, on a family plan) to iCloud. Or vice versa! Is there an easy way to do this? Maybe an app or a hack that can help? I'm somewhat  tech-savvy, but, I'd appreciate any guidance! Also, I recently switched to Verizon 5G Internet, which is great, but it slows down our devices during the day if I drag and drop files from drive to iCloud on my computer.  So, I need to transfer files at night, which is a real pain. Any advice or recommendations would be a lifesaver! Thanks in advance!"},
{"Title": "LTO-5 slow write speed, trying to find bottleneck", "Author": "u/fgt67cam", "Content": "I recently purchased a LTO-5 tape drive to back up some HDDs. The drive itself is working and connected via FC using a QLE2562 PCI-E. My plan was to original create a SAS RAID 0 to temp store the files being written to tape from the offline HDDs but I've had issues with the P410 controller not working so for now will have to connect the offline SATA drives to the server (Proliant ML330 G6). The drives I'm using aren't fast (ST3500312CS) but I did a read speed test using dd if=mytestfile of=/dev/null bs=1M count=1024 and they're getting around 80-90/MBs and online speed results show roughly the same. It's not fast but should be enough to do one backup copy before trying to fix the SAS issue. I've timed how long it's taking for it to do each file and the results: File 1 - 309 seconds - 10,045,430,602 bytes ~32.5/MBs File 2 - 291 seconds - 13,597,137,146 bytes ~46.7/MBs File 3 - 302 seconds - 13,967,991,165 bytes ~46.2/MBs File 4 - 94 seconds - 4,526,005,392 bytes ~48.1/MBs File 5 - 113 seconds - 5,139,777,083 bytes ~45.4/MBs File 6 - 107 seconds - 4,917,953,848 bytes ~45.9/MBs As first it seemed like the culprit was the HDDs so I installed a Samsung EVO 850. It scored 226/MBs with the same read test so much faster than the HDDs. I then had it write a 34GB file to tape and it took 528 seconds (not including tape rewind time) coming in at ~69.1/MBs which is still slow so I don't think the bottle neck is the SATA drives. The FC card is showing the link speed as 8Gbps and even if it was 1Gbps it's still too slow so I don't think it could be that. All that's left is the tape drive or Debian/drivers/software. I'm using \"tar cvf /dev/st0\" to write data to the tape. EDIT 22/05/24 I've since built a new computer with more modern hardware to see if it made a difference using a 5280K, 16GB RAM and GA-X99M-Gaming5. Tested with a 2TB HDD and RAM drive, both scored ~106/MBs which seems to be the maximum it can do. I don't know what speeds are good for LTO-5 and if you can realistically get anywhere close to maximum of 140/MBs."},
{"Title": "backup multiple windows and Linux machines, and several loose HDD/SSD/CD/DVD onto a single windows machine (not NAS, not cloud)", "Author": "u/tater1337", "Content": "Honestly I tried searching here, but either I get confused or I cannot tell is the solution will fit my needs I have [windows] my main PC work laptop a second home machine that does some tasks that I don't want on the main PC (like housing all the backups) personal laptop a bunch of loose hard drives, both SSD and HDD with data that I might want in the future(grandparents disks full of photos type stuff) a PC set up as a DVR for a bunch of security cameras(not wanting to save the footage, just an easy restore if something breaks and a reinstall is needed) Linux a couple of PCs used for Minecraft servers a PC set up for home assistant a couple of netbooks, some have software for 3d printers, CNC machines, Laser engravers, etc a whole load of raspberry pi's with various weird stuff a stack of DVDs and CDs that I'd like to have backup copied in case of physical damage right now I have some backups of some machines, but I have not been able to get everything backed up my plan take my recently purchased 16tb drive and dump everything on that, then get another drive to copy the first (or expand)....THEN set up a backup physically somewhere else what are my options? I've cloned drives with FileZilla, but only on a \"copy this HDD to a SSD to improve old machine\", not for backups I tried to get windows backup to try to backup my main PC to the secondary PC, but it fails more often than it works getting Linux and windows to play nicely together seems like black magic. more comfortable in windows than Linux tutorials, software options desired, dedicated NAS hardware solutions or cloud are not"},
{"Title": "Trying to find some forgotten RAID subsystem firmware.", "Author": "u/Zone_Purifier", "Content": "In an attempt to make it work properly with some newer drives, I've been hunting for the latest SAS firmware for a AXUS Y3-24S6SF8 RAID Subunit. AXUS is a former subsidiary of ASUS, one which hasn't been around since 2014 and might have some connection with Areca. This search has taken me to internet archive, ASUS, Areca, and random russian server hardware sites in search for this firmware. I've found some interesting leads and dead end phone numbers and emails, but nothing that yields the .bin (probably) that I need to update the system. If anyone has access to these files or might have ideas of where I can get firmware for such a seemingly obscure device, that would be great. Here's what I've found so far: AXUS Website, latest archive YOTTA III Firmware Page Archive AXUS Closure Notice AXUS Final Contact Information Notice YOTTA 3 User Manual And some system info from the unit itself: https://preview.redd.it/trying-to-find-some-forgotten-raid-subsystem-firmware-v0-vymbg5rh0c1d1.png https://preview.redd.it/trying-to-find-some-forgotten-raid-subsystem-firmware-v0-oi55a5rh0c1d1.png"},
{"Title": "Crucial X6, X9 or another for photos and videos backup?", "Author": "u/3dforlife", "Content": "Hi there! I've had several internal and external HDDs that I used to backup my photos and videos. However, almost all have died the last years. I just can't stand them anymore. For reference, I've bought two Seagate Barracuda Green 4TB, and I've sent them 3 times already to be replaced... At the moment I have 1,3TB of photos and videos in a WD Blue 4TB, and I edit photos and videos with it. Therefore, I'm thinking of buying a SSD to backup my media, since I haven't had any problems with them (granted, I've only used them as main drives). Is the Crucial X6 (119 â‚¬ 2TB) a good choice? It's on sale right now, but I've read that the cache depletes after having had 250GB written. After that, it writes at 40MBps, if I'm not mistaken. It is also QLC, which might affect longevity. Regarding the X9 (144â‚¬ 2TB), I can't find any review... I have bought the X9 pro (174â‚¬ 2TB) for my work, and it's very good, but it might be overkill for backups. A redditor, u/ JohnnieLouHansen said that if I didn't need the speed of an SSD and wanted reliability, to go with Western Digital Enterprise drives, gold. He also said that he has put them in customer PCs. One pair are going on 10 years at 10 hours per day and another set ran 24/7 for 6 years. I searched for them on Amazon, and used Fakespot; the reviews were graded D, i.e., not reliable any highly deceptive: https://amzn.eu/d/fHMwwAj What conclusion can I take from this? What do you recommend? Any of these, some other?"},
{"Title": "17.1 TB \"size on disk\" on a 12TB drive", "Author": "u/useless_shoehorn", "Content": "Hi. I have a bunch of movies on a 12TB drive. Some of them didn't finish torrenting in windows/NTFS. I wanted to extend this drive in windows, but my block size was too small (4096). I have since repented and spun up a TrueNas server with the other 12TB drive (1M block size). However, as I'm copying to it I'm running out of space. Even if every file needed another 1M for the block size change, that's only 70GB right? What am I missing? How is the size on disk 17.1TB? What does that even mean? https://preview.redd.it/17-1-tb-size-on-disk-on-a-12tb-drive-v0-ufx8hwhwp71d1.png https://preview.redd.it/17-1-tb-size-on-disk-on-a-12tb-drive-v0-p070vuhwp71d1.png"},
{"Title": "Strategy with hoarding and digesting large chunk of (small in size but large in number) folders and files", "Author": "u/cod201", "Content": "tldr: extracted more than 1.4 mils folder ( each inside have files)  into single folder in NTFS, ridiculous bad IO performance. Divided into 500 folders and got better IO, but i know i'm, dumb. Ask for better approach. Hi, I am using NTFS on Windows to run an research but i'm facing performance decrease, maybe due to data and index fragmentation. I receive multiple ZIP files everyday to extract, inside zip file is multiple folder, each folder is an unit of data for us to digest using Python, and upload them to Elastic (these in bright cyan). a brief structure of my data Because the server is headless, i just recognized the problem when i connected it again, when it reached 1,5 millions folders, approx. 1.5TB  inside JUST A SINGLE folder. (about 7TBs waiting to extracting but i stopped it) https://preview.redd.it/strategy-with-hoarding-and-digesting-large-chunk-of-small-v0-cu59lxq1kc1d1.png So, when I move/ rename.. a folder, it is extremely lagging, moving a small file barely took more than hour. Just view the properties take more than 1GB of RAM. https://preview.redd.it/strategy-with-hoarding-and-digesting-large-chunk-of-small-v0-ocwa17rujc1d1.png I've just moving these data into other disc and dividing into 500 folders, based on its name (from AA to ZZ), the performance just got better, but idk is there any better ways to storage and using these data? I use Python to work with these file (maybe upgrading to C#/go.. for better multi threaded performance) and after digesting, i would storage it for about 6-12 months before delete it. I know my strategy is somewhat inefficient, so i'm asking if i could make it better. Thanks"},
{"Title": "FPF: I see your trashy setup, and submit my own for disapproval.", "Author": "u/Lanky-Antelope7006", "Content": "No content"},
{"Title": "When they ask you why", "Author": "u/eidolons", "Content": "You tell them this is why."},
{"Title": "Megs, Gigs, _____, ______?", "Author": "u/cheater00", "Content": "It's very annoying that there isn't a good short word for \"Terabytes\" or \"Petabytes\". This is my momentary frustration and I thought I'd come here to complain about it."},
{"Title": "Comparing the data on two different hard drives", "Author": "u/Not-The-Dark-Lord-7", "Content": "Hi all, I am new to this sub so please forgive me if this isnâ€™t the right place or the right way to ask this question. Basically, I currently have an  Apple 2021 M1 iMac with 1 TB of storage, and I would like to backup my data. I can buy a $65 2 TB Seagate HDD, a $20 enclosure, use time machine to backup my data, and this would be a good solution (right? Iâ€™m saying these things as I understand them, but please correct anything I say thatâ€™s not entirely accurate). However, I have an old 1 TB HDD from an old iMac, because I swapped the HDD for an SSD. I think it might be convenient to use this for my backups instead. However, Iâ€™m like 90% I need to wipe and reformat the drive first, because when I just plug it in, it doesnâ€™t show up on Time Machine. Iâ€™ve looked at the drive, and it seems like pretty much all of the data on it is on my current computer. However, I donâ€™t want to lose any files that might only be on the HDD, and while Iâ€™m like 90% certain that all the data on the HDD is also on my iMac, Iâ€™d like to make certain before wiping it. So, two questions here: What is the easiest way to check and see if all the data on the HDD is also on the SSD? Do I even need to do that? Should I just bite the bullet and get a new 2 TB HDD? The HDD doesnâ€™t support SMART so I canâ€™t see exactly how many total read/writes are on it, but Iâ€™m fairly certain it really isnâ€™t that many, so that isnâ€™t a big concern of mine. Newer drives like the ones I was looking at from Seagate support SMART, so I guess that might be better for monitoring the driveâ€™s health. Also, Iâ€™m only using about 250 GB on my SSD right now, the 1 TB HDD is large enough for backups. I look forward to being enlightened by this community!"},
{"Title": "Fate of the Internet Archive", "Author": "u/Coolkatisa2511", "Content": "Now that the music lawsuit is going to happen, will this be the thing to kill the Internet Archive? Universal is a powerful company with a lot of money and they can easily kill off the website. What do you think will happen if/when the Internet Archive loses?"},
{"Title": "Nas advice", "Author": "u/foxhound13", "Content": "Complete newbie here, looking to purchase a Nas purely for storing and streaming video content to my laptop, what I'm trying to understand is the following: Lots of the cheaper options have 1gb ram, will that do for standard video play back from the device to a computer. (standard size files no 4k likely no VR) I'm not sure if ram is even a bottleneck here or not. Might be silly but How viable is using a torrent program to download video content to the NAS and is there any considerations i might want to make especially around download speed (Im fine with a lan connection if recommended) Do most NAS units come with password Protection software/abilities and a Lan port I'm in the market for an 8tb nas with drives included (4tb actual storage 4 redundancy I think) and room to grow for the cheapest possible if anyone has any recommendations. I don't think i require plex or any fancy ui stuff just straight up storage I can play video files from, any help is appriciated."},
{"Title": "Downsized from 5TB to 100GB on OneDrive Edu - Seeking Storage Solutions on a Budget!", "Author": "u/cuong3101", "Content": "Iâ€™m in a bit of a pickle and could use your collective wisdom. My OneDrive Edu account, which was my digital haven with 5TB of space, has been abruptly downsized to a mere 100GB. This change came without warning, as I missed the memo about Microsoft scaling down storage for educational institutions. Hereâ€™s my current setup: Three 3.5 inch 1TB HDDs (oneâ€™s a dedicated movie library) A 500GB Samsung 860 Evo SSD A â€œ2.5 / 3.5 inch 2 Bay USB3.0 1 to 1 Clone Hard Drive Dockâ€ (sans RAID, so itâ€™s just two separate drives) With my OneDrive now bursting at the seams with 1.3TB-1.5TB of data, Iâ€™m exploring storage alternatives that wonâ€™t break the bank. The NAS Synology DS220j or DS223j are within my financial reach, but they offer only two hard drive slots. Thereâ€™s also an old family computer gathering dust back home, powered by an i5 (6th or 8th gen) and a 300W power supply of dubious origin. Itâ€™s been out of action for a while, and Iâ€™m not even sure itâ€™s still operational. Reviving it would mean having it shipped over, which is a 4-5 day wait. So, dear Redditors, whatâ€™s my best move here? Should I invest in a budget NAS, attempt to resurrect the old PC, or is there another ingenious solution I havenâ€™t considered? UPDATE: Thanks for everyone's comments, after a week has passed I update the current situation as follows. Temporarily : I have manually backed up all data on OneDrive to 2 1TB hard drives. My data is temporarily safe. Future : Maybe I will find a way to take advantage of the old computer at home to use up the remaining hard drives and have access to the data in the other two hard drives at all times. If that doesn't work, I'm thinking of buying Synology's DS423 NAS device. The best-case scenario is that I could have 2 separate devices (1 Synology NAS and 1 NAS from my old computer), put them in 2 separate places and hope to find a way to sync them. In the long run, I will look into some more cloud storage packages to back up data from the other 2 devices. So I can have 3 copies of data: 1 on my personal computer that can be accessed at all times, 1 on the NAS device and 1 on the cloud. Hopefully this way I won't have to worry about data being at risk of being lost in the future. UPDATE 2: I was wondering why everyone suggested a 6TB hard drive to me so much. Then I realized that my poor English writing made people misunderstand that I needed to back up 5TB of data from OneDrive instead of 1.5TB."},
{"Title": "Trust me, I'm not always this trashy ðŸ˜", "Author": "u/zaca21", "Content": "No content"},
{"Title": "Seeking Advice on Optimizing my Storage Configuration in Proxmox with TrueNAS Scale and ZFS", "Author": "u/Vegan_Salad69", "Content": "Hi everyone, Iâ€™ve been homelabbing for about three years now and Iâ€™m looking for some advice on optimizing my storage setup. Hereâ€™s a quick overview of my current configuration: Server: Dell R750 Front: Twelve 3.5\" SATA/SAS bays Rear: Four 2.5\" SATA/SAS bays RAID Card: PERC 350H (supports RAID 1, 0, 10) (donâ€™t worry, I use it in passthrough mode and it works so far, even on ZFS. Yes, I know thatâ€™s also a problem, and I plan to buy an HBA or use the RAID card for its intended purpose) 4x 10Gig SFP+ Networking via DAC SFP+ Moduels (For more info on that i have a post of my entire homelab on my Profile) Drives: Eight 4TB HDDs Two 3TB HDDs Two 2TB HDDs Two SAS 600GB HDDs Two 2TB SATA SSDs Current Setup: Proxmox as the hypervisor TrueNAS Scale as a VM with passthrough for ZFS Iâ€™m facing potential performance issues because TrueNAS Scale is virtualized. Iâ€™m considering a few options and would love your input: Installing Proxmox on SSDs: Utilizing the SSDs for the OS to leverage their speed for the hypervisor. Using SAS drives as cache: Taking advantage of the 12Gbps speed for caching to enhance overall performance. Direct ZFS on Proxmox: Managing the drives directly in Proxmox with ZFS, but Iâ€™m concerned about losing functionalities like SMB sharing, permissions management, and user administration. I also have an old Synology RS2818RP+ that I could use: Synology NAS with 16 bays and 10GbE connectivity. Equipped with the E10M20-T1 (a 10Gig Network card with 2x 2TB NVMe SSDs for network cache). Considering moving all large HDDs to the Synology and using it as the main storage, and only hosting the VMs' main system on the Dell's SSDs (so each VM gets 32GB; the main storage is on the Synology). Direct 10GbE connection to the Dell R750 for high-speed access. (Block Access) Potential Solutions: Bare Metal TrueNAS Scale: Install TrueNAS directly on the server for maximum performance, but lose the flexibility of Proxmox. Proxmox with ZFS and Containers/VMs: Use Proxmox with ZFS for storage and handle additional services (SMB, permissions, Nextcloud) via LXC containers or VMs. Hybrid Approach: Keep Proxmox with ZFS pools, and run a VM for TrueNAS Scale to manage SMB and NFS, with other services running in containers/VMs on Proxmox. Leveraging Synology NAS: Use the Synology as the main storage system with 10GbE connectivity to the Dell R750. Utilize the NVMe SSD cache on the Synology for improved performance. Keep the SSDs and SAS drives in the Dell R750 for caching and high-speed storage needs. Iâ€™d love to hear your thoughts on these approaches or any other suggestions you might have. What would be the best way to balance performance, flexibility, and power efficiency in this setup? Thanks in advance for your help! I hope you all have a good weekend. Best wishes, keep hoarding. a VeganSalat"},
{"Title": "Finding good deals for HDDs and SSDs?", "Author": "u/Neurrone", "Content": "I currently have 4 x 4TB Seagate Ironwolfs and am looking to expand. Are there price trackers that I could use to get a sense of what a good deal would be, in terms of price per TB? Something that can send alerts when prices drop below a certain level would be an added bonus How safe is it to use used drives? They are half the price of new ones, but might fail more quickly."},
{"Title": "Optical Archiving", "Author": "Unknown author", "Content": "Just learned of the new DM for Archiving Blu-ray standard that should last 100 years. This is the player/burner Pioneer BDR-WX01DM JIS X6257 Compliant DM for Archive. Y'all may know this, but just decided to share, share your usage and impresions please."},
{"Title": "New CDs can hold 1+ petabytes of storage", "Author": "u/GiantJupiter45", "Content": "No content"},
{"Title": "Preserving SketchFab Viewer Experience Offline, with Annotations", "Author": "u/wellnowholdon", "Content": "Hello hello, If this is in the wrong place let me know. Recently, I subscribed to a website with some embedded SketchFab models available for viewing and interaction. Resultingly, it appears that I have been granted (for an expensive and limited time) indirect access to a very rich repository of engineering models that feature various diagrams, cutaways, animations, and annotations. This access allows for interacting with a variety of models, viewing associated educational annotations, and watching demonstrative animations that illustrate the working principles of the modeled object. Using Chrome, via the Network tab, I am able to see some line items of interest. These are firstly, *.binz files, alongside what appear to my eye to be associated texture maps, normal files, and so on as a variety of .jpeg files. There are also links to the sketchfab.com domain and similarly structured API calls. I am aware and have investigated the path leading to .binz extraction and conversion. That is not what I'm interested in. It is not a raw 3D model that I am seeking, it is this comprehensive experience. Having access to the base model is all well and good, but what I would like to do is preserve this \"SketchFab Viewer\" experience even after my membership ends with this website. So, I would like to ask assistance. If possible, could pointers be provided to a way for me to preserve offline the material I currently have access to? Alternatively, is there a method to somehow extract a SketchFab link that will persist after my membership expires? Thank you."},
{"Title": "Is it bad for an external HDD to spin while idling?", "Author": "u/BobbythebreinHeenan", "Content": "Iâ€™ve used external hard drives (almost always western digital) most of my life. One thing Iâ€™ve always noticed is that even when a drive isnâ€™t in use, it will be spinning. Is a drive thatâ€™s spinning while not in use bad? Is it better to disconnect it from the computer when not in use? Would just unplugging it from the computer sufficient? Or do i need to also disconnect it from power? Iâ€™ve never had a hard drive fail on me btw."},
{"Title": "Most effective way of using multiple storage devices", "Author": "u/csyst", "Content": "I am looking forward a solution so i can combine multiple harddrives and access them as one drive/mount. Of course something like RAID 0 or LVM would solve this requirement, but I donâ€™t want to loose all the data of all drives in case one drive fails, only the data of the failed drive. But since both variants use striping both ways donâ€™t work, i guess? Also I want to be able to extend the â€žvirtualâ€œ drive later and I donâ€™t want to loose space for redundancy, since I can get all the files again. Perhaps someone has an idea? Is there anything like a filesystem or mechanism that merges the drives on the fly?"},
{"Title": "Is a used 71% health 1tb Transcend m.2 nvme worth it for $26?", "Author": "u/ahmadmughal0", "Content": "What the title says, also it's in a third world country so it's not soo cheap for me, but cheaper than a new one. Is it a good catch? A 256gb ssd will be fine for me since I have a 1tb hdd buts it's slow/5400 rpm, in 2017 hp omen) but I'm wondering should i get this cheap with 71%health or a 256gb one new?"},
{"Title": "Trashy setups, mine.", "Author": "u/laggyservice", "Content": "No content"},
{"Title": "how to i set up a regular backup in WIN 11 of a folder.", "Author": "u/BatteryAcidEnj0yer", "Content": "I have a folder of \"family pictures\" and would like to have i back up automatically every few days from an m.2 ssd to a HDD (in the same PC) . The folder is about 200Gb+ so I'd be best if It would only copy new files. Preferably using built in tools or opensource solutions I'm using windows built in solution ( backup and restore (windows 7)) but I'm unsure if there is a better, more efficient and/or more reliable solution"},
{"Title": "Can anyone help me with some ideas on how to condense my storage into one place? As well, as recommendations of cloud providers", "Author": "u/Coloradozonian", "Content": "Hey there! I'm trying to simplify my digital storage, use one provide,  and cut down on subscriptions. As a photographer, mom, and blogger, So naturally 've accumulated a ton of photos, files, and albums, but they're all over the place between iCloud & Google One, I pay 2TB with each provider! Sometimes my photos do not upload to both clouds, and i go crazy trying to find it. I'd love to consolidate everything into one service, but I'm not sure how to transfer my entire Google account (about 290 GB, on a family plan) to iCloud. Or vice versa! Is there an easy way to do this? Maybe an app or a hack that can help? I'm somewhat  tech-savvy, but, I'd appreciate any guidance! Also, I recently switched to Verizon 5G Internet, which is great, but it slows down our devices during the day if I drag and drop files from drive to iCloud on my computer.  So, I need to transfer files at night, which is a real pain. Any advice or recommendations would be a lifesaver! Thanks in advance!"},
{"Title": "LTO-5 slow write speed, trying to find bottleneck", "Author": "u/fgt67cam", "Content": "I recently purchased a LTO-5 tape drive to back up some HDDs. The drive itself is working and connected via FC using a QLE2562 PCI-E. My plan was to original create a SAS RAID 0 to temp store the files being written to tape from the offline HDDs but I've had issues with the P410 controller not working so for now will have to connect the offline SATA drives to the server (Proliant ML330 G6). The drives I'm using aren't fast (ST3500312CS) but I did a read speed test using dd if=mytestfile of=/dev/null bs=1M count=1024 and they're getting around 80-90/MBs and online speed results show roughly the same. It's not fast but should be enough to do one backup copy before trying to fix the SAS issue. I've timed how long it's taking for it to do each file and the results: File 1 - 309 seconds - 10,045,430,602 bytes ~32.5/MBs File 2 - 291 seconds - 13,597,137,146 bytes ~46.7/MBs File 3 - 302 seconds - 13,967,991,165 bytes ~46.2/MBs File 4 - 94 seconds - 4,526,005,392 bytes ~48.1/MBs File 5 - 113 seconds - 5,139,777,083 bytes ~45.4/MBs File 6 - 107 seconds - 4,917,953,848 bytes ~45.9/MBs As first it seemed like the culprit was the HDDs so I installed a Samsung EVO 850. It scored 226/MBs with the same read test so much faster than the HDDs. I then had it write a 34GB file to tape and it took 528 seconds (not including tape rewind time) coming in at ~69.1/MBs which is still slow so I don't think the bottle neck is the SATA drives. The FC card is showing the link speed as 8Gbps and even if it was 1Gbps it's still too slow so I don't think it could be that. All that's left is the tape drive or Debian/drivers/software. I'm using \"tar cvf /dev/st0\" to write data to the tape. EDIT 22/05/24 I've since built a new computer with more modern hardware to see if it made a difference using a 5280K, 16GB RAM and GA-X99M-Gaming5. Tested with a 2TB HDD and RAM drive, both scored ~106/MBs which seems to be the maximum it can do. I don't know what speeds are good for LTO-5 and if you can realistically get anywhere close to maximum of 140/MBs."},
{"Title": "backup multiple windows and Linux machines, and several loose HDD/SSD/CD/DVD onto a single windows machine (not NAS, not cloud)", "Author": "u/tater1337", "Content": "Honestly I tried searching here, but either I get confused or I cannot tell is the solution will fit my needs I have [windows] my main PC work laptop a second home machine that does some tasks that I don't want on the main PC (like housing all the backups) personal laptop a bunch of loose hard drives, both SSD and HDD with data that I might want in the future(grandparents disks full of photos type stuff) a PC set up as a DVR for a bunch of security cameras(not wanting to save the footage, just an easy restore if something breaks and a reinstall is needed) Linux a couple of PCs used for Minecraft servers a PC set up for home assistant a couple of netbooks, some have software for 3d printers, CNC machines, Laser engravers, etc a whole load of raspberry pi's with various weird stuff a stack of DVDs and CDs that I'd like to have backup copied in case of physical damage right now I have some backups of some machines, but I have not been able to get everything backed up my plan take my recently purchased 16tb drive and dump everything on that, then get another drive to copy the first (or expand)....THEN set up a backup physically somewhere else what are my options? I've cloned drives with FileZilla, but only on a \"copy this HDD to a SSD to improve old machine\", not for backups I tried to get windows backup to try to backup my main PC to the secondary PC, but it fails more often than it works getting Linux and windows to play nicely together seems like black magic. more comfortable in windows than Linux tutorials, software options desired, dedicated NAS hardware solutions or cloud are not"},
{"Title": "Trying to find some forgotten RAID subsystem firmware.", "Author": "u/Zone_Purifier", "Content": "In an attempt to make it work properly with some newer drives, I've been hunting for the latest SAS firmware for a AXUS Y3-24S6SF8 RAID Subunit. AXUS is a former subsidiary of ASUS, one which hasn't been around since 2014 and might have some connection with Areca. This search has taken me to internet archive, ASUS, Areca, and random russian server hardware sites in search for this firmware. I've found some interesting leads and dead end phone numbers and emails, but nothing that yields the .bin (probably) that I need to update the system. If anyone has access to these files or might have ideas of where I can get firmware for such a seemingly obscure device, that would be great. Here's what I've found so far: AXUS Website, latest archive YOTTA III Firmware Page Archive AXUS Closure Notice AXUS Final Contact Information Notice YOTTA 3 User Manual And some system info from the unit itself: https://preview.redd.it/trying-to-find-some-forgotten-raid-subsystem-firmware-v0-vymbg5rh0c1d1.png https://preview.redd.it/trying-to-find-some-forgotten-raid-subsystem-firmware-v0-oi55a5rh0c1d1.png"},
{"Title": "Crucial X6, X9 or another for photos and videos backup?", "Author": "u/3dforlife", "Content": "Hi there! I've had several internal and external HDDs that I used to backup my photos and videos. However, almost all have died the last years. I just can't stand them anymore. For reference, I've bought two Seagate Barracuda Green 4TB, and I've sent them 3 times already to be replaced... At the moment I have 1,3TB of photos and videos in a WD Blue 4TB, and I edit photos and videos with it. Therefore, I'm thinking of buying a SSD to backup my media, since I haven't had any problems with them (granted, I've only used them as main drives). Is the Crucial X6 (119 â‚¬ 2TB) a good choice? It's on sale right now, but I've read that the cache depletes after having had 250GB written. After that, it writes at 40MBps, if I'm not mistaken. It is also QLC, which might affect longevity. Regarding the X9 (144â‚¬ 2TB), I can't find any review... I have bought the X9 pro (174â‚¬ 2TB) for my work, and it's very good, but it might be overkill for backups. A redditor, u/ JohnnieLouHansen said that if I didn't need the speed of an SSD and wanted reliability, to go with Western Digital Enterprise drives, gold. He also said that he has put them in customer PCs. One pair are going on 10 years at 10 hours per day and another set ran 24/7 for 6 years. I searched for them on Amazon, and used Fakespot; the reviews were graded D, i.e., not reliable any highly deceptive: https://amzn.eu/d/fHMwwAj What conclusion can I take from this? What do you recommend? Any of these, some other?"},
{"Title": "17.1 TB \"size on disk\" on a 12TB drive", "Author": "u/useless_shoehorn", "Content": "Hi. I have a bunch of movies on a 12TB drive. Some of them didn't finish torrenting in windows/NTFS. I wanted to extend this drive in windows, but my block size was too small (4096). I have since repented and spun up a TrueNas server with the other 12TB drive (1M block size). However, as I'm copying to it I'm running out of space. Even if every file needed another 1M for the block size change, that's only 70GB right? What am I missing? How is the size on disk 17.1TB? What does that even mean? https://preview.redd.it/17-1-tb-size-on-disk-on-a-12tb-drive-v0-ufx8hwhwp71d1.png https://preview.redd.it/17-1-tb-size-on-disk-on-a-12tb-drive-v0-p070vuhwp71d1.png"},
{"Title": "Strategy with hoarding and digesting large chunk of (small in size but large in number) folders and files", "Author": "u/cod201", "Content": "tldr: extracted more than 1.4 mils folder ( each inside have files)  into single folder in NTFS, ridiculous bad IO performance. Divided into 500 folders and got better IO, but i know i'm, dumb. Ask for better approach. Hi, I am using NTFS on Windows to run an research but i'm facing performance decrease, maybe due to data and index fragmentation. I receive multiple ZIP files everyday to extract, inside zip file is multiple folder, each folder is an unit of data for us to digest using Python, and upload them to Elastic (these in bright cyan). a brief structure of my data Because the server is headless, i just recognized the problem when i connected it again, when it reached 1,5 millions folders, approx. 1.5TB  inside JUST A SINGLE folder. (about 7TBs waiting to extracting but i stopped it) https://preview.redd.it/strategy-with-hoarding-and-digesting-large-chunk-of-small-v0-cu59lxq1kc1d1.png So, when I move/ rename.. a folder, it is extremely lagging, moving a small file barely took more than hour. Just view the properties take more than 1GB of RAM. https://preview.redd.it/strategy-with-hoarding-and-digesting-large-chunk-of-small-v0-ocwa17rujc1d1.png I've just moving these data into other disc and dividing into 500 folders, based on its name (from AA to ZZ), the performance just got better, but idk is there any better ways to storage and using these data? I use Python to work with these file (maybe upgrading to C#/go.. for better multi threaded performance) and after digesting, i would storage it for about 6-12 months before delete it. I know my strategy is somewhat inefficient, so i'm asking if i could make it better. Thanks"},
{"Title": "FPF: I see your trashy setup, and submit my own for disapproval.", "Author": "u/Lanky-Antelope7006", "Content": "No content"},
{"Title": "When they ask you why", "Author": "u/eidolons", "Content": "You tell them this is why."},
{"Title": "Megs, Gigs, _____, ______?", "Author": "u/cheater00", "Content": "It's very annoying that there isn't a good short word for \"Terabytes\" or \"Petabytes\". This is my momentary frustration and I thought I'd come here to complain about it."},
{"Title": "Comparing the data on two different hard drives", "Author": "u/Not-The-Dark-Lord-7", "Content": "Hi all, I am new to this sub so please forgive me if this isnâ€™t the right place or the right way to ask this question. Basically, I currently have an  Apple 2021 M1 iMac with 1 TB of storage, and I would like to backup my data. I can buy a $65 2 TB Seagate HDD, a $20 enclosure, use time machine to backup my data, and this would be a good solution (right? Iâ€™m saying these things as I understand them, but please correct anything I say thatâ€™s not entirely accurate). However, I have an old 1 TB HDD from an old iMac, because I swapped the HDD for an SSD. I think it might be convenient to use this for my backups instead. However, Iâ€™m like 90% I need to wipe and reformat the drive first, because when I just plug it in, it doesnâ€™t show up on Time Machine. Iâ€™ve looked at the drive, and it seems like pretty much all of the data on it is on my current computer. However, I donâ€™t want to lose any files that might only be on the HDD, and while Iâ€™m like 90% certain that all the data on the HDD is also on my iMac, Iâ€™d like to make certain before wiping it. So, two questions here: What is the easiest way to check and see if all the data on the HDD is also on the SSD? Do I even need to do that? Should I just bite the bullet and get a new 2 TB HDD? The HDD doesnâ€™t support SMART so I canâ€™t see exactly how many total read/writes are on it, but Iâ€™m fairly certain it really isnâ€™t that many, so that isnâ€™t a big concern of mine. Newer drives like the ones I was looking at from Seagate support SMART, so I guess that might be better for monitoring the driveâ€™s health. Also, Iâ€™m only using about 250 GB on my SSD right now, the 1 TB HDD is large enough for backups. I look forward to being enlightened by this community!"},
{"Title": "Fate of the Internet Archive", "Author": "u/Coolkatisa2511", "Content": "Now that the music lawsuit is going to happen, will this be the thing to kill the Internet Archive? Universal is a powerful company with a lot of money and they can easily kill off the website. What do you think will happen if/when the Internet Archive loses?"},
{"Title": "Nas advice", "Author": "u/foxhound13", "Content": "Complete newbie here, looking to purchase a Nas purely for storing and streaming video content to my laptop, what I'm trying to understand is the following: Lots of the cheaper options have 1gb ram, will that do for standard video play back from the device to a computer. (standard size files no 4k likely no VR) I'm not sure if ram is even a bottleneck here or not. Might be silly but How viable is using a torrent program to download video content to the NAS and is there any considerations i might want to make especially around download speed (Im fine with a lan connection if recommended) Do most NAS units come with password Protection software/abilities and a Lan port I'm in the market for an 8tb nas with drives included (4tb actual storage 4 redundancy I think) and room to grow for the cheapest possible if anyone has any recommendations. I don't think i require plex or any fancy ui stuff just straight up storage I can play video files from, any help is appriciated."},
{"Title": "Downsized from 5TB to 100GB on OneDrive Edu - Seeking Storage Solutions on a Budget!", "Author": "u/cuong3101", "Content": "Iâ€™m in a bit of a pickle and could use your collective wisdom. My OneDrive Edu account, which was my digital haven with 5TB of space, has been abruptly downsized to a mere 100GB. This change came without warning, as I missed the memo about Microsoft scaling down storage for educational institutions. Hereâ€™s my current setup: Three 3.5 inch 1TB HDDs (oneâ€™s a dedicated movie library) A 500GB Samsung 860 Evo SSD A â€œ2.5 / 3.5 inch 2 Bay USB3.0 1 to 1 Clone Hard Drive Dockâ€ (sans RAID, so itâ€™s just two separate drives) With my OneDrive now bursting at the seams with 1.3TB-1.5TB of data, Iâ€™m exploring storage alternatives that wonâ€™t break the bank. The NAS Synology DS220j or DS223j are within my financial reach, but they offer only two hard drive slots. Thereâ€™s also an old family computer gathering dust back home, powered by an i5 (6th or 8th gen) and a 300W power supply of dubious origin. Itâ€™s been out of action for a while, and Iâ€™m not even sure itâ€™s still operational. Reviving it would mean having it shipped over, which is a 4-5 day wait. So, dear Redditors, whatâ€™s my best move here? Should I invest in a budget NAS, attempt to resurrect the old PC, or is there another ingenious solution I havenâ€™t considered? UPDATE: Thanks for everyone's comments, after a week has passed I update the current situation as follows. Temporarily : I have manually backed up all data on OneDrive to 2 1TB hard drives. My data is temporarily safe. Future : Maybe I will find a way to take advantage of the old computer at home to use up the remaining hard drives and have access to the data in the other two hard drives at all times. If that doesn't work, I'm thinking of buying Synology's DS423 NAS device. The best-case scenario is that I could have 2 separate devices (1 Synology NAS and 1 NAS from my old computer), put them in 2 separate places and hope to find a way to sync them. In the long run, I will look into some more cloud storage packages to back up data from the other 2 devices. So I can have 3 copies of data: 1 on my personal computer that can be accessed at all times, 1 on the NAS device and 1 on the cloud. Hopefully this way I won't have to worry about data being at risk of being lost in the future. UPDATE 2: I was wondering why everyone suggested a 6TB hard drive to me so much. Then I realized that my poor English writing made people misunderstand that I needed to back up 5TB of data from OneDrive instead of 1.5TB."},
{"Title": "Trust me, I'm not always this trashy ðŸ˜", "Author": "u/zaca21", "Content": "No content"},
{"Title": "Seeking Advice on Optimizing my Storage Configuration in Proxmox with TrueNAS Scale and ZFS", "Author": "u/Vegan_Salad69", "Content": "Hi everyone, Iâ€™ve been homelabbing for about three years now and Iâ€™m looking for some advice on optimizing my storage setup. Hereâ€™s a quick overview of my current configuration: Server: Dell R750 Front: Twelve 3.5\" SATA/SAS bays Rear: Four 2.5\" SATA/SAS bays RAID Card: PERC 350H (supports RAID 1, 0, 10) (donâ€™t worry, I use it in passthrough mode and it works so far, even on ZFS. Yes, I know thatâ€™s also a problem, and I plan to buy an HBA or use the RAID card for its intended purpose) 4x 10Gig SFP+ Networking via DAC SFP+ Moduels (For more info on that i have a post of my entire homelab on my Profile) Drives: Eight 4TB HDDs Two 3TB HDDs Two 2TB HDDs Two SAS 600GB HDDs Two 2TB SATA SSDs Current Setup: Proxmox as the hypervisor TrueNAS Scale as a VM with passthrough for ZFS Iâ€™m facing potential performance issues because TrueNAS Scale is virtualized. Iâ€™m considering a few options and would love your input: Installing Proxmox on SSDs: Utilizing the SSDs for the OS to leverage their speed for the hypervisor. Using SAS drives as cache: Taking advantage of the 12Gbps speed for caching to enhance overall performance. Direct ZFS on Proxmox: Managing the drives directly in Proxmox with ZFS, but Iâ€™m concerned about losing functionalities like SMB sharing, permissions management, and user administration. I also have an old Synology RS2818RP+ that I could use: Synology NAS with 16 bays and 10GbE connectivity. Equipped with the E10M20-T1 (a 10Gig Network card with 2x 2TB NVMe SSDs for network cache). Considering moving all large HDDs to the Synology and using it as the main storage, and only hosting the VMs' main system on the Dell's SSDs (so each VM gets 32GB; the main storage is on the Synology). Direct 10GbE connection to the Dell R750 for high-speed access. (Block Access) Potential Solutions: Bare Metal TrueNAS Scale: Install TrueNAS directly on the server for maximum performance, but lose the flexibility of Proxmox. Proxmox with ZFS and Containers/VMs: Use Proxmox with ZFS for storage and handle additional services (SMB, permissions, Nextcloud) via LXC containers or VMs. Hybrid Approach: Keep Proxmox with ZFS pools, and run a VM for TrueNAS Scale to manage SMB and NFS, with other services running in containers/VMs on Proxmox. Leveraging Synology NAS: Use the Synology as the main storage system with 10GbE connectivity to the Dell R750. Utilize the NVMe SSD cache on the Synology for improved performance. Keep the SSDs and SAS drives in the Dell R750 for caching and high-speed storage needs. Iâ€™d love to hear your thoughts on these approaches or any other suggestions you might have. What would be the best way to balance performance, flexibility, and power efficiency in this setup? Thanks in advance for your help! I hope you all have a good weekend. Best wishes, keep hoarding. a VeganSalat"},
{"Title": "Finding good deals for HDDs and SSDs?", "Author": "u/Neurrone", "Content": "I currently have 4 x 4TB Seagate Ironwolfs and am looking to expand. Are there price trackers that I could use to get a sense of what a good deal would be, in terms of price per TB? Something that can send alerts when prices drop below a certain level would be an added bonus How safe is it to use used drives? They are half the price of new ones, but might fail more quickly."},
{"Title": "Optical Archiving", "Author": "Unknown author", "Content": "Just learned of the new DM for Archiving Blu-ray standard that should last 100 years. This is the player/burner Pioneer BDR-WX01DM JIS X6257 Compliant DM for Archive. Y'all may know this, but just decided to share, share your usage and impresions please."},
{"Title": "New CDs can hold 1+ petabytes of storage", "Author": "u/GiantJupiter45", "Content": "No content"},
{"Title": "There Is No Good Reason To Just Let Unsupported Video Games Die", "Author": "u/AbolishDisney", "Content": "No content"},
{"Title": "Finally... after seven years, a 6TB 2.5-inch external hard drive has made it to market.", "Author": "u/Torley_", "Content": "No content"},
{"Title": "The ArchiveTeam has a \"cost shameboard\" of the top users who are incurring the highest costs for the Internet Archive", "Author": "u/BananaBus43", "Content": "No content"},
{"Title": "IBM LTO4 TS2340 Error Code 5 - drive doesn't work", "Author": "u/awesomefreeman", "Content": "I bought drive via mail without checking it ðŸ˜«. When turned on, it shows error code 5, which corresponds to a hardware problem. I completely disassembled the drive, but could not find the reason. The mechanics inside are good, cassettes can be inserted and removed. The diagnostic program ITDT showed the same error. I donâ€™t know what to do, has anyone encountered a similar problem? https://preview.redd.it/ibm-lto4-ts2340-error-code-5-drive-doesnt-work-v0-knqoyrl5l61d1.jpg https://preview.redd.it/ibm-lto4-ts2340-error-code-5-drive-doesnt-work-v0-zexu05b9l61d1.jpg UPDATE: Entring and exiting maintence mode helped. But after insert cartridge  I've got the same error appears https://preview.redd.it/ibm-lto4-ts2340-error-code-5-drive-doesnt-work-v0-a6q0sqcyp61d1.png"},
{"Title": "Downloading Fullsize Pastperfectonline Images?", "Author": "u/Acrobatic_Owl_4101", "Content": "Hello, in the past I've seen various posts and websites on how to get fullsize images from behind the times museums that post VGA quality images that are mere thumbnails in today's modern screens.  I have not seen tips for getting images from Past Perfect Online sites though.  Does anyone know how I'd get a bigger image than the small \"zoom in\" I get clicking the image?"},
{"Title": "Reusing drives from a WD EX2 Ultra", "Author": "u/Physical_Taste_4487", "Content": "Hi. I currently have a WD My Cloud EX2 Ultra and want to move on from it. It has 2x16TB setup as JBOD. (Full of data) How do I go about using drives in a new enclosure? Is there any way of doing this whilst preserving the data? Or will I have to transfer the data to something else first? I donâ€™t mind doing this if I have to as long as I can use the drives in some other NAS/Enclosure once itâ€™s all done. Any advice much appreciated."},
{"Title": "Is it possible that some day in the distant future, we will have an archive with all the data that was ever uploaded to the internet (also deleted data) that is accessible by anyone?", "Author": "u/NeroD_175", "Content": "I just thought about how cool it would be if something like that existed. Just imagining being able to download whatever I'm thinking about instantly just by typing in a few keywords to find it, regardless if it was deleted or not.. Sure there would probably be some issues with that, like privacy concerns etc. but to me, data is memory and memories are by far the most valuable thing we have as human beings. As a datahoarder who has lost precious data/memories before, this is my ultimate fantasy."},
{"Title": "Sort photos into folders based on EXIFs Description", "Author": "u/SympatheticLion", "Content": "Hey Fellow Hoarders, I'm looking for an open source solution to sort 1000's of mid journey photos to group similar photos and put them into folders automagically based on metadata/EXIF description. The description is quite detailed and it would be ideal if it can be used to also rename the file too. I've been looking into photoprism, digikam, darktable and can't figure out which one will work for this purpose. Photoprism so far can recognize faces and group them, but not add it into a folder based on exif description. Anyone know of a solution that would work? Thanks in advance!!"},
{"Title": "Yes or no for NAS?", "Author": "u/gunzaj", "Content": "No content"},
{"Title": "Extreme Picture Finder Saving video in multiple files", "Author": "u/ReyArthurLXIX", "Content": "Extreme Picture Finder is saving videos in multiple files, an index followed by several videos named video0, video1 and so on, how can i save it all as 1 video?"},
{"Title": "Dell R720XD - Backplane HBA Card Query", "Author": "u/willdab34st", "Content": "Hi, I was going to purchase a second controller for my R720XD, front disk container and backplane is a perc mini mono setup. However I remembered I pulled a few SAS HBA's out of it when I got it. They are Dell g164p 6GB/S HBA cards, apparently they are the same board as the more common H200E just with no internal headers, just external. I'm thinking I can use these instead of purchasing another card, but unsure how I would hook this up, as far as I can deduce I'll need to plug in a backplane somewhere in the rear for the SATA board and I'll have to route the cables from outside the chassis. The g164p cards have a type of sfp connector, assuming this is standard for connecting backplanes etc and buying a backplane off of eBay for R720XD will work with this? Related question, I would assume it's not possible to split the front drive bays so some are served with different controllers? If not then I'd have to mount new drives for the HBA controller in the rear somewhere, there does look established slots there already, 2.5\"."},
{"Title": "Advice on building a DAS", "Author": "u/the_mcm", "Content": "Iâ€™m about to head off to college and want to have a single place for storage with some redundancy. I know itâ€™d be best to have some sort of offisite backup, but I donâ€™t really have the money for that right now judging by the amount of files I need to store. Iâ€™d mostly be storing a lot of video projects (abt 6tb+ currently), hoping to also do some editing off of the DAS, along with playing and storing a decent bit of dvd/blu ray rips (abt 2+ tb and growing). Iâ€™d also probably have a decent but more of other miscellaneous music, videos, etc to store. Doing some limited research I saw good things about the OWC Mercury quad paired with OWCâ€™s SoftRaid to do a RAID5 set up. Pricing things out, it seems like I could get 4 WD Red 8tb HDDs along with the OWC enclosure for right around my budget of about $1k, and would give me 24tb usable space. Does this seem like a reasonable set up for my needs, and would RAID5 be good if Iâ€™ll be writing and reading from the DAS often? Thanks."},
{"Title": "Will storing large files in a NAS wear out hard drives faster?", "Author": "u/Hostile_18", "Content": "Hi all. New to all this. Got my media NAS and am trying to decide between good x254 encodes at around 1.2-2.2gb or full 1080p remux episodes at around 7-12gb. My question is will the quality difference be worth it, and also will it significantly reduce the hard drives lifespan? I've got 6x 20tb Exos drives that have a work load of 550tb a year for 5 years, I guess half that if you run parity once a month. All this and that's not even counting huge 4k remux files. The X-files 1080p remux from the Blu-Ray discs is like 2tb alone. What's anyone elses experience hosting large media files? Have the hard drives held up over time? I have about 6 clients outside the local network, if that makes a significant difference."},
{"Title": "Good software for magazine scanning?", "Author": "u/vier86", "Content": "I am about to embark on a journey to scan and upload a large cache of magazines I have inherited onto https://archive.org/ . The plan is to debind each magazine and scan them on a flatbed Canon CanoScan 9000f. What software would be best to use for this? Should I convert each to a PDF, TIFF, JPEG? Anything else I should know?"},
{"Title": "Data consolidation â€“ home set-up - Mac and DAS", "Author": "u/intastellaburst", "Content": "I have searched through previous posts but they were all either really old or didnâ€™t address this kind of setup. I want to consolidate decades of data from different sources and put it all together on a DAS (or similar) that can plug into a 2018 MacBook Pro (with USB-C). Iâ€™m going with DAS-to-computer setup because I intend to use Backblaze to keep the whole thing backed up to the cloud for low-cost (directly connected drives are included in their unlimited backup and only need to be plugged in once a year to stay backed up). Ideally, Iâ€™d like around 10 TB, but Iâ€™m unsure how to get there. I was hoping to use SSD drives or M.2 in some kind of enclosure but they may be too expensive. It sounds like software RAID is better, but Iâ€™m unsure how to get that going on a Mac, and Iâ€™m not sure which enclosure/drives to get. Oh yes, and the main consideration is cost - I need to do this on the cheap, even if it's a little improvised or rough around the edges. I donâ€™t need screaming speed, but I do want to move and access many photos and videos without having to wait. If 2 or 3 large and slow SATA drives on a particular RAID setup gets me good enough performance through USB-3 or Thunderbolt, thatâ€™s also fine. There are so many options, and Iâ€™m a little bamboozled. So in summary: looking for hardware and Mac RAID suggestions DAS connected to a Mac with USB-C Around 10 TB Fast transfer, but not too expensive Happy to build and find parts Any thoughts?"},
{"Title": "efficient and visually pleasing way to combine all the images I downloaded from a Pinterest board?", "Author": "u/Late_Repair_9899", "Content": "Firstly, I have found my home. Secondly, I save hundreds of images and MP4s monthly. I like to see them digitally (and often print them when I can) because they inspire my art and life and imagination (but also who am I kidding it's a part of the hoarding cycle). I have been using the straight-up Photoshop Contact batch process, but they're always the same size and you can't include the MP4s. I also don't want to spend time on ANY of it (other than satisfying printing and cutting and looking)... so canva templates are OUT. Anything out there that would automate hundreds of images at once?"},
{"Title": "Stablebit Drivepool error", "Author": "u/Earbuduserr", "Content": "Anyone know what to do to get rid of this error?, Ive tried pressing duplicate now and automatically https://preview.redd.it/stablebit-drivepool-error-v0-auic8rk4s01d1.png"},
{"Title": "Need help Kasa download 30 days for possible neglect at managed care home resulting in death", "Author": "u/Yurt_lady", "Content": "I have a problem. I had a Kasa in my husbandâ€™s room at a memory care facility. I have Kasa care and an SD card. My husband passed away on Sunday. The Kasa camera is great but the software is crap. I need to download 30 days of videos and it seems that I have to do it one at a time. I canâ€™t do that. Is there a way to write a batch file or something to download them? Can I hire someone from this group to download the videos? This is a nightmare."},
{"Title": "DPI and archiving family photo prints", "Author": "u/Hoagiewave", "Content": "I need help picking out hardware. I'm going to be scanning some prints, I don't need to worry about film. I come from a normal family of illiterates that throw the negatives out when the prints are made. I'm not that close with my family and I'm not going to be inheriting most of these pictures so it is likely I wont easily ever have a chance to see these pictures again when my grandmother is gone. So while I have the chance I want to do this right and max out the quality of the scans I'm going to get. I've tried to figure this out by myself but it would be nice to get some other opinions. It seems like the Epson v600 might be good enough that it can capture more than enough data than prints can give. The v850 seems have slightly deeper resolution for shades of dark. A decent amount of the pictures will be black and white. Are there any other options I'm missing? It seems a bit strange that a 15 year old scanner would be the best option on the market. Does the 850 produce tangibly better results? Because of the context I want to do this as best I can now. Lastly what resolution should I aim for to have master scans? Thank you"},
{"Title": "copy utility with check boxes or exclusions", "Author": "u/jedihermit", "Content": "I'm looking for something that can select a subset of my rom folder and optionally exclude subfolders from each. For example I want to pick 5-10 folders but omit the video folders in each to save space on the target. These are typically linux systems but I have access to the folders in windows too."},
{"Title": "WD Wireless Pro NAS functionality not working, but usb is working?", "Author": "u/requirehelpwithstuff", "Content": "I searched around and saw some similar reddit posts here so I'm hoping this is my best shot to post it here as well. I have an old wireless pro that I havent used in a long time, so I swapped the hard drive in it and the wireless functionality seems like it has disappeared. The blinking leds indicates that the device boots and goes solid meaning it's broadcasting wifi, but I don't see anything to connect to. It works when plugged via usb with no issues. I tried 'updating' the firmware and resetting it, and i think it went thru, but still nothing. The last time I used it, I remember doing something similar and got to a point where i was able to see the wifi but never connected to it (didnt know the password was SN as a youtube guide said it was the ID of the hdd). Old hardware and don't really expect a hard answer, but any ideas are welcomed."},
{"Title": "How do I format a drive with 512e/4kn (22TB Seagate Exos X22)", "Author": "u/lintstah1337", "Content": "I am upgrading my old HDD to a bigger capacity. I am trying to clone an old HDD (18TB Seagate Exos X18) and would like to only use 1 partition. The cloning software has an option for \"4k alightnment\". Should I use 4k alignment?"},
{"Title": "Merging 35.2K Text files into one 604 GB monstrous text file.", "Author": "u/Latter-Ambassador-65", "Content": "I have no idea how I'm going to do this please comment. note : the drive im doing this on only has 117gb left on it avg file size 15mb - 10gb found a fix with sqlitebrowser ( doesn't merge into 1 text file but it works if you're just trying to search for something in the files ) https://sqlitebrowser.org/"},
{"Title": "External 2TB SSD Kingston vs 4TB HDD Toshiba Canvio", "Author": "u/droom2", "Content": "Both are at a discount, SSD $150usd and HDD $125usd. I'm on a budget, but I need to replace my old 1TB HDD external disk which started to fail after 10 years. I use it for 3Dmax library and backups"},
{"Title": "Compressing old avi files", "Author": "u/Pfftwhy", "Content": "Hey guys, as stated in the title I want to compress a lot of old avi files to a more space efficient format. I would like to keep as much quality as possible since the files itself are shot around 2005, my main concern is if i further reduce the quality the videos will be barely watchable. What would be the best way of doing this? EDIT Rephrased to make my question clearer EDIT 2: Video is interlaced & uses DV EDIT 3: ran mediainfo on one of the files as an example: Format                                   : AVI Format/Info                              : Audio Video Interleave Commercial name                          : DVCAM Format profile                           : OpenDML Format settings                          : WaveFormatEx File size                                : 19.1 GiB Duration                                 : 1 h 31 min Overall bit rate mode                    : Constant Overall bit rate                         : 29.8 Mb/s Frame rate                               : 25.000 FPS Recorded date                            : 2004-05-05 12:45:54.000 IsTruncated                              : Yes Video ID                                       : 0 Format                                   : DV Commercial name                          : DVCAM Codec ID                                 : dvsd Codec ID/Hint                            : Sony Duration                                 : 1 h 31 min Bit rate mode                            : Constant Bit rate                                 : 24.4 Mb/s Width                                    : 720 pixels Height                                   : 576 pixels Display aspect ratio                     : 4:3 Frame rate mode                          : Constant Frame rate                               : 25.000 FPS Standard                                 : PAL Color space                              : YUV Chroma subsampling                       : 4:2:0 Bit depth                                : 8 bits Scan type                                : Interlaced Scan order                               : Bottom Field First Compression mode                         : Lossy Bits/(Pixel*Frame)                       : 2.357 Time code of first frame                 : 00:00:18:20 Time code source                         : Subcode time code Stream size                              : 18.4 GiB (97%) Encoding settings                        : ae mode=full automatic / wb mode=automatic / white balance= / fcm=manual focus Audio ID                                       : 1 Format                                   : PCM Format settings                          : Little / Signed Codec ID                                 : 1 Duration                                 : 1 h 31 min Bit rate mode                            : Constant Bit rate                                 : 1 024 kb/s Channel(s)                               : 2 channels Sampling rate                            : 32.0 kHz Bit depth                                : 16 bits Stream size                              : 670 MiB (3%) Alignment                                : Aligned on interleaves Interleave, duration                     : 240  ms (6.00 video frames)"},
{"Title": "There Is No Good Reason To Just Let Unsupported Video Games Die", "Author": "u/AbolishDisney", "Content": "No content"},
{"Title": "Finally... after seven years, a 6TB 2.5-inch external hard drive has made it to market.", "Author": "u/Torley_", "Content": "No content"},
{"Title": "The ArchiveTeam has a \"cost shameboard\" of the top users who are incurring the highest costs for the Internet Archive", "Author": "u/BananaBus43", "Content": "No content"},
{"Title": "IBM LTO4 TS2340 Error Code 5 - drive doesn't work", "Author": "u/awesomefreeman", "Content": "I bought drive via mail without checking it ðŸ˜«. When turned on, it shows error code 5, which corresponds to a hardware problem. I completely disassembled the drive, but could not find the reason. The mechanics inside are good, cassettes can be inserted and removed. The diagnostic program ITDT showed the same error. I donâ€™t know what to do, has anyone encountered a similar problem? https://preview.redd.it/ibm-lto4-ts2340-error-code-5-drive-doesnt-work-v0-knqoyrl5l61d1.jpg https://preview.redd.it/ibm-lto4-ts2340-error-code-5-drive-doesnt-work-v0-zexu05b9l61d1.jpg UPDATE: Entring and exiting maintence mode helped. But after insert cartridge  I've got the same error appears https://preview.redd.it/ibm-lto4-ts2340-error-code-5-drive-doesnt-work-v0-a6q0sqcyp61d1.png"},
{"Title": "Downloading Fullsize Pastperfectonline Images?", "Author": "u/Acrobatic_Owl_4101", "Content": "Hello, in the past I've seen various posts and websites on how to get fullsize images from behind the times museums that post VGA quality images that are mere thumbnails in today's modern screens.  I have not seen tips for getting images from Past Perfect Online sites though.  Does anyone know how I'd get a bigger image than the small \"zoom in\" I get clicking the image?"},
{"Title": "Reusing drives from a WD EX2 Ultra", "Author": "u/Physical_Taste_4487", "Content": "Hi. I currently have a WD My Cloud EX2 Ultra and want to move on from it. It has 2x16TB setup as JBOD. (Full of data) How do I go about using drives in a new enclosure? Is there any way of doing this whilst preserving the data? Or will I have to transfer the data to something else first? I donâ€™t mind doing this if I have to as long as I can use the drives in some other NAS/Enclosure once itâ€™s all done. Any advice much appreciated."},
{"Title": "Is it possible that some day in the distant future, we will have an archive with all the data that was ever uploaded to the internet (also deleted data) that is accessible by anyone?", "Author": "u/NeroD_175", "Content": "I just thought about how cool it would be if something like that existed. Just imagining being able to download whatever I'm thinking about instantly just by typing in a few keywords to find it, regardless if it was deleted or not.. Sure there would probably be some issues with that, like privacy concerns etc. but to me, data is memory and memories are by far the most valuable thing we have as human beings. As a datahoarder who has lost precious data/memories before, this is my ultimate fantasy."},
{"Title": "Sort photos into folders based on EXIFs Description", "Author": "u/SympatheticLion", "Content": "Hey Fellow Hoarders, I'm looking for an open source solution to sort 1000's of mid journey photos to group similar photos and put them into folders automagically based on metadata/EXIF description. The description is quite detailed and it would be ideal if it can be used to also rename the file too. I've been looking into photoprism, digikam, darktable and can't figure out which one will work for this purpose. Photoprism so far can recognize faces and group them, but not add it into a folder based on exif description. Anyone know of a solution that would work? Thanks in advance!!"},
{"Title": "Yes or no for NAS?", "Author": "u/gunzaj", "Content": "No content"},
{"Title": "Extreme Picture Finder Saving video in multiple files", "Author": "u/ReyArthurLXIX", "Content": "Extreme Picture Finder is saving videos in multiple files, an index followed by several videos named video0, video1 and so on, how can i save it all as 1 video?"},
{"Title": "Dell R720XD - Backplane HBA Card Query", "Author": "u/willdab34st", "Content": "Hi, I was going to purchase a second controller for my R720XD, front disk container and backplane is a perc mini mono setup. However I remembered I pulled a few SAS HBA's out of it when I got it. They are Dell g164p 6GB/S HBA cards, apparently they are the same board as the more common H200E just with no internal headers, just external. I'm thinking I can use these instead of purchasing another card, but unsure how I would hook this up, as far as I can deduce I'll need to plug in a backplane somewhere in the rear for the SATA board and I'll have to route the cables from outside the chassis. The g164p cards have a type of sfp connector, assuming this is standard for connecting backplanes etc and buying a backplane off of eBay for R720XD will work with this? Related question, I would assume it's not possible to split the front drive bays so some are served with different controllers? If not then I'd have to mount new drives for the HBA controller in the rear somewhere, there does look established slots there already, 2.5\"."},
{"Title": "Advice on building a DAS", "Author": "u/the_mcm", "Content": "Iâ€™m about to head off to college and want to have a single place for storage with some redundancy. I know itâ€™d be best to have some sort of offisite backup, but I donâ€™t really have the money for that right now judging by the amount of files I need to store. Iâ€™d mostly be storing a lot of video projects (abt 6tb+ currently), hoping to also do some editing off of the DAS, along with playing and storing a decent bit of dvd/blu ray rips (abt 2+ tb and growing). Iâ€™d also probably have a decent but more of other miscellaneous music, videos, etc to store. Doing some limited research I saw good things about the OWC Mercury quad paired with OWCâ€™s SoftRaid to do a RAID5 set up. Pricing things out, it seems like I could get 4 WD Red 8tb HDDs along with the OWC enclosure for right around my budget of about $1k, and would give me 24tb usable space. Does this seem like a reasonable set up for my needs, and would RAID5 be good if Iâ€™ll be writing and reading from the DAS often? Thanks."},
{"Title": "Will storing large files in a NAS wear out hard drives faster?", "Author": "u/Hostile_18", "Content": "Hi all. New to all this. Got my media NAS and am trying to decide between good x254 encodes at around 1.2-2.2gb or full 1080p remux episodes at around 7-12gb. My question is will the quality difference be worth it, and also will it significantly reduce the hard drives lifespan? I've got 6x 20tb Exos drives that have a work load of 550tb a year for 5 years, I guess half that if you run parity once a month. All this and that's not even counting huge 4k remux files. The X-files 1080p remux from the Blu-Ray discs is like 2tb alone. What's anyone elses experience hosting large media files? Have the hard drives held up over time? I have about 6 clients outside the local network, if that makes a significant difference."},
{"Title": "Good software for magazine scanning?", "Author": "u/vier86", "Content": "I am about to embark on a journey to scan and upload a large cache of magazines I have inherited onto https://archive.org/ . The plan is to debind each magazine and scan them on a flatbed Canon CanoScan 9000f. What software would be best to use for this? Should I convert each to a PDF, TIFF, JPEG? Anything else I should know?"},
{"Title": "Data consolidation â€“ home set-up - Mac and DAS", "Author": "u/intastellaburst", "Content": "I have searched through previous posts but they were all either really old or didnâ€™t address this kind of setup. I want to consolidate decades of data from different sources and put it all together on a DAS (or similar) that can plug into a 2018 MacBook Pro (with USB-C). Iâ€™m going with DAS-to-computer setup because I intend to use Backblaze to keep the whole thing backed up to the cloud for low-cost (directly connected drives are included in their unlimited backup and only need to be plugged in once a year to stay backed up). Ideally, Iâ€™d like around 10 TB, but Iâ€™m unsure how to get there. I was hoping to use SSD drives or M.2 in some kind of enclosure but they may be too expensive. It sounds like software RAID is better, but Iâ€™m unsure how to get that going on a Mac, and Iâ€™m not sure which enclosure/drives to get. Oh yes, and the main consideration is cost - I need to do this on the cheap, even if it's a little improvised or rough around the edges. I donâ€™t need screaming speed, but I do want to move and access many photos and videos without having to wait. If 2 or 3 large and slow SATA drives on a particular RAID setup gets me good enough performance through USB-3 or Thunderbolt, thatâ€™s also fine. There are so many options, and Iâ€™m a little bamboozled. So in summary: looking for hardware and Mac RAID suggestions DAS connected to a Mac with USB-C Around 10 TB Fast transfer, but not too expensive Happy to build and find parts Any thoughts?"},
{"Title": "efficient and visually pleasing way to combine all the images I downloaded from a Pinterest board?", "Author": "u/Late_Repair_9899", "Content": "Firstly, I have found my home. Secondly, I save hundreds of images and MP4s monthly. I like to see them digitally (and often print them when I can) because they inspire my art and life and imagination (but also who am I kidding it's a part of the hoarding cycle). I have been using the straight-up Photoshop Contact batch process, but they're always the same size and you can't include the MP4s. I also don't want to spend time on ANY of it (other than satisfying printing and cutting and looking)... so canva templates are OUT. Anything out there that would automate hundreds of images at once?"},
{"Title": "Stablebit Drivepool error", "Author": "u/Earbuduserr", "Content": "Anyone know what to do to get rid of this error?, Ive tried pressing duplicate now and automatically https://preview.redd.it/stablebit-drivepool-error-v0-auic8rk4s01d1.png"},
{"Title": "Need help Kasa download 30 days for possible neglect at managed care home resulting in death", "Author": "u/Yurt_lady", "Content": "I have a problem. I had a Kasa in my husbandâ€™s room at a memory care facility. I have Kasa care and an SD card. My husband passed away on Sunday. The Kasa camera is great but the software is crap. I need to download 30 days of videos and it seems that I have to do it one at a time. I canâ€™t do that. Is there a way to write a batch file or something to download them? Can I hire someone from this group to download the videos? This is a nightmare."},
{"Title": "DPI and archiving family photo prints", "Author": "u/Hoagiewave", "Content": "I need help picking out hardware. I'm going to be scanning some prints, I don't need to worry about film. I come from a normal family of illiterates that throw the negatives out when the prints are made. I'm not that close with my family and I'm not going to be inheriting most of these pictures so it is likely I wont easily ever have a chance to see these pictures again when my grandmother is gone. So while I have the chance I want to do this right and max out the quality of the scans I'm going to get. I've tried to figure this out by myself but it would be nice to get some other opinions. It seems like the Epson v600 might be good enough that it can capture more than enough data than prints can give. The v850 seems have slightly deeper resolution for shades of dark. A decent amount of the pictures will be black and white. Are there any other options I'm missing? It seems a bit strange that a 15 year old scanner would be the best option on the market. Does the 850 produce tangibly better results? Because of the context I want to do this as best I can now. Lastly what resolution should I aim for to have master scans? Thank you"},
{"Title": "copy utility with check boxes or exclusions", "Author": "u/jedihermit", "Content": "I'm looking for something that can select a subset of my rom folder and optionally exclude subfolders from each. For example I want to pick 5-10 folders but omit the video folders in each to save space on the target. These are typically linux systems but I have access to the folders in windows too."},
{"Title": "WD Wireless Pro NAS functionality not working, but usb is working?", "Author": "u/requirehelpwithstuff", "Content": "I searched around and saw some similar reddit posts here so I'm hoping this is my best shot to post it here as well. I have an old wireless pro that I havent used in a long time, so I swapped the hard drive in it and the wireless functionality seems like it has disappeared. The blinking leds indicates that the device boots and goes solid meaning it's broadcasting wifi, but I don't see anything to connect to. It works when plugged via usb with no issues. I tried 'updating' the firmware and resetting it, and i think it went thru, but still nothing. The last time I used it, I remember doing something similar and got to a point where i was able to see the wifi but never connected to it (didnt know the password was SN as a youtube guide said it was the ID of the hdd). Old hardware and don't really expect a hard answer, but any ideas are welcomed."},
{"Title": "How do I format a drive with 512e/4kn (22TB Seagate Exos X22)", "Author": "u/lintstah1337", "Content": "I am upgrading my old HDD to a bigger capacity. I am trying to clone an old HDD (18TB Seagate Exos X18) and would like to only use 1 partition. The cloning software has an option for \"4k alightnment\". Should I use 4k alignment?"},
{"Title": "Merging 35.2K Text files into one 604 GB monstrous text file.", "Author": "u/Latter-Ambassador-65", "Content": "I have no idea how I'm going to do this please comment. note : the drive im doing this on only has 117gb left on it avg file size 15mb - 10gb found a fix with sqlitebrowser ( doesn't merge into 1 text file but it works if you're just trying to search for something in the files ) https://sqlitebrowser.org/"},
{"Title": "External 2TB SSD Kingston vs 4TB HDD Toshiba Canvio", "Author": "u/droom2", "Content": "Both are at a discount, SSD $150usd and HDD $125usd. I'm on a budget, but I need to replace my old 1TB HDD external disk which started to fail after 10 years. I use it for 3Dmax library and backups"},
{"Title": "Compressing old avi files", "Author": "u/Pfftwhy", "Content": "Hey guys, as stated in the title I want to compress a lot of old avi files to a more space efficient format. I would like to keep as much quality as possible since the files itself are shot around 2005, my main concern is if i further reduce the quality the videos will be barely watchable. What would be the best way of doing this? EDIT Rephrased to make my question clearer EDIT 2: Video is interlaced & uses DV EDIT 3: ran mediainfo on one of the files as an example: Format                                   : AVI Format/Info                              : Audio Video Interleave Commercial name                          : DVCAM Format profile                           : OpenDML Format settings                          : WaveFormatEx File size                                : 19.1 GiB Duration                                 : 1 h 31 min Overall bit rate mode                    : Constant Overall bit rate                         : 29.8 Mb/s Frame rate                               : 25.000 FPS Recorded date                            : 2004-05-05 12:45:54.000 IsTruncated                              : Yes Video ID                                       : 0 Format                                   : DV Commercial name                          : DVCAM Codec ID                                 : dvsd Codec ID/Hint                            : Sony Duration                                 : 1 h 31 min Bit rate mode                            : Constant Bit rate                                 : 24.4 Mb/s Width                                    : 720 pixels Height                                   : 576 pixels Display aspect ratio                     : 4:3 Frame rate mode                          : Constant Frame rate                               : 25.000 FPS Standard                                 : PAL Color space                              : YUV Chroma subsampling                       : 4:2:0 Bit depth                                : 8 bits Scan type                                : Interlaced Scan order                               : Bottom Field First Compression mode                         : Lossy Bits/(Pixel*Frame)                       : 2.357 Time code of first frame                 : 00:00:18:20 Time code source                         : Subcode time code Stream size                              : 18.4 GiB (97%) Encoding settings                        : ae mode=full automatic / wb mode=automatic / white balance= / fcm=manual focus Audio ID                                       : 1 Format                                   : PCM Format settings                          : Little / Signed Codec ID                                 : 1 Duration                                 : 1 h 31 min Bit rate mode                            : Constant Bit rate                                 : 1 024 kb/s Channel(s)                               : 2 channels Sampling rate                            : 32.0 kHz Bit depth                                : 16 bits Stream size                              : 670 MiB (3%) Alignment                                : Aligned on interleaves Interleave, duration                     : 240  ms (6.00 video frames)"},
{"Title": "How to decrypt video files?", "Author": "u/VibWhore", "Content": "So there's this educational institution which has provided me with a number of video lectures in a special format that can only be accessed throught an application called \"Player\" by HSC-PK available on Microsoft Store. I have those videos downloaded on my device but they are in a special format, which can only be accessed through the \"Player\" by entering the credentials in it. Is there anyway that I can figure out to convert those files into a normal .MP4 format through which I can open it and use it normally?"},
{"Title": "Is this a relabeled Ultrastar HC570?", "Author": "u/slash8793", "Content": "I bought what was supposed to be a 22TB Ultrastar HC570 from an Ebay seller. What I received appears to be from a shucked external drive. Before I return it, can anyone tell me if it's just a relabeled HC570? The model number is WD220EDGZ, and underneath it says \"SATA 6Gb/s IU HA570\". No mention of the Ultrastar name anywhere on the label. That's fine if it's the exact same thing, but I'm not sure it is. Did I get a lower quality drive? Thanks."},
{"Title": "seagate exos refurbished read speed hitting 130MBps", "Author": "u/honelnik", "Content": "I just got a refurbished exos ST16000NM000D (I can't find any datasheet, which quite bothers me) for my elitedesk 800 G4 and I am running the crypto-test from https://wiki.archlinux.org/title/Badblocks#Alternatives . After the intial 280-ish MBps write speeds (going down somewhere to 200), I am currently running the compare step and again, from 270-280 MBps in the begininng I am hitting 130 MBps speeds after some 13hrs of running: tps kB_read/s kB_wrtn/s kB_dscd/s kB_read kB_wrtn kB_dscd Device 1079.00 134.9M 0.0k 0.0k 134.9M 0.0k 0.0k sdd first time I am building a server with such a large drive so I do not have anything to compare it with. Does it get this slow over sustained read and at the end of a drive?"},
{"Title": "Has anyone noticed a sigificant decrease in USB drive reliability? many DOA's..", "Author": "u/appletechgeek", "Content": "Has anyone noticed a sigificant increase in problems with USB drives as of late? i know USB drives are never reliable to begin with, but it's become so bad that i'd rather use a USB to sata/m.2 adapter than consider usb drives to make even small transfers between machines, all usb drive's i buy new either come DOA, or die within a extremely short time span, i thought it was my machine at first, but at this point it's been happening to 5+ machines consistently mac's and pc's. whatever, the drives would either not get detected, or fail format's and get stuck in a raw state, or pretend they are being constantly accessed while they are not, this happens to sandisk and philips brand usb drives, both usb 2.0 and 3.0 drive's. both small 16 gig and bigger 64/128 gig drive's, anyone been experiecing the same?"},
{"Title": "Need some help with mergerfs + sonarr - keep a tv show on one drive?", "Author": "u/fillilutten", "Content": "MergerFS is set to default existing path - most free space. I have 3 disks in this mergerfs pool where one disk is full. I ran into a problem when sonarr tried to import a new season to a show that is already on the full disk. I would like to keep a whole show on the same drive but I of course don't care to have different shows on different drives. I solved it by manually moving the whole show to the /tv folder on a drive with free space. But are there any solution to automate this? Thanks!"},
{"Title": "Alternatives to Real-Debrid?", "Author": "u/Few_Landscape_573", "Content": "Do you guys know any good ones that donâ€™t log users info? Iâ€™m worried about security data breaches about my personal info."},
{"Title": "How do I store family photos?", "Author": "u/Wonder_Pretty", "Content": "First off I know nothing about computers, hard drives, cloud storage etc. But, I need help to understand how to store my family photos long term. My aunt passed away last Friday. I'm at her visitation right now. My sisters and my other aunt put together a few photo boards of my aunt. And seeing all of the photos reminds me that I seriously need to learn how to protect my family photos, videos, family history, my own personal files, etc. I know that I need to make multiple copies and use multiple ways of store things,  hard drives, cloud storage, external hard drives. Apart from that I know absolutely nothing about computers, I don't even have a computer. Currently I have things stored on a few thumb drives and I know I shouldn't rely on those for storage. Please help. Update: I want to thank everyone for their advice and suggestions. I'm sorry that I haven't replied to everyone's comments. Life is bit hectic at the moment. When I get some free time I'll be able to read and process everyone's comments. Again thank you all for your help I really appreciate it right now."},
{"Title": "Cheap external ssds > 4TB?", "Author": "u/newtoaster", "Content": "So we are selling the house and moving into an RV to travel full time. My PLEX server is a mac mini that currently has a 4 drive external enclosure that runs crazy hot and has a fan that sounds like a leaf blower. Power is at a premium off grid, so I want to replace it with something solid state. I need more than 4TB (6 would do). Whats my cheapest option at the moment for something thats not trash?"},
{"Title": "What's a reasonbale size for an external HDD?", "Author": "u/szetadom", "Content": "Hello! I'm planning on buying an external HDD to make a backup of my phone, photos, important documents, text files, etc. I don't know how big of an HDD to buy? I think I'll need about 2TBs of storage in the foreseeable future so I was thinking about getting a 4TB drive and call it a day. My question is that is it worth for me to buy a disk with more storage capacity like 8 TB or even larger?"},
{"Title": "What is the most data you have used in a month?", "Author": "u/Embire", "Content": "I am the CEO of one of Africa's largest ISP's and last month we clocked our highest data user ever. 98,1TB.  Keep in mind, we only sell 4G and 5G wireless connectivity.  I know that for a Fibre company, this is pretty normal. What is the most data you have ever used on your Mobile plan per month?"},
{"Title": "Solution To Growing Collection Of WD Elements External Drives?", "Author": "u/It_Is_JAMES", "Content": "With limited desk space and power outlets, I currently have 4 WD Elements external drives stacked on top of each other, ranging from 10-18TB. Wanting to get a 5th, but realizing that this probably isn't an ideal solution for heat or longevity of the drives. Currently, everything is going into a powered USB hub and connecting to my laptop. Was wondering what solutions are available that are worth looking into, that offer the best value for the price. NAS? A USB hard drive bay? Or, would buying a couple extra externals really not be a problem? I'll probably only buy 2 more drives before building a desktop that could hold a lot of the drives internally anyway."},
{"Title": "I can't mount this partition, I might have encrypted it via VeraCrypt but it's not taking my usual passwords. Am I screwed?", "Author": "u/RandomADHDaddy", "Content": "No content"},
{"Title": "Open source datasharing service", "Author": "Unknown author", "Content": "Hey guys! I have noticed that there is not much in the realm of open source datasharing services, so I created a Django REST / React app that allows for upload, download, reviewing, etc, of files. Not sure if would be useful to people. Also, please feel free add features. This is meant to be an open source project that allows research labs / people to share and review datasets without needing to pay for any online subscriptions. https://github.com/lxaw/DataDock"},
{"Title": "I just uploaded to The Internet Archive the most complete collection of games and software by Wiering Software.", "Author": "u/Impossible_Salary_38", "Content": "I just uploaded to The Internet Archive the most complete collection of games and software of Wiering Software company who made classic well known games for MS-DOS and ported them to other platforms like Android, Flash and Windows. The collection contains all the different versions of each game for each platform, plus it has the registered (complete) version of the games Super Angelo and Super Worms for DOS and a modded full version of Charlie the Duck for android. Since most of the games are shareware (demo), I think most people like me could only play the demo version of Super Angelo, limited to 3 levels only, since the shareware version is the easiest to find. The complete version has way more levels and the final ending. The Flash version of Super Angelo is also complete. Some of the games can't be be purchased anymore and remain in their demo version like Olaf & Elmar In the Castle of Nabokos. Some of the games include their user's manual, level maps (Super Worms) or source code (Mario & Luigi and Mario!!!). There are also two tools included, one for editing tiles, sprites and maps and another just for navigating folders with icons. Complete collection of Wiering Software games and software"},
{"Title": "So i can copy EVERYTHING on my D:/ drive into my backup storage, except for these 2 folders that gives the error (yes, they're portable p*rn games). Is my 2 years old evo 870 failing? even though the games still works just fine.", "Author": "u/Arctic-Warfare9000", "Content": "https://preview.redd.it/so-i-can-copy-everything-on-my-d-drive-into-my-backup-v0-zg8xhn1oct0d1.png https://preview.redd.it/so-i-can-copy-everything-on-my-d-drive-into-my-backup-v0-utizoqbbbt0d1.png I just can't do anything with the files (copy, move, del) except for the fact that I can still open anything inside like folders, text files, and even play both games with no issues. I'm currently in the process of backing up my personal files because i'm planning to assemble my newly bought pc parts and I want to do a fresh install on it. Planning to wipe the evo 870 ssd and do a fresh install on it and use it as my main storage. Both folders amount to 250gb in size. Is my SSD failing? how do i check?"},
{"Title": "Re-formatting SMR HDD as NTFS, or just keeping as exFAT?", "Author": "u/Caesar_35", "Content": "So, all of my previous drives have been your run-of-the-mill 2.5\" portables, all pre-formatted NTFS. I only use them for backup purposes, so phone pictures, game screenshots, some files, etc, which I manually copy across to two drives (muh redundancy). Lots of small files basically. My dutiful 2TB drive's recently gotten the dreaded red line of \"your space's running out\", and I saw a good deal on an 8TB 3.5\" external so jumped on it. However, it comes pre-formatted exFAT, which I'm aware isn't as well regarded as NTFS. I've only ever seen them on flashdrives myself. It's also an SMR drive (spooky, I know), so I'm not sure if NTFS or exFAT makes a difference there. Basically my question is, is it really worth while re-formatting to NTFS, or can I just keep it exFAT? This is going to be the backup drive of my backup drive, but I'd still rather it not get corrupted or have any other issues down the line. I'd also rather not unnecessarily spend time re-formatting it if there's not much reason to. I mean, I figure it should be fine since that's how it came out the box, but better safe than sorry. It's one of those newer-era Seagate Expansions with the diagonal lines, if that makes any difference. And lastly, if I do go the route of re-formatting, would a quick format be fine given it's never been used for anything? And any guidelines for allocation unit size, being an SMR drive (is smaller/larger size better)? Thanks all!"},
{"Title": "Will they ever change the sizing descriptions to actual space available?", "Author": "u/Strange_Advisor_", "Content": "I understand the gap with you buy a 10TB hard drive you get 9TB of actual usable space, but with the growth in the size of hard drives the gap is getting more and more. And lots of people seem to not realize they ARE getting what they pay for. So I cant help but wonder if they will ever change the naming convention to actual available space just to appease the masses?"},
{"Title": "Any software/programs that can \"rank\" files?", "Author": "u/ProjectSpaceRain", "Content": "I know this sounds kind of stupid but I'm wondering if there's anything that can organize or in this case 'rank' files based on given value. Let's say I want file A to be ranked 'Gold'  and file B to rank 'Silver'. It's almost similar to file tagging but really isn't. File tagging just tags whatever keyword is assigned to a file and doesn't actually 'rank' them, you'd want Gold to be above silver,...or something like that."},
{"Title": "Corrupted Nvme Bwin SSD showing as \"no media\" with 0b", "Author": "u/IsaacYPunto", "Content": "I got a dual boot system with win11 and ubuntu 23, everything was ok till yesterday when I powered off the system and then it showed a message like \"kernel must be mounted before\" then i restarted the system again  and the disk dissapeared. I tried to reformat it but even it doesn't appear in the UEFI. I tried to read it in Mac and Linux systems but nothing, then i try it on Windows and it shows a drive but an error message says \"cant get access to F:\\, incorrect function\" when I try to open it. On DiskManager appears as \"No media\" I think it can't mount it or it is not initialized for some reason. I tried to use Gparted, testdisk and EasyUS data recovery as well but it doesn't appear in any of them. Is there a solution? Ty soo much."},
{"Title": "Seeking Help Extracting Dog Photos from CCTV Footage (120GB, ~1 Month)", "Author": "u/BubblyJubsWhale", "Content": "Hi all, I'm hoping to find a way to extract images/clips of my dog from about 120GB of home CCTV as MP4 files (roughly one month's worth). He recently passed, and I want to preserve any memories I can. I have a home lab server and access to a RTX GPU, so I'm open to software, scripts, or even machine learning for animal detection (if feasible). The footage is mostly static shots of my living room, so automating the search would be amazing. I'm aware this might not be the most typical DataHoarder topic, but I'm hoping someone here has experience or ideas that could help. Any recommendations or advice are greatly appreciated! Thanks!"},
{"Title": "Shut Down NAS after Years running for a Vacation break?", "Author": "u/cometa73", "Content": "We are planning to take several vacations in the coming weeks. During this time, I would like to turn off everything in our household, as electricity prices in my country (Germany) are very high. Dev-Servers, APs, FW, etc. should not be a problem, but I have some concerns about my two NAS devices. NAS1 (Synology) with 2x4TB has been running continuously for 5.8 years without any issues, while NAS2 (QNAP) 2x12TB has been running for about 9 months. What is the life expectancy of the HDDs, especially for NAS1, if the HDDs have been running for almost 6 years and will now be off for about 10 weeks? Iâ€™m not concerned about data security; all data is backed up multiple times. I want to save some energy; although I can forego the savings if I have to buy new disks right after the vacation trip. Has anyone experience with this? Can I shut down the NAS devices? What do you recommend?"},
{"Title": "Been buying cheap SSDs on Ali and Temu", "Author": "u/ThyRhubarb", "Content": "I avoid Western brands especially Samsung which are the mostly fakes ones (really what's with all those 1080 pros). Got a $80 crucial p3 plus 2tb, $35 1 tb Fanxiang s660 off a pricing glitch from Temu. Apart from delayed shipping ($5 credit for me lol) product confirmed to be real with testing and device id. The Fanxiang got slightly faster read but slower write than the Crucial about 2.4 vs 2.8GB/s seq write 1GB (in a asm246X usb4 enclosure). Crucial one runs way hotter though while the Fanxiang stays cool even under load. 2x benchmark followed by 5 min SSD cloning from 200GB"},
{"Title": "Download hi-res image from Smithsonian website?", "Author": "u/Kresling", "Content": "I'd appreciate advice on how to get the full-size image files from the Smithsonian website: https://www.si.edu/object/group-photo-gallery-200-artists-provincetown-massachusetts:AAADCD_item_5861 I tried JDownloader and WFDownloader, but no luck so far."},
{"Title": "Should I have redundancy in a NAS if I have a physical copy in a separate location?", "Author": "u/TheSoupGuyMan", "Content": "I am planning on setting up my first NAS, and I was wondering if redundancy is still recommended even if I have a copy of the data updated regularly (~once a week)."},
{"Title": "Commercial grade photo scanners?", "Author": "u/crossfitdood", "Content": "Hey everyone. So I recently got into data hoarding. It first started with a Plex server, then i archived ALL of my photos. When I was showing my archived photos to my family in the Synology photos app, my grandma asked if I could digitize her thousands upon thousands of photos. Then she spread the word to her emblem club and elks lodge friends that I was doing it for her and now a half dozen of her friends want me to do theirs too. They offered to pay me for the service. I was looking at scanners, and it seems like consumer grade scanners are a hit and miss. They are either too slow, or the ones that are fast leave marks on the originals. I tried looking up commercial grade scanners but I'm not able to find anything. I'm sure they're expensive, but I've been wanting to start a side business for my wife, so that she can quit her current job and be at home with the kids more. So I'm willing to invest in a good quality machine if the market is there."}