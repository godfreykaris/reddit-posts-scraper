[
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Textual Serve (https://github.com/Textualize/textual-serve) is a project which serves TUIs (built with Textual) in the browser.\n  \n    This is self-hosted, so you don't need to rely on any external service."
},
{
    "title": "No title",
    "content": "Brandt Bucher talks on JIT compiler for Python at CPython Core Developer Sprint. Brandt is  a member of the Faster CPython project, which is working on making the reference implementation of the language faster via a variety of techniques.\n  \nhttps://www.youtube.com/watch?v=HxSHIpEQRjs"
},
{
    "title": "No title",
    "content": "CherrySaaS is a 100% open-source SaaS template that lets you build crazy beautiful SaaS using Chakra and Radix UIs powered by Reflex. It is currently a work in progress and requires community contribution because I don't have the energy to write the entire template. If I were to do an entire template, I would never open-source it as it would become a very large project. The reason is that it is a rare project as no *free SaaS template has ever been made in Pure-Python. There are proprietary 2 SaaS templates made using Reflex. So I decided to make a free one for the community too. Here's the planning on the notion I did not so long ago.If you want to be a collaborator, reach to me out on Discord (adarshgourabmahalik) or post your GitHub email in this discussion. I have looked at a SaaS template called shipfast.com (which is getting popular nowadays). Ah, yes this also became an inspiration but I wanted to make one free.Selling Shovels During The Gold Rush ❌Giving away shovels for free during the Gold Rush ✔Here's a discord link to preview the landing page of the template.https://discord.com/channels/1029853095527727165/1063735841333198938/1252953503106732053\n\n    [REPOSITORY]"
},
{
    "title": "No title",
    "content": "What my project does\n    Performs OCR on scanned Books using Microsoft Azure Document Intelligence read\n  Target audience\n    People who are unsatisfied with traditional OCR People who want to add clear text to the original PDF and not just extract the text. People who want to archive documents at best quality.\n  Comparasion\n    In my use case traditional OCR was near to useless. Tesseract was meh, Google API didn't process large files. Document Intelligence takes up to 500MB (although in practice a little less), and is possible to OCR 400-600 pages over books in batch by dividing and merging the source and results locally by only a few chunks. It doesn't provide the text in PDF form so that was my reason to start this project.\n  \n    Still in alpha and in separate modules and a lot of rigid coding, but it is working fine for my original task so thought maybe I'd showcase it.\n  \nhttps://github.com/DesertDoggy/json3pdf"
},
{
    "title": "No title",
    "content": "Source code: https://github.com/ben-n93/beachwatch\n\nWhat my Project DoesIn NSW, Australia the government provides an API with daily updates on beaches' pollution and water quality forecast.\n  \n    I thought I'd make a simple wrapper in Python to make it easier to get data from the API.\n  \nTarget AudienceMost likely Australian data scientists/analysts/developers interested in beach data."
},
{
    "title": "No title",
    "content": "https://www.gauge.sh/blog/parsing-python-asts-20x-faster-with-rust"
},
{
    "title": "No title",
    "content": "I should have saved the post but maybe 4-6 months ago I was reading a post (I am pretty sure it was in r/Python) where someone created a package that creates a visual for data contained within a list. For example, let’s say I have a data frame where one of the columns is named “colors” and each record contains a list of colors. One record might be [black,blue,yellow] another record might have [blue,yellow,black]. The visual had two parts where the top was a column chart to show the frequency of the list combinations and below the column chart was more of a table that showed each “color” as one column and then across the row for each color and under the columns from the chart above was an indicator of sorts that would be greyed out of the color for that row was not in the corresponding columns list and highlighted another color of it was. Anyways this is probably a long shot but either the package or the name of this visual would be super helpful. Thanks python community!"
},
{
    "title": "No title",
    "content": "I will buy a laptop for coding purposes but just started learning and practising Python using Pyecharm. What are the software requirements that lead to hardware specs a general Python coder must look into?\n  \n    Please suggest the hardware setup within a pocket friendly budget."
},
{
    "title": "No title",
    "content": "End goal is to produce PDF using external data and a template. Needs to support Jinja tags, conditionals and loops.\n  \n    Using https://github.com/Kozea/WeasyPrint and https://github.com/pallets/jinja as base stack (Open to other suggestions)\n  \n    I was thinking of building some base HTML templates but would be awesome if I could find a visual HTML editor that could produce code 100% compatible with Weasyprint so that end users can build templates by themselves or modify existing ones.\n  \n    Could be Wysiwyg based using https://editorjs.io or https://github.com/slab/quillor more advanced web builders like https://github.com/GrapesJS/grapesjs\n\n    Anybody built something similar?"
},
{
    "title": "No title",
    "content": "I'm going to show you how to get Scrapegraph AI up and running, how to set up a language model, how to process JSON, scrape websites, use different AI models, and even turning your data into audio. Sounds like a lot, but it's easier than you think, and I'll walk you through it step by step.\n  \nhttps://www.scrapingbee.com/blog/scrapegraph-ai-tutorial-scrape-websites-easily-with-llama-ai/"
},
{
    "title": "No title",
    "content": "What's rug library:\n\n    Library for fetching various stock data from the internet (official and unofficial APIs).\n  \nSource code:\n\nhttps://gitlab.com/imn1/rug\n\nReleases including changelog:\n\nhttps://gitlab.com/imn1/rug/-/releases"
},
{
    "title": "No title",
    "content": "NumPy 2.0.0 is the first major release since 2006.\n  \n\n\nhttps://github.com/numpy/numpy/releases/tag/v2.0.0\n\n\n\nhttps://numpy.org/devdocs/release/2.0.0-notes.html\n\n\n\nhttps://numpy.org/devdocs/numpy_2_0_migration_guide.html"
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    While looking for task queues, I found that there are many options available in the Python ecosystem, making it really hard to choose the right one. To get a sense of how each library performs and to help make an informed decision, I conducted a load test on some of the most popular ones: Python-RQ, ARQ, Celery, Huey, and Dramatiq.\n  \nTarget Audience\n\n    I hope my findings can help those who are also looking for a task queue solution in Python.\n  \nComparison\n\n    Most articles out there seem to focus on comparing the features of these libraries but rarely discuss performance. While there could be a lot of improvements on my tests, I think it still provide some different insights into how each library handles heavy loads and concurrency.\n  \nLinks:\n\n    You can read  my findings on my blog\n\n    Check out the source code: on Github\n\n    Thanks"
},
{
    "title": "No title",
    "content": "Hi, the different ways to delete duplicate elements from a list in python is discussed in this tutorial video. Also, techniques to remove duplicates from list of lists and list of dictionaries in python are explained\n  \nhttps://youtu.be/CN-xT7iqTl0?si=t_1BBKKg2zF047_D"
},
{
    "title": "No title",
    "content": "What My Project Does\n\nAurora is a fast, extensible Python static site generator. With Aurora, I can generate my personal website (~1,700 files, with multiple layers of jinja2 templates for each page) in < 4 seconds. Aurora generated 292,884 pages from a Hacker News post dataset in 2m:20s.\n  \n    Aurora supports incremental static regeneration, where pages can be regenerated in under 400ms, with hot reloading. I documented how this works on my blog.\n  \nTarget Audience\n\n    I'm building Aurora to help me run my website, but it is built to be general so you can use it for your own projects. I would love feedback!\n  \n    I want this to be a tool for running static sites in production, at scale.\n  \nComparison\n\n    Aurora is inspired by the folder structure of Jekyll, but is written in Python. It has a hooks API that lets you define custom Python functions that manipulate the state of a page. This allows you to implement custom behaviours in isolation of the engine itself. I use this to open link previews from a cache that I plan to use on my website, among other things."
},
{
    "title": "No title",
    "content": "Does anyone know of a full course to learn the Python Tkinter library? I want courses that\n  \n\n\n    Are in video format\n  \n\n\n    Are not FreeCodeCamp\n  \n\n\n    Do not exceed 8 hours\n  \n\n\n    Optionally in these full courses, I also want to see\n  \n\n\n    Examples with explanations\n  \n\n\n    Mini projects\n  \n\n\n    How to package and publish apps made with Tkinter"
},
{
    "title": "No title",
    "content": "Imagine having the option to write code once and run on multiple cores or on the cluster as part of the std lib. I know there's a company (currently) behind it - Anyscale, also not sure what the license is but other than that, what's holding the Py community back?"
},
{
    "title": "No title",
    "content": "I am new to python and currently working on simple 3 layer web application -\n  \n\n\n    frontend - ?\n  \n\n\n    backend API to fetch data from DB - python\n  \n\n\n    DB - cloud\n  \n\n\n    This application has main intention to fetch data from DB, display graphs , table format data etc.  also perform some combination analysis of data and show on UI.\n  \n    Which less complex and stable technology I should prefer for frontend ? python flask, Bulma, Mesop by google or any other ? Thank you."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Linting is essential to writing clean and readable code to share with others. A linter, like Ruff, is a tool that analyzes your code and looks for errors, stylistic issues, and suspicious constructs. Linting allows you to address issues and improve your code quality before you commit your code and share it with others.\n  \n    Ruff is a modern linter that’s extremely fast and has a simple interface, making it straightforward to use. It also aims to be a drop-in replacement for many other linting and formatting tools, such as Flake8, isort, and Black. It’s quickly becoming one of the most popular Python linters.\n  Installing Ruff\n    Now that you know why linting your code is important and how Ruff is a powerful tool for the job, it’s time to install it. Thankfully, Ruff works out of the box, so no complicated installation instructions or configurations are needed to start using it.\n  \n    Assuming your project is already set up with a virtual environment, you can install Ruff in the following ways:\n  ```bash\n$ python -m pip install ruff\n```\n\nYou can check that Ruff installed correctly by using the ruff version command:\n\n```bash\n$ ruff version\nruff 0.4.7\n```Linting Your Python Code\n    While linting helps keep your code consistent and error-free, it doesn’t guarantee that your code will be bug-free. Finding the bugs in your code is best handled with a debugger and adequate testing, which won’t be covered in this tutorial. Coming up in the next sections, you’ll learn how to use Ruff to check for errors and speed up your workflow.\n  Checking for Errors```bash\n$ ruff check\none_ring.py:1:8: F401 [*] `os` imported but unused\none_ring.py:10:12: F821 Undefined name `name`\nFound 2 errors.\n[*] 1 fixable with the `--fix` option.\n```\n    Success! Ruff found two errors. Not only does it show the file and line numbers of the errors, but it also gives you error codes and messages. In addition, it lets you know that one of the two errors is fixable. Great!\n  \n    You can tell Ruff to fix errors by applying the --fix flag. Here’s what happens when you follow its suggestion:\n  ```bash\n$ ruff check --fix\none_ring.py:9:12: F821 Undefined name `name`\nFound 2 errors (1 fixed, 1 remaining).\n```\n    You can find the rest of this Free tutorial here"
},
{
    "title": "No title",
    "content": "I am creating a Python program which models 3D shapes so that they can be saved and or interacted with (i.e. rotated). The process currently takes a while to render shapes consisting of multiple materials. The libraries being implemented are currently matplotlib and numpy. What would you advise for improving the rendering process (library choice etc)?"
},
{
    "title": "No title",
    "content": "Managing resources efficiently in Python is crucial to prevent memory leaks and ensure optimal performance. One of the best ways to handle resource allocation and deallocation is through the use of context managers. In my latest blog post, I dive deep into the concept of context managers, their significance, and how to implement them using both the built-in `with` statement and the `contextlib` module.\n  \n    Here's a brief overview of what you'll find in the article:\n  \nUnderstanding Context Managers:\n\n    What They Are: Context managers help manage resources such as file handling and database connections by setting up a temporary runtime context and cleaning up after the operations are completed.\n  \n    The `with` Statement: The primary way to use context managers in Python, ensuring that resources are properly handled even if an exception occurs.\n  \n    The `contextlib` Module: Provides utilities for creating and working with context managers, offering more control over resource management.\n  \nUsing Context Managers:\n\n    Built-in Context Managers: How to use the `with` statement with built-in context managers like file handling.\n  \nfile_path = \"Context.txt\"\n\nwith open(file_path, 'r') as file:\n\nfile_content = file.read()\n\nprint(\"The Content of file is:\")\n\nprint(file_content)\n\nCustom Context Managers: Implementing custom context managers using the `__enter__()` and `__exit__()` methods.\n  \nclass File:\n\ndef __init__(self, filename, mode):\n\nself.filename = filename\n\nself.mode = mode\n\ndef __enter__(self):\n\nprint(f\"Opening {self.filename} .......\")\n\nself.file = open(self.filename, self.mode)\n\nreturn self.file\n\ndef __exit__(self, exc_type, exc_value, traceback):\n\nprint(f\"Closing {self.filename} ......\")\n\nself.file.close()\n\nwith File('Context.txt', 'r') as file:\n\ncontent = file.read()\n\nprint(content)\n\nAdvanced Techniques:\n\n    The `contextlib` Decorator: Creating context managers using the `contextmanager` decorator.\n  \nfrom contextlib import contextmanager\n\ncontextmanager\n\ndef file(filename, mode):\n\nprint(\"This is the implicit ENTER block\")\n\nmy_file = open(filename, mode)\n\nyield my_file\n\nprint(\"This is the implicit EXIT block\")\n\nmy_file.close()\n\nwith file(\"Context.txt\", 'r') as file_content:\n\ncontent = file_content.read()\n\nprint(content)\n\nHandling Exceptions: Ensuring proper resource management and cleanup even in the presence of exceptions.\n  \n    I hope this guide helps you understand the importance of context managers and how to use them effectively in your Python projects. You can read the full article here for more detailed examples and explanations. Happy coding!"
},
{
    "title": "No title",
    "content": "Pieshell is a Python shell environment that combines the expressiveness of shell pipelines with the power of python iterators.\n  \n    It can be used in two major ways:\n  \n\n\n    As an interactive shell replacing e.g. bash\n  \n\n\n    As an ordinary python module replacing e.g. subprocess.Popen\n  \n\n\n    Obligatory example:\n  140:/home/oven/pieshell >>> for x in ls(-a) | tr(\"s\", \"S\"):\n...   if x.endswith('.py'):\n...      print x\n... \nSetup.py\n    Source code: https://github.com/redhog/pieshell\nWhat the project does\n    It's a replacement for the subprocess module, and for bash as an interactive shell, and makes interacting with shell pipelines easier.\n  Target Audience\n    System administrators, system software developers, data scientists\n  Comparison\n    While os.system is very limited but easy to use, subprocess.Popen offers a lot of flexibility, but the interface is very low level. Any actual pipelining of multiple programs is pretty much required to be done by e.g. a bash process, constructing the pipeline as a shell script string. Further, interacting with standard in and standard out requires careful IO handling.\n  \n    Pieshell on the other hand lets you construct pipelines as python objects. Standard io from a pipeline can be handled using iterators or async iterators. Pieshell has full asyncio integration."
},
{
    "title": "No title",
    "content": "I was wondering recently about any startup and any coding language that how does they make money. So I was curious to know about Python which is widely used"
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    AutoReVanced is a Python script that automates downloading and patching APKs using ReVanced patches from ApkPure. It's perfect for anyone wanting to patch their revanced app.\n  \nTarget Audience\n\n    Suitable for a fun side project or hobbyists, AutoReVanced is designed for anyone wanting to customize Android apps with ReVanced patches.\n  \nComparison\n\n    Unlike alternatives, AutoReVanced is automatic.\n  \n    GitHub: autorevanced"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hey all, my project abstract_factories is up to gauge interest and primarily feedback.\n  \n    The design goal is to make it easier to iterate on typical Content Creation pipeline tools (tool dev, rigging, validation, asset management etc) with a flexible framework to provide convenience, open and simple design and no dependencies (currently). It's an approach I've used a lot over the years and found it pretty versatile in production across numerous projects.\n  Key features\n\n\n    Auto-registration of matching items (types or instances) from any given path or python module.\n  \n\n\n    Simple or conditional item identifiers.\n  \n\n\n    Versioning.\n  \n\n\n    Recursive path searching (recursive module search in review).\n  \n\n\n    Dynamic resolving and importing modules in packaged (supports relative importing).\n  \n\nUsage Examples\n    There are a couple of simple examples given along with tests to cover all of the current features.\n  What the project does\n    It's a convenience package for creating scalable tools and frameworks using Abstract Factory design pattern.\n  Target Audience\n    Due to the solutions it's built for, it's aimed primarily at Technical Artists, Technical Animators, Pipeline and Tool Developers, but I'm interested in hearing about other possible applications.\n  Comparison\n    Compared to other Factory and Abstract Factory convenience packages, mine is based on the work from this GDC talk. The direct abstract-factories currently comes with a few more conveniences I've found useful during production. The idea stems from boiling down Pyblish to something that became a little more reusable when writing frameworks as opposed to being the framework.\n  \n    Suggestions, questions, comments etc welcome."
},
{
    "title": "No title",
    "content": "What My Project Does\n    Upload any PDF and have it converted into a podcast episode with two or more speakers discussing its contents.\n  \nhttps://github.com/knowsuchagency/pdf-to-podcast\nTarget Audience\n    Anyone, but other developers in-particular. The code is open-source on GitHub and there's a link to the source on https://pdf-to-podcast.com. I want the project to serve as an illustrative example of how to build useful things on top of LLMs with relatively little code.\n  Comparison\n    I just made this for fun. It's possible there are other similar projects"
},
{
    "title": "No title",
    "content": "Google Open sourced Mesop. Mesop is a Python-based UI framework that allows you to rapidly build web apps. Used at Google for rapid internal app development similar to Streamlit.\n  \n    find more here"
},
{
    "title": "No title",
    "content": "What my project does\nDataLine is an AI-driven open source and privacy-first platform for data exploration. Your data is accessed using your device and stored on your device.\n  \n    In simple terms, it's an interface that allows you to \"chat\" with your database/dataset. You can ask it explorative questions, e.g. \"what potential insights can I find in this data\", or specific questions \"who are my top five customers in the past 3 months\", and it will gladly oblige. The backend is written using FastAPI, and the frontend uses Reactjs.\n  \n    For me, it acts as a tool that gives me a 10x speed boost. The fact that it can now generate charts out of the data, live, blows my mind still.\n  Target Audience\n    Anyone who has data, regardless of whether they're technical or non-technical people. Devs, data scientists, marketing, sales, farmers, people working alone, in a startup, or in big enterprises.\n  Comparison\n\n\n    No data leaves your machine. In other words, no data is sent to 3rd parties. Not even to us.\n  \n\n\n    DataLine is free and open source - all other alternatives are paid and closed source. Anyone is free to check out the repo and contribute!\n  \n\n\n    Specializes in data exploration, generates charts and SQL, and allows editing and rerunning queries for flexibility."
},
{
    "title": "No title",
    "content": "Quick backstory:\n  \n    Upper floor of my house is sort of a man-cave until we decorate it, so during this time I have two PCs which I use to play games with a friend when we have extra time to waste. The other day I remembered the game mentioned in the title and we had lots of fun playing it (there's 3 different games in this series). I decided I'd transfer the save file to my main PC so I can play when he's not visiting and I quickly learned it's an extremely annoying process to transfer save files across different PCs. Long story short, you need to find a proper registry key (which isn't always located at same spot for some reason) and you need to locate a system.dat file also located in a folder that isn't always in the same place. This process gets tedious pretty quick, so I decided to use the power of Python to make my life easier.\n  \n    What the project does:\n  \n    It's essentially a CLI save handler for the game mentioned in the title. It has 5 slots where you can backup your current save or load the backup to the computer. It can also fix minor registry issues if needed.\n  \n    Target audience:\n  \n    Given that I'm about 20 years too late... I'd say mostly people with very slow PCs or people who like to inhale nostalgia.\n  \n    I learned a lot about using winreg and msvcrt and getch, so while I will likely get bored of the game in the coming weeks, I'm happy I learned something new in the meantime, plus maybe someone finds it useful!\n  \n    Source code: markomavrinac/yugioh_poc_save_handler: Yu-Gi-Oh! Power of Chaos save handler - A script to manage your save games across multiple computers (github.com)"
},
{
    "title": "No title",
    "content": "Streamlit is an open-source app framework that allows data scientists and analysts to create interactive web applications with ease.\n  \n    Using just a few lines of Python, you can turn data scripts into shareable web apps.\n  \n    And combined with a data visualization library like Plotly, you can create beautiful charts and maps with only a few lines of code.\n  \n    In this article, let me step you through how to use Streamlit to create a multi-page interactive application that visualizes Olympic medal data.\n  \n    The application will have three pages:\n  \n\n\n    an overview of medal counts,\n  \n\n\n    a country-specific analysis, and\n  \n\n\n    a choropleth map displaying global medal distributions.\n  \n\n\n    Let’s get to it!\n  \n    Link to free article HERE\n\n    Github repo HERE"
},
{
    "title": "No title",
    "content": "I have 4 years worth JVM languages (Java, Kotlin) and have a need to learn some Python.  What's a good resource to get up to speed quickly with idiomatic Python?"
},
{
    "title": "No title",
    "content": "Tempus is a calendar with horoscopes, reminders, etc made with PyQt6\n  What my Project does?\n    Tempus is a desktop-based calendar management application built with PyQt6, allowing users to manage their todos, reminders, and special dates efficiently. It offers features like adding, editing, and deleting tasks and reminders, as well as marking dates as special. Tempus ensures users stay organized and never miss important events. Plus, it shows you how many days are remaining until a special day in the dashboard.\n  Target Audience\n    Well, anyone who uses a desktop calendar app I guess?\n  Comparison\n    I did some research and couldn't find good calendar apps made with PyQt6.  If you guys knows any, please mention it below and I'm sorry in advance.\n  GitHub\nhttps://github.com/rohankishore/Tempus"
},
{
    "title": "No title",
    "content": "What my project does\n    It provides a fast pure-python implementation of an ordered, multi-valued dictionary.\n  Target audience\n    Python developers that need this kind of specialized functionality.\n  \n    This can be used in production. It has no dependencies. The code is unit-tested (almost fully, I'm working on it) It requires Python 3.12+\n  ComparisonComparison to dict and OrderedDict\ndict and OederedDict are already ordered, but they only allow one value per key. You could use a defaultdict of lists, but then you have these disadvantages:\n  \n\n\n    you can end up with empty lists within the dict if you aren't careful\n  \n\n\n    you lose the order of individual items within the dict:\n  \n\n\nitems = [(1, '1'), (2, '2'), (2, '22'), (1, '11')]\nnormal_dict = defaultdict(list)\nfor key, value in items:\n    normal_dict [key].append(value)\nom_dict = OrderedMultiDict(items)\nprint(list(normal_dict .items)) # prints [(1, ['1', '11']), (2, ['2', '22'])] \nprint(list(om\\_dict.items))     # prints [(1, '1'), (2, '2'), (2, '22'), (1, '11')]\n\n\n    iterating over all key/value pairs can be cumbersome as you need nested loops\n  \n\nComparison to omdict.\nOederedDict provides a (in my opinion) nicer interface with less surprising behavior or pitfalls. My implementation is also faster. e.g iterating over all items is ~5x faster.\n  More info\n    This started as a toy project, that later became useful to me, so I decided to cleanup the code, add tests, and publish it.\n  from better_orderedmultidict import OrderedMultiDict\nomd: OrderedMultiDict[int, int] = OrderedMultiDict([(1,1), (2,2), (1,11), (2,22)])\n\nfor key in reversed(omd.unique_keys()):\n    print(f\"{key}: {omd.getall(key)}\")\n# prints:\n# 2: [2, 22]\n# 1: [1, 11]\n\nprint(omd.popfirstitem())  # prints: (1, 1)\nprint(omd.poplast(2))  # prints: 22\n\nfor key in reversed(omd.unique_keys()):\n    print(f\"{key}: {omd.getall(key)}\")\n# prints:\n# 2: [2]\n# 1: [11]Installation\n    You can install Better-OrderedMultiDict using pip:\n  pip install better-orderedmultidictContributing\n    If you have any suggestions or improvements for Better-OrderedMultiDict, feel free to submit a pull request or open an issue on the GitHub repository. I appreciate any feedback or contributions!\n  Links\n    Here's the link to the GitHub repository: https://github.com/JoachimCoenen/Better-OrderedMultiDict\n\n    Here's the link to PyPi: https://pypi.org/project/better-orderedmultidict"
},
{
    "title": "No title",
    "content": "As the title says, I cant decide what to use for rest api for mye summer project. I am uni student, so this project will only be very small scale project. I have made simpel rest apis in sll of them, but still cant decide which one to actuslly use for my project. Do anyone have any tips for which might be right one? A thing to consider for me answel is how easy it is to host."
},
{
    "title": "No title",
    "content": "what my project does\n    This project is built to solve the issue of LLM unable to produce relevant answers for information in a particular context. uses the information  to train the model and stored it in a database and uses  this database to get relevant answers from the Model.\n  Target audiance\n    This project is for people who want to train a LLM on a particular piece of information.\n  comparison\n    This model only gives answers for information regarding the data you provided in the file. It will not answer any other questions including formal greetings.\n  \n    GitHub link :https://github.com/dharmateja2810/RAG-Retrieval-Augmented-Generation-Model"
},
{
    "title": "No title",
    "content": "Hi I’m looking for inspiration for some stupid python automation projects. If you have done something funny or stupid using python automation I would love to hear it."
},
{
    "title": "No title",
    "content": "Python is a great programming language, but sometimes the indentation can be terrible for some people (especially people with visual impairments).\n  \n    So i created Lython.\n  \n    What the project does:\n  \n    Lython replacing the Python indentation to lua-style code blocks.\n  \n    this is example lython code\n  def test(num)\n    for i in range(num) do\n        if i == 0 then\n            print(\"zero\")\n        elif i % 2 == 1 then\n            print(\"odd\")\n        else\n            print(\"even\")\n        end # if else\n    end # for\nend # def\n\ntest(10)\n    for more info, please visit lython repo.\n  \n    Target audience:\n  \n    Peoples with visual impairments (especially) and Programmers who want to write python code with new experience (generally)\n  \n    Repo & Source code:\n  \nguangrei/lython"
},
{
    "title": "No title",
    "content": "well, kind of.\n  \n    I made Pilgram, an infinite idle RPG where your character goes on adventures and notifies you when stuff happens.\n  What my project does\n    The bot provides a text interface with wich you can \"play\" an MMO RPG, it's basically an online idle adventure game\n  Target audience\n    It's a toy project that i made out of boredom, also it sounded cool\n  Comparison\n    I never heard of anything like this except for some really old browser games. Maybe i'm just not informed.\n  More info\n    How is it infinite? The secret is AI. Every quest and event in the game is generated by AI depending on the demand of the players, so in theory you can go on an infinite amount of quests.\n  \n    Why did i call it an MMO? Because you can kind of play with your friends by creating & joining guilds and by sending gifts to eachother. There even is a guild leaderboard to see who gets the most points :)\n  \n    The interface is exclusively text based, but the command interpreter i wrote is pretty easy to integrate in other places, even in GUIs if anyone wants to try.\n  \n    I tried out a lot of new things for this project, like using ORMs, writing unit tests (don't look at those, i kinda got bored after a short while), using AI & writing generic enough code that it can be swapped with any other implementation. I think most of the code i wrote is pretty ok, but you can tell me what to change & what to improve if you want.\n  Links\n    here's the link to the code: https://github.com/SudoOmbro/pilgram\n\n    if you wanna try out the version i'm running on my server start a conversation with pilgram_bot on Telegram, don't expect a balanced experience at first since that was kind of the last of my problems lol"
},
{
    "title": "No title",
    "content": "Hey guys!\n  \n    I'm excited to introduce Temporal Adjusters, a new Python package designed to make time series adjustments easier and more efficient. If you work with time series data, you'll find this tool incredibly useful for various temporal adjustments.\n  What my project does\n    Adjusters are a key tool for modifying temporal objects. They exist to externalize the process of adjustment, permitting different approaches, as per the strategy design pattern. Temporal Adjuster provides tools that help pinpoint very specific moments in time, without having to manually count days, weeks, or months. In essence, a Temporal Adjuster is a function that encapsulates a specific date/time manipulation rule. It operates on a temporal object (representing a date, time, or datetime) to produce a new temporal object adjusted according to the rule. Examples might be an adjuster that sets the date avoiding weekends, or one that sets the date to the last day of the month.\n  Installation\n    You can install Temporal Adjuster using pip:\n  pip install temporal-adjusterUsage\n    This package provides a set of predefined temporal adjusters that can be used to adjust a temporal object in various ways. For example:\n  >>> from datetime import date, datetime\n\n>>> from temporal_adjuster import TemporalAdjuster\n>>> from temporal_adjuster.common.enums import Weekday\n\n>>> TemporalAdjuster.first_day_of_next_week(date(2021, 1, 1))\ndatetime.date(2021, 1, 4)\n\n>>> TemporalAdjuster.last_day_of_last_month(datetime(2021, 1, 1))\ndatetime.datetime(2020, 12, 31)\n\n>>> TemporalAdjuster.first_of_year(Weekday.SATURDAY, date(2021, 1, 1))\ndatetime.date(2021, 1, 2)\n\n>>> TemporalAdjuster.nth_of_month(Weekday.SUNDAY, datetime(2021, 5, 1), 2)\ndatetime.datetime(2021, 5, 9)\n\n>>> TemporalAdjuster.next(Weekday.MONDAY, datetime(2021, 2, 11), 2)\ndatetime.datetime(2021, 2, 15)Contributing\n    If you have any suggestions or improvements for pynimbar, feel free to submit a pull request or open an issue on the GitHub repository as per the CONTRIBUTING document. We appreciate any feedback or contributions!\n  Target audience\n    This can be used in production. It has only one depedency, dateutils, which if you're manipulating temporal objects you probably already have. All the code is 100% unit-tested, as well as build tested for all supported Python versions.\n  Comparison\n    This is based on Java's native TemporalAdjuster interfaces, but I found no similar library/functionality for Python."
},
{
    "title": "No title",
    "content": "Like the title said. I created an API fro apkpure.com . I was creating a script to automate YouTube Revanced, but i couldn't find anyway to download the apk. You can try out the app here: https://github.com/anishomsy/apkpure\n\nWhat My Project Does\n\n    It allows you to download apk from apkpure. Users can easily fetch specific versions of Android apps programmatically.\n  \nTarget Audience\n\n    it is a hobby project, anyone can use it\n  \nComparison\n\n    I did not find any existing alternatives. So I created my own. The only other way was to download it manually which is very tedious.\n  \n    Please lmk how i can improve.\n  \n    Thank you"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What My Project Does\n    Cloudflare offers a free SQLite based database D1. I needed it for some personal project so I thought of creating a very simple wrapper for it. D1py let's you connect to D1 database in your cloudflare account and run SQL queries(CRUD operations).\n  Target audience\n    For those who need a simple wrapper for Cloudflare D1 API for their projects.\n  Comparison\n    Right now there are no Python wrappers or libraries for D1 yet.... that's why I thought of creating one. It's not perfect but it is my first attempt at writing a small library/package for doing a task.\n  Source\n    Repository: https://github.com/Suleman-Elahi/D1py\n\n    Feel free to drop any suggestions. Thanks."
},
{
    "title": "No title",
    "content": "https://github.com/perpetual-ml/perpetual\nWhat My Project Does\n    PerpetualBooster is a gradient boosting machine (GBM) algorithm which doesn't have hyperparameters to be tuned so that you can use it without needing hyperparameter optimization packages unlike other GBM algorithms. Similar to AutoML libraries, it has a budget parameter which ranges between (0, 1). Increasing the budget parameter increases predictive power of the algorithm and gives better results on unseen data. Start with a small budget and increase it once you are confident with your features. If you don't see any improvement with further increasing budget, it means that you are already extracting the most predictive power out of your data.\n  Target Audience\n    The project is meant for production. You can replace hyperparameter packages plus other gradient boosting algorithms with PerpetualBooster.\n  Comparison\n    Other gradient boosting algorithms (XGBoost, LightGBM, Catboost) and most of the machine learning algorithms need hyperparameter optimization for the best performance on unseen data. But PerpetualBooster doesn't have hyperparameters so it doesn't need hyperparameter tuning. It has a built-in generalization algorithm and provides the best performance.\n  \n    The following table summarizes the results for the California Housing dataset:\n  \n\n\n\n            Perpetual budget\n          \n            LightGBM n_estimators\n          \n            Perpetual mse\n          \n            LightGBM mse\n          \n            Perpetual cpu time\n          \n            LightGBM cpu time\n          \n            Speed-up\n          \n\n\n\n\n\n\n              0.33\n            \n              100\n            \n              0.192\n            \n              0.192\n            \n              10.1\n            \n              990\n            \n              98x\n            \n\n\n              0.35\n            \n              200\n            \n              0.190\n            \n              0.191\n            \n              11.0\n            \n              2030\n            \n              186x\n            \n\n\n              0.45\n            \n              300\n            \n              0.187\n            \n              0.188\n            \n              18.7\n            \n              3272\n            \n              179x"
},
{
    "title": "No title",
    "content": "Just released v0.8.0 of fastapi_problem to provide problem details for FastAPI applications. Hoping it can provide value to some other peoples projects.\n  \n    Code: https://github.com/NRWLDev/fastapi-problem\n\n    Docs: https://nrwldev.github.io/fastapi-problem/\n\n    Pypi: https://pypi.org/project/fastapi-problem/\nWhat My Project Does\n    Provides a simple exception handler and an underlying exception class heirarchy to remove the need to think about error management in your FastAPI project, just raise errors as appropriate and let the handler deal with responses.\n  Target Audience\n    Web developers\n  Comparison\n    There was a previous project that supported RFC7807 but that is no longer maintained, and is also made obsolete by RFC9457.\n  RFC9457\n    For anyone who does not make use of FastAPI, the underlying exception library has also been released, and can be used to implement handlers for any web framework you might be into.\n  \nhttps://github.com/NRWLDev/rfc9457\n\nhttps://pypi.org/project/rfc9457/"
},
{
    "title": "No title",
    "content": "When I was a beginner (or maybe still I am) I struggled a lot with pythons import function.\n  \n    Over the years I went over different approaches, how to handle imports and ended up using mostly exclusive poetry.\n  \n    I've met a lot of people struggling the same way, bit always could just explain very shortly my experience.\n  \n    I've now decided to write it down as a scenario, where I can show and explain my pitfalls:\n  \nhttps://github.com/MaKaNu/pyimport-explained"
},
{
    "title": "No title",
    "content": "What My Project Does\n    It is a CLI to get songs from ultimateguitar.\n  \n    How it looks like: https://youtu.be/Spm1IIaYo8Q\n\n    I've only tried it on linux.\n  \n    Available in debian and pypi.\n  Target audience\n    For musicians who also use the terminal and who don't especially like the ultimateguitar website.\n  Comparison\n    I'm not aware of other projects doing the same thing.\n  \n    Compared to the website, it can transpose and it is much faster.\n  Source\n    Project website: https://codeberg.org/ltworf/ultimateultimateguitar\n\n    Out of date website (just here to avoid the post to be auto-removed): https://github.com/ltworf/ultimateultimateguitar"
},
{
    "title": "No title",
    "content": "is there a proposal for a shorter exception handling syntax for those very frequent cases where a library function doesn't return \"error value\" like str.index ?\n  \n    something like instead of :\n  try:\n    i = my_str.index(\"sub\", st, en)\nexcept ValueError: # if \"sub\" has not been found\n    pass\nelse:\n    # do stuff with i (note that i usually want independent error handling here)\n    something like this :\n  i = my_str.index(\"sub\", st, en) except ValueError -1 # or maybe even return here\nif i == -1: # also can return right away if i want to avoid an indent next\n    # do stuff with i ...\n    i suspect there might be something \"un-pythonic\" here in what i am imagining , but please forgive me if that's the case . i am a fan of Python for many years , but haven't really invested any time in learning the philosophy\n  \n    so i am interested in what the community thinks about this , how ok would such syntax be from the point of the \"Python way\" , and if there is such a proposal i would like to know if i can consider maybe voting on it somehow"
},
{
    "title": "No title",
    "content": "uv is the \"pip but blazingly fast™️ because it's written in rust\" and is developed by the same folks that did ruff. In 0.2.11 they released an experimental/preview command of `uv add/remove` that adds a library to pyproject.toml. It's the first step to become a fully-fledged package manager!\n  \n    I noticed you can also manage python installations with uv using `uv toolchain` command (i.e. be like pyenv) and run tools (like a smaller version of pipx) with `uv run`.\n  \n    I'm genuinely excited about this, Python packaging is going to become such a smooth experience 😎\n  \n    Commands are in preview so expect missing stuff.\n  \n    (I bear no affiliation with astral)\n  \nhttps://github.com/astral-sh/uv"
},
{
    "title": "No title",
    "content": "Hi Python data folks,\n  \n    I am excited to share Pathway, a Python data processing framework we built for ETL and RAG pipelines.\n  \nhttps://github.com/pathwaycom/pathway\n\nWhat My Project Does\n\n    We started Pathway to solve event processing for IoT and geospatial indexing. Think freight train operations in unmapped depots bringing key merchandise from China to Europe. This was not something we could use Flink or Elastic for.\n  \n    Then we added more connectors for streaming ETL (Kafka, Postgres CDC…), data indexing (yay vectors!), and LLM wrappers for RAG. Today Pathway provides a data indexing layer for live data updates, stateless and stateful data transformations over streams, and retrieval of structured and unstructured data.\n  \n    Pathway ships with a Python API and a Rust runtime based on Differential Dataflow to perform incremental computation. All the pipeline is kept in memory and can be easily deployed with Docker and Kubernetes (pipelines-as-code).\n  \n    We built Pathway to support enterprises like F1 teams and processors of highly sensitive information to build mission-critical data pipelines. We do this by putting security and performance first. For example, you can build and deploy self-hosted RAG pipelines with local LLM models and Pathway’s in-memory vector index, so no data ever leaves your infrastructure. Pathway connectors and transformations work with live data by default, so you can avoid expensive reprocessing and rely on fresh data.\n  \n    You can install Pathway with pip and Docker, and get started with templates and notebooks:\n  \nhttps://pathway.com/developers/showcases\n\n    We also host demo RAG pipelines implemented 100% in Pathway, feel free to interact with their API endpoints:\n  \nhttps://pathway.com/solutions/rag-pipelines#try-it-out\n\n    We'd love to hear what you think of Pathway!"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Textual Serve (https://github.com/Textualize/textual-serve) is a project which serves TUIs (built with Textual) in the browser.\n  \n    This is self-hosted, so you don't need to rely on any external service."
},
{
    "title": "No title",
    "content": "Brandt Bucher talks on JIT compiler for Python at CPython Core Developer Sprint. Brandt is  a member of the Faster CPython project, which is working on making the reference implementation of the language faster via a variety of techniques.\n  \nhttps://www.youtube.com/watch?v=HxSHIpEQRjs"
},
{
    "title": "No title",
    "content": "CherrySaaS is a 100% open-source SaaS template that lets you build crazy beautiful SaaS using Chakra and Radix UIs powered by Reflex. It is currently a work in progress and requires community contribution because I don't have the energy to write the entire template. If I were to do an entire template, I would never open-source it as it would become a very large project. The reason is that it is a rare project as no *free SaaS template has ever been made in Pure-Python. There are proprietary 2 SaaS templates made using Reflex. So I decided to make a free one for the community too. Here's the planning on the notion I did not so long ago.If you want to be a collaborator, reach to me out on Discord (adarshgourabmahalik) or post your GitHub email in this discussion. I have looked at a SaaS template called shipfast.com (which is getting popular nowadays). Ah, yes this also became an inspiration but I wanted to make one free.Selling Shovels During The Gold Rush ❌Giving away shovels for free during the Gold Rush ✔Here's a discord link to preview the landing page of the template.https://discord.com/channels/1029853095527727165/1063735841333198938/1252953503106732053\n\n    [REPOSITORY]"
},
{
    "title": "No title",
    "content": "What my project does\n    Performs OCR on scanned Books using Microsoft Azure Document Intelligence read\n  Target audience\n    People who are unsatisfied with traditional OCR People who want to add clear text to the original PDF and not just extract the text. People who want to archive documents at best quality.\n  Comparasion\n    In my use case traditional OCR was near to useless. Tesseract was meh, Google API didn't process large files. Document Intelligence takes up to 500MB (although in practice a little less), and is possible to OCR 400-600 pages over books in batch by dividing and merging the source and results locally by only a few chunks. It doesn't provide the text in PDF form so that was my reason to start this project.\n  \n    Still in alpha and in separate modules and a lot of rigid coding, but it is working fine for my original task so thought maybe I'd showcase it.\n  \nhttps://github.com/DesertDoggy/json3pdf"
},
{
    "title": "No title",
    "content": "Source code: https://github.com/ben-n93/beachwatch\n\nWhat my Project DoesIn NSW, Australia the government provides an API with daily updates on beaches' pollution and water quality forecast.\n  \n    I thought I'd make a simple wrapper in Python to make it easier to get data from the API.\n  \nTarget AudienceMost likely Australian data scientists/analysts/developers interested in beach data."
},
{
    "title": "No title",
    "content": "https://www.gauge.sh/blog/parsing-python-asts-20x-faster-with-rust"
},
{
    "title": "No title",
    "content": "I should have saved the post but maybe 4-6 months ago I was reading a post (I am pretty sure it was in r/Python) where someone created a package that creates a visual for data contained within a list. For example, let’s say I have a data frame where one of the columns is named “colors” and each record contains a list of colors. One record might be [black,blue,yellow] another record might have [blue,yellow,black]. The visual had two parts where the top was a column chart to show the frequency of the list combinations and below the column chart was more of a table that showed each “color” as one column and then across the row for each color and under the columns from the chart above was an indicator of sorts that would be greyed out of the color for that row was not in the corresponding columns list and highlighted another color of it was. Anyways this is probably a long shot but either the package or the name of this visual would be super helpful. Thanks python community!"
},
{
    "title": "No title",
    "content": "I will buy a laptop for coding purposes but just started learning and practising Python using Pyecharm. What are the software requirements that lead to hardware specs a general Python coder must look into?\n  \n    Please suggest the hardware setup within a pocket friendly budget."
},
{
    "title": "No title",
    "content": "End goal is to produce PDF using external data and a template. Needs to support Jinja tags, conditionals and loops.\n  \n    Using https://github.com/Kozea/WeasyPrint and https://github.com/pallets/jinja as base stack (Open to other suggestions)\n  \n    I was thinking of building some base HTML templates but would be awesome if I could find a visual HTML editor that could produce code 100% compatible with Weasyprint so that end users can build templates by themselves or modify existing ones.\n  \n    Could be Wysiwyg based using https://editorjs.io or https://github.com/slab/quillor more advanced web builders like https://github.com/GrapesJS/grapesjs\n\n    Anybody built something similar?"
},
{
    "title": "No title",
    "content": "I'm going to show you how to get Scrapegraph AI up and running, how to set up a language model, how to process JSON, scrape websites, use different AI models, and even turning your data into audio. Sounds like a lot, but it's easier than you think, and I'll walk you through it step by step.\n  \nhttps://www.scrapingbee.com/blog/scrapegraph-ai-tutorial-scrape-websites-easily-with-llama-ai/"
},
{
    "title": "No title",
    "content": "What's rug library:\n\n    Library for fetching various stock data from the internet (official and unofficial APIs).\n  \nSource code:\n\nhttps://gitlab.com/imn1/rug\n\nReleases including changelog:\n\nhttps://gitlab.com/imn1/rug/-/releases"
},
{
    "title": "No title",
    "content": "NumPy 2.0.0 is the first major release since 2006.\n  \n\n\nhttps://github.com/numpy/numpy/releases/tag/v2.0.0\n\n\n\nhttps://numpy.org/devdocs/release/2.0.0-notes.html\n\n\n\nhttps://numpy.org/devdocs/numpy_2_0_migration_guide.html"
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    While looking for task queues, I found that there are many options available in the Python ecosystem, making it really hard to choose the right one. To get a sense of how each library performs and to help make an informed decision, I conducted a load test on some of the most popular ones: Python-RQ, ARQ, Celery, Huey, and Dramatiq.\n  \nTarget Audience\n\n    I hope my findings can help those who are also looking for a task queue solution in Python.\n  \nComparison\n\n    Most articles out there seem to focus on comparing the features of these libraries but rarely discuss performance. While there could be a lot of improvements on my tests, I think it still provide some different insights into how each library handles heavy loads and concurrency.\n  \nLinks:\n\n    You can read  my findings on my blog\n\n    Check out the source code: on Github\n\n    Thanks"
},
{
    "title": "No title",
    "content": "Hi, the different ways to delete duplicate elements from a list in python is discussed in this tutorial video. Also, techniques to remove duplicates from list of lists and list of dictionaries in python are explained\n  \nhttps://youtu.be/CN-xT7iqTl0?si=t_1BBKKg2zF047_D"
},
{
    "title": "No title",
    "content": "What My Project Does\n\nAurora is a fast, extensible Python static site generator. With Aurora, I can generate my personal website (~1,700 files, with multiple layers of jinja2 templates for each page) in < 4 seconds. Aurora generated 292,884 pages from a Hacker News post dataset in 2m:20s.\n  \n    Aurora supports incremental static regeneration, where pages can be regenerated in under 400ms, with hot reloading. I documented how this works on my blog.\n  \nTarget Audience\n\n    I'm building Aurora to help me run my website, but it is built to be general so you can use it for your own projects. I would love feedback!\n  \n    I want this to be a tool for running static sites in production, at scale.\n  \nComparison\n\n    Aurora is inspired by the folder structure of Jekyll, but is written in Python. It has a hooks API that lets you define custom Python functions that manipulate the state of a page. This allows you to implement custom behaviours in isolation of the engine itself. I use this to open link previews from a cache that I plan to use on my website, among other things."
},
{
    "title": "No title",
    "content": "Does anyone know of a full course to learn the Python Tkinter library? I want courses that\n  \n\n\n    Are in video format\n  \n\n\n    Are not FreeCodeCamp\n  \n\n\n    Do not exceed 8 hours\n  \n\n\n    Optionally in these full courses, I also want to see\n  \n\n\n    Examples with explanations\n  \n\n\n    Mini projects\n  \n\n\n    How to package and publish apps made with Tkinter"
},
{
    "title": "No title",
    "content": "Imagine having the option to write code once and run on multiple cores or on the cluster as part of the std lib. I know there's a company (currently) behind it - Anyscale, also not sure what the license is but other than that, what's holding the Py community back?"
},
{
    "title": "No title",
    "content": "I am new to python and currently working on simple 3 layer web application -\n  \n\n\n    frontend - ?\n  \n\n\n    backend API to fetch data from DB - python\n  \n\n\n    DB - cloud\n  \n\n\n    This application has main intention to fetch data from DB, display graphs , table format data etc.  also perform some combination analysis of data and show on UI.\n  \n    Which less complex and stable technology I should prefer for frontend ? python flask, Bulma, Mesop by google or any other ? Thank you."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Linting is essential to writing clean and readable code to share with others. A linter, like Ruff, is a tool that analyzes your code and looks for errors, stylistic issues, and suspicious constructs. Linting allows you to address issues and improve your code quality before you commit your code and share it with others.\n  \n    Ruff is a modern linter that’s extremely fast and has a simple interface, making it straightforward to use. It also aims to be a drop-in replacement for many other linting and formatting tools, such as Flake8, isort, and Black. It’s quickly becoming one of the most popular Python linters.\n  Installing Ruff\n    Now that you know why linting your code is important and how Ruff is a powerful tool for the job, it’s time to install it. Thankfully, Ruff works out of the box, so no complicated installation instructions or configurations are needed to start using it.\n  \n    Assuming your project is already set up with a virtual environment, you can install Ruff in the following ways:\n  ```bash\n$ python -m pip install ruff\n```\n\nYou can check that Ruff installed correctly by using the ruff version command:\n\n```bash\n$ ruff version\nruff 0.4.7\n```Linting Your Python Code\n    While linting helps keep your code consistent and error-free, it doesn’t guarantee that your code will be bug-free. Finding the bugs in your code is best handled with a debugger and adequate testing, which won’t be covered in this tutorial. Coming up in the next sections, you’ll learn how to use Ruff to check for errors and speed up your workflow.\n  Checking for Errors```bash\n$ ruff check\none_ring.py:1:8: F401 [*] `os` imported but unused\none_ring.py:10:12: F821 Undefined name `name`\nFound 2 errors.\n[*] 1 fixable with the `--fix` option.\n```\n    Success! Ruff found two errors. Not only does it show the file and line numbers of the errors, but it also gives you error codes and messages. In addition, it lets you know that one of the two errors is fixable. Great!\n  \n    You can tell Ruff to fix errors by applying the --fix flag. Here’s what happens when you follow its suggestion:\n  ```bash\n$ ruff check --fix\none_ring.py:9:12: F821 Undefined name `name`\nFound 2 errors (1 fixed, 1 remaining).\n```\n    You can find the rest of this Free tutorial here"
},
{
    "title": "No title",
    "content": "I am creating a Python program which models 3D shapes so that they can be saved and or interacted with (i.e. rotated). The process currently takes a while to render shapes consisting of multiple materials. The libraries being implemented are currently matplotlib and numpy. What would you advise for improving the rendering process (library choice etc)?"
},
{
    "title": "No title",
    "content": "Managing resources efficiently in Python is crucial to prevent memory leaks and ensure optimal performance. One of the best ways to handle resource allocation and deallocation is through the use of context managers. In my latest blog post, I dive deep into the concept of context managers, their significance, and how to implement them using both the built-in `with` statement and the `contextlib` module.\n  \n    Here's a brief overview of what you'll find in the article:\n  \nUnderstanding Context Managers:\n\n    What They Are: Context managers help manage resources such as file handling and database connections by setting up a temporary runtime context and cleaning up after the operations are completed.\n  \n    The `with` Statement: The primary way to use context managers in Python, ensuring that resources are properly handled even if an exception occurs.\n  \n    The `contextlib` Module: Provides utilities for creating and working with context managers, offering more control over resource management.\n  \nUsing Context Managers:\n\n    Built-in Context Managers: How to use the `with` statement with built-in context managers like file handling.\n  \nfile_path = \"Context.txt\"\n\nwith open(file_path, 'r') as file:\n\nfile_content = file.read()\n\nprint(\"The Content of file is:\")\n\nprint(file_content)\n\nCustom Context Managers: Implementing custom context managers using the `__enter__()` and `__exit__()` methods.\n  \nclass File:\n\ndef __init__(self, filename, mode):\n\nself.filename = filename\n\nself.mode = mode\n\ndef __enter__(self):\n\nprint(f\"Opening {self.filename} .......\")\n\nself.file = open(self.filename, self.mode)\n\nreturn self.file\n\ndef __exit__(self, exc_type, exc_value, traceback):\n\nprint(f\"Closing {self.filename} ......\")\n\nself.file.close()\n\nwith File('Context.txt', 'r') as file:\n\ncontent = file.read()\n\nprint(content)\n\nAdvanced Techniques:\n\n    The `contextlib` Decorator: Creating context managers using the `contextmanager` decorator.\n  \nfrom contextlib import contextmanager\n\ncontextmanager\n\ndef file(filename, mode):\n\nprint(\"This is the implicit ENTER block\")\n\nmy_file = open(filename, mode)\n\nyield my_file\n\nprint(\"This is the implicit EXIT block\")\n\nmy_file.close()\n\nwith file(\"Context.txt\", 'r') as file_content:\n\ncontent = file_content.read()\n\nprint(content)\n\nHandling Exceptions: Ensuring proper resource management and cleanup even in the presence of exceptions.\n  \n    I hope this guide helps you understand the importance of context managers and how to use them effectively in your Python projects. You can read the full article here for more detailed examples and explanations. Happy coding!"
},
{
    "title": "No title",
    "content": "Pieshell is a Python shell environment that combines the expressiveness of shell pipelines with the power of python iterators.\n  \n    It can be used in two major ways:\n  \n\n\n    As an interactive shell replacing e.g. bash\n  \n\n\n    As an ordinary python module replacing e.g. subprocess.Popen\n  \n\n\n    Obligatory example:\n  140:/home/oven/pieshell >>> for x in ls(-a) | tr(\"s\", \"S\"):\n...   if x.endswith('.py'):\n...      print x\n... \nSetup.py\n    Source code: https://github.com/redhog/pieshell\nWhat the project does\n    It's a replacement for the subprocess module, and for bash as an interactive shell, and makes interacting with shell pipelines easier.\n  Target Audience\n    System administrators, system software developers, data scientists\n  Comparison\n    While os.system is very limited but easy to use, subprocess.Popen offers a lot of flexibility, but the interface is very low level. Any actual pipelining of multiple programs is pretty much required to be done by e.g. a bash process, constructing the pipeline as a shell script string. Further, interacting with standard in and standard out requires careful IO handling.\n  \n    Pieshell on the other hand lets you construct pipelines as python objects. Standard io from a pipeline can be handled using iterators or async iterators. Pieshell has full asyncio integration."
},
{
    "title": "No title",
    "content": "I was wondering recently about any startup and any coding language that how does they make money. So I was curious to know about Python which is widely used"
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    AutoReVanced is a Python script that automates downloading and patching APKs using ReVanced patches from ApkPure. It's perfect for anyone wanting to patch their revanced app.\n  \nTarget Audience\n\n    Suitable for a fun side project or hobbyists, AutoReVanced is designed for anyone wanting to customize Android apps with ReVanced patches.\n  \nComparison\n\n    Unlike alternatives, AutoReVanced is automatic.\n  \n    GitHub: autorevanced"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hey all, my project abstract_factories is up to gauge interest and primarily feedback.\n  \n    The design goal is to make it easier to iterate on typical Content Creation pipeline tools (tool dev, rigging, validation, asset management etc) with a flexible framework to provide convenience, open and simple design and no dependencies (currently). It's an approach I've used a lot over the years and found it pretty versatile in production across numerous projects.\n  Key features\n\n\n    Auto-registration of matching items (types or instances) from any given path or python module.\n  \n\n\n    Simple or conditional item identifiers.\n  \n\n\n    Versioning.\n  \n\n\n    Recursive path searching (recursive module search in review).\n  \n\n\n    Dynamic resolving and importing modules in packaged (supports relative importing).\n  \n\nUsage Examples\n    There are a couple of simple examples given along with tests to cover all of the current features.\n  What the project does\n    It's a convenience package for creating scalable tools and frameworks using Abstract Factory design pattern.\n  Target Audience\n    Due to the solutions it's built for, it's aimed primarily at Technical Artists, Technical Animators, Pipeline and Tool Developers, but I'm interested in hearing about other possible applications.\n  Comparison\n    Compared to other Factory and Abstract Factory convenience packages, mine is based on the work from this GDC talk. The direct abstract-factories currently comes with a few more conveniences I've found useful during production. The idea stems from boiling down Pyblish to something that became a little more reusable when writing frameworks as opposed to being the framework.\n  \n    Suggestions, questions, comments etc welcome."
},
{
    "title": "No title",
    "content": "What My Project Does\n    Upload any PDF and have it converted into a podcast episode with two or more speakers discussing its contents.\n  \nhttps://github.com/knowsuchagency/pdf-to-podcast\nTarget Audience\n    Anyone, but other developers in-particular. The code is open-source on GitHub and there's a link to the source on https://pdf-to-podcast.com. I want the project to serve as an illustrative example of how to build useful things on top of LLMs with relatively little code.\n  Comparison\n    I just made this for fun. It's possible there are other similar projects"
},
{
    "title": "No title",
    "content": "Google Open sourced Mesop. Mesop is a Python-based UI framework that allows you to rapidly build web apps. Used at Google for rapid internal app development similar to Streamlit.\n  \n    find more here"
},
{
    "title": "No title",
    "content": "What my project does\nDataLine is an AI-driven open source and privacy-first platform for data exploration. Your data is accessed using your device and stored on your device.\n  \n    In simple terms, it's an interface that allows you to \"chat\" with your database/dataset. You can ask it explorative questions, e.g. \"what potential insights can I find in this data\", or specific questions \"who are my top five customers in the past 3 months\", and it will gladly oblige. The backend is written using FastAPI, and the frontend uses Reactjs.\n  \n    For me, it acts as a tool that gives me a 10x speed boost. The fact that it can now generate charts out of the data, live, blows my mind still.\n  Target Audience\n    Anyone who has data, regardless of whether they're technical or non-technical people. Devs, data scientists, marketing, sales, farmers, people working alone, in a startup, or in big enterprises.\n  Comparison\n\n\n    No data leaves your machine. In other words, no data is sent to 3rd parties. Not even to us.\n  \n\n\n    DataLine is free and open source - all other alternatives are paid and closed source. Anyone is free to check out the repo and contribute!\n  \n\n\n    Specializes in data exploration, generates charts and SQL, and allows editing and rerunning queries for flexibility."
},
{
    "title": "No title",
    "content": "Quick backstory:\n  \n    Upper floor of my house is sort of a man-cave until we decorate it, so during this time I have two PCs which I use to play games with a friend when we have extra time to waste. The other day I remembered the game mentioned in the title and we had lots of fun playing it (there's 3 different games in this series). I decided I'd transfer the save file to my main PC so I can play when he's not visiting and I quickly learned it's an extremely annoying process to transfer save files across different PCs. Long story short, you need to find a proper registry key (which isn't always located at same spot for some reason) and you need to locate a system.dat file also located in a folder that isn't always in the same place. This process gets tedious pretty quick, so I decided to use the power of Python to make my life easier.\n  \n    What the project does:\n  \n    It's essentially a CLI save handler for the game mentioned in the title. It has 5 slots where you can backup your current save or load the backup to the computer. It can also fix minor registry issues if needed.\n  \n    Target audience:\n  \n    Given that I'm about 20 years too late... I'd say mostly people with very slow PCs or people who like to inhale nostalgia.\n  \n    I learned a lot about using winreg and msvcrt and getch, so while I will likely get bored of the game in the coming weeks, I'm happy I learned something new in the meantime, plus maybe someone finds it useful!\n  \n    Source code: markomavrinac/yugioh_poc_save_handler: Yu-Gi-Oh! Power of Chaos save handler - A script to manage your save games across multiple computers (github.com)"
},
{
    "title": "No title",
    "content": "Streamlit is an open-source app framework that allows data scientists and analysts to create interactive web applications with ease.\n  \n    Using just a few lines of Python, you can turn data scripts into shareable web apps.\n  \n    And combined with a data visualization library like Plotly, you can create beautiful charts and maps with only a few lines of code.\n  \n    In this article, let me step you through how to use Streamlit to create a multi-page interactive application that visualizes Olympic medal data.\n  \n    The application will have three pages:\n  \n\n\n    an overview of medal counts,\n  \n\n\n    a country-specific analysis, and\n  \n\n\n    a choropleth map displaying global medal distributions.\n  \n\n\n    Let’s get to it!\n  \n    Link to free article HERE\n\n    Github repo HERE"
},
{
    "title": "No title",
    "content": "I have 4 years worth JVM languages (Java, Kotlin) and have a need to learn some Python.  What's a good resource to get up to speed quickly with idiomatic Python?"
},
{
    "title": "No title",
    "content": "Tempus is a calendar with horoscopes, reminders, etc made with PyQt6\n  What my Project does?\n    Tempus is a desktop-based calendar management application built with PyQt6, allowing users to manage their todos, reminders, and special dates efficiently. It offers features like adding, editing, and deleting tasks and reminders, as well as marking dates as special. Tempus ensures users stay organized and never miss important events. Plus, it shows you how many days are remaining until a special day in the dashboard.\n  Target Audience\n    Well, anyone who uses a desktop calendar app I guess?\n  Comparison\n    I did some research and couldn't find good calendar apps made with PyQt6.  If you guys knows any, please mention it below and I'm sorry in advance.\n  GitHub\nhttps://github.com/rohankishore/Tempus"
},
{
    "title": "No title",
    "content": "What my project does\n    It provides a fast pure-python implementation of an ordered, multi-valued dictionary.\n  Target audience\n    Python developers that need this kind of specialized functionality.\n  \n    This can be used in production. It has no dependencies. The code is unit-tested (almost fully, I'm working on it) It requires Python 3.12+\n  ComparisonComparison to dict and OrderedDict\ndict and OederedDict are already ordered, but they only allow one value per key. You could use a defaultdict of lists, but then you have these disadvantages:\n  \n\n\n    you can end up with empty lists within the dict if you aren't careful\n  \n\n\n    you lose the order of individual items within the dict:\n  \n\n\nitems = [(1, '1'), (2, '2'), (2, '22'), (1, '11')]\nnormal_dict = defaultdict(list)\nfor key, value in items:\n    normal_dict [key].append(value)\nom_dict = OrderedMultiDict(items)\nprint(list(normal_dict .items)) # prints [(1, ['1', '11']), (2, ['2', '22'])] \nprint(list(om\\_dict.items))     # prints [(1, '1'), (2, '2'), (2, '22'), (1, '11')]\n\n\n    iterating over all key/value pairs can be cumbersome as you need nested loops\n  \n\nComparison to omdict.\nOederedDict provides a (in my opinion) nicer interface with less surprising behavior or pitfalls. My implementation is also faster. e.g iterating over all items is ~5x faster.\n  More info\n    This started as a toy project, that later became useful to me, so I decided to cleanup the code, add tests, and publish it.\n  from better_orderedmultidict import OrderedMultiDict\nomd: OrderedMultiDict[int, int] = OrderedMultiDict([(1,1), (2,2), (1,11), (2,22)])\n\nfor key in reversed(omd.unique_keys()):\n    print(f\"{key}: {omd.getall(key)}\")\n# prints:\n# 2: [2, 22]\n# 1: [1, 11]\n\nprint(omd.popfirstitem())  # prints: (1, 1)\nprint(omd.poplast(2))  # prints: 22\n\nfor key in reversed(omd.unique_keys()):\n    print(f\"{key}: {omd.getall(key)}\")\n# prints:\n# 2: [2]\n# 1: [11]Installation\n    You can install Better-OrderedMultiDict using pip:\n  pip install better-orderedmultidictContributing\n    If you have any suggestions or improvements for Better-OrderedMultiDict, feel free to submit a pull request or open an issue on the GitHub repository. I appreciate any feedback or contributions!\n  Links\n    Here's the link to the GitHub repository: https://github.com/JoachimCoenen/Better-OrderedMultiDict\n\n    Here's the link to PyPi: https://pypi.org/project/better-orderedmultidict"
},
{
    "title": "No title",
    "content": "As the title says, I cant decide what to use for rest api for mye summer project. I am uni student, so this project will only be very small scale project. I have made simpel rest apis in sll of them, but still cant decide which one to actuslly use for my project. Do anyone have any tips for which might be right one? A thing to consider for me answel is how easy it is to host."
},
{
    "title": "No title",
    "content": "what my project does\n    This project is built to solve the issue of LLM unable to produce relevant answers for information in a particular context. uses the information  to train the model and stored it in a database and uses  this database to get relevant answers from the Model.\n  Target audiance\n    This project is for people who want to train a LLM on a particular piece of information.\n  comparison\n    This model only gives answers for information regarding the data you provided in the file. It will not answer any other questions including formal greetings.\n  \n    GitHub link :https://github.com/dharmateja2810/RAG-Retrieval-Augmented-Generation-Model"
},
{
    "title": "No title",
    "content": "Hi I’m looking for inspiration for some stupid python automation projects. If you have done something funny or stupid using python automation I would love to hear it."
},
{
    "title": "No title",
    "content": "Python is a great programming language, but sometimes the indentation can be terrible for some people (especially people with visual impairments).\n  \n    So i created Lython.\n  \n    What the project does:\n  \n    Lython replacing the Python indentation to lua-style code blocks.\n  \n    this is example lython code\n  def test(num)\n    for i in range(num) do\n        if i == 0 then\n            print(\"zero\")\n        elif i % 2 == 1 then\n            print(\"odd\")\n        else\n            print(\"even\")\n        end # if else\n    end # for\nend # def\n\ntest(10)\n    for more info, please visit lython repo.\n  \n    Target audience:\n  \n    Peoples with visual impairments (especially) and Programmers who want to write python code with new experience (generally)\n  \n    Repo & Source code:\n  \nguangrei/lython"
},
{
    "title": "No title",
    "content": "well, kind of.\n  \n    I made Pilgram, an infinite idle RPG where your character goes on adventures and notifies you when stuff happens.\n  What my project does\n    The bot provides a text interface with wich you can \"play\" an MMO RPG, it's basically an online idle adventure game\n  Target audience\n    It's a toy project that i made out of boredom, also it sounded cool\n  Comparison\n    I never heard of anything like this except for some really old browser games. Maybe i'm just not informed.\n  More info\n    How is it infinite? The secret is AI. Every quest and event in the game is generated by AI depending on the demand of the players, so in theory you can go on an infinite amount of quests.\n  \n    Why did i call it an MMO? Because you can kind of play with your friends by creating & joining guilds and by sending gifts to eachother. There even is a guild leaderboard to see who gets the most points :)\n  \n    The interface is exclusively text based, but the command interpreter i wrote is pretty easy to integrate in other places, even in GUIs if anyone wants to try.\n  \n    I tried out a lot of new things for this project, like using ORMs, writing unit tests (don't look at those, i kinda got bored after a short while), using AI & writing generic enough code that it can be swapped with any other implementation. I think most of the code i wrote is pretty ok, but you can tell me what to change & what to improve if you want.\n  Links\n    here's the link to the code: https://github.com/SudoOmbro/pilgram\n\n    if you wanna try out the version i'm running on my server start a conversation with pilgram_bot on Telegram, don't expect a balanced experience at first since that was kind of the last of my problems lol"
},
{
    "title": "No title",
    "content": "Hey guys!\n  \n    I'm excited to introduce Temporal Adjusters, a new Python package designed to make time series adjustments easier and more efficient. If you work with time series data, you'll find this tool incredibly useful for various temporal adjustments.\n  What my project does\n    Adjusters are a key tool for modifying temporal objects. They exist to externalize the process of adjustment, permitting different approaches, as per the strategy design pattern. Temporal Adjuster provides tools that help pinpoint very specific moments in time, without having to manually count days, weeks, or months. In essence, a Temporal Adjuster is a function that encapsulates a specific date/time manipulation rule. It operates on a temporal object (representing a date, time, or datetime) to produce a new temporal object adjusted according to the rule. Examples might be an adjuster that sets the date avoiding weekends, or one that sets the date to the last day of the month.\n  Installation\n    You can install Temporal Adjuster using pip:\n  pip install temporal-adjusterUsage\n    This package provides a set of predefined temporal adjusters that can be used to adjust a temporal object in various ways. For example:\n  >>> from datetime import date, datetime\n\n>>> from temporal_adjuster import TemporalAdjuster\n>>> from temporal_adjuster.common.enums import Weekday\n\n>>> TemporalAdjuster.first_day_of_next_week(date(2021, 1, 1))\ndatetime.date(2021, 1, 4)\n\n>>> TemporalAdjuster.last_day_of_last_month(datetime(2021, 1, 1))\ndatetime.datetime(2020, 12, 31)\n\n>>> TemporalAdjuster.first_of_year(Weekday.SATURDAY, date(2021, 1, 1))\ndatetime.date(2021, 1, 2)\n\n>>> TemporalAdjuster.nth_of_month(Weekday.SUNDAY, datetime(2021, 5, 1), 2)\ndatetime.datetime(2021, 5, 9)\n\n>>> TemporalAdjuster.next(Weekday.MONDAY, datetime(2021, 2, 11), 2)\ndatetime.datetime(2021, 2, 15)Contributing\n    If you have any suggestions or improvements for pynimbar, feel free to submit a pull request or open an issue on the GitHub repository as per the CONTRIBUTING document. We appreciate any feedback or contributions!\n  Target audience\n    This can be used in production. It has only one depedency, dateutils, which if you're manipulating temporal objects you probably already have. All the code is 100% unit-tested, as well as build tested for all supported Python versions.\n  Comparison\n    This is based on Java's native TemporalAdjuster interfaces, but I found no similar library/functionality for Python."
},
{
    "title": "No title",
    "content": "Like the title said. I created an API fro apkpure.com . I was creating a script to automate YouTube Revanced, but i couldn't find anyway to download the apk. You can try out the app here: https://github.com/anishomsy/apkpure\n\nWhat My Project Does\n\n    It allows you to download apk from apkpure. Users can easily fetch specific versions of Android apps programmatically.\n  \nTarget Audience\n\n    it is a hobby project, anyone can use it\n  \nComparison\n\n    I did not find any existing alternatives. So I created my own. The only other way was to download it manually which is very tedious.\n  \n    Please lmk how i can improve.\n  \n    Thank you"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What My Project Does\n    Cloudflare offers a free SQLite based database D1. I needed it for some personal project so I thought of creating a very simple wrapper for it. D1py let's you connect to D1 database in your cloudflare account and run SQL queries(CRUD operations).\n  Target audience\n    For those who need a simple wrapper for Cloudflare D1 API for their projects.\n  Comparison\n    Right now there are no Python wrappers or libraries for D1 yet.... that's why I thought of creating one. It's not perfect but it is my first attempt at writing a small library/package for doing a task.\n  Source\n    Repository: https://github.com/Suleman-Elahi/D1py\n\n    Feel free to drop any suggestions. Thanks."
},
{
    "title": "No title",
    "content": "https://github.com/perpetual-ml/perpetual\nWhat My Project Does\n    PerpetualBooster is a gradient boosting machine (GBM) algorithm which doesn't have hyperparameters to be tuned so that you can use it without needing hyperparameter optimization packages unlike other GBM algorithms. Similar to AutoML libraries, it has a budget parameter which ranges between (0, 1). Increasing the budget parameter increases predictive power of the algorithm and gives better results on unseen data. Start with a small budget and increase it once you are confident with your features. If you don't see any improvement with further increasing budget, it means that you are already extracting the most predictive power out of your data.\n  Target Audience\n    The project is meant for production. You can replace hyperparameter packages plus other gradient boosting algorithms with PerpetualBooster.\n  Comparison\n    Other gradient boosting algorithms (XGBoost, LightGBM, Catboost) and most of the machine learning algorithms need hyperparameter optimization for the best performance on unseen data. But PerpetualBooster doesn't have hyperparameters so it doesn't need hyperparameter tuning. It has a built-in generalization algorithm and provides the best performance.\n  \n    The following table summarizes the results for the California Housing dataset:\n  \n\n\n\n            Perpetual budget\n          \n            LightGBM n_estimators\n          \n            Perpetual mse\n          \n            LightGBM mse\n          \n            Perpetual cpu time\n          \n            LightGBM cpu time\n          \n            Speed-up\n          \n\n\n\n\n\n\n              0.33\n            \n              100\n            \n              0.192\n            \n              0.192\n            \n              10.1\n            \n              990\n            \n              98x\n            \n\n\n              0.35\n            \n              200\n            \n              0.190\n            \n              0.191\n            \n              11.0\n            \n              2030\n            \n              186x\n            \n\n\n              0.45\n            \n              300\n            \n              0.187\n            \n              0.188\n            \n              18.7\n            \n              3272\n            \n              179x"
},
{
    "title": "No title",
    "content": "Just released v0.8.0 of fastapi_problem to provide problem details for FastAPI applications. Hoping it can provide value to some other peoples projects.\n  \n    Code: https://github.com/NRWLDev/fastapi-problem\n\n    Docs: https://nrwldev.github.io/fastapi-problem/\n\n    Pypi: https://pypi.org/project/fastapi-problem/\nWhat My Project Does\n    Provides a simple exception handler and an underlying exception class heirarchy to remove the need to think about error management in your FastAPI project, just raise errors as appropriate and let the handler deal with responses.\n  Target Audience\n    Web developers\n  Comparison\n    There was a previous project that supported RFC7807 but that is no longer maintained, and is also made obsolete by RFC9457.\n  RFC9457\n    For anyone who does not make use of FastAPI, the underlying exception library has also been released, and can be used to implement handlers for any web framework you might be into.\n  \nhttps://github.com/NRWLDev/rfc9457\n\nhttps://pypi.org/project/rfc9457/"
},
{
    "title": "No title",
    "content": "When I was a beginner (or maybe still I am) I struggled a lot with pythons import function.\n  \n    Over the years I went over different approaches, how to handle imports and ended up using mostly exclusive poetry.\n  \n    I've met a lot of people struggling the same way, bit always could just explain very shortly my experience.\n  \n    I've now decided to write it down as a scenario, where I can show and explain my pitfalls:\n  \nhttps://github.com/MaKaNu/pyimport-explained"
},
{
    "title": "No title",
    "content": "What My Project Does\n    It is a CLI to get songs from ultimateguitar.\n  \n    How it looks like: https://youtu.be/Spm1IIaYo8Q\n\n    I've only tried it on linux.\n  \n    Available in debian and pypi.\n  Target audience\n    For musicians who also use the terminal and who don't especially like the ultimateguitar website.\n  Comparison\n    I'm not aware of other projects doing the same thing.\n  \n    Compared to the website, it can transpose and it is much faster.\n  Source\n    Project website: https://codeberg.org/ltworf/ultimateultimateguitar\n\n    Out of date website (just here to avoid the post to be auto-removed): https://github.com/ltworf/ultimateultimateguitar"
},
{
    "title": "No title",
    "content": "is there a proposal for a shorter exception handling syntax for those very frequent cases where a library function doesn't return \"error value\" like str.index ?\n  \n    something like instead of :\n  try:\n    i = my_str.index(\"sub\", st, en)\nexcept ValueError: # if \"sub\" has not been found\n    pass\nelse:\n    # do stuff with i (note that i usually want independent error handling here)\n    something like this :\n  i = my_str.index(\"sub\", st, en) except ValueError -1 # or maybe even return here\nif i == -1: # also can return right away if i want to avoid an indent next\n    # do stuff with i ...\n    i suspect there might be something \"un-pythonic\" here in what i am imagining , but please forgive me if that's the case . i am a fan of Python for many years , but haven't really invested any time in learning the philosophy\n  \n    so i am interested in what the community thinks about this , how ok would such syntax be from the point of the \"Python way\" , and if there is such a proposal i would like to know if i can consider maybe voting on it somehow"
},
{
    "title": "No title",
    "content": "uv is the \"pip but blazingly fast™️ because it's written in rust\" and is developed by the same folks that did ruff. In 0.2.11 they released an experimental/preview command of `uv add/remove` that adds a library to pyproject.toml. It's the first step to become a fully-fledged package manager!\n  \n    I noticed you can also manage python installations with uv using `uv toolchain` command (i.e. be like pyenv) and run tools (like a smaller version of pipx) with `uv run`.\n  \n    I'm genuinely excited about this, Python packaging is going to become such a smooth experience 😎\n  \n    Commands are in preview so expect missing stuff.\n  \n    (I bear no affiliation with astral)\n  \nhttps://github.com/astral-sh/uv"
},
{
    "title": "No title",
    "content": "Hi Python data folks,\n  \n    I am excited to share Pathway, a Python data processing framework we built for ETL and RAG pipelines.\n  \nhttps://github.com/pathwaycom/pathway\n\nWhat My Project Does\n\n    We started Pathway to solve event processing for IoT and geospatial indexing. Think freight train operations in unmapped depots bringing key merchandise from China to Europe. This was not something we could use Flink or Elastic for.\n  \n    Then we added more connectors for streaming ETL (Kafka, Postgres CDC…), data indexing (yay vectors!), and LLM wrappers for RAG. Today Pathway provides a data indexing layer for live data updates, stateless and stateful data transformations over streams, and retrieval of structured and unstructured data.\n  \n    Pathway ships with a Python API and a Rust runtime based on Differential Dataflow to perform incremental computation. All the pipeline is kept in memory and can be easily deployed with Docker and Kubernetes (pipelines-as-code).\n  \n    We built Pathway to support enterprises like F1 teams and processors of highly sensitive information to build mission-critical data pipelines. We do this by putting security and performance first. For example, you can build and deploy self-hosted RAG pipelines with local LLM models and Pathway’s in-memory vector index, so no data ever leaves your infrastructure. Pathway connectors and transformations work with live data by default, so you can avoid expensive reprocessing and rely on fresh data.\n  \n    You can install Pathway with pip and Docker, and get started with templates and notebooks:\n  \nhttps://pathway.com/developers/showcases\n\n    We also host demo RAG pipelines implemented 100% in Pathway, feel free to interact with their API endpoints:\n  \nhttps://pathway.com/solutions/rag-pipelines#try-it-out\n\n    We'd love to hear what you think of Pathway!"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "In a few weeks, Polars 1.0 will be out. How exciting!\n  \n    You can already try out the pre-release by running:\n  \n    ```\n  \n    pip install -U --pre polars```\n  \n    If you encounter any bugs, you can report them to https://github.com/pola-rs/polars/issues, so they can be fixed before 1.0 comes out.\n  \n    Release notes: https://github.com/pola-rs/polars/releases/tag/py-1.0.0-alpha.1"
},
{
    "title": "No title",
    "content": "There's a promotion right now to try PyCharm, get a 30% discount, and 100% of what you pay goes directly to the Django Software Foundation, which maintains Django and keeps it free for everyone.\n  \nhttps://jb.gg/2atgzm\n\n    I hope this kind of post is allowed."
},
{
    "title": "No title",
    "content": "(.venv) PS D:\\flpc> python .\\seed\\test.py\nOperation  | flpc (ms)  | re (ms)   \n----------------------------------\nCompile    | 1496.18077 | 0.00000\nSearch     | 19.67597   | 1721.07339\nFind Match | 15.62524   | 16.72506\nFull Match | 15.62500   | 0.00000\nSplit      | 0.00000    | 1722.88108\nFind All   | 3.02815    | 1660.32910\nFind Iter  | 5.96547    | 1672.50776\nSub        | 0.00000    | 1548.61116\nSubn       | 6.70719    | 1676.84698\nEscape     | 4.87757    | 0.00000\n(.venv) PS D:\\flpc>\nflpc is the name of the library. I named it (spelt as flacpuc). The strange thing is that why the compile time is high of flpc (rust) than of re module (implemented in Pure-Python) (it does the same thing what re.compile does in Python). The benchmark is done on:\n  PATTERN = r'(\\w+)\\s+(\\d+)'\nTEXT = ''.join(choices(ascii_letters + digits, k=1000))\n# choices function from random module\nITERATIONS = 100\n    The problem is that, the python should be slow in the parameter (Regex Compile). However, the rest of parameters looks great! VERY FAST!"
},
{
    "title": "No title",
    "content": "Hi guys! What are your experiences with Vedo or PyVista? Which one do you prefer? Did you have any specific issues which either of these libraries? I'm mostly interested in meshes and point clouds rendering."
},
{
    "title": "No title",
    "content": "About a year ago, I posted on this sub. I was terrified. I was launching a new framework. Another framework? Yes, I was crazy enough to think we needed yet another framework. Thankfully, the response was great. Many were excited to try it. Others were understandably skeptical, and respectfully asking good questions.\n  \n    This time, I'm posting for completely different reasons. I want to share a story. A story of which this sub, and hundreds of you, are part.\n  \n    It all started 2 years ago, when I was laid off from my analytics consulting job. I had a well-paying, comfortable job in the UK. Then I moved from the UK to Poland, where I live now, and continued working remotely. I was living the dream; earning a London salary while living in a place with a lower cost of living. Until it ended with a layoff.\n  \n    I thought, this is it. My career is dead. I didn't speak Polish properly, limiting my options. And finding another fully remote job working for the UK sounded overly optimistic at the time. Being in my mid 30s and with a family to support, I didn't want to start over again.\n  \n    I knew Python and data analytics quite well, and also had frontend skills I had gained throughout the years. So I thought... I need to show what I can do. I didn't have a portfolio at all; my GitHub was empty. After trying Streamlit, I thought the concept was great, but the execution wasn't. So I wrote an article on Medium, discussing how a better, faster alternative was possible. I also created a POC and shared it on GitHub.\n  \n    Thankfully, due to contacts at my previous job, I was able to find another remote job, working for the UK w. With even better pay. So naturally, I forgot about my portfolio-building efforts. But after a few months, an investor (VC) from Germany reached out to me. He had seen the Medium article and asked me whether I'd like to do this full time.\n  \n    I hesitated, but eventually decided to explore this further. I didn't need any investment though; my idea was quite simple. And to be honest, not too different from other frameworks, just faster. I had to think bigger. One day, at London Stansted Airport, while waiting to board a plane home, I decided to go for it and came up with the idea of no-code in the front, Python in the back. In other words, building the frontend using a visual editor, while allowing for full freedom in the backend using Python, and abstracting all the connectivity between.\n  \n    The VC liked the idea, but wasn't fully convinced about my ability to execute. He decided not to invest. But since I liked the idea and thought it could go somewhere, I decided to try building it myself, at night, after work. For 9 months, that was my reality. Nights, weekends. If my baby son would wake up, early mornings too.\n  \n    In May 2023, I managed to get the framework to a state I was happy with, and launched it. The response was very good. I eventually got to 1000 stars on GitHub, a milestone for any open source project. To a great extent, thanks to the support of communities such as r/python and r/opensource. Also, thanks to sites like Medium and Product Hunt.\n  \n    A few months later, in November 2023, the CTO of a multibillion AI company reached out to me. They wanted to acquire my framework, hire me, and build a team for me to continue developing it. I was ecstatic. He told me he'd go on a Thanksgiving break for a few days and that he'd reach out to me after. He never got back to me. Accepting that this wasn't going to happen was tough.\n  \n    Two weeks later, the CTO of another AI company called me, together with the CEO. They also wanted to acquire me and make me a part of their team. A smaller company, much more interesting and already quite established, with clients such as Accenture and Salesforce. But with grit and determination to win in the space of enterprise generative AI. This time, it did work out and my framework was finally acquired. Now I work for them and I lead a team focused on maintaining this open source project. \n  \n    Happy to answer any questions. And THANK YOU for your support r/python!!!\n  \n    For those curious:\n  \nhttps://github.com/writer/writer-framework"
},
{
    "title": "No title",
    "content": "What My Project Does\n    This project aims to create a small-scale text-to-video model that can generate videos based on text prompts.\n  Target audience\n    This project is designed for individuals who want to learn how to create their own text-to-video model from scratch but don't know where to start. It will provide a basic guide from beginning to end, covering everything from generating the training data to training a model and using that trained model to generate AI videos.\n  Comparison\n    Currently available text-to-video models require high computational power, and their complex code makes it difficult for Rookie developers to understand the practical implementation, beyond just the theory. To address this, I have created a small-scale GAN architecture, similar to text-to-video models, which can be trained on a CPU or a single T4 GPU.\n  GitHub\n    Code, documentation, and example can all be found on GitHub:\n  \nhttps://github.com/FareedKhan-dev/AI-text-to-video-model-from-scratch"
},
{
    "title": "No title",
    "content": "Hi all, it's been almost 1 year since the preview of Python in Excel has been revealed. So I wrote up a blog post pointing out what works well and what should be improved: https://www.xlwings.org/blog/my-thoughts-on-python-in-excel\n\n    Here’s the TL;DR:\n  \n\n\n    We wanted an alternative to VBA, but got an alternative to the Excel formula language\n  \n\n\n    Integrating the Jupyter notebook cells inside the Excel grid was a mistake\n  \n\n\n    Python in Excel isn’t suitable for Python beginners nor for interactive data analysis\n  \n\n\n    Right now, there are too many restrictions (can’t use your own packages and can’t connect to web APIs)\n  \n\n\n    Here are the current use cases I see for Python in Excel:\n  \n\n\n    Computationally intensive things like Monte Carlo simulations\n  \n\n\n    AI stuff via the included packages (scikit-learn, nltk, statsmodels, imbalanced-learn, gensim)\n  \n\n\n    Advanced visualizations via Matplotlib/Seaborn\n  \n\n\n    Time-series analysis (this is one of Excel’s blind spots)\n  \n\n\n    Not sure about data cleaning/data analysis: since you almost certainly need Power Query, it may actually be simpler and faster to just stick to Power Query (instead of using Power Query and Python in Excel together)"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Edit: Thanks a lot to those who pointed it out: The name of the concept in question is actually keyword-only arguments. **kwargs is lovely as well, though!\n  \n    I learned Python as my first language and it's the one I'm most proficient in. However, I've since written JavaScript, TypeScript, C#, and a little bit of Go.\n  \n    Even though each language has its own way of doing things, I find that I often miss being able to use kwargs for the sake of readability. This is what I mean:\n  some_function(semantic_parameter_name=value1, explanatory_parameter_name=value2)\n    Often times, the usage of kwargs is sufficiently explanatory of what the function does. Whether it's someone else's code or code that you've written a while back, not only does it save you having to peek at the function's signature/code, it also helps piece together what a block of code intends to do at first glance.\n  \n    At this point, for codebases that I maintain, I almost exclusively define my functions to force the usage of kwargs:\n  kwargs_are_mandatory(*, parameter1: int, parameter2: str) -> None:\n    return\n    When I read code in a language that doesn't support some form of kwargs, I find it more difficult and time consuming to wrap my head around what's happening.\n  \n    What are your thoughts on kwargs?"
},
{
    "title": "No title",
    "content": "I have open-sourced a Text-To-Video-AI generated which generates video from a topic by collecting relevant stock videos and stitching them together similar to popular video tools like Invideo, Pictory etc.\n  \n    Link to code :- https://github.com/SamurAIGPT/Text-To-Video-AI"
},
{
    "title": "No title",
    "content": "Hey everyone,\n  \n    I've been working on a simple router for Uvicorn called ASGIRouter. If you like how Flask handles routing but want to stick with ASGI, you might find this useful.\n  What My Project Does\n    ASGIRouter provides a minimalistic routing solution for ASGI applications. It offers a straightforward way to define routes, similar to Flask, but is built to work any asgi compatible webservers mainly uvicorn.\n  \n    This project is aimed at developers who prefer a minimalistic approach to routing in their ASGI applications. It's suitable for both toy projects and production use, depending on your needs.\n  \n    Compared to existing ASGI routers, ASGIRouter stands out for its simplicity and ease of use. While other routers might offer more features or complexity, ASGIRouter focuses on providing a minimalistic, Flask-like experience for those who want to keep things straightforward.\n  \n    Check it out and let me know what you think."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hey Python community!\n  \n    I’d like to introduce you to my indie product Apitally, a simple API monitoring & analytics tool for Python projects.\n  What My Project Does\n    Apitally provides insights into API traffic, errors, and performance, for the whole API, each endpoint and individual API consumers. It also monitors API uptime, alerting users when their API is down.\n  \n    Apitally directly integrates with various Python web frameworks (FastAPI, Django, Flask, Litestar) through middleware, which captures request & response metadata (never anything sensitive!) and asynchronously ships it to Apitally’s servers in regular intervals.\n  \n    The client library is open-source with the source code available on GitHub.\n  \n    Below is a code example, demonstrating how easy it is to set up Apitally for a FastAPI app (see complete setup guide here):\n  from fastapi import FastAPI\nfrom apitally.fastapi import ApitallyMiddleware\n\napp = FastAPI()\napp.add_middleware(\n    ApitallyMiddleware,\n    client_id=\"your-client-id\",\n    env=\"dev\",  # or \"prod\" etc.\n)Target Audience\n    Engineering teams, individual developers and product owners who build, ship and maintain REST APIs in Python.\n  Comparison\n    The big monitoring platforms (Datadog etc.) can be a bit overwhelming & expensive, particularly for simpler use cases. So Apitally’s key differentiators are simplicity & affordability, with the goal to make it as easy as possible for users to understand usage of their APIs.\n  \n    I hope people here find this useful. Please let me know what you think!"
},
{
    "title": "No title",
    "content": "While monitoring my network while doing some browser automation with selenium, I found strange traffic. After some digging I found https://github.com/SeleniumHQ/selenium/pull/13173 .Searching for SE_AVOID_STATS on google to disable this has only 7 results, and practially impossible to find.\n  \n    I didn't expect to see this kind of dark patterns telemetry in python packages - so yeah. Has anyone else seen this? Is this some sort of recent trend?"
},
{
    "title": "No title",
    "content": "Hello all! I have made an irc client with textual`. Source is available here: https://github.com/rmblau/textchat/\n\n    I would love any and all feedback on code quality and how it can be improved as well as people to test out the client. On first launch it will open a settings screen where you can input your user information once you hit the save button. Right now there's a bug that I'm working on resolving where that you'll have to quit the application once you enter your information and relaunch it to get it to connect. Feel free to file issues and contribute; I hope you all will find this fun and interesting!\n  What My Project Does\n    Only confirmed working on Linux right now.. Right now it does not support SASL, SSL, or znc. It's in alpha and can be installed from pypi. Once installed it can be ran from the cmd line with `textchat`\n  Target Audience\n    This is aimed at people who love irc as much as I do.\n  Comparison\n    There didn't seem to be any application like this so I decided to make it."
},
{
    "title": "No title",
    "content": "Just posted a video on a case study of a Python OpenCV algo that calculates the contact length between the tool and the chip in a metalworking machining process. The images have been captured with a high-speed camera.\n  \n    The Python code and documentation on my GitHub: https://github.com/FrunzaDan/Tool-Chip_Contact_Length\n\n    The video: https://youtu.be/bndai6SlF6E\n\n    Enjoy!\n  What My Project Does\n    The Python algo uses Hough lines to locate the edges of the tool and the chip and calculate the distance between them.\n  Target Audience\n    Python OpenCV enthusiasts and people in metalworking research.\n  Comparison\n    I haven't seen any application like this in metalworking machining."
},
{
    "title": "No title",
    "content": "I have several codebases with around 500+ different tests in each. If one of these tests fails, I need to spend ~20 seconds to find the right file, open it in neovim, and find the right test function. 20 seconds might not sound like much, but trying not to fat-finger paths in the terminal for this amount of time makes my blood boil.\n  \n    I wanted Pytest to do this for me, thought there would be a plugin for it. Google brought up no results, so I asked ChatGPT. It said there's a pytest-edit plugin that adds an --edit option to Pytest.\n  \n    There isn't. So I created just that. Enjoy. https://github.com/MrMino/pytest-edit\n\n    Now, my issue is that I don't know if it works on Windows/Mac with VS Code / PyCharm, etc. - so if anyone would like to spend some time on betatesting a small pytest plugin - issue reports & PRs very much welcome.\n  What My Project Does\n    It adds an --edit option to Pytest, that opens failing test code in the user's editor of choice.\n  Target Audience\n    Pytest users.\n  Comparison\n    AFAIK nothing like this on the market, but I hope I'm wrong.Think %edit magic from IPython but for failed pytest executions."
},
{
    "title": "No title",
    "content": "Like the only different is in pyqt you must share the code or buy a license and in pyside you can share it whether you want to or not. Yet i still see so many videos on pyqt and not pyside"
},
{
    "title": "No title",
    "content": "I have always been curious on how http servers works. Therefore, I decided to write a post on how they work and implementing a simple server in Python.\n  \nLink to blog post"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What my project does\n    Allow users to find out what frameworks, tools, engines a program / game was made in.\n  \n    Looks through a directory and searches for common folder structures, and file names.\n  \n    You can add the -d flag to do a \"deep dive\" and it will look through for strings inside of the binaries.\n  \nImage Example\nTarget Audience\n    Anyone! Developers looking to learn how other programs were made, people who are just interested.\n  Comparison\n    Not sure if there are any alternatives, but another way of finding out how a program runs is looking through the names of files & folders.\n  \n    GitHub: https://github.com/PossiblePanda/hdiw\n\n    Contributors are appreciated :), adding new frameworks, or improving the core of hdiw."
},
{
    "title": "No title",
    "content": "We're excited to share that our Kivy School crowdfunding project on Kickstarter is over 50% funded, but we only have 2 days left to reach our goal!\n  \n    We want to show our appreciation to everyone who has supported us. Even if we don't reach 100% funding, everyone who trusted us will still receive free access to all free resources at kivyschool.com and our course on Udemy.\n  \n    Kivy School is an organization made by volunteers to teach others how to create Python apps using the Kivy framework and deploy them on all platforms: Android, iOS, Windows, macOS, Linux, Raspberry and on your toaster!\n  \n    So if you are still interested on helping Kivy School or on having free access to our Udemy course, you can risk free pledge on the crowdfunding link above before it expires.\n  \n    Keep an eye at Kivy School, soon we will publishing about:\n  \n\n\n    Hot Reload on Android\n  \n\n\n    Supabase integration\n  \n\n\n    Sentry integration\n  \n\n\n    Using SQLAlchemy / SQLModel / Pydantic with Kivy\n  \n\n\n    GPS, Bluetooth, Wi-Fi, Android Services & much more!\n  \n\n\n    It is Python. It is open source. And it is free.\n  \n    Join us at Kivy School, and let's code together!"
},
{
    "title": "No title",
    "content": "Ok, so just so I have this straight:\n  \n\n\n    Asyncio runs in a single thread and uses cooperative multitasking to context switch between tasks\n  \n\n\n    The threading library creates threads and uses preemptive multitasking to context switch between threads\n  \n\n\n    Asyncio is more efficient than threading for the reasons above\n  \n\n\n    Both share the same CPU core/resources\n  \n\n\n    Multiprocessing is using additional cores to speed up CPU bound tasks\n  \n\n\n    So to summarize: a process can create threads and threads can create tasks\n  \n    Is it just me or do people confuse processes as threads or also confuses tasks as threads? This makes getting it all straight pretty confusing and so any help here to confirm what I’ve learned above would be appreciated 🙏"
},
{
    "title": "No title",
    "content": "Hey everyone - I wrote up a blog post on the problems that we've encountered using Celery: https://docs.hatchet.run/blog/problems-with-celery\n\n    Our issues with the Celery project were part of the reason why we started Hatchet.\n  \n    Would love to hear comments or feedback!"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "In a few weeks, Polars 1.0 will be out. How exciting!\n  \n    You can already try out the pre-release by running:\n  \n    ```\n  \n    pip install -U --pre polars```\n  \n    If you encounter any bugs, you can report them to https://github.com/pola-rs/polars/issues, so they can be fixed before 1.0 comes out.\n  \n    Release notes: https://github.com/pola-rs/polars/releases/tag/py-1.0.0-alpha.1"
},
{
    "title": "No title",
    "content": "There's a promotion right now to try PyCharm, get a 30% discount, and 100% of what you pay goes directly to the Django Software Foundation, which maintains Django and keeps it free for everyone.\n  \nhttps://jb.gg/2atgzm\n\n    I hope this kind of post is allowed."
},
{
    "title": "No title",
    "content": "(.venv) PS D:\\flpc> python .\\seed\\test.py\nOperation  | flpc (ms)  | re (ms)   \n----------------------------------\nCompile    | 1496.18077 | 0.00000\nSearch     | 19.67597   | 1721.07339\nFind Match | 15.62524   | 16.72506\nFull Match | 15.62500   | 0.00000\nSplit      | 0.00000    | 1722.88108\nFind All   | 3.02815    | 1660.32910\nFind Iter  | 5.96547    | 1672.50776\nSub        | 0.00000    | 1548.61116\nSubn       | 6.70719    | 1676.84698\nEscape     | 4.87757    | 0.00000\n(.venv) PS D:\\flpc>\nflpc is the name of the library. I named it (spelt as flacpuc). The strange thing is that why the compile time is high of flpc (rust) than of re module (implemented in Pure-Python) (it does the same thing what re.compile does in Python). The benchmark is done on:\n  PATTERN = r'(\\w+)\\s+(\\d+)'\nTEXT = ''.join(choices(ascii_letters + digits, k=1000))\n# choices function from random module\nITERATIONS = 100\n    The problem is that, the python should be slow in the parameter (Regex Compile). However, the rest of parameters looks great! VERY FAST!"
},
{
    "title": "No title",
    "content": "Hi guys! What are your experiences with Vedo or PyVista? Which one do you prefer? Did you have any specific issues which either of these libraries? I'm mostly interested in meshes and point clouds rendering."
},
{
    "title": "No title",
    "content": "About a year ago, I posted on this sub. I was terrified. I was launching a new framework. Another framework? Yes, I was crazy enough to think we needed yet another framework. Thankfully, the response was great. Many were excited to try it. Others were understandably skeptical, and respectfully asking good questions.\n  \n    This time, I'm posting for completely different reasons. I want to share a story. A story of which this sub, and hundreds of you, are part.\n  \n    It all started 2 years ago, when I was laid off from my analytics consulting job. I had a well-paying, comfortable job in the UK. Then I moved from the UK to Poland, where I live now, and continued working remotely. I was living the dream; earning a London salary while living in a place with a lower cost of living. Until it ended with a layoff.\n  \n    I thought, this is it. My career is dead. I didn't speak Polish properly, limiting my options. And finding another fully remote job working for the UK sounded overly optimistic at the time. Being in my mid 30s and with a family to support, I didn't want to start over again.\n  \n    I knew Python and data analytics quite well, and also had frontend skills I had gained throughout the years. So I thought... I need to show what I can do. I didn't have a portfolio at all; my GitHub was empty. After trying Streamlit, I thought the concept was great, but the execution wasn't. So I wrote an article on Medium, discussing how a better, faster alternative was possible. I also created a POC and shared it on GitHub.\n  \n    Thankfully, due to contacts at my previous job, I was able to find another remote job, working for the UK w. With even better pay. So naturally, I forgot about my portfolio-building efforts. But after a few months, an investor (VC) from Germany reached out to me. He had seen the Medium article and asked me whether I'd like to do this full time.\n  \n    I hesitated, but eventually decided to explore this further. I didn't need any investment though; my idea was quite simple. And to be honest, not too different from other frameworks, just faster. I had to think bigger. One day, at London Stansted Airport, while waiting to board a plane home, I decided to go for it and came up with the idea of no-code in the front, Python in the back. In other words, building the frontend using a visual editor, while allowing for full freedom in the backend using Python, and abstracting all the connectivity between.\n  \n    The VC liked the idea, but wasn't fully convinced about my ability to execute. He decided not to invest. But since I liked the idea and thought it could go somewhere, I decided to try building it myself, at night, after work. For 9 months, that was my reality. Nights, weekends. If my baby son would wake up, early mornings too.\n  \n    In May 2023, I managed to get the framework to a state I was happy with, and launched it. The response was very good. I eventually got to 1000 stars on GitHub, a milestone for any open source project. To a great extent, thanks to the support of communities such as r/python and r/opensource. Also, thanks to sites like Medium and Product Hunt.\n  \n    A few months later, in November 2023, the CTO of a multibillion AI company reached out to me. They wanted to acquire my framework, hire me, and build a team for me to continue developing it. I was ecstatic. He told me he'd go on a Thanksgiving break for a few days and that he'd reach out to me after. He never got back to me. Accepting that this wasn't going to happen was tough.\n  \n    Two weeks later, the CTO of another AI company called me, together with the CEO. They also wanted to acquire me and make me a part of their team. A smaller company, much more interesting and already quite established, with clients such as Accenture and Salesforce. But with grit and determination to win in the space of enterprise generative AI. This time, it did work out and my framework was finally acquired. Now I work for them and I lead a team focused on maintaining this open source project. \n  \n    Happy to answer any questions. And THANK YOU for your support r/python!!!\n  \n    For those curious:\n  \nhttps://github.com/writer/writer-framework"
},
{
    "title": "No title",
    "content": "What My Project Does\n    This project aims to create a small-scale text-to-video model that can generate videos based on text prompts.\n  Target audience\n    This project is designed for individuals who want to learn how to create their own text-to-video model from scratch but don't know where to start. It will provide a basic guide from beginning to end, covering everything from generating the training data to training a model and using that trained model to generate AI videos.\n  Comparison\n    Currently available text-to-video models require high computational power, and their complex code makes it difficult for Rookie developers to understand the practical implementation, beyond just the theory. To address this, I have created a small-scale GAN architecture, similar to text-to-video models, which can be trained on a CPU or a single T4 GPU.\n  GitHub\n    Code, documentation, and example can all be found on GitHub:\n  \nhttps://github.com/FareedKhan-dev/AI-text-to-video-model-from-scratch"
},
{
    "title": "No title",
    "content": "Hi all, it's been almost 1 year since the preview of Python in Excel has been revealed. So I wrote up a blog post pointing out what works well and what should be improved: https://www.xlwings.org/blog/my-thoughts-on-python-in-excel\n\n    Here’s the TL;DR:\n  \n\n\n    We wanted an alternative to VBA, but got an alternative to the Excel formula language\n  \n\n\n    Integrating the Jupyter notebook cells inside the Excel grid was a mistake\n  \n\n\n    Python in Excel isn’t suitable for Python beginners nor for interactive data analysis\n  \n\n\n    Right now, there are too many restrictions (can’t use your own packages and can’t connect to web APIs)\n  \n\n\n    Here are the current use cases I see for Python in Excel:\n  \n\n\n    Computationally intensive things like Monte Carlo simulations\n  \n\n\n    AI stuff via the included packages (scikit-learn, nltk, statsmodels, imbalanced-learn, gensim)\n  \n\n\n    Advanced visualizations via Matplotlib/Seaborn\n  \n\n\n    Time-series analysis (this is one of Excel’s blind spots)\n  \n\n\n    Not sure about data cleaning/data analysis: since you almost certainly need Power Query, it may actually be simpler and faster to just stick to Power Query (instead of using Power Query and Python in Excel together)"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Edit: Thanks a lot to those who pointed it out: The name of the concept in question is actually keyword-only arguments. **kwargs is lovely as well, though!\n  \n    I learned Python as my first language and it's the one I'm most proficient in. However, I've since written JavaScript, TypeScript, C#, and a little bit of Go.\n  \n    Even though each language has its own way of doing things, I find that I often miss being able to use kwargs for the sake of readability. This is what I mean:\n  some_function(semantic_parameter_name=value1, explanatory_parameter_name=value2)\n    Often times, the usage of kwargs is sufficiently explanatory of what the function does. Whether it's someone else's code or code that you've written a while back, not only does it save you having to peek at the function's signature/code, it also helps piece together what a block of code intends to do at first glance.\n  \n    At this point, for codebases that I maintain, I almost exclusively define my functions to force the usage of kwargs:\n  kwargs_are_mandatory(*, parameter1: int, parameter2: str) -> None:\n    return\n    When I read code in a language that doesn't support some form of kwargs, I find it more difficult and time consuming to wrap my head around what's happening.\n  \n    What are your thoughts on kwargs?"
},
{
    "title": "No title",
    "content": "I have open-sourced a Text-To-Video-AI generated which generates video from a topic by collecting relevant stock videos and stitching them together similar to popular video tools like Invideo, Pictory etc.\n  \n    Link to code :- https://github.com/SamurAIGPT/Text-To-Video-AI"
},
{
    "title": "No title",
    "content": "Hey everyone,\n  \n    I've been working on a simple router for Uvicorn called ASGIRouter. If you like how Flask handles routing but want to stick with ASGI, you might find this useful.\n  What My Project Does\n    ASGIRouter provides a minimalistic routing solution for ASGI applications. It offers a straightforward way to define routes, similar to Flask, but is built to work any asgi compatible webservers mainly uvicorn.\n  \n    This project is aimed at developers who prefer a minimalistic approach to routing in their ASGI applications. It's suitable for both toy projects and production use, depending on your needs.\n  \n    Compared to existing ASGI routers, ASGIRouter stands out for its simplicity and ease of use. While other routers might offer more features or complexity, ASGIRouter focuses on providing a minimalistic, Flask-like experience for those who want to keep things straightforward.\n  \n    Check it out and let me know what you think."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hey Python community!\n  \n    I’d like to introduce you to my indie product Apitally, a simple API monitoring & analytics tool for Python projects.\n  What My Project Does\n    Apitally provides insights into API traffic, errors, and performance, for the whole API, each endpoint and individual API consumers. It also monitors API uptime, alerting users when their API is down.\n  \n    Apitally directly integrates with various Python web frameworks (FastAPI, Django, Flask, Litestar) through middleware, which captures request & response metadata (never anything sensitive!) and asynchronously ships it to Apitally’s servers in regular intervals.\n  \n    The client library is open-source with the source code available on GitHub.\n  \n    Below is a code example, demonstrating how easy it is to set up Apitally for a FastAPI app (see complete setup guide here):\n  from fastapi import FastAPI\nfrom apitally.fastapi import ApitallyMiddleware\n\napp = FastAPI()\napp.add_middleware(\n    ApitallyMiddleware,\n    client_id=\"your-client-id\",\n    env=\"dev\",  # or \"prod\" etc.\n)Target Audience\n    Engineering teams, individual developers and product owners who build, ship and maintain REST APIs in Python.\n  Comparison\n    The big monitoring platforms (Datadog etc.) can be a bit overwhelming & expensive, particularly for simpler use cases. So Apitally’s key differentiators are simplicity & affordability, with the goal to make it as easy as possible for users to understand usage of their APIs.\n  \n    I hope people here find this useful. Please let me know what you think!"
},
{
    "title": "No title",
    "content": "While monitoring my network while doing some browser automation with selenium, I found strange traffic. After some digging I found https://github.com/SeleniumHQ/selenium/pull/13173 .Searching for SE_AVOID_STATS on google to disable this has only 7 results, and practially impossible to find.\n  \n    I didn't expect to see this kind of dark patterns telemetry in python packages - so yeah. Has anyone else seen this? Is this some sort of recent trend?"
},
{
    "title": "No title",
    "content": "Hello all! I have made an irc client with textual`. Source is available here: https://github.com/rmblau/textchat/\n\n    I would love any and all feedback on code quality and how it can be improved as well as people to test out the client. On first launch it will open a settings screen where you can input your user information once you hit the save button. Right now there's a bug that I'm working on resolving where that you'll have to quit the application once you enter your information and relaunch it to get it to connect. Feel free to file issues and contribute; I hope you all will find this fun and interesting!\n  What My Project Does\n    Only confirmed working on Linux right now.. Right now it does not support SASL, SSL, or znc. It's in alpha and can be installed from pypi. Once installed it can be ran from the cmd line with `textchat`\n  Target Audience\n    This is aimed at people who love irc as much as I do.\n  Comparison\n    There didn't seem to be any application like this so I decided to make it."
},
{
    "title": "No title",
    "content": "Just posted a video on a case study of a Python OpenCV algo that calculates the contact length between the tool and the chip in a metalworking machining process. The images have been captured with a high-speed camera.\n  \n    The Python code and documentation on my GitHub: https://github.com/FrunzaDan/Tool-Chip_Contact_Length\n\n    The video: https://youtu.be/bndai6SlF6E\n\n    Enjoy!\n  What My Project Does\n    The Python algo uses Hough lines to locate the edges of the tool and the chip and calculate the distance between them.\n  Target Audience\n    Python OpenCV enthusiasts and people in metalworking research.\n  Comparison\n    I haven't seen any application like this in metalworking machining."
},
{
    "title": "No title",
    "content": "I have several codebases with around 500+ different tests in each. If one of these tests fails, I need to spend ~20 seconds to find the right file, open it in neovim, and find the right test function. 20 seconds might not sound like much, but trying not to fat-finger paths in the terminal for this amount of time makes my blood boil.\n  \n    I wanted Pytest to do this for me, thought there would be a plugin for it. Google brought up no results, so I asked ChatGPT. It said there's a pytest-edit plugin that adds an --edit option to Pytest.\n  \n    There isn't. So I created just that. Enjoy. https://github.com/MrMino/pytest-edit\n\n    Now, my issue is that I don't know if it works on Windows/Mac with VS Code / PyCharm, etc. - so if anyone would like to spend some time on betatesting a small pytest plugin - issue reports & PRs very much welcome.\n  What My Project Does\n    It adds an --edit option to Pytest, that opens failing test code in the user's editor of choice.\n  Target Audience\n    Pytest users.\n  Comparison\n    AFAIK nothing like this on the market, but I hope I'm wrong.Think %edit magic from IPython but for failed pytest executions."
},
{
    "title": "No title",
    "content": "Like the only different is in pyqt you must share the code or buy a license and in pyside you can share it whether you want to or not. Yet i still see so many videos on pyqt and not pyside"
},
{
    "title": "No title",
    "content": "I have always been curious on how http servers works. Therefore, I decided to write a post on how they work and implementing a simple server in Python.\n  \nLink to blog post"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What my project does\n    Allow users to find out what frameworks, tools, engines a program / game was made in.\n  \n    Looks through a directory and searches for common folder structures, and file names.\n  \n    You can add the -d flag to do a \"deep dive\" and it will look through for strings inside of the binaries.\n  \nImage Example\nTarget Audience\n    Anyone! Developers looking to learn how other programs were made, people who are just interested.\n  Comparison\n    Not sure if there are any alternatives, but another way of finding out how a program runs is looking through the names of files & folders.\n  \n    GitHub: https://github.com/PossiblePanda/hdiw\n\n    Contributors are appreciated :), adding new frameworks, or improving the core of hdiw."
},
{
    "title": "No title",
    "content": "We're excited to share that our Kivy School crowdfunding project on Kickstarter is over 50% funded, but we only have 2 days left to reach our goal!\n  \n    We want to show our appreciation to everyone who has supported us. Even if we don't reach 100% funding, everyone who trusted us will still receive free access to all free resources at kivyschool.com and our course on Udemy.\n  \n    Kivy School is an organization made by volunteers to teach others how to create Python apps using the Kivy framework and deploy them on all platforms: Android, iOS, Windows, macOS, Linux, Raspberry and on your toaster!\n  \n    So if you are still interested on helping Kivy School or on having free access to our Udemy course, you can risk free pledge on the crowdfunding link above before it expires.\n  \n    Keep an eye at Kivy School, soon we will publishing about:\n  \n\n\n    Hot Reload on Android\n  \n\n\n    Supabase integration\n  \n\n\n    Sentry integration\n  \n\n\n    Using SQLAlchemy / SQLModel / Pydantic with Kivy\n  \n\n\n    GPS, Bluetooth, Wi-Fi, Android Services & much more!\n  \n\n\n    It is Python. It is open source. And it is free.\n  \n    Join us at Kivy School, and let's code together!"
},
{
    "title": "No title",
    "content": "Ok, so just so I have this straight:\n  \n\n\n    Asyncio runs in a single thread and uses cooperative multitasking to context switch between tasks\n  \n\n\n    The threading library creates threads and uses preemptive multitasking to context switch between threads\n  \n\n\n    Asyncio is more efficient than threading for the reasons above\n  \n\n\n    Both share the same CPU core/resources\n  \n\n\n    Multiprocessing is using additional cores to speed up CPU bound tasks\n  \n\n\n    So to summarize: a process can create threads and threads can create tasks\n  \n    Is it just me or do people confuse processes as threads or also confuses tasks as threads? This makes getting it all straight pretty confusing and so any help here to confirm what I’ve learned above would be appreciated 🙏"
},
{
    "title": "No title",
    "content": "Hey everyone - I wrote up a blog post on the problems that we've encountered using Celery: https://docs.hatchet.run/blog/problems-with-celery\n\n    Our issues with the Celery project were part of the reason why we started Hatchet.\n  \n    Would love to hear comments or feedback!"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What my project does\n    Allow developers to implement custom themes into their programs, while having a file format that is human readable\n  \n    Example:my_theme.eft\n- My Theme\n\nbackground_color : 255,255,255 : Color\ntitle : \"Hi\" : String\nnumber : 5 : Int\nenabled : true : BoolTarget Audience\n    Developers & users who make themes\n  Comparison\n    CSS Themes - May be difficult to implement, difficult to understand for people who aren't programmers\n  \n    JSON Themes - Viable option, may not be readable in some cases\n  \n    GitHub: https://github.com/PossiblePanda/EFT-py\n\n    Contributors are greatly appreciated :)\n  \n    If you have questions feel free to ask"
},
{
    "title": "No title",
    "content": "Hello there\n  \nWhat my project does:\n\n    I’m excited to share my Flappy Bird clone, written in PyQt! This project captures all the fun of the original game with key features like pressing the spacebar to make the bird jump. Yes, I know, getting that key feature was challenging! 😃 As Richard Watterson once said: \"10/10 game, would play again.\"\n  \nTarget Audience\n\n    This game is for anyone who’s bored and looking for a quick, fun way to pass the time. Whether you're a casual gamer or just curious, this Flappy Bird clone is a not so good way to relive the original experience.\n  \nComparison\n\n    Think of it as a faithful recreation of Flappy Bird with a PyQt twist.\n  \nUpdate\n\n    I had some time, so I made an update. The pipes now start from the middle.\n  \nCode\n\n    You can check out the code here. Please note that the code is definitely not the best, but hey it works!"
},
{
    "title": "No title",
    "content": "What my project does\n    Allow developers to easily log errors, messages, and warnings to the console, and an optional log file.\n  \n    Have you ever released a project, and then somebody runs your project and encounter an error, and you need to see their console? No worries, pandalog can store logs in a file wherever you choose. You can also have errors automatically be sent in the console & log through pandalog.\n  Target Audience\n    Developers\n  Comparison\n    using pandalog has many benefits over just using print, such as storing logs in a log file, colored output in console, extremely configurable\n  \n    You can download it on pypi by running pip install pandalog\n\n    GitHub: https://github.com/PossiblePanda/pandalog\n\n    Contributors are appreciated :)"
},
{
    "title": "No title",
    "content": "Hey everyone!\n  \n    I’m excited to share my first Python package: Melodica Notes. It's a CLI tool aimed at helping melodica players with musical scales, chords, and harmonics.\n  \nWhat My Project Does: Melodica Notes helps melodica players by providing easy access to musical scales, chords, and harmonic information directly from the command line. It's designed to be a simple yet powerful tool for both beginners and advanced players.\n  \nTarget Audience: This project is meant for anyone who plays the melodica (or piano), from casual hobbyists to serious musicians. It's also a project for developers interested in music-related applications. While it’s fully functional, I consider it an evolving tool and welcome contributions to enhance its features.\n  \nComparison: There are other musical tools out there, but Melodica Notes is specifically tailored for melodica players. Unlike general-purpose music theory tools, this CLI focuses on the needs and nuances of melodica playing, making it a unique addition to the musician's toolkit.\n  \n    I’d love to hear your thoughts and suggestions! Whether it's feedback, feature ideas, or pull requests, I welcome all contributions. Your insights can help make this tool even better.\n  \n    Check it out on PyPI and feel free to dive into the code on GitHub.\n  \n    Thanks for your support, and happy coding and playing 🎵"
},
{
    "title": "No title",
    "content": "What my project does:\n\n    It's an interactive tool to help you write json or yaml based on a JSON schema. I built this because I thought it would be helpful to write values.yaml files for Helm charts. But it can be used for a lot of other things like CICD configuration, OpenAPI specifications, etc.\n  \nTarget Audience\n\n    Developers mostly, I guess\n  \nComparison\n\n    I haven't seen anything similar to this. Except maybe spotlight for writing OpenAPI specs, except steer is from the command line.\n  \nCode:\n\nHere's the GitHub repo https://github.com/jcoelho93/steer"
},
{
    "title": "No title",
    "content": "Are there any studies, large-scale polls, or anything about async coding adoption in Python?\n  \n    I wonder how widely the community accepts it, how widespread its usage is, and what the general sentiment is towards it."
},
{
    "title": "No title",
    "content": "Without using decorators I think mostly we can’t build a decent application. They are everywhere.\n    I wrote an article to get an understanding of Decorators.\n  \nhttps://newsletter.piptrends.com/p/understanding-python-decorators\n\n    I hope this will give you a good understanding of Decorators if you don't know about them."
},
{
    "title": "No title",
    "content": "Link: https://github.com/prateekvellala/Archand\nWhat My Project Does\n    Archand allows you to control your mouse entirely using hand gestures which are performed in the air and captured via a webcam. Archand also has a speech-to-text feature which is activated by a specific gesture, transforming your spoken words into written text on your computer. With this, you can perform any task you would normally do with a keyboard as well, such as visiting websites, writing emails, texting people, etc.\n  \n    Archand has the following features, each controlled by a unique hand gesture:\n  \n\n\n    Move pointer\n  \n\n\n    Single left click\n  \n\n\n    Single right click\n  \n\n\n    Double left click\n  \n\n\n    Hold left click and move pointer (for dragging, etc)\n  \n\n\n    Scroll up\n  \n\n\n    Scroll down\n  \n\n\n    Enable your microphone, and then whatever you say will be converted to text and typed where your cursor is blinking (automating keyboard functionality)\n  \n\nTarget Audience\n    Everyone\n  Comparison\n    There is no comparison with any other projects, as I have not seen any that incorporate all the features I have implemented, which work accurately with both low-resolution integrated laptop webcams and high-end webcams. All the projects I've encountered with a similar concept mainly fall into three categories:\n  \n\n\n    They don't work at all, failing even to move the cursor smoothly.\n  \n\n\n    The cursor moves pretty well and smoothly, but they do not fully automate the mouse, as they always lack some other feature like double-clicking, right-clicking, or scrolling, etc.\n  \n\n\n    They have many features that work well, but require high-end webcams, such as the Logitech Brio."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What my project does\n    CopySave is an app that saves everything you copy in your clipboard locally, so it can be used later, thus saving time.\n  Target Audience\n    Everyone who works at a pc, with data. Programmers, especially.\n  Comparison\n    I couldn't find any similar applications. Of course there are some better ones out there.\n  \nhttps://github.com/mpiele/CopySave"
},
{
    "title": "No title",
    "content": "Hey Everyone 👋\n  \n    The author of Robyn here. For those unaware, Robyn is one of the fastest Python web frameworks with a Rust runtime.Robyn offers a variety of features designed to enhance your web development experience. However, one topic that has sparked mixed feelings within the community is Robyn's choice of not supporting ASGI. I'd love to hear your thoughts on this. Specifically, what specific features of ASGI do you miss in Robyn?\n  \n    You can find Robyn's documentation here. We're aiming for a v1.0 release soon, and your feedback will be invaluable in determining whether introducing ASGI support should be a priority.\n  \n    Please avoid generic responses like \"ASGI is a standard and should be supported.\"\n  \n    Instead, share detailed insights and evidence-based arguments to help me understand the tangible benefits ASGI could bring to Robyn or the lack of a specific ASGI feature that will hinder you from using Robyn.\n  \n    Looking forward to your feedback!\n  \n    Thanks again.\n  \n    Repo - https://github.com/sparckles/Robyn/Docs - https://robyn.tech/documentation"
},
{
    "title": "No title",
    "content": "tldr; https://www.youtube.com/playlist?list=PLsaeJ8d49kCnv20dizZqF_EjAoAByNfMj\n\n    long: Hello r/python! As a part of Tech Talks Weekly newsletter, I've put together a list of the most watched Python conference talks from 2023 as a youtube playlist. The list is ordered by the view count for your convenience. The talks come from conferences like PyCon (all locations), PyData (all locations), EuroPython, Conf42, and many more to give you a complete overview of the landscape.\n  \nI've built the playlist as a part of my newsletter called Tech Talks Weekly where once a week I send out all the recently uploaded tech conference talks across engineering conferences (see a recent issue and subscribe if this sounds useful).\n\n    Let me know what do you think!"
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    Zenaura is a cutting-edge Python library, leveraging Pyodide and PyScript, designed to empower developers to create lightweight, performant, stateful, component-based Single Page Applications (SPAs) with ease. By utilizing a virtual DOM implementation, Zenaura enhances performance, reactivity, responsiveness, and interactivity, allowing developers to build dynamic web applications using familiar Python concepts and syntax.\n  \nkey features\n\n\n\n    Exceptional Developer Experience: Intuitive and efficient development workflow.\n  \n\n\n    Smooth Learning Curve: Easy to learn and get started.\n  \n\n\n    Modular Code Structure: Write clean, readable, and maintainable code.\n  \n\n\n    Component-Based Architecture: Build reusable and scalable components.\n  \n\n\n    Page Management: Simplify page creation and navigation.\n  \n\n\n    Built-in Router: Seamless client-side routing.\n  \n\n\n    State and Props Management: Efficiently handle component states and properties.\n  \n\n\n    Dependency Injection: Manage dependencies effortlessly.\n  \n\n\n    Global States and Components: Share states and components across the application.\n  \n\n\n    Optimized Virtual DOM: Enhance application performance with a highly efficient virtual DOM.\n  \n\n\n    Component Lifecycle Methods: Control component behavior at different stages.\n  \n\n\n    Form Support: Easily manage form inputs and validation.\n  \n\n\n    API Integration: Integrate external APIs using the requests module.\n  \n\n\ntarget Audience\n\n    Python developers who want to build stateful, component based SPA using pure python.\n  \nComparison with existing SPA building libraries, frameworks:\n\n\n\n    Python Integration:\n  \n\n\n\n\n    Leverages PyScript and Pyodide: Zenaura allows your Python code to be compiled and transpiled into WebAssembly (WASM), enabling the execution of Python in the browser. This is a significant departure from traditional JavaScript-based frameworks like React, Angular, and Vue, which rely solely on JavaScript for client-side development.\n  \n\n\n\n\n    Developer Ecosystem:\n  \n\n\n\n\n    Pythonic Development: Zenaura enables Python developers to build modern web applications without needing to switch to JavaScript, providing a seamless experience for those who are more comfortable with Python.\n  \n\n\n    Unified Language: By using Python for both front-end and back-end development, Zenaura reduces the context-switching overhead and allows for a more cohesive development experience.\n  \n\n\n\n\n    Performance and Efficiency:\n  \n\n\n\n\n    Virtual DOM Implementation: Similar to React and Vue, Zenaura utilizes a virtual DOM to optimize rendering performance. However, Zenaura's implementation play more well with the virtual DOM as it update the real DOM in non-blocking asyn way. Also thanks to pydide the python interpreter is ported to WASM, which means less JS footprint , very light library sizes on every library developed around zenaura.\n  \n\n\n\n\n    Component-Based Architecture:\n  \n\n\n\n\n    Stateful Components: Zenaura's component-based architecture allows for building reusable, stateful components, akin to React and Vue. This promotes code reusability and modularity.\n  \n\n\n\n\n    Ease of Learning and Use:\n  \n\n\n\n\n    Smooth Learning Curve: Zenaura offers an intuitive and straightforward learning path, especially for developers already familiar with Python. This makes it accessible and easy to adopt compared to the steeper learning curves of frameworks like Angular.\n  \n\n\n\n\n    Ecosystem and Community:\n  \n\n\n\n\n    Growing Python Ecosystem: By integrating with the Python ecosystem, Zenaura can leverage existing Python libraries and tools, providing a rich set of functionalities and a vibrant community for support and collaboration.\n  \n\nResources:\n\n\n    GitHub Repository: https://github.com/ARAldhafeeri/Zenaura\n\n\n\n    Landing Page: https://araldhafeeri.github.io/zenaura-landing-page/\n\n\n\n    Documentation: https://araldhafeeri.github.io/Zenaura/"
},
{
    "title": "No title",
    "content": "Hey folks! I wanted to share Burr, an open-source project we've been working on that I'm really excited about.\n  Target Audience\n    Developers looking to integrate AI into their web services, or who are curious about state machines.\n  The problem\n    Most AI-application frameworks are overly opinionated about how to craft prompts, interact with LLMs, and store memory in a specific format. See this comment for a nice summary. The problem is they often overlook more production-critical aspects such as managing and persisting state, integrating telemetry, bringing apps to production, and seamlessly switching between human input and AI decisions.\n  What My Project Does\n    Our solution is to represent applications explicitly as state machines, which offers several advantages:\n  \n\n\n    Mentally model your system as a flowchart and directly translate it to code\n  \n\n\n    Execute custom hooks before/after step execution\n  \n\n\n    Decouple state persistence from application logic\n  \n\n\n    Rewind back in time/test counterfactuals (load up, fork, and debug)\n  \n\n\n    Query the exact (reproducible) application state at any point in time\n  \n\n\n    This is why we built Burr -- to make these capabilities easy and accessible. The design starts simple: define your actions as functions (or classes) and wire them together in an application. Each action reads from and writes to state, and the application orchestrates, deciding which action to delegate to next. An OS tracking UI lets you inspect the current state/get at *why* your application made a certain decision.\n  \n    While most people use it for LLM-based applications (where state is often complex and critical), we see potential for broader applications such as running time-series simulations, ML training, managing parallel jobs, and more. Burr is entirely dependency-free (using only the standard library), though it offers plugins that you can opt into.\n  \n    We've gotten some great initial traction, and would love more users and feedback. The repository has code examples + links to get started. Feel free to DM if you have any questions!"
},
{
    "title": "No title",
    "content": "Could you please stop using photos of snakes on your articles about Python?\n  \n    Not only is it unimaginative, stale, and cliché, but many of us also find it genuinely off-putting. Our passion certainly lies in coding, not necessarily in reptiles.\n  \n    P.S. Imagine 9 out of 10 articles on Windows featuring photos of pretty youknowwhat"
},
{
    "title": "No title",
    "content": "Hey folks, I work on dlt, the open source python library for turning messy jsons into clean relational tables or typed, clean parquet datasets.\n  \n    We recently created 2 new tools: A python-dict based REST API extractor where you can just declare how to extract, and a tool that can init the above source fully configured by reading an OpenAPI spec. The generation of the pipes is algorithmic and deterministic, not LLM based.\n  What My Project Does\ndlt-init-openapi, and the REST API toolkitare tool designed to simplify the creation of data pipelines by automating the integration with APIs defined by OpenAPI specifications. The pipelines generated are customizable Python pipelines that use the REST API source template that dlt offers (a declarative python-dict first way of writing pipelines).\n  Target Audience\ndlt-init-openapi is designed for data engineers, and other developers who frequently work with API data and require an efficient method to ingest and manage this data within their applications or services. It is particularly useful for those working in environments that support Python and is compatible with various operating systems, making it a versatile tool for both development and production environments.\n  \n    dlt's loader features automatic typing and schema evolution and processes data in microbatches to handle memory, reducing maintenance to almost nothing.\n  Comparison\n    Both the generation and the python declarative REST API source are new to our industry so it's hard to compare. dlt is open source and you will own your pipelines to run as you please in your existing orchestrators, as dlt is just a lightweight library that can run anywhere Python runs, including lightweight things like serverless functions.\n  \n    dlt is like requests + df.to_sql() on steroids, while the generator is similar to generators that create python clients for apis - which is what we basically do with extra info relevant to data engineering work (like incremental loading etc)\n  \n    Someone from community created a blog post comparing it to Airbyte's low code connector: https://untitleddata.company/blog/How-to-create-a-dlt-source-with-a-custom-authentication-method-rest-api-vs-airbyte-low-code\n\n    More Info\n  \n    For more detailed information on how dlt-init-openapi works and how you can integrate it into your projects, check out the links below:\n  \n\n\nGitHub Repository for the tool\n\n\n\nOpenAPI specs repository you can use\n\n\n\nVideo Walkthrough\n\n\n\nColab Demo\n\n\n\nDocumentation and Quick Start Guide\n\n\n\nblog: REST API toolkit which helps understand how to edit the generated pipeline"
},
{
    "title": "No title",
    "content": "Off with the hate, what have been the best Python projects you have worked on? What did the code look like? What were the standards? Why was it the best?"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What My Project Does\n(Link) Check this out : aditya-shrivastavv/ranwcopy\n\n    Python program which generates random words and sentences and copy them to clipboard🗒️.\n  \n    I created a script to automate Bing searches for reward generation\n  \n\n\n    👍 Excellent command line experience.\n  \n\n\n    🙂 User friendly.\n  \n\n\n    🔊 Produces sound so you don't have to start at it.\n  \n\n\n    🔁 Auto copy to clipboard🗒️\n  \n\n\n    💡 Intuitive help menu\n  \n\nTarget Audience\n    Anyone who wants to quickly get points from bing searches under there daily limit\n  Comparison\n    This is no comparison, this is a very unique approch to the problem. You will find many browser extensions which claim to do the same thing, but they don't work like the search engine expects\n  Commands\n    Help menu\n  ranwcopy -h\n#OR\nranwcopy --help\n    Start generating words (10 default with 8 seconds gap)\n  ranwcopy\n    Generate 20 words with 9 seconds gap\n  ranwcopy -i 20 -g 9\n# or\nranwcopy --iterations 20 --timegap 9\n    This is a semi automatic script"
},
{
    "title": "No title",
    "content": "This talks about mutability as Changes inside a function affect the original dictionary which could lead to unexpected behaviors and hard to debug issues.\n  \n    Here is a link to the video\n  \nhttps://www.youtube.com/watch?v=zTTDQePffxU"
},
{
    "title": "No title",
    "content": "py4cli (Scalable Argument Parser)\nTarget Audience\n    * Developers who want to develop scalable cli utility tools in python using declarative programming\n  Comparison \n    * Even Though Python have great libraries for passing command line arguments, those libraries aren't scalable for complex use case. So, I have developed a scalable argument parser, which not only helps in passing cli arguments, but also can alter the execution flow of the code based on arguments.\n  \n    * The Library have two variants minimal and moderate argument parsers, minimal can be used for creating simple cli tool, while moderate is vertically scaled version of minimal argument parser & helps in controlling execution flow of the tool in addition to routing the arguments to the respective methods.\n  What My Project Does\n    * The library works fine with windows & Linux supporting basic data types like int, float, str, list, dict, bool. Further developments for making the solution even more scalable is in progress.\n  \n    Kindly check out the project and documentation below,\n  \n    GitHub Link :  https://github.com/Palani-SN/py4cli ,\n  \n    * Kindly rate the project in GitHub with stars if you like\n  \n    PYPI Link : https://pypi.org/project/py4cli/\n\n    * Feel free to try this out with installation and usage.\n  \n    I am still actively developing it, so any feedback/comments would be appreciated!\n  \n    EDIT :\n  \nHow is it different than already existing tools :\n  \nargparse - argparse is good in supporting different data types, but might not be able to control the flow of the code, or the arguments passed in can not be hierarchical always, which is what I term as scalability. In py4cli, the motive is to have better scalability in terms of hierarchical argument parsing.\n  \nclick, typer & cyclopts - Even though they support hierarchical cli arguments parsing, I feel, they rely much on decorators and its arguments more than necessary, In py4cli, the motive is to have, no extra decorators, or annotations as code, all that needs to be done is define a derived class from one of the base class provided in the lib, as per need and you can directly pass arguments to different methods of the class like how you will pass args and kwargs to a function natively.\n  \nPy4Cli will be fulfilling the very basic aspects of cli interface to parse arguments, while ignoring on cli sophistication to concentrate on the scalability of the arguments passed, and in future to pass nested configuration files as inputs, with an emphasis on loosely coupled architecture.\n  \nAdditional Resources :\n  \n    docs : https://github.com/Palani-SN/py4cli/blob/main/README.md\n\n    examples : https://github.com/Palani-SN/py4cli/tree/main/EXAMPLES"
},
{
    "title": "No title",
    "content": "I'm happy to introduce fastc, a humble Python library designed to make text classification efficient and straightforward, especially in CPU environments. Whether you’re working on sentiment analysis, spam detection, or other text classification tasks, fastc is oriented for small models and avoids fine-tuning, making it perfect for resource-constrained settings. Despite its simple approach, the performance is quite good.\n  \n    Key Features\n  \n\n\n    Focused on CPU execution: Use efficient models like deepset/tinyroberta-6l-768d for embedding generation.\n  \n\n\n    Cosine Similarity Classification: Instead of fine-tuning, classify texts using cosine similarity between class embedding centroids and text embeddings.\n  \n\n\n    Efficient Multi-Classifier Execution: Run multiple classifiers without extra overhead when using the same model for embeddings.\n  \n\n\n    Easy Export and Loading with HuggingFace: Models can be easily exported to and loaded from HuggingFace. Unlike with fine-tuning, only one model for embeddings needs to be loaded in memory to serve any number of classifiers.\n  \n\n\nhttps://github.com/EveripediaNetwork/fastc"
},
{
    "title": "No title",
    "content": "https://youtu.be/sSPWHRpDZXo?si=b-HJ4Cu1sN-tFls1 This video explains how files ( all types) are encrypted and decrypted with PyAesCrypt module of python. Also using pypdf module , pdf files are password protected. Decryption of password protected pdf can also be done"
},
{
    "title": "No title",
    "content": "Good morrow all,\n  \n    I have a simple rest api I have initially developed using Flask. This is a super low utilization app, that may receive 10-12 requests per week. Currently, I have it running a local network using my main machine as the server. This has been great for testing and development, but I need to transition to a more permanent hosting situation. I have been looking at Azure Functions and this seems like the way to go, and would fall under the free tier from what I can tell. Is this the way to go? OR Should i look at other options?\n  \n    This is something for work, not a personal project."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What my project does\n    Allow developers to implement custom themes into their programs, while having a file format that is human readable\n  \n    Example:my_theme.eft\n- My Theme\n\nbackground_color : 255,255,255 : Color\ntitle : \"Hi\" : String\nnumber : 5 : Int\nenabled : true : BoolTarget Audience\n    Developers & users who make themes\n  Comparison\n    CSS Themes - May be difficult to implement, difficult to understand for people who aren't programmers\n  \n    JSON Themes - Viable option, may not be readable in some cases\n  \n    GitHub: https://github.com/PossiblePanda/EFT-py\n\n    Contributors are greatly appreciated :)\n  \n    If you have questions feel free to ask"
},
{
    "title": "No title",
    "content": "Hello there\n  \nWhat my project does:\n\n    I’m excited to share my Flappy Bird clone, written in PyQt! This project captures all the fun of the original game with key features like pressing the spacebar to make the bird jump. Yes, I know, getting that key feature was challenging! 😃 As Richard Watterson once said: \"10/10 game, would play again.\"\n  \nTarget Audience\n\n    This game is for anyone who’s bored and looking for a quick, fun way to pass the time. Whether you're a casual gamer or just curious, this Flappy Bird clone is a not so good way to relive the original experience.\n  \nComparison\n\n    Think of it as a faithful recreation of Flappy Bird with a PyQt twist.\n  \nUpdate\n\n    I had some time, so I made an update. The pipes now start from the middle.\n  \nCode\n\n    You can check out the code here. Please note that the code is definitely not the best, but hey it works!"
},
{
    "title": "No title",
    "content": "What my project does\n    Allow developers to easily log errors, messages, and warnings to the console, and an optional log file.\n  \n    Have you ever released a project, and then somebody runs your project and encounter an error, and you need to see their console? No worries, pandalog can store logs in a file wherever you choose. You can also have errors automatically be sent in the console & log through pandalog.\n  Target Audience\n    Developers\n  Comparison\n    using pandalog has many benefits over just using print, such as storing logs in a log file, colored output in console, extremely configurable\n  \n    You can download it on pypi by running pip install pandalog\n\n    GitHub: https://github.com/PossiblePanda/pandalog\n\n    Contributors are appreciated :)"
},
{
    "title": "No title",
    "content": "Hey everyone!\n  \n    I’m excited to share my first Python package: Melodica Notes. It's a CLI tool aimed at helping melodica players with musical scales, chords, and harmonics.\n  \nWhat My Project Does: Melodica Notes helps melodica players by providing easy access to musical scales, chords, and harmonic information directly from the command line. It's designed to be a simple yet powerful tool for both beginners and advanced players.\n  \nTarget Audience: This project is meant for anyone who plays the melodica (or piano), from casual hobbyists to serious musicians. It's also a project for developers interested in music-related applications. While it’s fully functional, I consider it an evolving tool and welcome contributions to enhance its features.\n  \nComparison: There are other musical tools out there, but Melodica Notes is specifically tailored for melodica players. Unlike general-purpose music theory tools, this CLI focuses on the needs and nuances of melodica playing, making it a unique addition to the musician's toolkit.\n  \n    I’d love to hear your thoughts and suggestions! Whether it's feedback, feature ideas, or pull requests, I welcome all contributions. Your insights can help make this tool even better.\n  \n    Check it out on PyPI and feel free to dive into the code on GitHub.\n  \n    Thanks for your support, and happy coding and playing 🎵"
},
{
    "title": "No title",
    "content": "What my project does:\n\n    It's an interactive tool to help you write json or yaml based on a JSON schema. I built this because I thought it would be helpful to write values.yaml files for Helm charts. But it can be used for a lot of other things like CICD configuration, OpenAPI specifications, etc.\n  \nTarget Audience\n\n    Developers mostly, I guess\n  \nComparison\n\n    I haven't seen anything similar to this. Except maybe spotlight for writing OpenAPI specs, except steer is from the command line.\n  \nCode:\n\nHere's the GitHub repo https://github.com/jcoelho93/steer"
},
{
    "title": "No title",
    "content": "Are there any studies, large-scale polls, or anything about async coding adoption in Python?\n  \n    I wonder how widely the community accepts it, how widespread its usage is, and what the general sentiment is towards it."
},
{
    "title": "No title",
    "content": "Without using decorators I think mostly we can’t build a decent application. They are everywhere.\n    I wrote an article to get an understanding of Decorators.\n  \nhttps://newsletter.piptrends.com/p/understanding-python-decorators\n\n    I hope this will give you a good understanding of Decorators if you don't know about them."
},
{
    "title": "No title",
    "content": "Link: https://github.com/prateekvellala/Archand\nWhat My Project Does\n    Archand allows you to control your mouse entirely using hand gestures which are performed in the air and captured via a webcam. Archand also has a speech-to-text feature which is activated by a specific gesture, transforming your spoken words into written text on your computer. With this, you can perform any task you would normally do with a keyboard as well, such as visiting websites, writing emails, texting people, etc.\n  \n    Archand has the following features, each controlled by a unique hand gesture:\n  \n\n\n    Move pointer\n  \n\n\n    Single left click\n  \n\n\n    Single right click\n  \n\n\n    Double left click\n  \n\n\n    Hold left click and move pointer (for dragging, etc)\n  \n\n\n    Scroll up\n  \n\n\n    Scroll down\n  \n\n\n    Enable your microphone, and then whatever you say will be converted to text and typed where your cursor is blinking (automating keyboard functionality)\n  \n\nTarget Audience\n    Everyone\n  Comparison\n    There is no comparison with any other projects, as I have not seen any that incorporate all the features I have implemented, which work accurately with both low-resolution integrated laptop webcams and high-end webcams. All the projects I've encountered with a similar concept mainly fall into three categories:\n  \n\n\n    They don't work at all, failing even to move the cursor smoothly.\n  \n\n\n    The cursor moves pretty well and smoothly, but they do not fully automate the mouse, as they always lack some other feature like double-clicking, right-clicking, or scrolling, etc.\n  \n\n\n    They have many features that work well, but require high-end webcams, such as the Logitech Brio."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What my project does\n    CopySave is an app that saves everything you copy in your clipboard locally, so it can be used later, thus saving time.\n  Target Audience\n    Everyone who works at a pc, with data. Programmers, especially.\n  Comparison\n    I couldn't find any similar applications. Of course there are some better ones out there.\n  \nhttps://github.com/mpiele/CopySave"
},
{
    "title": "No title",
    "content": "Hey Everyone 👋\n  \n    The author of Robyn here. For those unaware, Robyn is one of the fastest Python web frameworks with a Rust runtime.Robyn offers a variety of features designed to enhance your web development experience. However, one topic that has sparked mixed feelings within the community is Robyn's choice of not supporting ASGI. I'd love to hear your thoughts on this. Specifically, what specific features of ASGI do you miss in Robyn?\n  \n    You can find Robyn's documentation here. We're aiming for a v1.0 release soon, and your feedback will be invaluable in determining whether introducing ASGI support should be a priority.\n  \n    Please avoid generic responses like \"ASGI is a standard and should be supported.\"\n  \n    Instead, share detailed insights and evidence-based arguments to help me understand the tangible benefits ASGI could bring to Robyn or the lack of a specific ASGI feature that will hinder you from using Robyn.\n  \n    Looking forward to your feedback!\n  \n    Thanks again.\n  \n    Repo - https://github.com/sparckles/Robyn/Docs - https://robyn.tech/documentation"
},
{
    "title": "No title",
    "content": "tldr; https://www.youtube.com/playlist?list=PLsaeJ8d49kCnv20dizZqF_EjAoAByNfMj\n\n    long: Hello r/python! As a part of Tech Talks Weekly newsletter, I've put together a list of the most watched Python conference talks from 2023 as a youtube playlist. The list is ordered by the view count for your convenience. The talks come from conferences like PyCon (all locations), PyData (all locations), EuroPython, Conf42, and many more to give you a complete overview of the landscape.\n  \nI've built the playlist as a part of my newsletter called Tech Talks Weekly where once a week I send out all the recently uploaded tech conference talks across engineering conferences (see a recent issue and subscribe if this sounds useful).\n\n    Let me know what do you think!"
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    Zenaura is a cutting-edge Python library, leveraging Pyodide and PyScript, designed to empower developers to create lightweight, performant, stateful, component-based Single Page Applications (SPAs) with ease. By utilizing a virtual DOM implementation, Zenaura enhances performance, reactivity, responsiveness, and interactivity, allowing developers to build dynamic web applications using familiar Python concepts and syntax.\n  \nkey features\n\n\n\n    Exceptional Developer Experience: Intuitive and efficient development workflow.\n  \n\n\n    Smooth Learning Curve: Easy to learn and get started.\n  \n\n\n    Modular Code Structure: Write clean, readable, and maintainable code.\n  \n\n\n    Component-Based Architecture: Build reusable and scalable components.\n  \n\n\n    Page Management: Simplify page creation and navigation.\n  \n\n\n    Built-in Router: Seamless client-side routing.\n  \n\n\n    State and Props Management: Efficiently handle component states and properties.\n  \n\n\n    Dependency Injection: Manage dependencies effortlessly.\n  \n\n\n    Global States and Components: Share states and components across the application.\n  \n\n\n    Optimized Virtual DOM: Enhance application performance with a highly efficient virtual DOM.\n  \n\n\n    Component Lifecycle Methods: Control component behavior at different stages.\n  \n\n\n    Form Support: Easily manage form inputs and validation.\n  \n\n\n    API Integration: Integrate external APIs using the requests module.\n  \n\n\ntarget Audience\n\n    Python developers who want to build stateful, component based SPA using pure python.\n  \nComparison with existing SPA building libraries, frameworks:\n\n\n\n    Python Integration:\n  \n\n\n\n\n    Leverages PyScript and Pyodide: Zenaura allows your Python code to be compiled and transpiled into WebAssembly (WASM), enabling the execution of Python in the browser. This is a significant departure from traditional JavaScript-based frameworks like React, Angular, and Vue, which rely solely on JavaScript for client-side development.\n  \n\n\n\n\n    Developer Ecosystem:\n  \n\n\n\n\n    Pythonic Development: Zenaura enables Python developers to build modern web applications without needing to switch to JavaScript, providing a seamless experience for those who are more comfortable with Python.\n  \n\n\n    Unified Language: By using Python for both front-end and back-end development, Zenaura reduces the context-switching overhead and allows for a more cohesive development experience.\n  \n\n\n\n\n    Performance and Efficiency:\n  \n\n\n\n\n    Virtual DOM Implementation: Similar to React and Vue, Zenaura utilizes a virtual DOM to optimize rendering performance. However, Zenaura's implementation play more well with the virtual DOM as it update the real DOM in non-blocking asyn way. Also thanks to pydide the python interpreter is ported to WASM, which means less JS footprint , very light library sizes on every library developed around zenaura.\n  \n\n\n\n\n    Component-Based Architecture:\n  \n\n\n\n\n    Stateful Components: Zenaura's component-based architecture allows for building reusable, stateful components, akin to React and Vue. This promotes code reusability and modularity.\n  \n\n\n\n\n    Ease of Learning and Use:\n  \n\n\n\n\n    Smooth Learning Curve: Zenaura offers an intuitive and straightforward learning path, especially for developers already familiar with Python. This makes it accessible and easy to adopt compared to the steeper learning curves of frameworks like Angular.\n  \n\n\n\n\n    Ecosystem and Community:\n  \n\n\n\n\n    Growing Python Ecosystem: By integrating with the Python ecosystem, Zenaura can leverage existing Python libraries and tools, providing a rich set of functionalities and a vibrant community for support and collaboration.\n  \n\nResources:\n\n\n    GitHub Repository: https://github.com/ARAldhafeeri/Zenaura\n\n\n\n    Landing Page: https://araldhafeeri.github.io/zenaura-landing-page/\n\n\n\n    Documentation: https://araldhafeeri.github.io/Zenaura/"
},
{
    "title": "No title",
    "content": "Hey folks! I wanted to share Burr, an open-source project we've been working on that I'm really excited about.\n  Target Audience\n    Developers looking to integrate AI into their web services, or who are curious about state machines.\n  The problem\n    Most AI-application frameworks are overly opinionated about how to craft prompts, interact with LLMs, and store memory in a specific format. See this comment for a nice summary. The problem is they often overlook more production-critical aspects such as managing and persisting state, integrating telemetry, bringing apps to production, and seamlessly switching between human input and AI decisions.\n  What My Project Does\n    Our solution is to represent applications explicitly as state machines, which offers several advantages:\n  \n\n\n    Mentally model your system as a flowchart and directly translate it to code\n  \n\n\n    Execute custom hooks before/after step execution\n  \n\n\n    Decouple state persistence from application logic\n  \n\n\n    Rewind back in time/test counterfactuals (load up, fork, and debug)\n  \n\n\n    Query the exact (reproducible) application state at any point in time\n  \n\n\n    This is why we built Burr -- to make these capabilities easy and accessible. The design starts simple: define your actions as functions (or classes) and wire them together in an application. Each action reads from and writes to state, and the application orchestrates, deciding which action to delegate to next. An OS tracking UI lets you inspect the current state/get at *why* your application made a certain decision.\n  \n    While most people use it for LLM-based applications (where state is often complex and critical), we see potential for broader applications such as running time-series simulations, ML training, managing parallel jobs, and more. Burr is entirely dependency-free (using only the standard library), though it offers plugins that you can opt into.\n  \n    We've gotten some great initial traction, and would love more users and feedback. The repository has code examples + links to get started. Feel free to DM if you have any questions!"
},
{
    "title": "No title",
    "content": "Could you please stop using photos of snakes on your articles about Python?\n  \n    Not only is it unimaginative, stale, and cliché, but many of us also find it genuinely off-putting. Our passion certainly lies in coding, not necessarily in reptiles.\n  \n    P.S. Imagine 9 out of 10 articles on Windows featuring photos of pretty youknowwhat"
},
{
    "title": "No title",
    "content": "Hey folks, I work on dlt, the open source python library for turning messy jsons into clean relational tables or typed, clean parquet datasets.\n  \n    We recently created 2 new tools: A python-dict based REST API extractor where you can just declare how to extract, and a tool that can init the above source fully configured by reading an OpenAPI spec. The generation of the pipes is algorithmic and deterministic, not LLM based.\n  What My Project Does\ndlt-init-openapi, and the REST API toolkitare tool designed to simplify the creation of data pipelines by automating the integration with APIs defined by OpenAPI specifications. The pipelines generated are customizable Python pipelines that use the REST API source template that dlt offers (a declarative python-dict first way of writing pipelines).\n  Target Audience\ndlt-init-openapi is designed for data engineers, and other developers who frequently work with API data and require an efficient method to ingest and manage this data within their applications or services. It is particularly useful for those working in environments that support Python and is compatible with various operating systems, making it a versatile tool for both development and production environments.\n  \n    dlt's loader features automatic typing and schema evolution and processes data in microbatches to handle memory, reducing maintenance to almost nothing.\n  Comparison\n    Both the generation and the python declarative REST API source are new to our industry so it's hard to compare. dlt is open source and you will own your pipelines to run as you please in your existing orchestrators, as dlt is just a lightweight library that can run anywhere Python runs, including lightweight things like serverless functions.\n  \n    dlt is like requests + df.to_sql() on steroids, while the generator is similar to generators that create python clients for apis - which is what we basically do with extra info relevant to data engineering work (like incremental loading etc)\n  \n    Someone from community created a blog post comparing it to Airbyte's low code connector: https://untitleddata.company/blog/How-to-create-a-dlt-source-with-a-custom-authentication-method-rest-api-vs-airbyte-low-code\n\n    More Info\n  \n    For more detailed information on how dlt-init-openapi works and how you can integrate it into your projects, check out the links below:\n  \n\n\nGitHub Repository for the tool\n\n\n\nOpenAPI specs repository you can use\n\n\n\nVideo Walkthrough\n\n\n\nColab Demo\n\n\n\nDocumentation and Quick Start Guide\n\n\n\nblog: REST API toolkit which helps understand how to edit the generated pipeline"
},
{
    "title": "No title",
    "content": "Off with the hate, what have been the best Python projects you have worked on? What did the code look like? What were the standards? Why was it the best?"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What My Project Does\n(Link) Check this out : aditya-shrivastavv/ranwcopy\n\n    Python program which generates random words and sentences and copy them to clipboard🗒️.\n  \n    I created a script to automate Bing searches for reward generation\n  \n\n\n    👍 Excellent command line experience.\n  \n\n\n    🙂 User friendly.\n  \n\n\n    🔊 Produces sound so you don't have to start at it.\n  \n\n\n    🔁 Auto copy to clipboard🗒️\n  \n\n\n    💡 Intuitive help menu\n  \n\nTarget Audience\n    Anyone who wants to quickly get points from bing searches under there daily limit\n  Comparison\n    This is no comparison, this is a very unique approch to the problem. You will find many browser extensions which claim to do the same thing, but they don't work like the search engine expects\n  Commands\n    Help menu\n  ranwcopy -h\n#OR\nranwcopy --help\n    Start generating words (10 default with 8 seconds gap)\n  ranwcopy\n    Generate 20 words with 9 seconds gap\n  ranwcopy -i 20 -g 9\n# or\nranwcopy --iterations 20 --timegap 9\n    This is a semi automatic script"
},
{
    "title": "No title",
    "content": "This talks about mutability as Changes inside a function affect the original dictionary which could lead to unexpected behaviors and hard to debug issues.\n  \n    Here is a link to the video\n  \nhttps://www.youtube.com/watch?v=zTTDQePffxU"
},
{
    "title": "No title",
    "content": "py4cli (Scalable Argument Parser)\nTarget Audience\n    * Developers who want to develop scalable cli utility tools in python using declarative programming\n  Comparison \n    * Even Though Python have great libraries for passing command line arguments, those libraries aren't scalable for complex use case. So, I have developed a scalable argument parser, which not only helps in passing cli arguments, but also can alter the execution flow of the code based on arguments.\n  \n    * The Library have two variants minimal and moderate argument parsers, minimal can be used for creating simple cli tool, while moderate is vertically scaled version of minimal argument parser & helps in controlling execution flow of the tool in addition to routing the arguments to the respective methods.\n  What My Project Does\n    * The library works fine with windows & Linux supporting basic data types like int, float, str, list, dict, bool. Further developments for making the solution even more scalable is in progress.\n  \n    Kindly check out the project and documentation below,\n  \n    GitHub Link :  https://github.com/Palani-SN/py4cli ,\n  \n    * Kindly rate the project in GitHub with stars if you like\n  \n    PYPI Link : https://pypi.org/project/py4cli/\n\n    * Feel free to try this out with installation and usage.\n  \n    I am still actively developing it, so any feedback/comments would be appreciated!\n  \n    EDIT :\n  \nHow is it different than already existing tools :\n  \nargparse - argparse is good in supporting different data types, but might not be able to control the flow of the code, or the arguments passed in can not be hierarchical always, which is what I term as scalability. In py4cli, the motive is to have better scalability in terms of hierarchical argument parsing.\n  \nclick, typer & cyclopts - Even though they support hierarchical cli arguments parsing, I feel, they rely much on decorators and its arguments more than necessary, In py4cli, the motive is to have, no extra decorators, or annotations as code, all that needs to be done is define a derived class from one of the base class provided in the lib, as per need and you can directly pass arguments to different methods of the class like how you will pass args and kwargs to a function natively.\n  \nPy4Cli will be fulfilling the very basic aspects of cli interface to parse arguments, while ignoring on cli sophistication to concentrate on the scalability of the arguments passed, and in future to pass nested configuration files as inputs, with an emphasis on loosely coupled architecture.\n  \nAdditional Resources :\n  \n    docs : https://github.com/Palani-SN/py4cli/blob/main/README.md\n\n    examples : https://github.com/Palani-SN/py4cli/tree/main/EXAMPLES"
},
{
    "title": "No title",
    "content": "I'm happy to introduce fastc, a humble Python library designed to make text classification efficient and straightforward, especially in CPU environments. Whether you’re working on sentiment analysis, spam detection, or other text classification tasks, fastc is oriented for small models and avoids fine-tuning, making it perfect for resource-constrained settings. Despite its simple approach, the performance is quite good.\n  \n    Key Features\n  \n\n\n    Focused on CPU execution: Use efficient models like deepset/tinyroberta-6l-768d for embedding generation.\n  \n\n\n    Cosine Similarity Classification: Instead of fine-tuning, classify texts using cosine similarity between class embedding centroids and text embeddings.\n  \n\n\n    Efficient Multi-Classifier Execution: Run multiple classifiers without extra overhead when using the same model for embeddings.\n  \n\n\n    Easy Export and Loading with HuggingFace: Models can be easily exported to and loaded from HuggingFace. Unlike with fine-tuning, only one model for embeddings needs to be loaded in memory to serve any number of classifiers.\n  \n\n\nhttps://github.com/EveripediaNetwork/fastc"
},
{
    "title": "No title",
    "content": "https://youtu.be/sSPWHRpDZXo?si=b-HJ4Cu1sN-tFls1 This video explains how files ( all types) are encrypted and decrypted with PyAesCrypt module of python. Also using pypdf module , pdf files are password protected. Decryption of password protected pdf can also be done"
},
{
    "title": "No title",
    "content": "Good morrow all,\n  \n    I have a simple rest api I have initially developed using Flask. This is a super low utilization app, that may receive 10-12 requests per week. Currently, I have it running a local network using my main machine as the server. This has been great for testing and development, but I need to transition to a more permanent hosting situation. I have been looking at Azure Functions and this seems like the way to go, and would fall under the free tier from what I can tell. Is this the way to go? OR Should i look at other options?\n  \n    This is something for work, not a personal project."
},
{
    "title": "No title",
    "content": "The Python on Microcontrollers (and Raspberry Pi) Newsletter: subscribe for free\n    With the Python on Microcontrollers newsletter, you get all the latest information on Python running on hardware in one place! MicroPython, CircuitPython and Python on single Board Computers like Raspberry Pi & many more.\n  \n    The Python on Microcontrollers newsletter is the place for the latest news. It arrives Monday morning with all the week’s happenings. No advertising, no spam, easy to unsubscribe.\n  \n    11,023 subscribers - the largest Python on hardware newsletter out there.\n  \n    Catch all the weekly news on Python for Microcontrollers with adafruitdaily.com.\n  \n    This ad-free, spam-free weekly email is filled with CircuitPython, MicroPython, and Python information that you may have missed, all in one place!\n  \n    Ensure you catch the weekly Python on Hardware roundup– you can cancel anytime – try our spam-free newsletter today!\n  \nhttps://www.adafruitdaily.com/"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hello, I'm having code-review suggestion doubts about sorting alphabetically fields in classes, e.g. Pydantic models. For example, there's a model:\n  class Example(BaseModel):\n    id: int\n    name: str\n    surname: str\n    age: int\n    operation: str\n    One of developers suggests that fields should be sorted alphabetically:\n  class Example(BaseModel):\n    age: int\n    id: int\n    name: str\n    operation: str\n    surname: str\n    I think there shouldn't be any specific order but only developer' subjective look at importance and connection between fields, like \"name\" and \"surname\" should be next to each other because they are in some way connected. What is your opinion? Maybe there are some PEP8 rules about that?"
},
{
    "title": "No title",
    "content": "Do you feel like you're underutilizing tuples in you code? Maybe cause you think lists are always the correct choice, and tuples don't have a place to exist.\n  \nIn this video we will walk through the differences between lists and tuples, especially focusing on a difference very rarely discussed, albeit it being the most crucial one: the semantic. Following that we will elaborate how and when it is better to utilize either lists or tuples!\n  \n    Any feedback on the content would be highly appreciated ☺️\n  \nhttps://youtu.be/-sO4FG6W4ho"
},
{
    "title": "No title",
    "content": "As Python is one of the most popular languages, many C++ projects end up using Python bindings of some sort. Pytest and Sphinx are very popular frameworks, so many CMake modules have been written, and most projects end up including a copy of these modules or using some hardcoded paths.\n  \n    I wrote two Python packages to manage the installation and update of CMake configs for Pytest and Sphinx.\n  \n\n\nhttps://github.com/python-cmake/pytest-cmake\n\n\n\nhttps://github.com/python-cmake/sphinx-cmake\n\n\n\n    It uses the pip package management, providing a module for each package and automatically generating a configuration based on the package version found.\n  > pip install pytest-cmake\n> pip install sphinx-cmake\n    I hope this method can standardize module integration for common Python tools.\n  \n    Let me know what you think!"
},
{
    "title": "No title",
    "content": "Details about added features in the releases of Polars 0.20.17 to Polars 0.20.31\n  \n\n\nhttps://pola.rs/posts/polars-in-aggregate-jun24/"
},
{
    "title": "No title",
    "content": "I've made this simple little package to stretch out audios https://github.com/Mews/simpleaudiostretch\n\n    However I'm still new to uploading packages to pypi and doing documentation and the sorts, so I'd appreciate it if someone could review my project and see if what I'm doing are the best practices.\n  \n    Thank you in advance if anyone is willing to help"
},
{
    "title": "No title",
    "content": "Hello r/Python,\n  What My Project Does\n    I wanted to share a Python project I've been working on called WavePDE. WavePDE is a simulation and animation tool for studying wave equations in one or two dimensions. It's a handy tool for anyone interested in wave phenomena, also it's customizable and interactive. You can adjust domain size, grid resolution, wave speed, time step, boundary conditions (Dirichlet or Neumann), initial conditions, and more. Additionally, it is possible save your simulations as video files for further analysis or presentations.\n  Target Audience\n    I mainly created this tool while working on my research project. It is not yet complete since it deadens heavily on some parts I still didn't finish. It is about numeric computations of the wave equation on arbitrary boundaries. So I still need to apply some mask on these results and extend the Neumann conditions beyond the current implementation.\n  Comparison\n    This tool is way more customizable (at least imho) than other Python tools I found online. The code is more structured allowing for future extensibility. I also tried to make it as user-friendly as possible. I hope you find it useful and I would appreciate any feedback you might have. I still didn't implement tests, so if you find any bugs please let me know. Also, the documentation is lacking, but I'm working on it.\n  \n    You can find the code on GitHub: https://github.com/salastro/wavepde"
},
{
    "title": "No title",
    "content": "The Python Shiny library is a framework for building interactive web applications in Python.\n  \n    Developed by RStudio, the same team behind the Shiny library for R, this library is particularly useful for data scientists and analysts who want to build interactive dashboards and applications without having extensive front-end development skills.\n  \n    All that is needed is knowledge of the Shiny user interface Application Programming Interface (API).\n  \n    Python Shiny can be used to develop applications that allow users to interact with data in real time. Data scientists can quickly prototype data applications and share them with anyone.\n  \nHow easy is it to use? Let’s use a simple data set and a basic interactive data visualization to take it for a test drive.\n  \n    Free article HERE."
},
{
    "title": "No title",
    "content": "Months ago, PySimpleGUI relicensed from LGPL3 to a proprietary license/subscription model with the release of version 5 and nuked the source code and history from GitHub. Up until recently, the old versions of PySimpleGUI remained on PyPI. However, all but two of these have been deleted and those that remain are yanked.\n  \n    The important effect this has had is anyone who may have defined their requirements as something like PySimpleGUI<5 or PySimpleGUI==4.x.x for a now-deleted version, your installations will fail with a message like:\n  ERROR: No matching distribution found for pysimplegui<5\n    If you have no specific version requested for PySimpleGUI you will end up installing the version with a proprietary license and nagware.\n  \n    There are three options to deal with this without compeltely changing your code:\n  \n\n\n    Specify the latest yanked, but now unsupported version of PySimpleGUI PySimpleGUI==4.60.5 and hope they don't delete that some time in the future\n  \n\n\n    Use the supported LGPL fork, FreeSimpleGUI (full disclosure, I maintain this fork)\n  \n\n\n    Pay up for a PySimpleGUI 5 license."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "https://github.com/gauge-sh/tach\n\n    Hey everyone! Wanted to share some pretty significant updates to the tool I've been working on. Tach lets you define module boundaries and enforce rules across your modules, including isolation, dependencies, and strict interfaces. Some updates -\n  \n\n\n    Re-wrote the core in Rust, leading to a ~19x speed up on large repos\n  \n\n\n    Re-worked the interface, and added a TUI to let you interactively declare modules\n  \n\n\n    We built Tach to solve the “ball of mud” problem that we’ve ran into throughout all of my previous work experiences. Over time, the codebase would become tightly coupled together, making even simple changes/refactors painful. By setting up module boundaries and enforcing them early on, you can avoid all of this!\n  \n    Tach is the best way to grow a modular monolith without creating a ball of mud. If anyone has any questions or feedback, I’d love chat!\n  \nhttps://github.com/gauge-sh/tach\n\nWhat My Project Does\n\n    Tach enables you to interactively declare module boundaries, dependencies between modules, and strict interfaces for those modules. You can then enforce those declarations through a static code check.\n  \nTarget Audience \n  \n    Teams maintaining python monorepos.\n  \nComparison \n  \n    Import linter is probably the most similar tool - for a github discussion on the differences, check out this link - https://github.com/gauge-sh/tach/discussions/72"
},
{
    "title": "No title",
    "content": "Good \"time of day\" my fellow peeps\n  \nWhat my project does:\n\n    I wanted to share my Python game I've been slowly working on over the past... I'd say 1.5 years. It is a simple texted based resource collection game where you travel to different areas, collect resources, sell them in town but be careful there are bandits about, so don't go too far without having some cooked fish on you...\n  \nTarget Audience:\n\n    I'd say its mainly for well... everyone, anyone who enjoys text based games and anyone who wants to chill out on a rainy day when all the other games in their steam library are looking boring and they just want to relax...\n  \nComparison:\n\n    I'd say Colossal cave adventure but that is a much bigger... better... game I would call it, but this is just a simple \"learning python\" project I started a while ago and just recently got back into it so I said what the heck why not finish the game. but now I'm stuck as to what to do next, so I thought I'd ask for play testers to come and tell me how bad my coding and game was so I could try and make it more playable... because lord knows I made it so I know how to play it but what about other people.\n  \n    you can find the code on GitHub: https://github.com/littlebudddy321/New-Lands-RPG"
},
{
    "title": "No title",
    "content": "What My Project Does:\n  \n    Moonlighter is a game that includes a mechanic where you place items on shelves in your store and set the price. Customer's reactions give you hints about what prices would be ideal. These reactions take the form of four moods:\n  \n\n\n    ecstatic: price too low so they are extra happy\n  \n\n\n    content: price is what they were expecting,\n  \n\n\n    sad: price is too high to them but they buy anyway and this lowers the price everyone will pay for a certain period\n  \n\n\n    angry: price is too high so they don't buy\n  \n\n\n    I built a simplified version where a sad reaction doesn't lower the prices customers will accept for that item using Python and SQLite.\n  \n    The Bayesian bandits algorithm is an algorithm to optimize rewards when choosing among different options. The probability of different rewards (e.g. revenue) is kept track of and updated as rewards for options are collected. When a new option is to be selected a competition occurs where the rewards are sampled from these probability distributions and the option with the highest reward is chosen.\n  \n    For this simulation, the reward distributions are the probability that a price is the ideal price for that item. This scenario is so simple that the probability of any particular ideal price is flat or the same for all prices between an upper and lower bound and zero outside. This makes item/price selection simply randomly selecting a price from the lower to upper bounds for every item and selecting the item with the highest price.\n  \n    Customer reaction moods update the item upper/lower price bounds in these ways:\n  \n\n\n    ecstatic or content: lower bound is set to price plus 1 gold\n  \n\n\n    sad: lower bound is set to price if upper and lower bounds don't match\n  \n\n\n    angry: upper bound set to price minus 1 gold if the upper and lower bounds don't match\n  \n\n\n    The SQLite database keeps track of items in your inventory, items on shelves, customer reactions, item price bounds, and Thompson competitions (i.e. prices randomly chosen between price bounds for each item).\n  \n    The algorithm ended up identifying groups of items with the same ideal prices and selling them off from highest to lowest.\n  \n    For the full write up and a lot of pretty graphs check out the article in the link below. I've also included the Github link for those that want to see the full implementation and/or a Jupyter notebook where I generate the plots.\n  \n    Full write-up: https://cmshymansky.com/MoonlighterBayesianBanditsPricing/?source=rPython\n\n    Github: https://github.com/JaggedParadigm/moonlighter_bayesian_bandit_pricing\n\n    Target Audience:\n  \n    This a toy, though the Thompson sampling code could be hacked into something useful.\n  \n    Comparison:\n  \n    To my knowledge, I am the first to apply the Bayesian bandits algorithm to a Moonlighter shop simulation. However, pricing via Bayesian bandits is a classic application and there are many blogs and scientific papers on the topic."
},
{
    "title": "No title",
    "content": "What my project does: I’m excited to share about Mesop - a new, open-source Python UI framework that enables Python developers to quickly build delightful web apps in a scalable way.\n  \n    A small team of us at Google have been developing Mesop as an unofficial 20% project for the past few months. A wide range of research and product teams at Google have been using it to rapidly build internal apps and we’ve gotten a lot of positive feedback internally so now we’re looking to get feedback from the open-source community.\n  \nTarget audience: Python developers looking to build AI demos & internal apps.\n  \nComparison: We think that Mesop provides a unique approach to building web UIs in Python compared to existing alternatives like Streamlit and Gradio - making it both easy to get started and also flexible enough to build customized UIs for a wide range of use cases. You can learn more about why we built Mesop here.\n  \n    To look at some example Mesop apps, check out our demo gallery. Also, the demo gallery itself is built with Mesop which demonstrates the type of flexibility you have in building apps with Mesop.\n  \nGitHub repo: https://github.com/google/mesop"
},
{
    "title": "No title",
    "content": "My colleagues and I have been working on making Dask fast. It’s been fun. Dask DataFrame is now 20x faster and ~50% faster than Spark (but it depends a lot on the workload).\n  \n    I wrote a blog post on what we did: https://docs.coiled.io/blog/dask-dataframe-is-fast.html\n\n    Really, this came down not to doing one thing really well, but doing lots of small things “pretty good”. Some of the most prominent changes include:\n  \n\n\n    Apache Arrow support in pandas\n  \n\n\n    Better shuffling algorithm for faster joins\n  \n\n\n    Automatic query optimization\n  \n\n\n    There are a bunch of other improvements too like copy-on-write for pandas 2.0 which ensures copies are only triggered when necessary, GIL fixes in pandas, better serialization, a new parquet reader, etc. We were able to get a 20x speedup on traditional DataFrame benchmarks.\n  \n    I’d love it if people tried things out or suggested improvements we might have overlooked.\n  \n    Blog post: https://docs.coiled.io/blog/dask-dataframe-is-fast.html"
},
{
    "title": "No title",
    "content": "Hi everyone! I'm sharing with you a Python tool I've built and been using, intended to haste new-page creation in WordPress (with Elementor plugin).\n  \n    It's a simple app, but has great expansion potential and it's really easy to use.\n  \n    To start, you will previously need a WordPress site with Elementor installed and activated, and the content you want to introduce into the new page.\n  \n    Run the program, add sections, choose your desired structure, and select the right widgets for your content.\n  \n    Once you've loaded the content, add your credentials and click Confirm & Run (or just press Enter). The system will do the rest :)\n  \n    You can download and see the project at: https://github.com/MauBorre/WordPress-new-page-auto\n\n    Hope you find it useful! 😁"
},
{
    "title": "No title",
    "content": "Granian – the Rust HTTP server for Python applications – 1.4 was released!\n  \n    Blog post: https://polar.sh/emmett-framework/posts/granian-1-4\n\n    Release details: https://github.com/emmett-framework/granian/releases/tag/v1.4.0\n\n    Repo: https://github.com/emmett-framework/granian"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Please check out my Desktoppy Server.\n  \nWhat My Project Does\n\n    It allows you to run your own personal AI on your computer, say bye-bye rate-limits and paywalls from mainstream AI's.\n  \n    It uses ollama internally so you can use all the open-source Models but by default it's using:\n  \n\n\n    LLama3 for text-generation\n  \n\n\n    LLava for image recognition\n  \n\n\n    Stable Diffusion 2 for image generation\n  \n\n\nTarget Audience\n\n    Perfect for new-comers... I wish I had this when I started tackling AI dev.\n  \n    I think it can be a good base to create your awesome AI-powered products!Please let me know what you think about it!\n  \nComparison \n  \n    It differentiates from the other zillion starters by being very basic, allowing for full customization, and joining the 3 models together into 1 for a multi-modal feeling.\n  \n    Easiest possible setup, even for those who don't know the tools yet, all you need is Python3 installed on your PC. Basically a tutorial-starter-multimodal.\n  \n    Much love\n  \n    Link: https://github.com/TwistedMinda/desktoppy-serverThe very basic Web UI that goes along with it: https://github.com/TwistedMinda/desktoppy-web"
},
{
    "title": "No title",
    "content": "Hi everyone,\n  \n    I'd like to share couple of news regarding my personal project:\n  \n\n\n    New documentation written in Ludic showcasing it's capabilities: https://getludic.dev/docs/\n\n\n\n    New section regrading Layouts inspired from the Every Layout Book: https://getludic.dev/catalog/layouts\n\n\n\n    Cookiecutter template to get quickly started: https://github.com/paveldedik/ludic-template\n\n\n\n    I have a lot of plans with this project and I'd appreciate any feedback.\n  \nAbout The Project\n\n    Ludic allows web development in pure Python with components. It uses HTMX to add UI interactivity and has a catalog of components.\n  \nTarget Audience\n\n\n\n    Web developers\n  \n\n\n    People who want to build HTML pages in Python with typing\n  \n\n\n    People without knowledge of JavaScript who want to build interactive UIs\n  \n\n\n    People who want to use HTMX in their projects\n  \n\n\nComparison With Similar Tools\n\n\n\n\n            Feature\n          \n            Ludic\n          \n            FastUI\n          \n            Reflex\n          \n\n\n\n\n\n\n              HTML rendering\n            \n              Server Side\n            \n              Client Side\n            \n              Client Side\n            \n\n\n              Uses Template Engine\n            \n              No\n            \n              No\n            \n              No\n            \n\n\n              UI interactivity\n            \n</> htmx\n\nReact\n\nReact\n\n\n\n              Backend framework\n            \nStarlette\n\nFastAPI\n\nFastAPI\n\n\n\n              Client-Server Communication\n            \nHTML + REST\n\nJSON + REST\n\nWebSockets\n\n\n\n\n    Any feedback is highly appreciated."
},
{
    "title": "No title",
    "content": "Python 3.12 comes bundled with 50 command-line tools.\n  \n    For example, python -m webbrowser http://example.com opens a web browser, python -m sqlite3 launches a sqlite prompt, and python -m ast my_file.py shows the abstract syntax tree for a given Python file.\n  \n    I've dug into each of them and categorized them based on their purpose and how useful they are.\n  \nPython's many command-line tools"
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    Hello everyone! I've just released a new Python package, notion2pandas, which allows you to import a Notion database into a pandas dataframe with just one line of code, and to update a Notion database from a pandas dataframe also with just one line of code.\n  \nTarget Audience\n\n    Whether you're a data scientist, a data engineer, a Python enthusiast, or just curious, 'pip install notion2pandas' from the terminal, follow the tutorial in the README, and happy coding!\n  \n    🔗 GitLab repo: https://gitlab.com/Jaeger87/notion2pandas\n\nKey Features\n\n\n\nEasy to use. import in a single line of code, export with another single line of code\n  \n\n\nNo more boring parsing. You can import any Notion Database in a pandas framework\n  \n\n\nFlexibility. If you don't like the default parsing mode of a data provided by notion2pandas, you can use your own parse function for a specific kind of data.\n  \n\n\nMaintainability. If Notion broke something with an update, the possibility to provide a different parsing function allows you to use Notion2Pandas even if it's not updated with latest notion update.\n  \n\n\nQuick Start\n\n    In the ReadMe you can find everything you need to start.\n  \nComparison\n\n    When I started this project, I couldn't find anything capable of transforming a Notion database into a pandas DataFrame without specifying how to parse the data.\n  \n    If you got any kind of feedback I'm really curious to read it!"
},
{
    "title": "No title",
    "content": "If you're interested in Python multiprocessing, I'd appreciate if you read this and share your thoughts:\n  \n    tl;dr: I've implemented a cross-process request rate limiter, allowing for N requests per T seconds. See it in this Gist.\n  Problem\n    Request rate limiting (or throttling) requires a place in memory to track the the amount of calls already made - some kind of counter. Multiprocessing is not great at having a single shared variable.\n  \n    I have a use case for a multiprocessing system in which each process can make a number of requests to a REST API server. That server imposes a 1000 requests per minute limit. Hence I needed a way to implement a rate limiter that would work across processes and threads.\n  \n    I've spent the past 2 days digging through a ton of SO posts and articles suggesting how to do it, and I came at a few bad solutions. I finally came up with one that I think works quite well. It uses a multiprocessing.Manager, and its Value, Lock and Condition proxies.\n  Solution\n    I've created a CrossProcessThrottle class which stores that counter. The way that the information about the counter is shared with all the processes and threads is through a ThrottleBarrier class instance. Its wait method will do the following:\n  def wait(self):\n    with self._condition:\n        self._condition.wait()\n\n    with self._lock:\n        self._counter.value += 1\n\n\n    Wait for the shared Condition - this will stop all the processes and their threads and keep them dormant.\n  \n\n\n    If the CrossProcessThrottle calculates that we have available requests (ie. the counter is below max_requests, so we don't need to limit the requests), it uses Condition.notify(n)  (docs) in order to let n amount of threads through and carry out the request.\n  \n\n\n    Once approved, each process/thread will bump the shared Value, indicating that a new request was made.\n  \n\n\n    That Value is then used by the CrossProcessThrottle to figure out how many requests have been made since the last check, and adjust its counter. If counter is equal or greater than max_requests, the Condition will be used to stop all processes and threads, until enough time passes.\n  \n    The following is the example code using this system. You can find it in this Gist if you prefer.\n  import datetime\nfrom concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n\nfrom ratelimiter import ThrottleBarrier, CrossProcessesThrottle\n\n\ndef log(*args, **kwargs):\n    print(datetime.datetime.now().strftime('[%H:%M:%S]'), *args, **kwargs)\n\n\ndef task(i, j, throttle_barrier: ThrottleBarrier):\n    # This will block until there is a free slot to make a request\n    throttle_barrier.wait() \n    log(f'request: {i:2d}, {j:2d}  (process, thread)')\n    # make the request here...\n\n\ndef worker(i, throttle_barrier: ThrottleBarrier):\n    # example process worker, starting a bunch of threads\n    with ThreadPoolExecutor(max_workers=5) as executor:\n        for j in range(5):\n            executor.submit(task, i, j, throttle_barrier)\n\n\nif __name__ == '__main__':\n    cross_process_throttle = CrossProcessesThrottle(max_requests=3, per_seconds=10)\n    throttle_barrier = cross_process_throttle.get_barrier()\n\n    log('start')\n    futures = []\n    # schedule 9 jobs, which should exceed our limit of 3 requests per 10 seconds\n    with ProcessPoolExecutor(max_workers=10) as executor:\n    \n        for i in range(3):\n            futures.append(executor.submit(worker, i, throttle_barrier))\n\n        while len(futures):\n            # calling this method carries out the rate limit calculation\n            cross_process_throttle.cycle()\n\n            for future in futures:\n                if future.done():\n                    futures.remove(future)\n\n    log('finish')\n    I've uploaded the source code for CrossProcessThrottle and ThrottleBarrier as a Gist too. Calculating the counter is a bit more code, so I refrain from sharing it here, but in a nutshell:\n  \n\n\n    Store the last amount of requests made as last_counter, initialised as 0\n  \n\n\n    Every time the cycle() is called, compare the difference between the current counter and the last_counter\n\n\n\n    The difference is how many requests have been made since the last check, hence we increment the counter by that many.\n  \n\n\n    We calculate how many calls remaining are allowed: remaining_calls = max_requests - counter\n\n\n\n    And notify that many threads to go ahead and proceed: condition.notify(remaining_calls)\n\n\n\n    The actual process is a little more involved, as at the step 3 we need to store not only the amount of calls made, but also the times they've been made at - so that we can be checking against these later and decrease the counter. You can see it in detail in the Gist.\n  \n    If you've read through the code - what are your thoughts? Am I missing something here? In my tests it works out pretty nicely, producing:\n  [14:57:26] start\n[14:57:26] Calls in the last 10 seconds: current=0 :: remaining=3 :: total=0 :: next slot in=0s\n[14:57:27] request:  0,  1  (process, thread)\n[14:57:27] request:  0,  0  (process, thread)\n[14:57:27] request:  0,  2  (process, thread)\n[14:57:31] Calls in the last 10 seconds: current=3 :: remaining=0 :: total=3 :: next slot in=7s\n[14:57:36] Calls in the last 10 seconds: current=3 :: remaining=0 :: total=3 :: next slot in=2s\n[14:57:38] request:  0,  4  (process, thread)\n[14:57:38] request:  0,  3  (process, thread)\n[14:57:38] request:  1,  0  (process, thread)\n[14:57:41] Calls in the last 10 seconds: current=3 :: remaining=0 :: total=6 :: next slot in=7s\n[14:57:46] Calls in the last 10 seconds: current=3 :: remaining=0 :: total=6 :: next slot in=2s\n[14:57:48] request:  2,  0  (process, thread)\n[14:57:48] request:  1,  1  (process, thread)\n[14:57:48] request:  1,  2  (process, thread)\n[14:57:51] Calls in the last 10 seconds: current=3 :: remaining=0 :: total=9 :: next slot in=8s\n[14:57:56] Calls in the last 10 seconds: current=3 :: remaining=0 :: total=9 :: next slot in=3s\n[14:57:59] request:  2,  4  (process, thread)\n[14:57:59] request:  2,  2  (process, thread)\n[14:57:59] request:  2,  1  (process, thread)\n[14:58:01] Calls in the last 10 seconds: current=3 :: remaining=0 :: total=12 :: next slot in=8s\n[14:58:06] Calls in the last 10 seconds: current=3 :: remaining=0 :: total=12 :: next slot in=3s\n[14:58:09] request:  1,  3  (process, thread)\n[14:58:09] request:  1,  4  (process, thread)\n[14:58:09] request:  2,  3  (process, thread)\n[14:58:10] finish\n    I've also tested it with 1000s scheduled jobs to 60 processes, each spawning several threads, each of which simulates a request. The requests are limited as expected, up to N per T seconds.\n  \n    I really like that I can construct a single ThrottleBarrier instance that can be passed to all processes and simply call the wait method to get permission for a request. It feels like an elegant solution.\n  Research\n    There are a bunch of libraries for rate limiting, some claiming to support multiprocess, however I couldn't get them to do so:\n  \n\n\nhttps://pypi.org/project/ratelimit/\n\n\n\nhttps://pypi.org/project/ratelimiter/\n\n\n\nhttps://pypi.org/project/ratemate/\n\n\n\nhttps://github.com/JWCook/requests-ratelimiter\n\n\n\n    There's a few SO threads and posts discussing the process too, however they either don't consider multiprocessing, or when they do they don't allow using ProcessPoolExecutor:\n  \n\n\nhttps://stackoverflow.com/questions/69306420/rate-limit-api-multi-process\n\n\n\nhttps://stackoverflow.com/questions/40748687/python-api-rate-limiting-how-to-limit-api-calls-globally\n\n\n\nhttps://gist.github.com/justinvanwinkle/d9f04950083c4554835c1a35f9d22dad\n\n\n\nhttps://stackoverflow.com/questions/6920858/interprocess-communication-in-python\n\n\n\n    The issue with ProcessPoolExecutor comes up when you try to use shared resources as it raises an error along the lines of:\n  Synchronized objects should only be shared between processes through inheritance\n    And to be fair the Googling didn't really help me figuring out how to get around it, just finding more people struggling with the issue:\n  \n\n\nhttps://stackoverflow.com/questions/69907453/lock-objects-should-only-be-shared-between-processes-through-inheritance\n\n\n\nhttps://github.com/python/cpython/issues/79967#issuecomment-1455216546\n\n\n\n    The solution would be to not use the ProcessPoolExecutor but that was a bummer. This comment helped me to find the way I've ended up using:\n  \n\n\nhttps://stackoverflow.com/a/65377770/3508719\n\n\n\n    I'm glad that using the SyncManager and its proxies I managed to come up with a solution that allows me to use the executor.\n  Note\n\n\n    I use multiprocessing instead of multithreading as there is some post-processing done to the data returned from the REST API.\n  \n\n\n    I imagine that for better efficiency I could split the system into a single process that does a lot of multithreading for REST API interaction, and then pass the returned data to several processes for post-processing. I didn't have time to do it at the moment, but I'm aware of this as a potential alternative.\n  \n\n\n    I've built an earlier version of the rate limiter using multiprocessing Listener and Client - and carried out the communication through sockets/pipes. While this is useful to know about for inter-process communication, it turned out to be too slow and not support 100s of concurrent requests.\n  \n\n\n    If one of the existing libraries (eg. one of the ones I've listed) supports cross-process rate limiting with ProcessPoolExecutor, I'd love to see how to do it, please share an example!\n  \n\n\n    Multiprocessing can be a pain 😭\n  \n\n\n    Any feedback on my implementation welcome!"
},
{
    "title": "No title",
    "content": "I'm interested in using cython specifically for introducing static typing to parts of a code base. For anyone who has used cython, could you give any details about your experience with introducing it gradually, how it changed the deployment and execution processes, how well it played with code that is calling lots of 3rd party frameworks. Also curious to hear about any headaches or issues it introduced.\n  \n    I'm less interested in the performance benefits, more interested in static type checks. I do use mypy already but I'm left quite lacking with it compared to real compilation checks. I'm curious more generally about the possibility of having a code base that mixes static and dynamic typing, and if I could stay in Python while doing that instead of going to Rust that would really simplify things.\n  \n    Thanks!"
},
{
    "title": "No title",
    "content": "The Python on Microcontrollers (and Raspberry Pi) Newsletter: subscribe for free\n    With the Python on Microcontrollers newsletter, you get all the latest information on Python running on hardware in one place! MicroPython, CircuitPython and Python on single Board Computers like Raspberry Pi & many more.\n  \n    The Python on Microcontrollers newsletter is the place for the latest news. It arrives Monday morning with all the week’s happenings. No advertising, no spam, easy to unsubscribe.\n  \n    11,023 subscribers - the largest Python on hardware newsletter out there.\n  \n    Catch all the weekly news on Python for Microcontrollers with adafruitdaily.com.\n  \n    This ad-free, spam-free weekly email is filled with CircuitPython, MicroPython, and Python information that you may have missed, all in one place!\n  \n    Ensure you catch the weekly Python on Hardware roundup– you can cancel anytime – try our spam-free newsletter today!\n  \nhttps://www.adafruitdaily.com/"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hello, I'm having code-review suggestion doubts about sorting alphabetically fields in classes, e.g. Pydantic models. For example, there's a model:\n  class Example(BaseModel):\n    id: int\n    name: str\n    surname: str\n    age: int\n    operation: str\n    One of developers suggests that fields should be sorted alphabetically:\n  class Example(BaseModel):\n    age: int\n    id: int\n    name: str\n    operation: str\n    surname: str\n    I think there shouldn't be any specific order but only developer' subjective look at importance and connection between fields, like \"name\" and \"surname\" should be next to each other because they are in some way connected. What is your opinion? Maybe there are some PEP8 rules about that?"
},
{
    "title": "No title",
    "content": "Do you feel like you're underutilizing tuples in you code? Maybe cause you think lists are always the correct choice, and tuples don't have a place to exist.\n  \nIn this video we will walk through the differences between lists and tuples, especially focusing on a difference very rarely discussed, albeit it being the most crucial one: the semantic. Following that we will elaborate how and when it is better to utilize either lists or tuples!\n  \n    Any feedback on the content would be highly appreciated ☺️\n  \nhttps://youtu.be/-sO4FG6W4ho"
},
{
    "title": "No title",
    "content": "As Python is one of the most popular languages, many C++ projects end up using Python bindings of some sort. Pytest and Sphinx are very popular frameworks, so many CMake modules have been written, and most projects end up including a copy of these modules or using some hardcoded paths.\n  \n    I wrote two Python packages to manage the installation and update of CMake configs for Pytest and Sphinx.\n  \n\n\nhttps://github.com/python-cmake/pytest-cmake\n\n\n\nhttps://github.com/python-cmake/sphinx-cmake\n\n\n\n    It uses the pip package management, providing a module for each package and automatically generating a configuration based on the package version found.\n  > pip install pytest-cmake\n> pip install sphinx-cmake\n    I hope this method can standardize module integration for common Python tools.\n  \n    Let me know what you think!"
},
{
    "title": "No title",
    "content": "Details about added features in the releases of Polars 0.20.17 to Polars 0.20.31\n  \n\n\nhttps://pola.rs/posts/polars-in-aggregate-jun24/"
},
{
    "title": "No title",
    "content": "I've made this simple little package to stretch out audios https://github.com/Mews/simpleaudiostretch\n\n    However I'm still new to uploading packages to pypi and doing documentation and the sorts, so I'd appreciate it if someone could review my project and see if what I'm doing are the best practices.\n  \n    Thank you in advance if anyone is willing to help"
},
{
    "title": "No title",
    "content": "Hello r/Python,\n  What My Project Does\n    I wanted to share a Python project I've been working on called WavePDE. WavePDE is a simulation and animation tool for studying wave equations in one or two dimensions. It's a handy tool for anyone interested in wave phenomena, also it's customizable and interactive. You can adjust domain size, grid resolution, wave speed, time step, boundary conditions (Dirichlet or Neumann), initial conditions, and more. Additionally, it is possible save your simulations as video files for further analysis or presentations.\n  Target Audience\n    I mainly created this tool while working on my research project. It is not yet complete since it deadens heavily on some parts I still didn't finish. It is about numeric computations of the wave equation on arbitrary boundaries. So I still need to apply some mask on these results and extend the Neumann conditions beyond the current implementation.\n  Comparison\n    This tool is way more customizable (at least imho) than other Python tools I found online. The code is more structured allowing for future extensibility. I also tried to make it as user-friendly as possible. I hope you find it useful and I would appreciate any feedback you might have. I still didn't implement tests, so if you find any bugs please let me know. Also, the documentation is lacking, but I'm working on it.\n  \n    You can find the code on GitHub: https://github.com/salastro/wavepde"
},
{
    "title": "No title",
    "content": "The Python Shiny library is a framework for building interactive web applications in Python.\n  \n    Developed by RStudio, the same team behind the Shiny library for R, this library is particularly useful for data scientists and analysts who want to build interactive dashboards and applications without having extensive front-end development skills.\n  \n    All that is needed is knowledge of the Shiny user interface Application Programming Interface (API).\n  \n    Python Shiny can be used to develop applications that allow users to interact with data in real time. Data scientists can quickly prototype data applications and share them with anyone.\n  \nHow easy is it to use? Let’s use a simple data set and a basic interactive data visualization to take it for a test drive.\n  \n    Free article HERE."
},
{
    "title": "No title",
    "content": "Months ago, PySimpleGUI relicensed from LGPL3 to a proprietary license/subscription model with the release of version 5 and nuked the source code and history from GitHub. Up until recently, the old versions of PySimpleGUI remained on PyPI. However, all but two of these have been deleted and those that remain are yanked.\n  \n    The important effect this has had is anyone who may have defined their requirements as something like PySimpleGUI<5 or PySimpleGUI==4.x.x for a now-deleted version, your installations will fail with a message like:\n  ERROR: No matching distribution found for pysimplegui<5\n    If you have no specific version requested for PySimpleGUI you will end up installing the version with a proprietary license and nagware.\n  \n    There are three options to deal with this without compeltely changing your code:\n  \n\n\n    Specify the latest yanked, but now unsupported version of PySimpleGUI PySimpleGUI==4.60.5 and hope they don't delete that some time in the future\n  \n\n\n    Use the supported LGPL fork, FreeSimpleGUI (full disclosure, I maintain this fork)\n  \n\n\n    Pay up for a PySimpleGUI 5 license."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "https://github.com/gauge-sh/tach\n\n    Hey everyone! Wanted to share some pretty significant updates to the tool I've been working on. Tach lets you define module boundaries and enforce rules across your modules, including isolation, dependencies, and strict interfaces. Some updates -\n  \n\n\n    Re-wrote the core in Rust, leading to a ~19x speed up on large repos\n  \n\n\n    Re-worked the interface, and added a TUI to let you interactively declare modules\n  \n\n\n    We built Tach to solve the “ball of mud” problem that we’ve ran into throughout all of my previous work experiences. Over time, the codebase would become tightly coupled together, making even simple changes/refactors painful. By setting up module boundaries and enforcing them early on, you can avoid all of this!\n  \n    Tach is the best way to grow a modular monolith without creating a ball of mud. If anyone has any questions or feedback, I’d love chat!\n  \nhttps://github.com/gauge-sh/tach\n\nWhat My Project Does\n\n    Tach enables you to interactively declare module boundaries, dependencies between modules, and strict interfaces for those modules. You can then enforce those declarations through a static code check.\n  \nTarget Audience \n  \n    Teams maintaining python monorepos.\n  \nComparison \n  \n    Import linter is probably the most similar tool - for a github discussion on the differences, check out this link - https://github.com/gauge-sh/tach/discussions/72"
},
{
    "title": "No title",
    "content": "Good \"time of day\" my fellow peeps\n  \nWhat my project does:\n\n    I wanted to share my Python game I've been slowly working on over the past... I'd say 1.5 years. It is a simple texted based resource collection game where you travel to different areas, collect resources, sell them in town but be careful there are bandits about, so don't go too far without having some cooked fish on you...\n  \nTarget Audience:\n\n    I'd say its mainly for well... everyone, anyone who enjoys text based games and anyone who wants to chill out on a rainy day when all the other games in their steam library are looking boring and they just want to relax...\n  \nComparison:\n\n    I'd say Colossal cave adventure but that is a much bigger... better... game I would call it, but this is just a simple \"learning python\" project I started a while ago and just recently got back into it so I said what the heck why not finish the game. but now I'm stuck as to what to do next, so I thought I'd ask for play testers to come and tell me how bad my coding and game was so I could try and make it more playable... because lord knows I made it so I know how to play it but what about other people.\n  \n    you can find the code on GitHub: https://github.com/littlebudddy321/New-Lands-RPG"
},
{
    "title": "No title",
    "content": "What My Project Does:\n  \n    Moonlighter is a game that includes a mechanic where you place items on shelves in your store and set the price. Customer's reactions give you hints about what prices would be ideal. These reactions take the form of four moods:\n  \n\n\n    ecstatic: price too low so they are extra happy\n  \n\n\n    content: price is what they were expecting,\n  \n\n\n    sad: price is too high to them but they buy anyway and this lowers the price everyone will pay for a certain period\n  \n\n\n    angry: price is too high so they don't buy\n  \n\n\n    I built a simplified version where a sad reaction doesn't lower the prices customers will accept for that item using Python and SQLite.\n  \n    The Bayesian bandits algorithm is an algorithm to optimize rewards when choosing among different options. The probability of different rewards (e.g. revenue) is kept track of and updated as rewards for options are collected. When a new option is to be selected a competition occurs where the rewards are sampled from these probability distributions and the option with the highest reward is chosen.\n  \n    For this simulation, the reward distributions are the probability that a price is the ideal price for that item. This scenario is so simple that the probability of any particular ideal price is flat or the same for all prices between an upper and lower bound and zero outside. This makes item/price selection simply randomly selecting a price from the lower to upper bounds for every item and selecting the item with the highest price.\n  \n    Customer reaction moods update the item upper/lower price bounds in these ways:\n  \n\n\n    ecstatic or content: lower bound is set to price plus 1 gold\n  \n\n\n    sad: lower bound is set to price if upper and lower bounds don't match\n  \n\n\n    angry: upper bound set to price minus 1 gold if the upper and lower bounds don't match\n  \n\n\n    The SQLite database keeps track of items in your inventory, items on shelves, customer reactions, item price bounds, and Thompson competitions (i.e. prices randomly chosen between price bounds for each item).\n  \n    The algorithm ended up identifying groups of items with the same ideal prices and selling them off from highest to lowest.\n  \n    For the full write up and a lot of pretty graphs check out the article in the link below. I've also included the Github link for those that want to see the full implementation and/or a Jupyter notebook where I generate the plots.\n  \n    Full write-up: https://cmshymansky.com/MoonlighterBayesianBanditsPricing/?source=rPython\n\n    Github: https://github.com/JaggedParadigm/moonlighter_bayesian_bandit_pricing\n\n    Target Audience:\n  \n    This a toy, though the Thompson sampling code could be hacked into something useful.\n  \n    Comparison:\n  \n    To my knowledge, I am the first to apply the Bayesian bandits algorithm to a Moonlighter shop simulation. However, pricing via Bayesian bandits is a classic application and there are many blogs and scientific papers on the topic."
},
{
    "title": "No title",
    "content": "What my project does: I’m excited to share about Mesop - a new, open-source Python UI framework that enables Python developers to quickly build delightful web apps in a scalable way.\n  \n    A small team of us at Google have been developing Mesop as an unofficial 20% project for the past few months. A wide range of research and product teams at Google have been using it to rapidly build internal apps and we’ve gotten a lot of positive feedback internally so now we’re looking to get feedback from the open-source community.\n  \nTarget audience: Python developers looking to build AI demos & internal apps.\n  \nComparison: We think that Mesop provides a unique approach to building web UIs in Python compared to existing alternatives like Streamlit and Gradio - making it both easy to get started and also flexible enough to build customized UIs for a wide range of use cases. You can learn more about why we built Mesop here.\n  \n    To look at some example Mesop apps, check out our demo gallery. Also, the demo gallery itself is built with Mesop which demonstrates the type of flexibility you have in building apps with Mesop.\n  \nGitHub repo: https://github.com/google/mesop"
},
{
    "title": "No title",
    "content": "My colleagues and I have been working on making Dask fast. It’s been fun. Dask DataFrame is now 20x faster and ~50% faster than Spark (but it depends a lot on the workload).\n  \n    I wrote a blog post on what we did: https://docs.coiled.io/blog/dask-dataframe-is-fast.html\n\n    Really, this came down not to doing one thing really well, but doing lots of small things “pretty good”. Some of the most prominent changes include:\n  \n\n\n    Apache Arrow support in pandas\n  \n\n\n    Better shuffling algorithm for faster joins\n  \n\n\n    Automatic query optimization\n  \n\n\n    There are a bunch of other improvements too like copy-on-write for pandas 2.0 which ensures copies are only triggered when necessary, GIL fixes in pandas, better serialization, a new parquet reader, etc. We were able to get a 20x speedup on traditional DataFrame benchmarks.\n  \n    I’d love it if people tried things out or suggested improvements we might have overlooked.\n  \n    Blog post: https://docs.coiled.io/blog/dask-dataframe-is-fast.html"
},
{
    "title": "No title",
    "content": "Hi everyone! I'm sharing with you a Python tool I've built and been using, intended to haste new-page creation in WordPress (with Elementor plugin).\n  \n    It's a simple app, but has great expansion potential and it's really easy to use.\n  \n    To start, you will previously need a WordPress site with Elementor installed and activated, and the content you want to introduce into the new page.\n  \n    Run the program, add sections, choose your desired structure, and select the right widgets for your content.\n  \n    Once you've loaded the content, add your credentials and click Confirm & Run (or just press Enter). The system will do the rest :)\n  \n    You can download and see the project at: https://github.com/MauBorre/WordPress-new-page-auto\n\n    Hope you find it useful! 😁"
},
{
    "title": "No title",
    "content": "Granian – the Rust HTTP server for Python applications – 1.4 was released!\n  \n    Blog post: https://polar.sh/emmett-framework/posts/granian-1-4\n\n    Release details: https://github.com/emmett-framework/granian/releases/tag/v1.4.0\n\n    Repo: https://github.com/emmett-framework/granian"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Please check out my Desktoppy Server.\n  \nWhat My Project Does\n\n    It allows you to run your own personal AI on your computer, say bye-bye rate-limits and paywalls from mainstream AI's.\n  \n    It uses ollama internally so you can use all the open-source Models but by default it's using:\n  \n\n\n    LLama3 for text-generation\n  \n\n\n    LLava for image recognition\n  \n\n\n    Stable Diffusion 2 for image generation\n  \n\n\nTarget Audience\n\n    Perfect for new-comers... I wish I had this when I started tackling AI dev.\n  \n    I think it can be a good base to create your awesome AI-powered products!Please let me know what you think about it!\n  \nComparison \n  \n    It differentiates from the other zillion starters by being very basic, allowing for full customization, and joining the 3 models together into 1 for a multi-modal feeling.\n  \n    Easiest possible setup, even for those who don't know the tools yet, all you need is Python3 installed on your PC. Basically a tutorial-starter-multimodal.\n  \n    Much love\n  \n    Link: https://github.com/TwistedMinda/desktoppy-serverThe very basic Web UI that goes along with it: https://github.com/TwistedMinda/desktoppy-web"
},
{
    "title": "No title",
    "content": "Hi everyone,\n  \n    I'd like to share couple of news regarding my personal project:\n  \n\n\n    New documentation written in Ludic showcasing it's capabilities: https://getludic.dev/docs/\n\n\n\n    New section regrading Layouts inspired from the Every Layout Book: https://getludic.dev/catalog/layouts\n\n\n\n    Cookiecutter template to get quickly started: https://github.com/paveldedik/ludic-template\n\n\n\n    I have a lot of plans with this project and I'd appreciate any feedback.\n  \nAbout The Project\n\n    Ludic allows web development in pure Python with components. It uses HTMX to add UI interactivity and has a catalog of components.\n  \nTarget Audience\n\n\n\n    Web developers\n  \n\n\n    People who want to build HTML pages in Python with typing\n  \n\n\n    People without knowledge of JavaScript who want to build interactive UIs\n  \n\n\n    People who want to use HTMX in their projects\n  \n\n\nComparison With Similar Tools\n\n\n\n\n            Feature\n          \n            Ludic\n          \n            FastUI\n          \n            Reflex\n          \n\n\n\n\n\n\n              HTML rendering\n            \n              Server Side\n            \n              Client Side\n            \n              Client Side\n            \n\n\n              Uses Template Engine\n            \n              No\n            \n              No\n            \n              No\n            \n\n\n              UI interactivity\n            \n</> htmx\n\nReact\n\nReact\n\n\n\n              Backend framework\n            \nStarlette\n\nFastAPI\n\nFastAPI\n\n\n\n              Client-Server Communication\n            \nHTML + REST\n\nJSON + REST\n\nWebSockets\n\n\n\n\n    Any feedback is highly appreciated."
},
{
    "title": "No title",
    "content": "Python 3.12 comes bundled with 50 command-line tools.\n  \n    For example, python -m webbrowser http://example.com opens a web browser, python -m sqlite3 launches a sqlite prompt, and python -m ast my_file.py shows the abstract syntax tree for a given Python file.\n  \n    I've dug into each of them and categorized them based on their purpose and how useful they are.\n  \nPython's many command-line tools"
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    Hello everyone! I've just released a new Python package, notion2pandas, which allows you to import a Notion database into a pandas dataframe with just one line of code, and to update a Notion database from a pandas dataframe also with just one line of code.\n  \nTarget Audience\n\n    Whether you're a data scientist, a data engineer, a Python enthusiast, or just curious, 'pip install notion2pandas' from the terminal, follow the tutorial in the README, and happy coding!\n  \n    🔗 GitLab repo: https://gitlab.com/Jaeger87/notion2pandas\n\nKey Features\n\n\n\nEasy to use. import in a single line of code, export with another single line of code\n  \n\n\nNo more boring parsing. You can import any Notion Database in a pandas framework\n  \n\n\nFlexibility. If you don't like the default parsing mode of a data provided by notion2pandas, you can use your own parse function for a specific kind of data.\n  \n\n\nMaintainability. If Notion broke something with an update, the possibility to provide a different parsing function allows you to use Notion2Pandas even if it's not updated with latest notion update.\n  \n\n\nQuick Start\n\n    In the ReadMe you can find everything you need to start.\n  \nComparison\n\n    When I started this project, I couldn't find anything capable of transforming a Notion database into a pandas DataFrame without specifying how to parse the data.\n  \n    If you got any kind of feedback I'm really curious to read it!"
},
{
    "title": "No title",
    "content": "If you're interested in Python multiprocessing, I'd appreciate if you read this and share your thoughts:\n  \n    tl;dr: I've implemented a cross-process request rate limiter, allowing for N requests per T seconds. See it in this Gist.\n  Problem\n    Request rate limiting (or throttling) requires a place in memory to track the the amount of calls already made - some kind of counter. Multiprocessing is not great at having a single shared variable.\n  \n    I have a use case for a multiprocessing system in which each process can make a number of requests to a REST API server. That server imposes a 1000 requests per minute limit. Hence I needed a way to implement a rate limiter that would work across processes and threads.\n  \n    I've spent the past 2 days digging through a ton of SO posts and articles suggesting how to do it, and I came at a few bad solutions. I finally came up with one that I think works quite well. It uses a multiprocessing.Manager, and its Value, Lock and Condition proxies.\n  Solution\n    I've created a CrossProcessThrottle class which stores that counter. The way that the information about the counter is shared with all the processes and threads is through a ThrottleBarrier class instance. Its wait method will do the following:\n  def wait(self):\n    with self._condition:\n        self._condition.wait()\n\n    with self._lock:\n        self._counter.value += 1\n\n\n    Wait for the shared Condition - this will stop all the processes and their threads and keep them dormant.\n  \n\n\n    If the CrossProcessThrottle calculates that we have available requests (ie. the counter is below max_requests, so we don't need to limit the requests), it uses Condition.notify(n)  (docs) in order to let n amount of threads through and carry out the request.\n  \n\n\n    Once approved, each process/thread will bump the shared Value, indicating that a new request was made.\n  \n\n\n    That Value is then used by the CrossProcessThrottle to figure out how many requests have been made since the last check, and adjust its counter. If counter is equal or greater than max_requests, the Condition will be used to stop all processes and threads, until enough time passes.\n  \n    The following is the example code using this system. You can find it in this Gist if you prefer.\n  import datetime\nfrom concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n\nfrom ratelimiter import ThrottleBarrier, CrossProcessesThrottle\n\n\ndef log(*args, **kwargs):\n    print(datetime.datetime.now().strftime('[%H:%M:%S]'), *args, **kwargs)\n\n\ndef task(i, j, throttle_barrier: ThrottleBarrier):\n    # This will block until there is a free slot to make a request\n    throttle_barrier.wait() \n    log(f'request: {i:2d}, {j:2d}  (process, thread)')\n    # make the request here...\n\n\ndef worker(i, throttle_barrier: ThrottleBarrier):\n    # example process worker, starting a bunch of threads\n    with ThreadPoolExecutor(max_workers=5) as executor:\n        for j in range(5):\n            executor.submit(task, i, j, throttle_barrier)\n\n\nif __name__ == '__main__':\n    cross_process_throttle = CrossProcessesThrottle(max_requests=3, per_seconds=10)\n    throttle_barrier = cross_process_throttle.get_barrier()\n\n    log('start')\n    futures = []\n    # schedule 9 jobs, which should exceed our limit of 3 requests per 10 seconds\n    with ProcessPoolExecutor(max_workers=10) as executor:\n    \n        for i in range(3):\n            futures.append(executor.submit(worker, i, throttle_barrier))\n\n        while len(futures):\n            # calling this method carries out the rate limit calculation\n            cross_process_throttle.cycle()\n\n            for future in futures:\n                if future.done():\n                    futures.remove(future)\n\n    log('finish')\n    I've uploaded the source code for CrossProcessThrottle and ThrottleBarrier as a Gist too. Calculating the counter is a bit more code, so I refrain from sharing it here, but in a nutshell:\n  \n\n\n    Store the last amount of requests made as last_counter, initialised as 0\n  \n\n\n    Every time the cycle() is called, compare the difference between the current counter and the last_counter\n\n\n\n    The difference is how many requests have been made since the last check, hence we increment the counter by that many.\n  \n\n\n    We calculate how many calls remaining are allowed: remaining_calls = max_requests - counter\n\n\n\n    And notify that many threads to go ahead and proceed: condition.notify(remaining_calls)\n\n\n\n    The actual process is a little more involved, as at the step 3 we need to store not only the amount of calls made, but also the times they've been made at - so that we can be checking against these later and decrease the counter. You can see it in detail in the Gist.\n  \n    If you've read through the code - what are your thoughts? Am I missing something here? In my tests it works out pretty nicely, producing:\n  [14:57:26] start\n[14:57:26] Calls in the last 10 seconds: current=0 :: remaining=3 :: total=0 :: next slot in=0s\n[14:57:27] request:  0,  1  (process, thread)\n[14:57:27] request:  0,  0  (process, thread)\n[14:57:27] request:  0,  2  (process, thread)\n[14:57:31] Calls in the last 10 seconds: current=3 :: remaining=0 :: total=3 :: next slot in=7s\n[14:57:36] Calls in the last 10 seconds: current=3 :: remaining=0 :: total=3 :: next slot in=2s\n[14:57:38] request:  0,  4  (process, thread)\n[14:57:38] request:  0,  3  (process, thread)\n[14:57:38] request:  1,  0  (process, thread)\n[14:57:41] Calls in the last 10 seconds: current=3 :: remaining=0 :: total=6 :: next slot in=7s\n[14:57:46] Calls in the last 10 seconds: current=3 :: remaining=0 :: total=6 :: next slot in=2s\n[14:57:48] request:  2,  0  (process, thread)\n[14:57:48] request:  1,  1  (process, thread)\n[14:57:48] request:  1,  2  (process, thread)\n[14:57:51] Calls in the last 10 seconds: current=3 :: remaining=0 :: total=9 :: next slot in=8s\n[14:57:56] Calls in the last 10 seconds: current=3 :: remaining=0 :: total=9 :: next slot in=3s\n[14:57:59] request:  2,  4  (process, thread)\n[14:57:59] request:  2,  2  (process, thread)\n[14:57:59] request:  2,  1  (process, thread)\n[14:58:01] Calls in the last 10 seconds: current=3 :: remaining=0 :: total=12 :: next slot in=8s\n[14:58:06] Calls in the last 10 seconds: current=3 :: remaining=0 :: total=12 :: next slot in=3s\n[14:58:09] request:  1,  3  (process, thread)\n[14:58:09] request:  1,  4  (process, thread)\n[14:58:09] request:  2,  3  (process, thread)\n[14:58:10] finish\n    I've also tested it with 1000s scheduled jobs to 60 processes, each spawning several threads, each of which simulates a request. The requests are limited as expected, up to N per T seconds.\n  \n    I really like that I can construct a single ThrottleBarrier instance that can be passed to all processes and simply call the wait method to get permission for a request. It feels like an elegant solution.\n  Research\n    There are a bunch of libraries for rate limiting, some claiming to support multiprocess, however I couldn't get them to do so:\n  \n\n\nhttps://pypi.org/project/ratelimit/\n\n\n\nhttps://pypi.org/project/ratelimiter/\n\n\n\nhttps://pypi.org/project/ratemate/\n\n\n\nhttps://github.com/JWCook/requests-ratelimiter\n\n\n\n    There's a few SO threads and posts discussing the process too, however they either don't consider multiprocessing, or when they do they don't allow using ProcessPoolExecutor:\n  \n\n\nhttps://stackoverflow.com/questions/69306420/rate-limit-api-multi-process\n\n\n\nhttps://stackoverflow.com/questions/40748687/python-api-rate-limiting-how-to-limit-api-calls-globally\n\n\n\nhttps://gist.github.com/justinvanwinkle/d9f04950083c4554835c1a35f9d22dad\n\n\n\nhttps://stackoverflow.com/questions/6920858/interprocess-communication-in-python\n\n\n\n    The issue with ProcessPoolExecutor comes up when you try to use shared resources as it raises an error along the lines of:\n  Synchronized objects should only be shared between processes through inheritance\n    And to be fair the Googling didn't really help me figuring out how to get around it, just finding more people struggling with the issue:\n  \n\n\nhttps://stackoverflow.com/questions/69907453/lock-objects-should-only-be-shared-between-processes-through-inheritance\n\n\n\nhttps://github.com/python/cpython/issues/79967#issuecomment-1455216546\n\n\n\n    The solution would be to not use the ProcessPoolExecutor but that was a bummer. This comment helped me to find the way I've ended up using:\n  \n\n\nhttps://stackoverflow.com/a/65377770/3508719\n\n\n\n    I'm glad that using the SyncManager and its proxies I managed to come up with a solution that allows me to use the executor.\n  Note\n\n\n    I use multiprocessing instead of multithreading as there is some post-processing done to the data returned from the REST API.\n  \n\n\n    I imagine that for better efficiency I could split the system into a single process that does a lot of multithreading for REST API interaction, and then pass the returned data to several processes for post-processing. I didn't have time to do it at the moment, but I'm aware of this as a potential alternative.\n  \n\n\n    I've built an earlier version of the rate limiter using multiprocessing Listener and Client - and carried out the communication through sockets/pipes. While this is useful to know about for inter-process communication, it turned out to be too slow and not support 100s of concurrent requests.\n  \n\n\n    If one of the existing libraries (eg. one of the ones I've listed) supports cross-process rate limiting with ProcessPoolExecutor, I'd love to see how to do it, please share an example!\n  \n\n\n    Multiprocessing can be a pain 😭\n  \n\n\n    Any feedback on my implementation welcome!"
},
{
    "title": "No title",
    "content": "I'm interested in using cython specifically for introducing static typing to parts of a code base. For anyone who has used cython, could you give any details about your experience with introducing it gradually, how it changed the deployment and execution processes, how well it played with code that is calling lots of 3rd party frameworks. Also curious to hear about any headaches or issues it introduced.\n  \n    I'm less interested in the performance benefits, more interested in static type checks. I do use mypy already but I'm left quite lacking with it compared to real compilation checks. I'm curious more generally about the possibility of having a code base that mixes static and dynamic typing, and if I could stay in Python while doing that instead of going to Rust that would really simplify things.\n  \n    Thanks!"
},
{
    "title": "No title",
    "content": "What My Project Does: PyODMongo is a modern Python library that serves as a robust Object-Document Mapper (ODM) and seamlessly bridges the gap between Python and MongoDB. It offers an intuitive and efficient way to interact with documents.\n  \n\n\n    Built on top of Pydantic V2, PyODMongo ensures that documents in the database rigorously represent the structure of Python objects. This means that documents are saved and retrieved from the database exactly as a Python object is structured, regardless of how nested the objects are and whether they are stored persistently or by reference. PyODMongo can automatically populate these documents.\n  \n\n\n    Target Audience: Backend developers who want a simple and efficient way to work with MongoDB\n  \n\n\n    Comparison: ODMantic ODM\n  \n\n\nGitHub repository PyPi"
},
{
    "title": "No title",
    "content": "Streamlit is becoming an increasingly a popular framework for data visualization prototyping with Python. The Streamlit framework saves time, effort, and reduces the complexity traditionally associated with crafting maps and charts.Particularly if we approach application development with a modular approach.\n  \n    Starting simple, let’s put together 4 specific examples that leverage Streamlit for interactive data visualization:\n  \n\n\n    A global choropleth map for a dataset for a specific year.\n  \n\n\n    An animated global choropleth map for a dataset across a number of years\n  \n\n\n    An animated choropleth map for a specific region\n  \n\n\n    A line chart to provide an alternative representation of the data\n  \n\n\n    Link to tutorial HERE"
},
{
    "title": "No title",
    "content": "What My Project DoesThis is a small Python script that runs inside a Google Sheet by way of the Python add-on. It uses the reddit api to fetch posts from Ask Reddit twice daily. For posts with enough upvotes, it uses the OpenAI API to summarize an answer to the question based on the comments. I then inserts any new questions and their answers into the spreadsheet and uses the Twitter APIto also post the answer to Twitter I mean X. Should be interesting to anybody looking to connect (a subset) of those APIs.\n  \nTarget AudienceAnybody who is looking to mash-up different APIs (Python is great at this and I feel like it is getting a little harder to do this every year).\n  \nComparisonI'm not aware of any Python code that does this. Even finding a good example of the V2 twitter API is harder than it seems. To accomplish some of this, you could try to ask ChatGPT directly to summarize the answers for a url but when I tried it said it couldn't access Reddit.\n  \nResource\n\n\n\n    The spreadsheet where this happens\n\n\n\n    The twitter bot in action\n  \n\n\n    The source code (or make a copy of the spreadsheet to see)"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What My Project Does:\n\n    I finally got some time to attempt the 1 Billion Row Challenge (1BRC) (https://www.morling.dev/blog/one-billion-row-challenge/) where we are supposed to process a file with 1 billion records of temperature values for cities and print a sorted list with min, max and mean temperature per city.\n  \n    I am a sucker for optimization. So when I heard about 1BRC I got intrigued and in the last few days started experimenting with python implementations. I achieved my goal and implemented the fastest implementation running on CPython, without any external libraries. My motivation for CPython was so that I can apply any of the learning's in my day to day work, as I don't see us moving to PyPy any time sooner.\n  \n    You can check out the performance numbers and implementation at : https://github.com/pappuks/1brc\n\n    Few learning's:\n  \n\n\n    Python Multiprocessing is very powerful in enabling multi core processing and overcoming GIL bottleneck for multi-threading. Using `Pool.starmap` is the easiest way to spawn child processes and collect response.\n  \n\n\n    AI code generation can help you jump start your implementation, but it will most likely be sub-optimal and you need to spend time in optimizing the code by understanding the core logic.\n  \n\n\n    PyPy gives good boost over CPython but compatibility of PyPy with external libraries is a limiting factor.\n  \n\n\n    Mypyc compilation was not any faster than default CPython implementation. Always measure after making the change.\n  \n\n\n    Optimizing for PyPy does not make the implementation any faster in CPython, but optimizing for CPython does make the implementation faster in PyPy.\n  \n\n\nTarget Audience:\n\n    This is a hobby project, but most of the findings and learning can be applied to production projects as well. And given that all optimizations are done on CPython its applicability to production is easy.\n  \nComparison:\n\n    This is the fastest CPython implementation for solving the 1BRC problem. The detailed comparison is provided in the above github repository.\n  \n\n\n\n            Interpreter\n          \n            File\n          \n            Time (sec)\n          \n\n\n\n\n\n\n              Python3\n            \n              py_1brc_final.py\n            \n              24.882\n            \n\n\n              Python3\n            \n              py_1brc_mypyc.py (process_chunk.py precompiled using mypyc)\n            \n              24.441\n            \n\n\n              Python3\n            \ncalculateAverage.py (from https://github.com/ifnesi/1brc)\n            \n              36.303\n            \n\n\n              Python3\n            \ncalculateAveragePyPy.py (from https://github.com/ifnesi/1brc)\n            \n              60.60\n            \n\n\n              Python3\n            \n              doug_booty4.py (from https://github.com/dougmercer-yt/1brc)\n            \n              62.91"
},
{
    "title": "No title",
    "content": "Hey r/Python!\n  \nA while ago, I introduced you to PgQueuer, a Python library designed for handling job queues using Postgres native functionalities. If you've started using PgQueuer, I’m keen to initiate a discussion on your experiences with it.\n  \n\n\n    How and where have you integrated PgQueuer into your projects?\n  \n\n\n    Any difficulties or shortcomings you’ve experienced while using PgQueuer?\n  \n\n\n    Thoughts on the library’s efficiency and features?"
},
{
    "title": "No title",
    "content": "Hi all,\n  \n    I had previously a problem that I wanted to run some long running python scripts without being interrupted by the automatic suspend. I did not find a package that would solve the problem, so I decided to create my own. In the design, I have selected non-disruptive methods which do not rely on mouse movement or pressing a button like F15 or alter system settings. Instead, I've chosen methods that use the APIs and executables meant specifically for the purpose.\n  \n    I've just released wakepy 0.9.0 which supports Windows, macOS, Gnome, KDE and freedesktop.org compliant DEs.\n  \n    GitHub: https://github.com/fohrloop/wakepy\n\n    Comparison to other alternatives: typical other solutions rely on moving the mouse using some library or pressing F15. These might cause problems as your mouse will not be as accurate if it moves randomly, and pressing F15 or other key might have side effects on some systems. Other solutions might also prevent screen lock (e.g. wiggling mouse or pressing a button), but wakepy has a mode for just preventing the automatic sleep, which is better for security and advisable if the display is not required.\n  \n    Hope you like it, and I would be happy to hear your thoughts and answer to any questions!"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Problem Statement\n\n    I recently explored SonarQube for static code analysis. While it’s a great tool, the free edition lacks the ability to generate PDF reports, making it hard to share issues. There was no maintained plugin available, so I decided to solve this problem myself.\n  \nTarget Audience\n\n    This started as a hobby/side project, but I wanted to share it in case others find it useful. I'm open to suggestions and feedback!\n  \nComparison with Similar Tools\n\n    There was only one similar tool in the Sonar Marketplace, but it’s no longer maintained.\n  \nProject Details\n\n    I've developed and published a Python library called RedCoffee, which generates PDF reports from SonarQube analysis. You can find it on PyPi and GitHub.\n  \nLinks:\n\n\n\nPyPi: RedCoffee\n\n\n\nGitHub: RedCoffee Repository\n\n\n\n    Feel free to check it out and let me know your thoughts!"
},
{
    "title": "No title",
    "content": "Hello everyone!\n  What My Project Does\n    I'm excited to share ReqFlow - a Python library designed to make API testing straightforward and efficient. It offers a fluent interface for building and validating HTTP requests, making it a handy tool for small-sized testing frameworks or utilities. While it's still in development and might have some bugs, I would love your feedback and contributions to improve it!\n  Target Audience\n    It would be suitable for beginners due to its reduced entry barrier and also supports advanced use cases with a RestAssured-like approach.\n  Comparison\n    While standard approaches for API testing with Python (e.g., requests) definitely makes sense, ReqFlow provides a more fluent and expressive syntax, making it easier to write and understand tests.\n  \n    Check it out on GitHub and the docs at reqflow.org.\n  \n    All feedback and contributions are welcome! 🙂"
},
{
    "title": "No title",
    "content": "I'm super excited to share NiimPrintX, a desktop app I've been working on for NiimBot label printers. This is my first release, and I am actively working on adding new functionalities.\n  \nWhat My Project Does:NiimPrintX offers both a command line and graphical user interface app to connect with your NiimBot printer. It connects via Bluetooth and makes label printing a breeze. The app is developed completely using Python 3.12 and the Tkinter library for the GUI.\n  \nGitHub Repository: NiimPrintX\n\nTarget Audience:This project is aimed at hobbyists who use NiimBot label printers. It's a proof of concept project for me to learn GUI app development in Python.\n  \nComparison:Currently, there is no desktop app support for NiimBot thermal label printers. Only the official Android/iOS app is available, and it has limited functionality without a paid subscription. NiimPrintX aims to fill this gap by providing a free, more versatile desktop solution.\n  \nSupported Printer Models:\n\n\n\n    D11/B21/B1\n  \n\n\n    D110\n  \n\n\n    B18\n  \n\n\nCool Features:\n\n\n\nBluetooth Auto Discovery: Automatically finds your printer using its model name.\n  \n\n\nEasy Label Design: Create labels with a simple and intuitive GUI.\n  \n\n\nPredefined Icons: Spice up your labels with built-in icons.\n  \n\n\nCross-Platform: Works on Mac, Windows, and Linux.\n  \n\n\nAdvanced Print Options: Includes calibration features for perfect prints.\n  \n\n\nComing Soon:\n\n\n\nBarcode Creation: Make your own barcodes right in the app.\n  \n\n\nQR Code Printing: Generate and print QR codes.\n  \n\n\nBetter Object Alignment: More shapes and borders for your designs.\n  \n\n\n    I'm constantly working on adding new features, so keep an eye out for updates!\n  \n    Check out the GitHub repo for more info and installation instructions: NiimPrintX\n\n    I'd love to hear what you think! Drop a comment or open an issue on GitHub with any feedback or suggestions."
},
{
    "title": "No title",
    "content": "https://github.com/radumarias/zeroize-python\n\n\n\nWhat My Project Does: Clear secrets from memory. Built on stable Rust primitives which guarantee memory is zeroed using an operation will not be 'optimized away' by the compiler.\n  \n\n\nTarget Audience it can be used in production, it's just a simple wrapper over zeroize crate from Rust\n  \n\n\nComparison Personally I didn't found an easy and safe solution in Python to do this, hence I created this lib"
},
{
    "title": "No title",
    "content": "https://github.com/radumarias/rencrypt-python\n\n\n\nWhat My Project Does: A Python encryption library implemented in Rust. It supports AEAD with AES-GCM and ChaCha20Poly1305. It uses ring to handle encryption. If offers slightly higher speed compared to other Python libs, especially for small chunks of data.\n  \n\n\nTarget Audience This lib hasn't been audited, but it mostly wraps ring crate which is a well known library, so in principle it should offer as similar level of security. This is still under development. Please do not use it with sensitive data just yet.\n  \n\n\nComparison If offers slightly higher speed compared to other Python libs, especially for small chunks of data. I compared it to PyFLocker,  cryptography, NaCl (libsodium), PyCryptodome. The API also tries to be easy to use but it's more optimized for speed than usability."
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    Allows you to have a voice-to-voice interaction with an LLM, similar to the ChatGPT app, except with all inference running locally. You can choose from a few different open-weight models.\n  \nVideo running Phi-2 model on a MacBook Air with 8GB RAM, all CPU\n\nTarget Audience\n\n    Devs looking to experiment with integrating on-device AI into their software.\n  \nComparison\n\n\n\nJARVIS - an all API-based solution using DeepGram, OpenAI and ElevenLabs\n  \n\n\nLocal Talking LLM - a higher-latency, more resource intensive local approach using Whisper, Llama and Bark, but with no wake word.\n  \n\n\n    Source code: https://github.com/Picovoice/pico-cookbook/tree/main/recipes/llm-voice-assistant/python"
},
{
    "title": "No title",
    "content": "WHAT MY PROJECT DOES: Solves basic arithmetic problems in an interactive way in python.\n  \n    TARGET AUDIENCE: Anyone, it's just a program to get practice using loops, lists, and functions.\n  \n    COMPARISON: This program functions as a calculator without the use of the eval() function to make everything superfluously easy. It's not perfect and my next version is gonna try and address queries with parenthesis and multiple operators!\n  \n    See the below link for github: https://github.com/Zorgon589/Calculator/tree/main"
},
{
    "title": "No title",
    "content": "REncrypt\n\n\n\nWhat My Project Does A Python encryption library implemented in Rust. It supports AEAD with AES-GCM and ChaCha20Poly1305. It uses ring to handle encryption. If offers slightly higher speed compared to other Python libs, especially for small chunks of data. The API also tries to be easy to use but it's more optimized for speed than usability. So if you are open to experiment and want to achieve the highest possible encryption speed, consider giving it a try.\n  \n\n\nTarget Audience This is just a toy project as a learning experience\n  \n\n\nComparison This is slightly faster than PyFLocker which from my benchmarks is the faster among other Python libs like cryptography, NaCl (libsodium), PyCryptodome"
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    An open-source RAG (Retrieval-Augmented Generation) engine based on deep document understanding. It offers layout recognition, OCR-based chunking templates for data cleasing and provides hallucination-free answers with traceable citations. Compatible with mainstream LLMs.\n  \nTarget Audience\n\n    RAG applications developers.\n  \nComparison\n\n\n\n    It offers various chunking templates for various fils categories, such as resume, legal documents, table, and print copies.\n  \n\n\n    Enables human intervention in chunking, making the data cleansing process no longer a black box.\n  \n\n\n    It not only presents answers but also offers quick views of references and links to the citations when answering to queries.\n  \n\n\nLink: https://github.com/infiniflow/ragflow"
},
{
    "title": "No title",
    "content": "So I'm trying to do a small refresher in design patterns and I reached the Observer pattern.And I'm encounter a circular import error that I'm not sure how to solve.\n  \n    At first I had two files: `observers.py` and `subjects.py`, which each contained the abstract class and some concrete ones.But because each  had to know about the other, I got a circler import error.\n  \n    I tried to put them in the same file, but than the first cant use the second.Also tried to put the Observer in the \"subjects.py\" file, textualy before \"Subject\", that worked, but not clear to me why.I know that in compiled languages, they just use an interface, but we dont have it in Python.Tried to solved it in a various ways, but want to hear others, how you think this can be solved and opinons on this.\n  \n    The base classes are:\n  class Subject(ABC):\n    \n    @abstractmethod\n    def attach(self, observer: Observer) -> None:\n        # The Observer is in the method parameters, so we need to import it\n        pass\n    \nclass Observer(ABC):\n    \n    @abstractmethod\n    def update(self, subject: Subject) -> None:\n        # The Subjectis in the method parameters, so we need to import it\n        pass"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "[https://github.com/radumarias/rencrypt-python](https://github.com/radumarias/rencrypt-python)\n\n    * **What My Project Does**\n  \n    A Python encryption library implemented in Rust. It supports AEAD with AES-GCM and ChaCha20Poly1305. It uses ring to handle encryption.If offers slightly higher speed compared to other Python libs, especially for small chunks of data. The API also tries to be easy to use but it's more optimized for speed than usability.\n  \n    So if you are open to experiment and want to achieve the highest possible encryption speed, consider giving it a try.\n  \n    * **Target Audience**\n  \n    This is just a toy project as a learning experience\n  \n    * **Comparison**\n  \n    This is slightly faster than PyFLocker which from my benchmarks is the faster among other Python libs like cryptography, NaCl (libsodium), PyCryptodome"
},
{
    "title": "No title",
    "content": "What it does:\n\n    It is for building AI agents that perform tasks for you on Android using LLMs. Agents read the screen and perform actions like clicking, typing, and dragging.\n  \n    It includes a test suite of 116 tasks across 20 real-world apps to evaluate agent performance. Think of each task like a unit test, with a setup, evaluation, and tear down procedure. Every task is written in Python. The most powerful agents should be able to pass all of them.\n  \nTarget Audience:\n\n    Anyone looking to experiment with LLM for controlling Android UIs. You can download any app you’d like and test out the default agent, M3A, on it. Just give it a task like “Show my most recent purchases on Amazon.” You can also build your own agent.\n  \nComparison\n\n\n\n    For desktop OSes, there is OSWorld, although it requires costly commercial software (VMWare) to run. AndroidWorld only requires free Android emulator.\n  \n\n\n    While this is OSS and for research, the closest commercial product would be the Rabbit R1. They should test their agent on AndroidWorld to improve accuracy before shipping again :P\n  \n\n\n    Link to repo: https://github.com/google-research/android_world"
},
{
    "title": "No title",
    "content": "Hey everyone,\n  \n    I'm a Rio developer, and I just wanted to say thanks for all the feedback we've received so far! Since our launch, we've implemented a lot of the features you asked for!\n  \n    As requested, we are currently working on an in-depth technical description of Rio, explaining how it works under the hood. So stay tuned!\n  \n    We are looking forward to your feedback, so let us hear from you! :)\n  \nGitHub"
},
{
    "title": "No title",
    "content": "This years SO survey is out now. It includes questions for Python tooling and frameworks. Contribute when you can, it closes soon. It takes ~10 minutes to finish.\n  \n    Link to the survey: https://stackoverflow.az1.qualtrics.com/jfe/form/SV_6rJVT6XXsfTo1JI"
},
{
    "title": "No title",
    "content": "What my library does\n\n    You can easily and powerfully perform caching and memoizing operations in your Python projects using my library. This library is written in Rust, which makes its performance very fast and efficient. By using this library, you can use 7 different caching algorithms that allow you to choose the best algorithm based on your needs.\n  \n    One prominent feature of this library is its simplicity to work with. You just need to import the library into your project and then behave with it like a dictionary.\n  \n    Therefore, if you are looking for a powerful, fast, and simple library for caching and memoizing in Python, my library will be responsive to your needs. By using this library, you can improve the performance of your program and significantly reduce the execution time of your Python code.\n  \nTarget Audience\n\n    For anyone who needs caching and values speed\n  \nComparison\n\n    When compared to other caching libraries:\n  \n\n\n    It's very faster than others (about 5-20x)\n  \n\n\n    It's very simple and easy to use\n  \n\n\n    It's completely thread-safe (uses RwLock)\n  \n\n\n    It uses lower memory than others\n  \n\n\n    You can see benchmark here: https://github.com/awolverp/cachebox-benchmark\n\nMore Info\n\n    My project github: https://github.com/awolverp/cachebox"
},
{
    "title": "No title",
    "content": "What My Project Does: PyODMongo is a modern Python library that serves as a robust Object-Document Mapper (ODM) and seamlessly bridges the gap between Python and MongoDB. It offers an intuitive and efficient way to interact with documents.\n  \n\n\n    Built on top of Pydantic V2, PyODMongo ensures that documents in the database rigorously represent the structure of Python objects. This means that documents are saved and retrieved from the database exactly as a Python object is structured, regardless of how nested the objects are and whether they are stored persistently or by reference. PyODMongo can automatically populate these documents.\n  \n\n\n    Target Audience: Backend developers who want a simple and efficient way to work with MongoDB\n  \n\n\n    Comparison: ODMantic ODM\n  \n\n\nGitHub repository PyPi"
},
{
    "title": "No title",
    "content": "Streamlit is becoming an increasingly a popular framework for data visualization prototyping with Python. The Streamlit framework saves time, effort, and reduces the complexity traditionally associated with crafting maps and charts.Particularly if we approach application development with a modular approach.\n  \n    Starting simple, let’s put together 4 specific examples that leverage Streamlit for interactive data visualization:\n  \n\n\n    A global choropleth map for a dataset for a specific year.\n  \n\n\n    An animated global choropleth map for a dataset across a number of years\n  \n\n\n    An animated choropleth map for a specific region\n  \n\n\n    A line chart to provide an alternative representation of the data\n  \n\n\n    Link to tutorial HERE"
},
{
    "title": "No title",
    "content": "What My Project DoesThis is a small Python script that runs inside a Google Sheet by way of the Python add-on. It uses the reddit api to fetch posts from Ask Reddit twice daily. For posts with enough upvotes, it uses the OpenAI API to summarize an answer to the question based on the comments. I then inserts any new questions and their answers into the spreadsheet and uses the Twitter APIto also post the answer to Twitter I mean X. Should be interesting to anybody looking to connect (a subset) of those APIs.\n  \nTarget AudienceAnybody who is looking to mash-up different APIs (Python is great at this and I feel like it is getting a little harder to do this every year).\n  \nComparisonI'm not aware of any Python code that does this. Even finding a good example of the V2 twitter API is harder than it seems. To accomplish some of this, you could try to ask ChatGPT directly to summarize the answers for a url but when I tried it said it couldn't access Reddit.\n  \nResource\n\n\n\n    The spreadsheet where this happens\n\n\n\n    The twitter bot in action\n  \n\n\n    The source code (or make a copy of the spreadsheet to see)"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What My Project Does:\n\n    I finally got some time to attempt the 1 Billion Row Challenge (1BRC) (https://www.morling.dev/blog/one-billion-row-challenge/) where we are supposed to process a file with 1 billion records of temperature values for cities and print a sorted list with min, max and mean temperature per city.\n  \n    I am a sucker for optimization. So when I heard about 1BRC I got intrigued and in the last few days started experimenting with python implementations. I achieved my goal and implemented the fastest implementation running on CPython, without any external libraries. My motivation for CPython was so that I can apply any of the learning's in my day to day work, as I don't see us moving to PyPy any time sooner.\n  \n    You can check out the performance numbers and implementation at : https://github.com/pappuks/1brc\n\n    Few learning's:\n  \n\n\n    Python Multiprocessing is very powerful in enabling multi core processing and overcoming GIL bottleneck for multi-threading. Using `Pool.starmap` is the easiest way to spawn child processes and collect response.\n  \n\n\n    AI code generation can help you jump start your implementation, but it will most likely be sub-optimal and you need to spend time in optimizing the code by understanding the core logic.\n  \n\n\n    PyPy gives good boost over CPython but compatibility of PyPy with external libraries is a limiting factor.\n  \n\n\n    Mypyc compilation was not any faster than default CPython implementation. Always measure after making the change.\n  \n\n\n    Optimizing for PyPy does not make the implementation any faster in CPython, but optimizing for CPython does make the implementation faster in PyPy.\n  \n\n\nTarget Audience:\n\n    This is a hobby project, but most of the findings and learning can be applied to production projects as well. And given that all optimizations are done on CPython its applicability to production is easy.\n  \nComparison:\n\n    This is the fastest CPython implementation for solving the 1BRC problem. The detailed comparison is provided in the above github repository.\n  \n\n\n\n            Interpreter\n          \n            File\n          \n            Time (sec)\n          \n\n\n\n\n\n\n              Python3\n            \n              py_1brc_final.py\n            \n              24.882\n            \n\n\n              Python3\n            \n              py_1brc_mypyc.py (process_chunk.py precompiled using mypyc)\n            \n              24.441\n            \n\n\n              Python3\n            \ncalculateAverage.py (from https://github.com/ifnesi/1brc)\n            \n              36.303\n            \n\n\n              Python3\n            \ncalculateAveragePyPy.py (from https://github.com/ifnesi/1brc)\n            \n              60.60\n            \n\n\n              Python3\n            \n              doug_booty4.py (from https://github.com/dougmercer-yt/1brc)\n            \n              62.91"
},
{
    "title": "No title",
    "content": "Hey r/Python!\n  \nA while ago, I introduced you to PgQueuer, a Python library designed for handling job queues using Postgres native functionalities. If you've started using PgQueuer, I’m keen to initiate a discussion on your experiences with it.\n  \n\n\n    How and where have you integrated PgQueuer into your projects?\n  \n\n\n    Any difficulties or shortcomings you’ve experienced while using PgQueuer?\n  \n\n\n    Thoughts on the library’s efficiency and features?"
},
{
    "title": "No title",
    "content": "Hi all,\n  \n    I had previously a problem that I wanted to run some long running python scripts without being interrupted by the automatic suspend. I did not find a package that would solve the problem, so I decided to create my own. In the design, I have selected non-disruptive methods which do not rely on mouse movement or pressing a button like F15 or alter system settings. Instead, I've chosen methods that use the APIs and executables meant specifically for the purpose.\n  \n    I've just released wakepy 0.9.0 which supports Windows, macOS, Gnome, KDE and freedesktop.org compliant DEs.\n  \n    GitHub: https://github.com/fohrloop/wakepy\n\n    Comparison to other alternatives: typical other solutions rely on moving the mouse using some library or pressing F15. These might cause problems as your mouse will not be as accurate if it moves randomly, and pressing F15 or other key might have side effects on some systems. Other solutions might also prevent screen lock (e.g. wiggling mouse or pressing a button), but wakepy has a mode for just preventing the automatic sleep, which is better for security and advisable if the display is not required.\n  \n    Hope you like it, and I would be happy to hear your thoughts and answer to any questions!"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Problem Statement\n\n    I recently explored SonarQube for static code analysis. While it’s a great tool, the free edition lacks the ability to generate PDF reports, making it hard to share issues. There was no maintained plugin available, so I decided to solve this problem myself.\n  \nTarget Audience\n\n    This started as a hobby/side project, but I wanted to share it in case others find it useful. I'm open to suggestions and feedback!\n  \nComparison with Similar Tools\n\n    There was only one similar tool in the Sonar Marketplace, but it’s no longer maintained.\n  \nProject Details\n\n    I've developed and published a Python library called RedCoffee, which generates PDF reports from SonarQube analysis. You can find it on PyPi and GitHub.\n  \nLinks:\n\n\n\nPyPi: RedCoffee\n\n\n\nGitHub: RedCoffee Repository\n\n\n\n    Feel free to check it out and let me know your thoughts!"
},
{
    "title": "No title",
    "content": "Hello everyone!\n  What My Project Does\n    I'm excited to share ReqFlow - a Python library designed to make API testing straightforward and efficient. It offers a fluent interface for building and validating HTTP requests, making it a handy tool for small-sized testing frameworks or utilities. While it's still in development and might have some bugs, I would love your feedback and contributions to improve it!\n  Target Audience\n    It would be suitable for beginners due to its reduced entry barrier and also supports advanced use cases with a RestAssured-like approach.\n  Comparison\n    While standard approaches for API testing with Python (e.g., requests) definitely makes sense, ReqFlow provides a more fluent and expressive syntax, making it easier to write and understand tests.\n  \n    Check it out on GitHub and the docs at reqflow.org.\n  \n    All feedback and contributions are welcome! 🙂"
},
{
    "title": "No title",
    "content": "I'm super excited to share NiimPrintX, a desktop app I've been working on for NiimBot label printers. This is my first release, and I am actively working on adding new functionalities.\n  \nWhat My Project Does:NiimPrintX offers both a command line and graphical user interface app to connect with your NiimBot printer. It connects via Bluetooth and makes label printing a breeze. The app is developed completely using Python 3.12 and the Tkinter library for the GUI.\n  \nGitHub Repository: NiimPrintX\n\nTarget Audience:This project is aimed at hobbyists who use NiimBot label printers. It's a proof of concept project for me to learn GUI app development in Python.\n  \nComparison:Currently, there is no desktop app support for NiimBot thermal label printers. Only the official Android/iOS app is available, and it has limited functionality without a paid subscription. NiimPrintX aims to fill this gap by providing a free, more versatile desktop solution.\n  \nSupported Printer Models:\n\n\n\n    D11/B21/B1\n  \n\n\n    D110\n  \n\n\n    B18\n  \n\n\nCool Features:\n\n\n\nBluetooth Auto Discovery: Automatically finds your printer using its model name.\n  \n\n\nEasy Label Design: Create labels with a simple and intuitive GUI.\n  \n\n\nPredefined Icons: Spice up your labels with built-in icons.\n  \n\n\nCross-Platform: Works on Mac, Windows, and Linux.\n  \n\n\nAdvanced Print Options: Includes calibration features for perfect prints.\n  \n\n\nComing Soon:\n\n\n\nBarcode Creation: Make your own barcodes right in the app.\n  \n\n\nQR Code Printing: Generate and print QR codes.\n  \n\n\nBetter Object Alignment: More shapes and borders for your designs.\n  \n\n\n    I'm constantly working on adding new features, so keep an eye out for updates!\n  \n    Check out the GitHub repo for more info and installation instructions: NiimPrintX\n\n    I'd love to hear what you think! Drop a comment or open an issue on GitHub with any feedback or suggestions."
},
{
    "title": "No title",
    "content": "https://github.com/radumarias/zeroize-python\n\n\n\nWhat My Project Does: Clear secrets from memory. Built on stable Rust primitives which guarantee memory is zeroed using an operation will not be 'optimized away' by the compiler.\n  \n\n\nTarget Audience it can be used in production, it's just a simple wrapper over zeroize crate from Rust\n  \n\n\nComparison Personally I didn't found an easy and safe solution in Python to do this, hence I created this lib"
},
{
    "title": "No title",
    "content": "https://github.com/radumarias/rencrypt-python\n\n\n\nWhat My Project Does: A Python encryption library implemented in Rust. It supports AEAD with AES-GCM and ChaCha20Poly1305. It uses ring to handle encryption. If offers slightly higher speed compared to other Python libs, especially for small chunks of data.\n  \n\n\nTarget Audience This lib hasn't been audited, but it mostly wraps ring crate which is a well known library, so in principle it should offer as similar level of security. This is still under development. Please do not use it with sensitive data just yet.\n  \n\n\nComparison If offers slightly higher speed compared to other Python libs, especially for small chunks of data. I compared it to PyFLocker,  cryptography, NaCl (libsodium), PyCryptodome. The API also tries to be easy to use but it's more optimized for speed than usability."
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    Allows you to have a voice-to-voice interaction with an LLM, similar to the ChatGPT app, except with all inference running locally. You can choose from a few different open-weight models.\n  \nVideo running Phi-2 model on a MacBook Air with 8GB RAM, all CPU\n\nTarget Audience\n\n    Devs looking to experiment with integrating on-device AI into their software.\n  \nComparison\n\n\n\nJARVIS - an all API-based solution using DeepGram, OpenAI and ElevenLabs\n  \n\n\nLocal Talking LLM - a higher-latency, more resource intensive local approach using Whisper, Llama and Bark, but with no wake word.\n  \n\n\n    Source code: https://github.com/Picovoice/pico-cookbook/tree/main/recipes/llm-voice-assistant/python"
},
{
    "title": "No title",
    "content": "WHAT MY PROJECT DOES: Solves basic arithmetic problems in an interactive way in python.\n  \n    TARGET AUDIENCE: Anyone, it's just a program to get practice using loops, lists, and functions.\n  \n    COMPARISON: This program functions as a calculator without the use of the eval() function to make everything superfluously easy. It's not perfect and my next version is gonna try and address queries with parenthesis and multiple operators!\n  \n    See the below link for github: https://github.com/Zorgon589/Calculator/tree/main"
},
{
    "title": "No title",
    "content": "REncrypt\n\n\n\nWhat My Project Does A Python encryption library implemented in Rust. It supports AEAD with AES-GCM and ChaCha20Poly1305. It uses ring to handle encryption. If offers slightly higher speed compared to other Python libs, especially for small chunks of data. The API also tries to be easy to use but it's more optimized for speed than usability. So if you are open to experiment and want to achieve the highest possible encryption speed, consider giving it a try.\n  \n\n\nTarget Audience This is just a toy project as a learning experience\n  \n\n\nComparison This is slightly faster than PyFLocker which from my benchmarks is the faster among other Python libs like cryptography, NaCl (libsodium), PyCryptodome"
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    An open-source RAG (Retrieval-Augmented Generation) engine based on deep document understanding. It offers layout recognition, OCR-based chunking templates for data cleasing and provides hallucination-free answers with traceable citations. Compatible with mainstream LLMs.\n  \nTarget Audience\n\n    RAG applications developers.\n  \nComparison\n\n\n\n    It offers various chunking templates for various fils categories, such as resume, legal documents, table, and print copies.\n  \n\n\n    Enables human intervention in chunking, making the data cleansing process no longer a black box.\n  \n\n\n    It not only presents answers but also offers quick views of references and links to the citations when answering to queries.\n  \n\n\nLink: https://github.com/infiniflow/ragflow"
},
{
    "title": "No title",
    "content": "So I'm trying to do a small refresher in design patterns and I reached the Observer pattern.And I'm encounter a circular import error that I'm not sure how to solve.\n  \n    At first I had two files: `observers.py` and `subjects.py`, which each contained the abstract class and some concrete ones.But because each  had to know about the other, I got a circler import error.\n  \n    I tried to put them in the same file, but than the first cant use the second.Also tried to put the Observer in the \"subjects.py\" file, textualy before \"Subject\", that worked, but not clear to me why.I know that in compiled languages, they just use an interface, but we dont have it in Python.Tried to solved it in a various ways, but want to hear others, how you think this can be solved and opinons on this.\n  \n    The base classes are:\n  class Subject(ABC):\n    \n    @abstractmethod\n    def attach(self, observer: Observer) -> None:\n        # The Observer is in the method parameters, so we need to import it\n        pass\n    \nclass Observer(ABC):\n    \n    @abstractmethod\n    def update(self, subject: Subject) -> None:\n        # The Subjectis in the method parameters, so we need to import it\n        pass"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "[https://github.com/radumarias/rencrypt-python](https://github.com/radumarias/rencrypt-python)\n\n    * **What My Project Does**\n  \n    A Python encryption library implemented in Rust. It supports AEAD with AES-GCM and ChaCha20Poly1305. It uses ring to handle encryption.If offers slightly higher speed compared to other Python libs, especially for small chunks of data. The API also tries to be easy to use but it's more optimized for speed than usability.\n  \n    So if you are open to experiment and want to achieve the highest possible encryption speed, consider giving it a try.\n  \n    * **Target Audience**\n  \n    This is just a toy project as a learning experience\n  \n    * **Comparison**\n  \n    This is slightly faster than PyFLocker which from my benchmarks is the faster among other Python libs like cryptography, NaCl (libsodium), PyCryptodome"
},
{
    "title": "No title",
    "content": "What it does:\n\n    It is for building AI agents that perform tasks for you on Android using LLMs. Agents read the screen and perform actions like clicking, typing, and dragging.\n  \n    It includes a test suite of 116 tasks across 20 real-world apps to evaluate agent performance. Think of each task like a unit test, with a setup, evaluation, and tear down procedure. Every task is written in Python. The most powerful agents should be able to pass all of them.\n  \nTarget Audience:\n\n    Anyone looking to experiment with LLM for controlling Android UIs. You can download any app you’d like and test out the default agent, M3A, on it. Just give it a task like “Show my most recent purchases on Amazon.” You can also build your own agent.\n  \nComparison\n\n\n\n    For desktop OSes, there is OSWorld, although it requires costly commercial software (VMWare) to run. AndroidWorld only requires free Android emulator.\n  \n\n\n    While this is OSS and for research, the closest commercial product would be the Rabbit R1. They should test their agent on AndroidWorld to improve accuracy before shipping again :P\n  \n\n\n    Link to repo: https://github.com/google-research/android_world"
},
{
    "title": "No title",
    "content": "Hey everyone,\n  \n    I'm a Rio developer, and I just wanted to say thanks for all the feedback we've received so far! Since our launch, we've implemented a lot of the features you asked for!\n  \n    As requested, we are currently working on an in-depth technical description of Rio, explaining how it works under the hood. So stay tuned!\n  \n    We are looking forward to your feedback, so let us hear from you! :)\n  \nGitHub"
},
{
    "title": "No title",
    "content": "This years SO survey is out now. It includes questions for Python tooling and frameworks. Contribute when you can, it closes soon. It takes ~10 minutes to finish.\n  \n    Link to the survey: https://stackoverflow.az1.qualtrics.com/jfe/form/SV_6rJVT6XXsfTo1JI"
},
{
    "title": "No title",
    "content": "What my library does\n\n    You can easily and powerfully perform caching and memoizing operations in your Python projects using my library. This library is written in Rust, which makes its performance very fast and efficient. By using this library, you can use 7 different caching algorithms that allow you to choose the best algorithm based on your needs.\n  \n    One prominent feature of this library is its simplicity to work with. You just need to import the library into your project and then behave with it like a dictionary.\n  \n    Therefore, if you are looking for a powerful, fast, and simple library for caching and memoizing in Python, my library will be responsive to your needs. By using this library, you can improve the performance of your program and significantly reduce the execution time of your Python code.\n  \nTarget Audience\n\n    For anyone who needs caching and values speed\n  \nComparison\n\n    When compared to other caching libraries:\n  \n\n\n    It's very faster than others (about 5-20x)\n  \n\n\n    It's very simple and easy to use\n  \n\n\n    It's completely thread-safe (uses RwLock)\n  \n\n\n    It uses lower memory than others\n  \n\n\n    You can see benchmark here: https://github.com/awolverp/cachebox-benchmark\n\nMore Info\n\n    My project github: https://github.com/awolverp/cachebox"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hey all, we will close the Call for Proposals portal this Sunday, June 2, for our PyData Amsterdam 2024 Conference which will take place on September 18-20 in Amsterdam. We are looking for presentations that can captivate our audience, provide invaluable insights, and foster community learning. Don't miss this chance to speak on stage in front of over 800 attendees in the field of Data & AI. Submit a talk here > https://amsterdam2024.pydata.org/cfp/cfp"
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    Zango, built on top of Django, is further opinionated towards building enterprise ready custom business apps. Includes additional batteries for out of the box enterprise readiness and rapid app development. Growing ecosystem of packages that serves as building blocks of apps.\n  \n    Zango also enables multi-tenancy where each tenant, representing an app/microservices, can be deployed independently on the same underlying monolith. Tenants have logically seperated db, codebase as well as deployment. This significantly cuts down per app hosting cost and enables microservices pattern without the cost overhead.\n  \nTarget Audience\n\n    Enterprises: Benefits from the open core concept. No vendor lock-ins. Rapid development with out-of-the-box enterprise readiness.\n  \n    Startups: Get productive from day-1. Leverage packages to reach MVP really fast and not be constrained by limit on customizability (as with low-code/no-code solutions). Lowest cost of hosting if you have multiple apps or building microservices.\n  \n    Consulting/ Development companies: Increase development efficiency and optimize on hosting cost.\n  \n    You: If you are looking to develop any bespoke app, give it a try :)\n  \nComparison\n\n\n\n    Web dev frameworks(e.g. Django): Not opinionated for enterprise readiness/ business apps. Zango enables faster development, lower opex and and built-in compliance and enterprise readiness\n  \n\n\n    Proprietary platforms (e.g. Salesforce): No vendor lock-in. Faster development\n  \n\n\n    Low-Code / No-Code: Limited customizability.\n  \n\n\nMore Info\n\n    Know more at the project's Github repo:  https://github.com/Healthlane-Technologies/Zango"
},
{
    "title": "No title",
    "content": "What the Project Does\n\n    The idea is to provide an easy to use (and fully typed, including camera settings!) abstraction around libgphoto2, allowing even non-tech-savy users to write Python scripts/sequences to take pictures. Generally, it supports all cameras that libgphoto2 also supports!Possible use cases are:Source code/examples available here (this one can be used to automatically take an image once a lightning strike is detected): https://github.com/Zahlii/pyDSLR/blob/main/examples/lightning_trigger.py\n\n\n\n    Lightning trigger (showcased)\n  \n\n\n    Bulb capture (showcased)\n  \n\n\n    High Speed capture (e.g. using computer vision to detect animals and use the camera as part of a wildlife trap, partly showcased)\n  \n\n\n    Photo booths\n  \n\n\n    Timelapses (also for cameras that don't naturally support them)\n  \n\n\n    Focus bracketing (also for cameras that don't natively support them)\n  \n\n\n    Astro stacking (Taking hundreds of long exposures with fixed settings after another)\n  \n\n\n    With a computer-controllable astro mount we could also track the camera based on preview images\n  \n\n\nTarget Audience\n\n    For now, mainly Python hobby photographers, but in the future hopefully also less tech savy hobbysts.\n  \n    Right now it is obviously still a work in progress (with only types available for my Canon R6II), and I am inviting people to reach out to me if they are interested in participating or have cameras to add to our types :)\n  \nComparison with Other Libraries\n\n    When compared to other library around it:\n  \n\n\n    We wrap python-gphoto2's low level API\n  \n\n\n    gphoto2-cffi is an alternative, but not maintained in 7 years, lacks typing support and doesn't provide much benefits over existing low-level APIs"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "The Python on Microcontrollers (and Raspberry Pi) Newsletter: subscribe for free\n    With the Python on Microcontrollers newsletter, you get all the latest information on Python running on hardware in one place! MicroPython, CircuitPython and Python on single Board Computers like Raspberry Pi & many more.\n  \n    The Python on Microcontrollers newsletter is the place for the latest news. It arrives Monday morning with all the week’s happenings. No advertising, no spam, easy to unsubscribe.\n  \n    11,019 subscribers - the largest Python on hardware newsletter out there.\n  \n    Catch all the weekly news on Python for Microcontrollers with adafruitdaily.com.\n  \n    This ad-free, spam-free weekly email is filled with CircuitPython, MicroPython, and Python information that you may have missed, all in one place!\n  \n    Ensure you catch the weekly Python on Hardware roundup – you can cancel anytime . Try our spam-free newsletter today!\n  \nhttps://www.adafruitdaily.com/"
},
{
    "title": "No title",
    "content": "Poetry plugin to generate Dockerfile and images automatically\n  \n    This project lets you generate a docker image or just a Dockerfile for your poetry application without manual setup\n  \n    It is meant for production images.\n  \nhttps://github.com/nicoloboschi/poetry-dockerize-plugin\n\nhttps://pypi.org/project/poetry-dockerize-plugin/\n\n    Get started with\n  poetry self add poetry-dockerize-plugin@latest\n    This command generates a production-ready, optimized python image:\n  poetry dockerize\n    or to generate a Dockerfile\n  poetry dockerize --generate"
},
{
    "title": "No title",
    "content": "ObjectBox (GitHub) is an embedded database for Python objects and high-dimensional vectors. Today is it's first stable release for Python developers. It's very lightweight similar to SQLite, but built for objects so it's faster as there's no SQL layer in-between. It's the very first vector database that also runs on smaller low-memory devices. The article comes with first benchmarks and hints at the LangChain integration."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "https://github.com/must108/musicnotes\nWhat My Project Does\nmusicnotes is a small open-source project that lets you play musical instruments (currently, only piano and guitar) in your Python programs. I created this project as I wanted to create a simple and useful open-source project for beginner developers to easily contribute to the project. I know it's hard to find good open-source projects for new developers.\n  Target Audience\n    Developers looking to add sounds to small games, or just have fun while learning Python in general. This library could also be used to teach Python and coding in a fun way.\n  \n    This project was also made to allow new Python developers to easily contribute to open-source! Feel free to star the repository, and download with pip install musicnotes! You can also create a pull request with any changes you find useful, and visit the GitHub repository if you find any setbacks while using this module. There are a few things that can be worked on listed in the README of the repository if you're looking for a place to get started.\n  Comparison\n    This project is very simple and easy to use, and is easy to contribute to as well, which is one of the primary goals of the project."
},
{
    "title": "No title",
    "content": "I saw the words 'visual effects', just give me GIFs\n\n    Understandable, visit the Effects Showroom first. Then come back if you like what you see.\n  \nWhat My Project Does\n\n    TerminalTextEffects (TTE) is a terminal visual effects engine. TTE can be installed as a system application to produce effects in your terminal, or as a Python library to enable effects within your Python scripts/applications. TTE includes a growing library of built-in effects which showcase the engine's features.\n  \n    Use cases:\n  \n\n\n    Invoke at terminal launch to produce an animation (ex: fetch).\n  \n\n\n    Alias system commands to animate output.\n  \n\n\n    Invoke on SSH session to blow people's minds when they log in.\n  \n\n\n    Use in your project to produce animated prompts, logos, etc.\n  \n\n\nTarget Audience\n\n    TTE is a terminal toy (and now a Python library) that anybody can use to add visual flair to their terminal or projects. It works best in Linux but is functional in the new Windows Terminal.\n  \n    Every effect allows for significant customization including color gradient stops and directions as well as many effect-specific options. Customization is exposed via command-line arguments and through the Config class interface. The effect examples shown in the documentation represent a single configuration. Your experience can be very different with a little tweaking to match your system theme and preferences.\n  \nComparison\n\n    I don't know of any other projects like TTE. It's a completely useless and over-engineered side-project that's turned into a whole thing. Have fun.\n  \nMore Info\n\n    The GitHub README has some effect examples, installation instructions and some basic quick-start info."
},
{
    "title": "No title",
    "content": "Been working on a python tool for VS Code. Curious to get peoples' opinion on how they run python files (not notebooks) within VS Code. Do you typically run files python by:\n  \n\n\n    Typing the python command into the integrated terminal\n  \n\n\n    Clicking the run button at the top of the file\n  \n\n\n    Pressing F5 for debugging\n  \n\n\n    Pressing Ctrl+F5 for run but not debug\n  \n\n\n    Creating a custom keyboard shortcut\n  \n\n\n    Other\n  \n\n\n    Let me know your thoughts, I appreciate the insights!"
},
{
    "title": "No title",
    "content": "At work I'm getting the question to build a platform that will be used to navigate all kinds of business metrics to different levels of granularity. Ideally there is also insights on their relationships, and advice on actions to take.\n  \n    I have experience with both Dash and React, and my feeling says to go with React (and a python backend). Mostly because I foresee this application to grow over time, and managing big Dash applications (as a dev) can get clunky.\n  \n    However, in my team there's no extra JavaScript (let alone React) knowledge. While there is a solid base for Python. There might be opportunities to source outside the team/company, which I'd have to make a strong case for."
},
{
    "title": "No title",
    "content": "Have you ever faced a moment when your code is a mess of nested classes and functions, and you have to dig through dozens of levels to understand a simple function?\n  \n    Gloe (pronounced like “glow”) is a library designed to assist you organize your code into a type-safe flow, making it flat and linear.\n  \nWhat My Project Does\n\n    Here’s what it can do for you:\n  \n\n\n    Write type-safe pipelines with pure Python.\n  \n\n\n    Express your code as a set of atomic and extensible units of responsibility called transformers.\n  \n\n\n    Validate the input and output of transformers, and changes between them during execution.\n  \n\n\nMix sync and async code without worrying about its concurrent nature.\n  \n\n\n    Keep your code readable and maintainable, even for complex flows.\n  \n\n\nVisualize you pipelines and the data flowing through them.\n  \n\n\n    Use it anywhere without changing your existing workflow.\n  \n\n\nTarget Audience: any Python developer who sees their code as a flow (a series of sequential operations) and wants to improve its readability and maintainability. It's production-ready!\n  \nComparison: Currently, unlike platforms like Air Flow that include scheduler backends for task orchestration, Gloe’s primary purpose is to aid in development. The graph structure aims to make the code more flat and readable.\n  \nExample of usage in a server:\nsend_promotion = (\n    get_users >> (\n        filter_basic_subscription >> send_basic_subscription_promotion_email,\n        filter_premium_subscription >> send_premium_subscription_promotion_email,\n    ) >>\n    log_emails_result\n)\n\n@users_router.post('/send-promotion/{role}')\ndef send_promotion_emails_route(role: str):\n    return send_promotion(role)\nFull code.\n  \nLinks:github.com/ideos/gloe gloe.ideos.com.br"
},
{
    "title": "No title",
    "content": "What My Project Does:\n  \n    This project aims to provide a simple and efficient way to manage a collection of books through various API endpoints.\n  \n    This API allows you to:\n  \n\n\n    Get a list of all books.\n  \n\n\n    Add a new book.\n  \n\n\n    Get a book by its isbn.\n  \n\n\n    Update an existing book by its isbn.\n  \n\n\n    Delete a book by its isbn.\n  \n\n\n    API Endpoints:\n  \n\n\n    GET /api/v1/books - Retrieve all books.\n  \n\n\n    POST /api/v1/books - Add a new book.\n  \n\n\n    GET /api/v1/books/<ISBN> - Retrieve a book by its ISBN.\n  \n\n\n    PUT /api/v1/books/<ISBN> - Update a book by its ISBN.\n  \n\n\n    DELETE /api/v1/books/<ISBN> - Delete a book by its ISBN.\n  \n\n\n    Target Audience:\n  \n    Anyone who is interested to integrate book management api into their applications.\n  \n    Website API: Book Management API\n\n    GitHub Repo: Book-Management-API on GitHub\n\n    Follow Me:\n  \n\n\n    IG: @nordszamora\n\n\n\n    Threads: @nordszamora\n\n\n\n    Tiktok: @nordszamora\n\n\n\n    Github: @nordszamora"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What it does:Today, I released the first working version of my SH1106 app framework for Raspberry Pi on PyPI! The SH1106 is an affordable OLED screen, costing under $3, and it's perfect for projects of all sizes. This package enables the creation of apps for it with graphics support, state management, image conversion utilities, and custom fonts. Check it out here: SH1106 Framework on PyPI.\n  \nTarget audience:The package is mainly aimed at hobbyists who want to create small projects using the SH1106 OLED without having to manually write a lot of the graphics code typically needed on top of standard packages. I am also developing a hardware synthesizer keyboard from scratch that utilizes this framework extensively. So far, the framework handles the massive scaling required for this project excellently in terms of both code organization and performance.\n  \nComparison:This package offers several advantages over other SH1106 packages:\n  \n\n\nImproved Rendering Speed: It significantly speeds up the rendering time for a given frame by writing all graphical operations to a pixel array, which is then loaded onto the screen using low-level functions from the excellent luma.oled package.\n  \n\n\nEfficient Resource Management: All images and fonts are pre-loaded during the initialization of the framework, reducing the processing time during rendering.\n  \n\n\nState Management: A simple yet effective state management system is implemented, making app creation straightforward from the start.\n  \n\n\n    You can also check out the project on GitHub: SH1106 Framework on GitHub.\n  \n    I'd love to answer any questions you have in the comments! I hope you find some cool uses for it. Cheers! :)"
},
{
    "title": "No title",
    "content": "https://github.com/coryfitz/crowbar\n\nWhat it does:\n\n    I'm working on a way of simplifying your Python dependency management. Basically, it handles virtual environments so you don’t have to think about them.\n  \n    First: pip install crowbar-package-manager\n  \n    Basically you just install and run things with the crowbar command rather than pip: crowbar install package_name\n  \n    And then you also run things with the crowbar command rather than using \"python\" - crowbar then runs the program based on the packages in the local environment rather than having to activate your virtual environment.\n  \n    It's inspired by npm if you've used that with js.\n  \nTarget audience:\n\n    Anyone who currently uses the standard package management tools (requirements.txt, pip, etc) and wants to automate some of those processes.\n  \nComparison:\n\n    The workflow is most similar to Poetry but there are a couple of major differences - for one thing, Crowbar only does package management; it doesn't create a project structure for you. Also, Poetry puts all of your environments in a central repository - Crowbar keeps it in your project folder.\n  \n    Unlike Poetry or any of the other dependency management tools out there, you don't have to buy into a completely different way of structuring your dependencies or your projects. A project that you use Crowbar on is identical to one where you used pip, venv, and requirements.txt - and if you try Crowbar and decide you don't like it, just activate your virtual environment like normal."
},
{
    "title": "No title",
    "content": "When working with Matlab I love how I can run the code step by step to debug it. Even being able to \"step in\" functions and loops.\n  \n    Then, I was looking to an IDE with a similar functionality for Python. Nowadays I'm using Spyder."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    Because spotify made their lyrics menu a premium only feature, I thought I'd make my own replacement for it.The app connects to your spotify account, fetches the lyrics from various websites, and then syncs them automatically to what is currently playing. Basically does the exact same as the lyrics menu used to do.\n  \nTarget Audience\n\n    Anyone who wants to see the lyrics to songs really.\n  \nComparison\n\n    Most other apps that I've found are either browser only, or don't actually sync the lyrics to the song, they just show the entire lyrics at once.In comparison, my app shows the lyrics line by line, synced with the song, and also has (in my opinion lol) a fairly nice looking ui.It's also very easy to use for non programmers too, since you can just download an executable to use the app.\n  \n    It's available for free here https://github.com/Mews/spotify-lyrics"
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    Simplifies the interaction with the ShipEngine API with most response and requests built as objects, which in my opinion makes interaction much easier. This is my first released package so all criticism and feedback is very welcome.\n  \nTarget Audience\n\n    Anyone who deals with the current ShipEngine API using Python.\n  \nComparison\n\n    There is an official ShipEngine API module that is created by the company but I have found it somewhat lack luster with no way to create batches or bulk shipments (and other missing functionality), this is much more suited to accomplishing that task.\n  \nLinks\n\nhttps://github.com/Sen-tio/unofficial-shipengine"
},
{
    "title": "No title",
    "content": "Introducing Xenharmlib (Source code here)\n  \nWhat My Project Does\n\n    (taken from the docs) Xenharmlib is a music theory library for the exploration and research of microtonality, diatonic set theory, non-standard notations, and many more. The library implements a superset of Western classical music theory, so you can also use it to compose and analyze music in the boundaries of the common practice period or 20th century Western music.\n  \nTarget Audience\n\n    Composers who want to get answers to theoretical questions pertaining to structures of musical scales, note intervals, frequencies and frequency ratios in equal division tunings. People who want to explore microtonality or non-western musical theory in general.\n  \nComparison\n\n    * mingus Xenharmlib is pretty much on-par with features in mingus, however extends those features to all sorts of equal division temperaments.* pytuning supports more slightly tuning methods and export formats, however does not support microtonal notation or note / interval calculation* music21 is much more mature in providing an analytical toolset, however supports only traditional western equal temperament"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Good evening! I have created a new projectfor adding events to google calendar based on the text a user inputs.\n  What My Project Does\n    The project is a tool that uses large language models to understand the user's input and add events to the user's Google Calendar based on the user's input. It uses Ollama for natural language understanding and Google Calendar API for adding events to the user's calendar.\n  How My Project Works\n    Ollama uses Llama 3 with pre-instructions to act as a calendar event planner. The tool uses the model to generate responses to extract the event's details from the user's input inserted in the Web Interface. tool then asks the user to confirm the details extracted from the user's input and adds the event to the user's Google Calendar (example shown here)\n  References\n    Checkout my github repository AIPlanner for more details about the project."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hey all, we will close the Call for Proposals portal this Sunday, June 2, for our PyData Amsterdam 2024 Conference which will take place on September 18-20 in Amsterdam. We are looking for presentations that can captivate our audience, provide invaluable insights, and foster community learning. Don't miss this chance to speak on stage in front of over 800 attendees in the field of Data & AI. Submit a talk here > https://amsterdam2024.pydata.org/cfp/cfp"
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    Zango, built on top of Django, is further opinionated towards building enterprise ready custom business apps. Includes additional batteries for out of the box enterprise readiness and rapid app development. Growing ecosystem of packages that serves as building blocks of apps.\n  \n    Zango also enables multi-tenancy where each tenant, representing an app/microservices, can be deployed independently on the same underlying monolith. Tenants have logically seperated db, codebase as well as deployment. This significantly cuts down per app hosting cost and enables microservices pattern without the cost overhead.\n  \nTarget Audience\n\n    Enterprises: Benefits from the open core concept. No vendor lock-ins. Rapid development with out-of-the-box enterprise readiness.\n  \n    Startups: Get productive from day-1. Leverage packages to reach MVP really fast and not be constrained by limit on customizability (as with low-code/no-code solutions). Lowest cost of hosting if you have multiple apps or building microservices.\n  \n    Consulting/ Development companies: Increase development efficiency and optimize on hosting cost.\n  \n    You: If you are looking to develop any bespoke app, give it a try :)\n  \nComparison\n\n\n\n    Web dev frameworks(e.g. Django): Not opinionated for enterprise readiness/ business apps. Zango enables faster development, lower opex and and built-in compliance and enterprise readiness\n  \n\n\n    Proprietary platforms (e.g. Salesforce): No vendor lock-in. Faster development\n  \n\n\n    Low-Code / No-Code: Limited customizability.\n  \n\n\nMore Info\n\n    Know more at the project's Github repo:  https://github.com/Healthlane-Technologies/Zango"
},
{
    "title": "No title",
    "content": "What the Project Does\n\n    The idea is to provide an easy to use (and fully typed, including camera settings!) abstraction around libgphoto2, allowing even non-tech-savy users to write Python scripts/sequences to take pictures. Generally, it supports all cameras that libgphoto2 also supports!Possible use cases are:Source code/examples available here (this one can be used to automatically take an image once a lightning strike is detected): https://github.com/Zahlii/pyDSLR/blob/main/examples/lightning_trigger.py\n\n\n\n    Lightning trigger (showcased)\n  \n\n\n    Bulb capture (showcased)\n  \n\n\n    High Speed capture (e.g. using computer vision to detect animals and use the camera as part of a wildlife trap, partly showcased)\n  \n\n\n    Photo booths\n  \n\n\n    Timelapses (also for cameras that don't naturally support them)\n  \n\n\n    Focus bracketing (also for cameras that don't natively support them)\n  \n\n\n    Astro stacking (Taking hundreds of long exposures with fixed settings after another)\n  \n\n\n    With a computer-controllable astro mount we could also track the camera based on preview images\n  \n\n\nTarget Audience\n\n    For now, mainly Python hobby photographers, but in the future hopefully also less tech savy hobbysts.\n  \n    Right now it is obviously still a work in progress (with only types available for my Canon R6II), and I am inviting people to reach out to me if they are interested in participating or have cameras to add to our types :)\n  \nComparison with Other Libraries\n\n    When compared to other library around it:\n  \n\n\n    We wrap python-gphoto2's low level API\n  \n\n\n    gphoto2-cffi is an alternative, but not maintained in 7 years, lacks typing support and doesn't provide much benefits over existing low-level APIs"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "The Python on Microcontrollers (and Raspberry Pi) Newsletter: subscribe for free\n    With the Python on Microcontrollers newsletter, you get all the latest information on Python running on hardware in one place! MicroPython, CircuitPython and Python on single Board Computers like Raspberry Pi & many more.\n  \n    The Python on Microcontrollers newsletter is the place for the latest news. It arrives Monday morning with all the week’s happenings. No advertising, no spam, easy to unsubscribe.\n  \n    11,019 subscribers - the largest Python on hardware newsletter out there.\n  \n    Catch all the weekly news on Python for Microcontrollers with adafruitdaily.com.\n  \n    This ad-free, spam-free weekly email is filled with CircuitPython, MicroPython, and Python information that you may have missed, all in one place!\n  \n    Ensure you catch the weekly Python on Hardware roundup – you can cancel anytime . Try our spam-free newsletter today!\n  \nhttps://www.adafruitdaily.com/"
},
{
    "title": "No title",
    "content": "Poetry plugin to generate Dockerfile and images automatically\n  \n    This project lets you generate a docker image or just a Dockerfile for your poetry application without manual setup\n  \n    It is meant for production images.\n  \nhttps://github.com/nicoloboschi/poetry-dockerize-plugin\n\nhttps://pypi.org/project/poetry-dockerize-plugin/\n\n    Get started with\n  poetry self add poetry-dockerize-plugin@latest\n    This command generates a production-ready, optimized python image:\n  poetry dockerize\n    or to generate a Dockerfile\n  poetry dockerize --generate"
},
{
    "title": "No title",
    "content": "ObjectBox (GitHub) is an embedded database for Python objects and high-dimensional vectors. Today is it's first stable release for Python developers. It's very lightweight similar to SQLite, but built for objects so it's faster as there's no SQL layer in-between. It's the very first vector database that also runs on smaller low-memory devices. The article comes with first benchmarks and hints at the LangChain integration."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "https://github.com/must108/musicnotes\nWhat My Project Does\nmusicnotes is a small open-source project that lets you play musical instruments (currently, only piano and guitar) in your Python programs. I created this project as I wanted to create a simple and useful open-source project for beginner developers to easily contribute to the project. I know it's hard to find good open-source projects for new developers.\n  Target Audience\n    Developers looking to add sounds to small games, or just have fun while learning Python in general. This library could also be used to teach Python and coding in a fun way.\n  \n    This project was also made to allow new Python developers to easily contribute to open-source! Feel free to star the repository, and download with pip install musicnotes! You can also create a pull request with any changes you find useful, and visit the GitHub repository if you find any setbacks while using this module. There are a few things that can be worked on listed in the README of the repository if you're looking for a place to get started.\n  Comparison\n    This project is very simple and easy to use, and is easy to contribute to as well, which is one of the primary goals of the project."
},
{
    "title": "No title",
    "content": "I saw the words 'visual effects', just give me GIFs\n\n    Understandable, visit the Effects Showroom first. Then come back if you like what you see.\n  \nWhat My Project Does\n\n    TerminalTextEffects (TTE) is a terminal visual effects engine. TTE can be installed as a system application to produce effects in your terminal, or as a Python library to enable effects within your Python scripts/applications. TTE includes a growing library of built-in effects which showcase the engine's features.\n  \n    Use cases:\n  \n\n\n    Invoke at terminal launch to produce an animation (ex: fetch).\n  \n\n\n    Alias system commands to animate output.\n  \n\n\n    Invoke on SSH session to blow people's minds when they log in.\n  \n\n\n    Use in your project to produce animated prompts, logos, etc.\n  \n\n\nTarget Audience\n\n    TTE is a terminal toy (and now a Python library) that anybody can use to add visual flair to their terminal or projects. It works best in Linux but is functional in the new Windows Terminal.\n  \n    Every effect allows for significant customization including color gradient stops and directions as well as many effect-specific options. Customization is exposed via command-line arguments and through the Config class interface. The effect examples shown in the documentation represent a single configuration. Your experience can be very different with a little tweaking to match your system theme and preferences.\n  \nComparison\n\n    I don't know of any other projects like TTE. It's a completely useless and over-engineered side-project that's turned into a whole thing. Have fun.\n  \nMore Info\n\n    The GitHub README has some effect examples, installation instructions and some basic quick-start info."
},
{
    "title": "No title",
    "content": "Been working on a python tool for VS Code. Curious to get peoples' opinion on how they run python files (not notebooks) within VS Code. Do you typically run files python by:\n  \n\n\n    Typing the python command into the integrated terminal\n  \n\n\n    Clicking the run button at the top of the file\n  \n\n\n    Pressing F5 for debugging\n  \n\n\n    Pressing Ctrl+F5 for run but not debug\n  \n\n\n    Creating a custom keyboard shortcut\n  \n\n\n    Other\n  \n\n\n    Let me know your thoughts, I appreciate the insights!"
},
{
    "title": "No title",
    "content": "At work I'm getting the question to build a platform that will be used to navigate all kinds of business metrics to different levels of granularity. Ideally there is also insights on their relationships, and advice on actions to take.\n  \n    I have experience with both Dash and React, and my feeling says to go with React (and a python backend). Mostly because I foresee this application to grow over time, and managing big Dash applications (as a dev) can get clunky.\n  \n    However, in my team there's no extra JavaScript (let alone React) knowledge. While there is a solid base for Python. There might be opportunities to source outside the team/company, which I'd have to make a strong case for."
},
{
    "title": "No title",
    "content": "Have you ever faced a moment when your code is a mess of nested classes and functions, and you have to dig through dozens of levels to understand a simple function?\n  \n    Gloe (pronounced like “glow”) is a library designed to assist you organize your code into a type-safe flow, making it flat and linear.\n  \nWhat My Project Does\n\n    Here’s what it can do for you:\n  \n\n\n    Write type-safe pipelines with pure Python.\n  \n\n\n    Express your code as a set of atomic and extensible units of responsibility called transformers.\n  \n\n\n    Validate the input and output of transformers, and changes between them during execution.\n  \n\n\nMix sync and async code without worrying about its concurrent nature.\n  \n\n\n    Keep your code readable and maintainable, even for complex flows.\n  \n\n\nVisualize you pipelines and the data flowing through them.\n  \n\n\n    Use it anywhere without changing your existing workflow.\n  \n\n\nTarget Audience: any Python developer who sees their code as a flow (a series of sequential operations) and wants to improve its readability and maintainability. It's production-ready!\n  \nComparison: Currently, unlike platforms like Air Flow that include scheduler backends for task orchestration, Gloe’s primary purpose is to aid in development. The graph structure aims to make the code more flat and readable.\n  \nExample of usage in a server:\nsend_promotion = (\n    get_users >> (\n        filter_basic_subscription >> send_basic_subscription_promotion_email,\n        filter_premium_subscription >> send_premium_subscription_promotion_email,\n    ) >>\n    log_emails_result\n)\n\n@users_router.post('/send-promotion/{role}')\ndef send_promotion_emails_route(role: str):\n    return send_promotion(role)\nFull code.\n  \nLinks:github.com/ideos/gloe gloe.ideos.com.br"
},
{
    "title": "No title",
    "content": "What My Project Does:\n  \n    This project aims to provide a simple and efficient way to manage a collection of books through various API endpoints.\n  \n    This API allows you to:\n  \n\n\n    Get a list of all books.\n  \n\n\n    Add a new book.\n  \n\n\n    Get a book by its isbn.\n  \n\n\n    Update an existing book by its isbn.\n  \n\n\n    Delete a book by its isbn.\n  \n\n\n    API Endpoints:\n  \n\n\n    GET /api/v1/books - Retrieve all books.\n  \n\n\n    POST /api/v1/books - Add a new book.\n  \n\n\n    GET /api/v1/books/<ISBN> - Retrieve a book by its ISBN.\n  \n\n\n    PUT /api/v1/books/<ISBN> - Update a book by its ISBN.\n  \n\n\n    DELETE /api/v1/books/<ISBN> - Delete a book by its ISBN.\n  \n\n\n    Target Audience:\n  \n    Anyone who is interested to integrate book management api into their applications.\n  \n    Website API: Book Management API\n\n    GitHub Repo: Book-Management-API on GitHub\n\n    Follow Me:\n  \n\n\n    IG: @nordszamora\n\n\n\n    Threads: @nordszamora\n\n\n\n    Tiktok: @nordszamora\n\n\n\n    Github: @nordszamora"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What it does:Today, I released the first working version of my SH1106 app framework for Raspberry Pi on PyPI! The SH1106 is an affordable OLED screen, costing under $3, and it's perfect for projects of all sizes. This package enables the creation of apps for it with graphics support, state management, image conversion utilities, and custom fonts. Check it out here: SH1106 Framework on PyPI.\n  \nTarget audience:The package is mainly aimed at hobbyists who want to create small projects using the SH1106 OLED without having to manually write a lot of the graphics code typically needed on top of standard packages. I am also developing a hardware synthesizer keyboard from scratch that utilizes this framework extensively. So far, the framework handles the massive scaling required for this project excellently in terms of both code organization and performance.\n  \nComparison:This package offers several advantages over other SH1106 packages:\n  \n\n\nImproved Rendering Speed: It significantly speeds up the rendering time for a given frame by writing all graphical operations to a pixel array, which is then loaded onto the screen using low-level functions from the excellent luma.oled package.\n  \n\n\nEfficient Resource Management: All images and fonts are pre-loaded during the initialization of the framework, reducing the processing time during rendering.\n  \n\n\nState Management: A simple yet effective state management system is implemented, making app creation straightforward from the start.\n  \n\n\n    You can also check out the project on GitHub: SH1106 Framework on GitHub.\n  \n    I'd love to answer any questions you have in the comments! I hope you find some cool uses for it. Cheers! :)"
},
{
    "title": "No title",
    "content": "https://github.com/coryfitz/crowbar\n\nWhat it does:\n\n    I'm working on a way of simplifying your Python dependency management. Basically, it handles virtual environments so you don’t have to think about them.\n  \n    First: pip install crowbar-package-manager\n  \n    Basically you just install and run things with the crowbar command rather than pip: crowbar install package_name\n  \n    And then you also run things with the crowbar command rather than using \"python\" - crowbar then runs the program based on the packages in the local environment rather than having to activate your virtual environment.\n  \n    It's inspired by npm if you've used that with js.\n  \nTarget audience:\n\n    Anyone who currently uses the standard package management tools (requirements.txt, pip, etc) and wants to automate some of those processes.\n  \nComparison:\n\n    The workflow is most similar to Poetry but there are a couple of major differences - for one thing, Crowbar only does package management; it doesn't create a project structure for you. Also, Poetry puts all of your environments in a central repository - Crowbar keeps it in your project folder.\n  \n    Unlike Poetry or any of the other dependency management tools out there, you don't have to buy into a completely different way of structuring your dependencies or your projects. A project that you use Crowbar on is identical to one where you used pip, venv, and requirements.txt - and if you try Crowbar and decide you don't like it, just activate your virtual environment like normal."
},
{
    "title": "No title",
    "content": "When working with Matlab I love how I can run the code step by step to debug it. Even being able to \"step in\" functions and loops.\n  \n    Then, I was looking to an IDE with a similar functionality for Python. Nowadays I'm using Spyder."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    Because spotify made their lyrics menu a premium only feature, I thought I'd make my own replacement for it.The app connects to your spotify account, fetches the lyrics from various websites, and then syncs them automatically to what is currently playing. Basically does the exact same as the lyrics menu used to do.\n  \nTarget Audience\n\n    Anyone who wants to see the lyrics to songs really.\n  \nComparison\n\n    Most other apps that I've found are either browser only, or don't actually sync the lyrics to the song, they just show the entire lyrics at once.In comparison, my app shows the lyrics line by line, synced with the song, and also has (in my opinion lol) a fairly nice looking ui.It's also very easy to use for non programmers too, since you can just download an executable to use the app.\n  \n    It's available for free here https://github.com/Mews/spotify-lyrics"
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    Simplifies the interaction with the ShipEngine API with most response and requests built as objects, which in my opinion makes interaction much easier. This is my first released package so all criticism and feedback is very welcome.\n  \nTarget Audience\n\n    Anyone who deals with the current ShipEngine API using Python.\n  \nComparison\n\n    There is an official ShipEngine API module that is created by the company but I have found it somewhat lack luster with no way to create batches or bulk shipments (and other missing functionality), this is much more suited to accomplishing that task.\n  \nLinks\n\nhttps://github.com/Sen-tio/unofficial-shipengine"
},
{
    "title": "No title",
    "content": "Introducing Xenharmlib (Source code here)\n  \nWhat My Project Does\n\n    (taken from the docs) Xenharmlib is a music theory library for the exploration and research of microtonality, diatonic set theory, non-standard notations, and many more. The library implements a superset of Western classical music theory, so you can also use it to compose and analyze music in the boundaries of the common practice period or 20th century Western music.\n  \nTarget Audience\n\n    Composers who want to get answers to theoretical questions pertaining to structures of musical scales, note intervals, frequencies and frequency ratios in equal division tunings. People who want to explore microtonality or non-western musical theory in general.\n  \nComparison\n\n    * mingus Xenharmlib is pretty much on-par with features in mingus, however extends those features to all sorts of equal division temperaments.* pytuning supports more slightly tuning methods and export formats, however does not support microtonal notation or note / interval calculation* music21 is much more mature in providing an analytical toolset, however supports only traditional western equal temperament"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Good evening! I have created a new projectfor adding events to google calendar based on the text a user inputs.\n  What My Project Does\n    The project is a tool that uses large language models to understand the user's input and add events to the user's Google Calendar based on the user's input. It uses Ollama for natural language understanding and Google Calendar API for adding events to the user's calendar.\n  How My Project Works\n    Ollama uses Llama 3 with pre-instructions to act as a calendar event planner. The tool uses the model to generate responses to extract the event's details from the user's input inserted in the Web Interface. tool then asks the user to confirm the details extracted from the user's input and adds the event to the user's Google Calendar (example shown here)\n  References\n    Checkout my github repository AIPlanner for more details about the project."
},
{
    "title": "No title",
    "content": "Introducing PyPods\n\nWhat My Project Does\n\n    A Python library designed to manage monolithic project architectures by isolating dependencies.\n  \n    Traditionally, monolithic architectures cluster all dependencies into one project, creating complexities and potential conflicts. PyPods offers a solution by isolating these dependencies and enabling the main project to communicate with them via remote procedure calls.\n  \n    This approach eliminates the need to install dependencies directly in the main project. Feel free to take a look and I am happy to receive some feedback!\n  \nTarget Audience\n\n    Production grade.\n  \nComparison\n\n    This solution is inspired by Babashka pods in the Clojure world."
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    Hi! This is my first time doing a python project more than a few hours in size.\n  \n    I made a chat app which features E2E encryption using a passcode and has a multiclient architecture.\n  \n    All comments are welcome!\n  \nTarget Audience\n\n    It is just a toy project for my portfolio.\n  \nComparison\n\n    Compared to other chat clients, this one uses a passphrase to encrypt all data, with the passphrase being chosen out of the app, for instance on a dinner.\n  \n    But I think that IRC already has this, so it doesn't differ much XD.\n  \n    Git link:\n  \nhttps://github.com/xxzoltanxx/Balvan-Chat"
},
{
    "title": "No title",
    "content": "Hi everyone, Ive reached a state of my current project, where I want to share it with you, and gather some feedback. This is my first time using rye and I am surprised, how Hassle-Free building a package with it went. Source Code: github\nInstallationpython -m pip install rye-tui\n    for CLI Tools I recommend using pipx or rye.\n  pipx install rye-tuirye install rye-tui\n    After Installation you can open the TUI using trye in your Terminal. On first use a config file is generated. After that use trye again in your rye managed project\n  What My Project Does\n    A Text-based User Interface (TUI) for rye written in python using Textual\nCurrent State\n    Currently rye-tui supports the following functionalities of rye:\n  \n\n\n    creating new projects (flag-support coming soon)\n  \n\n\n    adding normal and dev dependencies (flag-support coming soon)\n  \n\n\n    pinning versions\n  \n\n\n    Syncing (flag-support coming soon)\n  \n\n\n    changing rye's configuration (sources and default coming soon)\n  \n\nTarget Audience\n    Python developers and rye users who like a UI to manage their rye projects.\n  Comparison\n    To my knowledge there is no similar tool for rye. Maybe the Anaconda UI comes closest for Anaconda Users.\n  Last Words\n    Feel free to try(e) it out. Happy to hear your feedback."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What My Project Does:I built a pipeline of Dagger modules to send my wife and I SMSs twice a week with actionable financial advice generated by AI based on data from bank accounts regarding our daily spending.\n  \nDetails:\n\n    Dagger is an open source programmable CI/CD engine. I built each step in the pipeline as a Dagger method. Dagger spins up ephemeral containers, running everything within its own container. I use GitHub Actions to trigger dagger methods that;\n  \n\n\n    retrieve data from a source\n  \n\n\n    filter for new transactions\n  \n\n\n    Categorizes transactions using a zero shot model, facebook/bart-large-mnli through the HuggingFace API. This process is optimized by sending data in dynamically sized batches asynchronously. \n  \n\n\n    Writes the data to a MongoDB database\n  \n\n\n    Retrieves the data, using Atlas search to aggregate the data by week and categories\n  \n\n\n    Sends the data to openAI to generate financial advice. In this module, I implement a memory using LangChain. I store this memory in MongoDB to persist the memory between build runs. I designed the database to rewrite the data whenever I receive new data. The memory keeps track of feedback given, enabling the advice to improve based on feedback\n  \n\n\n    This response is sent via SMS through the TextBelt API\n  \n\n\n    Full Blog: https://emmanuelsibanda.hashnode.dev/a-dagger-pipeline-sending-weekly-smss-with-financial-advice-generated-by-ai\n\n    Video Demo: https://youtu.be/S45n89gzH4Y\n\n    GitHub Repo: https://github.com/EmmS21/daggerverse\n\nTarget Audience: Personal project (family and friends)\n  \nComparison:\n  \n    We have too many budgeting apps and wanted to receive this advice via SMS, personalizing it based on our changing financial goals\n  \n    A screenshot of the message sent: https://ibb.co/Qk1wXQK"
},
{
    "title": "No title",
    "content": "There was a JSX-style syntax preprocessor for Python called \"Packed,\" which allowed us to write JSX inside Python (*.pyx and *.py) files. It's unclear why they chose *.pyx for the file extension, as it conflicts with the naming of Cythonic file extensions (I have checked their issues). This project might have thrived with sufficient contributions and could have changed the way apps are built. However, the project is now archived on GitHub. The last commit was 5 years ago (LICENSE), and the last development commit was 9 years ago. This repository needs someone to revive it, but I don't have enough experience to take on that task. Even though I don't have enough information, we should start with Rust + Python to build a compiler (aka. template replacer) (this doesn't compile Python but replaces all JSX with a dictionary) and cleaner syntax. Integration with Django (Packed has an example too), Flask, FastAPI, Robyn etc.\n  \n    We may also need plugins for the language server, I recommend supporting with *.pyh or *.psx (a fork renamed name) the extension file name (Derived from Python + HTML). VSCODE and NVIM Extensions are required to build support for this. The existing modern syntax of native Python will continue to support this syntax. I made a Handlebars Extension for the community back in the day of 2022 but I don't want to continue the development for it because later I disliked the syntax of handlebars (opinion, you're point of view may contrast with my thoughts). We can use emmet for writing easy HTML.\n  @packed\ndef tag(self):\n    share = get_share_link()\n    return <a href={share}>Share on internet</a>\n    The main point of view is that somehow make returnable components as 👆instead of doing this 👇\n  def app():\n    return div(div(p(\"Hello World\")),span(\"Not a Good Approach for someone (opinion)\"))"
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    Hey everyone! I'm the CEO of the company that built an SDK that makes it easy to build custom code interpreters for AI apps.\n  \n    We're a company called E2B [0]. We're building and open-source [1] secure environments for running untrusted AI-generated code and AI agents. We call these environments sandboxes and they are built on top of micro VM called Firecracker [2]. We specifically decided to use Firecrackers instead of containers because of their security and ability to do snapshots.\n  \n    You can think of us as giving small cloud computers to LLMs.\n  \n    We recently created a dedicated SDK for building custom code interpreters in Python or JS/TS. We saw this need after a lot of our users have been adding code execution capabilities to their AI apps with our core SDK [3]. These use cases were often centered around AI data analysis so code interpreter-like behavior made sense\n  \n    The way our code interpret SDK works is by spawning an E2B sandbox with Jupyter Server. We then communicate with this Jupyter server through Jupyter Kernel messaging protocol [4]. Here's our cookbook showing how to add code interpreter to different models [5].\n  \n    We don't do any wrapping around LLM, any prompting, or any agent-like framework. We leave all of that to our users. We're really just a boring code execution layer that sits at the bottom. We're building for the future software that will be building another software.\n  \n    Our long-term plan is to build an automated AWS for AI apps and agents where AI can build and deploy its own software while giving developers powerful observability into what's happening inside our sandboxes. With everything being open-source.\n  \n    Happy to answer any questions and hear feedback!\n  \nTarget Audience You can use it in production. We have companies using us in production already.\n  \nComparison Alternatives we usually see are serverless functions or Docker containers. Both have security issues. With serverless functions you can leak data between users and with containers you don't really have true isolation. Containers were made for packaging and portability, not security.\n  \nLinks\n\nhttps://github.com/e2b-dev/code-interpreter\n\n    [0] https://e2b.dev/\n\n    [1] https://github.com/e2b-dev\n\n    [2] https://github.com/firecracker-microvm/firecracker\n\n    [3] https://e2b.dev/docs\n\n    [4] https://jupyter-client.readthedocs.io/en/latest/messaging.html\n\n    [5] https://github.com/e2b-dev/e2b-cookbook"
},
{
    "title": "No title",
    "content": "https://github.com/william7491681/APOD_Wallpaper_Script\nWhat my project does\n    NASA has a ton of accessible API's, one of which being the APOD (Astronomy Picture Of the Day) API. I made a script to get the last 9 pictures of the day and set them as my Windows 10 background, and then used task scheduler to have the script re-run every day at noon and whenever the computer boots up.\n  \n    It's fairly hard coded for my setup (specific file paths, 1920x1080 monitor, etc), but it shouldn't be too hard to change if one wanted to.\n  Target audience\n    Anyone who likes space backgrounds\n  Comparison\n    Idk, automod made me put this section"
},
{
    "title": "No title",
    "content": "I hit publish on a blogpost last week on running Spark, Dask, DuckDB, and Polars on the TPC-H benchmark across a variety of scales (10 GiB, 100 GiB, 1 TiB, 10 TiB), both locally on a Macbook Pro and on the cloud.  It’s a broad set of configurations.  The results are interesting.\n  \n    No project wins uniformly.  They all perform differently at different scales: \n  \n\n\n    DuckDB and Polars are crazy fast on local machines\n  \n\n\n    Dask and DuckDB seem to win on cloud and at scale\n  \n\n\n    Dask ends up being most robust, especially at scale\n  \n\n\n    DuckDB does shockingly well on large datasets on a single large machine\n  \n\n\n    Spark performs oddly poorly, despite being the standard choice 😢\n  \n\n\n    Tons of charts in this post to try to make sense of the data.  If folks are curious, here’s the post:\n  \nhttps://docs.coiled.io/blog/tpch.html\n\n    And here's the code. Performance isn’t everything of course.  Each project has its die-hard fans/critics for loads of different reasons. I'd be curious to hear if people want to defend/critique their project of choice."
},
{
    "title": "No title",
    "content": "What My Project Does:\n\n    Hello all, I am a student at Stanford University, I was on a gap year due to medical conditions and to utilitze my time I was studying deep learning.\n  \n    And Voila...\n  \n    I've developed a deep learning library, DeepFusion!\n  \nDetails:\n\n    It's customizable and has an easily accessible and highly intuitive codebase. One can just dive right in and effortlessly understand the source code.\n  \n    You can download it from:\n  \n\n\n    github at https://github.com/atharvaaalok/deepfusion\n\n\n\n    or install using pip install deepfusion (easy!)\n  \n\n\n    For a series of examples explaining the usage and features refer demo or tutorials.\n  \nTarget Audience:\n\n    Machine learning and python enthusiasts.\n  \nComparison:\n\n    DeepFusion allows explicit access to all activations in a neural network, therefore, making applications such as neural style transfer much easier to perform. It also provides an easy user interface for forward and backward pass profiling, multiple loss functions, automated training, gpu training etc."
},
{
    "title": "No title",
    "content": "GitHub Repo: https://github.com/sepandhaghighi/mytimer\n\nWhat My Project Does:\n\nMyTimer is a Python CLI project that provides a simple, efficient timer for terminal users, particularly targeting the geek community. It allows users to set timers directly from their command line interface, offering a distraction-free experience.\n  mytimer --hour=12 --minute=34 --second=56 --alarm --alarm-repeat=5\n\n ___    ______          ______   _     _         _______  _______   \n(___)  (_____ \\        (_____ \\ | |   (_)       (_______)(_______)  \n   _     ____) )   _    _____) )| |_____    _    ______   ______    \n  | |   / ____/   (_)  (_____ ( |_____  |  (_)  (_____ \\ |  ___ \\   \n _| |_ | (_____    _    _____) )      | |   _    _____) )| |___) )  \n(_____)|_______)  (_)  (______/       |_|  (_)  (______/ |______/   \nTarget Audience:\n\n    Developers who spend a significant amount of time working in the terminal :)\n  \nComparison:\n\n\n\nMyTimer supports more features compared to countdown\n\n\n\nMyTimer offers a greater variety of faces and functions than timer-cli"
},
{
    "title": "No title",
    "content": "First timer this year, currently at the airport leaving Pittsburgh after 6 days of PyCon...\n  \n    I've never seen such an intelligent, inclusive, humble, diverse, and inspiring group of human beings.  The Python community serves as a beautiful model of what tech culture should strive towards. I could go on and on about how much fun I had, but in short, thanks to all the volunteers, staff, and FOSS developers that have cultivated such an amazing culture."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I'm giving a talk on polars in July. It's been pretty fast for us, but I'm curious to hear some examples of improvements other people have seen. I got one process down from over three minutes to around 10 seconds.Also curious whether people have switched over to using polars instead of pandas or they reserve it for specific use cases."
},
{
    "title": "No title",
    "content": "Hi everyone, I was building something that required me to communicate over USB to Raspberry Pi Pico using Pyusb Python. So I decided to make a blog post about it showing the concepts, process, and source code.Check out the blog post here!Check out the source code here!"
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    I made this library to help assert test responses inline while directing the comparison to be as rigid or lax as it needs to be.\n  \nMotivation\n\n    I write a lot of tests that assert values in complex nested dictionaries. But really I only need to check some parts in the response, not all of it.\n  \n    I often find myself transforming the response or maliciously extracting the important parts I need - in order to satisfy the assertions. This gets messy and can make tests hard to follow.\n  \nTarget Audience\n\n    Anyone who writes tests. This is particularly useful if you generate fake data in your tests with something like Faker, Factory Boy, or Model Bakery.\n  \nComparison\n\n    I have not found a like-project. Searched high and low in PyPI. If such a library existed, I would not have written one myself.\n  \n    Feedback appreciated.\n  \nSee PyPI project for basic use and github tests for more complex examples."
},
{
    "title": "No title",
    "content": "Midnight Player - a simple python audiplayer for playing audio\n  \nWhat My Project Does: My project is just an audio player for playing music, it can play audio from folders, supports different audio formats like Flac, Mp3 and can show some information about the track.\n  \n    Player uses subprocess to access ffmpeg, then it decodes the audio file into pcm format, then plays this file using sounddevice library. the use of these libraries is to ensure that the audio file does not lose quality during processing.\n  \nTarget Audience: This project was made to increase experience in python programming and to understand how the audio playback process works, but the project is also useful for people who are interested in learning the structure of the audio player as it is open source.\n  \nComparison: First of all you should understand that this player is not trying to compete with large-scale projects like AIMP because I developed this project alone and the project was written in a short period of time. But if you compare with other python audio players on github you will notice that many people use wrong libraries like qmediaplayer or pygame mixer to create their audio player, which are not designed for wide support of audio formats, my project is much more complex to operate audio file.\n  \n    Packages and source code can be found here: https://github.com/Niamorro/Midnight-Player"
},
{
    "title": "No title",
    "content": "Hi! I've been enjoying using PyPI's trusted publishing for the Python packages I maintain and I threw together a little post showing how I'm using that along with Poetry to publish a package from GitHub\n  \nhttps://johnfraney.ca/blog/how-to-publish-a-python-package-with-poetry-and-github-actions/\n\n    If you've got any tips for publishing a Python package, I'd be happy to hear those, too"
},
{
    "title": "No title",
    "content": "My project below, to put it simply, periodically checks the console.log for when a player join event occurs, when it does, it extracts the player's identifiers (player_name and identity_id). This is then checked against either, a JSON or, a database. I have incorporated standard logging, command-line arguments and threading to handle each player process individually.\n  \n    The target audience for this is the Arma Reforger community, for which, the application is made for.\n  \n    Currently, to my knowledge, there is no application like this available to the Arma Reforger community.\n  \n    I am very open to feedback, contributions and advice as want to expand this as much as possible!\n  \nhttps://github.com/BreathXV/ReforgerWhitelistPy"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "ComparisonIt is inspired from the existing tree command on linux and windows too So basically it is just like the tree command, it shows you a tree of the current directory structure.\n  \nWhat My Project DoesIt basically gives you a birds eye view of your dir structure and quickly navigate to the folder you want to without having to know its path or doing cd ../../.. many times.\n  \n    There are a bunch of command line args such as setting the paths, flags to show dot directories, set head height (no. of parent dirs shown) and tail height (depth).\n  \n    You can traverse around the tree using various key presses (inspired from vim keybindings) and based on the given argument (-o, -c or --copy) you can output the value (the node to which you traversed), cd into it and have it copied into your clipboard.J\n  \n    I had created this for my assignment and had a lot of fun with it. Tried to implement as much clean code and good design as I could but its still a mess and active work in progress tbh (added unit tests lol). And the rendering is still a little slow.\n  \n    Do check it out: pranavpa8788/trav: A Traversible Tree command line program (github.com) and let me know what you guys think. It is built with support for Windows and Linux, some installation stuff might be needed though, and I'll update those steps soon in the github page\n  \nTarget Audience\n\n    For anyone really, especially if you use a lot of terminal\n  \n    (Had to add the titles because my post was getting auto-deleted lol)\n  \n    Link to video demo: https://streamable.com/ds911k"
},
{
    "title": "No title",
    "content": "I've seen programmable semantics (eval-hacking, macros) in LISPs and in Haskell-likes(Monads/Template Haskell), the overall techinque in OOP languages is called \"Aspect Oriented Programming\".  Has this kind of thing been discussed before, and is it Pythonic it could allow a lot of Python code to be shorter.  Python has sys.set_trace that sort of allows some form of programmable semantics but its mostly for debugging.\n  \n    Programmable assignment(variables) are like setters/getters/properties, but instead of being run on o.x = 5, you could run them on \"all local assignments\" isnside a context manager or in a decorated function.  On every assignment you could do stuff like log the values, update dependencies, notify objects, do a database transaction, do persistance, calculate other values, without having to explicitly do so for every assignment statement.\n  \n    Programmable semicolons (such as Haskell Monads, or reprogramming Lisp do/progn/let) could allow you to have the same code run either synchronous, async, get undo/history support, break on error, rollback, logging in between lines, changing local/global variables in between each line, database access in between lines, checking values for errors, ignoring certain statements, etc...  You can think of a semicolon like an \"unrolled for loop\"/iterator ran for each code line.  It would be like async but you can change a piece of code to be sync or async at run time by changing the context manager you are in.  Programmable \"call\" can change the default call operation in a context manager for all functions and be similar to semicolons.\n  \n    Programmable eval would allow you to change the order of operations, choose to ignore certain functions, allow you replace certain expensive expressions with others, allow you to keep a trace of all evaluations taking place, you can turn an expression/program into an interator allowing you to pretty cool stuff."
},
{
    "title": "No title",
    "content": "Haven't seen this syntax used very often and was wondering why. During error handling, if you have something to run independent of the success, you can use finally.\n  from your_library import DataProcess\n\n\nengine = DataProcess()\n\ntry:\n    engine.io()\n    engine.process()\n    engine.some_more_io()\nexcept Exception as e:\n    engine.revert()\n    raise e\nfinally:\n    engine.cleanup()\n    VS\n  from your_library import DataProcess\n\n\nengine = DataProcess()\n\ntry:\n    engine.io()\n    engine.process()\n    engine.some_more_io()\nexcept Exception as e:\n    engine.revert()\n    engine.cleanup()\n    raise e\nengine.cleanup()\n    VS\n  from your_library import DataProcess\nfrom contextlib import contextmanager\n\n\n@contextmanager\ndef process_data(engine: DataProcess):\n    try:\n        engine.io()\n        yield engine\n    except Exception as e:\n        engine.revert()\n        raise e\n    finally:\n        engine.cleanup()\n\nproc = DataProcess()\nwith process_data(proc) as engine:\n    engine.process()\n    engine.some_more_io()"
},
{
    "title": "No title",
    "content": "Durable Python enables developers to write Python code while an underlying system ensures reliability and resilience.\n  \n    It automatically handles state persistence, fault tolerance, and retry mechanisms, allowing developers to focus on business logic without worrying about infrastructure concerns.\n  \n    Consider the following code, in case the process terminates in the middle of execution, in case the process is killed or due to hardware failure, the process will not complete.\n  import requests\nimport time\n\n\nSLEEP_SECONDS = 3\nURL = \"http://localhost:9980/webtools/api/msgs\"\n\ndef on_http_get(data):\n    for i in range(10):\n        print(\"Loop iteration: %d of 10\" % (i + 1))\n\n        # Send a POST request to the application\n        requests.post(URL, data = \"This is my \" + str(i) + \" iteration...\")\n        time.sleep(SLEEP_SECONDS)\n    But actually, I would like the process to survive restarts and continue from the spot it terminated, especially if it's a long running process. For this we need Durable Python.\n  \n    I was wondering which use cases can take advantage of this technology."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Introducing PyPods\n\nWhat My Project Does\n\n    A Python library designed to manage monolithic project architectures by isolating dependencies.\n  \n    Traditionally, monolithic architectures cluster all dependencies into one project, creating complexities and potential conflicts. PyPods offers a solution by isolating these dependencies and enabling the main project to communicate with them via remote procedure calls.\n  \n    This approach eliminates the need to install dependencies directly in the main project. Feel free to take a look and I am happy to receive some feedback!\n  \nTarget Audience\n\n    Production grade.\n  \nComparison\n\n    This solution is inspired by Babashka pods in the Clojure world."
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    Hi! This is my first time doing a python project more than a few hours in size.\n  \n    I made a chat app which features E2E encryption using a passcode and has a multiclient architecture.\n  \n    All comments are welcome!\n  \nTarget Audience\n\n    It is just a toy project for my portfolio.\n  \nComparison\n\n    Compared to other chat clients, this one uses a passphrase to encrypt all data, with the passphrase being chosen out of the app, for instance on a dinner.\n  \n    But I think that IRC already has this, so it doesn't differ much XD.\n  \n    Git link:\n  \nhttps://github.com/xxzoltanxx/Balvan-Chat"
},
{
    "title": "No title",
    "content": "Hi everyone, Ive reached a state of my current project, where I want to share it with you, and gather some feedback. This is my first time using rye and I am surprised, how Hassle-Free building a package with it went. Source Code: github\nInstallationpython -m pip install rye-tui\n    for CLI Tools I recommend using pipx or rye.\n  pipx install rye-tuirye install rye-tui\n    After Installation you can open the TUI using trye in your Terminal. On first use a config file is generated. After that use trye again in your rye managed project\n  What My Project Does\n    A Text-based User Interface (TUI) for rye written in python using Textual\nCurrent State\n    Currently rye-tui supports the following functionalities of rye:\n  \n\n\n    creating new projects (flag-support coming soon)\n  \n\n\n    adding normal and dev dependencies (flag-support coming soon)\n  \n\n\n    pinning versions\n  \n\n\n    Syncing (flag-support coming soon)\n  \n\n\n    changing rye's configuration (sources and default coming soon)\n  \n\nTarget Audience\n    Python developers and rye users who like a UI to manage their rye projects.\n  Comparison\n    To my knowledge there is no similar tool for rye. Maybe the Anaconda UI comes closest for Anaconda Users.\n  Last Words\n    Feel free to try(e) it out. Happy to hear your feedback."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What My Project Does:I built a pipeline of Dagger modules to send my wife and I SMSs twice a week with actionable financial advice generated by AI based on data from bank accounts regarding our daily spending.\n  \nDetails:\n\n    Dagger is an open source programmable CI/CD engine. I built each step in the pipeline as a Dagger method. Dagger spins up ephemeral containers, running everything within its own container. I use GitHub Actions to trigger dagger methods that;\n  \n\n\n    retrieve data from a source\n  \n\n\n    filter for new transactions\n  \n\n\n    Categorizes transactions using a zero shot model, facebook/bart-large-mnli through the HuggingFace API. This process is optimized by sending data in dynamically sized batches asynchronously. \n  \n\n\n    Writes the data to a MongoDB database\n  \n\n\n    Retrieves the data, using Atlas search to aggregate the data by week and categories\n  \n\n\n    Sends the data to openAI to generate financial advice. In this module, I implement a memory using LangChain. I store this memory in MongoDB to persist the memory between build runs. I designed the database to rewrite the data whenever I receive new data. The memory keeps track of feedback given, enabling the advice to improve based on feedback\n  \n\n\n    This response is sent via SMS through the TextBelt API\n  \n\n\n    Full Blog: https://emmanuelsibanda.hashnode.dev/a-dagger-pipeline-sending-weekly-smss-with-financial-advice-generated-by-ai\n\n    Video Demo: https://youtu.be/S45n89gzH4Y\n\n    GitHub Repo: https://github.com/EmmS21/daggerverse\n\nTarget Audience: Personal project (family and friends)\n  \nComparison:\n  \n    We have too many budgeting apps and wanted to receive this advice via SMS, personalizing it based on our changing financial goals\n  \n    A screenshot of the message sent: https://ibb.co/Qk1wXQK"
},
{
    "title": "No title",
    "content": "There was a JSX-style syntax preprocessor for Python called \"Packed,\" which allowed us to write JSX inside Python (*.pyx and *.py) files. It's unclear why they chose *.pyx for the file extension, as it conflicts with the naming of Cythonic file extensions (I have checked their issues). This project might have thrived with sufficient contributions and could have changed the way apps are built. However, the project is now archived on GitHub. The last commit was 5 years ago (LICENSE), and the last development commit was 9 years ago. This repository needs someone to revive it, but I don't have enough experience to take on that task. Even though I don't have enough information, we should start with Rust + Python to build a compiler (aka. template replacer) (this doesn't compile Python but replaces all JSX with a dictionary) and cleaner syntax. Integration with Django (Packed has an example too), Flask, FastAPI, Robyn etc.\n  \n    We may also need plugins for the language server, I recommend supporting with *.pyh or *.psx (a fork renamed name) the extension file name (Derived from Python + HTML). VSCODE and NVIM Extensions are required to build support for this. The existing modern syntax of native Python will continue to support this syntax. I made a Handlebars Extension for the community back in the day of 2022 but I don't want to continue the development for it because later I disliked the syntax of handlebars (opinion, you're point of view may contrast with my thoughts). We can use emmet for writing easy HTML.\n  @packed\ndef tag(self):\n    share = get_share_link()\n    return <a href={share}>Share on internet</a>\n    The main point of view is that somehow make returnable components as 👆instead of doing this 👇\n  def app():\n    return div(div(p(\"Hello World\")),span(\"Not a Good Approach for someone (opinion)\"))"
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    Hey everyone! I'm the CEO of the company that built an SDK that makes it easy to build custom code interpreters for AI apps.\n  \n    We're a company called E2B [0]. We're building and open-source [1] secure environments for running untrusted AI-generated code and AI agents. We call these environments sandboxes and they are built on top of micro VM called Firecracker [2]. We specifically decided to use Firecrackers instead of containers because of their security and ability to do snapshots.\n  \n    You can think of us as giving small cloud computers to LLMs.\n  \n    We recently created a dedicated SDK for building custom code interpreters in Python or JS/TS. We saw this need after a lot of our users have been adding code execution capabilities to their AI apps with our core SDK [3]. These use cases were often centered around AI data analysis so code interpreter-like behavior made sense\n  \n    The way our code interpret SDK works is by spawning an E2B sandbox with Jupyter Server. We then communicate with this Jupyter server through Jupyter Kernel messaging protocol [4]. Here's our cookbook showing how to add code interpreter to different models [5].\n  \n    We don't do any wrapping around LLM, any prompting, or any agent-like framework. We leave all of that to our users. We're really just a boring code execution layer that sits at the bottom. We're building for the future software that will be building another software.\n  \n    Our long-term plan is to build an automated AWS for AI apps and agents where AI can build and deploy its own software while giving developers powerful observability into what's happening inside our sandboxes. With everything being open-source.\n  \n    Happy to answer any questions and hear feedback!\n  \nTarget Audience You can use it in production. We have companies using us in production already.\n  \nComparison Alternatives we usually see are serverless functions or Docker containers. Both have security issues. With serverless functions you can leak data between users and with containers you don't really have true isolation. Containers were made for packaging and portability, not security.\n  \nLinks\n\nhttps://github.com/e2b-dev/code-interpreter\n\n    [0] https://e2b.dev/\n\n    [1] https://github.com/e2b-dev\n\n    [2] https://github.com/firecracker-microvm/firecracker\n\n    [3] https://e2b.dev/docs\n\n    [4] https://jupyter-client.readthedocs.io/en/latest/messaging.html\n\n    [5] https://github.com/e2b-dev/e2b-cookbook"
},
{
    "title": "No title",
    "content": "https://github.com/william7491681/APOD_Wallpaper_Script\nWhat my project does\n    NASA has a ton of accessible API's, one of which being the APOD (Astronomy Picture Of the Day) API. I made a script to get the last 9 pictures of the day and set them as my Windows 10 background, and then used task scheduler to have the script re-run every day at noon and whenever the computer boots up.\n  \n    It's fairly hard coded for my setup (specific file paths, 1920x1080 monitor, etc), but it shouldn't be too hard to change if one wanted to.\n  Target audience\n    Anyone who likes space backgrounds\n  Comparison\n    Idk, automod made me put this section"
},
{
    "title": "No title",
    "content": "I hit publish on a blogpost last week on running Spark, Dask, DuckDB, and Polars on the TPC-H benchmark across a variety of scales (10 GiB, 100 GiB, 1 TiB, 10 TiB), both locally on a Macbook Pro and on the cloud.  It’s a broad set of configurations.  The results are interesting.\n  \n    No project wins uniformly.  They all perform differently at different scales: \n  \n\n\n    DuckDB and Polars are crazy fast on local machines\n  \n\n\n    Dask and DuckDB seem to win on cloud and at scale\n  \n\n\n    Dask ends up being most robust, especially at scale\n  \n\n\n    DuckDB does shockingly well on large datasets on a single large machine\n  \n\n\n    Spark performs oddly poorly, despite being the standard choice 😢\n  \n\n\n    Tons of charts in this post to try to make sense of the data.  If folks are curious, here’s the post:\n  \nhttps://docs.coiled.io/blog/tpch.html\n\n    And here's the code. Performance isn’t everything of course.  Each project has its die-hard fans/critics for loads of different reasons. I'd be curious to hear if people want to defend/critique their project of choice."
},
{
    "title": "No title",
    "content": "What My Project Does:\n\n    Hello all, I am a student at Stanford University, I was on a gap year due to medical conditions and to utilitze my time I was studying deep learning.\n  \n    And Voila...\n  \n    I've developed a deep learning library, DeepFusion!\n  \nDetails:\n\n    It's customizable and has an easily accessible and highly intuitive codebase. One can just dive right in and effortlessly understand the source code.\n  \n    You can download it from:\n  \n\n\n    github at https://github.com/atharvaaalok/deepfusion\n\n\n\n    or install using pip install deepfusion (easy!)\n  \n\n\n    For a series of examples explaining the usage and features refer demo or tutorials.\n  \nTarget Audience:\n\n    Machine learning and python enthusiasts.\n  \nComparison:\n\n    DeepFusion allows explicit access to all activations in a neural network, therefore, making applications such as neural style transfer much easier to perform. It also provides an easy user interface for forward and backward pass profiling, multiple loss functions, automated training, gpu training etc."
},
{
    "title": "No title",
    "content": "GitHub Repo: https://github.com/sepandhaghighi/mytimer\n\nWhat My Project Does:\n\nMyTimer is a Python CLI project that provides a simple, efficient timer for terminal users, particularly targeting the geek community. It allows users to set timers directly from their command line interface, offering a distraction-free experience.\n  mytimer --hour=12 --minute=34 --second=56 --alarm --alarm-repeat=5\n\n ___    ______          ______   _     _         _______  _______   \n(___)  (_____ \\        (_____ \\ | |   (_)       (_______)(_______)  \n   _     ____) )   _    _____) )| |_____    _    ______   ______    \n  | |   / ____/   (_)  (_____ ( |_____  |  (_)  (_____ \\ |  ___ \\   \n _| |_ | (_____    _    _____) )      | |   _    _____) )| |___) )  \n(_____)|_______)  (_)  (______/       |_|  (_)  (______/ |______/   \nTarget Audience:\n\n    Developers who spend a significant amount of time working in the terminal :)\n  \nComparison:\n\n\n\nMyTimer supports more features compared to countdown\n\n\n\nMyTimer offers a greater variety of faces and functions than timer-cli"
},
{
    "title": "No title",
    "content": "First timer this year, currently at the airport leaving Pittsburgh after 6 days of PyCon...\n  \n    I've never seen such an intelligent, inclusive, humble, diverse, and inspiring group of human beings.  The Python community serves as a beautiful model of what tech culture should strive towards. I could go on and on about how much fun I had, but in short, thanks to all the volunteers, staff, and FOSS developers that have cultivated such an amazing culture."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I'm giving a talk on polars in July. It's been pretty fast for us, but I'm curious to hear some examples of improvements other people have seen. I got one process down from over three minutes to around 10 seconds.Also curious whether people have switched over to using polars instead of pandas or they reserve it for specific use cases."
},
{
    "title": "No title",
    "content": "Hi everyone, I was building something that required me to communicate over USB to Raspberry Pi Pico using Pyusb Python. So I decided to make a blog post about it showing the concepts, process, and source code.Check out the blog post here!Check out the source code here!"
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    I made this library to help assert test responses inline while directing the comparison to be as rigid or lax as it needs to be.\n  \nMotivation\n\n    I write a lot of tests that assert values in complex nested dictionaries. But really I only need to check some parts in the response, not all of it.\n  \n    I often find myself transforming the response or maliciously extracting the important parts I need - in order to satisfy the assertions. This gets messy and can make tests hard to follow.\n  \nTarget Audience\n\n    Anyone who writes tests. This is particularly useful if you generate fake data in your tests with something like Faker, Factory Boy, or Model Bakery.\n  \nComparison\n\n    I have not found a like-project. Searched high and low in PyPI. If such a library existed, I would not have written one myself.\n  \n    Feedback appreciated.\n  \nSee PyPI project for basic use and github tests for more complex examples."
},
{
    "title": "No title",
    "content": "Midnight Player - a simple python audiplayer for playing audio\n  \nWhat My Project Does: My project is just an audio player for playing music, it can play audio from folders, supports different audio formats like Flac, Mp3 and can show some information about the track.\n  \n    Player uses subprocess to access ffmpeg, then it decodes the audio file into pcm format, then plays this file using sounddevice library. the use of these libraries is to ensure that the audio file does not lose quality during processing.\n  \nTarget Audience: This project was made to increase experience in python programming and to understand how the audio playback process works, but the project is also useful for people who are interested in learning the structure of the audio player as it is open source.\n  \nComparison: First of all you should understand that this player is not trying to compete with large-scale projects like AIMP because I developed this project alone and the project was written in a short period of time. But if you compare with other python audio players on github you will notice that many people use wrong libraries like qmediaplayer or pygame mixer to create their audio player, which are not designed for wide support of audio formats, my project is much more complex to operate audio file.\n  \n    Packages and source code can be found here: https://github.com/Niamorro/Midnight-Player"
},
{
    "title": "No title",
    "content": "Hi! I've been enjoying using PyPI's trusted publishing for the Python packages I maintain and I threw together a little post showing how I'm using that along with Poetry to publish a package from GitHub\n  \nhttps://johnfraney.ca/blog/how-to-publish-a-python-package-with-poetry-and-github-actions/\n\n    If you've got any tips for publishing a Python package, I'd be happy to hear those, too"
},
{
    "title": "No title",
    "content": "My project below, to put it simply, periodically checks the console.log for when a player join event occurs, when it does, it extracts the player's identifiers (player_name and identity_id). This is then checked against either, a JSON or, a database. I have incorporated standard logging, command-line arguments and threading to handle each player process individually.\n  \n    The target audience for this is the Arma Reforger community, for which, the application is made for.\n  \n    Currently, to my knowledge, there is no application like this available to the Arma Reforger community.\n  \n    I am very open to feedback, contributions and advice as want to expand this as much as possible!\n  \nhttps://github.com/BreathXV/ReforgerWhitelistPy"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "ComparisonIt is inspired from the existing tree command on linux and windows too So basically it is just like the tree command, it shows you a tree of the current directory structure.\n  \nWhat My Project DoesIt basically gives you a birds eye view of your dir structure and quickly navigate to the folder you want to without having to know its path or doing cd ../../.. many times.\n  \n    There are a bunch of command line args such as setting the paths, flags to show dot directories, set head height (no. of parent dirs shown) and tail height (depth).\n  \n    You can traverse around the tree using various key presses (inspired from vim keybindings) and based on the given argument (-o, -c or --copy) you can output the value (the node to which you traversed), cd into it and have it copied into your clipboard.J\n  \n    I had created this for my assignment and had a lot of fun with it. Tried to implement as much clean code and good design as I could but its still a mess and active work in progress tbh (added unit tests lol). And the rendering is still a little slow.\n  \n    Do check it out: pranavpa8788/trav: A Traversible Tree command line program (github.com) and let me know what you guys think. It is built with support for Windows and Linux, some installation stuff might be needed though, and I'll update those steps soon in the github page\n  \nTarget Audience\n\n    For anyone really, especially if you use a lot of terminal\n  \n    (Had to add the titles because my post was getting auto-deleted lol)\n  \n    Link to video demo: https://streamable.com/ds911k"
},
{
    "title": "No title",
    "content": "I've seen programmable semantics (eval-hacking, macros) in LISPs and in Haskell-likes(Monads/Template Haskell), the overall techinque in OOP languages is called \"Aspect Oriented Programming\".  Has this kind of thing been discussed before, and is it Pythonic it could allow a lot of Python code to be shorter.  Python has sys.set_trace that sort of allows some form of programmable semantics but its mostly for debugging.\n  \n    Programmable assignment(variables) are like setters/getters/properties, but instead of being run on o.x = 5, you could run them on \"all local assignments\" isnside a context manager or in a decorated function.  On every assignment you could do stuff like log the values, update dependencies, notify objects, do a database transaction, do persistance, calculate other values, without having to explicitly do so for every assignment statement.\n  \n    Programmable semicolons (such as Haskell Monads, or reprogramming Lisp do/progn/let) could allow you to have the same code run either synchronous, async, get undo/history support, break on error, rollback, logging in between lines, changing local/global variables in between each line, database access in between lines, checking values for errors, ignoring certain statements, etc...  You can think of a semicolon like an \"unrolled for loop\"/iterator ran for each code line.  It would be like async but you can change a piece of code to be sync or async at run time by changing the context manager you are in.  Programmable \"call\" can change the default call operation in a context manager for all functions and be similar to semicolons.\n  \n    Programmable eval would allow you to change the order of operations, choose to ignore certain functions, allow you replace certain expensive expressions with others, allow you to keep a trace of all evaluations taking place, you can turn an expression/program into an interator allowing you to pretty cool stuff."
},
{
    "title": "No title",
    "content": "Haven't seen this syntax used very often and was wondering why. During error handling, if you have something to run independent of the success, you can use finally.\n  from your_library import DataProcess\n\n\nengine = DataProcess()\n\ntry:\n    engine.io()\n    engine.process()\n    engine.some_more_io()\nexcept Exception as e:\n    engine.revert()\n    raise e\nfinally:\n    engine.cleanup()\n    VS\n  from your_library import DataProcess\n\n\nengine = DataProcess()\n\ntry:\n    engine.io()\n    engine.process()\n    engine.some_more_io()\nexcept Exception as e:\n    engine.revert()\n    engine.cleanup()\n    raise e\nengine.cleanup()\n    VS\n  from your_library import DataProcess\nfrom contextlib import contextmanager\n\n\n@contextmanager\ndef process_data(engine: DataProcess):\n    try:\n        engine.io()\n        yield engine\n    except Exception as e:\n        engine.revert()\n        raise e\n    finally:\n        engine.cleanup()\n\nproc = DataProcess()\nwith process_data(proc) as engine:\n    engine.process()\n    engine.some_more_io()"
},
{
    "title": "No title",
    "content": "Durable Python enables developers to write Python code while an underlying system ensures reliability and resilience.\n  \n    It automatically handles state persistence, fault tolerance, and retry mechanisms, allowing developers to focus on business logic without worrying about infrastructure concerns.\n  \n    Consider the following code, in case the process terminates in the middle of execution, in case the process is killed or due to hardware failure, the process will not complete.\n  import requests\nimport time\n\n\nSLEEP_SECONDS = 3\nURL = \"http://localhost:9980/webtools/api/msgs\"\n\ndef on_http_get(data):\n    for i in range(10):\n        print(\"Loop iteration: %d of 10\" % (i + 1))\n\n        # Send a POST request to the application\n        requests.post(URL, data = \"This is my \" + str(i) + \" iteration...\")\n        time.sleep(SLEEP_SECONDS)\n    But actually, I would like the process to survive restarts and continue from the spot it terminated, especially if it's a long running process. For this we need Durable Python.\n  \n    I was wondering which use cases can take advantage of this technology."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "https://www.humblebundle.com/books/dive-into-dev-ops-no-starch-books\n\n    Be sure to click on \"Adjust Donation\" and \"Custom Amount\" and then max out the amount going to the Python Software Foundation. (From $1.75 to $24.50!)\n  \n    For $30 you get the following ebooks from No Starch Press:\n  \n\n\n    Automate the Boring Stuff with Python, 2nd Edition\n  \n\n\n    DevOps for the Desperate\n  \n\n\n    How Linux Works, 3rd Edition\n  \n\n\n    The Book of Kubernetes\n  \n\n\n    PowerShell for Sysadmins\n  \n\n\n    Practical Vulnerability Management\n  \n\n\n    Practical SQL, 2nd Edition\n  \n\n\n    Practical Linux Forensics\n  \n\n\n    Eloquent JavaScript, 3rd Edition\n  \n\n\n    Cybersecurity for Small Networks\n  \n\n\n    The Linux Command Line, 2nd Edition\n  \n\n\n    Web Security for Developers\n  \n\n\n    MySQL Crash Course\n  \n\n\n    Designing Secure Software\n  \n\n\n    Network Programming with Go\n  \n\n\n    Practice of Network Security Monitoring\n  \n\n\n    Network Flow Analysis\n  \n\n\n    Absolute FreeBSD, 3rd Edition\n  \n\n\n    Absolute OpenBSD, 2nd Edition\n  \n\n\n    Linux Firewalls\n  \n\n\n    Pentesting Azure Applications\n  \n\n\n    The Book of PF, 3rd Edition"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hey r/python!\n  \n    I wrote a guide on how to use Pytest, covering a bunch of important features like designing tests, filtering tests, parameterizing tests, fixtures, and more. Check it out on this link."
},
{
    "title": "No title",
    "content": "live-cells-py (Live Cells Python) is a reactive programming library which I ported from Live Cells for Dart.\n  What my project Does:\n    You can declare cells which are observable containers for data:\n  import live_cells as lc\n\na = lc.mutable(0)\n    Cells can be defined as a function of other cells:\n  a = lc.mutable(0)\nb = lc.mutable(1)\n\nc = lc.computed(lambda: a() + b())\nc is defined as the sum of the values of cells a and b. The value of c is automatically recomputed when the value of either a or b changes.\n  \n    The definition of c can be simplified to the following:\n  c = a + b\n    Which reads like an ordinary variable definition\n  \n    You can define a watch function which runs whenever the value of a cell changes:\n  lc.watch(lambda: print(f'The sum is {c()}'))\n    This watch function, which prints the value of c to standard output, is run automatically whenever the value of c changes.\n  \n    More complex computed cells and watch functions can be defined using decorators:\n  n = lc.mutable(5)\n\n@lc.computed\ndef n_factorial():\n    result = 1\n    m = n()\n\n    while m > 0:\n        result *= m\n        m -= 1\n\n    return m\n\n@lc.watch\ndef watch_factorial():\n   print(f'{n()}! = {n_factorial()}')\n    I've found this paradigm to be very useful for handling events and keeping the state of an application, be it a GUI desktop application, systems software or a server, in sync between its various components, which is why I ported this library to Python so I can use the same paradigm, with a similar API, on the backend as well.\n  Target Audience\n    This project is intended for those who are looking for a declarative solution to handling and reacting to events in Python applications that is simple and intuitive to use and doesn't require excessive boilerplate. Particularly if you're used to working with signals in JavaScript, you will quickly pick up this library.\n  Comparison\n    The de-facto standard for reactive programming is the ReactiveX (RX) series of libraries available for various programming languages. The main difference between RxPy and Live Cells is in the design of the API, with the main difference being that cells are self-subscribing. Referring to the examples shown in the previous sections, you do not have to explicitly \"connect\", \"subscribe\" to cells nor do you need a \"map\" or \"zip\" construct to build more complicated reactive pipelines. Instead you simply reference whatever you need and the subscription to the dependencies is handled automatically by the library.\n  \n    The source code and package is available at:\n  \nhttps://github.com/alex-gutev/live_cells_py https://pypi.org/project/live-cells-py/\n\n    The documentation is available at:\n  \nhttps://alex-gutev.github.io/live_cells_py/basics/cells.html"
},
{
    "title": "No title",
    "content": "You guys really need to check this. I believe new comers to python would love to tinker with the android ecosystem from the safety of python :-)\n  \n    Imgur: https://imgur.com/gallery/DtfwOVi\n\nhttps://www.kickstarter.com/projects/kivyschool/the-pain-free-python-on-android-essentials-course\n\n    Edit: added imgur link."
},
{
    "title": "No title",
    "content": "I’m an intern in a company and I automated some processes using python. My company’s IT wing said that as long as it is a licensed software you can use it in our company.\n  \n    In my mind I was like where the f I’m going to get a license for an open source software.\n  \n    Note : They mention that another team has been using licensed python. I thought either IT is so stupid or that team is so smart that they brought license for pycharm or anaconda (claim that it is a Python license) and fooled IT.\n  \n    If I am wrong then tell me where I can get that license.\n  \n    And I am also looking for job in data analyst."
},
{
    "title": "No title",
    "content": "What My Project DoesHey everyone, just released 8 new pip components for plotly and dash including:\n  \n\n\nFull Calendar Component - A Full Calendar Component for Dash\n  \n\n\nDash Summernote - A rich text WYSIWYG Editor for Dash\n  \n\n\nDash Emoji Mart - A Slack-like Emoji Picker for Dash\n  \n\n\nDash Charty - A Charting Library for Dash\n  \n\n\nDash Image Gallery - A Image Gallery Component for Dash\n  \n\n\nDash Swiper - A Swiper Component for Dash\n  \n\n\nDash Insta Stories - An Instagram Stories Component for Dash\n  \n\n\nDash Credit Cards - A Credit Card Component for Dash\n  \n\n\n    Documentation can be found here:\n  \nhttps://pip-docs.onrender.com/\n\n    The repo for the github can be found here:\n  \nhttps://github.com/pip-install-python/pip-docs\n\nTarget Audience\n\n    Plotly dash and Python developers.\n  \nComparison \n  \n    All these are new components for the dash framework, but based on javascript or react projects which were forked and edited to work specifically for dash."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I made a python package for S-transform with Hyperbolic window (Hyperbolic S-transform or HSTransform package). This is my first time publishing a python package, so the project is still far from stable and still under beta release.\n  \n\n\n    What my project does: This transformation is applied to signal processing, analyzing transient changes of a signal during very short-time. Some special use case can be in power system signal, or Geophysical signal analysis, or MRI ...\n  \n\n\n    Target audience: anyone who is interested in signal processing or power system analysis or geographical analysis.\n  \n\n\n    Comparison: The comparison with Wavelet Transform has been shown. (which shows more potential in detecting transient changes)\n  \n\n\n    I would highly appreciate some feedback, before progressing further.\n  \n    HSTransform is available on pypi.\n  \nLink to source code in github\nQuick Usageimport numpy as np\nfrom hstransform import HSTransform\n\n# Create input signal (for example: Voltage signal)\nt = np.linspace(0, 10, 100) # timeseries\nV_m = 220*np.sqrt(2)  # peak voltage\nf_V = 50  # frequency\nphi_V = 0  # phase\n\nV_clean = V_m * np.sin(2 * np.pi * f_V * t + phi_V)\n# Create voltage sag/dip (80% of the nominal voltage for 0.15 second)\nV_sag = np.where((t >= 2) & (t <= 3.5), 0.5 * V_clean, V_clean)\n\n# Create an instance of HSTransform\n\nhs = HSTransform()\n\n# Perform the transform\nsignal = V_sag\nS_transformed = hs.fit_transform(t, signal)"
},
{
    "title": "No title",
    "content": "https://brunodantas.github.io/pydash-cheatsheet/en/\n\n\n\n    What my project does: pydash is a library with great potential to make you code more Functional and simple. I made this cheatsheet a while ago to highlight some of the most useful functions of the library, since there are so many. I hope it's useful.\n  \n\n\n    Target audience: anyone who is interested in pydash, functional programming, not reinventing the wheel.\n  \n\n\n    Comparison: on Google you can find cheatsheets for Lodash, which is the original Javascript library which pydash is inspired by, but no cheatsheets for pydash itself. Note that many pydash functions are already implemented in modern Python, so I did not include those in the cheatsheet.\n  \n\n\n    I made this programatically using Material for Mkdocs, which I also recommend.\n  \nhttps://github.com/brunodantas/pydash-cheatsheet"
},
{
    "title": "No title",
    "content": "Hey all,\n  \n    I am not the original creator, but found that 4yo project, and decided to revive it!\n  \nWhat my project does: IconMatch is library allowing you to extract icons and letter positions from image or from display! There is also realtime demo on repo showcasing how it works!\n  \nTarget Audience: For all detecting objects from display!\n  \nComparison: I did not find other project like that - but it was my first find too! It is also not OCR!\n  \nhttps://github.com/NativeSensors/IconMatch\n\n    Have fun!"
},
{
    "title": "No title",
    "content": "Interactive web applications for data visualization improve user engagement and understanding.\n  \n    These days, Streamlit is a very popular framework used to provide web applications for data science.\n  \n    It is a terrific programming tool to have in you Python knowledge toolbox.\n  \n    Here’s a fun and practical tutorial on how to create a simple interactive and dynamic Streamlit application.\n  \n    This application generates a beautiful and original map using the prettymaps library.\n  \n    Free article: HERE"
},
{
    "title": "No title",
    "content": "What does my project do\n    Hi everyone\n  \n    I’m excited to share a project I’ve been working on: an interactive 8-puzzle game built using Python and Pygame. This project also includes several solvers based on classic search algorithms.\n  Technical details:\n\n\n    Python: the primary language in the project.\n  \n\n\n    Pygame: for rendering and handling user interaction.\n  \n\n\n    Search algorithms: implement depth first search (dfs) and A star search for solving the puzzle. By default A star search is used because it finds the solution faster than dfs\n  \n\nTarget Audience\n    This is a toy project I did for fun. You can find the project in GitHub: link\n\n    I would love to get your feedback, contributions, and if you find it interesting or helpful, please give it a star on GitHub. Your support and feedback will help me improve and add more features!Thank you for checking out my project!"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Optimize Your Python Workflow: Proven Techniques for Crafting Production-Ready Code\n  \nLink"
},
{
    "title": "No title",
    "content": "Hey everyone!\n  \n    I recently completed a fun project that I think fans of \"An Understated Dominance\" will appreciate, especially those who have struggled with reading the novel online due to website clutter and distractions.\n  \n    I've created a clean, distraction-free EPUB version of the novel that you can download and read on your preferred device. You can get it here: EPUB Download.\n  \nWhy I did this: The original website hosting the novel has a lot of distractions that can take away from the reading experience. My goal was to provide a seamless reading experience that allows you to fully immerse yourself in the story without unnecessary interruptions.\n  \nProject Details: For those interested in the technical side, here's a brief overview of how I created the EPUB:\n  \n\n\nWeb Scraping: I used Python with libraries like requests and BeautifulSoup to scrape all 2240 chapters of the novel.\n  \n\n\nConcurrency: Leveraged concurrent.futures to speed up the scraping process.\n  \n\n\nRetries: Implemented retry logic with tenacity to handle any request failures.\n  \n\n\nEPUB Creation: Used ebooklib to compile the scraped content into an EPUB file, complete with chapter headers and a cover image.\n  \n\n\n    You can check out the full code for this project on my GitHub: GitHub Repository.\n  \nFeedback & Collaboration: I’d love to hear your thoughts on the EPUB. Feel free to download, read, and let me know if you encounter any issues or have suggestions for improvements. If you're interested in collaborating on similar projects or have any ideas, I'm all ears!\n  \n    Enjoy the novel, and happy reading!"
},
{
    "title": "No title",
    "content": "What My Project Does\nPicodi is a lightweight and easy-to-use Dependency Injection (DI) library for Python. Picodi supports both synchronous and asynchronous contexts and offers features like resource lifecycle management. Think about Picodi as a decorator that helps you manage your dependencies without the need for a full-blown DI container.\n  Key Features\n\n\n    🌟 Simple and lightweight\n  \n\n\n    📦 Zero dependencies\n  \n\n\n    ⏱️ Supports both sync and async contexts\n  \n\n\n    🔄 Resource lifecycle management\n  \n\n\n    🔍 Type hints support\n  \n\n\n    🐍 Python & PyPy 3.10+ support\n  \n\nQuick Start\n    Here’s a quick example of how Picodi works:\n  import asyncio\nfrom collections.abc import Callable\nfrom datetime import date\nfrom typing import Any\nimport httpx\nfrom picodi import Provide, init_resources, inject, resource, shutdown_resources\nfrom picodi.helpers import get_value\n\n\ndef get_settings() -> dict:\n    return {\n        \"nasa_api\": {\n            \"api_key\": \"DEMO_KEY\",\n            \"base_url\": \"https://api.nasa.gov\",\n            \"timeout\": 10,\n        }\n    }\n\n@inject\ndef get_setting(path: str, settings: dict = Provide(get_settings)) -> Callable[[], Any]:\n    value = get_value(path, settings)\n    return lambda: value\n\n@resource\n@inject\nasync def get_nasa_client(\n    api_key: str = Provide(get_setting(\"nasa_api.api_key\")),\n    base_url: str = Provide(get_setting(\"nasa_api.base_url\")),\n    timeout: int = Provide(get_setting(\"nasa_api.timeout\")),\n) -> httpx.AsyncClient:\n    async with httpx.AsyncClient(\n        base_url=base_url, params={\"api_key\": api_key}, timeout=timeout\n    ) as client:\n        yield client\n\n@inject\nasync def get_apod(\n    date: date, client: httpx.AsyncClient = Provide(get_nasa_client)\n) -> dict[str, Any]:\n    response = await client.get(\"/planetary/apod\", params={\"date\": date.isoformat()})\n    response.raise_for_status()\n    return response.json()\n\nasync def main():\n    await init_resources()\n    apod_data = await get_apod(date(2011, 7, 19))\n    print(\"Title:\", apod_data[\"title\"])\n    await shutdown_resources()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n    This example demonstrates how Picodi handles dependency injection for both synchronous and asynchronous functions, manages resource lifecycles, and provides a clean and efficient way to structure your code.\n  \n    For more examples and detailed documentation, check out the GitHub repository\nTarget Audience\n    Picodi is perfect for developers who want to simplify dependency management in their Python applications, but don't want to deal with the complexity of larger DI frameworks. Picodi can help you write cleaner and more maintainable code.\n  Comparison\n    Unlike other DI libraries, Picodi does not have wiring, a large set of different types of providers, or the concept of a container.\n  \n    Picodi prioritizes simplicity, so it includes only the most essential features: dependency injection, resource lifecycle management, and dependency overriding.\n  Get Involved\n    Picodi is still in the experimental stage, and I'm looking for feedback from the community. If you have any suggestions, encounter any issues, or want to contribute, please check out the GitHub repository and let me know."
},
{
    "title": "No title",
    "content": "Homoiconic, what does it mean? In simple terms, homoiconic code is when code is treated as data and can be manipulated as you would data. This means the code can be changed, new functions and variables added, the code can generate new code or even examine and modify its own structure and behavior all while it is running. That’s why homoiconic languages like Lisp are so powerful. But what if we can make a homoiconic python code, where the code and the data are one and the same and can be modified in the same way?\n  \n    This guide does a good job in trying to explain how you would create a python version of the “Lisp in Lisp” code which would give you access to all those homoiconic features that Lisp brags of like the macro systems, the expressiveness and flexibility, the metaprogramming etc. while still using python. What do you guys think of this?"
},
{
    "title": "No title",
    "content": "What My Project Does\n\nhttps://github.com/treykeown/arguably\n\narguably makes it super simple to define complex CLIs. It uses your function signatures and docstrings to set everything up. Here's how it works:\n  \n\n\n    Adding the @arguably.command decorator to a function makes it appear on the CLI.\n  \n\n\n    If multiple functions are decorated, they'll all be set up as subcommands. You can even set up multiple levels of subcommands.\n  \n\n\n    The function name, signature, and docstring are used to automatically set up the CLI\n  \n\n\n    Call arguably.run() to parse the arguments and invoke the appropriate command\n  \n\n\n    A small example:\n  #!/usr/bin/env python3\nimport arguably\n\n@arguably.command\ndef some_function(required, not_required=2, *others: int, option: float = 3.14):\n    \"\"\"\n    this function is on the command line!\n\n    Args:\n        required: a required argument\n        not_required: this one isn't required, since it has a default value\n        *others: all the other positional arguments go here\n        option: [-x] keyword-only args are options, short name is in brackets\n    \"\"\"\n    print(f\"{required=}, {not_required=}, {others=}, {option=}\")\n\nif __name__ == \"__main__\":\n    arguably.run()\n    becomes\n  user@machine:~$ ./readme-1.py -h\nusage: readme-1.py [-h] [-x OPTION] required [not-required] [others ...]\n\nthis function is on the command line!\n\npositional arguments:\n  required             a required parameter (type: str)\n  not-required         this one isn't required, since it has a default (type: int, default: 2)\n  others               all the other positional arguments go here (type: int)\n\noptions:\n  -h, --help           show this help message and exit\n  -x, --option OPTION  an option, short name is in brackets (type: float, default: 3.14)\n    It can easily hand some very complex cases, like passing in QEMU-style arguments to automatically instantiated different types of classes:\n  user@machine:~$ ./readme-2.py --nic tap,model=e1000 --nic user,hostfwd=tcp::10022-:22\nnic=[TapNic(model='e1000'), UserNic(hostfwd='tcp::10022-:22')]\n    You can also auto-generate a CLI for your script through python3 -m arguably your_script.py, more on that here.\n  \nTarget Audience\n\n    If you're writing a script or tool, and you need a quick and effective way to run it from the command line, arguably was made for you. It's great for things where a CLI is essential, but doesn't need tons of customization. arguably makes some opinionated decisions that keep things simple for you, but doesn't expose ways of handling things like error messages.\n  \n    I put in the work to create GitHub workflows, documentation, and proper tests for arguably. I want this to be useful for the community at large, and a tool that you can rely on. Let me know if you're having trouble with your use case!\n  \nComparison\n\n    There are plenty of other tools for making CLIs out there. My goal was to build one that's unobtrusive and easy to integrate. I wrote a whole page on the project goals here: https://treykeown.github.io/arguably/why/\n\n    A quick comparison:\n  \n\n\nargparse - this is what arguably uses under the hood. The end user experience should be similar - arguably just aims to make it easy to set up.\n  \n\n\nclick - a powerhouse with all the tools you'd ever want. Use this if you need extensive customization and don't mind some verbosity.\n  \n\n\ntyper - also a great option, and some aspects are similar design-wise. It also uses functions with a decorator to set up commands, and also uses the function signature. A bit more verbose, though like click, has more customization options.\n  \n\n\nfire - super easy to generate CLIs. arguably tries to improve on this by utilizing type hints for argument conversion, and being a little more of a middle ground between this and the more traditional ways of writing CLIs in Python.\n  \n\n\n    This project has been a labor of love to make CLI generation as easy as it should be. Thanks for checking it out!"
},
{
    "title": "No title",
    "content": "With Python i created a tool that enables users to download LinkedIn Learning courses, including the often overlooked but incredibly valuable exercise files. This feature sets our project apart, offering a complete learning experience by providing both the course videos and the materials needed for practical application.What great about it and beyond other tools in the same genre concerned LinkedIn Learning Downloaders, now you can download the whole courses from a path link. this is was never possible without Python.\n  \n     For more detailed information, visit the repo : https://github.com/M0r0cc4nGh0st/LinkedIn-Learning-Downloader"
},
{
    "title": "No title",
    "content": "Hello there, I just created a template for creating a backend for your SaaS products.\n  \nWhat my project does: It is a FastAPI project/template for creating SaaS backends and admin dashboards.\n  \nComparison: Out of the box, it supports\n  \n\n\n    Licence key generation and validation.\n  \n\n\n    OAuth 2 authentication with scopes.\n  \n\n\n    Endpoints with pagination and filters to easily integrate with an admin dashboard.\n  \n\n\n    Passwords are securely stored using hashing.\n  \n\n\n    used PostgreSQL for database\n  \n\n\nTarget Audience: Production\n  \nCheck it here!\n\nUpdate 1: Added pre-commit hooks, tox for testing and linting."
},
{
    "title": "No title",
    "content": "Recently, I found out about the this \"Easter egg\" in python3. Adding import this into a py file will print \"The Zen of Python\" by Tim Peters. Also, this has two attributes: this.s and this.d, which I guess form the actual Easter egg. this.s returns an encrypted version of \"The Zen\" and this.d well, see for yourself, maybe you'll solve the puzzle."
},
{
    "title": "No title",
    "content": "I made a YouTube video which previews the zoom and explains the code, which you can find here: https://youtu.be/HtNUFdh2sjg\n\nWhat my project does: it creates a Mandelbrot Zoom.\n  \nComparison: it uses Pillow and consists of just 2 main blocks of code: one is the main function that finds which points are in the Mandelbrot Set and the other is the main loop that applies appropriate colors to each image. It gives the option of being black and white OR in color.\n  \n    It works fairly well but can definitely be faster if parallelized. I'd love to hear any suggestions on how it can be improved.\n  \nTarget Audience: fun/toy project\n  \n    Source code is here: https://github.com/AbideByReason/Python_Notebooks/tree/main"
},
{
    "title": "No title",
    "content": "What the project does: data animation library for time-series data. Currently it supports the following chart types:\n  \n\n\n    Bar races\n  \n\n\n    Animated Pie Charts\n  \n\n\n    Animated Line Charts\n  \n\n\n    Animated Stacked Area Charts\n  \n\n\n    Animated (World) Maps\n  \n\n\n    You can find some simple example charts here: https://www.sjdataviz.com/software\n\n    It is on pypi, you can install it using:\n  \npip install sjvisualizer\n\n    It is fully based on TkInter to draw the graph shapes to the screen, which gives a lot of flexibility. You can also mix and match the different chart types in a single animation.\n  \n    Target audience: people interested in data animation for presentations or social media content creation\n  \n    Alternatives: I only know one alternative which is bar-chart-race, the ways sjvisualizer is better:\n  \n\n\n    Smoother animation, bar-chart-race isn't the quite choppy I would say\n  \n\n\n    Load custom icons for each data category (flag icons for countries for example)\n  \n\n\n    Number of supported chart types\n  \n\n\n    Mix and match different chart types in a single animation, have a bar race to show the ranking, and a smaller pie chart showing the percentages of the whole\n  \n\n\n    Based on TkInter, easy to add custom elements through the standard python GUI library\n  \n\n\n    Topics to improve (contributions welcome):\n  \n\n\n    Documentation\n  \n\n\n    Improve built in screen recorder, performance takes a hit when using the built in screen recorder\n  \n\n\n    Additional chart types: bubble charts, lollipop charts, etc\n  \n\n\n    Improve the way data can be loaded into the library (currently only supports reading into a dataframe from Excel)\n  \n\n\n    Sorry for the long post, you can find it here on GitHub: https://github.com/SjoerdTilmans/sjvisualizer"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "https://www.humblebundle.com/books/dive-into-dev-ops-no-starch-books\n\n    Be sure to click on \"Adjust Donation\" and \"Custom Amount\" and then max out the amount going to the Python Software Foundation. (From $1.75 to $24.50!)\n  \n    For $30 you get the following ebooks from No Starch Press:\n  \n\n\n    Automate the Boring Stuff with Python, 2nd Edition\n  \n\n\n    DevOps for the Desperate\n  \n\n\n    How Linux Works, 3rd Edition\n  \n\n\n    The Book of Kubernetes\n  \n\n\n    PowerShell for Sysadmins\n  \n\n\n    Practical Vulnerability Management\n  \n\n\n    Practical SQL, 2nd Edition\n  \n\n\n    Practical Linux Forensics\n  \n\n\n    Eloquent JavaScript, 3rd Edition\n  \n\n\n    Cybersecurity for Small Networks\n  \n\n\n    The Linux Command Line, 2nd Edition\n  \n\n\n    Web Security for Developers\n  \n\n\n    MySQL Crash Course\n  \n\n\n    Designing Secure Software\n  \n\n\n    Network Programming with Go\n  \n\n\n    Practice of Network Security Monitoring\n  \n\n\n    Network Flow Analysis\n  \n\n\n    Absolute FreeBSD, 3rd Edition\n  \n\n\n    Absolute OpenBSD, 2nd Edition\n  \n\n\n    Linux Firewalls\n  \n\n\n    Pentesting Azure Applications\n  \n\n\n    The Book of PF, 3rd Edition"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hey r/python!\n  \n    I wrote a guide on how to use Pytest, covering a bunch of important features like designing tests, filtering tests, parameterizing tests, fixtures, and more. Check it out on this link."
},
{
    "title": "No title",
    "content": "live-cells-py (Live Cells Python) is a reactive programming library which I ported from Live Cells for Dart.\n  What my project Does:\n    You can declare cells which are observable containers for data:\n  import live_cells as lc\n\na = lc.mutable(0)\n    Cells can be defined as a function of other cells:\n  a = lc.mutable(0)\nb = lc.mutable(1)\n\nc = lc.computed(lambda: a() + b())\nc is defined as the sum of the values of cells a and b. The value of c is automatically recomputed when the value of either a or b changes.\n  \n    The definition of c can be simplified to the following:\n  c = a + b\n    Which reads like an ordinary variable definition\n  \n    You can define a watch function which runs whenever the value of a cell changes:\n  lc.watch(lambda: print(f'The sum is {c()}'))\n    This watch function, which prints the value of c to standard output, is run automatically whenever the value of c changes.\n  \n    More complex computed cells and watch functions can be defined using decorators:\n  n = lc.mutable(5)\n\n@lc.computed\ndef n_factorial():\n    result = 1\n    m = n()\n\n    while m > 0:\n        result *= m\n        m -= 1\n\n    return m\n\n@lc.watch\ndef watch_factorial():\n   print(f'{n()}! = {n_factorial()}')\n    I've found this paradigm to be very useful for handling events and keeping the state of an application, be it a GUI desktop application, systems software or a server, in sync between its various components, which is why I ported this library to Python so I can use the same paradigm, with a similar API, on the backend as well.\n  Target Audience\n    This project is intended for those who are looking for a declarative solution to handling and reacting to events in Python applications that is simple and intuitive to use and doesn't require excessive boilerplate. Particularly if you're used to working with signals in JavaScript, you will quickly pick up this library.\n  Comparison\n    The de-facto standard for reactive programming is the ReactiveX (RX) series of libraries available for various programming languages. The main difference between RxPy and Live Cells is in the design of the API, with the main difference being that cells are self-subscribing. Referring to the examples shown in the previous sections, you do not have to explicitly \"connect\", \"subscribe\" to cells nor do you need a \"map\" or \"zip\" construct to build more complicated reactive pipelines. Instead you simply reference whatever you need and the subscription to the dependencies is handled automatically by the library.\n  \n    The source code and package is available at:\n  \nhttps://github.com/alex-gutev/live_cells_py https://pypi.org/project/live-cells-py/\n\n    The documentation is available at:\n  \nhttps://alex-gutev.github.io/live_cells_py/basics/cells.html"
},
{
    "title": "No title",
    "content": "You guys really need to check this. I believe new comers to python would love to tinker with the android ecosystem from the safety of python :-)\n  \n    Imgur: https://imgur.com/gallery/DtfwOVi\n\nhttps://www.kickstarter.com/projects/kivyschool/the-pain-free-python-on-android-essentials-course\n\n    Edit: added imgur link."
},
{
    "title": "No title",
    "content": "I’m an intern in a company and I automated some processes using python. My company’s IT wing said that as long as it is a licensed software you can use it in our company.\n  \n    In my mind I was like where the f I’m going to get a license for an open source software.\n  \n    Note : They mention that another team has been using licensed python. I thought either IT is so stupid or that team is so smart that they brought license for pycharm or anaconda (claim that it is a Python license) and fooled IT.\n  \n    If I am wrong then tell me where I can get that license.\n  \n    And I am also looking for job in data analyst."
},
{
    "title": "No title",
    "content": "What My Project DoesHey everyone, just released 8 new pip components for plotly and dash including:\n  \n\n\nFull Calendar Component - A Full Calendar Component for Dash\n  \n\n\nDash Summernote - A rich text WYSIWYG Editor for Dash\n  \n\n\nDash Emoji Mart - A Slack-like Emoji Picker for Dash\n  \n\n\nDash Charty - A Charting Library for Dash\n  \n\n\nDash Image Gallery - A Image Gallery Component for Dash\n  \n\n\nDash Swiper - A Swiper Component for Dash\n  \n\n\nDash Insta Stories - An Instagram Stories Component for Dash\n  \n\n\nDash Credit Cards - A Credit Card Component for Dash\n  \n\n\n    Documentation can be found here:\n  \nhttps://pip-docs.onrender.com/\n\n    The repo for the github can be found here:\n  \nhttps://github.com/pip-install-python/pip-docs\n\nTarget Audience\n\n    Plotly dash and Python developers.\n  \nComparison \n  \n    All these are new components for the dash framework, but based on javascript or react projects which were forked and edited to work specifically for dash."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I made a python package for S-transform with Hyperbolic window (Hyperbolic S-transform or HSTransform package). This is my first time publishing a python package, so the project is still far from stable and still under beta release.\n  \n\n\n    What my project does: This transformation is applied to signal processing, analyzing transient changes of a signal during very short-time. Some special use case can be in power system signal, or Geophysical signal analysis, or MRI ...\n  \n\n\n    Target audience: anyone who is interested in signal processing or power system analysis or geographical analysis.\n  \n\n\n    Comparison: The comparison with Wavelet Transform has been shown. (which shows more potential in detecting transient changes)\n  \n\n\n    I would highly appreciate some feedback, before progressing further.\n  \n    HSTransform is available on pypi.\n  \nLink to source code in github\nQuick Usageimport numpy as np\nfrom hstransform import HSTransform\n\n# Create input signal (for example: Voltage signal)\nt = np.linspace(0, 10, 100) # timeseries\nV_m = 220*np.sqrt(2)  # peak voltage\nf_V = 50  # frequency\nphi_V = 0  # phase\n\nV_clean = V_m * np.sin(2 * np.pi * f_V * t + phi_V)\n# Create voltage sag/dip (80% of the nominal voltage for 0.15 second)\nV_sag = np.where((t >= 2) & (t <= 3.5), 0.5 * V_clean, V_clean)\n\n# Create an instance of HSTransform\n\nhs = HSTransform()\n\n# Perform the transform\nsignal = V_sag\nS_transformed = hs.fit_transform(t, signal)"
},
{
    "title": "No title",
    "content": "https://brunodantas.github.io/pydash-cheatsheet/en/\n\n\n\n    What my project does: pydash is a library with great potential to make you code more Functional and simple. I made this cheatsheet a while ago to highlight some of the most useful functions of the library, since there are so many. I hope it's useful.\n  \n\n\n    Target audience: anyone who is interested in pydash, functional programming, not reinventing the wheel.\n  \n\n\n    Comparison: on Google you can find cheatsheets for Lodash, which is the original Javascript library which pydash is inspired by, but no cheatsheets for pydash itself. Note that many pydash functions are already implemented in modern Python, so I did not include those in the cheatsheet.\n  \n\n\n    I made this programatically using Material for Mkdocs, which I also recommend.\n  \nhttps://github.com/brunodantas/pydash-cheatsheet"
},
{
    "title": "No title",
    "content": "Hey all,\n  \n    I am not the original creator, but found that 4yo project, and decided to revive it!\n  \nWhat my project does: IconMatch is library allowing you to extract icons and letter positions from image or from display! There is also realtime demo on repo showcasing how it works!\n  \nTarget Audience: For all detecting objects from display!\n  \nComparison: I did not find other project like that - but it was my first find too! It is also not OCR!\n  \nhttps://github.com/NativeSensors/IconMatch\n\n    Have fun!"
},
{
    "title": "No title",
    "content": "Interactive web applications for data visualization improve user engagement and understanding.\n  \n    These days, Streamlit is a very popular framework used to provide web applications for data science.\n  \n    It is a terrific programming tool to have in you Python knowledge toolbox.\n  \n    Here’s a fun and practical tutorial on how to create a simple interactive and dynamic Streamlit application.\n  \n    This application generates a beautiful and original map using the prettymaps library.\n  \n    Free article: HERE"
},
{
    "title": "No title",
    "content": "What does my project do\n    Hi everyone\n  \n    I’m excited to share a project I’ve been working on: an interactive 8-puzzle game built using Python and Pygame. This project also includes several solvers based on classic search algorithms.\n  Technical details:\n\n\n    Python: the primary language in the project.\n  \n\n\n    Pygame: for rendering and handling user interaction.\n  \n\n\n    Search algorithms: implement depth first search (dfs) and A star search for solving the puzzle. By default A star search is used because it finds the solution faster than dfs\n  \n\nTarget Audience\n    This is a toy project I did for fun. You can find the project in GitHub: link\n\n    I would love to get your feedback, contributions, and if you find it interesting or helpful, please give it a star on GitHub. Your support and feedback will help me improve and add more features!Thank you for checking out my project!"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Optimize Your Python Workflow: Proven Techniques for Crafting Production-Ready Code\n  \nLink"
},
{
    "title": "No title",
    "content": "Hey everyone!\n  \n    I recently completed a fun project that I think fans of \"An Understated Dominance\" will appreciate, especially those who have struggled with reading the novel online due to website clutter and distractions.\n  \n    I've created a clean, distraction-free EPUB version of the novel that you can download and read on your preferred device. You can get it here: EPUB Download.\n  \nWhy I did this: The original website hosting the novel has a lot of distractions that can take away from the reading experience. My goal was to provide a seamless reading experience that allows you to fully immerse yourself in the story without unnecessary interruptions.\n  \nProject Details: For those interested in the technical side, here's a brief overview of how I created the EPUB:\n  \n\n\nWeb Scraping: I used Python with libraries like requests and BeautifulSoup to scrape all 2240 chapters of the novel.\n  \n\n\nConcurrency: Leveraged concurrent.futures to speed up the scraping process.\n  \n\n\nRetries: Implemented retry logic with tenacity to handle any request failures.\n  \n\n\nEPUB Creation: Used ebooklib to compile the scraped content into an EPUB file, complete with chapter headers and a cover image.\n  \n\n\n    You can check out the full code for this project on my GitHub: GitHub Repository.\n  \nFeedback & Collaboration: I’d love to hear your thoughts on the EPUB. Feel free to download, read, and let me know if you encounter any issues or have suggestions for improvements. If you're interested in collaborating on similar projects or have any ideas, I'm all ears!\n  \n    Enjoy the novel, and happy reading!"
},
{
    "title": "No title",
    "content": "What My Project Does\nPicodi is a lightweight and easy-to-use Dependency Injection (DI) library for Python. Picodi supports both synchronous and asynchronous contexts and offers features like resource lifecycle management. Think about Picodi as a decorator that helps you manage your dependencies without the need for a full-blown DI container.\n  Key Features\n\n\n    🌟 Simple and lightweight\n  \n\n\n    📦 Zero dependencies\n  \n\n\n    ⏱️ Supports both sync and async contexts\n  \n\n\n    🔄 Resource lifecycle management\n  \n\n\n    🔍 Type hints support\n  \n\n\n    🐍 Python & PyPy 3.10+ support\n  \n\nQuick Start\n    Here’s a quick example of how Picodi works:\n  import asyncio\nfrom collections.abc import Callable\nfrom datetime import date\nfrom typing import Any\nimport httpx\nfrom picodi import Provide, init_resources, inject, resource, shutdown_resources\nfrom picodi.helpers import get_value\n\n\ndef get_settings() -> dict:\n    return {\n        \"nasa_api\": {\n            \"api_key\": \"DEMO_KEY\",\n            \"base_url\": \"https://api.nasa.gov\",\n            \"timeout\": 10,\n        }\n    }\n\n@inject\ndef get_setting(path: str, settings: dict = Provide(get_settings)) -> Callable[[], Any]:\n    value = get_value(path, settings)\n    return lambda: value\n\n@resource\n@inject\nasync def get_nasa_client(\n    api_key: str = Provide(get_setting(\"nasa_api.api_key\")),\n    base_url: str = Provide(get_setting(\"nasa_api.base_url\")),\n    timeout: int = Provide(get_setting(\"nasa_api.timeout\")),\n) -> httpx.AsyncClient:\n    async with httpx.AsyncClient(\n        base_url=base_url, params={\"api_key\": api_key}, timeout=timeout\n    ) as client:\n        yield client\n\n@inject\nasync def get_apod(\n    date: date, client: httpx.AsyncClient = Provide(get_nasa_client)\n) -> dict[str, Any]:\n    response = await client.get(\"/planetary/apod\", params={\"date\": date.isoformat()})\n    response.raise_for_status()\n    return response.json()\n\nasync def main():\n    await init_resources()\n    apod_data = await get_apod(date(2011, 7, 19))\n    print(\"Title:\", apod_data[\"title\"])\n    await shutdown_resources()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n    This example demonstrates how Picodi handles dependency injection for both synchronous and asynchronous functions, manages resource lifecycles, and provides a clean and efficient way to structure your code.\n  \n    For more examples and detailed documentation, check out the GitHub repository\nTarget Audience\n    Picodi is perfect for developers who want to simplify dependency management in their Python applications, but don't want to deal with the complexity of larger DI frameworks. Picodi can help you write cleaner and more maintainable code.\n  Comparison\n    Unlike other DI libraries, Picodi does not have wiring, a large set of different types of providers, or the concept of a container.\n  \n    Picodi prioritizes simplicity, so it includes only the most essential features: dependency injection, resource lifecycle management, and dependency overriding.\n  Get Involved\n    Picodi is still in the experimental stage, and I'm looking for feedback from the community. If you have any suggestions, encounter any issues, or want to contribute, please check out the GitHub repository and let me know."
},
{
    "title": "No title",
    "content": "Homoiconic, what does it mean? In simple terms, homoiconic code is when code is treated as data and can be manipulated as you would data. This means the code can be changed, new functions and variables added, the code can generate new code or even examine and modify its own structure and behavior all while it is running. That’s why homoiconic languages like Lisp are so powerful. But what if we can make a homoiconic python code, where the code and the data are one and the same and can be modified in the same way?\n  \n    This guide does a good job in trying to explain how you would create a python version of the “Lisp in Lisp” code which would give you access to all those homoiconic features that Lisp brags of like the macro systems, the expressiveness and flexibility, the metaprogramming etc. while still using python. What do you guys think of this?"
},
{
    "title": "No title",
    "content": "What My Project Does\n\nhttps://github.com/treykeown/arguably\n\narguably makes it super simple to define complex CLIs. It uses your function signatures and docstrings to set everything up. Here's how it works:\n  \n\n\n    Adding the @arguably.command decorator to a function makes it appear on the CLI.\n  \n\n\n    If multiple functions are decorated, they'll all be set up as subcommands. You can even set up multiple levels of subcommands.\n  \n\n\n    The function name, signature, and docstring are used to automatically set up the CLI\n  \n\n\n    Call arguably.run() to parse the arguments and invoke the appropriate command\n  \n\n\n    A small example:\n  #!/usr/bin/env python3\nimport arguably\n\n@arguably.command\ndef some_function(required, not_required=2, *others: int, option: float = 3.14):\n    \"\"\"\n    this function is on the command line!\n\n    Args:\n        required: a required argument\n        not_required: this one isn't required, since it has a default value\n        *others: all the other positional arguments go here\n        option: [-x] keyword-only args are options, short name is in brackets\n    \"\"\"\n    print(f\"{required=}, {not_required=}, {others=}, {option=}\")\n\nif __name__ == \"__main__\":\n    arguably.run()\n    becomes\n  user@machine:~$ ./readme-1.py -h\nusage: readme-1.py [-h] [-x OPTION] required [not-required] [others ...]\n\nthis function is on the command line!\n\npositional arguments:\n  required             a required parameter (type: str)\n  not-required         this one isn't required, since it has a default (type: int, default: 2)\n  others               all the other positional arguments go here (type: int)\n\noptions:\n  -h, --help           show this help message and exit\n  -x, --option OPTION  an option, short name is in brackets (type: float, default: 3.14)\n    It can easily hand some very complex cases, like passing in QEMU-style arguments to automatically instantiated different types of classes:\n  user@machine:~$ ./readme-2.py --nic tap,model=e1000 --nic user,hostfwd=tcp::10022-:22\nnic=[TapNic(model='e1000'), UserNic(hostfwd='tcp::10022-:22')]\n    You can also auto-generate a CLI for your script through python3 -m arguably your_script.py, more on that here.\n  \nTarget Audience\n\n    If you're writing a script or tool, and you need a quick and effective way to run it from the command line, arguably was made for you. It's great for things where a CLI is essential, but doesn't need tons of customization. arguably makes some opinionated decisions that keep things simple for you, but doesn't expose ways of handling things like error messages.\n  \n    I put in the work to create GitHub workflows, documentation, and proper tests for arguably. I want this to be useful for the community at large, and a tool that you can rely on. Let me know if you're having trouble with your use case!\n  \nComparison\n\n    There are plenty of other tools for making CLIs out there. My goal was to build one that's unobtrusive and easy to integrate. I wrote a whole page on the project goals here: https://treykeown.github.io/arguably/why/\n\n    A quick comparison:\n  \n\n\nargparse - this is what arguably uses under the hood. The end user experience should be similar - arguably just aims to make it easy to set up.\n  \n\n\nclick - a powerhouse with all the tools you'd ever want. Use this if you need extensive customization and don't mind some verbosity.\n  \n\n\ntyper - also a great option, and some aspects are similar design-wise. It also uses functions with a decorator to set up commands, and also uses the function signature. A bit more verbose, though like click, has more customization options.\n  \n\n\nfire - super easy to generate CLIs. arguably tries to improve on this by utilizing type hints for argument conversion, and being a little more of a middle ground between this and the more traditional ways of writing CLIs in Python.\n  \n\n\n    This project has been a labor of love to make CLI generation as easy as it should be. Thanks for checking it out!"
},
{
    "title": "No title",
    "content": "With Python i created a tool that enables users to download LinkedIn Learning courses, including the often overlooked but incredibly valuable exercise files. This feature sets our project apart, offering a complete learning experience by providing both the course videos and the materials needed for practical application.What great about it and beyond other tools in the same genre concerned LinkedIn Learning Downloaders, now you can download the whole courses from a path link. this is was never possible without Python.\n  \n     For more detailed information, visit the repo : https://github.com/M0r0cc4nGh0st/LinkedIn-Learning-Downloader"
},
{
    "title": "No title",
    "content": "Hello there, I just created a template for creating a backend for your SaaS products.\n  \nWhat my project does: It is a FastAPI project/template for creating SaaS backends and admin dashboards.\n  \nComparison: Out of the box, it supports\n  \n\n\n    Licence key generation and validation.\n  \n\n\n    OAuth 2 authentication with scopes.\n  \n\n\n    Endpoints with pagination and filters to easily integrate with an admin dashboard.\n  \n\n\n    Passwords are securely stored using hashing.\n  \n\n\n    used PostgreSQL for database\n  \n\n\nTarget Audience: Production\n  \nCheck it here!\n\nUpdate 1: Added pre-commit hooks, tox for testing and linting."
},
{
    "title": "No title",
    "content": "Recently, I found out about the this \"Easter egg\" in python3. Adding import this into a py file will print \"The Zen of Python\" by Tim Peters. Also, this has two attributes: this.s and this.d, which I guess form the actual Easter egg. this.s returns an encrypted version of \"The Zen\" and this.d well, see for yourself, maybe you'll solve the puzzle."
},
{
    "title": "No title",
    "content": "I made a YouTube video which previews the zoom and explains the code, which you can find here: https://youtu.be/HtNUFdh2sjg\n\nWhat my project does: it creates a Mandelbrot Zoom.\n  \nComparison: it uses Pillow and consists of just 2 main blocks of code: one is the main function that finds which points are in the Mandelbrot Set and the other is the main loop that applies appropriate colors to each image. It gives the option of being black and white OR in color.\n  \n    It works fairly well but can definitely be faster if parallelized. I'd love to hear any suggestions on how it can be improved.\n  \nTarget Audience: fun/toy project\n  \n    Source code is here: https://github.com/AbideByReason/Python_Notebooks/tree/main"
},
{
    "title": "No title",
    "content": "What the project does: data animation library for time-series data. Currently it supports the following chart types:\n  \n\n\n    Bar races\n  \n\n\n    Animated Pie Charts\n  \n\n\n    Animated Line Charts\n  \n\n\n    Animated Stacked Area Charts\n  \n\n\n    Animated (World) Maps\n  \n\n\n    You can find some simple example charts here: https://www.sjdataviz.com/software\n\n    It is on pypi, you can install it using:\n  \npip install sjvisualizer\n\n    It is fully based on TkInter to draw the graph shapes to the screen, which gives a lot of flexibility. You can also mix and match the different chart types in a single animation.\n  \n    Target audience: people interested in data animation for presentations or social media content creation\n  \n    Alternatives: I only know one alternative which is bar-chart-race, the ways sjvisualizer is better:\n  \n\n\n    Smoother animation, bar-chart-race isn't the quite choppy I would say\n  \n\n\n    Load custom icons for each data category (flag icons for countries for example)\n  \n\n\n    Number of supported chart types\n  \n\n\n    Mix and match different chart types in a single animation, have a bar race to show the ranking, and a smaller pie chart showing the percentages of the whole\n  \n\n\n    Based on TkInter, easy to add custom elements through the standard python GUI library\n  \n\n\n    Topics to improve (contributions welcome):\n  \n\n\n    Documentation\n  \n\n\n    Improve built in screen recorder, performance takes a hit when using the built in screen recorder\n  \n\n\n    Additional chart types: bubble charts, lollipop charts, etc\n  \n\n\n    Improve the way data can be loaded into the library (currently only supports reading into a dataframe from Excel)\n  \n\n\n    Sorry for the long post, you can find it here on GitHub: https://github.com/SjoerdTilmans/sjvisualizer"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "The Python on Microcontrollers (and Raspberry Pi) Newsletter: subscribe for free\n    With the Python on Microcontrollers newsletter, you get all the latest information on Python running on hardware in one place! MicroPython, CircuitPython and Python on single Board Computers like Raspberry Pi & many more.\n  \nThe Python on Microcontrollers newsletter is the place for the latest news. It arrives Monday morning with all the week’s happenings. No advertising, no spam, easy to unsubscribe.\n\n10,998 subscribers - the largest Python on hardware newsletter out there. (2 more for 11k!)\n  \nCatch all the weekly news on Python for Microcontrollers with adafruitdaily.com.\n\n    This ad-free, spam-free weekly email is filled with CircuitPython, MicroPython, and Python information that you may have missed, all in one place!\n  \n    Ensure you catch the weekly Python on Hardware roundup– you can cancel anytime – try our spam-free newsletter today!\n  \nhttps://www.adafruitdaily.com/"
},
{
    "title": "No title",
    "content": "Check this video tutorial to explore different AutoEDA python packages like pandas-profiling, sweetviz, dataprep,etc which can enable automatic data analysis within minutes without any effort : https://youtu.be/Z7RgmM4cI2I?si=8GGM50qqlN0lGzry"
},
{
    "title": "No title",
    "content": "Hey! Messing around with instaviz, cool library, highly recommend. You can visualize a function's bytecode as well as AST and some other stuff.\n  \n    i entered this:\n  def f():\n  x = 1 + 2 - 10**2\n  return x\n    I was expecting the AST nodes for 1 + 2 - 10**2 to be rearranged somehow, with 10**2 being moved to the left hand of the expression, because exponents get evaluated before addition/subtraction. but no! just looks like this:\n  \n    ... (more tree up here)\n  \n    BinOp\n  \n    |                    \\                 \\\n  \n    BinOp          Sub             BinOp|    \\      \\                           /    |    \\\n  \n    1  ADD  2                       10 POW 2\n  \n    I was assuming operator precedence was implemented as the AST level. Seems no - I would assume that the tree would've had the 10 POW 2 on the left. Does it happen at the control flow graph phase? I can imagine the interpreter itself handles it.\n  \n    danke danke danke danke"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "https://github.com/nevakrien/time_machine_pip\n\n    this is a fairly simple project barely anything to it but I think its promisingthe idea is to put pip in a time machine so it can not use package versions that were made after the project is made.\n  \n    I am doing this by proxiying pypi and cutting out the newer versions.\n  \n    initial tests show that pip respects the proxy and works like you would expect"
},
{
    "title": "No title",
    "content": "Motivation\n    Chatbots are among the most popular applications of large language models (LLMs). Often, an LLM's internal knowledge base is adequate for answering users questions. However, in those cases, the model may generate outdated, incorrect, or too generic responses when specificity is expected. These challenges can be partially addressed by supplementing the LLM with an external knowledge base and employing the retrieval-augmented generation (RAG) technique.\n  \n    However, if user queries are complex, it may be necessary to break the task into several sub-parts. In such cases, relying solely on the RAG technique may not be sufficient, and the use of agents may be required.\n  \n    The fundamental concept of agents involves using a language model to determine a sequence of actions (including the usage of external tools) and their order. One possible action could be retrieving data from an external knowledge base in response to a user's query. In this tutorial, we will develop a simple Agent that accesses multiple data sources and invokes data retrieval when needed. We will use a Dingo framework that allows the development of LLM pipelines and autonomous agents.\n  RAG Agent Architecture and Technical Stack\n    The application will consist of the following components:\n  \n\n\nStreamlit: provides a frontend interface for users to interact with a chatbot.\n  \n\n\nFastAPI: facilitates communication between the frontend and backend.\n  \n\n\nDingo Agent: agent powered by GPT-4 Turbo model from OpenAI that has access to provided knowledge bases and invokes data retrieval from them if needed.\n  \n\n\n    LLMs docs: a vector store containing documentation about the recently released Phi-3 (from Microsoft) and Llama 3 (from Meta) models.\n  \n\n\n    Audio gen docs: a vector store containing documentation about the recently released OpenVoice model from MyShell.\n  \n\n\nEmbedding V3 small model from OpenAI: computes text embeddings.\n  \n\n\nQDrant: vector database that stores embedded chunks of text.\n  \n\nImplementationStep 0:\n    Install the Dingo framework:\n  pip install agent-dingo\n    Set the OPENAI_API_KEY environment variable to your OpenAI API key:\n  export OPENAI_API_KEY=your-api-keyStep 1:\n    Create a component.py file, and initialize an embedding model, a chat model, and two vector stores: one for storing documentation of Llama 3 and Phi-3, and another for storing documentation of OpenVoice.\n  # component.py\nfrom agent_dingo.rag.embedders.openai import OpenAIEmbedder\nfrom agent_dingo.rag.vector_stores.qdrant import Qdrant\nfrom agent_dingo.llm.openai import OpenAI\n\n# Initialize an embedding model\nembedder = OpenAIEmbedder(model=\"text-embedding-3-small\")\n\n# Initialize a vector store with information about Phi-3 and Llama 3 models\nllm_vector_store = Qdrant(collection_name=\"llm\", embedding_size=1536, path=\"./qdrant_db_llm\")\n\n# Initialize a vector store with information about OpenVoice model\naudio_gen_vector_store = Qdrant(collection_name=\"audio_gen\", embedding_size=1536, path=\"./qdrant_db_audio_gen\")\n\n# Initialize an LLM\nllm = OpenAI(model = \"gpt-3.5-turbo\")Step 2:\n    Create a build.py file. Parse, chunk into smaller pieces, and embed websites containing documentation of the above-mentioned models. The embedded chunks are used to populate the corresponding vector stores.\n  # build.py\nfrom components import llm_vector_store, audio_gen_vector_store, embedder\nfrom agent_dingo.rag.readers.web import WebpageReader\nfrom agent_dingo.rag.chunkers.recursive import RecursiveChunker\n\n# Read the content of the websites\nreader = WebpageReader()\nphi_3_docs = reader.read(\"https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/\")\nllama_3_docs = reader.read(\"https://ai.meta.com/blog/meta-llama-3/\")\nopenvoice_docs = reader.read(\"https://research.myshell.ai/open-voice\")\n\n# Chunk the documents\nchunker = RecursiveChunker(chunk_size=512)\nphi_3_chunks = chunker.chunk(phi_3_docs)\nllama_3_chunks = chunker.chunk(llama_3_docs)\nopenvoice_chunks = chunker.chunk(openvoice_docs)\n\n# Embed the chunks\nfor doc in [phi_3_chunks, llama_3_chunks, openvoice_chunks]:\n    embedder.embed_chunks(doc)\n\n# Populate LLM vector store with embedded chunks about Phi-3 and Llama 3\nfor chunk in [phi_3_chunks, llama_3_chunks]:\n    llm_vector_store.upsert_chunks(chunk)\n\n# Populate audio gen vector store with embedded chunks about OpenVoice\naudio_gen_vector_store.upsert_chunks(openvoice_chunks)\n    Run the script:\n  python build.py\n    At this step, we have successfully created vector stores.\n  Step 3:\n    Create serve.py file, and build a RAG pipeline. To access the pipeline from the Streamlit application, we can serve it using the serve_pipeline function, which provides a REST API compatible with the OpenAI API.\n  # serve.py\nfrom agent_dingo.agent import Agent\nfrom agent_dingo.serve import serve_pipeline\nfrom components import llm_vector_store, audio_gen_vector_store, embedder, llm\n\nagent = Agent(llm, max_function_calls=3)\n\n# Define a function that an agent can call if needed\nu/agent.function\ndef retrieve(topic: str, query: str) -> str:\n    \"\"\"Retrieves the documents from the vector store based on the similarity to the query.\n    This function is to be used to retrieve the additional information in order to answer users' queries.\n\n    Parameters\n    ----------\n    topic : str\n        The topic, can be either \"large_language_models\" or \"audio_generation_models\".\n        \"large_language_models\" covers the documentation of Phi-3 family of models from Microsoft and Llama 3 model from Meta.\n        \"audio_generation_models\" covers the documentation of OpenVoice voice cloning model from MyShell.\n        Enum: [\"large_language_models\", \"audio_generation_models\"]\n    query : str\n        A string that is used for similarity search of document chunks.\n\n    Returns\n    -------\n    str\n        JSON-formatted string with retrieved chunks.\n    \"\"\"\n    print(f'called retrieve with topic {topic} and query {query}')\n    if topic == \"large_language_models\":\n        vs = llm_vector_store\n    elif topic == \"audio_generation_models\":\n        vs = audio_gen_vector_store\n    else:\n        return \"Unknown topic. The topic must be one of `large_language_models` or `audio_generation_models`\"\n    query_embedding = embedder.embed(query)[0]\n    retrieved_chunks = vs.retrieve(k=5, query=query_embedding)\n    print(f'retrieved data: {retrieved_chunks}')\n    return str([chunk.content for chunk in retrieved_chunks])\n\n# Create a pipeline\npipeline = agent.as_pipeline()\n\n# Serve the pipeline\nserve_pipeline(\n    {\"gpt-agent\": pipeline},\n    host=\"127.0.0.1\",\n    port=8000,\n    is_async=False,\n)\n    Run the script:\n  python serve.py\n    At this stage, we have an openai-compatible backend with a model named gpt-agent, running on http://127.0.0.1:8000/. The Streamlit application will send requests to this backend.\n  Step 4:\n    Create app.py file, and build a chatbot UI:\n  # app.py\nimport streamlit as st\nfrom openai import OpenAI\n\nst.title(\"🦊 Agent\")\n\n# provide any string as an api_key parameter\nclient = OpenAI(base_url=\"http://127.0.0.1:8000\", api_key=\"123\")\n\nif \"openai_model\" not in st.session_state:\n    st.session_state[\"openai_model\"] = \"gpt-agent\"\nif \"messages\" not in st.session_state:\n    st.session_state.messages = []\n\nfor message in st.session_state.messages:\n    avatar = \"🦊\" if message[\"role\"] == \"assistant\" else \"👤\"\n    with st.chat_message(message[\"role\"], avatar=avatar):\n        st.markdown(message[\"content\"])\n\nif prompt := st.chat_input(\"How can I assist you today?\"):\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n    with st.chat_message(\"user\", avatar=\"👤\"):\n        st.markdown(prompt)\n\n    with st.chat_message(\"assistant\", avatar=\"🦊\"):\n        stream = client.chat.completions.create(\n            model=st.session_state[\"openai_model\"],\n            messages=[\n                {\"role\": m[\"role\"], \"content\": m[\"content\"]}\n                for m in st.session_state.messages\n            ],\n            stream=False,\n        )\n        response = st.write_stream((i for i in stream.choices[0].message.content))\n    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n    Run the application:\n  streamlit run app.py\n    🎉 We have successfully built an Agent that is augmented with the technical documentation of several newly released generative models and can retrieve information from these documents if necessary. Let’s ask some technical questions, and check the generated output:\n  Conclusion\n    In this tutorial, we have developed a RAG agent that can access external knowledge bases, selectively decide whether to access the external data, which data source to use (and how many times), and how to rewrite the user's query before retrieving the data.\n  \n    It can be seen that the Dingo framework enhances the development of LLM-based applications by allowing developers to quickly and easily create application prototypes."
},
{
    "title": "No title",
    "content": "What are all the changes needs to be done when I change the version of Wergzeug from 2.3.8 to 3.0.0 ?\n  \n    There are some CVE fixes available in the latest 3.x version of werkzueg. To take the fixes as part of my code, we want to upgrade the version. When I do so, I’ve faced lot of breakages. I found some on documents and release notes. But it would be easier if someone already did some changes regarding this."
},
{
    "title": "No title",
    "content": "Hi,\n  \n    Recently I played a bit with LLMs, specifcally exploring ways of running the models locally and building prompts using LangChain. As a result ended up coding a small recommendation system, powered with Llama3-7b model, which suggests topics to read on HackerNews.\n  \n    Wanted to share my experiences, so I wrote a small article where I described all my findings.Hope you'll like it: https://lukaszksiezak.github.io/ScrapyToLLM/\n\n    Github repo: https://github.com/lukaszksiezak/ScrapyToLLM\n\nWhat the project does:\n\n    It's a Python application which uses scrapy to scrape HackerNews page. Scraped articles are pipelined to redis, which is then feeding Llama3 using langchain. Prompter is configured to serve a user articles which are matching his request.\n  \nTarget Audience:\n  \n    I think it suits the best all the people who are looking for a Hello World projects using LLMs. I think it also reveals some difficulties related to LLM tech, what potential problems could be found in production systems.\n  \nComparison:\n\n    Recommendation systems are widely used and known, however LLMs are the ones which may work out of the box when appropriate prompt is given. It's kind of interesting to explore various usages of the technology and take part in fast grow of that stack.\n  \n    Cheers."
},
{
    "title": "No title",
    "content": "Official Event"
},
{
    "title": "No title",
    "content": "Miguel Algorri and Arnau Pont Vílchez here, blat co-founders!\n  \nTarget AudiencePeople who need to collect public data from the web (pricing, articles, reviews, leads etc).\n  \nWhat does our Project Do?At blat we aim to deliver production-ready web scraping code in minutes (written in Python, Scrapy framework).This is feasible thanks to our Web Scraping AI Agent 🧠. Here our CLI to interact with the Web Scraping AI Agent (github). Too good to be true? Check our video\n\nComparisonThere are lots of other tools in the market, like Zyte, Apify, Kadoa. All those are great tools for web scraping purposes. The main difference with our competitors is that we give you the Python code that's ready to use (you host it, you run it). Also, once created, the code does not use AI for parsing HTMLs, so it's more efficient and deterministic.\n  \nWhat are we looking for?We encourage you to register as a alpha testers 💪 if you are willing to have a better and more automated web scraping experience. \n  \nHere our CLI to interact with the Web Scraping AI Agent (github)"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    A few days ago I wrote a simple python script (\"Atlas\") that turns the Apple Health export.xml file (which is about 1 GB in my case, with about 10 years of data) into a very simple parquet file (a bit like a compressed CSV) that is also way smaller (40 MB).\n  \n    The parquet file has 5 columns:\n  \n\n\n    type (e.g. \"CyclingDistance\")\n  \n\n\n    value (e.g. \"12.100\")\n  \n\n\n    and 3 datetime timestamps:\n  \n\n\n    start\n  \n\n\n    end\n  \n\n\n    created\n  \n\n\n    This makes it way easier to do data exploration. Here are a few example charts I generated using Clickhouse (chDB) and Vega-Altair in a Quarto notebook.\n  \n    Step Count:\n  \nhttps://x.com/__tosh/status/1785397655784337684\n\n    Environmental Noise:\n  \nhttps://x.com/__tosh/status/1787530483208786029\n\n    Sleep Duration & States:\n  \nhttps://x.com/__tosh/status/1786505867438768254\n\n    Coffee Consumption:\n  \nhttps://x.com/__tosh/status/1783906333911076996\n\n    Coffee after 17:00:\n  \nhttps://twitter.com/__tosh/status/1789304034442043421/photo/1\n\nTarget Audience\n\n    For everyone who would like to explore their own Apple Health data or see how to work with a simple .parquet file using Clickhouse (chDB), Vega-Altair and Quarto.\n  \n    Quarto notebook: https://github.com/atlaslib/atlas/blob/main/examples/apple-health-exploration-clickhouse-chdb-altair-quarto/index.qmd\n\n    In the repo on Github I've added also added instructions for how to get your export.xml file from Apple Health and how to install the python script via pip to use it as a command line tool:\n  \nhttps://github.com/atlaslib/atlas\n\n    (⭐️ star to stay tuned for updates)\n  \n    Curious if you have charts that you would be interested in. Happy to add more examples over the next days!\n  \nComparison\n\n    This is me playing around with the data and wrapping the script up in a pip package to make it easier for others to install and use.\n  \n    You can also explore the data in the Apple Health app but why would you if you can also explore it with your favorite programming language?"
},
{
    "title": "No title",
    "content": "Hello,\n  \n    I made this Minesweeper bot that I wanted to share with you all.\n  \nWhat My Project Does -\n  \n    The bot takes a screenshot of the board and runs a classification algorithm to extract the contents of the board. It then analyzes the board, finds as many mines as it can, and sends clicks. If it cannot find any mines then it guesses the most probable position of a mine.\n  \nTarget Audience -\n  \n    It's a toy project for anyone interested in algorithms or problem-solving.\n  \nComparison -\n  \n    This is just my attempt at making a minesweeper bot. I'm sure there are many bots out there that are much more efficient than this.\n  \n    do let me know, if you feel anything can be done better :)"
},
{
    "title": "No title",
    "content": "Hey guys, I have worked on building multiple ai/ml usecases and their specific backends. But now I want build interfaces for easy and quick integration. I saw a blog which used FastUI which looks quick decent but when I tried it just showed me a Json of elements on the page. Are there any other libraries I should use? 🤔"
},
{
    "title": "No title",
    "content": "So it was a long time ago in the good old Python 2.x days (circa 2010 probably) that I had learned PyGame with some tutorials at my former work place. But nowadays since I mostly freelance with business apps, I never felt the need for it.\n  \n    But since such a game development project is on the horizon after all these years, I was wondering if PyGame can still be up for the task with Python 3.x? Or is there a better Python library available these days?\n  \n    I don't need any advanced gaming features of modern day VFX or anything, all I need is some basic Mario/Luigi style graphics, that's all!"
},
{
    "title": "No title",
    "content": "If you have ever wanted to track the size of your PyInstaller packages in CI, Bencher now supports tracking your package size: https://bencher.dev/docs/how-to/track-file-size/"
},
{
    "title": "No title",
    "content": "My wife and I use the Huckleberry app to track our baby's sleep periods. Although the free version of the app allows you to view a number of sleep-related metrics, I also wanted to see whether his longest nightly sleep stretches were getting longer over time. Therefore, I created a Python project to help me answer this and other questions I had about my baby's sleep.\n  What My Project Does\n    This project reads in data from a Huckleberry .csv export (or a separate custom .csv file); analyzes its sleep information; and then produces a number of visualizations.\n  \n    Personally, I've found that running the code and viewing its output helps reassure me that our baby is making progress with sleep, even if he seems to have some setbacks now and then! I hope you'll find it useful as well in evaluating the effectiveness of your sleep training approach.\n  Target Audience\n    This project can be useful for any parent who wishes to see how his or her baby's sleep is improving over time. (It could be used for other age ranges as well, but the code and visualizations are geared towards infant sleep data.)\n  \nThe project's readme has instructions on using the code to track your own baby's sleep data.\n  Comparison\n    This project is released under the open-source MIT license, so you are welcome to use and modify it for free. (I imagine that this is not the case for many sleep analysis tools.)\n  \n    As noted earlier, the project allows you to see how your baby's longest daily sleep stretch has improved over time. (The longer your baby sleeps at any given point, the longer you get to sleep, so I think this metric is of great interest to most parents!) I don't think the free version of Huckleberry includes this data in line chart form, though you can get a sense of this improvement by scrolling through your daily sleep data.\n  \n    This script also separates individaul sleep entries into their respective daytime and nighttime components. For instance, if your baby slept from 6 AM to 9 AM, and you've specified the nighttime period to end at 7 AM (the default setting), the script will treat this entry as one hour of nighttime sleep and 2 hours of daytime sleep. I don't think Huckleberry offers this same functionality, though I could be wrong.\n  \n    (Note: The sample data shown within the project is completely made up using another Python script, and is not meant to reflect normal sleep patterns in infants.)"
},
{
    "title": "No title",
    "content": "I'm working on an open-source framework for converting Python notebooks into web applications, it is called Mercury. Recently, I have added an option to execute notebooks with REST API. You can pass paramters in POST request body to the notebook, execute all cells and return JSON as response. I'm also running a SaaS service, Mercury Cloud where you can deploy notebooks with one-click with unique website domain.\n  What My Project Does\n    It makes Python notebooks extermely easy to integrate with custom ChatGPT, so GPT can execute Python notebooks and get response. It is 3-steps process:\n  \n\n\n    Create Python notebook, with Mercury widgets to accept parameters and return JSON response.\n  \n\n\n    Deploy notebook online - it can be done in 1 minute with Mercury Cloud\n\n\n\n    Configure ChatGPT Actions - it is quick, because Mercury automatically generates OpenAPI schema based on your notebooks.\n  \n\n\n    You can read more in article how ChatGPT is talking with all my Python notebooks.\n  Target Audience\n    This solution is perfect for people:\n  \n\n\n    that would like to quickly create custom API for ChatGPT with Python,\n  \n\n\n    that don't want to manage server by themself.\n  \n\nComparison\n    I think that building API with Python notebooks is alternative for full REST API development with Django, Flask or FastAPI, if you quickly need few endpoints that will expose your Python code.\n  Examples\n    I have created example notebooks that are used by ChatGPT:\n  \n\n\nnotebook to send email directly from ChatGPT\n\n\n\nnotebook to query Postgres database with SQL from ChatGPT\n\n\n\nnotebook to access Google Sheets in custom ChatGPT"
},
{
    "title": "No title",
    "content": "Hello guys\n  \n    I recently decided to move from nodejs(expressjs) to python for general purposes but mostly for backend. I have couple of questions.\n  \n\n\n    Will i regret my migration to python? :)\n  \n\n\n    Which framework you suggest for backend solo dev?\n  \n\n\n    And what tips are you suggesting me in general to get used to python."
},
{
    "title": "No title",
    "content": "Hey there, you are probably familiar with REST APIs.We at dlt library added a new way to get data from apis (and dlt can already load it with best practice to db or parquet). We already did some internal hackathons but we would appreciate your feedback so we can improve it further- Our new REST API Source is a short, declarative configuration driven way of creating sources.- Our new REST API Client is a collection of Python helpers used by the above source, which you can also use as a standalone, config-free, imperative high-level abstraction for building pipelines.You can read more about the source here or go to our docs for the REST APIClient infohttps://github.com/dlt-hub/verified-sources/tree/master/sources/rest_apiPS: see you at Pycon Pittsburgh!"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What My Project Does:I'm excited to introduce my latest project built with Python – an interactive data visualization application using Plotly Dash. This project aims to empower users to explore and analyze datasets dynamically through interactive visualizations. By leveraging Plotly Dash's capabilities, users can interact with data in real-time, customize visualizations on the fly, and gain deeper insights with just a few clicks.\n  \nTarget Audience:This project caters to a wide range of users, from data enthusiasts and analysts to professionals seeking to communicate insights effectively. Whether you're a data scientist exploring patterns in large datasets or a business analyst presenting findings to stakeholders, this tool is designed to streamline your workflow and enhance your data storytelling capabilities. It's suitable for both production-grade applications and educational purposes, offering a versatile platform for data visualization tasks of varying complexity.\n  \nComparison:Unlike traditional static charts or cumbersome data exploration tools, this Plotly Dash application stands out for its interactivity and flexibility. While existing alternatives may offer basic charting functionalities, they often lack the dynamic capabilities required for in-depth data exploration. With Plotly Dash, users can manipulate charts in real-time, zoom in on specific data points, filter datasets dynamically, and even integrate interactive components like dropdowns and sliders for a more immersive experience. This project takes data visualization to the next level by providing a user-friendly interface coupled with powerful interactivity, setting it apart as a top choice for visualizing and analyzing datasets.\n  \nSource Code:You can access the source code for this project on GitHub: Interactive Data Visualization with Plotly Dash\n\nWebsite:For more information and to see the project in action, visit : https://www.aspiresoftserv.com/"
},
{
    "title": "No title",
    "content": "https://slint.dev/blog/slint-1.6-released\n\n    Slint is a declarative GUI toolkit to build native user interfaces for desktop and embedded applications. Find more information at https://slint.dev/ or check out the source code at https://github.com/slint-ui/slint.\n  \n    EDIT: The Python APIs are currently in alpha. More info -- https://github.com/slint-ui/slint/tree/master/api/python"
},
{
    "title": "No title",
    "content": "Hi,\n  \n    Just want to know how difficult is it to manage your own pypi clone and how do you recommend to create a seperation between dev and prod systems."
},
{
    "title": "No title",
    "content": "🦑 Squid Proxy Manager Script\n    Hello fellow Python enthusiasts!\n  \n    I've created a Python script that makes managing your Squid Proxy Server a breeze. If you're looking for an efficient and straightforward way to interact with your Squid server remotely, this script is for you. 🎉\n  What My Project Does\n    The Squid Proxy Manager script allows you to manage your Squid Proxy Server remotely using a simple command-line interface. Here are some of the key features:\n  \n\n\nCheck Squid Service Status: Quickly check if your Squid service is running or not.\n  \n\n\nStart/Stop/Restart Service: Easily control the Squid service remotely.\n  \n\n\nView Logs: Access the latest entries in your Squid access logs.\n  \n\n\nView Configuration: Display the current Squid configuration file.\n  \n\n\nUpdate Configuration: Replace the existing Squid configuration with a new one.\n  \n\n\nReload Service: Reload the Squid service to apply changes without restarting.\n  \n\nTarget Audience\n    This script is designed for anyone who manages a Squid Proxy Server and prefers a command-line tool for remote management. If you are comfortable using Python and SSH, this tool will streamline your workflow and enhance your productivity.\n  Differences\n    Here are some aspects that make this Squid Proxy Manager script stand out:\n  \n\n\nRemote Management: Manage your Squid server without needing physical access, thanks to SSH connectivity.\n  \n\n\nEase of Use: The script provides a simple and intuitive command-line interface, making it easy to perform various tasks.\n  \n\n\nComprehensive Features: From checking service status to updating configurations and viewing logs, this script covers all essential Squid management tasks.\n  \n\n\nError Handling and Logging: Detailed logging and error handling ensure you know exactly what's happening and can troubleshoot issues effectively.\n  \n\n🚀 Usage\n\n\nInstallation:\n  \n\n\n    Ensure you have the required libraries installed:\n  pip install paramiko termcolor\n\n\n\n\nRunning the Script:\n  \n\n\n    Use the script with appropriate arguments to manage your Squid Proxy Server. Here's an example command to check the Squid service status:\n  ./squid_proxy_manager.py 192.168.2.111 22 username password --check-status\n\n\n\n\nUpdating Configuration:\n  \n\n\n    Create a new configuration file (e.g., new_squid.conf) with your desired settings.\n  \n\n\n    Run the script to update the Squid configuration:\n  ./squid_proxy_manager.py 192.168.2.111 22 username password --update-config new_squid.conf\n\n\n\n💻 Script Example\n    Here's a snippet of the script to give you an idea of its simplicity and functionality:\n  #!/usr/bin/env python3\n\nimport paramiko\nimport argparse\nimport logging\nimport sys\nimport os\nfrom termcolor import colored\n\nclass SquidProxyManager:\n    def __init__(self, hostname, port, username, password):\n        self.hostname = hostname\n        self.port = port\n        self.username = username\n        self.password = password\n        self.client = paramiko.SSHClient()\n        self.client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n\n    def connect(self):\n        try:\n            logging.info(colored(\"Attempting to connect to {}:{}\".format(self.hostname, self.port), 'cyan'))\n            self.client.connect(self.hostname, port=self.port, username=self.username, password=self.password)\n            logging.info(colored(f\"Connected to {self.hostname} on port {self.port}\", 'green'))\n        except Exception as e:\n            logging.error(colored(f\"Failed to connect: {e}\", 'red'))\n            sys.exit(1)\n\n    def disconnect(self):\n        self.client.close()\n        logging.info(colored(\"Disconnected from the server\", 'green'))\n\n    def execute_command(self, command):\n        logging.info(colored(\"Executing command: {}\".format(command), 'cyan'))\n        try:\n            stdin, stdout, stderr = self.client.exec_command(command)\n            stdout.channel.recv_exit_status()\n            out = stdout.read().decode()\n            err = stderr.read().decode()\n            if err:\n                logging.error(colored(f\"Error executing command '{command}': {err}\", 'red'))\n            else:\n                logging.info(colored(f\"Successfully executed command '{command}'\", 'green'))\n            return out, err\n        except Exception as e:\n            logging.error(colored(f\"Exception during command execution '{command}': {e}\", 'red'))\n            return \"\", str(e)\n\n    # More functions here...\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Squid Proxy Manager\")\n    parser.add_argument('hostname', help=\"IP address of the Squid proxy server\")\n    parser.add_argument('port', type=int, help=\"Port number for SSH connection\")\n    parser.add_argument('username', help=\"SSH username\")\n    parser.add_argument('password', help=\"SSH password\")\n    parser.add_argument('--check-status', action='store_true', help=\"Check Squid service status\")\n    parser.add.add_argument('--start', action='store_true', help=\"Start Squid service\")\n    parser.add.add_argument('--stop', action='store_true', help=\"Stop Squid service\")\n    parser.add.add_argument('--restart', action='store_true', help=\"Restart Squid service\")\n    parser.add.add_argument('--view-logs', action='store_true', help=\"View Squid logs\")\n    parser.add.add_argument('--view-config', action='store_true', help=\"View Squid configuration\")\n    parser.add.add_argument('--update-config', help=\"Update Squid configuration with provided data\")\n    parser.add.add_argument('--reload', action='store_true', help=\"Reload Squid service\")\n    return parser.parse_args()\n\ndef main():\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n    args = parse_args()\n    logging.info(colored(\"Initializing Squid Proxy Manager script\", 'cyan'))\n\n    manager = SquidProxyManager(args.hostname, args.port, args.username, args.password)\n    manager.connect()\n\n    try:\n        if args.check_status:\n            manager.check_squid_status()\n        if args.start:\n            manager.start_squid()\n        if args.stop:\n            manager.stop_squid()\n        if args.restart:\n            manager.restart_squid()\n        if args.view_logs:\n            manager.view_squid_logs()\n        if args.view_config:\n            manager.view_squid_config()\n        if args.update_config:\n            if not args.update_config.endswith('.conf'):\n                logging.error(colored(\"The provided file must have a .conf extension\", 'red'))\n            elif not os.path.isfile(args.update_config):\n                logging.error(colored(f\"Configuration file {args.update_config} not found\", 'red'))\n            else:\n                try:\n                    with open(args.update_config, 'r') as config_file:\n                        config_data = config_file.read()\n                    manager.update_squid_config(config_data)\n                except Exception as e:\n                    logging.error(colored(f\"Error reading configuration file {args.update_config}: {e}\", 'red'))\n        if args.reload:\n            manager.reload_squid()\n    finally:\n        manager.disconnect()\n        logging.info(colored(\"Squid Proxy Manager operations completed\", 'green'))\n\nif __name__ == \"__main__\":\n    main()🌟 Benefits\n\n\nRemote Management: No need to be physically present to manage your Squid server.\n  \n\n\nEase of Use: Simple command-line interface for quick operations.\n  \n\n\nVersatility: Supports various Squid management tasks, from checking status to updating configurations and viewing logs.\n  \n\n📢 Get Involved!\n    If you find this script useful, feel free to give it a try and share your feedback. Contributions and suggestions are always welcome! Comments however, that are unhelpful and serve no purpose to better the script or the author in their python scripting abilities are not welcome! Keep the nasty to yourself.\n  Access the script\n    You can find the script here on GitHub.\n  \n    Happy coding! 🚀"
},
{
    "title": "No title",
    "content": "The Python on Microcontrollers (and Raspberry Pi) Newsletter: subscribe for free\n    With the Python on Microcontrollers newsletter, you get all the latest information on Python running on hardware in one place! MicroPython, CircuitPython and Python on single Board Computers like Raspberry Pi & many more.\n  \nThe Python on Microcontrollers newsletter is the place for the latest news. It arrives Monday morning with all the week’s happenings. No advertising, no spam, easy to unsubscribe.\n\n10,998 subscribers - the largest Python on hardware newsletter out there. (2 more for 11k!)\n  \nCatch all the weekly news on Python for Microcontrollers with adafruitdaily.com.\n\n    This ad-free, spam-free weekly email is filled with CircuitPython, MicroPython, and Python information that you may have missed, all in one place!\n  \n    Ensure you catch the weekly Python on Hardware roundup– you can cancel anytime – try our spam-free newsletter today!\n  \nhttps://www.adafruitdaily.com/"
},
{
    "title": "No title",
    "content": "Check this video tutorial to explore different AutoEDA python packages like pandas-profiling, sweetviz, dataprep,etc which can enable automatic data analysis within minutes without any effort : https://youtu.be/Z7RgmM4cI2I?si=8GGM50qqlN0lGzry"
},
{
    "title": "No title",
    "content": "Hey! Messing around with instaviz, cool library, highly recommend. You can visualize a function's bytecode as well as AST and some other stuff.\n  \n    i entered this:\n  def f():\n  x = 1 + 2 - 10**2\n  return x\n    I was expecting the AST nodes for 1 + 2 - 10**2 to be rearranged somehow, with 10**2 being moved to the left hand of the expression, because exponents get evaluated before addition/subtraction. but no! just looks like this:\n  \n    ... (more tree up here)\n  \n    BinOp\n  \n    |                    \\                 \\\n  \n    BinOp          Sub             BinOp|    \\      \\                           /    |    \\\n  \n    1  ADD  2                       10 POW 2\n  \n    I was assuming operator precedence was implemented as the AST level. Seems no - I would assume that the tree would've had the 10 POW 2 on the left. Does it happen at the control flow graph phase? I can imagine the interpreter itself handles it.\n  \n    danke danke danke danke"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "https://github.com/nevakrien/time_machine_pip\n\n    this is a fairly simple project barely anything to it but I think its promisingthe idea is to put pip in a time machine so it can not use package versions that were made after the project is made.\n  \n    I am doing this by proxiying pypi and cutting out the newer versions.\n  \n    initial tests show that pip respects the proxy and works like you would expect"
},
{
    "title": "No title",
    "content": "Motivation\n    Chatbots are among the most popular applications of large language models (LLMs). Often, an LLM's internal knowledge base is adequate for answering users questions. However, in those cases, the model may generate outdated, incorrect, or too generic responses when specificity is expected. These challenges can be partially addressed by supplementing the LLM with an external knowledge base and employing the retrieval-augmented generation (RAG) technique.\n  \n    However, if user queries are complex, it may be necessary to break the task into several sub-parts. In such cases, relying solely on the RAG technique may not be sufficient, and the use of agents may be required.\n  \n    The fundamental concept of agents involves using a language model to determine a sequence of actions (including the usage of external tools) and their order. One possible action could be retrieving data from an external knowledge base in response to a user's query. In this tutorial, we will develop a simple Agent that accesses multiple data sources and invokes data retrieval when needed. We will use a Dingo framework that allows the development of LLM pipelines and autonomous agents.\n  RAG Agent Architecture and Technical Stack\n    The application will consist of the following components:\n  \n\n\nStreamlit: provides a frontend interface for users to interact with a chatbot.\n  \n\n\nFastAPI: facilitates communication between the frontend and backend.\n  \n\n\nDingo Agent: agent powered by GPT-4 Turbo model from OpenAI that has access to provided knowledge bases and invokes data retrieval from them if needed.\n  \n\n\n    LLMs docs: a vector store containing documentation about the recently released Phi-3 (from Microsoft) and Llama 3 (from Meta) models.\n  \n\n\n    Audio gen docs: a vector store containing documentation about the recently released OpenVoice model from MyShell.\n  \n\n\nEmbedding V3 small model from OpenAI: computes text embeddings.\n  \n\n\nQDrant: vector database that stores embedded chunks of text.\n  \n\nImplementationStep 0:\n    Install the Dingo framework:\n  pip install agent-dingo\n    Set the OPENAI_API_KEY environment variable to your OpenAI API key:\n  export OPENAI_API_KEY=your-api-keyStep 1:\n    Create a component.py file, and initialize an embedding model, a chat model, and two vector stores: one for storing documentation of Llama 3 and Phi-3, and another for storing documentation of OpenVoice.\n  # component.py\nfrom agent_dingo.rag.embedders.openai import OpenAIEmbedder\nfrom agent_dingo.rag.vector_stores.qdrant import Qdrant\nfrom agent_dingo.llm.openai import OpenAI\n\n# Initialize an embedding model\nembedder = OpenAIEmbedder(model=\"text-embedding-3-small\")\n\n# Initialize a vector store with information about Phi-3 and Llama 3 models\nllm_vector_store = Qdrant(collection_name=\"llm\", embedding_size=1536, path=\"./qdrant_db_llm\")\n\n# Initialize a vector store with information about OpenVoice model\naudio_gen_vector_store = Qdrant(collection_name=\"audio_gen\", embedding_size=1536, path=\"./qdrant_db_audio_gen\")\n\n# Initialize an LLM\nllm = OpenAI(model = \"gpt-3.5-turbo\")Step 2:\n    Create a build.py file. Parse, chunk into smaller pieces, and embed websites containing documentation of the above-mentioned models. The embedded chunks are used to populate the corresponding vector stores.\n  # build.py\nfrom components import llm_vector_store, audio_gen_vector_store, embedder\nfrom agent_dingo.rag.readers.web import WebpageReader\nfrom agent_dingo.rag.chunkers.recursive import RecursiveChunker\n\n# Read the content of the websites\nreader = WebpageReader()\nphi_3_docs = reader.read(\"https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/\")\nllama_3_docs = reader.read(\"https://ai.meta.com/blog/meta-llama-3/\")\nopenvoice_docs = reader.read(\"https://research.myshell.ai/open-voice\")\n\n# Chunk the documents\nchunker = RecursiveChunker(chunk_size=512)\nphi_3_chunks = chunker.chunk(phi_3_docs)\nllama_3_chunks = chunker.chunk(llama_3_docs)\nopenvoice_chunks = chunker.chunk(openvoice_docs)\n\n# Embed the chunks\nfor doc in [phi_3_chunks, llama_3_chunks, openvoice_chunks]:\n    embedder.embed_chunks(doc)\n\n# Populate LLM vector store with embedded chunks about Phi-3 and Llama 3\nfor chunk in [phi_3_chunks, llama_3_chunks]:\n    llm_vector_store.upsert_chunks(chunk)\n\n# Populate audio gen vector store with embedded chunks about OpenVoice\naudio_gen_vector_store.upsert_chunks(openvoice_chunks)\n    Run the script:\n  python build.py\n    At this step, we have successfully created vector stores.\n  Step 3:\n    Create serve.py file, and build a RAG pipeline. To access the pipeline from the Streamlit application, we can serve it using the serve_pipeline function, which provides a REST API compatible with the OpenAI API.\n  # serve.py\nfrom agent_dingo.agent import Agent\nfrom agent_dingo.serve import serve_pipeline\nfrom components import llm_vector_store, audio_gen_vector_store, embedder, llm\n\nagent = Agent(llm, max_function_calls=3)\n\n# Define a function that an agent can call if needed\nu/agent.function\ndef retrieve(topic: str, query: str) -> str:\n    \"\"\"Retrieves the documents from the vector store based on the similarity to the query.\n    This function is to be used to retrieve the additional information in order to answer users' queries.\n\n    Parameters\n    ----------\n    topic : str\n        The topic, can be either \"large_language_models\" or \"audio_generation_models\".\n        \"large_language_models\" covers the documentation of Phi-3 family of models from Microsoft and Llama 3 model from Meta.\n        \"audio_generation_models\" covers the documentation of OpenVoice voice cloning model from MyShell.\n        Enum: [\"large_language_models\", \"audio_generation_models\"]\n    query : str\n        A string that is used for similarity search of document chunks.\n\n    Returns\n    -------\n    str\n        JSON-formatted string with retrieved chunks.\n    \"\"\"\n    print(f'called retrieve with topic {topic} and query {query}')\n    if topic == \"large_language_models\":\n        vs = llm_vector_store\n    elif topic == \"audio_generation_models\":\n        vs = audio_gen_vector_store\n    else:\n        return \"Unknown topic. The topic must be one of `large_language_models` or `audio_generation_models`\"\n    query_embedding = embedder.embed(query)[0]\n    retrieved_chunks = vs.retrieve(k=5, query=query_embedding)\n    print(f'retrieved data: {retrieved_chunks}')\n    return str([chunk.content for chunk in retrieved_chunks])\n\n# Create a pipeline\npipeline = agent.as_pipeline()\n\n# Serve the pipeline\nserve_pipeline(\n    {\"gpt-agent\": pipeline},\n    host=\"127.0.0.1\",\n    port=8000,\n    is_async=False,\n)\n    Run the script:\n  python serve.py\n    At this stage, we have an openai-compatible backend with a model named gpt-agent, running on http://127.0.0.1:8000/. The Streamlit application will send requests to this backend.\n  Step 4:\n    Create app.py file, and build a chatbot UI:\n  # app.py\nimport streamlit as st\nfrom openai import OpenAI\n\nst.title(\"🦊 Agent\")\n\n# provide any string as an api_key parameter\nclient = OpenAI(base_url=\"http://127.0.0.1:8000\", api_key=\"123\")\n\nif \"openai_model\" not in st.session_state:\n    st.session_state[\"openai_model\"] = \"gpt-agent\"\nif \"messages\" not in st.session_state:\n    st.session_state.messages = []\n\nfor message in st.session_state.messages:\n    avatar = \"🦊\" if message[\"role\"] == \"assistant\" else \"👤\"\n    with st.chat_message(message[\"role\"], avatar=avatar):\n        st.markdown(message[\"content\"])\n\nif prompt := st.chat_input(\"How can I assist you today?\"):\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n    with st.chat_message(\"user\", avatar=\"👤\"):\n        st.markdown(prompt)\n\n    with st.chat_message(\"assistant\", avatar=\"🦊\"):\n        stream = client.chat.completions.create(\n            model=st.session_state[\"openai_model\"],\n            messages=[\n                {\"role\": m[\"role\"], \"content\": m[\"content\"]}\n                for m in st.session_state.messages\n            ],\n            stream=False,\n        )\n        response = st.write_stream((i for i in stream.choices[0].message.content))\n    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n    Run the application:\n  streamlit run app.py\n    🎉 We have successfully built an Agent that is augmented with the technical documentation of several newly released generative models and can retrieve information from these documents if necessary. Let’s ask some technical questions, and check the generated output:\n  Conclusion\n    In this tutorial, we have developed a RAG agent that can access external knowledge bases, selectively decide whether to access the external data, which data source to use (and how many times), and how to rewrite the user's query before retrieving the data.\n  \n    It can be seen that the Dingo framework enhances the development of LLM-based applications by allowing developers to quickly and easily create application prototypes."
},
{
    "title": "No title",
    "content": "What are all the changes needs to be done when I change the version of Wergzeug from 2.3.8 to 3.0.0 ?\n  \n    There are some CVE fixes available in the latest 3.x version of werkzueg. To take the fixes as part of my code, we want to upgrade the version. When I do so, I’ve faced lot of breakages. I found some on documents and release notes. But it would be easier if someone already did some changes regarding this."
},
{
    "title": "No title",
    "content": "Hi,\n  \n    Recently I played a bit with LLMs, specifcally exploring ways of running the models locally and building prompts using LangChain. As a result ended up coding a small recommendation system, powered with Llama3-7b model, which suggests topics to read on HackerNews.\n  \n    Wanted to share my experiences, so I wrote a small article where I described all my findings.Hope you'll like it: https://lukaszksiezak.github.io/ScrapyToLLM/\n\n    Github repo: https://github.com/lukaszksiezak/ScrapyToLLM\n\nWhat the project does:\n\n    It's a Python application which uses scrapy to scrape HackerNews page. Scraped articles are pipelined to redis, which is then feeding Llama3 using langchain. Prompter is configured to serve a user articles which are matching his request.\n  \nTarget Audience:\n  \n    I think it suits the best all the people who are looking for a Hello World projects using LLMs. I think it also reveals some difficulties related to LLM tech, what potential problems could be found in production systems.\n  \nComparison:\n\n    Recommendation systems are widely used and known, however LLMs are the ones which may work out of the box when appropriate prompt is given. It's kind of interesting to explore various usages of the technology and take part in fast grow of that stack.\n  \n    Cheers."
},
{
    "title": "No title",
    "content": "Official Event"
},
{
    "title": "No title",
    "content": "Miguel Algorri and Arnau Pont Vílchez here, blat co-founders!\n  \nTarget AudiencePeople who need to collect public data from the web (pricing, articles, reviews, leads etc).\n  \nWhat does our Project Do?At blat we aim to deliver production-ready web scraping code in minutes (written in Python, Scrapy framework).This is feasible thanks to our Web Scraping AI Agent 🧠. Here our CLI to interact with the Web Scraping AI Agent (github). Too good to be true? Check our video\n\nComparisonThere are lots of other tools in the market, like Zyte, Apify, Kadoa. All those are great tools for web scraping purposes. The main difference with our competitors is that we give you the Python code that's ready to use (you host it, you run it). Also, once created, the code does not use AI for parsing HTMLs, so it's more efficient and deterministic.\n  \nWhat are we looking for?We encourage you to register as a alpha testers 💪 if you are willing to have a better and more automated web scraping experience. \n  \nHere our CLI to interact with the Web Scraping AI Agent (github)"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    A few days ago I wrote a simple python script (\"Atlas\") that turns the Apple Health export.xml file (which is about 1 GB in my case, with about 10 years of data) into a very simple parquet file (a bit like a compressed CSV) that is also way smaller (40 MB).\n  \n    The parquet file has 5 columns:\n  \n\n\n    type (e.g. \"CyclingDistance\")\n  \n\n\n    value (e.g. \"12.100\")\n  \n\n\n    and 3 datetime timestamps:\n  \n\n\n    start\n  \n\n\n    end\n  \n\n\n    created\n  \n\n\n    This makes it way easier to do data exploration. Here are a few example charts I generated using Clickhouse (chDB) and Vega-Altair in a Quarto notebook.\n  \n    Step Count:\n  \nhttps://x.com/__tosh/status/1785397655784337684\n\n    Environmental Noise:\n  \nhttps://x.com/__tosh/status/1787530483208786029\n\n    Sleep Duration & States:\n  \nhttps://x.com/__tosh/status/1786505867438768254\n\n    Coffee Consumption:\n  \nhttps://x.com/__tosh/status/1783906333911076996\n\n    Coffee after 17:00:\n  \nhttps://twitter.com/__tosh/status/1789304034442043421/photo/1\n\nTarget Audience\n\n    For everyone who would like to explore their own Apple Health data or see how to work with a simple .parquet file using Clickhouse (chDB), Vega-Altair and Quarto.\n  \n    Quarto notebook: https://github.com/atlaslib/atlas/blob/main/examples/apple-health-exploration-clickhouse-chdb-altair-quarto/index.qmd\n\n    In the repo on Github I've added also added instructions for how to get your export.xml file from Apple Health and how to install the python script via pip to use it as a command line tool:\n  \nhttps://github.com/atlaslib/atlas\n\n    (⭐️ star to stay tuned for updates)\n  \n    Curious if you have charts that you would be interested in. Happy to add more examples over the next days!\n  \nComparison\n\n    This is me playing around with the data and wrapping the script up in a pip package to make it easier for others to install and use.\n  \n    You can also explore the data in the Apple Health app but why would you if you can also explore it with your favorite programming language?"
},
{
    "title": "No title",
    "content": "Hello,\n  \n    I made this Minesweeper bot that I wanted to share with you all.\n  \nWhat My Project Does -\n  \n    The bot takes a screenshot of the board and runs a classification algorithm to extract the contents of the board. It then analyzes the board, finds as many mines as it can, and sends clicks. If it cannot find any mines then it guesses the most probable position of a mine.\n  \nTarget Audience -\n  \n    It's a toy project for anyone interested in algorithms or problem-solving.\n  \nComparison -\n  \n    This is just my attempt at making a minesweeper bot. I'm sure there are many bots out there that are much more efficient than this.\n  \n    do let me know, if you feel anything can be done better :)"
},
{
    "title": "No title",
    "content": "Hey guys, I have worked on building multiple ai/ml usecases and their specific backends. But now I want build interfaces for easy and quick integration. I saw a blog which used FastUI which looks quick decent but when I tried it just showed me a Json of elements on the page. Are there any other libraries I should use? 🤔"
},
{
    "title": "No title",
    "content": "So it was a long time ago in the good old Python 2.x days (circa 2010 probably) that I had learned PyGame with some tutorials at my former work place. But nowadays since I mostly freelance with business apps, I never felt the need for it.\n  \n    But since such a game development project is on the horizon after all these years, I was wondering if PyGame can still be up for the task with Python 3.x? Or is there a better Python library available these days?\n  \n    I don't need any advanced gaming features of modern day VFX or anything, all I need is some basic Mario/Luigi style graphics, that's all!"
},
{
    "title": "No title",
    "content": "If you have ever wanted to track the size of your PyInstaller packages in CI, Bencher now supports tracking your package size: https://bencher.dev/docs/how-to/track-file-size/"
},
{
    "title": "No title",
    "content": "My wife and I use the Huckleberry app to track our baby's sleep periods. Although the free version of the app allows you to view a number of sleep-related metrics, I also wanted to see whether his longest nightly sleep stretches were getting longer over time. Therefore, I created a Python project to help me answer this and other questions I had about my baby's sleep.\n  What My Project Does\n    This project reads in data from a Huckleberry .csv export (or a separate custom .csv file); analyzes its sleep information; and then produces a number of visualizations.\n  \n    Personally, I've found that running the code and viewing its output helps reassure me that our baby is making progress with sleep, even if he seems to have some setbacks now and then! I hope you'll find it useful as well in evaluating the effectiveness of your sleep training approach.\n  Target Audience\n    This project can be useful for any parent who wishes to see how his or her baby's sleep is improving over time. (It could be used for other age ranges as well, but the code and visualizations are geared towards infant sleep data.)\n  \nThe project's readme has instructions on using the code to track your own baby's sleep data.\n  Comparison\n    This project is released under the open-source MIT license, so you are welcome to use and modify it for free. (I imagine that this is not the case for many sleep analysis tools.)\n  \n    As noted earlier, the project allows you to see how your baby's longest daily sleep stretch has improved over time. (The longer your baby sleeps at any given point, the longer you get to sleep, so I think this metric is of great interest to most parents!) I don't think the free version of Huckleberry includes this data in line chart form, though you can get a sense of this improvement by scrolling through your daily sleep data.\n  \n    This script also separates individaul sleep entries into their respective daytime and nighttime components. For instance, if your baby slept from 6 AM to 9 AM, and you've specified the nighttime period to end at 7 AM (the default setting), the script will treat this entry as one hour of nighttime sleep and 2 hours of daytime sleep. I don't think Huckleberry offers this same functionality, though I could be wrong.\n  \n    (Note: The sample data shown within the project is completely made up using another Python script, and is not meant to reflect normal sleep patterns in infants.)"
},
{
    "title": "No title",
    "content": "I'm working on an open-source framework for converting Python notebooks into web applications, it is called Mercury. Recently, I have added an option to execute notebooks with REST API. You can pass paramters in POST request body to the notebook, execute all cells and return JSON as response. I'm also running a SaaS service, Mercury Cloud where you can deploy notebooks with one-click with unique website domain.\n  What My Project Does\n    It makes Python notebooks extermely easy to integrate with custom ChatGPT, so GPT can execute Python notebooks and get response. It is 3-steps process:\n  \n\n\n    Create Python notebook, with Mercury widgets to accept parameters and return JSON response.\n  \n\n\n    Deploy notebook online - it can be done in 1 minute with Mercury Cloud\n\n\n\n    Configure ChatGPT Actions - it is quick, because Mercury automatically generates OpenAPI schema based on your notebooks.\n  \n\n\n    You can read more in article how ChatGPT is talking with all my Python notebooks.\n  Target Audience\n    This solution is perfect for people:\n  \n\n\n    that would like to quickly create custom API for ChatGPT with Python,\n  \n\n\n    that don't want to manage server by themself.\n  \n\nComparison\n    I think that building API with Python notebooks is alternative for full REST API development with Django, Flask or FastAPI, if you quickly need few endpoints that will expose your Python code.\n  Examples\n    I have created example notebooks that are used by ChatGPT:\n  \n\n\nnotebook to send email directly from ChatGPT\n\n\n\nnotebook to query Postgres database with SQL from ChatGPT\n\n\n\nnotebook to access Google Sheets in custom ChatGPT"
},
{
    "title": "No title",
    "content": "Hello guys\n  \n    I recently decided to move from nodejs(expressjs) to python for general purposes but mostly for backend. I have couple of questions.\n  \n\n\n    Will i regret my migration to python? :)\n  \n\n\n    Which framework you suggest for backend solo dev?\n  \n\n\n    And what tips are you suggesting me in general to get used to python."
},
{
    "title": "No title",
    "content": "Hey there, you are probably familiar with REST APIs.We at dlt library added a new way to get data from apis (and dlt can already load it with best practice to db or parquet). We already did some internal hackathons but we would appreciate your feedback so we can improve it further- Our new REST API Source is a short, declarative configuration driven way of creating sources.- Our new REST API Client is a collection of Python helpers used by the above source, which you can also use as a standalone, config-free, imperative high-level abstraction for building pipelines.You can read more about the source here or go to our docs for the REST APIClient infohttps://github.com/dlt-hub/verified-sources/tree/master/sources/rest_apiPS: see you at Pycon Pittsburgh!"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What My Project Does:I'm excited to introduce my latest project built with Python – an interactive data visualization application using Plotly Dash. This project aims to empower users to explore and analyze datasets dynamically through interactive visualizations. By leveraging Plotly Dash's capabilities, users can interact with data in real-time, customize visualizations on the fly, and gain deeper insights with just a few clicks.\n  \nTarget Audience:This project caters to a wide range of users, from data enthusiasts and analysts to professionals seeking to communicate insights effectively. Whether you're a data scientist exploring patterns in large datasets or a business analyst presenting findings to stakeholders, this tool is designed to streamline your workflow and enhance your data storytelling capabilities. It's suitable for both production-grade applications and educational purposes, offering a versatile platform for data visualization tasks of varying complexity.\n  \nComparison:Unlike traditional static charts or cumbersome data exploration tools, this Plotly Dash application stands out for its interactivity and flexibility. While existing alternatives may offer basic charting functionalities, they often lack the dynamic capabilities required for in-depth data exploration. With Plotly Dash, users can manipulate charts in real-time, zoom in on specific data points, filter datasets dynamically, and even integrate interactive components like dropdowns and sliders for a more immersive experience. This project takes data visualization to the next level by providing a user-friendly interface coupled with powerful interactivity, setting it apart as a top choice for visualizing and analyzing datasets.\n  \nSource Code:You can access the source code for this project on GitHub: Interactive Data Visualization with Plotly Dash\n\nWebsite:For more information and to see the project in action, visit : https://www.aspiresoftserv.com/"
},
{
    "title": "No title",
    "content": "https://slint.dev/blog/slint-1.6-released\n\n    Slint is a declarative GUI toolkit to build native user interfaces for desktop and embedded applications. Find more information at https://slint.dev/ or check out the source code at https://github.com/slint-ui/slint.\n  \n    EDIT: The Python APIs are currently in alpha. More info -- https://github.com/slint-ui/slint/tree/master/api/python"
},
{
    "title": "No title",
    "content": "Hi,\n  \n    Just want to know how difficult is it to manage your own pypi clone and how do you recommend to create a seperation between dev and prod systems."
},
{
    "title": "No title",
    "content": "🦑 Squid Proxy Manager Script\n    Hello fellow Python enthusiasts!\n  \n    I've created a Python script that makes managing your Squid Proxy Server a breeze. If you're looking for an efficient and straightforward way to interact with your Squid server remotely, this script is for you. 🎉\n  What My Project Does\n    The Squid Proxy Manager script allows you to manage your Squid Proxy Server remotely using a simple command-line interface. Here are some of the key features:\n  \n\n\nCheck Squid Service Status: Quickly check if your Squid service is running or not.\n  \n\n\nStart/Stop/Restart Service: Easily control the Squid service remotely.\n  \n\n\nView Logs: Access the latest entries in your Squid access logs.\n  \n\n\nView Configuration: Display the current Squid configuration file.\n  \n\n\nUpdate Configuration: Replace the existing Squid configuration with a new one.\n  \n\n\nReload Service: Reload the Squid service to apply changes without restarting.\n  \n\nTarget Audience\n    This script is designed for anyone who manages a Squid Proxy Server and prefers a command-line tool for remote management. If you are comfortable using Python and SSH, this tool will streamline your workflow and enhance your productivity.\n  Differences\n    Here are some aspects that make this Squid Proxy Manager script stand out:\n  \n\n\nRemote Management: Manage your Squid server without needing physical access, thanks to SSH connectivity.\n  \n\n\nEase of Use: The script provides a simple and intuitive command-line interface, making it easy to perform various tasks.\n  \n\n\nComprehensive Features: From checking service status to updating configurations and viewing logs, this script covers all essential Squid management tasks.\n  \n\n\nError Handling and Logging: Detailed logging and error handling ensure you know exactly what's happening and can troubleshoot issues effectively.\n  \n\n🚀 Usage\n\n\nInstallation:\n  \n\n\n    Ensure you have the required libraries installed:\n  pip install paramiko termcolor\n\n\n\n\nRunning the Script:\n  \n\n\n    Use the script with appropriate arguments to manage your Squid Proxy Server. Here's an example command to check the Squid service status:\n  ./squid_proxy_manager.py 192.168.2.111 22 username password --check-status\n\n\n\n\nUpdating Configuration:\n  \n\n\n    Create a new configuration file (e.g., new_squid.conf) with your desired settings.\n  \n\n\n    Run the script to update the Squid configuration:\n  ./squid_proxy_manager.py 192.168.2.111 22 username password --update-config new_squid.conf\n\n\n\n💻 Script Example\n    Here's a snippet of the script to give you an idea of its simplicity and functionality:\n  #!/usr/bin/env python3\n\nimport paramiko\nimport argparse\nimport logging\nimport sys\nimport os\nfrom termcolor import colored\n\nclass SquidProxyManager:\n    def __init__(self, hostname, port, username, password):\n        self.hostname = hostname\n        self.port = port\n        self.username = username\n        self.password = password\n        self.client = paramiko.SSHClient()\n        self.client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n\n    def connect(self):\n        try:\n            logging.info(colored(\"Attempting to connect to {}:{}\".format(self.hostname, self.port), 'cyan'))\n            self.client.connect(self.hostname, port=self.port, username=self.username, password=self.password)\n            logging.info(colored(f\"Connected to {self.hostname} on port {self.port}\", 'green'))\n        except Exception as e:\n            logging.error(colored(f\"Failed to connect: {e}\", 'red'))\n            sys.exit(1)\n\n    def disconnect(self):\n        self.client.close()\n        logging.info(colored(\"Disconnected from the server\", 'green'))\n\n    def execute_command(self, command):\n        logging.info(colored(\"Executing command: {}\".format(command), 'cyan'))\n        try:\n            stdin, stdout, stderr = self.client.exec_command(command)\n            stdout.channel.recv_exit_status()\n            out = stdout.read().decode()\n            err = stderr.read().decode()\n            if err:\n                logging.error(colored(f\"Error executing command '{command}': {err}\", 'red'))\n            else:\n                logging.info(colored(f\"Successfully executed command '{command}'\", 'green'))\n            return out, err\n        except Exception as e:\n            logging.error(colored(f\"Exception during command execution '{command}': {e}\", 'red'))\n            return \"\", str(e)\n\n    # More functions here...\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Squid Proxy Manager\")\n    parser.add_argument('hostname', help=\"IP address of the Squid proxy server\")\n    parser.add_argument('port', type=int, help=\"Port number for SSH connection\")\n    parser.add_argument('username', help=\"SSH username\")\n    parser.add_argument('password', help=\"SSH password\")\n    parser.add_argument('--check-status', action='store_true', help=\"Check Squid service status\")\n    parser.add.add_argument('--start', action='store_true', help=\"Start Squid service\")\n    parser.add.add_argument('--stop', action='store_true', help=\"Stop Squid service\")\n    parser.add.add_argument('--restart', action='store_true', help=\"Restart Squid service\")\n    parser.add.add_argument('--view-logs', action='store_true', help=\"View Squid logs\")\n    parser.add.add_argument('--view-config', action='store_true', help=\"View Squid configuration\")\n    parser.add.add_argument('--update-config', help=\"Update Squid configuration with provided data\")\n    parser.add.add_argument('--reload', action='store_true', help=\"Reload Squid service\")\n    return parser.parse_args()\n\ndef main():\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n    args = parse_args()\n    logging.info(colored(\"Initializing Squid Proxy Manager script\", 'cyan'))\n\n    manager = SquidProxyManager(args.hostname, args.port, args.username, args.password)\n    manager.connect()\n\n    try:\n        if args.check_status:\n            manager.check_squid_status()\n        if args.start:\n            manager.start_squid()\n        if args.stop:\n            manager.stop_squid()\n        if args.restart:\n            manager.restart_squid()\n        if args.view_logs:\n            manager.view_squid_logs()\n        if args.view_config:\n            manager.view_squid_config()\n        if args.update_config:\n            if not args.update_config.endswith('.conf'):\n                logging.error(colored(\"The provided file must have a .conf extension\", 'red'))\n            elif not os.path.isfile(args.update_config):\n                logging.error(colored(f\"Configuration file {args.update_config} not found\", 'red'))\n            else:\n                try:\n                    with open(args.update_config, 'r') as config_file:\n                        config_data = config_file.read()\n                    manager.update_squid_config(config_data)\n                except Exception as e:\n                    logging.error(colored(f\"Error reading configuration file {args.update_config}: {e}\", 'red'))\n        if args.reload:\n            manager.reload_squid()\n    finally:\n        manager.disconnect()\n        logging.info(colored(\"Squid Proxy Manager operations completed\", 'green'))\n\nif __name__ == \"__main__\":\n    main()🌟 Benefits\n\n\nRemote Management: No need to be physically present to manage your Squid server.\n  \n\n\nEase of Use: Simple command-line interface for quick operations.\n  \n\n\nVersatility: Supports various Squid management tasks, from checking status to updating configurations and viewing logs.\n  \n\n📢 Get Involved!\n    If you find this script useful, feel free to give it a try and share your feedback. Contributions and suggestions are always welcome! Comments however, that are unhelpful and serve no purpose to better the script or the author in their python scripting abilities are not welcome! Keep the nasty to yourself.\n  Access the script\n    You can find the script here on GitHub.\n  \n    Happy coding! 🚀"
},
{
    "title": "No title",
    "content": "github\n\n\n\ndocumentation\n\n\nWhat my project does :\n    It gets the dominant color/color palette from given image.\n  Target Audience:\n    Anyone\n  Usage\n    modern_colorthief exposes two functions get_color and get_palette\n  \n    Here is how to use get_color:\n  from modern_colorthief import get_color\n\n# Path to any image\npath = ...\n\nprint(get_color(path)) # returns tuple[int,int,int]\n    Here is how to use get_palette:\n  from modern_colorthief import get_color\n\n# Path to any image\npath = ...\n\nprint(get_palette(path)) # returns list[tuple[int,int,int]]Goals:\n\n\n    Bring color-thief-rs to python\n  \n\nBenchmarks:\nWritten in deatils\n\n    Gist:\n  Python Took:            0.09976800000004005\nCPP Took:               0.008461299999908078\nRUST Took:              0.008549499994842336\n\n\nPython Took:            0.0960583999985829\nCPP Took:               0.008564600000681821\nRUST Took:              0.007692700004554354DifferencesWith fast-colorthief\n\n\n    Supports more architectures. ( pybind11 vs pyo3 )\n  \n\n\n    Doesn't have a hard dependency on numpy\n  \n\n\n    Code is simple compared to fast-colorthief's CPP codebase\n  \n\n\n    Automated tooling powered by maturin and github-actions\n  \n\n\n    The size of fast-colorthief is 52kb-60kb.\n  \n\nWith color-thief-py\n\n\n    Superior execution time (nearly 100x)\n  \n\n\n    Doesn't have a hard dependency on pillow\n  \n\n\n    color-thief's codebase is not in par with modern python versions\n  \n\n\n    If you like this project please star this repository"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hey,\n  \n    I am (re)releasing a project called Frame that I've been working on to create a language and transpiler to easily create state machines/automata in Python. It also is able to generate UML documentation as well.\n  \n    This project is for people who are interested in programming state machines for a wide range of purposes such as game programming, workflows, MBSE modeling as well as school projects for comp sci theory. It is also useful simply for generating flow documentation.\n  \n    The Framepiler (Frame transpiler) is in beta at this time. It would be great to get feedback from the Python community on any gaps in key functionality or bugs.\n  \n    Low-code/no-code workflow tools are often problematic for creating state machine like flows. Frame is intended to give a textual way to accomplish the same thing, but without having to \"draw\" your software and with the ability to use all the standard devops tooling and processes for \"normal\" development processes.\n  \n    There is also a VSCode extension and a playground environment to experiment in.\n  \n    Very much hoping to connect with people who might find this interesting and useful. If that is you, please take a look at the Overview and the Getting Started articles. Here is a link to the GitHub Framepiler Project as well.\n  \n    Please LMK if you have any questions or interest in the project.\n  \n    Thanks!\n  \n    Mark"
},
{
    "title": "No title",
    "content": "What My Project Does  - This is a Python package to easily add string token based pagination. Currently it supports SQLModel and SQLAlchemy ORMs.\n  \n    Recently I wanted to add pagination in one of my Python projects and in the API response, I had to return a string next page token. Now I could not find a straight-forward way of doing this in Python. All of the tutorials or blog posts I saw, there in the response the server always returned a page_number, page_size, and total_elements and then the onus was on the calling service to adjust this accordingly. \n  \nComparison - The current packages and methods requires some changes in the app layer as well. I tried using a few but those did not satisfy the use case and were also a bit harder to implement. I could not find a easy to use option. The present ones returned integers instead of a string token\n  \n    I wanted it to be simpler, just like OpenSearch - you call its search API and it returns 10 elements and a next_page_token and then for the next 10 (or you configure this using the size parameter) you use the next_page_token in the subsequent request to get to the new page.\n  \n    I ended up doing a lot of if-else checks and encoding and decoding, so I decided to create this library.\n  \nTarget Audience - This is production ready, have been using it in one of my projects. Hope some of you folks find it useful :)\n  \n    Here is the link to the PyPi repository and here is the GitHub repo"
},
{
    "title": "No title",
    "content": "Main Changes\n\n\n\n    Add GUI functions\n  \n\n\n    Vehicle tracking: You can now track a specific vehicle to see their route\n  \n\n\n    Dataframe viewer: Stats can be confirmed\n  \n\n\n\n\n    Improve vehicle routing functions\n  \n\n\n    Add example of routing optimization\n\n\n\n\n\n    Change documentation's theme for better indexing\n  \n\n\nUXsim\n\nUXsim is a free, open-source macroscopic and mesoscopic network traffic flow simulator written in Python. It simulates the movements of car travelers and traffic congestion in road networks. It is suitable for simulating large-scale (e.g., city-scale) traffic phenomena. UXsim is especially useful for scientific and educational purposes because of its simple, lightweight, and customizable features, but users are free to use UXsim for any purpose."
},
{
    "title": "No title",
    "content": "Hi everyone!\n  \n    I recently finished a small side project for my graduating thesis, which is about experimenting with RAG-based frameworks in improving resume screening.\n  \nWhat my project does:\n\n    The project for the thesis is a GPT-4 Chatbot with RAG Fusion retrieval. Given a job description as input, the system retrieves the most relevant candidate profiles to perform follow-up tasks such as analysis, summarization, and decision-making, which can assist the screening process better.\n  \n    The revolving idea is that the similarity-based retrieval process can effectively narrow the initial large pool of applicants down to the most relevant resumes. However, this simple similarity ranking should not be used to evaluate a candidate's actual ability. Therefore, the top resumes are used to augment the GPT-4 Chatbot so it can be conditioned on these profiles and perform further downstream tasks.\n  \nTarget audience:\n\n    The repo contains the link to my paper and the notebooks that were used to design the prototype program and conduct some experiments. For the newcomers to RAG/RAG Fusion, or people who are just interested in building a RAG-based chatbots, this can be especially helpful. Feel free to check them out too!\n  \nComparison:\n\n    I'm not sure if there's any similar project out there, but the program is sort of designed to move the resume screening process away from existing keyword-based methods. It's much more versatile in use cases and also more effective in handling resumes.\n  \n    The project is very far from being perfect. Because of that, I share this with the hope to receive suggestions and feedback from you. If you have time, please give the project a visit here: GitHub"
},
{
    "title": "No title",
    "content": "Five years ago, I posted about JModelica, a fantastic open-source tool for simulating complex systems that combined the ease of Python with the strength of Modelica.\n  \n    Sadly, the project went quiet, but I'm thrilled to share that, thanks to the dedication of a few folks (myself included!), JModelica is back! You can find the revived project on GitHub: https://github.com/JModelica/JModelica.\n  What JModelica Does:\n    JModelica provides a way to write complex simulations using the Modelica language, which is known for its ability to handle differential equations and model physical systems beautifully. The magic of JModelica lies in its Python integration—you can solve your Modelica models and access the results directly in Python for in-depth analysis, visualization, and even optimization using libraries you already love!\n  Target Audience:\n    This project is geared toward anyone interested in modeling and simulating complex systems, particularly those with a background in engineering, physics, or related fields. If you've struggled with Python's ODE solvers or wish for a more elegant way to model physical interactions, JModelica offers a compelling solution. It's ready for research, educational projects, and even more ambitious endeavors!\n  Comparison:\n    JModelica stands alongside OpenModelica as a champion of open-source Modelica tools. While OpenModelica is known for its user-friendly graphical interface, JModelica shines in its seamless integration with Python, giving you the best of both worlds! It's a powerful alternative to proprietary software like Simulink, providing transparency, flexibility, and a thriving community.\n  \n    We're actively working on squashing bugs, adding features, and making JModelica more accessible across different platforms (Windows and macOS support are on the horizon!).\n  \n    Anyone interested in contributing is welcome! Whether you're a Modelica expert or a curious newcomer, this project has a place for you. Check out the GitHub repository to explore the code, open issues, or submit pull requests."
},
{
    "title": "No title",
    "content": "What My Project Does\nWhat My Project Does: snipinator is a CLI to embed (testable) snippets from your codebase into your README, using Jinja2 and functions provided by snipinator to assist with embedding code, shell output, etc.\n  \n    Please provide any feedback in the comments or GH issues.\n  Target Audience\nTarget Audience: Developers of {GitHub,other} projects that have a README. It works for me, it might work for you.\n  Comparison\nFeatures:\n\n\n\n    Supports anything Jinja2 supports.\n  \n\n\n    First-class support for python source code.\n  \n\n\n    Can include python function signatures, docstrings, entire function source code, classes.\n  \n\n\n\n\n    Snip from any source code language.\n  \n\n\n    Put delimiter markers into the code (e.g # START_SNIPPET, # END_TEMPLATE), and use snippet().\n  \n\n\n\n\n    First-class support for Markdown templates (with backtickify, decomentify).\n  \n\n\n    Can include shell output.\n  \n\n\n    Supports ANSI colors with SVG output.\n  \n\n\n\n\n    More robust references/links to local files using path().\n  \n\n\n    I keep a table of similar projects in my README at realazthat/snipinator: Related Projects.\n  \n    Not complete, and not necessarily up to date. Make a PR to README.md.jinja, (see realazthat/snipinator/Contributions) to insert/modify the table.\n  \n\n\n\n            Project\n          \n            Stars\n          \n            Last Update\n          \n            Language\n          \n            Platform\n          \n            Similarity X Obviousness\n          \n\n\n\n\n\n\nmdx-js/mdx\n\n              16.8k\n            \n2024/04/17\n\n              JS\n            \n              N/A\n            \n              ⭐⭐⭐⭐⭐\n            \n\n\nzakhenry/embedme\n\n              222\n            \n2023/11/08\n\n              JS\n            \n              N/A\n            \n              ⭐⭐⭐⭐⭐\n            \n\n\ncmacmackin/markdown-include\n\n              95\n            \n2023/02/07\n\n              Python\n            \n              N/A\n            \n              ⭐⭐⭐⭐⭐\n            \n\n\nBurdetteLamar/markdown_helper\n\n              38\n            \n2020/03/16\n\n              Ruby\n            \n              N/A\n            \n              ⭐⭐⭐⭐⭐\n            \n\n\nSimonCropp/MarkdownSnippets\n\n              23\n            \n2024/04/23\n\n              .NET\n            \n              N/A\n            \n              ⭐⭐⭐⭐⭐\n            \n\n\nendocode/snippetextractor\n\n              4\n            \n2014/08/16\n\n              C++\n            \n              N/A\n            \n              ⭐⭐⭐⭐⭐\n            \n\n\npolywrap/doc-snippets\n\n              3\n            \n2023/09/26\n\n              JS\n            \n              N/A\n            \n              ⭐⭐⭐⭐⭐\n            \n\n\nJulianCataldo/remark-embed\n\n              2\n            \n2022/09/22\n\n              JS\n            \n              N/A\n            \n              ⭐⭐⭐⭐⭐\n            \n\n\nxrd/oreilly-snippets\n\n              2\n            \n2015/10/15\n\n              Ruby\n            \n              N/A\n            \n              ⭐⭐⭐⭐⭐\n            \n\n\nDamonOehlman/injectcode\n\n              1\n            \n2021/08/01\n\n              JS\n            \n              N/A\n            \n              ⭐⭐⭐⭐⭐\n            \n\n\nelectrovir/markdown-code-example-inserter\n\n              1\n            \n2024/02/19\n\n              JS\n            \n              N/A\n            \n              ⭐⭐⭐⭐⭐\n            \n\n\nandersfischernielsen/Simple-Embedded-Markdown-Code-Snippets\n\n              1\n            \n2021/02/12\n\n              JS\n            \n              N/A\n            \n              ⭐⭐⭐⭐⭐\n            \n\n\nildar-shaimordanov/git-markdown-snippet\n\n              0\n            \n2021/09/14\n\n              Perl\n            \n              N/A\n            \n              ⭐⭐⭐⭐⭐\n            \n\n\nteyc/markdown-snippet\n\n              0\n            \n2024/01/22\n\n              Powershell\n            \n              N/A\n            \n              ⭐⭐⭐⭐⭐\n            \n\n\nmarc-bouvier-graveyard/baldir_markdown\n\n              0\n            \n2020/06/15\n\n              Python\n            \n              N/A\n            \n              ⭐⭐⭐⭐⭐\n            \n\n\ndineshsonachalam/markdown-autodocs\n\n              176\n            \n2022/09/19\n\n              JS\n            \n              GH Action\n            \n              ⭐⭐⭐⭐\n            \n\n\ntokusumi/markdown-embed-code\n\n              28\n            \n2022/01/05\n\n              Python\n            \n              GH Action\n            \n              ⭐⭐⭐⭐\n            \n\n\nsammndhr/gridsome-remark-embed-snippet\n\n              2\n            \n2021/06/14\n\n              JS\n            \nGridsome\n\n              ⭐⭐⭐⭐\n            \n\n\nNativeScript/markdown-snippet-injector\n\n              4\n            \n2019/01/24\n\n              JS\n            \n              N/A\n            \n              ⭐⭐⭐⭐\n            \n\n\nfuxingloh/remark-code-import-replace\n\n              0\n            \n2022/12/21\n\n              JS\n            \n              Remark?\n            \n              ⭐⭐⭐⭐\n            \n\n\nszkiba/mdcode\n\n              15\n            \n2014/02/12\n\n              Go\n            \n              N/A\n            \n              ⭐⭐⭐\n            \n\n\ndevincornell/pymddoc\n\n              0\n            \n2023/12/01\n\n              Python\n            \n              Python\n            \n              ⭐⭐⭐\n            \n\n\nshiftkey/scribble (docs)\n            \n              40\n            \n2013/08/08\n\n              .NET\n            \n              N/A\n            \n              ⭐⭐\n            \n\n\ncalebpeterson/jest-transformer-test-md\n\n              2\n            \n2020/08/21\n\n              JS\n            \n              Jest Tests\n            \n              ⭐⭐\n            \n\n\ntjstankus/commitate\n\n              0\n            \n2014/05/29\n\n              Ruby\n            \n              N/A\n            \n              ⭐\n            \n\n\nGitHub Docs: Creating a permanent link to a code snippet\n\n              N/A\n            \n              N/A\n            \n              N/A\n            \n              N/A\n            \n              ⭐\n            \n\n\njavierfernandes/markdown-exercises\n\n              1\n            \n2017/05/01\n\n              JS\n            \n              N/A\n            \n              ⭐\n            \n\n\ngatsby-remark-embed-snippet\n\n              N/A (55k)\n            \n2024/01/23\n\n              JS\n            \nGatsby\n\n              ⭐\n            \n\n\nARMmbed/snippet\n\n              6\n            \n2021/08/05\n\n              Python\n            \n              N/A\n            \n              ?\n            \n\n\ndrewavis/markdowninclude\n\n              1\n            \n2024/04/06\n\n              JS\n            \n              VSCode Extension\n            \n              ?\n            \n\n\nromnn/embedme\n\n              0\n            \n2024/04/18\n\n              Go\n            \n              N/A\n            \n              ?\n            \n\n\n\n    The 5 star projects have the bare minimum of being able to embed a file, and run via CLI.\n  \n\n\n    Snipinator does have other features (such as shell()), implemented as I needed them (and listed below) which I do not think any of these have in combination.\n  \n\n\n    Some of these projects are not CLIs.\n  \n\n\nmdx-js/mdx is the closest in terms of flexibility, but it is JS + components, which may not be everyone's cup of tea.\n  \n\n\nUsage:\n\n    Example template README: (./snipinator/examples/EXAMPLE.md.jinja2):\n  # A README\n\nHere is a code snippet:\n\n<!--{{ pysnippet(path='snipinator/examples/code.py', symbol='MyClass', backtickify='py', decomentify='nl') }}-->\n\nNote that `code.py` has a test:\n{{path('./snipinator/examples/code_test.py', link='md')}}.\n    Generating the README:\n  $ python -m snipinator.cli -t snipinator/examples/EXAMPLE.md.jinja2\n<!--\n\nWARNING: This file is auto-generated by snipinator. Do not edit directly.\nSOURCE: `snipinator/examples/EXAMPLE.md.jinja2`.\n\n-->\n# A README\n\nHere is a code snippet:\n\n<!---->\n```py\nclass MyClass:\n  \"\"\"This is a global class\"\"\"\n\n  def __init__(self, name):\n    self.name = name\n\n  def MyClassMethod(self):\n    \"\"\"This is a method of MyClass\"\"\"\n    print(self.name)\n```\n<!---->\n\nNote that `code.py` has a test:\n[./snipinator/examples/code_test.py](./snipinator/examples/code_test.py)."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What My Project Does\n    Overlaying intensity plots onto a geographical map using cartopy/matplotlib can be complex. So we created this map_plotter package to abstract away that complexity for a common use case.\n  \nInstallation\n\n    (opinionated use of conda to avoid cartopy dependency hell and install precompiled binaries)\n  conda install cartopy\ngit clone git@github.com:amentumspace/map_plotter.git\ncd map_plotter\npip install .\nUsage\nimport map_plotter\nmap_plotter.plot(lons_g, lats_g, variable, units=\"m/s\", img_name=\"image.png\",\n    save=True, plot=True, title=\"something\", zlims=[0,10])\n    Whereby:\n  \n\n\nlons_g and lats_g represent 2D matrices / grids of longitudes and latitudes.\n  \n\n\nvalues is the matrix of values to be plotted (same grid dimensions).\n  \n\n\nunits and img_name (self explanatory).\n  \n\n\nsave & plot boolean flags to save the file and plot to screen, respectively.\n  \n\n\nzlims define the color scale minimum and maximum.\n  \n\nTarget Audience\n    Python developers or data scientists or scientists or any Pythonista wanting a simple way to quickly plot an intensity map onto a geographical map.\n  Comparison\n    Differs from using cartopy and matplotlib in its ease-of-use, but it is less customisable (can't change projections, colors). Regardless, it's convenient and at least provides a starting point for customisation. Similar functionality can be had from geopandas or folium (although cartopy/matplotlib suited our needs better)."
},
{
    "title": "No title",
    "content": "What My Project Does\n  \n\n\n    This is a music video of the output of a Python program: https://www.youtube.com/watch?v=Sjk4UMpJqVs\n\n    I'm the author of Automate the Boring Stuff with Python and I teach people to code. As part of that, I created something I call \"scroll art\". Scroll art is a program that prints text from a loop, eventually filling the screen and causing the text to scroll up. (Something like those BASIC programs that are 10 PRINT \"HELLO\"; 20 GOTO 10)\n  \n    Once printed, text cannot be erased, it can only be scrolled up. It's an easy and artistic way for beginners to get into coding, but it's surprising how sophisticated they can become.\n  \n    The source code for this animation is here: https://github.com/asweigart/scrollart/blob/main/python/forbiddenzone.py (read the comments at the top to figure out how to run it with the forbiddenzonecontrol.py program which is also in that repo)\n  \n    The output text is procedurally generated from random numbers, so like a lava lamp, it is unpredictable and never exactly the same twice.\n  \n    This video is a collection of scroll art to the music of \"The Forbidden Zone,\" which was released in 1980 by the band Oingo Boingo, led by Danny Elfman (known for composing the theme song to The Simpsons.) It was used in a cult classic movie of the same name, but also the intro for the short-run Dilbert animated series.\n  \n\n\n    Target Audience\n  \n\n\n    Anyone (including beginners) who wants ideas for creating generative art without needing to know a ton of math or graphics concepts. You can make scroll art with print() and loops and random numbers. But there's a surprising amount of sophistication you can put into these programs as well.\n  \n\n\n    Comparison\n  \n\n\n    Because it's just text, scroll art doesn't have such a high barrier to entry compared with many computer graphics and generative artwork. The constraints lower expectations and encourage creativity within a simple context.\n  \n    I've produced scroll art examples on https://scrollart.org\n\n    I also gave a talk on scroll art at PyTexas 2024: https://www.youtube.com/watch?v=SyKUBXJLL50"
},
{
    "title": "No title",
    "content": "After playing around with a dataframe—applying filters or other transformations—I'm curious about your methods for reviewing the changes.\n  \n    In VS Code, the variable explorer is quite handy for a quick look at the modified dataframe. Alternatively, when working in a Jupyter notebook within VS Code, exporting the data to an Excel file provides a detailed view and allows for an easy deep dive into the results. What are your preferred practices for ensuring your data adjustments are precisely what you intended?"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hi everybody,\n  \n    over the last year I've been developing a library that adds some Cython 3.0 annotations to existing python code.\n  \nWhat My Project Does:\n\n    For example if it sees a for i in range(): in a function it recognizes i as an integer and adds a i = cython.declare(cython.int)line at the beginning of the function.\n  \n    It actually uses the built-in ast module under the hood for parsing, I found it a super useful library!\n  \nTarget Audience:\n\n    It is a side project I made mainly for fun. I don't know if it can be of interest to anybody, or if it could have some potential utility.\n  \nComparison:\n\n    I did not find anything similar. There are a lot of very cool projects like mypyc for example, but nothing that does this tiny little code generation specific to Cython.\n  \n    The link to the repository is here:\n  \nhttps://github.com/nucccc/markarth"
},
{
    "title": "No title",
    "content": "Hey folks, looking to use one library to implement some background scheduling logic on my application.\n  \n    I find in Google search APScheduler to be frequently mentioned, but I can see the Schedule package has more GH stars.\n  \n    Was curious if anybody has used one of them, and which one would you recommend based on your own experience."
},
{
    "title": "No title",
    "content": "Excited to share my personal open-source project: Notolog - Python Markdown Editor (MIT License).\n  \n    The main motivation for developing another markdown editor was my passion for learning new things and enhancing my development skills in Python. I developed it in my spare time over a few months, despite having no prior experience in creating full-scale Python applications.\n  \nWhat My Project Does\n\n    ∗ Multiplatform\n  \n    ∗ Markdown async syntax highlighting created by me\n  \n    ∗ Several pre-installed color themes\n  \n    ∗ Supports English and 17 other languages right out of the box\n  \n    ∗ Integration with OpenAI API for AI-assisted features\n  \n    ∗ Optional file encryption/decryption\n  \nTarget Audience\n\n    Primarily developers who write markdown documents and notes.\n  \nComparison\n\n    This is more of a personal learning project, so it's hard to compare it directly with others.\n  \nHow to install\n\n    Discover Notolog on GitHub 🌟 and PyPI.\n  \n    Installation is as easy as running a single command:\n  pip install notolog"
},
{
    "title": "No title",
    "content": "I have been using zabbix for monitoring a lot of metrics in my work, none of the most popular zabbix were capable of doing async tasks, so I've developed some simple package capable of doing this.\n  \n    Tests, examples and how-tos can be found here: https://github.com/gustavofbreunig/zabbix-sender-async\n\nWhat My Project Does\n\n    Send zabbix sender messages using asyncio tasks.\n  \nTarget Audience\n\n    SysAdmins who use Zabbix to monitor a large number of metrics.\n  \nComparison\n\n    Instead of doing traditional way, using these abandoned library: https://github.com/adubkov/py-zabbix\nfrom pyzabbix import ZabbixMetric, ZabbixSender\n\n# Send metrics to zabbix trapper\npacket = [\n  ZabbixMetric('hostname1', 'test[cpu_usage]', 2),\n  ZabbixMetric('hostname1', 'test[system_status]', \"OK\"),\n  ZabbixMetric('hostname1', 'test[disk_io]', '0.1'),\n  ZabbixMetric('hostname1', 'test[cpu_usage]', 20, 1411598020),\n]\n\nresult = ZabbixSender(use_config=True).send(packet)\n    You can do this:\n  async def sendmetrics():\n    sender = AsyncSender('localhost', 10051)\n    metric = ItemData(host='hostname', key='test.metric.text', value='test package import')\n    result = await sender.send(metric)"
},
{
    "title": "No title",
    "content": "What my project does: It supports services like IBM Watson, Acapela and Stream labs' demo websites to convert your text to speech.\n  \nTarget audience: It's a toy project and would not recommend you to use in Production.\n  \nComparison: It's wayyyyy easy to use. Just pip install and use in your project. No extra setup required like other libraries. Also supports various languages and voices and accents. Check docs for more.\n  \nHere is the link to repository.\n  \n    Please go do check it out and star it if it's helpful to you guys. Thank you.\n  \n    I made this library taking inspiration from this php tts library by chrisjp."
},
{
    "title": "No title",
    "content": "I made a library to create interactive plots in the terminal (pip install itrm). It uses braille characters (by default) to display the data with sub-character resolution. There are several keybindings for moving a vertical cursor left and right, for zooming in or out on data, and for changing which curve to focus on. There are occasions (such as when working with a server) where MatPlotLib is not an option and the terminal is the only available tool. But, in my opinion, it is actually faster to use this tool (itrm) to zoom in on interesting parts of data and analyze patterns than using other tools like MatPlotLib. In fact, with large data sets (~1 million points), this tool actually renders faster than MatPlotLib. Please check it out and let know what you think."
},
{
    "title": "No title",
    "content": "Hi! I want to share a library I've built recently. IBind is a REST and WebSocket Python client for Interactive Brokers Client Portal Web API. It is directed at IBKR users.\n  \n    You can find IBind on GitHub: https://github.com/Voyz/ibind\n\nWhat My Project Does:\n\n    It is a REST and WebSocket API for the Interactive Brokers' Web API.\n  \n    I'm particularly proud of a few things in this release:\n  \n\n\n    The REST and WebSocket API clients are based on an abstract base class RestClient and WsClient accordingly. These could be implemented to use some other Web APIs in a relatively straightforward way. I have in fact used a version of that WsClient for a cryptocurrency WebSocket API, and it is nice to see it adapt to a different environment.\n  \n\n\n    I've covered most of the codebase with automated tests (appx 80%). Contrary to some of my other libraries, these are mainly integration tests which feel to provide a stronger test coverage than only unit tests.\n  \n\n\n    I've learned how to use class mixins in this project, and it aids the maintainability by a lot! The REST client itself is pretty barebone, but has a lot of mixin classes - all corresponding to the endpoint categories the broker uses, making it easy to search for the right piece of code and documentation.\n  \n\n\n    There's a lot of things that make this client as plug-and-play as possible. The broker requires the user to specify a bunch of things - account ids, certificates, URLs, etc. - which the class either reads from the environment variables or assumes (given that some things would be common for most users). In either case, all these are customisable by parameters if needed, but it is nice to just write client = IbkrClient() in various projects having set just a couple of env vars.\n  \n\n\n    I think the documentation is pretty in-depth but readable. It's always hard to judge whether docs are well written, but I think it is nicely broken down. Also, I managed to use pydoc-markdown package to create API reference in markdown, which works nicely with the GitHub Wiki. I'd prefer it to be even easier, but compared to Sphinx and readthedocs it's a much quicker job.\n  \n\n\n    The WebSocket class does a ton to keep the connection alive and recover from connection losses. Maintaining active subscriptions after a re-connect can be a real pain, and I think this class does it in a nice and reliable way. I've tested it for various types of connectivity loss, and it manages to recover and re-establish the WebSocket data stream. Pretty crucial in the trading environment.\n  \n\n\n    I made a nice logo for it 🥳\n  \n\n\nTarget Audience:\n\n    Traders using IBKR who want to automate their trading through this Web API.\n  \nComparison (A brief comparison explaining how it differs from existing alternatives.) :\n\n    There are two similar libraries that I know of. They aren't bad, but seem not very well maintained and incomplete:\n  \n\n\nhttps://github.com/areed1192/interactive-brokers-api - outdated and stale, last update 3 years ago\n  \n\n\nhttps://github.com/utilmon/EasyIB - stale and incomplete\n  \n\n\n    The library I've published covers a much wider range of endpoints, adds WebSocket support and a bunch of wrapper methods to simplify the usage of the API.\n  \n    IBind has a bunch of features that make using the IBKR APIs much easier. Some of these are:\n  \n    REST:\n  \n\n\nAutomated question/answer handling - streamlining placing orders.\n  \n\n\nParallel requests - speeding up collection of price data.\n  \n\n\nRate limiting - guarding against account bans.\n  \n\n\nConid unpacking - helping to find the right contract.\n  \n\n\n    WebSocket:\n  \n\n\nWebSocket thread lifecycle handling - ensuring the connection is alive.\n  \n\n\nThread-safe Queue data stream - exposing the collected data in a safe way.\n  \n\n\nInternal subscription tracking - recreating subscriptions upon re-connections.\n  \n\n\nHealth monitoring - Acting on unusual ping or heartbeat.\n  \n\n\n    REST Example:\n  from ibind import IbkrClient\n\n# Construct the client\nclient = IbkrClient()\n\nprint(client.tickle().data)\n    WebSocket Example:\n  from ibind import IbkrWsKey, IbkrWsClient\n\n# Construct the client.\nws_client = IbkrWsClient(start=True)\n\n# Choose the WebSocket channel\nkey = IbkrWsKey.PNL\n\n# Subscribe to the PNL channel\nws_client.subscribe(channel=key.channel)\n\nprint(ws_client.get(key))\n    I just wanted to share my experience of publishing Open Source. For some reason I get a lot of motivation when I can publish code that makes peoples' lives easier. The library could use some code review on it, so if you’d feel like reading some code and helping out - drop me a message. Other than that, happy to answer any questions, and - if you are an algo trader - let me know if you get a chance to use it. Thanks for reading!"
},
{
    "title": "No title",
    "content": "Python Streamlit is a terrific tool for creating interactive data visualizations.\n  \n    It packages all your visualizations up into a neat little application - including charts and maps - and displays them in your default browser. No muss, no fuss.\n  \n    Recently, I found a new dataset (to me) on the UN High Commission for Refugees (UNHCR) website. It contains country-to-country movements for refugees both from origin country and country of asylum\n  \n    Using this dataset, here's a step-by-step on how to code a Python Streamlit application that has:\n  \n\n\nA dropdown menu to select by country\n  \n\n\nA second dropdown menu to select by year\n  \n\n\nRadio buttons (2) to select country of origin or county of asylum\n  \n\n\nA global choropleth map to display the results by country and year.\n  \n\n\n    Free article HERE."
},
{
    "title": "No title",
    "content": "Hey folks,\n  \n    I've been dabbling with a Python project recently that's all about making life easier for us I.T. people. It's a nifty little tool that calculates IP subnets and does IP calculations from the command or CLI.\n  \n    Here's the GitHub link and the code:\n  \nhttps://github.com/nicanorflavier/ipnet\n\n    I’m pretty stoked about it, but I know there’s always room for improvement. So, I thought, better to turn to than the wise minds of this python community?\n  \n    I’m all ears for any feedback, tips, tricks, or advice you guys might have. Thanks a ton in advance!"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Wrote this as a tool to keep README coverage badges up to date without relying on 3rd party services or having to do anything extra, thought others might get some utility out of it: coverage-pre-commit.\n  \n    A .coverage file is expected at the root of the project, generated by running coverage run directly or using a plugin such as pytest-cov when running tests.\n  \n    Most convenient when used as a pre-push hook imo. Feel free to opine, be it positive or negative!"
},
{
    "title": "No title",
    "content": "I'm thrilled to share my first open-source project with you all: PyWolt! 🎉\n  \nPyWolt is a Python library that makes it super easy to interact with the Wolt API.\n  What My Project Does:\n\n\n    Discover Venues: Find nearby spots to grab a bite.\n  \n\n\n    Explore Menus: Dive into a venue's menu and pick your favorites.\n  \n\nTarget Audience:\n\n\nSoftware Engineers: Professionals who build web or mobile applications, particularly those in the food delivery or restaurant industry, looking to incorporate Wolt's services seamlessly into their platforms.\n  \n\n\nData Scientists/Analysts: Individuals analyzing food delivery data, consumer behavior, or market trends, who may utilize PyWolt to gather data from Wolt's API for analysis and insights.\n  \n\n\nStudents/Learners: Those studying Python programming, web development, or API integration, who can use PyWolt as a practical example or learning tool to understand how to interact with RESTful APIs in Python.\n  \n\n\nFreelancers/Entrepreneurs: Independent developers or startup founders looking to build new food-related applications or services leveraging Wolt's platform without reinventing the wheel.\n  \n\nComparison:\n\n\nwoltcheck: only offers a script to check if a wolt restaurant is ready to deliver to your location.\n  \n\n\nwhat-to-eat: a pretty neat cli tool that offers all of pywolt's functionality. In my opinion it overcomplicates things a little, and doesn't offer straight-forward RESTful functionality to interact with the API itself."
},
{
    "title": "No title",
    "content": "github\n\n\n\ndocumentation\n\n\nWhat my project does :\n    It gets the dominant color/color palette from given image.\n  Target Audience:\n    Anyone\n  Usage\n    modern_colorthief exposes two functions get_color and get_palette\n  \n    Here is how to use get_color:\n  from modern_colorthief import get_color\n\n# Path to any image\npath = ...\n\nprint(get_color(path)) # returns tuple[int,int,int]\n    Here is how to use get_palette:\n  from modern_colorthief import get_color\n\n# Path to any image\npath = ...\n\nprint(get_palette(path)) # returns list[tuple[int,int,int]]Goals:\n\n\n    Bring color-thief-rs to python\n  \n\nBenchmarks:\nWritten in deatils\n\n    Gist:\n  Python Took:            0.09976800000004005\nCPP Took:               0.008461299999908078\nRUST Took:              0.008549499994842336\n\n\nPython Took:            0.0960583999985829\nCPP Took:               0.008564600000681821\nRUST Took:              0.007692700004554354DifferencesWith fast-colorthief\n\n\n    Supports more architectures. ( pybind11 vs pyo3 )\n  \n\n\n    Doesn't have a hard dependency on numpy\n  \n\n\n    Code is simple compared to fast-colorthief's CPP codebase\n  \n\n\n    Automated tooling powered by maturin and github-actions\n  \n\n\n    The size of fast-colorthief is 52kb-60kb.\n  \n\nWith color-thief-py\n\n\n    Superior execution time (nearly 100x)\n  \n\n\n    Doesn't have a hard dependency on pillow\n  \n\n\n    color-thief's codebase is not in par with modern python versions\n  \n\n\n    If you like this project please star this repository"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hey,\n  \n    I am (re)releasing a project called Frame that I've been working on to create a language and transpiler to easily create state machines/automata in Python. It also is able to generate UML documentation as well.\n  \n    This project is for people who are interested in programming state machines for a wide range of purposes such as game programming, workflows, MBSE modeling as well as school projects for comp sci theory. It is also useful simply for generating flow documentation.\n  \n    The Framepiler (Frame transpiler) is in beta at this time. It would be great to get feedback from the Python community on any gaps in key functionality or bugs.\n  \n    Low-code/no-code workflow tools are often problematic for creating state machine like flows. Frame is intended to give a textual way to accomplish the same thing, but without having to \"draw\" your software and with the ability to use all the standard devops tooling and processes for \"normal\" development processes.\n  \n    There is also a VSCode extension and a playground environment to experiment in.\n  \n    Very much hoping to connect with people who might find this interesting and useful. If that is you, please take a look at the Overview and the Getting Started articles. Here is a link to the GitHub Framepiler Project as well.\n  \n    Please LMK if you have any questions or interest in the project.\n  \n    Thanks!\n  \n    Mark"
},
{
    "title": "No title",
    "content": "What My Project Does  - This is a Python package to easily add string token based pagination. Currently it supports SQLModel and SQLAlchemy ORMs.\n  \n    Recently I wanted to add pagination in one of my Python projects and in the API response, I had to return a string next page token. Now I could not find a straight-forward way of doing this in Python. All of the tutorials or blog posts I saw, there in the response the server always returned a page_number, page_size, and total_elements and then the onus was on the calling service to adjust this accordingly. \n  \nComparison - The current packages and methods requires some changes in the app layer as well. I tried using a few but those did not satisfy the use case and were also a bit harder to implement. I could not find a easy to use option. The present ones returned integers instead of a string token\n  \n    I wanted it to be simpler, just like OpenSearch - you call its search API and it returns 10 elements and a next_page_token and then for the next 10 (or you configure this using the size parameter) you use the next_page_token in the subsequent request to get to the new page.\n  \n    I ended up doing a lot of if-else checks and encoding and decoding, so I decided to create this library.\n  \nTarget Audience - This is production ready, have been using it in one of my projects. Hope some of you folks find it useful :)\n  \n    Here is the link to the PyPi repository and here is the GitHub repo"
},
{
    "title": "No title",
    "content": "Main Changes\n\n\n\n    Add GUI functions\n  \n\n\n    Vehicle tracking: You can now track a specific vehicle to see their route\n  \n\n\n    Dataframe viewer: Stats can be confirmed\n  \n\n\n\n\n    Improve vehicle routing functions\n  \n\n\n    Add example of routing optimization\n\n\n\n\n\n    Change documentation's theme for better indexing\n  \n\n\nUXsim\n\nUXsim is a free, open-source macroscopic and mesoscopic network traffic flow simulator written in Python. It simulates the movements of car travelers and traffic congestion in road networks. It is suitable for simulating large-scale (e.g., city-scale) traffic phenomena. UXsim is especially useful for scientific and educational purposes because of its simple, lightweight, and customizable features, but users are free to use UXsim for any purpose."
},
{
    "title": "No title",
    "content": "Hi everyone!\n  \n    I recently finished a small side project for my graduating thesis, which is about experimenting with RAG-based frameworks in improving resume screening.\n  \nWhat my project does:\n\n    The project for the thesis is a GPT-4 Chatbot with RAG Fusion retrieval. Given a job description as input, the system retrieves the most relevant candidate profiles to perform follow-up tasks such as analysis, summarization, and decision-making, which can assist the screening process better.\n  \n    The revolving idea is that the similarity-based retrieval process can effectively narrow the initial large pool of applicants down to the most relevant resumes. However, this simple similarity ranking should not be used to evaluate a candidate's actual ability. Therefore, the top resumes are used to augment the GPT-4 Chatbot so it can be conditioned on these profiles and perform further downstream tasks.\n  \nTarget audience:\n\n    The repo contains the link to my paper and the notebooks that were used to design the prototype program and conduct some experiments. For the newcomers to RAG/RAG Fusion, or people who are just interested in building a RAG-based chatbots, this can be especially helpful. Feel free to check them out too!\n  \nComparison:\n\n    I'm not sure if there's any similar project out there, but the program is sort of designed to move the resume screening process away from existing keyword-based methods. It's much more versatile in use cases and also more effective in handling resumes.\n  \n    The project is very far from being perfect. Because of that, I share this with the hope to receive suggestions and feedback from you. If you have time, please give the project a visit here: GitHub"
},
{
    "title": "No title",
    "content": "Five years ago, I posted about JModelica, a fantastic open-source tool for simulating complex systems that combined the ease of Python with the strength of Modelica.\n  \n    Sadly, the project went quiet, but I'm thrilled to share that, thanks to the dedication of a few folks (myself included!), JModelica is back! You can find the revived project on GitHub: https://github.com/JModelica/JModelica.\n  What JModelica Does:\n    JModelica provides a way to write complex simulations using the Modelica language, which is known for its ability to handle differential equations and model physical systems beautifully. The magic of JModelica lies in its Python integration—you can solve your Modelica models and access the results directly in Python for in-depth analysis, visualization, and even optimization using libraries you already love!\n  Target Audience:\n    This project is geared toward anyone interested in modeling and simulating complex systems, particularly those with a background in engineering, physics, or related fields. If you've struggled with Python's ODE solvers or wish for a more elegant way to model physical interactions, JModelica offers a compelling solution. It's ready for research, educational projects, and even more ambitious endeavors!\n  Comparison:\n    JModelica stands alongside OpenModelica as a champion of open-source Modelica tools. While OpenModelica is known for its user-friendly graphical interface, JModelica shines in its seamless integration with Python, giving you the best of both worlds! It's a powerful alternative to proprietary software like Simulink, providing transparency, flexibility, and a thriving community.\n  \n    We're actively working on squashing bugs, adding features, and making JModelica more accessible across different platforms (Windows and macOS support are on the horizon!).\n  \n    Anyone interested in contributing is welcome! Whether you're a Modelica expert or a curious newcomer, this project has a place for you. Check out the GitHub repository to explore the code, open issues, or submit pull requests."
},
{
    "title": "No title",
    "content": "What My Project Does\nWhat My Project Does: snipinator is a CLI to embed (testable) snippets from your codebase into your README, using Jinja2 and functions provided by snipinator to assist with embedding code, shell output, etc.\n  \n    Please provide any feedback in the comments or GH issues.\n  Target Audience\nTarget Audience: Developers of {GitHub,other} projects that have a README. It works for me, it might work for you.\n  Comparison\nFeatures:\n\n\n\n    Supports anything Jinja2 supports.\n  \n\n\n    First-class support for python source code.\n  \n\n\n    Can include python function signatures, docstrings, entire function source code, classes.\n  \n\n\n\n\n    Snip from any source code language.\n  \n\n\n    Put delimiter markers into the code (e.g # START_SNIPPET, # END_TEMPLATE), and use snippet().\n  \n\n\n\n\n    First-class support for Markdown templates (with backtickify, decomentify).\n  \n\n\n    Can include shell output.\n  \n\n\n    Supports ANSI colors with SVG output.\n  \n\n\n\n\n    More robust references/links to local files using path().\n  \n\n\n    I keep a table of similar projects in my README at realazthat/snipinator: Related Projects.\n  \n    Not complete, and not necessarily up to date. Make a PR to README.md.jinja, (see realazthat/snipinator/Contributions) to insert/modify the table.\n  \n\n\n\n            Project\n          \n            Stars\n          \n            Last Update\n          \n            Language\n          \n            Platform\n          \n            Similarity X Obviousness\n          \n\n\n\n\n\n\nmdx-js/mdx\n\n              16.8k\n            \n2024/04/17\n\n              JS\n            \n              N/A\n            \n              ⭐⭐⭐⭐⭐\n            \n\n\nzakhenry/embedme\n\n              222\n            \n2023/11/08\n\n              JS\n            \n              N/A\n            \n              ⭐⭐⭐⭐⭐\n            \n\n\ncmacmackin/markdown-include\n\n              95\n            \n2023/02/07\n\n              Python\n            \n              N/A\n            \n              ⭐⭐⭐⭐⭐\n            \n\n\nBurdetteLamar/markdown_helper\n\n              38\n            \n2020/03/16\n\n              Ruby\n            \n              N/A\n            \n              ⭐⭐⭐⭐⭐\n            \n\n\nSimonCropp/MarkdownSnippets\n\n              23\n            \n2024/04/23\n\n              .NET\n            \n              N/A\n            \n              ⭐⭐⭐⭐⭐\n            \n\n\nendocode/snippetextractor\n\n              4\n            \n2014/08/16\n\n              C++\n            \n              N/A\n            \n              ⭐⭐⭐⭐⭐\n            \n\n\npolywrap/doc-snippets\n\n              3\n            \n2023/09/26\n\n              JS\n            \n              N/A\n            \n              ⭐⭐⭐⭐⭐\n            \n\n\nJulianCataldo/remark-embed\n\n              2\n            \n2022/09/22\n\n              JS\n            \n              N/A\n            \n              ⭐⭐⭐⭐⭐\n            \n\n\nxrd/oreilly-snippets\n\n              2\n            \n2015/10/15\n\n              Ruby\n            \n              N/A\n            \n              ⭐⭐⭐⭐⭐\n            \n\n\nDamonOehlman/injectcode\n\n              1\n            \n2021/08/01\n\n              JS\n            \n              N/A\n            \n              ⭐⭐⭐⭐⭐\n            \n\n\nelectrovir/markdown-code-example-inserter\n\n              1\n            \n2024/02/19\n\n              JS\n            \n              N/A\n            \n              ⭐⭐⭐⭐⭐\n            \n\n\nandersfischernielsen/Simple-Embedded-Markdown-Code-Snippets\n\n              1\n            \n2021/02/12\n\n              JS\n            \n              N/A\n            \n              ⭐⭐⭐⭐⭐\n            \n\n\nildar-shaimordanov/git-markdown-snippet\n\n              0\n            \n2021/09/14\n\n              Perl\n            \n              N/A\n            \n              ⭐⭐⭐⭐⭐\n            \n\n\nteyc/markdown-snippet\n\n              0\n            \n2024/01/22\n\n              Powershell\n            \n              N/A\n            \n              ⭐⭐⭐⭐⭐\n            \n\n\nmarc-bouvier-graveyard/baldir_markdown\n\n              0\n            \n2020/06/15\n\n              Python\n            \n              N/A\n            \n              ⭐⭐⭐⭐⭐\n            \n\n\ndineshsonachalam/markdown-autodocs\n\n              176\n            \n2022/09/19\n\n              JS\n            \n              GH Action\n            \n              ⭐⭐⭐⭐\n            \n\n\ntokusumi/markdown-embed-code\n\n              28\n            \n2022/01/05\n\n              Python\n            \n              GH Action\n            \n              ⭐⭐⭐⭐\n            \n\n\nsammndhr/gridsome-remark-embed-snippet\n\n              2\n            \n2021/06/14\n\n              JS\n            \nGridsome\n\n              ⭐⭐⭐⭐\n            \n\n\nNativeScript/markdown-snippet-injector\n\n              4\n            \n2019/01/24\n\n              JS\n            \n              N/A\n            \n              ⭐⭐⭐⭐\n            \n\n\nfuxingloh/remark-code-import-replace\n\n              0\n            \n2022/12/21\n\n              JS\n            \n              Remark?\n            \n              ⭐⭐⭐⭐\n            \n\n\nszkiba/mdcode\n\n              15\n            \n2014/02/12\n\n              Go\n            \n              N/A\n            \n              ⭐⭐⭐\n            \n\n\ndevincornell/pymddoc\n\n              0\n            \n2023/12/01\n\n              Python\n            \n              Python\n            \n              ⭐⭐⭐\n            \n\n\nshiftkey/scribble (docs)\n            \n              40\n            \n2013/08/08\n\n              .NET\n            \n              N/A\n            \n              ⭐⭐\n            \n\n\ncalebpeterson/jest-transformer-test-md\n\n              2\n            \n2020/08/21\n\n              JS\n            \n              Jest Tests\n            \n              ⭐⭐\n            \n\n\ntjstankus/commitate\n\n              0\n            \n2014/05/29\n\n              Ruby\n            \n              N/A\n            \n              ⭐\n            \n\n\nGitHub Docs: Creating a permanent link to a code snippet\n\n              N/A\n            \n              N/A\n            \n              N/A\n            \n              N/A\n            \n              ⭐\n            \n\n\njavierfernandes/markdown-exercises\n\n              1\n            \n2017/05/01\n\n              JS\n            \n              N/A\n            \n              ⭐\n            \n\n\ngatsby-remark-embed-snippet\n\n              N/A (55k)\n            \n2024/01/23\n\n              JS\n            \nGatsby\n\n              ⭐\n            \n\n\nARMmbed/snippet\n\n              6\n            \n2021/08/05\n\n              Python\n            \n              N/A\n            \n              ?\n            \n\n\ndrewavis/markdowninclude\n\n              1\n            \n2024/04/06\n\n              JS\n            \n              VSCode Extension\n            \n              ?\n            \n\n\nromnn/embedme\n\n              0\n            \n2024/04/18\n\n              Go\n            \n              N/A\n            \n              ?\n            \n\n\n\n    The 5 star projects have the bare minimum of being able to embed a file, and run via CLI.\n  \n\n\n    Snipinator does have other features (such as shell()), implemented as I needed them (and listed below) which I do not think any of these have in combination.\n  \n\n\n    Some of these projects are not CLIs.\n  \n\n\nmdx-js/mdx is the closest in terms of flexibility, but it is JS + components, which may not be everyone's cup of tea.\n  \n\n\nUsage:\n\n    Example template README: (./snipinator/examples/EXAMPLE.md.jinja2):\n  # A README\n\nHere is a code snippet:\n\n<!--{{ pysnippet(path='snipinator/examples/code.py', symbol='MyClass', backtickify='py', decomentify='nl') }}-->\n\nNote that `code.py` has a test:\n{{path('./snipinator/examples/code_test.py', link='md')}}.\n    Generating the README:\n  $ python -m snipinator.cli -t snipinator/examples/EXAMPLE.md.jinja2\n<!--\n\nWARNING: This file is auto-generated by snipinator. Do not edit directly.\nSOURCE: `snipinator/examples/EXAMPLE.md.jinja2`.\n\n-->\n# A README\n\nHere is a code snippet:\n\n<!---->\n```py\nclass MyClass:\n  \"\"\"This is a global class\"\"\"\n\n  def __init__(self, name):\n    self.name = name\n\n  def MyClassMethod(self):\n    \"\"\"This is a method of MyClass\"\"\"\n    print(self.name)\n```\n<!---->\n\nNote that `code.py` has a test:\n[./snipinator/examples/code_test.py](./snipinator/examples/code_test.py)."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What My Project Does\n    Overlaying intensity plots onto a geographical map using cartopy/matplotlib can be complex. So we created this map_plotter package to abstract away that complexity for a common use case.\n  \nInstallation\n\n    (opinionated use of conda to avoid cartopy dependency hell and install precompiled binaries)\n  conda install cartopy\ngit clone git@github.com:amentumspace/map_plotter.git\ncd map_plotter\npip install .\nUsage\nimport map_plotter\nmap_plotter.plot(lons_g, lats_g, variable, units=\"m/s\", img_name=\"image.png\",\n    save=True, plot=True, title=\"something\", zlims=[0,10])\n    Whereby:\n  \n\n\nlons_g and lats_g represent 2D matrices / grids of longitudes and latitudes.\n  \n\n\nvalues is the matrix of values to be plotted (same grid dimensions).\n  \n\n\nunits and img_name (self explanatory).\n  \n\n\nsave & plot boolean flags to save the file and plot to screen, respectively.\n  \n\n\nzlims define the color scale minimum and maximum.\n  \n\nTarget Audience\n    Python developers or data scientists or scientists or any Pythonista wanting a simple way to quickly plot an intensity map onto a geographical map.\n  Comparison\n    Differs from using cartopy and matplotlib in its ease-of-use, but it is less customisable (can't change projections, colors). Regardless, it's convenient and at least provides a starting point for customisation. Similar functionality can be had from geopandas or folium (although cartopy/matplotlib suited our needs better)."
},
{
    "title": "No title",
    "content": "What My Project Does\n  \n\n\n    This is a music video of the output of a Python program: https://www.youtube.com/watch?v=Sjk4UMpJqVs\n\n    I'm the author of Automate the Boring Stuff with Python and I teach people to code. As part of that, I created something I call \"scroll art\". Scroll art is a program that prints text from a loop, eventually filling the screen and causing the text to scroll up. (Something like those BASIC programs that are 10 PRINT \"HELLO\"; 20 GOTO 10)\n  \n    Once printed, text cannot be erased, it can only be scrolled up. It's an easy and artistic way for beginners to get into coding, but it's surprising how sophisticated they can become.\n  \n    The source code for this animation is here: https://github.com/asweigart/scrollart/blob/main/python/forbiddenzone.py (read the comments at the top to figure out how to run it with the forbiddenzonecontrol.py program which is also in that repo)\n  \n    The output text is procedurally generated from random numbers, so like a lava lamp, it is unpredictable and never exactly the same twice.\n  \n    This video is a collection of scroll art to the music of \"The Forbidden Zone,\" which was released in 1980 by the band Oingo Boingo, led by Danny Elfman (known for composing the theme song to The Simpsons.) It was used in a cult classic movie of the same name, but also the intro for the short-run Dilbert animated series.\n  \n\n\n    Target Audience\n  \n\n\n    Anyone (including beginners) who wants ideas for creating generative art without needing to know a ton of math or graphics concepts. You can make scroll art with print() and loops and random numbers. But there's a surprising amount of sophistication you can put into these programs as well.\n  \n\n\n    Comparison\n  \n\n\n    Because it's just text, scroll art doesn't have such a high barrier to entry compared with many computer graphics and generative artwork. The constraints lower expectations and encourage creativity within a simple context.\n  \n    I've produced scroll art examples on https://scrollart.org\n\n    I also gave a talk on scroll art at PyTexas 2024: https://www.youtube.com/watch?v=SyKUBXJLL50"
},
{
    "title": "No title",
    "content": "After playing around with a dataframe—applying filters or other transformations—I'm curious about your methods for reviewing the changes.\n  \n    In VS Code, the variable explorer is quite handy for a quick look at the modified dataframe. Alternatively, when working in a Jupyter notebook within VS Code, exporting the data to an Excel file provides a detailed view and allows for an easy deep dive into the results. What are your preferred practices for ensuring your data adjustments are precisely what you intended?"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hi everybody,\n  \n    over the last year I've been developing a library that adds some Cython 3.0 annotations to existing python code.\n  \nWhat My Project Does:\n\n    For example if it sees a for i in range(): in a function it recognizes i as an integer and adds a i = cython.declare(cython.int)line at the beginning of the function.\n  \n    It actually uses the built-in ast module under the hood for parsing, I found it a super useful library!\n  \nTarget Audience:\n\n    It is a side project I made mainly for fun. I don't know if it can be of interest to anybody, or if it could have some potential utility.\n  \nComparison:\n\n    I did not find anything similar. There are a lot of very cool projects like mypyc for example, but nothing that does this tiny little code generation specific to Cython.\n  \n    The link to the repository is here:\n  \nhttps://github.com/nucccc/markarth"
},
{
    "title": "No title",
    "content": "Hey folks, looking to use one library to implement some background scheduling logic on my application.\n  \n    I find in Google search APScheduler to be frequently mentioned, but I can see the Schedule package has more GH stars.\n  \n    Was curious if anybody has used one of them, and which one would you recommend based on your own experience."
},
{
    "title": "No title",
    "content": "Excited to share my personal open-source project: Notolog - Python Markdown Editor (MIT License).\n  \n    The main motivation for developing another markdown editor was my passion for learning new things and enhancing my development skills in Python. I developed it in my spare time over a few months, despite having no prior experience in creating full-scale Python applications.\n  \nWhat My Project Does\n\n    ∗ Multiplatform\n  \n    ∗ Markdown async syntax highlighting created by me\n  \n    ∗ Several pre-installed color themes\n  \n    ∗ Supports English and 17 other languages right out of the box\n  \n    ∗ Integration with OpenAI API for AI-assisted features\n  \n    ∗ Optional file encryption/decryption\n  \nTarget Audience\n\n    Primarily developers who write markdown documents and notes.\n  \nComparison\n\n    This is more of a personal learning project, so it's hard to compare it directly with others.\n  \nHow to install\n\n    Discover Notolog on GitHub 🌟 and PyPI.\n  \n    Installation is as easy as running a single command:\n  pip install notolog"
},
{
    "title": "No title",
    "content": "I have been using zabbix for monitoring a lot of metrics in my work, none of the most popular zabbix were capable of doing async tasks, so I've developed some simple package capable of doing this.\n  \n    Tests, examples and how-tos can be found here: https://github.com/gustavofbreunig/zabbix-sender-async\n\nWhat My Project Does\n\n    Send zabbix sender messages using asyncio tasks.\n  \nTarget Audience\n\n    SysAdmins who use Zabbix to monitor a large number of metrics.\n  \nComparison\n\n    Instead of doing traditional way, using these abandoned library: https://github.com/adubkov/py-zabbix\nfrom pyzabbix import ZabbixMetric, ZabbixSender\n\n# Send metrics to zabbix trapper\npacket = [\n  ZabbixMetric('hostname1', 'test[cpu_usage]', 2),\n  ZabbixMetric('hostname1', 'test[system_status]', \"OK\"),\n  ZabbixMetric('hostname1', 'test[disk_io]', '0.1'),\n  ZabbixMetric('hostname1', 'test[cpu_usage]', 20, 1411598020),\n]\n\nresult = ZabbixSender(use_config=True).send(packet)\n    You can do this:\n  async def sendmetrics():\n    sender = AsyncSender('localhost', 10051)\n    metric = ItemData(host='hostname', key='test.metric.text', value='test package import')\n    result = await sender.send(metric)"
},
{
    "title": "No title",
    "content": "What my project does: It supports services like IBM Watson, Acapela and Stream labs' demo websites to convert your text to speech.\n  \nTarget audience: It's a toy project and would not recommend you to use in Production.\n  \nComparison: It's wayyyyy easy to use. Just pip install and use in your project. No extra setup required like other libraries. Also supports various languages and voices and accents. Check docs for more.\n  \nHere is the link to repository.\n  \n    Please go do check it out and star it if it's helpful to you guys. Thank you.\n  \n    I made this library taking inspiration from this php tts library by chrisjp."
},
{
    "title": "No title",
    "content": "I made a library to create interactive plots in the terminal (pip install itrm). It uses braille characters (by default) to display the data with sub-character resolution. There are several keybindings for moving a vertical cursor left and right, for zooming in or out on data, and for changing which curve to focus on. There are occasions (such as when working with a server) where MatPlotLib is not an option and the terminal is the only available tool. But, in my opinion, it is actually faster to use this tool (itrm) to zoom in on interesting parts of data and analyze patterns than using other tools like MatPlotLib. In fact, with large data sets (~1 million points), this tool actually renders faster than MatPlotLib. Please check it out and let know what you think."
},
{
    "title": "No title",
    "content": "Hi! I want to share a library I've built recently. IBind is a REST and WebSocket Python client for Interactive Brokers Client Portal Web API. It is directed at IBKR users.\n  \n    You can find IBind on GitHub: https://github.com/Voyz/ibind\n\nWhat My Project Does:\n\n    It is a REST and WebSocket API for the Interactive Brokers' Web API.\n  \n    I'm particularly proud of a few things in this release:\n  \n\n\n    The REST and WebSocket API clients are based on an abstract base class RestClient and WsClient accordingly. These could be implemented to use some other Web APIs in a relatively straightforward way. I have in fact used a version of that WsClient for a cryptocurrency WebSocket API, and it is nice to see it adapt to a different environment.\n  \n\n\n    I've covered most of the codebase with automated tests (appx 80%). Contrary to some of my other libraries, these are mainly integration tests which feel to provide a stronger test coverage than only unit tests.\n  \n\n\n    I've learned how to use class mixins in this project, and it aids the maintainability by a lot! The REST client itself is pretty barebone, but has a lot of mixin classes - all corresponding to the endpoint categories the broker uses, making it easy to search for the right piece of code and documentation.\n  \n\n\n    There's a lot of things that make this client as plug-and-play as possible. The broker requires the user to specify a bunch of things - account ids, certificates, URLs, etc. - which the class either reads from the environment variables or assumes (given that some things would be common for most users). In either case, all these are customisable by parameters if needed, but it is nice to just write client = IbkrClient() in various projects having set just a couple of env vars.\n  \n\n\n    I think the documentation is pretty in-depth but readable. It's always hard to judge whether docs are well written, but I think it is nicely broken down. Also, I managed to use pydoc-markdown package to create API reference in markdown, which works nicely with the GitHub Wiki. I'd prefer it to be even easier, but compared to Sphinx and readthedocs it's a much quicker job.\n  \n\n\n    The WebSocket class does a ton to keep the connection alive and recover from connection losses. Maintaining active subscriptions after a re-connect can be a real pain, and I think this class does it in a nice and reliable way. I've tested it for various types of connectivity loss, and it manages to recover and re-establish the WebSocket data stream. Pretty crucial in the trading environment.\n  \n\n\n    I made a nice logo for it 🥳\n  \n\n\nTarget Audience:\n\n    Traders using IBKR who want to automate their trading through this Web API.\n  \nComparison (A brief comparison explaining how it differs from existing alternatives.) :\n\n    There are two similar libraries that I know of. They aren't bad, but seem not very well maintained and incomplete:\n  \n\n\nhttps://github.com/areed1192/interactive-brokers-api - outdated and stale, last update 3 years ago\n  \n\n\nhttps://github.com/utilmon/EasyIB - stale and incomplete\n  \n\n\n    The library I've published covers a much wider range of endpoints, adds WebSocket support and a bunch of wrapper methods to simplify the usage of the API.\n  \n    IBind has a bunch of features that make using the IBKR APIs much easier. Some of these are:\n  \n    REST:\n  \n\n\nAutomated question/answer handling - streamlining placing orders.\n  \n\n\nParallel requests - speeding up collection of price data.\n  \n\n\nRate limiting - guarding against account bans.\n  \n\n\nConid unpacking - helping to find the right contract.\n  \n\n\n    WebSocket:\n  \n\n\nWebSocket thread lifecycle handling - ensuring the connection is alive.\n  \n\n\nThread-safe Queue data stream - exposing the collected data in a safe way.\n  \n\n\nInternal subscription tracking - recreating subscriptions upon re-connections.\n  \n\n\nHealth monitoring - Acting on unusual ping or heartbeat.\n  \n\n\n    REST Example:\n  from ibind import IbkrClient\n\n# Construct the client\nclient = IbkrClient()\n\nprint(client.tickle().data)\n    WebSocket Example:\n  from ibind import IbkrWsKey, IbkrWsClient\n\n# Construct the client.\nws_client = IbkrWsClient(start=True)\n\n# Choose the WebSocket channel\nkey = IbkrWsKey.PNL\n\n# Subscribe to the PNL channel\nws_client.subscribe(channel=key.channel)\n\nprint(ws_client.get(key))\n    I just wanted to share my experience of publishing Open Source. For some reason I get a lot of motivation when I can publish code that makes peoples' lives easier. The library could use some code review on it, so if you’d feel like reading some code and helping out - drop me a message. Other than that, happy to answer any questions, and - if you are an algo trader - let me know if you get a chance to use it. Thanks for reading!"
},
{
    "title": "No title",
    "content": "Python Streamlit is a terrific tool for creating interactive data visualizations.\n  \n    It packages all your visualizations up into a neat little application - including charts and maps - and displays them in your default browser. No muss, no fuss.\n  \n    Recently, I found a new dataset (to me) on the UN High Commission for Refugees (UNHCR) website. It contains country-to-country movements for refugees both from origin country and country of asylum\n  \n    Using this dataset, here's a step-by-step on how to code a Python Streamlit application that has:\n  \n\n\nA dropdown menu to select by country\n  \n\n\nA second dropdown menu to select by year\n  \n\n\nRadio buttons (2) to select country of origin or county of asylum\n  \n\n\nA global choropleth map to display the results by country and year.\n  \n\n\n    Free article HERE."
},
{
    "title": "No title",
    "content": "Hey folks,\n  \n    I've been dabbling with a Python project recently that's all about making life easier for us I.T. people. It's a nifty little tool that calculates IP subnets and does IP calculations from the command or CLI.\n  \n    Here's the GitHub link and the code:\n  \nhttps://github.com/nicanorflavier/ipnet\n\n    I’m pretty stoked about it, but I know there’s always room for improvement. So, I thought, better to turn to than the wise minds of this python community?\n  \n    I’m all ears for any feedback, tips, tricks, or advice you guys might have. Thanks a ton in advance!"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Wrote this as a tool to keep README coverage badges up to date without relying on 3rd party services or having to do anything extra, thought others might get some utility out of it: coverage-pre-commit.\n  \n    A .coverage file is expected at the root of the project, generated by running coverage run directly or using a plugin such as pytest-cov when running tests.\n  \n    Most convenient when used as a pre-push hook imo. Feel free to opine, be it positive or negative!"
},
{
    "title": "No title",
    "content": "I'm thrilled to share my first open-source project with you all: PyWolt! 🎉\n  \nPyWolt is a Python library that makes it super easy to interact with the Wolt API.\n  What My Project Does:\n\n\n    Discover Venues: Find nearby spots to grab a bite.\n  \n\n\n    Explore Menus: Dive into a venue's menu and pick your favorites.\n  \n\nTarget Audience:\n\n\nSoftware Engineers: Professionals who build web or mobile applications, particularly those in the food delivery or restaurant industry, looking to incorporate Wolt's services seamlessly into their platforms.\n  \n\n\nData Scientists/Analysts: Individuals analyzing food delivery data, consumer behavior, or market trends, who may utilize PyWolt to gather data from Wolt's API for analysis and insights.\n  \n\n\nStudents/Learners: Those studying Python programming, web development, or API integration, who can use PyWolt as a practical example or learning tool to understand how to interact with RESTful APIs in Python.\n  \n\n\nFreelancers/Entrepreneurs: Independent developers or startup founders looking to build new food-related applications or services leveraging Wolt's platform without reinventing the wheel.\n  \n\nComparison:\n\n\nwoltcheck: only offers a script to check if a wolt restaurant is ready to deliver to your location.\n  \n\n\nwhat-to-eat: a pretty neat cli tool that offers all of pywolt's functionality. In my opinion it overcomplicates things a little, and doesn't offer straight-forward RESTful functionality to interact with the API itself."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I'm Paul, one of the creators of Rio. Over the years I've tried many different established python GUI frameworks, but none of them really satisfied me. So I teamed up with a few like minded developers and spent the last few months to create our own framework. Rio is the result of this effort.\n  What My Project Does\n    Rio is a brand new GUI framework that lets you create modern web apps in just a few lines of Python. Our goal is to simplify web and app development, so you can focus on the things you care about, instead of wasting countless hours on frustrating user interface details.\n  \n    We do this by following the core principles of Python that we all know and love. Python is supposed to be simple and compact - and so is Rio. There is no need to learn any additional languages such as HTML, CSS or JavaScript, because all of the UI, Logic, Components and even layouting is done entirely in Python. There’s not even a distinction between front-end and back-end. Rio handles all of the communication transparently for you.\n  Key Features\n\n\nFull-Stack Web Development: Rio handles front-end and backend for you. In fact, you won't even notice they exist. Create your UI, and Rio will take care of the rest.\n  \n\n\nPython Native: Rio apps are written in 100% Python, meaning you don't need to write a single line of CSS or JavaScript.\n  \n\n\nModern Python: We embrace modern Python features, such as type annotations and asynchrony. This keeps your code clean and maintainable, and helps your code editor help you out with code completions and type checking.\n  \n\n\nPython Debugger Compatible: Since Rio runs on Python, you can connect directly to the running process with a debugger. This makes it easy to identify and fix bugs in your code.\n  \n\n\nDeclarative Interface: Rio apps are built using reusable components, inspired by React, Flutter & Vue. They're declaratively combined to create modular and maintainable UIs.\n  \n\n\nBatteries included: Over 50 builtin components based on Google's Material Design\n  \n\n\nDemo Video\nTarget Audience\n    Whether you need to build dashboards, CRUD apps, or just want to make a personal website, Rio makes it possible without any web development knowledge. Because Rio was developed from the ground up for Python programmers, it was designed to be concise and readable, just like Python itself.\n  Comparison\n    Rio doesn't just serve HTML templates like you might be used to from frameworks like Flask. In Rio you define components as simple dataclasses with a React/Flutter style build method. Rio continuously watches your attributes for changes and updates the UI as necessary.\n  class MyComponent(rio.Component):\n    clicks: int = 0\n\n    def _on_press(self) -> None:\n        self.clicks += 1\n\n    def build(self) -> rio.Component:\n        return rio.Column(\n            rio.Button('Click me', on_press=self._on_press),\n            rio.Text(f'You clicked the button {self.clicks} time(s)'),\n        )\n\napp = rio.App(build=MyComponent)\napp.run_in_browser()\n    Notice how there is no need for any explicit HTTP requests. In fact there isn't even a distinction between frontend and backend. Rio handles all communication transparently for you. Unlike ancient libraries like Tkinter, Rio ships with over 50 builtin components in Google's Material Design. Moreover the same exact codebase can be used for both local apps and websites.\n  We Want Your Feedback!\n    The first alpha version of Rio is available on PyPi now:\n  pip install rio-ui\nrio new my-project --template tic-tac-toe\ncd my-project\nrio run\n\n\nDiscord\n\n\n\nGitHub\n\n\n\nTutorial\n\n\n\nWebsite\n\n\n\n    Let us know what you think - any feedback, ideas, or even a helping hand are hugely welcome! Just hop on our Discord server and say hello!"
},
{
    "title": "No title",
    "content": "Python 3.13.0 beta 1 was released today.\n  \n    The feature I'm most excited about is the new Python REPL.\n  \nHere's a summary of my favorite features in the new REPL along with animated gifs.\n  \n    The TLDR:\n  \n\n\n    Support for block-leveling history and block-level editing\n  \n\n\n    Pasting code (even with blank lines within it) works as expected now\n  \n\n\n    Typing exit will exit (no more Use exit() or Ctrl-D (i.e. EOF) to exit message)"
},
{
    "title": "No title",
    "content": "I was doing some light reading and stumbled across Steve Gribbles Power vs Speed Calculator and thought I'd give it a go at rebuilding it based on his Physics model using Python. Then I wrote an article about. Thought I'd share it with you all: Calculating Virtual Cycling Power (jasonlei.com)"
},
{
    "title": "No title",
    "content": "InterProcessPyObjects Python package\ngithub.com/FI-Mihej/InterProcessPyObjects If you like the project, consider giving it a star on GitHub to show your support and help further development. :)\n  \npypi.org/project/InterProcessPyObjects\nWhat My Project Does\n\n    InterProcessPyObjects is a part of the Cengal library. If you have any questions or would like to participate in discussions, feel free to join the Cengal Discord. Your support and involvement are greatly appreciated as Cengal evolves.\n  \n\n    This high-performance package delivers blazing-fast inter-process communication through shared memory, enabling Python objects to be shared across processes with exceptional efficiency. By minimizing the need for frequent serialization-deserialization, it enhances overall speed and responsiveness. The package offers a comprehensive suite of functionalities designed to support a diverse array of Python types and facilitate asynchronous IPC, optimizing performance for demanding applications.\n  Target Audience\n    This project is designed for production environments, offering a stable API suitable for developers looking to implement fast inter-process communication. Whether you're building complex systems or require robust data sharing and modification across processes, InterProcessPyObjects is ready to meet your needs.\n  Comparison\n    Comparison with multiprocessing.shared_memory\n\n    While both InterProcessPyObjects and multiprocessing.shared_memory facilitate inter-process communication, there are several key differences to note. Unlike multiprocessing.shared_memory, InterProcessPyObjects offers the following enhancements:\n  \n\n\n    High-Performance Mutable Objects: Both connected processes can modify shared objects at runtime, and these changes are immediately reflected on the other side. This feature not only increases flexibility but also delivers exceptional performance, with the capability to handle up to several million changes per second.\n  \n\n\n    Synchronization Features: Ensures that operations are thread-safe and data integrity is maintained across processes.\n  \n\n\n    Message Queue: Integrates a system for queuing messages, making communication between processes more structured and reliable.\n  \n\n\n    Extended Type Support: Supports a broad range of data types, including custom classes, which goes beyond the basic types typically handled by multiprocessing.shared_memory.\n  \n\n\n    These features make InterProcessPyObjects a more robust option for developers requiring advanced inter-process communication capabilities.\n  API State\n    Stable. Guaranteed not to have breaking changes in the future. (see github.com/FI-Mihej/InterProcessPyObjects?tab=readme-ov-file#api-state for details)\n  Key Features\n\n\n    Shared Memory Communication:\n  \n\n\n    Enables sharing of Python objects directly between processes using shared memory.\n  \n\n\n    Utilizes a linked list of global messages to inform connected processes about new shared objects.\n  \n\n\n\n\n    Lock-Free Synchronization:\n  \n\n\n    Uses memory barriers for efficient communication, avoiding slow syscalls.\n  \n\n\n    Ensures each process can access and modify shared memory without contention.\n  \n\n\n\n\n    Supported Python Types:\n  \n\n\n    Handles various Python data structures including:\n  \n\n\n    Basic types: None, bool, 64-bit int, large int (arbitrary precision integers), float, complex, bytes, bytearray, str.\n  \n\n\n    Standard types: Decimal, slice, datetime, timedelta, timezone, date, time\n\n\n\n    Containers: tuple, list, classes inherited from: AbstractSet (frozenset), MutableSet (set), Mapping and MutableMapping (dict).\n  \n\n\n    Pickable classes instances: custom classes including dataclass\n\n\n\n\n\n    Allows mutable containers (lists, sets, mappings) to save basic types (None, bool, 64 bit int, float) internally, optimizing memory use and speed.\n  \n\n\n\n\n    NumPy and Torch Support:\n  \n\n\n    Supports numpy arrays by creating shared bytes objects coupled with independent arrays.\n  \n\n\n    Supports torch tensors by coupling them with shared numpy arrays.\n  \n\n\n\n\n    Custom Class Support:\n  \n\n\n    Projects pickable custom classes instances (including dataclasses) onto shared dictionaries in shared memory.\n  \n\n\n    Modifies the class instance to override attribute access methods, managing data fields within the shared dictionary.\n  \n\n\n    supports classes with or without __dict__ attr\n  \n\n\n    supports classes with or without __slots__ attr\n  \n\n\n\n\n    Asyncio Compatibility:\n  \n\n\n    Provides a wrapper module for async-await functionality, integrating seamlessly with asyncio.\n  \n\n\n    Ensures asynchronous operations work smoothly with the package's lock-free approach.\n  \n\n\n\nMain principles\n\n\n    only one process has access to the shared memory at the same time\n  \n\n\n    working cycle:\n  \n\n\n    work on your tasks\n  \n\n\n    acquire access to shared memory\n  \n\n\n    work with shared memory as fast as possible (read and/or update data structures in shared memory)\n  \n\n\n    release access to shared memory\n  \n\n\n    continue your work on other tasks\n  \n\n\n\n\n    do not forget to manually destroy your shared objects when they are not needed already\n  \n\n\n    feel free to not destroy your shared object if you need it for a whole run and/or do not care about the shared memory waste\n  \n\n\n    data will not be preserved between Creator's sessions. Shared memory will be wiped just before Creator finished its work with a shared memory instance (Consumer's session will be finished already at this point)\n  \n\nExamples\n\n\n    An async examples (with asyncio):\n  \n\n\nsender.py\n\n\n\nreceiver.py\n\n\n\nshared_objects__types.py\n\n\n\n\nReceiver.py performance measurements\n\n\n    CPU: i5-3570@3.40GHz (Ivy Bridge)\n  \n\n\n    RAM: 32 GBytes, DDR3, dual channel, 655 MHz\n  \n\n\n    OS: Ubuntu 20.04.6 LTS under WSL2. Windows 10\n  \n\nasync with ashared_memory_context_manager.if_has_messages() as shared_memory:\n    # Taking a message with an object from the queue.\n    sso: SomeSharedObject = shared_memory.value.take_message()  # 5_833 iterations/seconds\n\n    # We create local variables once in order to access them many times in the future, ensuring high performance.\n    # Applying a principle that is widely recommended for improving Python code.\n    company_metrics: List = sso.company_info.company_metrics  # 12_479 iterations/seconds\n    some_employee: Employee = sso.company_info.some_employee  # 10_568 iterations/seconds\n    data_dict: Dict = sso.data_dict  # 16_362 iterations/seconds\n    numpy_ndarray: np.ndarray = data_dict['key3']  # 26_223 iterations/seconds\n\n# Optimal work with shared data (through local variables):\nasync with ashared_memory_context_manager as shared_memory:\n    # List\n    k = company_metrics[CompanyMetrics.avg_salary]  # 1_535_267 iterations/seconds\n    k = company_metrics[CompanyMetrics.employees]  # 1_498_278 iterations/seconds\n    k = company_metrics[CompanyMetrics.in_a_good_state]  # 1_154_454 iterations/seconds\n    k = company_metrics[CompanyMetrics.websites]  # 380_258 iterations/seconds\n    company_metrics[CompanyMetrics.annual_income] = 2_000_000.0  # 1_380_983 iterations/seconds\n    company_metrics[CompanyMetrics.employees] = 20  # 1_352_799 iterations/seconds\n    company_metrics[CompanyMetrics.avg_salary] = 5_000.0  # 1_300_966 iterations/seconds\n    company_metrics[CompanyMetrics.in_a_good_state] = None  # 1_224_573 iterations/seconds\n    company_metrics[CompanyMetrics.in_a_good_state] = False  # 1_213_175 iterations/seconds\n    company_metrics[CompanyMetrics.avg_salary] += 1.1  # 299_415 iterations/seconds\n    company_metrics[CompanyMetrics.employees] += 1  # 247_476 iterations/seconds\n    company_metrics[CompanyMetrics.emails] = tuple()  # 55_335 iterations/seconds (memory allocation performance is planned to be improved)\n    company_metrics[CompanyMetrics.emails] = ('sails@company.com',)  # 30_314 iterations/seconds (memory allocation performance is planned to be improved)\n    company_metrics[CompanyMetrics.emails] = ('sails@company.com', 'support@company.com')  # 20_860 iterations/seconds (memory allocation performance is planned to be improved)\n    company_metrics[CompanyMetrics.websites] = ['http://company.com', 'http://company.org']  # 10_465 iterations/seconds (memory allocation performance is planned to be improved)\n    \n    # Method call on a shared object that changes a property through the method\n    some_employee.increase_years_of_employment()  # 80548 iterations/seconds\n\n    # Object properties\n    k = sso.int_value  # 850_098 iterations/seconds\n    k = sso.str_value  # 228_966 iterations/seconds\n    sso.int_value = 200  # 207_480 iterations/seconds\n    sso.int_value += 1  # 152_263 iterations/seconds\n    sso.str_value = 'Hello. '  # 52_390 iterations/seconds (memory allocation performance is planned to be improved)\n    sso.str_value += '!'  # 35_823 iterations/seconds (memory allocation performance is planned to be improved)\n\n    # Numpy.ndarray\n    numpy_ndarray += 10  # 403_646 iterations/seconds\n    numpy_ndarray -= 15  # 402_107 iterations/seconds\n\n    # Dict\n    k = data_dict['key1']  # 87_558 iterations/seconds\n    k = data_dict[('key', 2)]  # 49_338 iterations/seconds\n    data_dict['key1'] = 200  # 86_744 iterations/seconds\n    data_dict['key1'] += 3  # 41_409 iterations/seconds\n    data_dict['key1'] *= 1  # 40_927 iterations/seconds\n    data_dict[('key', 2)] = 'value2'  # 31_460 iterations/seconds (memory allocation performance is planned to be improved)\n    data_dict[('key', 2)] = data_dict[('key', 2)] + 'd'  # 18_972 iterations/seconds (memory allocation performance is planned to be improved)\n    data_dict[('key', 2)] = 'value2'  # 10_941 iterations/seconds (memory allocation performance is planned to be improved)\n    data_dict[('key', 2)] += 'd'  # 16_568 iterations/seconds (memory allocation performance is planned to be improved)\n\n# An example of non-optimal work with shared data (without using a local variables):\nasync with ashared_memory_context_manager as shared_memory:\n    # An example of a non-optimal method call (without using a local variable) that changes a property through the method\n    sso.company_info.some_employee.increase_years_of_employment()  # 9_418 iterations/seconds\n\n    # An example of non-optimal work with object properties (without using local variables)\n    k = sso.company_info.income  # 20_445 iterations/seconds\n    sso.company_info.income = 3_000_000.0  # 13_899 iterations/seconds\n    sso.company_info.income *= 1.1  # 17_272 iterations/seconds \n    sso.company_info.income += 500_000.0  # 18_376 iterations/seconds\n    \n    # Example of non-optimal usage of numpy.ndarray without a proper local variable\n    data_dict['key3'] += 10  # 6_319 iterations/seconds\n\n# Notify the sender about the completion of work on the shared object\nasync with ashared_memory_context_manager as shared_memory:\n    sso.some_processing_stage_control = True  # 298_968 iterations/secondsThroughput Benchmarks\n\n\n    CPU: i5-3570@3.40GHz (Ivy Bridge)\n  \n\n\n    RAM: 32 GBytes, DDR3, dual channel, 655 MHz\n  \n\n\n    OS: Ubuntu 20.04.6 LTS under WSL2. Windows 10\n  \n\nRefference results (sysbench)sysbench memory --memory-oper=write run5499.28 MiB/secBenchmarks results table GiB/s\n\n\n\n            Approach\n          \n            sync/async\n          \n            Throughput GiB/s\n          \n\n\n\n\n\n\n              InterProcessPyObjects (sync)\n            \n              sync\n            \n              3.770\n            \n\n\n              InterProcessPyObjects + uvloop\n            \n              async\n            \n              3.222\n            \n\n\n              InterProcessPyObjects + asyncio\n            \n              async\n            \n              3.079\n            \n\n\n              multiprocessing.shared_memory *\n            \n              sync\n            \n              2.685\n            \n\n\n              uvloop.UnixDomainSockets\n            \n              async\n            \n              0.966\n            \n\n\n              asyncio + cengal.Streams\n            \n              async\n            \n              0.942\n            \n\n\n              uvloop.Streams\n            \n              async\n            \n              0.922\n            \n\n\n              asyncio.Streams\n            \n              async\n            \n              0.784\n            \n\n\n              asyncio.UnixDomainSockets\n            \n              async\n            \n              0.708\n            \n\n\n              multiprocessing.Queue\n            \n              sync\n            \n              0.669\n            \n\n\n              multiprocessing.Pipe\n            \n              sync\n            \n              0.469\n            \n\n\n\n* multiprocessing.shared_memory.py - simple implementation. This is a simple implementation because it uses a similar approach to the one used in uvloop.*, asyncio.*, multiprocessing.Queue, and multiprocessing.Pipe benchmarking scripts. Similar implementations are expected to be used by the majority of projects.\n  Todo\n\n\n    Connect more than two processes\n  \n\n\n    Use third-party fast hashing implementations instead of or in addition to built in hash() call\n  \n\n\n    Continuous performance improvements\n  \n\nConclusion\n    This Python package provides a robust solution for inter-process communication, supporting a variety of Python data structures, types, and third-party libraries. Its lock-free synchronization and asyncio compatibility make it an ideal choice for high-performance, concurrent execution.\n  Based on Cengal\n    This is a stand-alone package for a specific Cengal module. Package is designed to offer users the ability to install specific Cengal functionality without the burden of the library's full set of dependencies.\n  \n    The core of this approach lies in our 'cengal-light' package, which houses both Python and compiled Cengal modules. The 'cengal' package itself serves as a lightweight shell, devoid of its own modules, but dependent on 'cengal-light[full]' for a complete Cengal library installation with all required dependencies.\n  \n    An equivalent import:\n  from cengal.hardware.memory.shared_memory import *\nfrom cengal.parallel_execution.asyncio.ashared_memory_manager import *\n    Cengal library can be installed by:\n  pip install cengal\nhttps://github.com/FI-Mihej/Cengal\n\nhttps://pypi.org/project/cengal/\nProjects using Cengal\n\n\nCengalPolyBuild - A Comprehensive and Hackable Build System for Multilingual Python Packages: Cython (including automatic conversion from Python to Cython), C/C++, Objective-C, Go, and Nim, with ongoing expansions to include additional languages. (Planned to be released soon)\n  \n\n\ncengal_app_dir_path_finder - A Python module offering a unified API for easy retrieval of OS-specific application directories, enhancing data management across Windows, Linux, and macOS\n  \n\n\ncengal_cpu_info - Extended, cached CPU info with consistent output format.\n  \n\n\ncengal_memory_barriers - Fast cross-platform memory barriers for Python.\n  \n\n\nflet_async - wrapper which makes Flet async and brings booth Cengal.coroutines and asyncio to Flet (Flutter based UI)\n  \n\n\njustpy_containers - wrapper around JustPy in order to bring more security and more production-needed features to JustPy (VueJS based UI)\n  \n\n\nBensbach - decompiler from Unreal Engine 3 bytecode to a Lisp-like script and compiler back to Unreal Engine 3 bytecode. Made for a game modding purposes\n  \n\n\nRealistic-Damage-Model-mod-for-Long-War - Mod for both the original XCOM:EW and the mod Long War. Was made with a Bensbach, which was made with Cengal\n  \n\n\nSmartCATaloguer.com - TagDB based catalog of images (tags), music albums (genre tags) and apps (categories)\n  \n\nLicense\n    Licensed under the Apache License, Version 2.0."
},
{
    "title": "No title",
    "content": "Source code\n\nWhat My Project Does:\n\n    It acts as a wrapper for the AzuraCast API, providing custom functions and classes for more straightforward use of the API in python projects.\n  \nTarget Audience:\n\n    Python users who are interested in programmatically interacting with online radios hosted on AzuraCast.\n  \nComparison:\n\n    The idea of API Wrappers is not new. However, I noticed that the only existing wrapper for this API is written in PHP, which I am not experienced with. I created this project so I, and other python programmers by extension, could have an easier time working with the API.\n  \n    This is my first \"major\" programming project, so thoughts and feedback are welcome and greatly appreciated.\n  \n    PS: Shoutout to PRAW for \"inspiring\" basically everything about the project's structure and functionality."
},
{
    "title": "No title",
    "content": "PYPI\n\n    (From the README, Released Last Year, Edited by Grammarly)\n  \nGithub\n\npip install diskcache\n\n    The cloud-based computing of 2024 puts a premium on memory. Gigabytes of space are left on disks as processes vie for memory. Memcached (and sometimes Redis) is used as a cache among these processes. Wouldn’t it be nice to leverage empty disk space for caching?\n  \n    Django is Python’s most popular web framework and has several caching backends. Unfortunately, the file-based cache in Django is essentially broken. The culling method is random and large caches repeatedly scan a cache directory which slows linearly with growth. Can you allow it to take sixty milliseconds to store a key in a cache with a thousand items?\n  \n    Is it that fast?\n  In [1]: import pylibmc\nIn [2]: client = pylibmc.Client(['127.0.0.1'], binary=True)\nIn [3]: client[b'key'] = b'value'\nIn [4]: %timeit client[b'key']\n\n10000 loops, best of 3: 25.4 µs per loop\n\nIn [5]: import diskcache as dc\nIn [6]: cache = dc.Cache('tmp')\nIn [7]: cache[b'key'] = b'value'\nIn [8]: %timeit cache[b'key']\n\n100000 loops, best of 3: 11.8 µs per loop"
},
{
    "title": "No title",
    "content": "I am using quart framework (https://quart.palletsprojects.com) for a number of microservices in a SaaS application. However, I hardly hear anything about this framework on any social media platform which seems to be dominated by FastAPI. Also I'm unable to find which all projects/companies are using this framework. All this is leading to anxiety around the future of this project.\n  \n    Are there any well known projects / companies which are using this framework for microservices?"
},
{
    "title": "No title",
    "content": "I made this visualisation with this code.\n  \n    I have three questions:\n  \n\n\n    Is Plotly supposed to be this cumbersome to tweak? Would other libraries require the same amount of code to add the details I did?\n  \n\n\n    Can my code be reduced in size? Maybe it's me who is complicating things with Plotly and there are easier ways to do what I am doing.\n  \n\n\n    Any R enthusiast who can tell me how much shorter this code would look like with ggplot2? I asked ChatGPT but the result was garbage.\n  \n\n\n    Bonus question: This took me an entire morning. Is it normal to be \"that slow\" to plot a simple figure?"
},
{
    "title": "No title",
    "content": "Hi all, I initially started this adventure by trying to automate bug fixes with the help of LLMs. However, I received feedback saying the fixes aren't always correct, leading to the question: why bother reviewing PRs that might add more issues? (It's really hard for LLMs to say \"I don't know\").\n\n    So, I decided to focus on reliability perfecting unit tests.\n  \n    The source code is available at: https://github.com/CaptureFlow/captureflow-py\n\nWhat My Project Does:\n\n    It incorporates a tracer client-side Python library and a backend that accumulates such traces and is capable of proposing code improvements and generating tests for your repository. It traverses the execution graph, extracts relevant parts, enriches them with implementation data from the GitHub API, and then generates tests with the help of GPT4.\n  \nTarget Audience:\n\n    Python users interested in discovering what LLMs can achieve when given detailed runtime information from your app. Generally, this approach somewhat reverses the concept of TDD, but in my day job I deal with many legacy apps that have poor test coverage, and having it really helps. I suspect I’m not alone in finding value in this.\n  \nComparison:\n\n    I think idea of using LLMs to generate tests is not new, but generating them actually based on application's performance is inspired by Jane Street's article on how they automated test boilerplate creation and recent Facebook's research paper.\n  \nDisclaimer: More work needs to be done to make it work for any Python app and not just a subset of FastAPI servers. I'm curious if you folks would find it useful.\n  \nExample: you can check this PR for reference. feedback / stars / contributions are welcome."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hello all,\n  \n    This is probably my first post here. I usuallly lurk around here and Django subreddits.\n  \n    I've been brewing up an idea and I need your input before I take the plunge!\n  \n    Picture this: a website like FreeCodeCamp but for python and related technologies, a learning oasis where anyone can kickstart their journey from Python newbie to job-ready pro, and it's all free!\n  \n    But here's the thing, I want this to be our platform, crafted with your needs and dreams in mind. So, before I start, I need to know: would this be something that gets you excited?\n  \n    Imagine quizzes helping you find your starting point, interactive challenges that keep you in the zone, and a supportive community to cheer you on every step of the way. Plus, videos, written tutorials, and a progress tracker to keep you motivated!\n  \n    What would make you go, \"Wow, I need this in my life!\"?\n  \n    What features would you love to see?\n  \n    Any suggestions or wild ideas ?\n  \n    My aim is to give back to the community by assisting new learners in navigating common pitfalls when they start their Python journey"
},
{
    "title": "No title",
    "content": "With the current global deluge of data and information, there has never been a more important to visualize your data in a clear and simple manner.\n  \n    Python is a terrific tool to help us do this.\n  \n    The key to this lies in choosing the the right data visualization techniques to tell the most interesting and relevant story.\n  \n    Three useful visuals are:\n  \n\n\nsmall multiples\n\n\n\nheat maps\n\n\n\nstacked area charts\n\n\n\n    In this tutorial, using pandas, seaborn, and matplotlib.pyplot, we create the Python code for each data visual\n  \n    Link to free Tutorial"
},
{
    "title": "No title",
    "content": "In rpc/serialization systems, we often need to send namespace/path/filename/fieldName/packageName/moduleName/className/enumValue string between processes.Those strings are mostly ascii strings. In order to transfer between processes, we encode such strings using utf-8 encodings. Such encoding will take one byte for every char, which is not space efficient actually.If we take a deeper look, we will found that most chars are lowercase chars, ., $ and _, which can be expressed in a much smaller range 0~32. But one byte can represent range 0~255, the significant bits are wasted, and this cost is not ignorable. In a dynamic serialization framework, such meta will take considerable cost compared to actual data.So we proposed a new string encoding which we called meta string encoding in Fury. It will encode most chars using 5 bits instead of 8 bits in utf-8 encoding, which can bring 37.5% space cost savings compared to utf-8 encoding.For string can't be represented by 5 bits, we also proposed encoding using 6 bits which can bring 25% space cost savings\n  \n    More details can be found in: https://fury.apache.org/blog/fury_meta_string_37_5_percent_space_efficient_encoding_than_utf8 and https://github.com/apache/incubator-fury/blob/main/docs/specification/xlang_serialization_spec.md#meta-string"
},
{
    "title": "No title",
    "content": "I've made my first bit of useful software and I wanted to share it here. I'd love some feedback (and it would be amazing to hear if someone has used it!)\n  \nWhat My Project Does:\n\n    Using the third party requests package, the script interacts with the Spotify web API to request all albums from the given Artist, then all the tracks from all of those albums. It then goes through the list to remove any duplicates and also tries to remove any unwanted versions (only done by examining the name of the track, since Spotify does not attribute a version type to its tracks). Once that's done a playlist is then created on your Spotify account with the name of the Artist and all the tracks are posted there in chronological (essentially per album) order.\n  \nTarget Audience:\n\n    Anyone who struggles like me when they find a new Artist and they want to listen to every conceivable song from them!\n  \n    Link to GitHub: https://github.com/RJW20/spotify-artist-to-playlist"
},
{
    "title": "No title",
    "content": "I'm doing most of my work behind a government firewall, and I'm having trouble connecting to certain sites.   I can do the usual \"pip\" installs just fine, but I'm talking about packages that need to download data to do their job.  An example is the NLTK (Natural Language Toolkit) package, which downloads dictionaries, lookup tables for sentiment analysis, and so on.  I know what sites to open up for that particular problem (pastebin.com and nltk.org), but I wonder if anybody's made a list of such sites for different packages.\n  \n    I can ask for the two sites I know about to be opened up, but I'd like to have a more comprehensive list so I don't have to go through the red tape multiple times."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I'd like to call attention to pip 24.1 beta asit is unusual for the pip team to release betas:\n  \n\n\nhttps://pip.pypa.io/en/latest/news/#b1-2024-05-06\n\n\n\nhttps://pypi.org/project/pip/24.1b1/\n\n\n\n    You can install with:\n  python -m pip install pip==24.1b1\n    In particular they have upgraded their vendored version of packaging from 21.3 to 24.0, this was a big effort and fixed many bugs, included significant performance improvements, and will allow pip to support free threaded packages. However, it also means legacy versions and specifiers are no longer compatible with pip.\n  \n    Because this was such a big land the pip maintainers have released a beta in the hopes people will test their workflows, and if something fails in an expected way report their steps as best as possible back to pip: https://github.com/pypa/pip/issues\n\n    I've been testing, and contributing a little bit, to the improved performance in this release, it is most noticeable on large dependency trees or long backtracking. For example, a dry run of \"apache-airflow[all]\" using cached packages on my machine goes from ~418 seconds to ~185 seconds."
},
{
    "title": "No title",
    "content": "Do you understand how asyncio works behind the scenes? Read this article and see how you can use Python generators to create your own version of asyncio, and then use the __await__ dunder method to use the async/await keywords to come full circle!\n  \nhttps://jacobpadilla.com/articles/recreating-asyncio"
},
{
    "title": "No title",
    "content": "Just interested in how people approach this, typically I just use VSCode or QtCreator to build simple projects. However I now want to automate some of the build process such as running uic, and rcc.\n  \n    I've tried to use CMake but can't seem to get it to work without a lot of custom scripting (for example the AUTOUIC etc functions need c++ projects), can't see any info on running uic in QtCreator (which would be ideal but python support is really just an after thought).\n  \n    I could write some Makefiles but this is a little ad-hoc and also confuses the IDE's (and at the end of the day I want a simple process for my students to use and I already teach cmake for C++ dev).\n  \n    So I guess my questions are what workflows do people use, can you recommend any tools to help, or do you just have a per project script to run uic and rcc?\n  \n    (I may cross post this in both qt and python subreddits as I'm not sure where it fits best)"
},
{
    "title": "No title",
    "content": "Excited to finally showcase this!\n  \n    It's still pretty rough around the edges, but I'm finally happy enough with the feature set and curious to see what the community thinks about a framework like this.\n  \n    Code: github.com/crpier/relax-py\n\n    Documentation: crpier.github.io/relax-py\n\nWhat My Project Does\n\nrelax-py is a Python framework for building full-stack applications with htmx\n\n    It provides tools for writing HTML in a manner similar to simple_html (which also inspired the decision to use standard Python to write HTML, rather than use Jinja2 or to make something like templ work in Python)\n  \n    It has:\n  \n\n\n    Hot Module Replacement (meaning, when you update the code that generates HTML templates, the browser also updates instantly) - see the video in the documentation for a quick demo of this\n  \n\n\n    URL resolution with type hinting - you can get the URL of an endpoint to use in your templates by using the function that handles that URL, and get help from static typing (for example, for putting path parameters in the URL)\n  \n\n\n    Helpers for dependency injection\n  \n\n\n    In essence, this framework is just a bunch of decorators and functions over starlette, meaning everything that starlette has can be used alongside the framework.\n  \nTarget Audience\n\n    Developers interested in building web applications with htmx that like new shiny things and static typing support\n  \nComparison\n\n    As far as I know, the only other backend framework that has Hot Module Replacement is turbo in Ruby on Rails, but there might be something I missed.\n  \n    As for other points of comparison with other frameworks:\n  \n\n\n    Django\n  \n\n\nrelax is less opinionated about what's done in the backend (.e.g there is preference to what ORM is used)\n  \n\n\n    using standard Python code to generate HTML has nicer static typing\n  \n\n\n    the URL resolution is more complex and provides errors in the IDE by way of static typing\n  \n\n\n    the component decorator provides nicer ways to reuse template functions and helpers for interoperability with JavaScript\n  \n\n\n\n\ntempl in Go\n  \n\n\n    templ allows writing actual HTML in go files, but requires an additional compilation step\n  \n\n\n    plugins for whatever IDE/code editor is used are needed parsing templ files\n  \n\n\n\n\n    FastAPI (with something to generate HTML like simple_html or Jinja2)\n  \n\n\n    since FastAPI is built for RESTful APIs, it lacks niceties like URL resolution, or a mechanism to manage the sprawling mess of interconnected HTML components that apps tend to develop\n  \n\n\n    dependency injection in FastAPI is \"encouraged\" to happen in the path functions, but in relax it's meant to happen at any level of the app (either in path functions, or in service-level functions, or in util functions)\n  \n\n\n\n\n    simple_html (with a backend like Flask or FastAPI): the main differences between simple_html and the relax.html module are that\n  \n\n\n    CSS classes are provided as a list of strings - this makes it easier to reuse them in different components, and will make it easier to implement other helpers in the future, like a Python version of tailwind-merge, or a formatter that sorts tailwind classes\n  \n\n\n    htmx-related attributes are included in the elements\n  \n\n\n    inserting children to an HTML element is done after instantiating the element, making it easier to reuse components\n  \n\n\n\n\n    Here's the code again: github.com/crpier/relax-py\n\n    There's more details in the documentation: crpier.github.io/relax-py\n\n    While this framework is definitely not production ready, in the \"Other\" page of the documentation there's an example app built with this framework, which shows how it can be used in conjuction with some real-life scenarios (production environment for tailwind with plugins, working in a bunch of interactivity with JavaScript, in either separate js files and inline scripts, Dockerfiles and deployments, authentication and authorization, configuration etc.)\n  \n    Please let me know what you think (are there better alternatives, is writing HTML in standard Python a deal-breaker, is investing in making something templ in Python worth it?)\n  \n    Hope you're intrigued by this!"
},
{
    "title": "No title",
    "content": "python-oracledb 2.2, the Oracle Database driver, has been released with support for Oracle Database 23ai features such as the VECTOR and BOOLEAN data types, Implicit Connection Pooling, and improved connection performance.  See the release announcement."
},
{
    "title": "No title",
    "content": "See https://github.com/BenHanson/pylexertl\n\n    I will see about registering as an official library when I am happy I have completed all the bindings. I added all the missing functions for the rules objects today, so things are in reasonable shape already.\n  \n    My python experience has been limited up until now, but it is big for my new role.\n  \n    I have a runtime parser generator https://github.com/BenHanson/parsertl17 which I also plan to add bindings for.\n  \n    I hope this is of interest to somebody!\n  \nWhat My Project Does\n\n    Allows you to build lexical analysers at runtime and use them to lex text (in this case utf-8)\n  \nTarget Audience\n\n    The C++ library has been used in production for over 10 years.\n  \nComparison\n\n    I'm not aware of any competing library."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    PgQueuer is a Python library designed to manage job queues using  PostgreSQL features. It leverages PostgreSQL's native LISTEN/NOTIFY, along with advanced locking mechanisms, to handle job queues efficiently. This allows for real-time job processing, concurrency, and reliable task execution without the need for a separate queuing system.\n  \nTarget Audience\n\n    PgQueuer is ideal for developers and teams who already use PostgreSQL in their projects and are looking for a simple, integrated way to handle background tasks and job queues. It's designed for production use, offering a dependable solution that scales seamlessly with existing PostgreSQL databases.\n  \nComparison\n\n    Unlike many other job queue solutions that require additional services or complex setups (such as Redis or RabbitMQ), PgQueuer operates directly within PostgreSQL. This removes the overhead of integrating and maintaining separate systems for job management.\n  \nHow PgQueuer stands out\n\n\n\nIntegration Simplicity: Integrates directly with existing PostgreSQL setups without additional infrastructure.\n  \n\n\nEfficiency: Uses PostgreSQL’s FOR UPDATE SKIP LOCKED for high concurrency, allowing multiple workers to process tasks simultaneously without conflict.\n  \n\n\nReal-time Updates: Utilizes PostgreSQL's LISTEN/NOTIFY for immediate job processing updates, reducing latency compared to polling-based systems.\n  \n\n\nRequest for Feedback on Useful Features\n\n    Im always looking to improve PgQueuer and make it more useful for our users. If you have any features you'd like to see, or if there's something you think could be improved, please let me know! Your feedback is invaluable! Share your thoughts, suggestions, or feature requests either here in the comments or via GitHub."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I'm Paul, one of the creators of Rio. Over the years I've tried many different established python GUI frameworks, but none of them really satisfied me. So I teamed up with a few like minded developers and spent the last few months to create our own framework. Rio is the result of this effort.\n  What My Project Does\n    Rio is a brand new GUI framework that lets you create modern web apps in just a few lines of Python. Our goal is to simplify web and app development, so you can focus on the things you care about, instead of wasting countless hours on frustrating user interface details.\n  \n    We do this by following the core principles of Python that we all know and love. Python is supposed to be simple and compact - and so is Rio. There is no need to learn any additional languages such as HTML, CSS or JavaScript, because all of the UI, Logic, Components and even layouting is done entirely in Python. There’s not even a distinction between front-end and back-end. Rio handles all of the communication transparently for you.\n  Key Features\n\n\nFull-Stack Web Development: Rio handles front-end and backend for you. In fact, you won't even notice they exist. Create your UI, and Rio will take care of the rest.\n  \n\n\nPython Native: Rio apps are written in 100% Python, meaning you don't need to write a single line of CSS or JavaScript.\n  \n\n\nModern Python: We embrace modern Python features, such as type annotations and asynchrony. This keeps your code clean and maintainable, and helps your code editor help you out with code completions and type checking.\n  \n\n\nPython Debugger Compatible: Since Rio runs on Python, you can connect directly to the running process with a debugger. This makes it easy to identify and fix bugs in your code.\n  \n\n\nDeclarative Interface: Rio apps are built using reusable components, inspired by React, Flutter & Vue. They're declaratively combined to create modular and maintainable UIs.\n  \n\n\nBatteries included: Over 50 builtin components based on Google's Material Design\n  \n\n\nDemo Video\nTarget Audience\n    Whether you need to build dashboards, CRUD apps, or just want to make a personal website, Rio makes it possible without any web development knowledge. Because Rio was developed from the ground up for Python programmers, it was designed to be concise and readable, just like Python itself.\n  Comparison\n    Rio doesn't just serve HTML templates like you might be used to from frameworks like Flask. In Rio you define components as simple dataclasses with a React/Flutter style build method. Rio continuously watches your attributes for changes and updates the UI as necessary.\n  class MyComponent(rio.Component):\n    clicks: int = 0\n\n    def _on_press(self) -> None:\n        self.clicks += 1\n\n    def build(self) -> rio.Component:\n        return rio.Column(\n            rio.Button('Click me', on_press=self._on_press),\n            rio.Text(f'You clicked the button {self.clicks} time(s)'),\n        )\n\napp = rio.App(build=MyComponent)\napp.run_in_browser()\n    Notice how there is no need for any explicit HTTP requests. In fact there isn't even a distinction between frontend and backend. Rio handles all communication transparently for you. Unlike ancient libraries like Tkinter, Rio ships with over 50 builtin components in Google's Material Design. Moreover the same exact codebase can be used for both local apps and websites.\n  We Want Your Feedback!\n    The first alpha version of Rio is available on PyPi now:\n  pip install rio-ui\nrio new my-project --template tic-tac-toe\ncd my-project\nrio run\n\n\nDiscord\n\n\n\nGitHub\n\n\n\nTutorial\n\n\n\nWebsite\n\n\n\n    Let us know what you think - any feedback, ideas, or even a helping hand are hugely welcome! Just hop on our Discord server and say hello!"
},
{
    "title": "No title",
    "content": "Python 3.13.0 beta 1 was released today.\n  \n    The feature I'm most excited about is the new Python REPL.\n  \nHere's a summary of my favorite features in the new REPL along with animated gifs.\n  \n    The TLDR:\n  \n\n\n    Support for block-leveling history and block-level editing\n  \n\n\n    Pasting code (even with blank lines within it) works as expected now\n  \n\n\n    Typing exit will exit (no more Use exit() or Ctrl-D (i.e. EOF) to exit message)"
},
{
    "title": "No title",
    "content": "I was doing some light reading and stumbled across Steve Gribbles Power vs Speed Calculator and thought I'd give it a go at rebuilding it based on his Physics model using Python. Then I wrote an article about. Thought I'd share it with you all: Calculating Virtual Cycling Power (jasonlei.com)"
},
{
    "title": "No title",
    "content": "InterProcessPyObjects Python package\ngithub.com/FI-Mihej/InterProcessPyObjects If you like the project, consider giving it a star on GitHub to show your support and help further development. :)\n  \npypi.org/project/InterProcessPyObjects\nWhat My Project Does\n\n    InterProcessPyObjects is a part of the Cengal library. If you have any questions or would like to participate in discussions, feel free to join the Cengal Discord. Your support and involvement are greatly appreciated as Cengal evolves.\n  \n\n    This high-performance package delivers blazing-fast inter-process communication through shared memory, enabling Python objects to be shared across processes with exceptional efficiency. By minimizing the need for frequent serialization-deserialization, it enhances overall speed and responsiveness. The package offers a comprehensive suite of functionalities designed to support a diverse array of Python types and facilitate asynchronous IPC, optimizing performance for demanding applications.\n  Target Audience\n    This project is designed for production environments, offering a stable API suitable for developers looking to implement fast inter-process communication. Whether you're building complex systems or require robust data sharing and modification across processes, InterProcessPyObjects is ready to meet your needs.\n  Comparison\n    Comparison with multiprocessing.shared_memory\n\n    While both InterProcessPyObjects and multiprocessing.shared_memory facilitate inter-process communication, there are several key differences to note. Unlike multiprocessing.shared_memory, InterProcessPyObjects offers the following enhancements:\n  \n\n\n    High-Performance Mutable Objects: Both connected processes can modify shared objects at runtime, and these changes are immediately reflected on the other side. This feature not only increases flexibility but also delivers exceptional performance, with the capability to handle up to several million changes per second.\n  \n\n\n    Synchronization Features: Ensures that operations are thread-safe and data integrity is maintained across processes.\n  \n\n\n    Message Queue: Integrates a system for queuing messages, making communication between processes more structured and reliable.\n  \n\n\n    Extended Type Support: Supports a broad range of data types, including custom classes, which goes beyond the basic types typically handled by multiprocessing.shared_memory.\n  \n\n\n    These features make InterProcessPyObjects a more robust option for developers requiring advanced inter-process communication capabilities.\n  API State\n    Stable. Guaranteed not to have breaking changes in the future. (see github.com/FI-Mihej/InterProcessPyObjects?tab=readme-ov-file#api-state for details)\n  Key Features\n\n\n    Shared Memory Communication:\n  \n\n\n    Enables sharing of Python objects directly between processes using shared memory.\n  \n\n\n    Utilizes a linked list of global messages to inform connected processes about new shared objects.\n  \n\n\n\n\n    Lock-Free Synchronization:\n  \n\n\n    Uses memory barriers for efficient communication, avoiding slow syscalls.\n  \n\n\n    Ensures each process can access and modify shared memory without contention.\n  \n\n\n\n\n    Supported Python Types:\n  \n\n\n    Handles various Python data structures including:\n  \n\n\n    Basic types: None, bool, 64-bit int, large int (arbitrary precision integers), float, complex, bytes, bytearray, str.\n  \n\n\n    Standard types: Decimal, slice, datetime, timedelta, timezone, date, time\n\n\n\n    Containers: tuple, list, classes inherited from: AbstractSet (frozenset), MutableSet (set), Mapping and MutableMapping (dict).\n  \n\n\n    Pickable classes instances: custom classes including dataclass\n\n\n\n\n\n    Allows mutable containers (lists, sets, mappings) to save basic types (None, bool, 64 bit int, float) internally, optimizing memory use and speed.\n  \n\n\n\n\n    NumPy and Torch Support:\n  \n\n\n    Supports numpy arrays by creating shared bytes objects coupled with independent arrays.\n  \n\n\n    Supports torch tensors by coupling them with shared numpy arrays.\n  \n\n\n\n\n    Custom Class Support:\n  \n\n\n    Projects pickable custom classes instances (including dataclasses) onto shared dictionaries in shared memory.\n  \n\n\n    Modifies the class instance to override attribute access methods, managing data fields within the shared dictionary.\n  \n\n\n    supports classes with or without __dict__ attr\n  \n\n\n    supports classes with or without __slots__ attr\n  \n\n\n\n\n    Asyncio Compatibility:\n  \n\n\n    Provides a wrapper module for async-await functionality, integrating seamlessly with asyncio.\n  \n\n\n    Ensures asynchronous operations work smoothly with the package's lock-free approach.\n  \n\n\n\nMain principles\n\n\n    only one process has access to the shared memory at the same time\n  \n\n\n    working cycle:\n  \n\n\n    work on your tasks\n  \n\n\n    acquire access to shared memory\n  \n\n\n    work with shared memory as fast as possible (read and/or update data structures in shared memory)\n  \n\n\n    release access to shared memory\n  \n\n\n    continue your work on other tasks\n  \n\n\n\n\n    do not forget to manually destroy your shared objects when they are not needed already\n  \n\n\n    feel free to not destroy your shared object if you need it for a whole run and/or do not care about the shared memory waste\n  \n\n\n    data will not be preserved between Creator's sessions. Shared memory will be wiped just before Creator finished its work with a shared memory instance (Consumer's session will be finished already at this point)\n  \n\nExamples\n\n\n    An async examples (with asyncio):\n  \n\n\nsender.py\n\n\n\nreceiver.py\n\n\n\nshared_objects__types.py\n\n\n\n\nReceiver.py performance measurements\n\n\n    CPU: i5-3570@3.40GHz (Ivy Bridge)\n  \n\n\n    RAM: 32 GBytes, DDR3, dual channel, 655 MHz\n  \n\n\n    OS: Ubuntu 20.04.6 LTS under WSL2. Windows 10\n  \n\nasync with ashared_memory_context_manager.if_has_messages() as shared_memory:\n    # Taking a message with an object from the queue.\n    sso: SomeSharedObject = shared_memory.value.take_message()  # 5_833 iterations/seconds\n\n    # We create local variables once in order to access them many times in the future, ensuring high performance.\n    # Applying a principle that is widely recommended for improving Python code.\n    company_metrics: List = sso.company_info.company_metrics  # 12_479 iterations/seconds\n    some_employee: Employee = sso.company_info.some_employee  # 10_568 iterations/seconds\n    data_dict: Dict = sso.data_dict  # 16_362 iterations/seconds\n    numpy_ndarray: np.ndarray = data_dict['key3']  # 26_223 iterations/seconds\n\n# Optimal work with shared data (through local variables):\nasync with ashared_memory_context_manager as shared_memory:\n    # List\n    k = company_metrics[CompanyMetrics.avg_salary]  # 1_535_267 iterations/seconds\n    k = company_metrics[CompanyMetrics.employees]  # 1_498_278 iterations/seconds\n    k = company_metrics[CompanyMetrics.in_a_good_state]  # 1_154_454 iterations/seconds\n    k = company_metrics[CompanyMetrics.websites]  # 380_258 iterations/seconds\n    company_metrics[CompanyMetrics.annual_income] = 2_000_000.0  # 1_380_983 iterations/seconds\n    company_metrics[CompanyMetrics.employees] = 20  # 1_352_799 iterations/seconds\n    company_metrics[CompanyMetrics.avg_salary] = 5_000.0  # 1_300_966 iterations/seconds\n    company_metrics[CompanyMetrics.in_a_good_state] = None  # 1_224_573 iterations/seconds\n    company_metrics[CompanyMetrics.in_a_good_state] = False  # 1_213_175 iterations/seconds\n    company_metrics[CompanyMetrics.avg_salary] += 1.1  # 299_415 iterations/seconds\n    company_metrics[CompanyMetrics.employees] += 1  # 247_476 iterations/seconds\n    company_metrics[CompanyMetrics.emails] = tuple()  # 55_335 iterations/seconds (memory allocation performance is planned to be improved)\n    company_metrics[CompanyMetrics.emails] = ('sails@company.com',)  # 30_314 iterations/seconds (memory allocation performance is planned to be improved)\n    company_metrics[CompanyMetrics.emails] = ('sails@company.com', 'support@company.com')  # 20_860 iterations/seconds (memory allocation performance is planned to be improved)\n    company_metrics[CompanyMetrics.websites] = ['http://company.com', 'http://company.org']  # 10_465 iterations/seconds (memory allocation performance is planned to be improved)\n    \n    # Method call on a shared object that changes a property through the method\n    some_employee.increase_years_of_employment()  # 80548 iterations/seconds\n\n    # Object properties\n    k = sso.int_value  # 850_098 iterations/seconds\n    k = sso.str_value  # 228_966 iterations/seconds\n    sso.int_value = 200  # 207_480 iterations/seconds\n    sso.int_value += 1  # 152_263 iterations/seconds\n    sso.str_value = 'Hello. '  # 52_390 iterations/seconds (memory allocation performance is planned to be improved)\n    sso.str_value += '!'  # 35_823 iterations/seconds (memory allocation performance is planned to be improved)\n\n    # Numpy.ndarray\n    numpy_ndarray += 10  # 403_646 iterations/seconds\n    numpy_ndarray -= 15  # 402_107 iterations/seconds\n\n    # Dict\n    k = data_dict['key1']  # 87_558 iterations/seconds\n    k = data_dict[('key', 2)]  # 49_338 iterations/seconds\n    data_dict['key1'] = 200  # 86_744 iterations/seconds\n    data_dict['key1'] += 3  # 41_409 iterations/seconds\n    data_dict['key1'] *= 1  # 40_927 iterations/seconds\n    data_dict[('key', 2)] = 'value2'  # 31_460 iterations/seconds (memory allocation performance is planned to be improved)\n    data_dict[('key', 2)] = data_dict[('key', 2)] + 'd'  # 18_972 iterations/seconds (memory allocation performance is planned to be improved)\n    data_dict[('key', 2)] = 'value2'  # 10_941 iterations/seconds (memory allocation performance is planned to be improved)\n    data_dict[('key', 2)] += 'd'  # 16_568 iterations/seconds (memory allocation performance is planned to be improved)\n\n# An example of non-optimal work with shared data (without using a local variables):\nasync with ashared_memory_context_manager as shared_memory:\n    # An example of a non-optimal method call (without using a local variable) that changes a property through the method\n    sso.company_info.some_employee.increase_years_of_employment()  # 9_418 iterations/seconds\n\n    # An example of non-optimal work with object properties (without using local variables)\n    k = sso.company_info.income  # 20_445 iterations/seconds\n    sso.company_info.income = 3_000_000.0  # 13_899 iterations/seconds\n    sso.company_info.income *= 1.1  # 17_272 iterations/seconds \n    sso.company_info.income += 500_000.0  # 18_376 iterations/seconds\n    \n    # Example of non-optimal usage of numpy.ndarray without a proper local variable\n    data_dict['key3'] += 10  # 6_319 iterations/seconds\n\n# Notify the sender about the completion of work on the shared object\nasync with ashared_memory_context_manager as shared_memory:\n    sso.some_processing_stage_control = True  # 298_968 iterations/secondsThroughput Benchmarks\n\n\n    CPU: i5-3570@3.40GHz (Ivy Bridge)\n  \n\n\n    RAM: 32 GBytes, DDR3, dual channel, 655 MHz\n  \n\n\n    OS: Ubuntu 20.04.6 LTS under WSL2. Windows 10\n  \n\nRefference results (sysbench)sysbench memory --memory-oper=write run5499.28 MiB/secBenchmarks results table GiB/s\n\n\n\n            Approach\n          \n            sync/async\n          \n            Throughput GiB/s\n          \n\n\n\n\n\n\n              InterProcessPyObjects (sync)\n            \n              sync\n            \n              3.770\n            \n\n\n              InterProcessPyObjects + uvloop\n            \n              async\n            \n              3.222\n            \n\n\n              InterProcessPyObjects + asyncio\n            \n              async\n            \n              3.079\n            \n\n\n              multiprocessing.shared_memory *\n            \n              sync\n            \n              2.685\n            \n\n\n              uvloop.UnixDomainSockets\n            \n              async\n            \n              0.966\n            \n\n\n              asyncio + cengal.Streams\n            \n              async\n            \n              0.942\n            \n\n\n              uvloop.Streams\n            \n              async\n            \n              0.922\n            \n\n\n              asyncio.Streams\n            \n              async\n            \n              0.784\n            \n\n\n              asyncio.UnixDomainSockets\n            \n              async\n            \n              0.708\n            \n\n\n              multiprocessing.Queue\n            \n              sync\n            \n              0.669\n            \n\n\n              multiprocessing.Pipe\n            \n              sync\n            \n              0.469\n            \n\n\n\n* multiprocessing.shared_memory.py - simple implementation. This is a simple implementation because it uses a similar approach to the one used in uvloop.*, asyncio.*, multiprocessing.Queue, and multiprocessing.Pipe benchmarking scripts. Similar implementations are expected to be used by the majority of projects.\n  Todo\n\n\n    Connect more than two processes\n  \n\n\n    Use third-party fast hashing implementations instead of or in addition to built in hash() call\n  \n\n\n    Continuous performance improvements\n  \n\nConclusion\n    This Python package provides a robust solution for inter-process communication, supporting a variety of Python data structures, types, and third-party libraries. Its lock-free synchronization and asyncio compatibility make it an ideal choice for high-performance, concurrent execution.\n  Based on Cengal\n    This is a stand-alone package for a specific Cengal module. Package is designed to offer users the ability to install specific Cengal functionality without the burden of the library's full set of dependencies.\n  \n    The core of this approach lies in our 'cengal-light' package, which houses both Python and compiled Cengal modules. The 'cengal' package itself serves as a lightweight shell, devoid of its own modules, but dependent on 'cengal-light[full]' for a complete Cengal library installation with all required dependencies.\n  \n    An equivalent import:\n  from cengal.hardware.memory.shared_memory import *\nfrom cengal.parallel_execution.asyncio.ashared_memory_manager import *\n    Cengal library can be installed by:\n  pip install cengal\nhttps://github.com/FI-Mihej/Cengal\n\nhttps://pypi.org/project/cengal/\nProjects using Cengal\n\n\nCengalPolyBuild - A Comprehensive and Hackable Build System for Multilingual Python Packages: Cython (including automatic conversion from Python to Cython), C/C++, Objective-C, Go, and Nim, with ongoing expansions to include additional languages. (Planned to be released soon)\n  \n\n\ncengal_app_dir_path_finder - A Python module offering a unified API for easy retrieval of OS-specific application directories, enhancing data management across Windows, Linux, and macOS\n  \n\n\ncengal_cpu_info - Extended, cached CPU info with consistent output format.\n  \n\n\ncengal_memory_barriers - Fast cross-platform memory barriers for Python.\n  \n\n\nflet_async - wrapper which makes Flet async and brings booth Cengal.coroutines and asyncio to Flet (Flutter based UI)\n  \n\n\njustpy_containers - wrapper around JustPy in order to bring more security and more production-needed features to JustPy (VueJS based UI)\n  \n\n\nBensbach - decompiler from Unreal Engine 3 bytecode to a Lisp-like script and compiler back to Unreal Engine 3 bytecode. Made for a game modding purposes\n  \n\n\nRealistic-Damage-Model-mod-for-Long-War - Mod for both the original XCOM:EW and the mod Long War. Was made with a Bensbach, which was made with Cengal\n  \n\n\nSmartCATaloguer.com - TagDB based catalog of images (tags), music albums (genre tags) and apps (categories)\n  \n\nLicense\n    Licensed under the Apache License, Version 2.0."
},
{
    "title": "No title",
    "content": "Source code\n\nWhat My Project Does:\n\n    It acts as a wrapper for the AzuraCast API, providing custom functions and classes for more straightforward use of the API in python projects.\n  \nTarget Audience:\n\n    Python users who are interested in programmatically interacting with online radios hosted on AzuraCast.\n  \nComparison:\n\n    The idea of API Wrappers is not new. However, I noticed that the only existing wrapper for this API is written in PHP, which I am not experienced with. I created this project so I, and other python programmers by extension, could have an easier time working with the API.\n  \n    This is my first \"major\" programming project, so thoughts and feedback are welcome and greatly appreciated.\n  \n    PS: Shoutout to PRAW for \"inspiring\" basically everything about the project's structure and functionality."
},
{
    "title": "No title",
    "content": "PYPI\n\n    (From the README, Released Last Year, Edited by Grammarly)\n  \nGithub\n\npip install diskcache\n\n    The cloud-based computing of 2024 puts a premium on memory. Gigabytes of space are left on disks as processes vie for memory. Memcached (and sometimes Redis) is used as a cache among these processes. Wouldn’t it be nice to leverage empty disk space for caching?\n  \n    Django is Python’s most popular web framework and has several caching backends. Unfortunately, the file-based cache in Django is essentially broken. The culling method is random and large caches repeatedly scan a cache directory which slows linearly with growth. Can you allow it to take sixty milliseconds to store a key in a cache with a thousand items?\n  \n    Is it that fast?\n  In [1]: import pylibmc\nIn [2]: client = pylibmc.Client(['127.0.0.1'], binary=True)\nIn [3]: client[b'key'] = b'value'\nIn [4]: %timeit client[b'key']\n\n10000 loops, best of 3: 25.4 µs per loop\n\nIn [5]: import diskcache as dc\nIn [6]: cache = dc.Cache('tmp')\nIn [7]: cache[b'key'] = b'value'\nIn [8]: %timeit cache[b'key']\n\n100000 loops, best of 3: 11.8 µs per loop"
},
{
    "title": "No title",
    "content": "I am using quart framework (https://quart.palletsprojects.com) for a number of microservices in a SaaS application. However, I hardly hear anything about this framework on any social media platform which seems to be dominated by FastAPI. Also I'm unable to find which all projects/companies are using this framework. All this is leading to anxiety around the future of this project.\n  \n    Are there any well known projects / companies which are using this framework for microservices?"
},
{
    "title": "No title",
    "content": "I made this visualisation with this code.\n  \n    I have three questions:\n  \n\n\n    Is Plotly supposed to be this cumbersome to tweak? Would other libraries require the same amount of code to add the details I did?\n  \n\n\n    Can my code be reduced in size? Maybe it's me who is complicating things with Plotly and there are easier ways to do what I am doing.\n  \n\n\n    Any R enthusiast who can tell me how much shorter this code would look like with ggplot2? I asked ChatGPT but the result was garbage.\n  \n\n\n    Bonus question: This took me an entire morning. Is it normal to be \"that slow\" to plot a simple figure?"
},
{
    "title": "No title",
    "content": "Hi all, I initially started this adventure by trying to automate bug fixes with the help of LLMs. However, I received feedback saying the fixes aren't always correct, leading to the question: why bother reviewing PRs that might add more issues? (It's really hard for LLMs to say \"I don't know\").\n\n    So, I decided to focus on reliability perfecting unit tests.\n  \n    The source code is available at: https://github.com/CaptureFlow/captureflow-py\n\nWhat My Project Does:\n\n    It incorporates a tracer client-side Python library and a backend that accumulates such traces and is capable of proposing code improvements and generating tests for your repository. It traverses the execution graph, extracts relevant parts, enriches them with implementation data from the GitHub API, and then generates tests with the help of GPT4.\n  \nTarget Audience:\n\n    Python users interested in discovering what LLMs can achieve when given detailed runtime information from your app. Generally, this approach somewhat reverses the concept of TDD, but in my day job I deal with many legacy apps that have poor test coverage, and having it really helps. I suspect I’m not alone in finding value in this.\n  \nComparison:\n\n    I think idea of using LLMs to generate tests is not new, but generating them actually based on application's performance is inspired by Jane Street's article on how they automated test boilerplate creation and recent Facebook's research paper.\n  \nDisclaimer: More work needs to be done to make it work for any Python app and not just a subset of FastAPI servers. I'm curious if you folks would find it useful.\n  \nExample: you can check this PR for reference. feedback / stars / contributions are welcome."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hello all,\n  \n    This is probably my first post here. I usuallly lurk around here and Django subreddits.\n  \n    I've been brewing up an idea and I need your input before I take the plunge!\n  \n    Picture this: a website like FreeCodeCamp but for python and related technologies, a learning oasis where anyone can kickstart their journey from Python newbie to job-ready pro, and it's all free!\n  \n    But here's the thing, I want this to be our platform, crafted with your needs and dreams in mind. So, before I start, I need to know: would this be something that gets you excited?\n  \n    Imagine quizzes helping you find your starting point, interactive challenges that keep you in the zone, and a supportive community to cheer you on every step of the way. Plus, videos, written tutorials, and a progress tracker to keep you motivated!\n  \n    What would make you go, \"Wow, I need this in my life!\"?\n  \n    What features would you love to see?\n  \n    Any suggestions or wild ideas ?\n  \n    My aim is to give back to the community by assisting new learners in navigating common pitfalls when they start their Python journey"
},
{
    "title": "No title",
    "content": "With the current global deluge of data and information, there has never been a more important to visualize your data in a clear and simple manner.\n  \n    Python is a terrific tool to help us do this.\n  \n    The key to this lies in choosing the the right data visualization techniques to tell the most interesting and relevant story.\n  \n    Three useful visuals are:\n  \n\n\nsmall multiples\n\n\n\nheat maps\n\n\n\nstacked area charts\n\n\n\n    In this tutorial, using pandas, seaborn, and matplotlib.pyplot, we create the Python code for each data visual\n  \n    Link to free Tutorial"
},
{
    "title": "No title",
    "content": "In rpc/serialization systems, we often need to send namespace/path/filename/fieldName/packageName/moduleName/className/enumValue string between processes.Those strings are mostly ascii strings. In order to transfer between processes, we encode such strings using utf-8 encodings. Such encoding will take one byte for every char, which is not space efficient actually.If we take a deeper look, we will found that most chars are lowercase chars, ., $ and _, which can be expressed in a much smaller range 0~32. But one byte can represent range 0~255, the significant bits are wasted, and this cost is not ignorable. In a dynamic serialization framework, such meta will take considerable cost compared to actual data.So we proposed a new string encoding which we called meta string encoding in Fury. It will encode most chars using 5 bits instead of 8 bits in utf-8 encoding, which can bring 37.5% space cost savings compared to utf-8 encoding.For string can't be represented by 5 bits, we also proposed encoding using 6 bits which can bring 25% space cost savings\n  \n    More details can be found in: https://fury.apache.org/blog/fury_meta_string_37_5_percent_space_efficient_encoding_than_utf8 and https://github.com/apache/incubator-fury/blob/main/docs/specification/xlang_serialization_spec.md#meta-string"
},
{
    "title": "No title",
    "content": "I've made my first bit of useful software and I wanted to share it here. I'd love some feedback (and it would be amazing to hear if someone has used it!)\n  \nWhat My Project Does:\n\n    Using the third party requests package, the script interacts with the Spotify web API to request all albums from the given Artist, then all the tracks from all of those albums. It then goes through the list to remove any duplicates and also tries to remove any unwanted versions (only done by examining the name of the track, since Spotify does not attribute a version type to its tracks). Once that's done a playlist is then created on your Spotify account with the name of the Artist and all the tracks are posted there in chronological (essentially per album) order.\n  \nTarget Audience:\n\n    Anyone who struggles like me when they find a new Artist and they want to listen to every conceivable song from them!\n  \n    Link to GitHub: https://github.com/RJW20/spotify-artist-to-playlist"
},
{
    "title": "No title",
    "content": "I'm doing most of my work behind a government firewall, and I'm having trouble connecting to certain sites.   I can do the usual \"pip\" installs just fine, but I'm talking about packages that need to download data to do their job.  An example is the NLTK (Natural Language Toolkit) package, which downloads dictionaries, lookup tables for sentiment analysis, and so on.  I know what sites to open up for that particular problem (pastebin.com and nltk.org), but I wonder if anybody's made a list of such sites for different packages.\n  \n    I can ask for the two sites I know about to be opened up, but I'd like to have a more comprehensive list so I don't have to go through the red tape multiple times."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I'd like to call attention to pip 24.1 beta asit is unusual for the pip team to release betas:\n  \n\n\nhttps://pip.pypa.io/en/latest/news/#b1-2024-05-06\n\n\n\nhttps://pypi.org/project/pip/24.1b1/\n\n\n\n    You can install with:\n  python -m pip install pip==24.1b1\n    In particular they have upgraded their vendored version of packaging from 21.3 to 24.0, this was a big effort and fixed many bugs, included significant performance improvements, and will allow pip to support free threaded packages. However, it also means legacy versions and specifiers are no longer compatible with pip.\n  \n    Because this was such a big land the pip maintainers have released a beta in the hopes people will test their workflows, and if something fails in an expected way report their steps as best as possible back to pip: https://github.com/pypa/pip/issues\n\n    I've been testing, and contributing a little bit, to the improved performance in this release, it is most noticeable on large dependency trees or long backtracking. For example, a dry run of \"apache-airflow[all]\" using cached packages on my machine goes from ~418 seconds to ~185 seconds."
},
{
    "title": "No title",
    "content": "Do you understand how asyncio works behind the scenes? Read this article and see how you can use Python generators to create your own version of asyncio, and then use the __await__ dunder method to use the async/await keywords to come full circle!\n  \nhttps://jacobpadilla.com/articles/recreating-asyncio"
},
{
    "title": "No title",
    "content": "Just interested in how people approach this, typically I just use VSCode or QtCreator to build simple projects. However I now want to automate some of the build process such as running uic, and rcc.\n  \n    I've tried to use CMake but can't seem to get it to work without a lot of custom scripting (for example the AUTOUIC etc functions need c++ projects), can't see any info on running uic in QtCreator (which would be ideal but python support is really just an after thought).\n  \n    I could write some Makefiles but this is a little ad-hoc and also confuses the IDE's (and at the end of the day I want a simple process for my students to use and I already teach cmake for C++ dev).\n  \n    So I guess my questions are what workflows do people use, can you recommend any tools to help, or do you just have a per project script to run uic and rcc?\n  \n    (I may cross post this in both qt and python subreddits as I'm not sure where it fits best)"
},
{
    "title": "No title",
    "content": "Excited to finally showcase this!\n  \n    It's still pretty rough around the edges, but I'm finally happy enough with the feature set and curious to see what the community thinks about a framework like this.\n  \n    Code: github.com/crpier/relax-py\n\n    Documentation: crpier.github.io/relax-py\n\nWhat My Project Does\n\nrelax-py is a Python framework for building full-stack applications with htmx\n\n    It provides tools for writing HTML in a manner similar to simple_html (which also inspired the decision to use standard Python to write HTML, rather than use Jinja2 or to make something like templ work in Python)\n  \n    It has:\n  \n\n\n    Hot Module Replacement (meaning, when you update the code that generates HTML templates, the browser also updates instantly) - see the video in the documentation for a quick demo of this\n  \n\n\n    URL resolution with type hinting - you can get the URL of an endpoint to use in your templates by using the function that handles that URL, and get help from static typing (for example, for putting path parameters in the URL)\n  \n\n\n    Helpers for dependency injection\n  \n\n\n    In essence, this framework is just a bunch of decorators and functions over starlette, meaning everything that starlette has can be used alongside the framework.\n  \nTarget Audience\n\n    Developers interested in building web applications with htmx that like new shiny things and static typing support\n  \nComparison\n\n    As far as I know, the only other backend framework that has Hot Module Replacement is turbo in Ruby on Rails, but there might be something I missed.\n  \n    As for other points of comparison with other frameworks:\n  \n\n\n    Django\n  \n\n\nrelax is less opinionated about what's done in the backend (.e.g there is preference to what ORM is used)\n  \n\n\n    using standard Python code to generate HTML has nicer static typing\n  \n\n\n    the URL resolution is more complex and provides errors in the IDE by way of static typing\n  \n\n\n    the component decorator provides nicer ways to reuse template functions and helpers for interoperability with JavaScript\n  \n\n\n\n\ntempl in Go\n  \n\n\n    templ allows writing actual HTML in go files, but requires an additional compilation step\n  \n\n\n    plugins for whatever IDE/code editor is used are needed parsing templ files\n  \n\n\n\n\n    FastAPI (with something to generate HTML like simple_html or Jinja2)\n  \n\n\n    since FastAPI is built for RESTful APIs, it lacks niceties like URL resolution, or a mechanism to manage the sprawling mess of interconnected HTML components that apps tend to develop\n  \n\n\n    dependency injection in FastAPI is \"encouraged\" to happen in the path functions, but in relax it's meant to happen at any level of the app (either in path functions, or in service-level functions, or in util functions)\n  \n\n\n\n\n    simple_html (with a backend like Flask or FastAPI): the main differences between simple_html and the relax.html module are that\n  \n\n\n    CSS classes are provided as a list of strings - this makes it easier to reuse them in different components, and will make it easier to implement other helpers in the future, like a Python version of tailwind-merge, or a formatter that sorts tailwind classes\n  \n\n\n    htmx-related attributes are included in the elements\n  \n\n\n    inserting children to an HTML element is done after instantiating the element, making it easier to reuse components\n  \n\n\n\n\n    Here's the code again: github.com/crpier/relax-py\n\n    There's more details in the documentation: crpier.github.io/relax-py\n\n    While this framework is definitely not production ready, in the \"Other\" page of the documentation there's an example app built with this framework, which shows how it can be used in conjuction with some real-life scenarios (production environment for tailwind with plugins, working in a bunch of interactivity with JavaScript, in either separate js files and inline scripts, Dockerfiles and deployments, authentication and authorization, configuration etc.)\n  \n    Please let me know what you think (are there better alternatives, is writing HTML in standard Python a deal-breaker, is investing in making something templ in Python worth it?)\n  \n    Hope you're intrigued by this!"
},
{
    "title": "No title",
    "content": "python-oracledb 2.2, the Oracle Database driver, has been released with support for Oracle Database 23ai features such as the VECTOR and BOOLEAN data types, Implicit Connection Pooling, and improved connection performance.  See the release announcement."
},
{
    "title": "No title",
    "content": "See https://github.com/BenHanson/pylexertl\n\n    I will see about registering as an official library when I am happy I have completed all the bindings. I added all the missing functions for the rules objects today, so things are in reasonable shape already.\n  \n    My python experience has been limited up until now, but it is big for my new role.\n  \n    I have a runtime parser generator https://github.com/BenHanson/parsertl17 which I also plan to add bindings for.\n  \n    I hope this is of interest to somebody!\n  \nWhat My Project Does\n\n    Allows you to build lexical analysers at runtime and use them to lex text (in this case utf-8)\n  \nTarget Audience\n\n    The C++ library has been used in production for over 10 years.\n  \nComparison\n\n    I'm not aware of any competing library."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    PgQueuer is a Python library designed to manage job queues using  PostgreSQL features. It leverages PostgreSQL's native LISTEN/NOTIFY, along with advanced locking mechanisms, to handle job queues efficiently. This allows for real-time job processing, concurrency, and reliable task execution without the need for a separate queuing system.\n  \nTarget Audience\n\n    PgQueuer is ideal for developers and teams who already use PostgreSQL in their projects and are looking for a simple, integrated way to handle background tasks and job queues. It's designed for production use, offering a dependable solution that scales seamlessly with existing PostgreSQL databases.\n  \nComparison\n\n    Unlike many other job queue solutions that require additional services or complex setups (such as Redis or RabbitMQ), PgQueuer operates directly within PostgreSQL. This removes the overhead of integrating and maintaining separate systems for job management.\n  \nHow PgQueuer stands out\n\n\n\nIntegration Simplicity: Integrates directly with existing PostgreSQL setups without additional infrastructure.\n  \n\n\nEfficiency: Uses PostgreSQL’s FOR UPDATE SKIP LOCKED for high concurrency, allowing multiple workers to process tasks simultaneously without conflict.\n  \n\n\nReal-time Updates: Utilizes PostgreSQL's LISTEN/NOTIFY for immediate job processing updates, reducing latency compared to polling-based systems.\n  \n\n\nRequest for Feedback on Useful Features\n\n    Im always looking to improve PgQueuer and make it more useful for our users. If you have any features you'd like to see, or if there's something you think could be improved, please let me know! Your feedback is invaluable! Share your thoughts, suggestions, or feature requests either here in the comments or via GitHub."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hello r/python,\n  \n    I've developed a Python script that allows you to reboot your router remotely via SSH! This script handles the countdown and checks when the router is back online after a reboot.\n  What My Project Does:\nKey Features:\n\n\n\nAutomated Router Reboot: Remotely trigger a reboot of your router.\n  \n\n\nMonitoring: After sending the reboot command, the script counts down from 350 seconds and starts checking the router's status by pinging it after the first 100 seconds have passed.\n  \n\n\nFlexibility: You can pass arguments dynamically (router IP, username, password, and port) or use hardcoded values within the script.\n  \n\n\nMethod of Execution: To execute the script from the command line:\n  python3 reboot-router.py --ip <router_ip> --username <username> --password <password> --port <port_number>\n    Default values are set, but it's highly recommended to pass arguments to the script for security reasons.\n  Target Audience:\n    This script is intended for:\n  \n\n\nTech Enthusiasts and Home Users who enjoy managing their home network setups and want a quick way to automate router management.\n  \n\nRequirements:\nRequired Modules and Programs:\n\n\n\nPython 3: The script is written in Python 3. Ensure you have Python 3.6 or newer installed.\n  \n\n\nsubprocess and argparse modules: These are standard libraries in Python and should be available with your Python installation.\n  \n\n\nsshpass: This utility is used for noninteractive password authentication with SSH. Install it using your package manager, e.g., sudo apt-get install sshpass for Debian/Ubuntu.\n  \n\nImportant Router Configuration:\n    Before using this script, make sure your router is configured to:\n  \n\n\nEnable SSH Access: Ensure SSH is turned on and configured to accept password authentication. This setting is usually found under the Administration tab in your router settings.\n  \n\n\nAllow ICMP Echo (Ping) Requests: Some routers disable ICMP Echo requests by default for security. You must enable Respond ICMP Echo (ping) Request from WAN under the Firewall tab.\n  \n\nComparison:\n    Unlike many GUI-based tools, this script provides a simple, lightweight command-line solution easily integrated into larger automation workflows or triggered manually without logging into the router interface.\n  For People New to Python:\n    If you're new to scripting or network management, be cautious about storing sensitive information like passwords directly in scripts. While hardcoded values can be used for ease and demonstration, the best practice is to pass these securely as arguments to prevent exposure.\n  Access to the script\n    You can access the script on my GitHub page here\n\n    Feel free to use, modify, and share this script! I look forward to your feedback and enhancements!\n  \n    Cheers -J"
},
{
    "title": "No title",
    "content": "Hello folks, I am the author of NiPyAPI, the defacto Python client for the Apache NiFi project.\n  \n    For those unfamiliar, Apache NiFi is an open-source ETL tool designed around the flow-based programming paradigm. It excels at continuously moving and managing arbitrary data flows between disparate systems with low latency at large scale, and is often contrasted with Airflow.\n  \n    Amongst many features, NiFi allows for Command & Control via a native UI, enabling live edits to data routing and transformation. NiFi also enforces an API-first approach, and produces a swagger definition during code compilation, which allowed creation of a Python client to enable automated Data Flow testing and a Design & Deploy approach by running it through Swagger Codegen 2.3 tool shaped by mustache templates. This produces a very verbose low-level client, which I then leverage in higher-level operational functions and demo scripts in the library.\n  \n    However it's always bugged me that this produces literally 10's of thousands of lines of boilerplate, so potentially replacing the low level client with something more modern would ideally massively reduce the codebase without impacting the high-level functionality of the library.\n  \n    Now, I first started it mid last decade with Python2 for broad compatibility and times have moved on significantly (as has my skill with Python) but it's finally time to drop Python2 support and move to a better build & release process, which opens the door to a larger update, and I am frankly not across modern best-practices around ClientGen so I come seeking r/Python's advice: How would you modernise this venerable artefact?\n  \n    I could simply move to a newer version of Swagger Codegen, or move into the OpenAPI Generator, I could get fancy with something like Fern - or I could leave well enough alone and just accept that the tons of boilerplate are at least very readable."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Listen at https://podcast.pythontest.com/episodes/220-juggling-pyconEven if you never get a chance to go to PyCon, I hope this interview helps you get a feel for the welcoming aspect of the Python community.\n  \n    The juggling at PyCon is one of the inspirations for PythonPeople.fm, one of PythonTests's sibling podcasts.\n  \n    Do you have any conference tips to add?"
},
{
    "title": "No title",
    "content": "Hey, python folks ! I have been coding in python for around 3 years, 2 years professionally. I have worked with asyncio, typing and other stuff that is needed to build a server. I was looking for a small but impactful enough open source core python library/application to work on. I tried cpython but it seems to be beyond my capability at the moment. As for my interests I was interested in lower level stuff as well as libraries like asyncio and celery. Any suggestions for libraries that could use a bit of help and teach me some stuff as well would be appreciated"
},
{
    "title": "No title",
    "content": "Python Streamlit is terrific for putting together interactive dashboards.\n  \n    Combined with the geopandas library, streamlit can easily display GIS data points on a map for you.\n  \n    Forest fires in my home province of British Columbia, Canada have been really bad recently. NASA has a terrific dataset that keeps track of forest fires by country.\n  \n    Can I use Streamlit to access this dataset and display a map off all the fires within a certain area (BC) for a particular time frame (2021)?\n  \n    And can I give the user the ability to choose a month?\n  \n    You bet! Let me step you through how!\n  \n    FREE tutorial (with code):\n  \nhttps://johnloewen.substack.com/p/simple-interactive-python-streamlit"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hello everybody,\n  \n    Thank you for having us here, and a huge \"Thank you\" to the moderators for letting us post.\n  \n    We have just released the latest edition of The Quick Python Book by the one-and-only Naomi Ceder, and I wanted to share that news with the community.\n  \n    Many of you are already familiar with Naomi's work and her massive contributions to the world of Python programming language.\n  \n    The Quick Python Book has aided over 100,000 developers in mastering Python. The Fourth Edition of the book has been revised to include the latest features, control structures, and libraries of Python, along with new coverage of working with AI-generated Python code. Naomi, the author, has beautifully balanced the details of the language with the insights and advice required to accomplish any task. Her personal touch has made learning Python an enjoyable experience for countless developers.\n  \n    📚 You can find the book here: https://mng.bz/aEQj\n\n    📖 Get into the liveBook: https://mng.bz/gvee\n\n    And last but not the least, get 46% off with code: receder46\n\n    Hope you find the book helpful.\n  \n    Thank you.\n  \n    Cheers,"
},
{
    "title": "No title",
    "content": "Excited to share my pypi package typedparser I have been working on for around 1 year now.\n  \nWhat My Project Does: It enables writing CLI scripts and create an \"args\" variable with autocompleted members and type checks, but still keeps the simple and universally understood syntax of the stdlib argarse module.\n  \nTarget Audience: For stability, I battletested it in my research projects and added automatic builds as well as 80%+ test coverage. So I believe it is pretty stable.\n  \nComparison: For typing functionality it uses the attrs package as backend. It also provides some additional features for object and dictionary manipulation. Of course there are many other CLI argument packages out there, but this one stands out in that it tries to keep the syntax of the argparse standard library as much as possible, making it easy for others to figure out what your script does. Check it out and let me know what you think."
},
{
    "title": "No title",
    "content": "Where can I find a decent comparison (pros and cons) of these 5 solutions? They seem to be solving the same problem, which is, afaiu, separating the frontend ‘annoyance’ from Python scripting  / math.\n  \n\n\n    ⁠Reflex (used to be called Pynecone) https://reflex.dev\n\n\n\n    ⁠Streamlit https://streamlit.io\n\n\n\n    ⁠Gradio https://gradio.app\n\n\n\n    ⁠Dash https://dash.plotly.com\n\n\n\n    ⁠Panel https://panel.holoviz.org/\n\n\n\n    ⁠Anvil https://anvil.works/\n\n\n\n    Quarto\n  \n\n\n    My use case: user access the web app, choose some parameters, selects things that go or not into a model. Python returns results of my math. Needs to be somewhat eye-candy and I need to use a lot of pictures to get the user input (i.e. “which of these figures you like most? 1,2,3. User clicks on “3”, 3 is considered in the model."
},
{
    "title": "No title",
    "content": "What my project does:\n  \n    It basically takes a formula string like you'd get from Openpyxl like \"=SUM(A1:B2)\" and breaks it all out into a dictionary structure for you to then navigate through, modify, and then reformat that modified structure back into an excel friendly formula string again!\n  \n    Target Audience: (People who modify Excel formula strings in automated spreadsheet modification scripts. Or people who need to analyze formulas in a spreadsheet to do some kind of logic based on that analysis).\n  \n    Disclaimer: For most people some simple regex pattern matching and str replaces would be fine to modify formulas but if you need a more structured approach to working with these strings, this package has you covered!\n  \n    How does it differ compared to other projects: There are libraries like Openpyxl that allow you to tokenize and translate formulas but that's currently where it ends. It doesn't allow you to systematically parse out a formula and replace those pieces and add new structures and what not into it. Currently the best you can really do is translate formulas and anything other than that would need to rely on regex string matching logic or string replacements. (Which still would be fine for most people, but this just adds another layer of organization and scalability to the format).\n  \n    More info about it here: https://github.com/Voltaic314/ExcelFormulaParser\n\n    To install, just do: pip install ExcelFormulaParser\n  \n    Thank you for reading this!! Hope you guys find it useful if you're ever systematically modifying (or analyzing) spreadsheets!"
},
{
    "title": "No title",
    "content": "Hello everyone! I'd like to announce version 1.10.0: https://hatch.pypa.io/latest/blog/2024/05/02/hatch-v1100/\n\n    Feel free to provide any feedback either here or as a discussion on the repo: https://github.com/pypa/hatch"
},
{
    "title": "No title",
    "content": "I'd like to explore other people's setup and perhaps try need things or extra tools. What kind IDE, any extra tools to make it easier for you, etc. Looking forward to everyone's responses!"
},
{
    "title": "No title",
    "content": "I made a tutorial on how to build a server-to-server Zoom OAuth application using Python. This application can transcribe Zoom meeting recordings, print the transcripts to the terminal, and save the transcripts as text files.\n  \n\n\nvideo tutorial\n\n\n\nrepo\n\n\n\nwritten tutorial\n\n\n\n    This tutorial covers:\n  \n\n\n    Setting up OAuth authentication for server-to-server apps\n  \n\n\n    Utilizing the Zoom API to access recordings\n  \n\n\n    Implementing automatic transcription using Python"
},
{
    "title": "No title",
    "content": "I created this tutorial after overcoming a difficult challenge myself: uploading 5GB+ files to AWS. This approach allows the browser to securely upload directly to an S3 bucket without the file having to travel through the backend server. The implementation is written in python (backend) and vanilla js (frontend)."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I have a simple backend REST API service that is serving a few ML models. I have made it \"secured\" by implementing an API key in order call those endpoints.\n  \n    I was wondering, how common it is for people to use services that can be self-hosted as their authentication/authorization.\n  \n    If it is common and reliable, what are the best options to go for?\n  \n    I've read that building your own authentication/authorization service with email verification, password reset, and social auth can be a pain.\n  \n    Also, did some googling and found this General - Fief. Has anyone ever tried using this? If so, how was the experience?\n  \n    Thanks in advance."
},
{
    "title": "No title",
    "content": "Hello Pythonistas!I just wrote a blog post about measuring performance inside pytest test cases. We dive into why it’s important to test for performance and how to integrate the measurements in the CI.Here is the link to the blog: https://codspeed.io/blog/one-pytest-marker-to-track-the-performance-of-your-tests"
},
{
    "title": "No title",
    "content": "Hello world,I wrote an article about creating diagrams from code on Jupyter Notebook inside VS Code. It will give you a brief on the setup and also an overview of concepts. Within 5 minutes, you should be able to start making cool architecture diagrams.\n  \n    [TO MODERATOR: This link does not contain any paywalled or paid content. All the contents are available for free]\n  \n    Article link: https://ashgaikwad.substack.com/p/how-to-create-architecture-diagrams"
},
{
    "title": "No title",
    "content": "Hey everyone 👋\n  \nTL;DRSince everyone is talking about the Humane AI Pin and the Rabbit R1, I decided to make a short 5 minute tutorial on how people can setup and customize their own little AI assistant on their machine.\n  \n    I've uploaded a video tutorial here: https://www.youtube.com/watch?v=2fD_SAouoOs&ab_channel=2BytesGoat\n\n    And the Github code is here: https://github.com/2BYTESGOAT/AI-ASSISTANT\n\nLonger version\n\n\n\nWhat my project does: It's the starter code for an AI assistant that you can run locally. More precisely, it's a ChatGPT / Llama 2 agent that has access to Google Search and can get businesses nearby based on your location. The tool can be easily extended to support other APIs.\n  \n\n\nTarget audience: Pythoneers that are curious about LLMs and LLM related libraries.\n  \n\n\nComparison: It was inspired by projects such as the Humane AI Pin and the Rabbit R1. Though it's a inferior version to those, it serves more as a playground for people to develop their own AI assistants."
},
{
    "title": "No title",
    "content": "The Python on Microcontrollers (and Raspberry Pi) Newsletter: subscribe for free\n    With the Python on Microcontrollers newsletter, you get all the latest information on Python running on hardware in one place! MicroPython, CircuitPython and Python on single Board Computers like Raspberry Pi & many more.\n  \n    The Python on Microcontrollers newsletter is the place for the latest news. It arrives Monday morning with all the week’s happenings. No advertising, no spam, easy to unsubscribe.\n  \n    10,958 subscribers - the largest Python on hardware newsletter out there.\n  \n    Catch all the weekly news on Python for Microcontrollers with adafruitdaily.com.\n  \n    This ad-free, spam-free weekly email is filled with CircuitPython, MicroPython, and Python information that you may have missed, all in one place!\n  \n    Ensure you catch the weekly Python on Hardware roundup– you can cancel anytime – try our spam-free newsletter today!\n  \nhttps://www.adafruitdaily.com/"
},
{
    "title": "No title",
    "content": "Hi everyone! We've recently written up a guide for anyone running a Python server (ex. Flask, Django, FastAPI) to instrument their app to debug slow downs and errors using the CNCF OpenTelemetry project and their Python instrumentation package.\n  \n    It's really straightforward to get started with just a few lines of added code and commands to any Python project, so hopefully helps some people out as they're looking to add better instrumentation to their servers.\n  \n    Here's the tutorial: https://www.hyperdx.io/blog/opentelemetry-python-server-auto-instrumentation"
},
{
    "title": "No title",
    "content": "What my project does:\n\n    I wanted to share an open-source project I’ve been working on called k8sAI. It’s a personal AI Kubernetes expert that can answer questions about your cluster, suggests commands, and even executes relevant kubectl commands to help diagnose and suggest fixes to your cluster, all in the CLI!\n  \nTarget Audience:\n\n    As a relative newcomer to k8s, this tool has really streamlined my workflow. I can ask questions about my cluster, k8sAI will run kubectl commands to gather info, and then answer those question. It’s also found several issues in my cluster for me - all I’ve had to do is point it in the right direction. I’ve really enjoyed making and using this so I thought it could be useful for others. Added bonus is that you don’t need to copy and paste into ChatGPT anymore!\n  \n    k8sAI operates with read-only kubectl commands to make sure your cluster stays safe.\n  \n    All you need is an OpenAI API key and a valid kubectl config. Start chatting with k8sAI using:\n  $ pip install k8sAI\n$ k8sAI chat\n    or to fix an issue:\n  $ k8sAI fix -p=\"take a look at the failing pod in the test namespace\"\n    Would love to get any feedback you guys have!\n  \nHere's the repo for anyone who wants to take a look\n  \nComparison:I found a tool (k8sGPT) that I enjoyed using, but I felt it was still missing a few pieces on the chatbot side. You can't chat back and forth with k8sGPT and it doesn't suggest commands for you to execute, so I decided to make this."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hello r/python,\n  \n    I've developed a Python script that allows you to reboot your router remotely via SSH! This script handles the countdown and checks when the router is back online after a reboot.\n  What My Project Does:\nKey Features:\n\n\n\nAutomated Router Reboot: Remotely trigger a reboot of your router.\n  \n\n\nMonitoring: After sending the reboot command, the script counts down from 350 seconds and starts checking the router's status by pinging it after the first 100 seconds have passed.\n  \n\n\nFlexibility: You can pass arguments dynamically (router IP, username, password, and port) or use hardcoded values within the script.\n  \n\n\nMethod of Execution: To execute the script from the command line:\n  python3 reboot-router.py --ip <router_ip> --username <username> --password <password> --port <port_number>\n    Default values are set, but it's highly recommended to pass arguments to the script for security reasons.\n  Target Audience:\n    This script is intended for:\n  \n\n\nTech Enthusiasts and Home Users who enjoy managing their home network setups and want a quick way to automate router management.\n  \n\nRequirements:\nRequired Modules and Programs:\n\n\n\nPython 3: The script is written in Python 3. Ensure you have Python 3.6 or newer installed.\n  \n\n\nsubprocess and argparse modules: These are standard libraries in Python and should be available with your Python installation.\n  \n\n\nsshpass: This utility is used for noninteractive password authentication with SSH. Install it using your package manager, e.g., sudo apt-get install sshpass for Debian/Ubuntu.\n  \n\nImportant Router Configuration:\n    Before using this script, make sure your router is configured to:\n  \n\n\nEnable SSH Access: Ensure SSH is turned on and configured to accept password authentication. This setting is usually found under the Administration tab in your router settings.\n  \n\n\nAllow ICMP Echo (Ping) Requests: Some routers disable ICMP Echo requests by default for security. You must enable Respond ICMP Echo (ping) Request from WAN under the Firewall tab.\n  \n\nComparison:\n    Unlike many GUI-based tools, this script provides a simple, lightweight command-line solution easily integrated into larger automation workflows or triggered manually without logging into the router interface.\n  For People New to Python:\n    If you're new to scripting or network management, be cautious about storing sensitive information like passwords directly in scripts. While hardcoded values can be used for ease and demonstration, the best practice is to pass these securely as arguments to prevent exposure.\n  Access to the script\n    You can access the script on my GitHub page here\n\n    Feel free to use, modify, and share this script! I look forward to your feedback and enhancements!\n  \n    Cheers -J"
},
{
    "title": "No title",
    "content": "Hello folks, I am the author of NiPyAPI, the defacto Python client for the Apache NiFi project.\n  \n    For those unfamiliar, Apache NiFi is an open-source ETL tool designed around the flow-based programming paradigm. It excels at continuously moving and managing arbitrary data flows between disparate systems with low latency at large scale, and is often contrasted with Airflow.\n  \n    Amongst many features, NiFi allows for Command & Control via a native UI, enabling live edits to data routing and transformation. NiFi also enforces an API-first approach, and produces a swagger definition during code compilation, which allowed creation of a Python client to enable automated Data Flow testing and a Design & Deploy approach by running it through Swagger Codegen 2.3 tool shaped by mustache templates. This produces a very verbose low-level client, which I then leverage in higher-level operational functions and demo scripts in the library.\n  \n    However it's always bugged me that this produces literally 10's of thousands of lines of boilerplate, so potentially replacing the low level client with something more modern would ideally massively reduce the codebase without impacting the high-level functionality of the library.\n  \n    Now, I first started it mid last decade with Python2 for broad compatibility and times have moved on significantly (as has my skill with Python) but it's finally time to drop Python2 support and move to a better build & release process, which opens the door to a larger update, and I am frankly not across modern best-practices around ClientGen so I come seeking r/Python's advice: How would you modernise this venerable artefact?\n  \n    I could simply move to a newer version of Swagger Codegen, or move into the OpenAPI Generator, I could get fancy with something like Fern - or I could leave well enough alone and just accept that the tons of boilerplate are at least very readable."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Listen at https://podcast.pythontest.com/episodes/220-juggling-pyconEven if you never get a chance to go to PyCon, I hope this interview helps you get a feel for the welcoming aspect of the Python community.\n  \n    The juggling at PyCon is one of the inspirations for PythonPeople.fm, one of PythonTests's sibling podcasts.\n  \n    Do you have any conference tips to add?"
},
{
    "title": "No title",
    "content": "Hey, python folks ! I have been coding in python for around 3 years, 2 years professionally. I have worked with asyncio, typing and other stuff that is needed to build a server. I was looking for a small but impactful enough open source core python library/application to work on. I tried cpython but it seems to be beyond my capability at the moment. As for my interests I was interested in lower level stuff as well as libraries like asyncio and celery. Any suggestions for libraries that could use a bit of help and teach me some stuff as well would be appreciated"
},
{
    "title": "No title",
    "content": "Python Streamlit is terrific for putting together interactive dashboards.\n  \n    Combined with the geopandas library, streamlit can easily display GIS data points on a map for you.\n  \n    Forest fires in my home province of British Columbia, Canada have been really bad recently. NASA has a terrific dataset that keeps track of forest fires by country.\n  \n    Can I use Streamlit to access this dataset and display a map off all the fires within a certain area (BC) for a particular time frame (2021)?\n  \n    And can I give the user the ability to choose a month?\n  \n    You bet! Let me step you through how!\n  \n    FREE tutorial (with code):\n  \nhttps://johnloewen.substack.com/p/simple-interactive-python-streamlit"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hello everybody,\n  \n    Thank you for having us here, and a huge \"Thank you\" to the moderators for letting us post.\n  \n    We have just released the latest edition of The Quick Python Book by the one-and-only Naomi Ceder, and I wanted to share that news with the community.\n  \n    Many of you are already familiar with Naomi's work and her massive contributions to the world of Python programming language.\n  \n    The Quick Python Book has aided over 100,000 developers in mastering Python. The Fourth Edition of the book has been revised to include the latest features, control structures, and libraries of Python, along with new coverage of working with AI-generated Python code. Naomi, the author, has beautifully balanced the details of the language with the insights and advice required to accomplish any task. Her personal touch has made learning Python an enjoyable experience for countless developers.\n  \n    📚 You can find the book here: https://mng.bz/aEQj\n\n    📖 Get into the liveBook: https://mng.bz/gvee\n\n    And last but not the least, get 46% off with code: receder46\n\n    Hope you find the book helpful.\n  \n    Thank you.\n  \n    Cheers,"
},
{
    "title": "No title",
    "content": "Excited to share my pypi package typedparser I have been working on for around 1 year now.\n  \nWhat My Project Does: It enables writing CLI scripts and create an \"args\" variable with autocompleted members and type checks, but still keeps the simple and universally understood syntax of the stdlib argarse module.\n  \nTarget Audience: For stability, I battletested it in my research projects and added automatic builds as well as 80%+ test coverage. So I believe it is pretty stable.\n  \nComparison: For typing functionality it uses the attrs package as backend. It also provides some additional features for object and dictionary manipulation. Of course there are many other CLI argument packages out there, but this one stands out in that it tries to keep the syntax of the argparse standard library as much as possible, making it easy for others to figure out what your script does. Check it out and let me know what you think."
},
{
    "title": "No title",
    "content": "Where can I find a decent comparison (pros and cons) of these 5 solutions? They seem to be solving the same problem, which is, afaiu, separating the frontend ‘annoyance’ from Python scripting  / math.\n  \n\n\n    ⁠Reflex (used to be called Pynecone) https://reflex.dev\n\n\n\n    ⁠Streamlit https://streamlit.io\n\n\n\n    ⁠Gradio https://gradio.app\n\n\n\n    ⁠Dash https://dash.plotly.com\n\n\n\n    ⁠Panel https://panel.holoviz.org/\n\n\n\n    ⁠Anvil https://anvil.works/\n\n\n\n    Quarto\n  \n\n\n    My use case: user access the web app, choose some parameters, selects things that go or not into a model. Python returns results of my math. Needs to be somewhat eye-candy and I need to use a lot of pictures to get the user input (i.e. “which of these figures you like most? 1,2,3. User clicks on “3”, 3 is considered in the model."
},
{
    "title": "No title",
    "content": "What my project does:\n  \n    It basically takes a formula string like you'd get from Openpyxl like \"=SUM(A1:B2)\" and breaks it all out into a dictionary structure for you to then navigate through, modify, and then reformat that modified structure back into an excel friendly formula string again!\n  \n    Target Audience: (People who modify Excel formula strings in automated spreadsheet modification scripts. Or people who need to analyze formulas in a spreadsheet to do some kind of logic based on that analysis).\n  \n    Disclaimer: For most people some simple regex pattern matching and str replaces would be fine to modify formulas but if you need a more structured approach to working with these strings, this package has you covered!\n  \n    How does it differ compared to other projects: There are libraries like Openpyxl that allow you to tokenize and translate formulas but that's currently where it ends. It doesn't allow you to systematically parse out a formula and replace those pieces and add new structures and what not into it. Currently the best you can really do is translate formulas and anything other than that would need to rely on regex string matching logic or string replacements. (Which still would be fine for most people, but this just adds another layer of organization and scalability to the format).\n  \n    More info about it here: https://github.com/Voltaic314/ExcelFormulaParser\n\n    To install, just do: pip install ExcelFormulaParser\n  \n    Thank you for reading this!! Hope you guys find it useful if you're ever systematically modifying (or analyzing) spreadsheets!"
},
{
    "title": "No title",
    "content": "Hello everyone! I'd like to announce version 1.10.0: https://hatch.pypa.io/latest/blog/2024/05/02/hatch-v1100/\n\n    Feel free to provide any feedback either here or as a discussion on the repo: https://github.com/pypa/hatch"
},
{
    "title": "No title",
    "content": "I'd like to explore other people's setup and perhaps try need things or extra tools. What kind IDE, any extra tools to make it easier for you, etc. Looking forward to everyone's responses!"
},
{
    "title": "No title",
    "content": "I made a tutorial on how to build a server-to-server Zoom OAuth application using Python. This application can transcribe Zoom meeting recordings, print the transcripts to the terminal, and save the transcripts as text files.\n  \n\n\nvideo tutorial\n\n\n\nrepo\n\n\n\nwritten tutorial\n\n\n\n    This tutorial covers:\n  \n\n\n    Setting up OAuth authentication for server-to-server apps\n  \n\n\n    Utilizing the Zoom API to access recordings\n  \n\n\n    Implementing automatic transcription using Python"
},
{
    "title": "No title",
    "content": "I created this tutorial after overcoming a difficult challenge myself: uploading 5GB+ files to AWS. This approach allows the browser to securely upload directly to an S3 bucket without the file having to travel through the backend server. The implementation is written in python (backend) and vanilla js (frontend)."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I have a simple backend REST API service that is serving a few ML models. I have made it \"secured\" by implementing an API key in order call those endpoints.\n  \n    I was wondering, how common it is for people to use services that can be self-hosted as their authentication/authorization.\n  \n    If it is common and reliable, what are the best options to go for?\n  \n    I've read that building your own authentication/authorization service with email verification, password reset, and social auth can be a pain.\n  \n    Also, did some googling and found this General - Fief. Has anyone ever tried using this? If so, how was the experience?\n  \n    Thanks in advance."
},
{
    "title": "No title",
    "content": "Hello Pythonistas!I just wrote a blog post about measuring performance inside pytest test cases. We dive into why it’s important to test for performance and how to integrate the measurements in the CI.Here is the link to the blog: https://codspeed.io/blog/one-pytest-marker-to-track-the-performance-of-your-tests"
},
{
    "title": "No title",
    "content": "Hello world,I wrote an article about creating diagrams from code on Jupyter Notebook inside VS Code. It will give you a brief on the setup and also an overview of concepts. Within 5 minutes, you should be able to start making cool architecture diagrams.\n  \n    [TO MODERATOR: This link does not contain any paywalled or paid content. All the contents are available for free]\n  \n    Article link: https://ashgaikwad.substack.com/p/how-to-create-architecture-diagrams"
},
{
    "title": "No title",
    "content": "Hey everyone 👋\n  \nTL;DRSince everyone is talking about the Humane AI Pin and the Rabbit R1, I decided to make a short 5 minute tutorial on how people can setup and customize their own little AI assistant on their machine.\n  \n    I've uploaded a video tutorial here: https://www.youtube.com/watch?v=2fD_SAouoOs&ab_channel=2BytesGoat\n\n    And the Github code is here: https://github.com/2BYTESGOAT/AI-ASSISTANT\n\nLonger version\n\n\n\nWhat my project does: It's the starter code for an AI assistant that you can run locally. More precisely, it's a ChatGPT / Llama 2 agent that has access to Google Search and can get businesses nearby based on your location. The tool can be easily extended to support other APIs.\n  \n\n\nTarget audience: Pythoneers that are curious about LLMs and LLM related libraries.\n  \n\n\nComparison: It was inspired by projects such as the Humane AI Pin and the Rabbit R1. Though it's a inferior version to those, it serves more as a playground for people to develop their own AI assistants."
},
{
    "title": "No title",
    "content": "The Python on Microcontrollers (and Raspberry Pi) Newsletter: subscribe for free\n    With the Python on Microcontrollers newsletter, you get all the latest information on Python running on hardware in one place! MicroPython, CircuitPython and Python on single Board Computers like Raspberry Pi & many more.\n  \n    The Python on Microcontrollers newsletter is the place for the latest news. It arrives Monday morning with all the week’s happenings. No advertising, no spam, easy to unsubscribe.\n  \n    10,958 subscribers - the largest Python on hardware newsletter out there.\n  \n    Catch all the weekly news on Python for Microcontrollers with adafruitdaily.com.\n  \n    This ad-free, spam-free weekly email is filled with CircuitPython, MicroPython, and Python information that you may have missed, all in one place!\n  \n    Ensure you catch the weekly Python on Hardware roundup– you can cancel anytime – try our spam-free newsletter today!\n  \nhttps://www.adafruitdaily.com/"
},
{
    "title": "No title",
    "content": "Hi everyone! We've recently written up a guide for anyone running a Python server (ex. Flask, Django, FastAPI) to instrument their app to debug slow downs and errors using the CNCF OpenTelemetry project and their Python instrumentation package.\n  \n    It's really straightforward to get started with just a few lines of added code and commands to any Python project, so hopefully helps some people out as they're looking to add better instrumentation to their servers.\n  \n    Here's the tutorial: https://www.hyperdx.io/blog/opentelemetry-python-server-auto-instrumentation"
},
{
    "title": "No title",
    "content": "What my project does:\n\n    I wanted to share an open-source project I’ve been working on called k8sAI. It’s a personal AI Kubernetes expert that can answer questions about your cluster, suggests commands, and even executes relevant kubectl commands to help diagnose and suggest fixes to your cluster, all in the CLI!\n  \nTarget Audience:\n\n    As a relative newcomer to k8s, this tool has really streamlined my workflow. I can ask questions about my cluster, k8sAI will run kubectl commands to gather info, and then answer those question. It’s also found several issues in my cluster for me - all I’ve had to do is point it in the right direction. I’ve really enjoyed making and using this so I thought it could be useful for others. Added bonus is that you don’t need to copy and paste into ChatGPT anymore!\n  \n    k8sAI operates with read-only kubectl commands to make sure your cluster stays safe.\n  \n    All you need is an OpenAI API key and a valid kubectl config. Start chatting with k8sAI using:\n  $ pip install k8sAI\n$ k8sAI chat\n    or to fix an issue:\n  $ k8sAI fix -p=\"take a look at the failing pod in the test namespace\"\n    Would love to get any feedback you guys have!\n  \nHere's the repo for anyone who wants to take a look\n  \nComparison:I found a tool (k8sGPT) that I enjoyed using, but I felt it was still missing a few pieces on the chatbot side. You can't chat back and forth with k8sGPT and it doesn't suggest commands for you to execute, so I decided to make this."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "GitHub\nWhat My Project Does\nPkgInspect is a comprehensive tool designed to inspect and compare Python packages and Python versions effortlessly. It equips users with a comprehensive set of tools and utility classes to retrieve essential information from installed Python packages, compare versions seamlessly, and extract various details about Python installations with ease.\n  Target Audience\n    Developers and Python enthusiasts looking to streamline the process of inspecting Python packages, comparing versions, and extracting vital information from Python installations will find PkgInspect invaluable. Many current modules such as importlib_metadata and pkg_resources are fairly limited on what items can be inspected and retrieved for a specified python package. Also noticed pkg_resources has also deprecated some of its important retrieval methods.\n  Comparison\nPkgInspect stands out from other Python package inspection tools due to its robust features. Unlike traditional methods that require manual inspection and comparison, PkgInspect automates the process, saving developers valuable time and effort. With PkgInspect, you can effortlessly retrieve package information, compare versions across different Python installations, and extract crucial details with just a few simple commands.\n  Key Features\n\n\nInspect Packages: Retrieve comprehensive information about installed Python packages.\n  \n\n\nCompare Versions: Seamlessly compare package data across different Python versions.\n  \n\n\nRetrieve Installed Pythons: Identify and list installed Python versions effortlessly.\n  \n\n\nInspect PyPI Packages: Gather detailed information about packages from the Python Package Index (PyPI).\n  \n\n\nFetch Available Updates: Stay up-to-date with available updates for a package from the current version.\n  \n\n\nList Inspection Fieldnames: Access a list of available fieldnames for package inspection.\n  \n\n\nRetrieve Package Metrics: Extract OS statistics about a package effortlessly.\n  \n\n\nFetch GitHub Statistics: Retrieve insightful statistics about a package from GitHub effortlessly.\n  \n\n\nRetrieve all Python Packages: Easily list all installed Python packages for a given Python version.\n  \n\n\n    Main Components\n  \nCore Modules\n\n\n\nPkgInspect: Inspects Python packages and retrieves package information.\n  \n\n\nPkgVersions: Retrieves and compares package data across different Python versions.\n  \n\n\nPkgMetrics: Extracts OS statistics about a package.\n  \n\n\nFunctions\n\n\n\ninspect_package: Inspects a Python package and retrieves package information.\n  \n\n\ninspect_pypi: Inspects a package from the Python Package Index (PyPI).\n  \n\n\nget_available_updates: Fetches available updates for a package from the current version.\n  \n\n\nget_installed_pythons: Identifies and lists installed Python versions.\n  \n\n\nget_version_packages: Lists all installed Python packages for a given Python version.\n  \n\n\npkg_version_compare: Compares package data across different Python versions.\n  \n\nInspection Field Options\nAny other field name will be treated as a file name to inspect from the packages' site-path directory.\n- `short_meta` (dict[str, Any]): Returns a dictionary of the most important metadata fields.\n    - If only one field is needed, you can use any of the following metadata fields.\n    - Possible Fields instead of `short_meta`:\n        - `Metadata-Version` (PackageVersion)\n        - `Name` (str)\n        - `Summary` (str)\n        - `Author-email` (str)\n        - `Home-page` (str)\n        - `Download-URL` (str)\n        - `Platform(s)` (set)\n        - `Author` (str)\n        - `Classifier(s)` (set)\n        - `Description-Content-Type` (str)\n- `short_license` (str): Returns the name of the license being used.\n- `metadata` (str): Returns the contents of the METADATA file.\n- `installer` (str): Returns the installer tool used for installation.\n- `license` (str): Returns the contents of the LICENSE file.\n- `record` (str): Returns the list of installed files.\n- `wheel` (str): Returns information about the Wheel distribution format.\n- `requested` (str): Returns information about the requested installation.\n- `authors` (str): Returns the contents of the AUTHORS.md file.\n- `entry_points` (str): Returns the contents of the entry_points.txt file.\n- `top_level` (str): Returns the contents of the top_level.txt file.\n- `source_file` (str): Returns the source file path for the specified package.\n- `source_code` (str): Returns the source code contents for the specified package.\n- `doc` (str): Returns the documentation for the specified package.\n\n- `Pkg` Custom Class Fields\n    - `PkgInspect fields`: Possible Fields from the `PkgInspect` class.\n        - `site_path` (Path): Returns the site path of the package.\n        - `package_paths` (Iterable[Path]): Returns the package paths of the package.\n        - `package_versions` (Generator[tuple[str, tuple[tuple[Any, str]]]]): Returns the package versions of the package.\n        - `pyversions` (tuple[Path]): Returns the Python versions of the package.\n        - `installed_pythons` (TupleOfPkgVersions): Returns the installed Python versions of the package.\n        - `site_packages` (Iterable[str]): Returns the site packages of the package.\n        - `islatest_version` (bool): Returns True if the package is the latest version.\n        - `isinstalled_version` (bool): Returns True if the package is the installed version.\n        - `installed_version` (PackageVersion): Returns the installed version of the package.\n        - `available_updates` (TupleOfPkgVersions): Returns the available updates of the package.\n\n    - `PkgVersions fields`: Possible Fields from the `PkgVersions` class.\n        - `initial_version` (PackageVersion): Returns the initial version of the package.\n        - `installed_version` (PackageVersion): Returns the installed version of the package.\n        - `latest_version` (PackageVersion): Returns the latest version of the package.\n        - `total_versions` (int): Returns the total number of versions of the package.\n        - `version_history` (TupleOfPkgVersions): Returns the version history of the specified package.\n        - `package_url`: Returns the URL of the package on PyPI.\n        - `github_stats_url` (str): Returns the GitHub statistics URL of the package.\n        - `github_stats` (dict[str, Any]): Returns the GitHub statistics of the package.\n            - The GitHub statistics are returned as a dictionary \\\n                containing the following fields which can accessed using the `item` parameter:\n                - `Forks` (int): Returns the number of forks on GitHub.\n                - `Stars` (int): Returns the number of stars on GitHub.\n                - `Watchers` (int): Returns the number of watchers on GitHub.\n                - `Contributors` (int): Returns the number of contributors on GitHub.\n                - `Dependencies` (int): Returns the number of dependencies on GitHub.\n                - `Dependent repositories` (int): Returns the number of dependent repositories on GitHub.\n                - `Dependent packages` (int): Returns the number of dependent packages on GitHub.\n                - `Repository size` (NamedTuple): Returns the size of the repository on GitHub.\n                - `SourceRank` (int): Returns the SourceRank of the package on GitHub.\n                - `Total releases` (int): Returns the total number of releases on GitHub.\n    \n    - `PkgMetrics fields`: Possible Fields from the `PkgMetrics` class.\n        - `all_metric_stats` (dict[str, Any]): Returns all the OS statistics of the package.\n        - `total_size` (int): Returns the total size of the package.\n        - `date_installed` (datetime): Returns the date the package was installed.\n\n- `pypistats fields`: Possible Fields from the `pypistats` module.\n    - `all-pypi-stats` (dict[str, Any]): Returns all the statistics of the package on PyPI into a single dictionary.\n    - `stats-overall` (dict[str, Any]): Returns the overall statistics of the package on PyPI.\n    - `stats-major` (dict[str, Any]): Returns the major version statistics of the package on PyPI.\n    - `stats-minor` (dict[str, Any]): Returns the minor version statistics of the package on PyPI.\n    - `stats-recent` (dict[str, Any]): Returns the recent statistics of the package on PyPI.\n    - `stats-system` (dict[str, Any]): Returns the system statistics of the package on PyPI.Downsides & Limitations\nMy algorithms are fairly well but do come with some important downsides.\n\n\n\nPkgInspect will ONLY inspect packages that are python files or contains a dist-info folder in the site-packages folder for a given Python version. Was not able to efficiently figure out a way to retrieve all necessary packages without containing unrelevant folders/files. Some personal packages may be skipped otherwise.\n  \n\n\n    Beta (pre-releases) has not been implemented yet.\n  \n\n\n    As many files may be handled, the runtime may be slow for some users.\n  \n\n\n    The demand for a project like this is not so much in-demand but have noticed many people, including my self, still seeking for a project like this. However, this type of project does seem to exceed my experience level with Python and algorithms (hence the downsides) so not entirely sure how far this project may come in the future. Was hoping for it to be GUI based if possible.\n  \n\nUsage Examplesfrom pkg_inspect import inspect_package\n\ninspect_package(\"pkg_inspect\", itemOrfile=\"initial_version\")\n# Output (Format - DateTimeAndVersion):\n('May 02, 2024', '0.1.0')\n\n\ninspect_package(\"pkg_inspect\", itemOrfile=\"version_history\")\n# Output (Format - tuple[DateTimeAndVersion]):\n(('May 02, 2024', '0.1.2'), ('May 02, 2024', '0.1.1'), ('May 02, 2024', '0.1.0'))\n\n\ninspect_package(\"pkg_inspect\", pyversion=\"3.12\", itemOrfile=\"short_meta\")\n# Output (Format dict[str, Any]):\n{'Author': 'Yousef Abuzahrieh',\n 'Author-email': 'yousefzahrieh17@gmail.com',\n 'Classifiers': {'Development Status 4 Beta',\n                 'Intended Audience Developers',\n                 'License OSI Approved Apache Software License',\n                 'Operating System OS Independent',\n                 'Programming Language Python 3',\n                 'Programming Language Python 3 Only',\n                 'Topic Utilities'},\n 'Description-Content-Type': 'text/markdown',\n 'Download-URL': 'https://github.com/yousefabuz17/PkgInspect.git',\n 'Home-page': 'https://github.com/yousefabuz17/PkgInspect',\n 'License': 'Apache Software License',\n 'Metadata-Version': <Version('2.1')>,\n 'Name': 'pkg-inspect',\n 'Platforms': {'Windows', 'MacOS', 'Linux'},\n 'Summary': 'A comprehensive tools to inspect Python packages and Python '\n            'installations.'}\n\n\ninspect_package(\"pandas\", pyversion=\"3.12\", itemOrfile=\"github_stats\")\n# Output (Format - dict[str, Any]):\n{'Contributors': '1.09K',\n 'Dependencies': 3,\n 'Dependent packages': '41.3K',\n 'Dependent repositories': '38.4K',\n 'Forks': '17.3K',\n 'Repository size': Stats(symbolic='338.000 KB (Kilobytes)', calculated_size=338.0, bytes_size=346112.0),\n 'SourceRank': 32,\n 'Stars': '41.9K',\n 'Total releases': 126,\n 'Watchers': 1116}"
},
{
    "title": "No title",
    "content": "If you want to learn to code, I've released 2,000 free sign ups for my course following my Automate the Boring Stuff with Python book (each has 1,000 sign ups, use the other one if one is sold out):\n  \n    *The sign ups are all used up, but you can still watch all the videos for free. Read below!\n  \nhttps://udemy.com/course/automate/?couponCode=MAY2024FREE\n\nhttps://udemy.com/course/automate/?couponCode=MAY2024FREE2\n\n    If you are reading this after the sign ups are used up, you can always find the first 15 of the course's 50 videos are free on YouTube if you want to preview them. YOU CAN ALSO WATCH THE VIDEOS WITHOUT SIGNING UP FOR THE COURSE. All of the videos on the course webpage have \"preview\" turned on. Scroll down to find and click \"Expand All Sections\" and then click the preview link. You won't have access to the forums and other materials, but you can watch the videos.\n  \nNOTE: Be sure to BUY the course for $0, and not sign up for Udemy's subscription plan. The subscription plan is free for the first seven days and then they charge you. It's selected by default. If you are on a laptop and can't click the BUY checkbox, try shrinking the browser window. Some have reported it works in mobile view.\n\nI'm also working on another Udemy course that follows my recent book \"Beyond the Basic Stuff with Python\". So far I have the first 15 of the planned 56 videos done. You can watch them for free on YouTube.\n\nFrequently Asked Questions: (read this before posting questions)\n  \n\n\n    This course is for beginners and assumes no previous programming experience, but the second half is useful for experienced programmers who want to learn about various third-party Python modules.\n  \n\n\n    If you don't have time to take the course now, that's fine. Signing up gives you lifetime access so you can work on it at your own pace.\n  \n\n\n    This Udemy course covers roughly the same content as the 1st edition book (the book has a little bit more, but all the basics are covered in the online course), which you can read for free online at https://inventwithpython.com\n\n\n\n    The 2nd edition of Automate the Boring Stuff with Python is free online: https://automatetheboringstuff.com/2e/\n\n\n\n    I do plan on updating the Udemy course, but it'll take a while because I have other book projects I'm working on. If you sign up for this Udemy course, you'll get the updated content automatically once I finish it. It won't be a separate course.\n  \n\n\n    It's totally fine to start on the first edition and then read the second edition later. I'll be writing a blog post to guide first edition readers to the parts of the second edition they should read.\n  \n\n\nYou're not too old to learn to code. You don't need to be \"good at math\" to be good at coding.\n\n\n\n    Signing up is the first step. Actually finishing the course is the next. :) There are several ways to get/stay motivated. I suggest getting a \"gym buddy\" to learn with. Check out r/ProgrammingBuddies"
},
{
    "title": "No title",
    "content": "Can you guys suggest some very good book for GUI development in Python?\n  \n    I'm currently working on a visualizer that needs many features to plot data on a 3D and 2D space. Using PyQt for this as it has threading support."
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    I'm making a simple configclass for handling configuration in smaller projects and scripts. Goal is to be as simple to start with as creating a dataclass.\n  \n    The module itself works off dataclass and when you use it you just define a dataclass as normal, but decorate it with @configclass() instead.\n  \n    Example:\n  from configclass import configclass\n\n@configclass()\nclass Settings:\n    foo: bool = False\n    url: str = \"\"\n    footoo: bool = True\n    my_model: str = \"model.pt\"\n\nsetting = Settings.load()\n\nprint(setting.foo, setting.footoo, setting.my_model)\n    From that you got\n  \n\n\n    JSON config file support (config.json)\n  \n\n\n    YAML config file support (config.yaml)\n  \n\n\n    Command line support (argparse)\n  \n\n\n    Env variables support (CONFIG_SETTINGNAME)\n  \n\n\n    It also support nested structures via nested dataclass classes.\n  \nComparison\n\n    It's meant as a quick and lightweight alternative to larger and more comprehensive config systems, for the small programs and scripts where you'd just use a dataclass, and maybe load the values from a config file.\n  \nTarget Audience\n\n    Since it's pretty new and raw I wouldn't recommend it for heavy production settings or complex projects. That said, it should work fine for most cases.\n  \n    While I've worked with python for quite some time, this is the first time I've tried making a package, so I'd like some feedback on the project and it's structure before I push it anywhere. It'd also be nice to stress test it and shake out some bugs.\n  \n    More info and code at https://github.com/TheTerrasque/python-configclass"
},
{
    "title": "No title",
    "content": "https://github.com/Never-Over/tach\n\nWhat My Project Does\n\ntach is a lightweight Python tool that enforces boundaries and dependencies in your Python project. Inspired by nx, tach helps you maintain a decoupled and modular Python codebase.\n  \n    An earlier version of this tool was called modguard, which we shared here.\n  \n    By default, Python allows you to import and use anything, anywhere. Over time, this results in modules that were intended to be separate getting tightly coupled together, and domain boundaries breaking down. We experienced this first-hand at a unicorn startup, where the eng team paused development for over a year in an attempt to split up packages into independent services. This attempt ultimately failed.\n  \n    This problem occurs because:\n  \n\n\n    It's much easier to add to an existing package rather than create a new one\n  \n\n\n    Junior devs have a limited understanding of the existing architecture\n  \n\n\n    External pressure leading to shortcuts and overlooking best practices\n  \n\n\n    Efforts we've seen to fix this problem always came up short. A patchwork of solutions would attempt to solve this from different angles, such as developer education, CODEOWNERs, standard guides, refactors, and more. However, none of these addressed the root cause.\n  \n    With tach, you can:\n  \n\n\n    Declare your packages (package.yml)\n  \n\n\n    Define dependencies between packages (tach.yml)\n  \n\n\n    Enforce those dependencies (tach check)\n  \n\n\n    You can also enforce a strict interface for each package. This means that only imports that are directly listed in __init__.py can be imported by other packages.\n  \ntach is:\n  \n\n\n    fully open source\n  \n\n\n    able to be adopted incrementally (tach init and tach add)\n  \n\n\n    implemented with no runtime footprint\n  \n\n\n    interoperable with your existing tooling\n  \n\n\n    We hope you give it a try! We'd love any feedback.\n  \nGitHub\n\nTarget Audience\n\n    Python developers who want to maintain quality while shipping quickly\n  \nComparison\n\n    This tool is an evolution of a tool we previously built, modguard. It's very similar to nx's module boundaries tool, although they don't support Python."
},
{
    "title": "No title",
    "content": "Approaching a year into my journey of learning Python (I come from a C background) I finally understand the beauty and ubiquity of the langauge. Initially, I was hesitant to learn a dynamically typed and interpreted language when job requirements required me to learn it (I help support a team of Power Systems Engineers, and our professional software uses python for scripting). My first attempts were extremely unpythonic and it felt like I was fighting the language (index based looping, declaring variables before use, C style procedural code on top of Pandas antipatterns) and the lack of brackets I found appalling. Then I had my first code review with a helpful Senior engineer. We refactored my code together and something beautiful came together. He then told me to read the Zen of Python. It was love at first \"import this\". I was hooked. Every waking moment for weeks I was learning all I could muster about python.\n  \n    Now later on in my journey, and having written several complex systems in python over the course of the past 6 months, it is truly incredible what you can accomplish with just the standard library. Need a temporary file? There is a module for that. Want to serialize data? Multiple modules just for that purpose. Embarrassingly parallel basic scripting tasks? 3 lines of code later and it is now multiprocessed. The list goes on and on.\n  \n    Archimedes once famously said \"Give me a lever long enough and a fulcrum on which to place it, and I shall move the world\". With an understanding of all that is available out of the box as the fulcrum so to speak, it is amazing the breadth of problems that the standard library proves to be a long enough lever. On our compute cluster, we try to keep dependencies to an absolute minimum. Sticking to the standard library, Pandas, and the provided API for the software with a clever enough implementation rarely if ever feels like a limitation.\n  \n    So next time you feel the need to pip install something new, be sure to check the Python documentation. You just might already have all the tools you need already at your disposal!"
},
{
    "title": "No title",
    "content": "Hey all! I recently made a library for sending text files over Discord DMs\n  \nhttps://pypi.org/project/ext-message/\n\nhttps://github.com/jwjeffr/ext-message/\n\nWhat my project does\n\n    This code is a command-line interface for sending text files to Discord DMs from a bot.\n  \nTarget audience\n\n    My personal use-case is for my research, where I run long simulations (~72 hours) on a computing cluster. These simulations generate log files, so I can send myself the log file after the simulation, letting me:\n  \n    - Know that the simulation is done\n  \n    - Check that the simulation ran as intended\n  \n    without having to SSH into the cluster. As of now, those running similarly heavy code looking to be notified when it finishes is my target audience, but I would love to hear other creative use cases to expand that audience!\n  \nComparison\n\n    Not sure what's out there in terms of other libraries for doing similar things. This was partially for production and partially a toy project forcing me to become more familiar with discord.py"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What My Project Does: Allows users to run a pytest test from a notebook or other REPL and capture local variables for inspection / debugging. While I think users should very often prefer debugging tests with a debugger, in certain situations where one deals with a nontrivial amount of data or many complex objects, being able to inspect and manipulate objects in a notebook can be helpful.\n  \nTarget Audience: Primarily data scientists, but more generally anyone who tests with large/complex objects that can be difficult to understand in a debugger.\n  \nComparison: I'm not aware of any alternatives, other than copy-and-pasting your pytest tests in a notebook.\n  \nRepo with examples: https://github.com/rhshadrach/pytest-ndb"
},
{
    "title": "No title",
    "content": "https://www.hindustantimes.com/business/google-layoffs-sundar-pichai-led-company-fires-entire-python-team-for-cheaper-labour-101714379453603.html\n\n    Are there any ramifications for the Python community outside of  Google?"
},
{
    "title": "No title",
    "content": "https://github.com/dalleyg/pyrseus\n\nWhat My Project Does\n\n    Pyrseus extends Python’s concurrent.futures asynchronous and concurrent programming package with:\n  \n\n\n    a collection of non-concurrent executors for light workloads and troubleshooting,\n  \n\n\n    ExecutorCtx, a factory for easily switching between different executors, and\n  \n\n\n    a collection of ready-built ExecutorCtx plugins, supporting executors from concurrent.futures, ipyparallel, loky, mpi4py, and itself. Where relevant, optional cloudpickle-enhanced plugins are also provided.\n  \n\n\n    Full documentation is available at https://pyrseus.readthedocs.io/en/latest/.\n  \nTarget Audience\n\n    Python developers who already use concurrent.futures that want at least one of the following:\n  \n\n\n    an easier way to troubleshoot problems with their tasks without having to rewrite the control code,\n  \n\n\n    an easier way to switch between concurrent and non-concurrent execution without having to rewrite the control code, and/or\n  \n\n\n    an easier way to dynamically change what executor class is used at runtime.\n  \n\n\nComparison\n\n\n\n    InlineExecutor is similar to various open source classes that already exist, but almost always as internal implementation details in much larger projects.\n  \n\n\n    NoCatchExecutor is a less common class in other libraries, since it intentionally has non-standard error handling. We include it because it can be useful in situations where one wants true fail-fast behavior.\n  \n\n\n    We're not aware of the other pickle-testing serial executors existing elsewhere.\n  \n\n\n    ExecutorCtx ties together the executors provided by Pyrseus. We're not aware of any open source libraries that support (a) both serial and concurrent executors, and (b) do so as a standalone package instead of as part of a much larger package.\n  \n\n\n    It's worth giving special mention to the loky project. It provides (a) a better version of concurrent.futures.ProcessPoolExecutor and (b) a reusable executor factory. Pyrseus can use loky's executors as backends, if it's installed. Our package extends loky by making its reusable executors usable as context managers. We also supply non-concurrent executors and make it easy to switch to them at runtime, as discussed above.\n  \n\n\n    See the notebooks and the plugins list at https://pyrseus.readthedocs.io/en/latest/ for more details."
},
{
    "title": "No title",
    "content": "are there any groups, meets, events etc i can get involved with locally in london uk?\n  \n    i've got moderate python chops as well as a wide range of other skills and tools, notably construction and event organisation/production/management, but also 3d design and print, some electronics and others.\n  \n    i work flexible hours and honestly am looking to develop a network and experiences to land my first job that at least involves some coding - i'm happy to commit significant time and resource to interesting projects to get eventually my foot or at least my nose in a door somewhere. (i dont mean i would stop contributing to the project once i land a job!)\n  \n    thanks!"
},
{
    "title": "No title",
    "content": "Source Code: https://github.com/dhilipsiva/py-compress-compare\nAnalyzing Python Compression Libraries: zlib, LZ4, Brotli, and Zstandard\n    When dealing with large volumes of data, compression can be a critical factor in enhancing performance, reducing storage costs, and speeding up network transfers. In this blog post, we will dive into a comparison of four popular Python compression libraries—zlib, LZ4, Brotli, and Zstandard—using a real-world dataset to evaluate their performance in terms of compression ratio and time efficiency.\n  The Experiment Setup\n    Our test involved a dataset roughly 581 KB in size, named sample_data.json. We executed compression and decompression using each library as follows:\n  \n\n\n    Compression was performed 1000 times.\n  \n\n\n    Decompression was repeated 10,000 times.\n  \n\n\n    This rigorous testing framework ensures that we obtain a solid understanding of each library's performance under heavy load.\n  Compression Ratio\n    The compression ratio is a key metric that represents how effectively a compression algorithm can reduce the size of the input data. Here’s how each library scored:\n  \n\n\n    Zlib achieved a compression ratio of 27.84,\n  \n\n\n    LZ4 came in at 18.23,\n  \n\n\n    Brotli impressed with a ratio of 64.78,\n  \n\n\n    Zstandard offered a ratio of 43.42.\n  \n\n\n    From these results, Brotli leads with the highest compression ratio, indicating its superior efficiency in data size reduction. Zstandard also shows strong performance, while LZ4, though lower, still provides a reasonable reduction.\n  Compression Time\n    Efficiency isn't just about space savings; time is equally crucial. Here’s how long each library took to compress the data:\n  \n\n\n    Zlib: 7.34 seconds,\n  \n\n\n    LZ4: 0.13 seconds,\n  \n\n\n    Brotli: 204.18 seconds,\n  \n\n\n    Zstandard: 0.15 seconds.\n  \n\n\n    LZ4 and Zstandard excel in speed, with LZ4 being slightly faster. Zlib offers a middle ground, but Brotli, despite its high compression efficiency, takes significantly longer, which could be a drawback for real-time applications.\n  Decompression Time\n    Decompression time is vital for applications where data needs to be rapidly restored to its original state:\n  \n\n\n    Zlib: 11.99 seconds,\n  \n\n\n    LZ4: 0.46 seconds,\n  \n\n\n    Brotli: 0.99 seconds,\n  \n\n\n    Zstandard: 0.46 seconds.\n  \n\n\n    Again, LZ4 and Zstandard show excellent performance, both under half a second. Brotli presents a decent time despite its lengthy compression time, while zlib lags behind in this aspect.\n  Conclusion\n    Each library has its strengths and weaknesses:\n  \n\n\n    Brotli is your go-to for maximum compression but at the cost of time, making it suitable for applications where compression time is less critical.\n  \n\n\n    Zstandard offers a great balance between compression ratio and speed, recommended for a wide range of applications.\n  \n\n\n    LZ4 shines in speed, ideal for scenarios requiring rapid data processing.\n  \n\n\n    Zlib provides moderate performance across the board.\n  \n\n\n    Choosing the right library depends on your specific needs, whether it’s speed, space, or a balance of both. This experiment provides a clear picture of what to expect from these libraries, helping you make an informed decision based on your application's requirements."
},
{
    "title": "No title",
    "content": "Hi!\n  \n    I've just sorted through some of my books on Leanpub and decided to make the book on Python Regular Expressions free to download for a couple of days.\n  \n    Why? Just for fun and learning and to get some valuable feedback.\n  \n\n\n\n    Free ebook link: https://leanpub.com/regexpython/\n\n\n\n    Regex Video Tutorials: https://blog.finxter.com/python-regex/\n\n\n\n\n    This book helps you learn regular expressions chapter by chapter. Each chapter first teaches the concepts and then asks you to solve Python puzzles (\"Guess the code!\") to master the concepts. The main idea is to keep it lightweight and fun.\n  \n\nHere's the ToC:\n\n\n\n    Introduction\n  \n\n\n    Applications\n  \n\n\n    About\n  \n\n\n    Puzzle Learning\n  \n\n\n    Basics\n  \n\n\n    Special Symbols\n  \n\n\n    Character Sets\n  \n\n\n    Dot Regex\n  \n\n\n    Asterisk Quantifier\n  \n\n\n    Plus Quantifier\n  \n\n\n    ? Quantifier\n  \n\n\n    Quantifier Differentiation\n  \n\n\n    Greediness\n  \n\n\n    Line and String Boundaries\n  \n\n\n    OR Regex\n  \n\n\n    AND Regex\n  \n\n\n    NOT Regex\n  \n\n\n    Matching Groups\n  \n\n\n    Split Method\n  \n\n\n    Substitution Method\n  \n\n\n    Compile Method\n  \n\n\n    Bonus Puzzles\n  \n\n\n    Final Remarks\n  \n\n\n\n    I hope you enjoy learning from the book! Feel free to send me your feedback.\n  \n    Happy learning \"R[e-x]+\"! 🤓"
},
{
    "title": "No title",
    "content": "The primary vector for malicious code running in software developer environments (e.g., local system, CI/CD runners, production servers, etc.) is software dependencies. This is third-party code which often means open-source software, also known as running code from strangers on the internet.\n  \n    The prized goal for attackers is arbitrary code execution. It’s the stuff high CVE scores are made of and often the topic of how vulnerabilities can turn into exploits. It’s the foothold needed to run cryptominers, steal secrets, or encrypt data for ransom. It’s no wonder why threat actors want it, but how do they get it? Sutton’s Law makes it obvious why they go after open-source software: because executing arbitrary code is easy there.\n  \n    This is a series examining the methods malicious Python code gains execution. Some of the methods are obvious and some are potentially undiscovered or at least not found in the wild, yet. What they all mostly have in common is the reliance on a software dependency in the form of a Python package, which is where we begin.\n  \nhttps://blog.phylum.io/how-malicious-python-code-gains-execution/"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hi folks!\n  \n    I am currently looking for a US (west coast best) based job where I could grow as a python programmer, specifically analysing medical images.\n  \n    Looking online I found some labs where they work with images, but I thought I'd ask (not sure if it's the right sub tho) cause I'm sure this way I'll hear about many places that deal with Medical Imaging.\n  \n    Thanks :)"
},
{
    "title": "No title",
    "content": "Source code: https://github.com/ben-n93/tab-palWhat my Project DoesMakes it quicker, easier and prettier to create custom colour palettes in Tableau.\n  \n    Rather than having to find your Preferences.tps file every time and edit the XML you can just launch tab-pal from the command-line and make any changes you want (adding/deleting new palettes, adding/removing colours.\n  \n    You also get a preview of the colours you've added.\n  \nTarget AudienceData/business analysts and data scientists and who work with Tableau.ComparisonThere are some websites that let you dynamically create the Preferences.tps XML file however this still requires you to locate the file and copy and paste  over it with any changes you make"
},
{
    "title": "No title",
    "content": "Link to GitHub Page\nWhat My Project Does\n    I created a simple financial analysis tool using the SimFin API and the GPT API. This tool summarizes financial information and makes it digestable for people who don't know a lot about finance.\n  Target Audience\n    This is more of a toy project for those interested in AI and Finance. It's meant for people without a ton of GPT experience to see a tangible use-case of generative AI within the field.\n  Comparison\n    Even though this is a simple tool, I don't know of any existing alternative.\n  Example\n    Input:\n  \n    Analyze Apple's q1 2023 earnings\n  \n    Output:\n  \n    AI Assistant:  Based on the provided financial information, here is a summary of the company's finances:\n  \n\n\n    Balance Sheet:- Total Assets: $346,747,000,000- Total Liabilities: $290,020,000,000- Total Equity: $56,727,000,000The company has a strong balance sheet with a significant amount of assets compared to its liabilities. This indicates a healthy financial position.\n  \n\n\n    Cash Flow:- Net Cash from Operating Activities: $34,005,000,000- Net Cash from Investing Activities: -$1,445,000,000- Net Cash from Financing Activities: -$35,563,000,000- Net Change in Cash: -$3,003,000,000The company generated positive cash flow from its operating activities, but had negative cash flow from investing and financing activities. As a result, there was a decrease in cash during the period.\n  \n\n\n    Profitability Metrics:- Gross Profit Margin: 42.96%- Operating Margin: 30.74%- Net Profit Margin: 25.61%- Return on Equity: 52.88%- Return on Assets: 8.65%- Return on Invested Capital: 15.28%The company has healthy profitability metrics, indicating efficient operations and good returns on investment.\n  \n\n\n    Liquidity Metrics:- Current Ratio: 1.01The company has a current ratio slightly above 1, which suggests it has enough current assets to cover its short-term liabilities. However, it is important to note that a current ratio of exactly 1 may indicate limited liquidity.\n  \n\n\n    Solvency Metrics:- Liabilities to Equity Ratio: 5.11- Debt Ratio: 32.04%The company has a relatively high liabilities to equity ratio, indicating a significant amount of debt compared to equity. The debt ratio is moderate, suggesting that a significant portion of the company's assets is financed by debt.\n  \n\n\n    Cash Flow Metrics:- Free Cash Flow: $39,273,000,000- Free Cash Flow to Net Income: 1.31- Cash Return on Invested Capital: 17.77%The company has positive free cash flow, indicating its ability to generate cash after accounting for capital expenditures. The free cash flow to net income ratio suggests that the company is efficient in converting its net income into free cash flow. The cash return on invested capital is also positive, indicating good returns on the capital invested.\n  \n\n\n    Other Important Metrics:- Piotroski F-Score: 4- Net Debt / EBITDA: 1.53- Dividend Payout Ratio: 12.56%The Piotroski F-Score of 4 suggests that the company has a moderate financial strength. The net debt to EBITDA ratio indicates the company's ability to repay its debt, with a ratio of 1.53. The dividend payout ratio suggests that the company distributes a portion of its earnings as dividends.Based on the provided information, the company appears to be in a strong financial position with healthy profitability metrics and positive cash flow. However, it is important to conduct further research and analysis to fully understand the company's financial health and prospects.\n  \n\n\n    Feel free to fork it, make PRs for it,or leave feedback!"
},
{
    "title": "No title",
    "content": "source: https://github.com/ObaraEmmanuel/Formation\n\n    pypi: https://pypi.org/project/formation-studio/\n\nWhat My Project Does\n\n    Allows you to inspect widgets in your running Tk app in real-time. You can view the widget hierarchy, modify widget attributes, adjust widget layout and run arbitrary code to interact with your widgets through the embedded Python REPL console. It works just like DevTools in a browser. This debugger is part of the Formation studio project which is a drag-n-drop graphical UI builder for Tkinter.\n  \nTarget Audience\n\n    Any Tk developer seeking to have an easier time debugging their UI or seeking to experiment with the Tk framework with minimal effort.\n  \nComparison\n\n    There is no project currently doing this same thing.\n  \nUsage\n\n    It comes bundled with Formation Studio so the installation is as simple as\n  pip install formation-studio\n    You don't have to change anything in your code. Simply use the following command and the debugger will attach itself to your app:\n  formation-dbg /path/to/your/tk/app.py\n    In the embedded python REPL console you can access a simple debugger API as follows:\n  # Access a list of all widgets currently selected\nwidgets = debugger.selection\n\n# Access the root widget usually a Tk object\nroot = debugger.root"
},
{
    "title": "No title",
    "content": "Source Code: https://github.com/JeanExtreme002/FastSnake\n\n    PyPI: https://pypi.org/project/FastSnake/\n\nWhat My Project Does\n\n    FastSnake is a command-line tool that allows you to easily create, expand, run, and test Python solutions for competitive programming problems. 🐍🏁\n  \n    This project provides useful CLI tools for competitive programming, such as test case generators, algorithms and data structures, tools for platforms Codeforces and AtCoder, and other features that assist you during the development and testing of solutions, besides building a nice directory structure to develop your solutions. 🤓\n  \nTarget Audience\n\n    This project was developed for programming competitors, focusing on users of CodeForces and AtCoder, who need to quickly develop solutions for complex problems\n  \nComparison\n\n    There is no project currently doing this same thing.\n  \nHow to Install\n\n    Just type the following commands:\n  $ pip install fastsnake\n$ fastsnake -v\nDocumentation\n\n    Explore the documentation of FastSnake package at repository's project."
},
{
    "title": "No title",
    "content": "Hey everyone,\n  \n    I am creating \"Space Science with Python\" tutorials on YouTube with free and open accessible Python code on GitHub. It is not fancy or a \"super high animation quality YouTube production\". I am just an astrophysicists (what a difficult word to write), working in industry who continues with science as a hobby. Ha! And I have some crazy niche knowledge that is not covered by Gemini or chatGPT (yet).\n  \n    Anyway, I am creating now a tutorial on meteor science and wanted to show you how to create a coordinate system that co-rotates with the Earth while it is revolving around the Sun.\n  \nWhy is this interesting?\n\n    Well, you may have heard about certain meteor streams like the Perseids in August or the Geminids in December. Dedicated streams that are associated with e.g. a particular comet or asteroid. However there are meteors that appear \"random\". So called sporadics. These sporadics have certain source regions, like e.g. the Apex.\n  \nCool, what the heck is an Apex?\n\n    The Apex is Earth's \"flight direction\" in the Solar System. Imagine viewing the Sun and Earth from top of the Solar System: Apex is rotating with the Earth. Thus, it is not a fixed coordinate system. Take this image from Sky & Telescope that helps you imaging this stuff.\n  \n    Now the resulting regions of interests can be seen on this NASA page. In a sky map / plot you see different sources.\n  \n    ...but wouldn't it be cool to do it yourself? With your own data and Python code?\n\n    And that's where I try to jump in. Check out the code and the corresponding video.\n  \n    I am looking forward to any feedback / comment"
},
{
    "title": "No title",
    "content": "Hi, I'm a cybersecurity enthusiastic. And I've made a web crawler/scraper tool to extract links and sensitive information against target websites. You can find it here: https://github.com/PadishahIII/SecretScraper.\n  \nWhat My Project Does SecretScraper is a highly configurable web scraper tool that crawls links, extracts subdomains from target websites and finds sensitive data using regular expressions. The features included in the SecretScraper are:\n  \n\n\n    Web crawler: extract links using both DOM hierarchy and regex\n  \n\n\n    Support for domain whitelist and blacklist\n  \n\n\n    Support multiple targets, enter target URLs from a file\n  \n\n\n    Scalable customisation: header, proxy, timeout, cookie, scrape depth, follow redirect, etc.\n  \n\n\n    Built-in regex to search for sensitive information: hyperscan is employed for higher performance\n  \n\n\n    Flexible configuration in yaml format\n  \n\n\nTarget Audience SecretScraper is made for penetration tester or web developer who can use this tool for info-gathering and finding any sensitive data or route of any website.\n  \nComparison A similar project is LinkFinder, an awesome python script written to discover endpoints and their parameters in JavaScript files. But I was expecting a project with more general use and more functionality. So I am developing this project half for practice and half with the intension of integrating it in a larger design.\n  \nUse Case There is full documentation available in Github: https://github.com/PadishahIII/SecretScraper. Simply install via pip install secretscraper and see secretscraper --help."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "While it's unfortunate to have two constructs—TypeGuard and TypeIs—with slightly different behaviors, I'm glad that the latter is less surprising.\n  \nhttps://rednafi.com/python/typeguard_vs_typeis/"
},
{
    "title": "No title",
    "content": "GitHub\nWhat My Project Does\nPkgInspect is a comprehensive tool designed to inspect and compare Python packages and Python versions effortlessly. It equips users with a comprehensive set of tools and utility classes to retrieve essential information from installed Python packages, compare versions seamlessly, and extract various details about Python installations with ease.\n  Target Audience\n    Developers and Python enthusiasts looking to streamline the process of inspecting Python packages, comparing versions, and extracting vital information from Python installations will find PkgInspect invaluable. Many current modules such as importlib_metadata and pkg_resources are fairly limited on what items can be inspected and retrieved for a specified python package. Also noticed pkg_resources has also deprecated some of its important retrieval methods.\n  Comparison\nPkgInspect stands out from other Python package inspection tools due to its robust features. Unlike traditional methods that require manual inspection and comparison, PkgInspect automates the process, saving developers valuable time and effort. With PkgInspect, you can effortlessly retrieve package information, compare versions across different Python installations, and extract crucial details with just a few simple commands.\n  Key Features\n\n\nInspect Packages: Retrieve comprehensive information about installed Python packages.\n  \n\n\nCompare Versions: Seamlessly compare package data across different Python versions.\n  \n\n\nRetrieve Installed Pythons: Identify and list installed Python versions effortlessly.\n  \n\n\nInspect PyPI Packages: Gather detailed information about packages from the Python Package Index (PyPI).\n  \n\n\nFetch Available Updates: Stay up-to-date with available updates for a package from the current version.\n  \n\n\nList Inspection Fieldnames: Access a list of available fieldnames for package inspection.\n  \n\n\nRetrieve Package Metrics: Extract OS statistics about a package effortlessly.\n  \n\n\nFetch GitHub Statistics: Retrieve insightful statistics about a package from GitHub effortlessly.\n  \n\n\nRetrieve all Python Packages: Easily list all installed Python packages for a given Python version.\n  \n\n\n    Main Components\n  \nCore Modules\n\n\n\nPkgInspect: Inspects Python packages and retrieves package information.\n  \n\n\nPkgVersions: Retrieves and compares package data across different Python versions.\n  \n\n\nPkgMetrics: Extracts OS statistics about a package.\n  \n\n\nFunctions\n\n\n\ninspect_package: Inspects a Python package and retrieves package information.\n  \n\n\ninspect_pypi: Inspects a package from the Python Package Index (PyPI).\n  \n\n\nget_available_updates: Fetches available updates for a package from the current version.\n  \n\n\nget_installed_pythons: Identifies and lists installed Python versions.\n  \n\n\nget_version_packages: Lists all installed Python packages for a given Python version.\n  \n\n\npkg_version_compare: Compares package data across different Python versions.\n  \n\nInspection Field Options\nAny other field name will be treated as a file name to inspect from the packages' site-path directory.\n- `short_meta` (dict[str, Any]): Returns a dictionary of the most important metadata fields.\n    - If only one field is needed, you can use any of the following metadata fields.\n    - Possible Fields instead of `short_meta`:\n        - `Metadata-Version` (PackageVersion)\n        - `Name` (str)\n        - `Summary` (str)\n        - `Author-email` (str)\n        - `Home-page` (str)\n        - `Download-URL` (str)\n        - `Platform(s)` (set)\n        - `Author` (str)\n        - `Classifier(s)` (set)\n        - `Description-Content-Type` (str)\n- `short_license` (str): Returns the name of the license being used.\n- `metadata` (str): Returns the contents of the METADATA file.\n- `installer` (str): Returns the installer tool used for installation.\n- `license` (str): Returns the contents of the LICENSE file.\n- `record` (str): Returns the list of installed files.\n- `wheel` (str): Returns information about the Wheel distribution format.\n- `requested` (str): Returns information about the requested installation.\n- `authors` (str): Returns the contents of the AUTHORS.md file.\n- `entry_points` (str): Returns the contents of the entry_points.txt file.\n- `top_level` (str): Returns the contents of the top_level.txt file.\n- `source_file` (str): Returns the source file path for the specified package.\n- `source_code` (str): Returns the source code contents for the specified package.\n- `doc` (str): Returns the documentation for the specified package.\n\n- `Pkg` Custom Class Fields\n    - `PkgInspect fields`: Possible Fields from the `PkgInspect` class.\n        - `site_path` (Path): Returns the site path of the package.\n        - `package_paths` (Iterable[Path]): Returns the package paths of the package.\n        - `package_versions` (Generator[tuple[str, tuple[tuple[Any, str]]]]): Returns the package versions of the package.\n        - `pyversions` (tuple[Path]): Returns the Python versions of the package.\n        - `installed_pythons` (TupleOfPkgVersions): Returns the installed Python versions of the package.\n        - `site_packages` (Iterable[str]): Returns the site packages of the package.\n        - `islatest_version` (bool): Returns True if the package is the latest version.\n        - `isinstalled_version` (bool): Returns True if the package is the installed version.\n        - `installed_version` (PackageVersion): Returns the installed version of the package.\n        - `available_updates` (TupleOfPkgVersions): Returns the available updates of the package.\n\n    - `PkgVersions fields`: Possible Fields from the `PkgVersions` class.\n        - `initial_version` (PackageVersion): Returns the initial version of the package.\n        - `installed_version` (PackageVersion): Returns the installed version of the package.\n        - `latest_version` (PackageVersion): Returns the latest version of the package.\n        - `total_versions` (int): Returns the total number of versions of the package.\n        - `version_history` (TupleOfPkgVersions): Returns the version history of the specified package.\n        - `package_url`: Returns the URL of the package on PyPI.\n        - `github_stats_url` (str): Returns the GitHub statistics URL of the package.\n        - `github_stats` (dict[str, Any]): Returns the GitHub statistics of the package.\n            - The GitHub statistics are returned as a dictionary \\\n                containing the following fields which can accessed using the `item` parameter:\n                - `Forks` (int): Returns the number of forks on GitHub.\n                - `Stars` (int): Returns the number of stars on GitHub.\n                - `Watchers` (int): Returns the number of watchers on GitHub.\n                - `Contributors` (int): Returns the number of contributors on GitHub.\n                - `Dependencies` (int): Returns the number of dependencies on GitHub.\n                - `Dependent repositories` (int): Returns the number of dependent repositories on GitHub.\n                - `Dependent packages` (int): Returns the number of dependent packages on GitHub.\n                - `Repository size` (NamedTuple): Returns the size of the repository on GitHub.\n                - `SourceRank` (int): Returns the SourceRank of the package on GitHub.\n                - `Total releases` (int): Returns the total number of releases on GitHub.\n    \n    - `PkgMetrics fields`: Possible Fields from the `PkgMetrics` class.\n        - `all_metric_stats` (dict[str, Any]): Returns all the OS statistics of the package.\n        - `total_size` (int): Returns the total size of the package.\n        - `date_installed` (datetime): Returns the date the package was installed.\n\n- `pypistats fields`: Possible Fields from the `pypistats` module.\n    - `all-pypi-stats` (dict[str, Any]): Returns all the statistics of the package on PyPI into a single dictionary.\n    - `stats-overall` (dict[str, Any]): Returns the overall statistics of the package on PyPI.\n    - `stats-major` (dict[str, Any]): Returns the major version statistics of the package on PyPI.\n    - `stats-minor` (dict[str, Any]): Returns the minor version statistics of the package on PyPI.\n    - `stats-recent` (dict[str, Any]): Returns the recent statistics of the package on PyPI.\n    - `stats-system` (dict[str, Any]): Returns the system statistics of the package on PyPI.Downsides & Limitations\nMy algorithms are fairly well but do come with some important downsides.\n\n\n\nPkgInspect will ONLY inspect packages that are python files or contains a dist-info folder in the site-packages folder for a given Python version. Was not able to efficiently figure out a way to retrieve all necessary packages without containing unrelevant folders/files. Some personal packages may be skipped otherwise.\n  \n\n\n    Beta (pre-releases) has not been implemented yet.\n  \n\n\n    As many files may be handled, the runtime may be slow for some users.\n  \n\n\n    The demand for a project like this is not so much in-demand but have noticed many people, including my self, still seeking for a project like this. However, this type of project does seem to exceed my experience level with Python and algorithms (hence the downsides) so not entirely sure how far this project may come in the future. Was hoping for it to be GUI based if possible.\n  \n\nUsage Examplesfrom pkg_inspect import inspect_package\n\ninspect_package(\"pkg_inspect\", itemOrfile=\"initial_version\")\n# Output (Format - DateTimeAndVersion):\n('May 02, 2024', '0.1.0')\n\n\ninspect_package(\"pkg_inspect\", itemOrfile=\"version_history\")\n# Output (Format - tuple[DateTimeAndVersion]):\n(('May 02, 2024', '0.1.2'), ('May 02, 2024', '0.1.1'), ('May 02, 2024', '0.1.0'))\n\n\ninspect_package(\"pkg_inspect\", pyversion=\"3.12\", itemOrfile=\"short_meta\")\n# Output (Format dict[str, Any]):\n{'Author': 'Yousef Abuzahrieh',\n 'Author-email': 'yousefzahrieh17@gmail.com',\n 'Classifiers': {'Development Status 4 Beta',\n                 'Intended Audience Developers',\n                 'License OSI Approved Apache Software License',\n                 'Operating System OS Independent',\n                 'Programming Language Python 3',\n                 'Programming Language Python 3 Only',\n                 'Topic Utilities'},\n 'Description-Content-Type': 'text/markdown',\n 'Download-URL': 'https://github.com/yousefabuz17/PkgInspect.git',\n 'Home-page': 'https://github.com/yousefabuz17/PkgInspect',\n 'License': 'Apache Software License',\n 'Metadata-Version': <Version('2.1')>,\n 'Name': 'pkg-inspect',\n 'Platforms': {'Windows', 'MacOS', 'Linux'},\n 'Summary': 'A comprehensive tools to inspect Python packages and Python '\n            'installations.'}\n\n\ninspect_package(\"pandas\", pyversion=\"3.12\", itemOrfile=\"github_stats\")\n# Output (Format - dict[str, Any]):\n{'Contributors': '1.09K',\n 'Dependencies': 3,\n 'Dependent packages': '41.3K',\n 'Dependent repositories': '38.4K',\n 'Forks': '17.3K',\n 'Repository size': Stats(symbolic='338.000 KB (Kilobytes)', calculated_size=338.0, bytes_size=346112.0),\n 'SourceRank': 32,\n 'Stars': '41.9K',\n 'Total releases': 126,\n 'Watchers': 1116}"
},
{
    "title": "No title",
    "content": "If you want to learn to code, I've released 2,000 free sign ups for my course following my Automate the Boring Stuff with Python book (each has 1,000 sign ups, use the other one if one is sold out):\n  \n    *The sign ups are all used up, but you can still watch all the videos for free. Read below!\n  \nhttps://udemy.com/course/automate/?couponCode=MAY2024FREE\n\nhttps://udemy.com/course/automate/?couponCode=MAY2024FREE2\n\n    If you are reading this after the sign ups are used up, you can always find the first 15 of the course's 50 videos are free on YouTube if you want to preview them. YOU CAN ALSO WATCH THE VIDEOS WITHOUT SIGNING UP FOR THE COURSE. All of the videos on the course webpage have \"preview\" turned on. Scroll down to find and click \"Expand All Sections\" and then click the preview link. You won't have access to the forums and other materials, but you can watch the videos.\n  \nNOTE: Be sure to BUY the course for $0, and not sign up for Udemy's subscription plan. The subscription plan is free for the first seven days and then they charge you. It's selected by default. If you are on a laptop and can't click the BUY checkbox, try shrinking the browser window. Some have reported it works in mobile view.\n\nI'm also working on another Udemy course that follows my recent book \"Beyond the Basic Stuff with Python\". So far I have the first 15 of the planned 56 videos done. You can watch them for free on YouTube.\n\nFrequently Asked Questions: (read this before posting questions)\n  \n\n\n    This course is for beginners and assumes no previous programming experience, but the second half is useful for experienced programmers who want to learn about various third-party Python modules.\n  \n\n\n    If you don't have time to take the course now, that's fine. Signing up gives you lifetime access so you can work on it at your own pace.\n  \n\n\n    This Udemy course covers roughly the same content as the 1st edition book (the book has a little bit more, but all the basics are covered in the online course), which you can read for free online at https://inventwithpython.com\n\n\n\n    The 2nd edition of Automate the Boring Stuff with Python is free online: https://automatetheboringstuff.com/2e/\n\n\n\n    I do plan on updating the Udemy course, but it'll take a while because I have other book projects I'm working on. If you sign up for this Udemy course, you'll get the updated content automatically once I finish it. It won't be a separate course.\n  \n\n\n    It's totally fine to start on the first edition and then read the second edition later. I'll be writing a blog post to guide first edition readers to the parts of the second edition they should read.\n  \n\n\nYou're not too old to learn to code. You don't need to be \"good at math\" to be good at coding.\n\n\n\n    Signing up is the first step. Actually finishing the course is the next. :) There are several ways to get/stay motivated. I suggest getting a \"gym buddy\" to learn with. Check out r/ProgrammingBuddies"
},
{
    "title": "No title",
    "content": "Can you guys suggest some very good book for GUI development in Python?\n  \n    I'm currently working on a visualizer that needs many features to plot data on a 3D and 2D space. Using PyQt for this as it has threading support."
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    I'm making a simple configclass for handling configuration in smaller projects and scripts. Goal is to be as simple to start with as creating a dataclass.\n  \n    The module itself works off dataclass and when you use it you just define a dataclass as normal, but decorate it with @configclass() instead.\n  \n    Example:\n  from configclass import configclass\n\n@configclass()\nclass Settings:\n    foo: bool = False\n    url: str = \"\"\n    footoo: bool = True\n    my_model: str = \"model.pt\"\n\nsetting = Settings.load()\n\nprint(setting.foo, setting.footoo, setting.my_model)\n    From that you got\n  \n\n\n    JSON config file support (config.json)\n  \n\n\n    YAML config file support (config.yaml)\n  \n\n\n    Command line support (argparse)\n  \n\n\n    Env variables support (CONFIG_SETTINGNAME)\n  \n\n\n    It also support nested structures via nested dataclass classes.\n  \nComparison\n\n    It's meant as a quick and lightweight alternative to larger and more comprehensive config systems, for the small programs and scripts where you'd just use a dataclass, and maybe load the values from a config file.\n  \nTarget Audience\n\n    Since it's pretty new and raw I wouldn't recommend it for heavy production settings or complex projects. That said, it should work fine for most cases.\n  \n    While I've worked with python for quite some time, this is the first time I've tried making a package, so I'd like some feedback on the project and it's structure before I push it anywhere. It'd also be nice to stress test it and shake out some bugs.\n  \n    More info and code at https://github.com/TheTerrasque/python-configclass"
},
{
    "title": "No title",
    "content": "https://github.com/Never-Over/tach\n\nWhat My Project Does\n\ntach is a lightweight Python tool that enforces boundaries and dependencies in your Python project. Inspired by nx, tach helps you maintain a decoupled and modular Python codebase.\n  \n    An earlier version of this tool was called modguard, which we shared here.\n  \n    By default, Python allows you to import and use anything, anywhere. Over time, this results in modules that were intended to be separate getting tightly coupled together, and domain boundaries breaking down. We experienced this first-hand at a unicorn startup, where the eng team paused development for over a year in an attempt to split up packages into independent services. This attempt ultimately failed.\n  \n    This problem occurs because:\n  \n\n\n    It's much easier to add to an existing package rather than create a new one\n  \n\n\n    Junior devs have a limited understanding of the existing architecture\n  \n\n\n    External pressure leading to shortcuts and overlooking best practices\n  \n\n\n    Efforts we've seen to fix this problem always came up short. A patchwork of solutions would attempt to solve this from different angles, such as developer education, CODEOWNERs, standard guides, refactors, and more. However, none of these addressed the root cause.\n  \n    With tach, you can:\n  \n\n\n    Declare your packages (package.yml)\n  \n\n\n    Define dependencies between packages (tach.yml)\n  \n\n\n    Enforce those dependencies (tach check)\n  \n\n\n    You can also enforce a strict interface for each package. This means that only imports that are directly listed in __init__.py can be imported by other packages.\n  \ntach is:\n  \n\n\n    fully open source\n  \n\n\n    able to be adopted incrementally (tach init and tach add)\n  \n\n\n    implemented with no runtime footprint\n  \n\n\n    interoperable with your existing tooling\n  \n\n\n    We hope you give it a try! We'd love any feedback.\n  \nGitHub\n\nTarget Audience\n\n    Python developers who want to maintain quality while shipping quickly\n  \nComparison\n\n    This tool is an evolution of a tool we previously built, modguard. It's very similar to nx's module boundaries tool, although they don't support Python."
},
{
    "title": "No title",
    "content": "Approaching a year into my journey of learning Python (I come from a C background) I finally understand the beauty and ubiquity of the langauge. Initially, I was hesitant to learn a dynamically typed and interpreted language when job requirements required me to learn it (I help support a team of Power Systems Engineers, and our professional software uses python for scripting). My first attempts were extremely unpythonic and it felt like I was fighting the language (index based looping, declaring variables before use, C style procedural code on top of Pandas antipatterns) and the lack of brackets I found appalling. Then I had my first code review with a helpful Senior engineer. We refactored my code together and something beautiful came together. He then told me to read the Zen of Python. It was love at first \"import this\". I was hooked. Every waking moment for weeks I was learning all I could muster about python.\n  \n    Now later on in my journey, and having written several complex systems in python over the course of the past 6 months, it is truly incredible what you can accomplish with just the standard library. Need a temporary file? There is a module for that. Want to serialize data? Multiple modules just for that purpose. Embarrassingly parallel basic scripting tasks? 3 lines of code later and it is now multiprocessed. The list goes on and on.\n  \n    Archimedes once famously said \"Give me a lever long enough and a fulcrum on which to place it, and I shall move the world\". With an understanding of all that is available out of the box as the fulcrum so to speak, it is amazing the breadth of problems that the standard library proves to be a long enough lever. On our compute cluster, we try to keep dependencies to an absolute minimum. Sticking to the standard library, Pandas, and the provided API for the software with a clever enough implementation rarely if ever feels like a limitation.\n  \n    So next time you feel the need to pip install something new, be sure to check the Python documentation. You just might already have all the tools you need already at your disposal!"
},
{
    "title": "No title",
    "content": "Hey all! I recently made a library for sending text files over Discord DMs\n  \nhttps://pypi.org/project/ext-message/\n\nhttps://github.com/jwjeffr/ext-message/\n\nWhat my project does\n\n    This code is a command-line interface for sending text files to Discord DMs from a bot.\n  \nTarget audience\n\n    My personal use-case is for my research, where I run long simulations (~72 hours) on a computing cluster. These simulations generate log files, so I can send myself the log file after the simulation, letting me:\n  \n    - Know that the simulation is done\n  \n    - Check that the simulation ran as intended\n  \n    without having to SSH into the cluster. As of now, those running similarly heavy code looking to be notified when it finishes is my target audience, but I would love to hear other creative use cases to expand that audience!\n  \nComparison\n\n    Not sure what's out there in terms of other libraries for doing similar things. This was partially for production and partially a toy project forcing me to become more familiar with discord.py"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What My Project Does: Allows users to run a pytest test from a notebook or other REPL and capture local variables for inspection / debugging. While I think users should very often prefer debugging tests with a debugger, in certain situations where one deals with a nontrivial amount of data or many complex objects, being able to inspect and manipulate objects in a notebook can be helpful.\n  \nTarget Audience: Primarily data scientists, but more generally anyone who tests with large/complex objects that can be difficult to understand in a debugger.\n  \nComparison: I'm not aware of any alternatives, other than copy-and-pasting your pytest tests in a notebook.\n  \nRepo with examples: https://github.com/rhshadrach/pytest-ndb"
},
{
    "title": "No title",
    "content": "https://www.hindustantimes.com/business/google-layoffs-sundar-pichai-led-company-fires-entire-python-team-for-cheaper-labour-101714379453603.html\n\n    Are there any ramifications for the Python community outside of  Google?"
},
{
    "title": "No title",
    "content": "https://github.com/dalleyg/pyrseus\n\nWhat My Project Does\n\n    Pyrseus extends Python’s concurrent.futures asynchronous and concurrent programming package with:\n  \n\n\n    a collection of non-concurrent executors for light workloads and troubleshooting,\n  \n\n\n    ExecutorCtx, a factory for easily switching between different executors, and\n  \n\n\n    a collection of ready-built ExecutorCtx plugins, supporting executors from concurrent.futures, ipyparallel, loky, mpi4py, and itself. Where relevant, optional cloudpickle-enhanced plugins are also provided.\n  \n\n\n    Full documentation is available at https://pyrseus.readthedocs.io/en/latest/.\n  \nTarget Audience\n\n    Python developers who already use concurrent.futures that want at least one of the following:\n  \n\n\n    an easier way to troubleshoot problems with their tasks without having to rewrite the control code,\n  \n\n\n    an easier way to switch between concurrent and non-concurrent execution without having to rewrite the control code, and/or\n  \n\n\n    an easier way to dynamically change what executor class is used at runtime.\n  \n\n\nComparison\n\n\n\n    InlineExecutor is similar to various open source classes that already exist, but almost always as internal implementation details in much larger projects.\n  \n\n\n    NoCatchExecutor is a less common class in other libraries, since it intentionally has non-standard error handling. We include it because it can be useful in situations where one wants true fail-fast behavior.\n  \n\n\n    We're not aware of the other pickle-testing serial executors existing elsewhere.\n  \n\n\n    ExecutorCtx ties together the executors provided by Pyrseus. We're not aware of any open source libraries that support (a) both serial and concurrent executors, and (b) do so as a standalone package instead of as part of a much larger package.\n  \n\n\n    It's worth giving special mention to the loky project. It provides (a) a better version of concurrent.futures.ProcessPoolExecutor and (b) a reusable executor factory. Pyrseus can use loky's executors as backends, if it's installed. Our package extends loky by making its reusable executors usable as context managers. We also supply non-concurrent executors and make it easy to switch to them at runtime, as discussed above.\n  \n\n\n    See the notebooks and the plugins list at https://pyrseus.readthedocs.io/en/latest/ for more details."
},
{
    "title": "No title",
    "content": "are there any groups, meets, events etc i can get involved with locally in london uk?\n  \n    i've got moderate python chops as well as a wide range of other skills and tools, notably construction and event organisation/production/management, but also 3d design and print, some electronics and others.\n  \n    i work flexible hours and honestly am looking to develop a network and experiences to land my first job that at least involves some coding - i'm happy to commit significant time and resource to interesting projects to get eventually my foot or at least my nose in a door somewhere. (i dont mean i would stop contributing to the project once i land a job!)\n  \n    thanks!"
},
{
    "title": "No title",
    "content": "Source Code: https://github.com/dhilipsiva/py-compress-compare\nAnalyzing Python Compression Libraries: zlib, LZ4, Brotli, and Zstandard\n    When dealing with large volumes of data, compression can be a critical factor in enhancing performance, reducing storage costs, and speeding up network transfers. In this blog post, we will dive into a comparison of four popular Python compression libraries—zlib, LZ4, Brotli, and Zstandard—using a real-world dataset to evaluate their performance in terms of compression ratio and time efficiency.\n  The Experiment Setup\n    Our test involved a dataset roughly 581 KB in size, named sample_data.json. We executed compression and decompression using each library as follows:\n  \n\n\n    Compression was performed 1000 times.\n  \n\n\n    Decompression was repeated 10,000 times.\n  \n\n\n    This rigorous testing framework ensures that we obtain a solid understanding of each library's performance under heavy load.\n  Compression Ratio\n    The compression ratio is a key metric that represents how effectively a compression algorithm can reduce the size of the input data. Here’s how each library scored:\n  \n\n\n    Zlib achieved a compression ratio of 27.84,\n  \n\n\n    LZ4 came in at 18.23,\n  \n\n\n    Brotli impressed with a ratio of 64.78,\n  \n\n\n    Zstandard offered a ratio of 43.42.\n  \n\n\n    From these results, Brotli leads with the highest compression ratio, indicating its superior efficiency in data size reduction. Zstandard also shows strong performance, while LZ4, though lower, still provides a reasonable reduction.\n  Compression Time\n    Efficiency isn't just about space savings; time is equally crucial. Here’s how long each library took to compress the data:\n  \n\n\n    Zlib: 7.34 seconds,\n  \n\n\n    LZ4: 0.13 seconds,\n  \n\n\n    Brotli: 204.18 seconds,\n  \n\n\n    Zstandard: 0.15 seconds.\n  \n\n\n    LZ4 and Zstandard excel in speed, with LZ4 being slightly faster. Zlib offers a middle ground, but Brotli, despite its high compression efficiency, takes significantly longer, which could be a drawback for real-time applications.\n  Decompression Time\n    Decompression time is vital for applications where data needs to be rapidly restored to its original state:\n  \n\n\n    Zlib: 11.99 seconds,\n  \n\n\n    LZ4: 0.46 seconds,\n  \n\n\n    Brotli: 0.99 seconds,\n  \n\n\n    Zstandard: 0.46 seconds.\n  \n\n\n    Again, LZ4 and Zstandard show excellent performance, both under half a second. Brotli presents a decent time despite its lengthy compression time, while zlib lags behind in this aspect.\n  Conclusion\n    Each library has its strengths and weaknesses:\n  \n\n\n    Brotli is your go-to for maximum compression but at the cost of time, making it suitable for applications where compression time is less critical.\n  \n\n\n    Zstandard offers a great balance between compression ratio and speed, recommended for a wide range of applications.\n  \n\n\n    LZ4 shines in speed, ideal for scenarios requiring rapid data processing.\n  \n\n\n    Zlib provides moderate performance across the board.\n  \n\n\n    Choosing the right library depends on your specific needs, whether it’s speed, space, or a balance of both. This experiment provides a clear picture of what to expect from these libraries, helping you make an informed decision based on your application's requirements."
},
{
    "title": "No title",
    "content": "Hi!\n  \n    I've just sorted through some of my books on Leanpub and decided to make the book on Python Regular Expressions free to download for a couple of days.\n  \n    Why? Just for fun and learning and to get some valuable feedback.\n  \n\n\n\n    Free ebook link: https://leanpub.com/regexpython/\n\n\n\n    Regex Video Tutorials: https://blog.finxter.com/python-regex/\n\n\n\n\n    This book helps you learn regular expressions chapter by chapter. Each chapter first teaches the concepts and then asks you to solve Python puzzles (\"Guess the code!\") to master the concepts. The main idea is to keep it lightweight and fun.\n  \n\nHere's the ToC:\n\n\n\n    Introduction\n  \n\n\n    Applications\n  \n\n\n    About\n  \n\n\n    Puzzle Learning\n  \n\n\n    Basics\n  \n\n\n    Special Symbols\n  \n\n\n    Character Sets\n  \n\n\n    Dot Regex\n  \n\n\n    Asterisk Quantifier\n  \n\n\n    Plus Quantifier\n  \n\n\n    ? Quantifier\n  \n\n\n    Quantifier Differentiation\n  \n\n\n    Greediness\n  \n\n\n    Line and String Boundaries\n  \n\n\n    OR Regex\n  \n\n\n    AND Regex\n  \n\n\n    NOT Regex\n  \n\n\n    Matching Groups\n  \n\n\n    Split Method\n  \n\n\n    Substitution Method\n  \n\n\n    Compile Method\n  \n\n\n    Bonus Puzzles\n  \n\n\n    Final Remarks\n  \n\n\n\n    I hope you enjoy learning from the book! Feel free to send me your feedback.\n  \n    Happy learning \"R[e-x]+\"! 🤓"
},
{
    "title": "No title",
    "content": "The primary vector for malicious code running in software developer environments (e.g., local system, CI/CD runners, production servers, etc.) is software dependencies. This is third-party code which often means open-source software, also known as running code from strangers on the internet.\n  \n    The prized goal for attackers is arbitrary code execution. It’s the stuff high CVE scores are made of and often the topic of how vulnerabilities can turn into exploits. It’s the foothold needed to run cryptominers, steal secrets, or encrypt data for ransom. It’s no wonder why threat actors want it, but how do they get it? Sutton’s Law makes it obvious why they go after open-source software: because executing arbitrary code is easy there.\n  \n    This is a series examining the methods malicious Python code gains execution. Some of the methods are obvious and some are potentially undiscovered or at least not found in the wild, yet. What they all mostly have in common is the reliance on a software dependency in the form of a Python package, which is where we begin.\n  \nhttps://blog.phylum.io/how-malicious-python-code-gains-execution/"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hi folks!\n  \n    I am currently looking for a US (west coast best) based job where I could grow as a python programmer, specifically analysing medical images.\n  \n    Looking online I found some labs where they work with images, but I thought I'd ask (not sure if it's the right sub tho) cause I'm sure this way I'll hear about many places that deal with Medical Imaging.\n  \n    Thanks :)"
},
{
    "title": "No title",
    "content": "Source code: https://github.com/ben-n93/tab-palWhat my Project DoesMakes it quicker, easier and prettier to create custom colour palettes in Tableau.\n  \n    Rather than having to find your Preferences.tps file every time and edit the XML you can just launch tab-pal from the command-line and make any changes you want (adding/deleting new palettes, adding/removing colours.\n  \n    You also get a preview of the colours you've added.\n  \nTarget AudienceData/business analysts and data scientists and who work with Tableau.ComparisonThere are some websites that let you dynamically create the Preferences.tps XML file however this still requires you to locate the file and copy and paste  over it with any changes you make"
},
{
    "title": "No title",
    "content": "Link to GitHub Page\nWhat My Project Does\n    I created a simple financial analysis tool using the SimFin API and the GPT API. This tool summarizes financial information and makes it digestable for people who don't know a lot about finance.\n  Target Audience\n    This is more of a toy project for those interested in AI and Finance. It's meant for people without a ton of GPT experience to see a tangible use-case of generative AI within the field.\n  Comparison\n    Even though this is a simple tool, I don't know of any existing alternative.\n  Example\n    Input:\n  \n    Analyze Apple's q1 2023 earnings\n  \n    Output:\n  \n    AI Assistant:  Based on the provided financial information, here is a summary of the company's finances:\n  \n\n\n    Balance Sheet:- Total Assets: $346,747,000,000- Total Liabilities: $290,020,000,000- Total Equity: $56,727,000,000The company has a strong balance sheet with a significant amount of assets compared to its liabilities. This indicates a healthy financial position.\n  \n\n\n    Cash Flow:- Net Cash from Operating Activities: $34,005,000,000- Net Cash from Investing Activities: -$1,445,000,000- Net Cash from Financing Activities: -$35,563,000,000- Net Change in Cash: -$3,003,000,000The company generated positive cash flow from its operating activities, but had negative cash flow from investing and financing activities. As a result, there was a decrease in cash during the period.\n  \n\n\n    Profitability Metrics:- Gross Profit Margin: 42.96%- Operating Margin: 30.74%- Net Profit Margin: 25.61%- Return on Equity: 52.88%- Return on Assets: 8.65%- Return on Invested Capital: 15.28%The company has healthy profitability metrics, indicating efficient operations and good returns on investment.\n  \n\n\n    Liquidity Metrics:- Current Ratio: 1.01The company has a current ratio slightly above 1, which suggests it has enough current assets to cover its short-term liabilities. However, it is important to note that a current ratio of exactly 1 may indicate limited liquidity.\n  \n\n\n    Solvency Metrics:- Liabilities to Equity Ratio: 5.11- Debt Ratio: 32.04%The company has a relatively high liabilities to equity ratio, indicating a significant amount of debt compared to equity. The debt ratio is moderate, suggesting that a significant portion of the company's assets is financed by debt.\n  \n\n\n    Cash Flow Metrics:- Free Cash Flow: $39,273,000,000- Free Cash Flow to Net Income: 1.31- Cash Return on Invested Capital: 17.77%The company has positive free cash flow, indicating its ability to generate cash after accounting for capital expenditures. The free cash flow to net income ratio suggests that the company is efficient in converting its net income into free cash flow. The cash return on invested capital is also positive, indicating good returns on the capital invested.\n  \n\n\n    Other Important Metrics:- Piotroski F-Score: 4- Net Debt / EBITDA: 1.53- Dividend Payout Ratio: 12.56%The Piotroski F-Score of 4 suggests that the company has a moderate financial strength. The net debt to EBITDA ratio indicates the company's ability to repay its debt, with a ratio of 1.53. The dividend payout ratio suggests that the company distributes a portion of its earnings as dividends.Based on the provided information, the company appears to be in a strong financial position with healthy profitability metrics and positive cash flow. However, it is important to conduct further research and analysis to fully understand the company's financial health and prospects.\n  \n\n\n    Feel free to fork it, make PRs for it,or leave feedback!"
},
{
    "title": "No title",
    "content": "source: https://github.com/ObaraEmmanuel/Formation\n\n    pypi: https://pypi.org/project/formation-studio/\n\nWhat My Project Does\n\n    Allows you to inspect widgets in your running Tk app in real-time. You can view the widget hierarchy, modify widget attributes, adjust widget layout and run arbitrary code to interact with your widgets through the embedded Python REPL console. It works just like DevTools in a browser. This debugger is part of the Formation studio project which is a drag-n-drop graphical UI builder for Tkinter.\n  \nTarget Audience\n\n    Any Tk developer seeking to have an easier time debugging their UI or seeking to experiment with the Tk framework with minimal effort.\n  \nComparison\n\n    There is no project currently doing this same thing.\n  \nUsage\n\n    It comes bundled with Formation Studio so the installation is as simple as\n  pip install formation-studio\n    You don't have to change anything in your code. Simply use the following command and the debugger will attach itself to your app:\n  formation-dbg /path/to/your/tk/app.py\n    In the embedded python REPL console you can access a simple debugger API as follows:\n  # Access a list of all widgets currently selected\nwidgets = debugger.selection\n\n# Access the root widget usually a Tk object\nroot = debugger.root"
},
{
    "title": "No title",
    "content": "Source Code: https://github.com/JeanExtreme002/FastSnake\n\n    PyPI: https://pypi.org/project/FastSnake/\n\nWhat My Project Does\n\n    FastSnake is a command-line tool that allows you to easily create, expand, run, and test Python solutions for competitive programming problems. 🐍🏁\n  \n    This project provides useful CLI tools for competitive programming, such as test case generators, algorithms and data structures, tools for platforms Codeforces and AtCoder, and other features that assist you during the development and testing of solutions, besides building a nice directory structure to develop your solutions. 🤓\n  \nTarget Audience\n\n    This project was developed for programming competitors, focusing on users of CodeForces and AtCoder, who need to quickly develop solutions for complex problems\n  \nComparison\n\n    There is no project currently doing this same thing.\n  \nHow to Install\n\n    Just type the following commands:\n  $ pip install fastsnake\n$ fastsnake -v\nDocumentation\n\n    Explore the documentation of FastSnake package at repository's project."
},
{
    "title": "No title",
    "content": "Hey everyone,\n  \n    I am creating \"Space Science with Python\" tutorials on YouTube with free and open accessible Python code on GitHub. It is not fancy or a \"super high animation quality YouTube production\". I am just an astrophysicists (what a difficult word to write), working in industry who continues with science as a hobby. Ha! And I have some crazy niche knowledge that is not covered by Gemini or chatGPT (yet).\n  \n    Anyway, I am creating now a tutorial on meteor science and wanted to show you how to create a coordinate system that co-rotates with the Earth while it is revolving around the Sun.\n  \nWhy is this interesting?\n\n    Well, you may have heard about certain meteor streams like the Perseids in August or the Geminids in December. Dedicated streams that are associated with e.g. a particular comet or asteroid. However there are meteors that appear \"random\". So called sporadics. These sporadics have certain source regions, like e.g. the Apex.\n  \nCool, what the heck is an Apex?\n\n    The Apex is Earth's \"flight direction\" in the Solar System. Imagine viewing the Sun and Earth from top of the Solar System: Apex is rotating with the Earth. Thus, it is not a fixed coordinate system. Take this image from Sky & Telescope that helps you imaging this stuff.\n  \n    Now the resulting regions of interests can be seen on this NASA page. In a sky map / plot you see different sources.\n  \n    ...but wouldn't it be cool to do it yourself? With your own data and Python code?\n\n    And that's where I try to jump in. Check out the code and the corresponding video.\n  \n    I am looking forward to any feedback / comment"
},
{
    "title": "No title",
    "content": "Hi, I'm a cybersecurity enthusiastic. And I've made a web crawler/scraper tool to extract links and sensitive information against target websites. You can find it here: https://github.com/PadishahIII/SecretScraper.\n  \nWhat My Project Does SecretScraper is a highly configurable web scraper tool that crawls links, extracts subdomains from target websites and finds sensitive data using regular expressions. The features included in the SecretScraper are:\n  \n\n\n    Web crawler: extract links using both DOM hierarchy and regex\n  \n\n\n    Support for domain whitelist and blacklist\n  \n\n\n    Support multiple targets, enter target URLs from a file\n  \n\n\n    Scalable customisation: header, proxy, timeout, cookie, scrape depth, follow redirect, etc.\n  \n\n\n    Built-in regex to search for sensitive information: hyperscan is employed for higher performance\n  \n\n\n    Flexible configuration in yaml format\n  \n\n\nTarget Audience SecretScraper is made for penetration tester or web developer who can use this tool for info-gathering and finding any sensitive data or route of any website.\n  \nComparison A similar project is LinkFinder, an awesome python script written to discover endpoints and their parameters in JavaScript files. But I was expecting a project with more general use and more functionality. So I am developing this project half for practice and half with the intension of integrating it in a larger design.\n  \nUse Case There is full documentation available in Github: https://github.com/PadishahIII/SecretScraper. Simply install via pip install secretscraper and see secretscraper --help."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "While it's unfortunate to have two constructs—TypeGuard and TypeIs—with slightly different behaviors, I'm glad that the latter is less surprising.\n  \nhttps://rednafi.com/python/typeguard_vs_typeis/"
},
{
    "title": "No title",
    "content": "Pypy has been able to speed up pure python code by a factor of 5 or more for a number of years. The only disadvantage it has is the difficulty in handling C extensions which are very commonly used in practice.\n  \nhttps://peps.python.org/pep-0744 seems to be talking about speed ups of 5-10%. Why are the goals so much more modest than what pypy can already achieve?"
},
{
    "title": "No title",
    "content": "What My Project Does The Physics Engine Called PhysEng, provides an easy to use environment and visualization combo in which to try out different physics or even provide a template to design your own accelleration/velocity fields. Besides the visualization aspect and numpy the basic functions of the Engine are written completely in 100% python. The features included in the Engine are:\n  \n\n\n    Particles, Soft Bodies, Anchor points\n  \n\n\n    Built in Fields: Drag, Uniform Force Fields, Gravity Between Particles, Octree Gravity etc\n  \n\n\n    Make your own: There are standard templates included in the Examples to design your own fields\n  \n\n\n    Springs - Construct Soft Bodies using Springs. (Built in soft bodies: Cloth and ball\n  \n\n\nTarget Audience PhysEng is made for people who just want to try out different simple simulations or want to design their own physics.\n  \nComparison Looking through github I could never really find a simple and easy-to-use library that did not require me to install some weird libraries or that felt like it was hiding some process from me through using packages. This package is a solution to this since everything is written in python nothing is a secret and can be directed easily.\n  \nGet PhysEng There is full documentation available in the Github repo: https://github.com/levi2234/PhysEng"
},
{
    "title": "No title",
    "content": "I've noticed at work that my coworkers and I try out small things in different ways. Small things like if you want to try that adding two datetimes together behaves in the way you expect. Some people use jupyter notebook for this and others run python interactively in a separate command prompt.\n  \n    I usually run debug in whatever IDE I'm using and then letting it stop at the code I'm currently developing and then using the debug console to test things out. Sometimes this means just leaving the debugger at a breakpoint for half an hour while I continue writing code. Is my way of doing it weird or does it have any disadvantages? How do you usually test out things on the go in a good way?"
},
{
    "title": "No title",
    "content": "What My Project Does\narchivefile is a wrapper around tarfile, zipfile, py7zr, and rarfile. The above libraries are excellent when you are dealing with a single archive format but things quickly get annoying when you have a bunch of mixed archives such as .zip, .7z, .cbr, .tar.gz, etc because each library has a slightly different syntax and quirks which you need to deal with. archivefile wraps the common methods from the above libraries to provide a unified interface that takes care of said differences under the hood. However, it's not as powerful as the libraries it wraps due to lack of support for features that are unique to a specific archive format and library.\n  Target audience\n    Anyone who's using python to deal with different archive formats\n  Comparison\n\n\n    ZipFile, TarFile, RarFile, and py7zr - These are libraries that mine wraps since each of them can only deal with a single archive format\n  \n\n\n    shutil - Shutil can only deal with zipfile and tarfile and only allows full packing or full extraction.\n  \n\n\n    patool - Excellent library that deals with wider range of formats than mine but in doing so it provides less granular control over each ArchiveFile falls somewhere between the powerful dedicated library and the far less powerful universal libaries.\n  \n\nLinks\n    Repository: https://github.com/Ravencentric/archivefile Docs: https://ravencentric.github.io/archivefile"
},
{
    "title": "No title",
    "content": "What My Project Does Shoots is essentially a \"data lake\" where you can easily store pandas dataframes, and retrieve them later or from different locations or in different tools. Shoots has a client and a server. After choosing a place to run the server, you can easily use the client to \"put\" and \"get\" dataframes. Shoots supports SQL, allowing you to put very large dataframes, and then use a query to only get a subset. Shoots also allows you to resample on the server.\n  # put a dataframe, uploads it to the server  \ndf = pd.read\\_csv('sensor\\_data.csv')  \nshoots.put(\"sensor\\_data\", dataframe=df, mode=PutMode.REPLACE)  \n\n\n# retrieve the whole data frame  \ndf0 = shoots.get(\"sensor\\_data\")  \nprint(df0)  \n\n# or use sql to retrieve just some of the data  \nsql = 'select \"Sensor\\_1\" from sensor\\_data where \"Sensor\\_2\" < .2'  \ndf1 = shoots.get(\"sensor\\_data\", sql=sql)\nTarget Audience Shoots is designed to be used in production by data scientists and other python devs using pandas. The server is configurable to run in various settings, including locally on a laptop if desired. It is useful for anyone who wants to share dataframes, or store dataframes so they can be easily accessed from different sources.\n  \nComparison To my knowledge, Shoots is the only data lake with a client that is 100% pandas native. The get() method returns pandas dataframes natively, so there is no cumbersome translations such as required from typical databases and data lakes. The server is build on top of Apache Arrow Flight, and is very efficient with storage because it uses Parquet as the storage format natively. While the Shoots client does all of the heavy listing, if desired, the server can be accessed with any Apache Flight client library, so other languages are supported by the server.\n  \nGet Shoots There is full documentation available in the Github repo: https://github.com/rickspencer3/shoots\n\n    It is packaged for Pypi as well: (https://pypi.org/project/shoots/) ```pip install shoots\""
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hello ,\n  \n    I'm excited to share a project that was initially intended to use in my graduating product(Capstone)\n  \nWhat My Proeject Does\n\n    I made NLLB-200 Distill 350M model to translating English to Korean\n  \nTarget Audience\n\n    GPU servers are quite expensive, so I made it for university students who can't cost the server (like me.)\n  \nComparison\n\n    It's even smaller and faster the other NLLB-200 model. so it can be run with CPU!\n  \n    more details are in my page\n  \n    If you know Korean, please give me a lot of feedback\n  \nhttps://github.com/newfull5/NLLB-200-Distilled-350M-en-ko\n\n    thank you!!"
},
{
    "title": "No title",
    "content": "Hi\n  \n    I've made a package called mpl_ascii which is a backend for matplotlib. You can find it here: https://github.com/chriscave/mpl_ascii\n\n    I would love to share it with others and see what you guys think\n  \nWhat it is\n\n    It is a backend for matplotlib that converts your plots into ASCII characters.\n  \n    At the moment I have only made support for: bar charts, scatter plots and line plots but if there's demand for more then I would love to keep working on it.\n  \nTarget Audience:\n\n    Anyone using matplotlib to create plots who might also want to track how their plots change with their codebase (i.e. version control).\n  \nComparison:\n\n    There are a few plotting libraries that produce ASCII plots but I have only come across this one that is a backend for matplotlib: https://github.com/gooofy/drawilleplot. I think it's a great package and it is really clever code but I found it a little lacking when you have multiple colours in a plot. Let me know if you know of other matploblib backends that does similar things.\n  \nUse case:\n\n    A use case I can think of is for version controlling your plots. Having your plot as a txt format means it can be much easier to see the diff and the files you are committing are much smaller.\n  \n    Since it is only a backend to matplotlib then you only need to switch to it and you don't need to recreate your plots in a different plotting library.\n  \n    Thanks for reading and let me know what you think! :)"
},
{
    "title": "No title",
    "content": "There is no shebang line that actually works across platforms for python 3.\n  \n    I would like one that works on unmodified :\n  \n\n\n    Debian shell (Dropped python2, falls under PEP 394)\n  \n\n\n    Older Linux shells that still have python pointing to python2 (PEP 394)\n  \n\n\n    Windows cmd.exe shell (this really just means one that will work with PEP 397)\n  \n\n\n    Gitbash for Windows (sort of a weird half sibling that respects shebangs)\n  \n\n\n    The best work around I have found is:\n  \n\n\n    use #!/usr/bin/env python3\n\n\n\n    on Windows copy python.exe to python3.exe\n\n\n\n    Then make sure both are in your path for unix-like shells.\n  \n\n\n\n\n    Debian make sure python-is-python2 or python-is-python3 is installed, in case you come upon a #!/usr/bin/env python.\n  \n\n\n    As Windows adopts more and more Unix-like behavior, and more distros drop python2, having completely different \"portability\" rules is going to become a larger problem.\n  \n    A significant compatibility enhancement would be if the official python packages for Windows just included a python3.exe to comply with PEP 394. This could be a copy of python.exe like my workaround, or one could be a minimal executable that just hands off to the other or to py.\n  \n    An alternative would be adding py and pyw from PEP 397 to PEP 394. and having people move to the shebang #!/usr/bin/env py -3.\n  \n    The belt and suspenders compatibility approach is all platforms should have a py, pyw, and python3 executable that can launch python3 scripts if requested. And python should be an executable than runs some version of python.\n  \n    I am curious what others are using out there? Do others launch python scripts from inside gitbash? do you have a seperate window for running the script and git actions? Are you manually choosing the python executable on the command line?"
},
{
    "title": "No title",
    "content": "Hello wonderful community,\n  \n    Today I'll present to you pyaair, a scraper made pure on Python https://github.com/johnbalvin/pyaair\n\n    Easy instalation\n  \n    ` ` `pip install pyaair ` ` `\n  \n    Easy Usage\n  \n    ` ` ` airports=pyaair.airports(\"miami\",\"\") ` ` `\n  \n    Always remember, only use selenium, puppeteer, playwright etc when it's strictly necesary\n  \n    Let me know what you think,\n  \n    thanks\n  \n    About me:\n  \n    I'm full stack developer specialized on web scraping and backend, with 6-7 years of experience"
},
{
    "title": "No title",
    "content": "Link: https://github.com/Axorax/tkforge\n\n    Hey, my name is Axorax. I have been programming for a few years now. I started making a lot more projects in Python recently and this is one of them. I decided to call the project TkForge.\n  \nWhat My Project Does TkForge allows you to turn your Figma design into code. So, you can make the UI for an app in Figma and add input fields, buttons and much more and name them properly then you can run TkForge to convert your Figma design into code. The names need to be the element that you want. For example; if you want a button element then you can name it \"button\" or \"button Hello World!\". The \"Hello World!\" portion will just get ignored. All of the text after the first space is ignored. However, for some elements, they matter. Like, if you want a textbox element with the placeholder text of \"Hello\" then you need to name it \"textbox Hello\".\n  \nTarget Audience It is meant for anyone who wants to create a GUI in Python easily. Dealing with Tkinter can be a pain a lot of times and it also takes a long time. Using TkForge, you can make better UI's and also get a lot of work done in a short amount of time. Anyone who is new to Python or even an expert can use TkForge to speed up their development times and get a better result. You can TkForge in your production app or for a demo app and really anywhere if you are also using Python.\n  \nComparison There is another project called TkDesigner that does the same sort of thing. But TkForge is able to generate better code. TkForge also supports a lot more elements. Placeholder text for textbox and textarea are not built-in to Python. But TkForge has support for those even though using them requires you to handle a lot of situations by yourself (TkForge provides functions for these situations to be handled correctly, you need to implement them were needed yourself).\n  \n    Thank you for reading! <3"
},
{
    "title": "No title",
    "content": "Excited to share milkcow, my first python package. I'd love any feedback, and to continue to build out the parts of this package that show potential.\n  \nhttps://pypi.org/project/milkcow/ https://github.com/SamReynoso/milkcow\nWhat MilkCow Does\n    Milkcow automates database creation and offers in-memory key-value mapping for data handling. Whether you're building middleware, local storage, or multiprocessing scripts.\n  Target Audience\n    MilkCow is designed for developers looking to streamline the development process. It caters to those who want to simplify data.\n  Comparison\n    Milkcow aims for simplicity. Milkcow offers a way for making it easier for developers to get started. Additional functionalities, including database creation and the in-memory datastore, enhancing its usability.\n  from milkcow import ObjectCow\n\noc = ObjectCow(Record)\noc.push('Bob', records)\nobjs = oc.new('Bob')\nk, v = oc.items()\nfor k in oc.keys()\nnew = oc.new(k)from milkcow import MilkCow\n\nmc = MilkCow(Record)\nmc.pull('Bob')\nmc.push('Alice', list[Record])\nsender = mc.sender.new_sender()\nsender = mc.sender.all_sender()\nsender = mc.sender.keyed_sender('Alice')\nsender.send()"
},
{
    "title": "No title",
    "content": "Hey, happy Friday (don't push to prod). Me and some friends are building a no-code platform to run code improvement agents (really in BETA) .\n  \n    We want to have a quality agent for each language, and I would really appreciate your feedback on python best practices and standards. The agents are created by defining the steps that you want to apply in natural language. Right now our Python agent has the following steps:\n  \n\n\n    Use descriptive naming for functions and variables.\n  \n\n\n    Add Type Hints.\n  \n\n\n    Add proper docstrings.\n  \n\n\n    Make docstrings follow PEP-257 standard.\n  \n\n\n    All variables and functions should be snake_case.\n  \n\n\n    Add proper input validation that checks for type and not null. If the input validation fails raise an Exception.\n  \n\n\n    Add useful logs for debugging with the logging library.\n  \n\n\n    In case you want to check our tool, we have a free playground right now at GitGud and are working on github PR integrations.Happy coding and thank you in advance for your help!\n  \n    Edit: Of course the steps are really basic right now, we are still testing the POC, so any feedback would be really appreciated"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Social media like Reddit, Hacker News, Twitter, etc. contain tons of genuine discussions that you might want to analyze automatically with sentiment analysis. For example you might want to monitor what people say about you, your product, your competitors, etc.\n  \n    I made a technical article that shows how to implement such a sentiment analysis pipeline using the following steps:\n  \n\n\n    Implement social media listening\n  \n\n\n    Integrate the data in your system with an API webhook processed in Python/FastAPI\n  \n\n\n    Analyze the sentiment thanks to generative AI models like GPT-4, LLaMA 3, ChatDolphin, etc.\n  \n\n\n    Here it is: https://kwatch.io/how-to-build-a-social-media-sentiment-analysis-pipeline\n\n    I hope you will find it useful. If you have some comments about how to improve this pipeline I would love to hear them!\n  \n    Arthur"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hey everyone,\n  \n    I’ve recently launched a new website aimed at helping fellow programmers ace their Python interviews. It’s not just limited to Python though; it also covers essential topics like big-O notation, object-oriented programming, design patterns, and more!\n  \n    I’d love to hear your thoughts and feedback on the content, layout, and anything else you think could be improved.\n  \n    Check it out here https://hlop3z.github.io/interviews-python/ and let me know what you think. Your input is invaluable in making this resource the best it can be. Thanks in advance for your time and insights! 🚀🐍\n  \n    Note: It’s mainly to be used in a computer or tablet. You can see it in your mobile, but some sections won’t look as intended."
},
{
    "title": "No title",
    "content": "Listen at podcast.pythontest.com/219\n\n    When starting a SaaS project using Django, there are tons of decisions. I've asked Cory Zue, creator of SaaS Pegasus, to help me sift through some common SaaS/Django decisions."
},
{
    "title": "No title",
    "content": "Hey folks,I have just posted an article for those who want to go a little bit beyond the basic usage of OTEL and understand how it works under the hood. The post quickly touches on:- 🔭 History and the idea of OpenTelemetry- 🧵 Distributed traces & spans. How span collection happens on the service side- 💼 Baggage & trace ctx propagation- 📈 Metrics collection. Views & aggregations. Metrics readers- 📑 OTEL Logging integration- 🤝 Semantic conventions and why that is importantBlog Post: https://www.romaglushko.com/blog/opentelemetry-sdk/Let me know what do you think and hope this is helpful for someone 🙌"
},
{
    "title": "No title",
    "content": "Hi y'all,\n  \n    This might be off topic a bit from the normal stuff that gets posted here, but a while back I was messing around on a Minecraft server that I was hosting on Aternos (a free online Minecraft server host).\n  \n    Now since the server wasn't running on my computer (and since Aternos has a timeout policy to save server resources) it became annoying for my buddies to hop on the server if I was busy. They'd have to ask me to manually start the server from the webpage.\n  \n    So I had a free Saturday a couple of months ago and decided to remedy this problem.\n  \n    My source code is hosted on a GIGO Dev environment if y'all wanna check it out:https://www.gigo.dev/challenge/1732810200471044096\n\n    But to summarize it quickly... I used Playwright and the Discord API in Python to simulate a browser navigating to an Aternos server and starting it. For this tutorial you will need to make an Aternos acct, but their platform is free and very useful if you want to spin up a Minecraft server quickly without using your own resources.\n  \n    You simply need to configure a trigger command or @ the bot and set up a webhook for the the discord server you want to use.\n  \n    There's a full tutorial on how I set it up in the link along with the project structure, but truth be told the basic implementation is pretty simple and can be tweaked to work really however you want.\n  \n    Just wanted to share this to see if anyone had done something similar before, or if I'm just insane and made this mundane problem into a way bigger endeavor than it should be"
},
{
    "title": "No title",
    "content": "Just getting started with pre-commit and I think it's awesome. Looking to find out what other code automation tools people are using. Let me know what works for you and why. Thanks!"
},
{
    "title": "No title",
    "content": "Sensor-App is an Android App that's main focus is to help create a real-time mobile sensor data stream for computer applications, data collection, AR, VR, etc.\n  \n    Github: SensorApp\nFeatures of Sensor-App\n\n\n    Real-Time Sensor Data display\n  \n\n\n    Faster Real-Time Sensor Data Streaming via TCP Sockets\n  \n\n\n    Simple and Easy setup of Data Streaming Server\n  \n\nWhat my Project Does\n    My project is aimed to help provide a Real-Time Mobile Sensor data streaming service.\n  Target Audience\n    Computer Programmers, Data Scientists, AR and VR enthusiasts\n  Remarks\n\n\n    This Application was made with help of Beeware Tools.\n  \n\n\n    I made this Application to test out abilities of Beeware Tools, and get Experience in Android App develpoment with python\n  \n\n\n    Currently the Project only contains Accelerometer, and I will update it soon to support other sensors too.\n  \n\n\n    I am always open hear advice, constructive criticism about my project.\n  \n\n\n    I would like to hear your opinion of my project :}\n  \n\n\n    Thanks for Reading, hope you try out my project :)"
},
{
    "title": "No title",
    "content": "Version 1.2.0 of UXsim is released, which allows simulating taxis, shared mobility and self-driving taxis!\n  \nMain Changes in 1.2.0\n\n\n\n    Add taxi (aka. shared mobility) functions\n  \n\n\n    A standard vehicle in UXsim just travel from A to B and disappear. This is like a private owned vehicle.\n  \n\n\n    From this update, a Vehicle with mode=\"taxi\" behave like a taxi. Specifically, they travel through a network by passing through specific nodes that are dynamically updated, simulating passenger pickup and drop-off.\n  \n\n\n    New sub-module uxsim.TaxiHandler handles these matters.\n  \n\n\n    Built-in vehicle-to-passneger matching methods are also available.\n  \n\n\n    This addresses Issue #41\n  \n\n\n\n\n    From now on, we follow the Semantic Versioning rigorously.\n  \n\n\nUXsim\n\nUXsim is a free, open-source macroscopic and mesoscopic network traffic flow simulator written in Python. It simulates the movements of car travelers and traffic congestion in road networks. It is suitable for simulating large-scale (e.g., city-scale) traffic phenomena. UXsim is especially useful for scientific and educational purposes because of its simple, lightweight, and customizable features, but users are free to use UXsim for any purpose."
},
{
    "title": "No title",
    "content": "I recently took a job with a professor, that includes helping with the grading of biweekly assignments. So I basically have now 30 Notebooks that I have to grade. Top of my head I can think of these approaches:\n  \n\n\n    Convert to PDF and write into the PDF\n  \n\n\n    Duplicate the Notebook and write the comments in extra blocks\n  \n\n\n    Create a .txt file with all my note\n  \n\n\n    Does anybody have experience with this and can share their workflow?"
},
{
    "title": "No title",
    "content": "I want to hear your description of the project you are most proud of. I was just trying to find something on the Internet, but I didn't find much of interest. The only difficult project was CustomTkinter, which, as far as I know, was done entirely in Tkinter. Since it is possible to create something this complex, I'm sure you have something cool too.Note: I am expecting to hear projects more complex than a calculator :)"
},
{
    "title": "No title",
    "content": "Pypy has been able to speed up pure python code by a factor of 5 or more for a number of years. The only disadvantage it has is the difficulty in handling C extensions which are very commonly used in practice.\n  \nhttps://peps.python.org/pep-0744 seems to be talking about speed ups of 5-10%. Why are the goals so much more modest than what pypy can already achieve?"
},
{
    "title": "No title",
    "content": "What My Project Does The Physics Engine Called PhysEng, provides an easy to use environment and visualization combo in which to try out different physics or even provide a template to design your own accelleration/velocity fields. Besides the visualization aspect and numpy the basic functions of the Engine are written completely in 100% python. The features included in the Engine are:\n  \n\n\n    Particles, Soft Bodies, Anchor points\n  \n\n\n    Built in Fields: Drag, Uniform Force Fields, Gravity Between Particles, Octree Gravity etc\n  \n\n\n    Make your own: There are standard templates included in the Examples to design your own fields\n  \n\n\n    Springs - Construct Soft Bodies using Springs. (Built in soft bodies: Cloth and ball\n  \n\n\nTarget Audience PhysEng is made for people who just want to try out different simple simulations or want to design their own physics.\n  \nComparison Looking through github I could never really find a simple and easy-to-use library that did not require me to install some weird libraries or that felt like it was hiding some process from me through using packages. This package is a solution to this since everything is written in python nothing is a secret and can be directed easily.\n  \nGet PhysEng There is full documentation available in the Github repo: https://github.com/levi2234/PhysEng"
},
{
    "title": "No title",
    "content": "I've noticed at work that my coworkers and I try out small things in different ways. Small things like if you want to try that adding two datetimes together behaves in the way you expect. Some people use jupyter notebook for this and others run python interactively in a separate command prompt.\n  \n    I usually run debug in whatever IDE I'm using and then letting it stop at the code I'm currently developing and then using the debug console to test things out. Sometimes this means just leaving the debugger at a breakpoint for half an hour while I continue writing code. Is my way of doing it weird or does it have any disadvantages? How do you usually test out things on the go in a good way?"
},
{
    "title": "No title",
    "content": "What My Project Does\narchivefile is a wrapper around tarfile, zipfile, py7zr, and rarfile. The above libraries are excellent when you are dealing with a single archive format but things quickly get annoying when you have a bunch of mixed archives such as .zip, .7z, .cbr, .tar.gz, etc because each library has a slightly different syntax and quirks which you need to deal with. archivefile wraps the common methods from the above libraries to provide a unified interface that takes care of said differences under the hood. However, it's not as powerful as the libraries it wraps due to lack of support for features that are unique to a specific archive format and library.\n  Target audience\n    Anyone who's using python to deal with different archive formats\n  Comparison\n\n\n    ZipFile, TarFile, RarFile, and py7zr - These are libraries that mine wraps since each of them can only deal with a single archive format\n  \n\n\n    shutil - Shutil can only deal with zipfile and tarfile and only allows full packing or full extraction.\n  \n\n\n    patool - Excellent library that deals with wider range of formats than mine but in doing so it provides less granular control over each ArchiveFile falls somewhere between the powerful dedicated library and the far less powerful universal libaries.\n  \n\nLinks\n    Repository: https://github.com/Ravencentric/archivefile Docs: https://ravencentric.github.io/archivefile"
},
{
    "title": "No title",
    "content": "What My Project Does Shoots is essentially a \"data lake\" where you can easily store pandas dataframes, and retrieve them later or from different locations or in different tools. Shoots has a client and a server. After choosing a place to run the server, you can easily use the client to \"put\" and \"get\" dataframes. Shoots supports SQL, allowing you to put very large dataframes, and then use a query to only get a subset. Shoots also allows you to resample on the server.\n  # put a dataframe, uploads it to the server  \ndf = pd.read\\_csv('sensor\\_data.csv')  \nshoots.put(\"sensor\\_data\", dataframe=df, mode=PutMode.REPLACE)  \n\n\n# retrieve the whole data frame  \ndf0 = shoots.get(\"sensor\\_data\")  \nprint(df0)  \n\n# or use sql to retrieve just some of the data  \nsql = 'select \"Sensor\\_1\" from sensor\\_data where \"Sensor\\_2\" < .2'  \ndf1 = shoots.get(\"sensor\\_data\", sql=sql)\nTarget Audience Shoots is designed to be used in production by data scientists and other python devs using pandas. The server is configurable to run in various settings, including locally on a laptop if desired. It is useful for anyone who wants to share dataframes, or store dataframes so they can be easily accessed from different sources.\n  \nComparison To my knowledge, Shoots is the only data lake with a client that is 100% pandas native. The get() method returns pandas dataframes natively, so there is no cumbersome translations such as required from typical databases and data lakes. The server is build on top of Apache Arrow Flight, and is very efficient with storage because it uses Parquet as the storage format natively. While the Shoots client does all of the heavy listing, if desired, the server can be accessed with any Apache Flight client library, so other languages are supported by the server.\n  \nGet Shoots There is full documentation available in the Github repo: https://github.com/rickspencer3/shoots\n\n    It is packaged for Pypi as well: (https://pypi.org/project/shoots/) ```pip install shoots\""
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hello ,\n  \n    I'm excited to share a project that was initially intended to use in my graduating product(Capstone)\n  \nWhat My Proeject Does\n\n    I made NLLB-200 Distill 350M model to translating English to Korean\n  \nTarget Audience\n\n    GPU servers are quite expensive, so I made it for university students who can't cost the server (like me.)\n  \nComparison\n\n    It's even smaller and faster the other NLLB-200 model. so it can be run with CPU!\n  \n    more details are in my page\n  \n    If you know Korean, please give me a lot of feedback\n  \nhttps://github.com/newfull5/NLLB-200-Distilled-350M-en-ko\n\n    thank you!!"
},
{
    "title": "No title",
    "content": "Hi\n  \n    I've made a package called mpl_ascii which is a backend for matplotlib. You can find it here: https://github.com/chriscave/mpl_ascii\n\n    I would love to share it with others and see what you guys think\n  \nWhat it is\n\n    It is a backend for matplotlib that converts your plots into ASCII characters.\n  \n    At the moment I have only made support for: bar charts, scatter plots and line plots but if there's demand for more then I would love to keep working on it.\n  \nTarget Audience:\n\n    Anyone using matplotlib to create plots who might also want to track how their plots change with their codebase (i.e. version control).\n  \nComparison:\n\n    There are a few plotting libraries that produce ASCII plots but I have only come across this one that is a backend for matplotlib: https://github.com/gooofy/drawilleplot. I think it's a great package and it is really clever code but I found it a little lacking when you have multiple colours in a plot. Let me know if you know of other matploblib backends that does similar things.\n  \nUse case:\n\n    A use case I can think of is for version controlling your plots. Having your plot as a txt format means it can be much easier to see the diff and the files you are committing are much smaller.\n  \n    Since it is only a backend to matplotlib then you only need to switch to it and you don't need to recreate your plots in a different plotting library.\n  \n    Thanks for reading and let me know what you think! :)"
},
{
    "title": "No title",
    "content": "There is no shebang line that actually works across platforms for python 3.\n  \n    I would like one that works on unmodified :\n  \n\n\n    Debian shell (Dropped python2, falls under PEP 394)\n  \n\n\n    Older Linux shells that still have python pointing to python2 (PEP 394)\n  \n\n\n    Windows cmd.exe shell (this really just means one that will work with PEP 397)\n  \n\n\n    Gitbash for Windows (sort of a weird half sibling that respects shebangs)\n  \n\n\n    The best work around I have found is:\n  \n\n\n    use #!/usr/bin/env python3\n\n\n\n    on Windows copy python.exe to python3.exe\n\n\n\n    Then make sure both are in your path for unix-like shells.\n  \n\n\n\n\n    Debian make sure python-is-python2 or python-is-python3 is installed, in case you come upon a #!/usr/bin/env python.\n  \n\n\n    As Windows adopts more and more Unix-like behavior, and more distros drop python2, having completely different \"portability\" rules is going to become a larger problem.\n  \n    A significant compatibility enhancement would be if the official python packages for Windows just included a python3.exe to comply with PEP 394. This could be a copy of python.exe like my workaround, or one could be a minimal executable that just hands off to the other or to py.\n  \n    An alternative would be adding py and pyw from PEP 397 to PEP 394. and having people move to the shebang #!/usr/bin/env py -3.\n  \n    The belt and suspenders compatibility approach is all platforms should have a py, pyw, and python3 executable that can launch python3 scripts if requested. And python should be an executable than runs some version of python.\n  \n    I am curious what others are using out there? Do others launch python scripts from inside gitbash? do you have a seperate window for running the script and git actions? Are you manually choosing the python executable on the command line?"
},
{
    "title": "No title",
    "content": "Hello wonderful community,\n  \n    Today I'll present to you pyaair, a scraper made pure on Python https://github.com/johnbalvin/pyaair\n\n    Easy instalation\n  \n    ` ` `pip install pyaair ` ` `\n  \n    Easy Usage\n  \n    ` ` ` airports=pyaair.airports(\"miami\",\"\") ` ` `\n  \n    Always remember, only use selenium, puppeteer, playwright etc when it's strictly necesary\n  \n    Let me know what you think,\n  \n    thanks\n  \n    About me:\n  \n    I'm full stack developer specialized on web scraping and backend, with 6-7 years of experience"
},
{
    "title": "No title",
    "content": "Link: https://github.com/Axorax/tkforge\n\n    Hey, my name is Axorax. I have been programming for a few years now. I started making a lot more projects in Python recently and this is one of them. I decided to call the project TkForge.\n  \nWhat My Project Does TkForge allows you to turn your Figma design into code. So, you can make the UI for an app in Figma and add input fields, buttons and much more and name them properly then you can run TkForge to convert your Figma design into code. The names need to be the element that you want. For example; if you want a button element then you can name it \"button\" or \"button Hello World!\". The \"Hello World!\" portion will just get ignored. All of the text after the first space is ignored. However, for some elements, they matter. Like, if you want a textbox element with the placeholder text of \"Hello\" then you need to name it \"textbox Hello\".\n  \nTarget Audience It is meant for anyone who wants to create a GUI in Python easily. Dealing with Tkinter can be a pain a lot of times and it also takes a long time. Using TkForge, you can make better UI's and also get a lot of work done in a short amount of time. Anyone who is new to Python or even an expert can use TkForge to speed up their development times and get a better result. You can TkForge in your production app or for a demo app and really anywhere if you are also using Python.\n  \nComparison There is another project called TkDesigner that does the same sort of thing. But TkForge is able to generate better code. TkForge also supports a lot more elements. Placeholder text for textbox and textarea are not built-in to Python. But TkForge has support for those even though using them requires you to handle a lot of situations by yourself (TkForge provides functions for these situations to be handled correctly, you need to implement them were needed yourself).\n  \n    Thank you for reading! <3"
},
{
    "title": "No title",
    "content": "Excited to share milkcow, my first python package. I'd love any feedback, and to continue to build out the parts of this package that show potential.\n  \nhttps://pypi.org/project/milkcow/ https://github.com/SamReynoso/milkcow\nWhat MilkCow Does\n    Milkcow automates database creation and offers in-memory key-value mapping for data handling. Whether you're building middleware, local storage, or multiprocessing scripts.\n  Target Audience\n    MilkCow is designed for developers looking to streamline the development process. It caters to those who want to simplify data.\n  Comparison\n    Milkcow aims for simplicity. Milkcow offers a way for making it easier for developers to get started. Additional functionalities, including database creation and the in-memory datastore, enhancing its usability.\n  from milkcow import ObjectCow\n\noc = ObjectCow(Record)\noc.push('Bob', records)\nobjs = oc.new('Bob')\nk, v = oc.items()\nfor k in oc.keys()\nnew = oc.new(k)from milkcow import MilkCow\n\nmc = MilkCow(Record)\nmc.pull('Bob')\nmc.push('Alice', list[Record])\nsender = mc.sender.new_sender()\nsender = mc.sender.all_sender()\nsender = mc.sender.keyed_sender('Alice')\nsender.send()"
},
{
    "title": "No title",
    "content": "Hey, happy Friday (don't push to prod). Me and some friends are building a no-code platform to run code improvement agents (really in BETA) .\n  \n    We want to have a quality agent for each language, and I would really appreciate your feedback on python best practices and standards. The agents are created by defining the steps that you want to apply in natural language. Right now our Python agent has the following steps:\n  \n\n\n    Use descriptive naming for functions and variables.\n  \n\n\n    Add Type Hints.\n  \n\n\n    Add proper docstrings.\n  \n\n\n    Make docstrings follow PEP-257 standard.\n  \n\n\n    All variables and functions should be snake_case.\n  \n\n\n    Add proper input validation that checks for type and not null. If the input validation fails raise an Exception.\n  \n\n\n    Add useful logs for debugging with the logging library.\n  \n\n\n    In case you want to check our tool, we have a free playground right now at GitGud and are working on github PR integrations.Happy coding and thank you in advance for your help!\n  \n    Edit: Of course the steps are really basic right now, we are still testing the POC, so any feedback would be really appreciated"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Social media like Reddit, Hacker News, Twitter, etc. contain tons of genuine discussions that you might want to analyze automatically with sentiment analysis. For example you might want to monitor what people say about you, your product, your competitors, etc.\n  \n    I made a technical article that shows how to implement such a sentiment analysis pipeline using the following steps:\n  \n\n\n    Implement social media listening\n  \n\n\n    Integrate the data in your system with an API webhook processed in Python/FastAPI\n  \n\n\n    Analyze the sentiment thanks to generative AI models like GPT-4, LLaMA 3, ChatDolphin, etc.\n  \n\n\n    Here it is: https://kwatch.io/how-to-build-a-social-media-sentiment-analysis-pipeline\n\n    I hope you will find it useful. If you have some comments about how to improve this pipeline I would love to hear them!\n  \n    Arthur"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hey everyone,\n  \n    I’ve recently launched a new website aimed at helping fellow programmers ace their Python interviews. It’s not just limited to Python though; it also covers essential topics like big-O notation, object-oriented programming, design patterns, and more!\n  \n    I’d love to hear your thoughts and feedback on the content, layout, and anything else you think could be improved.\n  \n    Check it out here https://hlop3z.github.io/interviews-python/ and let me know what you think. Your input is invaluable in making this resource the best it can be. Thanks in advance for your time and insights! 🚀🐍\n  \n    Note: It’s mainly to be used in a computer or tablet. You can see it in your mobile, but some sections won’t look as intended."
},
{
    "title": "No title",
    "content": "Listen at podcast.pythontest.com/219\n\n    When starting a SaaS project using Django, there are tons of decisions. I've asked Cory Zue, creator of SaaS Pegasus, to help me sift through some common SaaS/Django decisions."
},
{
    "title": "No title",
    "content": "Hey folks,I have just posted an article for those who want to go a little bit beyond the basic usage of OTEL and understand how it works under the hood. The post quickly touches on:- 🔭 History and the idea of OpenTelemetry- 🧵 Distributed traces & spans. How span collection happens on the service side- 💼 Baggage & trace ctx propagation- 📈 Metrics collection. Views & aggregations. Metrics readers- 📑 OTEL Logging integration- 🤝 Semantic conventions and why that is importantBlog Post: https://www.romaglushko.com/blog/opentelemetry-sdk/Let me know what do you think and hope this is helpful for someone 🙌"
},
{
    "title": "No title",
    "content": "Hi y'all,\n  \n    This might be off topic a bit from the normal stuff that gets posted here, but a while back I was messing around on a Minecraft server that I was hosting on Aternos (a free online Minecraft server host).\n  \n    Now since the server wasn't running on my computer (and since Aternos has a timeout policy to save server resources) it became annoying for my buddies to hop on the server if I was busy. They'd have to ask me to manually start the server from the webpage.\n  \n    So I had a free Saturday a couple of months ago and decided to remedy this problem.\n  \n    My source code is hosted on a GIGO Dev environment if y'all wanna check it out:https://www.gigo.dev/challenge/1732810200471044096\n\n    But to summarize it quickly... I used Playwright and the Discord API in Python to simulate a browser navigating to an Aternos server and starting it. For this tutorial you will need to make an Aternos acct, but their platform is free and very useful if you want to spin up a Minecraft server quickly without using your own resources.\n  \n    You simply need to configure a trigger command or @ the bot and set up a webhook for the the discord server you want to use.\n  \n    There's a full tutorial on how I set it up in the link along with the project structure, but truth be told the basic implementation is pretty simple and can be tweaked to work really however you want.\n  \n    Just wanted to share this to see if anyone had done something similar before, or if I'm just insane and made this mundane problem into a way bigger endeavor than it should be"
},
{
    "title": "No title",
    "content": "Just getting started with pre-commit and I think it's awesome. Looking to find out what other code automation tools people are using. Let me know what works for you and why. Thanks!"
},
{
    "title": "No title",
    "content": "Sensor-App is an Android App that's main focus is to help create a real-time mobile sensor data stream for computer applications, data collection, AR, VR, etc.\n  \n    Github: SensorApp\nFeatures of Sensor-App\n\n\n    Real-Time Sensor Data display\n  \n\n\n    Faster Real-Time Sensor Data Streaming via TCP Sockets\n  \n\n\n    Simple and Easy setup of Data Streaming Server\n  \n\nWhat my Project Does\n    My project is aimed to help provide a Real-Time Mobile Sensor data streaming service.\n  Target Audience\n    Computer Programmers, Data Scientists, AR and VR enthusiasts\n  Remarks\n\n\n    This Application was made with help of Beeware Tools.\n  \n\n\n    I made this Application to test out abilities of Beeware Tools, and get Experience in Android App develpoment with python\n  \n\n\n    Currently the Project only contains Accelerometer, and I will update it soon to support other sensors too.\n  \n\n\n    I am always open hear advice, constructive criticism about my project.\n  \n\n\n    I would like to hear your opinion of my project :}\n  \n\n\n    Thanks for Reading, hope you try out my project :)"
},
{
    "title": "No title",
    "content": "Version 1.2.0 of UXsim is released, which allows simulating taxis, shared mobility and self-driving taxis!\n  \nMain Changes in 1.2.0\n\n\n\n    Add taxi (aka. shared mobility) functions\n  \n\n\n    A standard vehicle in UXsim just travel from A to B and disappear. This is like a private owned vehicle.\n  \n\n\n    From this update, a Vehicle with mode=\"taxi\" behave like a taxi. Specifically, they travel through a network by passing through specific nodes that are dynamically updated, simulating passenger pickup and drop-off.\n  \n\n\n    New sub-module uxsim.TaxiHandler handles these matters.\n  \n\n\n    Built-in vehicle-to-passneger matching methods are also available.\n  \n\n\n    This addresses Issue #41\n  \n\n\n\n\n    From now on, we follow the Semantic Versioning rigorously.\n  \n\n\nUXsim\n\nUXsim is a free, open-source macroscopic and mesoscopic network traffic flow simulator written in Python. It simulates the movements of car travelers and traffic congestion in road networks. It is suitable for simulating large-scale (e.g., city-scale) traffic phenomena. UXsim is especially useful for scientific and educational purposes because of its simple, lightweight, and customizable features, but users are free to use UXsim for any purpose."
},
{
    "title": "No title",
    "content": "I recently took a job with a professor, that includes helping with the grading of biweekly assignments. So I basically have now 30 Notebooks that I have to grade. Top of my head I can think of these approaches:\n  \n\n\n    Convert to PDF and write into the PDF\n  \n\n\n    Duplicate the Notebook and write the comments in extra blocks\n  \n\n\n    Create a .txt file with all my note\n  \n\n\n    Does anybody have experience with this and can share their workflow?"
},
{
    "title": "No title",
    "content": "I want to hear your description of the project you are most proud of. I was just trying to find something on the Internet, but I didn't find much of interest. The only difficult project was CustomTkinter, which, as far as I know, was done entirely in Tkinter. Since it is possible to create something this complex, I'm sure you have something cool too.Note: I am expecting to hear projects more complex than a calculator :)"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hello everyone., on today new scraper I created the python version for the zillow scraper.\n  \nhttps://github.com/johnbalvin/pyzill\n\nWhat My Project Does\n\n    The library will get zillow listings and details.I didn't created a defined structured like on the Go version just because it's not as easy to maintain this kind of projects on python like on Go.It is made on pure python with HTTP requests, so no selenium, puppeteer, playwright etc. or none of those automation libraries that I hate.\n  \nTarget Audience\n\n    This project target could be real state agents probably, so lets say you want to track the real price history of properties around an area, you can use it track it\n  \nComparison \n  \n    There are libraries similar outhere but they look outdated, most of the time, scraping projects need to ne on constant maintance due to changed on the page or api\n  \npip install pyzill\n\n    Let me know what ou think, thanks\n  \n    about me:I'm full stack developer specialized on web scraping and backend, with 6-7 years of experience"
},
{
    "title": "No title",
    "content": "What My Project Does?\n\n    Command-line tool to simplify the management of DNS records for domains hosted on Netlify, whether you want to migrate to Netlify or migrate away from Netlify.\n  \n\n\n    Import Zone File from Godaddy, NameCheap or others to Netlify - helpful in migrating nameservers.\n  \n\n\n    Export DNS records from Netlify as zonefile to be import it to Godaddy, Namecheap or other servers.\n  \n\n\nZonefile is a list of all the DNS records for a given domain\n  \n    Here is the url - https://github.com/sumansaurabh/netlify-dns-manager\n\nTarget Audience (e.g., Is it meant for production, just a toy project, etc.)\n\n    Anyone who is intended to use Netlify DNS.\n  \nComparison (A brief comparison explaining how it differs from existing alternatives.)\n\n    There is no such tool that can help in managing DNS in Netlify - hence I have created it."
},
{
    "title": "No title",
    "content": "Take the excellent nuitka, compile python code to C, turn it into web assembly, and you got Python in the browser, without the usual runtime overhead:https://wasmer.io/posts/py2wasm-a-python-to-wasm-compiler\n\n    While the doc states you can get this effect by doing:\n  pip install py2wasm\npy2wasm myprogram.py -o myprogram.wasm\n    wasmer run myprogram.wasm\n  \n    You still need the wasmer WASM runtime (curl https://get.wasmer.io -sSfL | sh for Unix users, iwr https://win.wasmer.io -useb | iex for Windows user with Powershell), however. But more than that, since you need nuikta, it means you need a C compiler installed. While in Ubuntu it's a just an sudo apt install build-essential, it will require a bit more work on Windows and Mac."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I'm trying to decide whether to use typeguard or stick to assert isinstance in the places where I care. Has anyone done benchmarking testing of the overhead of using type guards \"at\"typeguard decorator ?"
},
{
    "title": "No title",
    "content": "https://github.com/RadoTheProgrammer/pwdgen\nWhat my project does\n    My project generate simple, strong, memorable and easy-to-type passwords.\n  \n    The system is simple: it generate 2 pronounceable words separated by a special char, with a number at start or end.\n  \n    I tried creating a password generator that combines simplicity, security, memorability, and ease of type.\n  \n    This should be secure enough because it can generate 4e15 possibilities of passwords and uses the secrets module.\n  Target audience\n    For anyone who need to have passwords easily.\n  Comparison\n    Most passwords manager generate completely passwords with completely random characters that aren't very easy to memorize or tape.\n  \n    Examples include Dashlane, Norton, Avast.\n  \n    Or other like Bitwarden generate passwords that are not really fast-to-type.\n  \n    The mine generate sth like 7Xy-Bonuwucete 0Qubyby+Pomafy , or 7Zuxogu:Lebuwo .\n  Usage\n    You can install it with pip install pwd-generator and use the cli version:\n  pwdgen\n    To use it in a python code\n  import pwdgen\nprint(pwdgen.generate())Changes from last post\n    I already made another post for this, but this was not well received because my code use the random module.\n  \n    I updated it to use the secrets module."
},
{
    "title": "No title",
    "content": "Hi all,\n  \n    I used polars (a Rust-powered “Blazingly Fast DataFrame Library”) to analyze seven years of my own music royalties data. Vega-Altair (Python wrapper for Vega-Lite) powers the (often interactive) visualizations.\n  \n    Link to the article: https://osc.garden/blog/data-analysis-music-streaming/\n\n    It was a lot of fun learning polars and setting up the graphs with Vega-Altair.\n  \n    Would love to hear any comments and suggestions.\n  \n    Link to r/dataisbeautiful post: https://www.reddit.com/r/dataisbeautiful/comments/1ca7x6z/oc_my_music_needs_more_than_200k_plays_on/"
},
{
    "title": "No title",
    "content": "PyPiSource\n\nWhat My Project Doesfind_where is a Python package that provides a function to find values in dictionaries where a specified key matches a given value, similar to filtering in SQL.\n  \nTarget AudienceThis is my first attempt at creating a Python package so I would describe this as a toy project at this stage but am definitely looking for feedback from the wider community.\n  \nComparisonI mainly wrote this package because I kept on writing the same iterable based code when trying to find a value, given a key:\n  data = {\n\"people\": [\n    {\"first_name\": \"John\", \"last_name\": \"Smith\", \"age\": 25},\n    {\"first_name\": \"Alice\", \"last_name\": \"Jones\", \"age\": 32},\n    ]\n}\n\nfirst_name = None\n\nfor result in data[\"people\"]:\n    if result[\"age\"] == 32:\n        first_name = result[\"first_name\"]\n        break\n\nprint(first_name)\n    When using find_where, you can simply run:\n  from find_where import find_where\n\ndata = {\n    \"people\": [\n        {\"first_name\": \"John\", \"last_name\": \"Smith\", \"age\": 25},\n        {\"first_name\": \"Alice\", \"last_name\": \"Jones\", \"age\": 32},\n    ]\n}\n\nfirst_name = find_where(data[\"people\"], \"first_name\", age=32)\n\nprint(first_name)\n    Appreciate any feedback, constructive or otherwise!"
},
{
    "title": "No title",
    "content": "I have started at a new job and had the idea that it would probably be clever to set up my developing environment in exactly the same way as my predecessor did. Because:\n  \n\n\n    This should help resolving errors quicker in the transition period\n  \n\n\n    His code was good and clean and it appears that he knows what he is doing\n  \n\n\n    we were using mostly the same tools (VScode etc.) anyways.\n  \n\n\n    He set up his virtual environments (VE)s with conda/mamba. I vaguely remembered that I also used to do that but then stopped for some reason and switched to the virtualenv package. But I did not remember why anymore. So I just set up my VEs in the same way, it should not really make any difference anyways (so I thought). Well, fast forward about two weeks and now I have VEs that occasionally (but not always) exist twice in the same folders under the same name (according to mamba info --envs) and that are at the same time completely empty (according to mamba list) and contain all packages I have installed anywhere, ever (according to pip list). I usually install packages via pip and I assume this may have fucked things up in combination with mamba? I'll probably switch back to virtualenv again and add a \"do not use conda/mamba VEs !!!\" in my notes. I am working on Windows. Is mamba better on Linux?"
},
{
    "title": "No title",
    "content": "I'm sharing some of the Python tutorials I made.  I teach Ukrainian teenagers and university students Python programming and data science for free.\n  \nHere is an introduction to Pandas.\n  \n    It shows:\n  \n\n\n    How to read data into a Pandas dataframe from a CSV file.\n  \n\n\n    Show how to print out the column names.\n  \n\n\n    See and set the index.\n  \n\n\n    Print sections of the dataframe.\n  \n\n\n    Do basic statistics.\n  \n\n\n    Create a dataframe from a dictionary."
},
{
    "title": "No title",
    "content": "I was wondering if anyone has come across anything like https://github.com/a-h/templ or https://hono.dev/guides/jsx, but for python. For context, I am familiar with jinja2, mako, etc. but find them to be unintuitive due to the loose coupling of logic (ex: database calls) and templating (ex: generating a list from the database results). Therefore, I am looking for a \"inline\" templating solution."
},
{
    "title": "No title",
    "content": "https://github.com/leftmove/cria\n\n    My name is Anonyo, and I am a seventeen year old from Southeast Michigan. This is my second open source project. I built Cria, a Python library that allows you to run LLMs programmatically through Python. Cria is designed so there is as little friction as possible — getting started takes just five lines of code.\n  \n    I created this library because I was using OpenAI in my project, and kept running into rate limits. With local LLMs getting better and better, I sought to switch, but found command line configurations to be limited. Running and configuring LLMs can be trivial, but programs like ollama make it easier. The only problem I found with ollama though, was the lack of features in its Python client.\n  \n    To counteract this, I built Cria — a program that is concise, efficient, and most importantly programmable. The name is based off of Meta's llama, as baby llamas are called crias. Cria solves pain points of ollama - it manages the ollama server for you, saves your message history, streams responses by default, and allows easier management of multiple LLMs at once.\n  \n    The target audience for this project is anyone that uses AI. This library was built in a weekend, and is meant to make running LLMs easier for anyone that wants to go local. I built this for myself, but it could be used at scale as ollama runs its own server (there would of course have to be extra configuration though).\n  \n    In Python, while there are libraries to run LLMs, they are feature limited. Ollama provides the most extensive toolset, and so Cria uses that. Ollama has its own Python library, but I found that to be cumbersome when running multiple LLMs. There are similar libraries, but existing alternatives involve strictly the command line (from what I've seen).\n  \n    While this isn't my first open source project, it's my first Python library. The code may be a little rough around the edges, so I would appreciate any and all suggestions.\n  \n    Thank you!"
},
{
    "title": "No title",
    "content": "Hi there,\n  \n    My team at Bytewax and I have been working on a series of hands-on guides on streaming data and I am excited to share how one can handle missing values in real-time in Python.\n  \n    While some parts of the guide are simplified, for example, we use a random number generator as an input source, the algorithmic part is production-ready.\n  \n    We are taking advantage of the Numpy library + stateful operators in Bytewax.\n  \n    Other input sources are available, too.\n  \nhttps://bytewax.io/guides/handling-missing-values"
},
{
    "title": "No title",
    "content": "What My Project Does\n    Molly is designed to monitor time series data in a SQL database such as MySQL or PostgreSQL.\n  \n    Currently supported data quality feature:- staleness: when the data was last updated- completeness: whether any data is missing in a given date range\n  \n    Currently support messaging service:- Slack Web API\n  \n    Currently support scheduling service:- Apache Airflow- crontab\n  Target Audience\n    people in need of a plug-n-play dq monitoring service\n  Comparison\n    It's open source. It's free. It's written in pure python. And it's made for modern data stack so you can try it out with minimum effort.\n  \nI'd appreciate it if you could check it out on GitHub. All feedback and/or contributions are welcomed!\n\n    GitHub: https://github.com/flaviaouyang/molly"
},
{
    "title": "No title",
    "content": "For my application, Scipy's solvers are not fast enough so I am looking to speed up by using another package. These are some packages I have found so far:\n  \n\n\nNumbaLSODA\n\n\n\nDifferentialEquations.jl\n\n\n\nTorchquad(Although doesn't solve ODEs, just integrates quickly, but that could be wrapped I guess)\n  \n\n\nTorchdiffeq\n\n\n\nPyDSTool\n\n\n\n    I will be experimenting with these packages, but I was wondering whether anyone has experience with them and has done work on them before. Also, I am wondering whether there are any packages I should check out?"
},
{
    "title": "No title",
    "content": "New developers print out strings to their terminal. It’s how we learn! But printing out to the terminal isn’t what you do with most professional applications.\n  \n    In those cases, you log into files. Sometimes, you log into multiple locations at once. These logs may serve as an audit trail for compliance purposes or help the engineers debug what went wrong.\n  \nPython Logging teaches you how to log in the Python programming language. Python is one of the most popular programming languages in the world. Python comes with a logging module that makes logging easy.\n  What You’ll Learn\n    In this book, you will learn how about the following:\n  \n\n\n    Logger objects\n  \n\n\n    Log levels\n  \n\n\n    Log handlers\n  \n\n\n    Formatting your logs\n  \n\n\n    Log configuration\n  \n\n\n    Logging decorators\n  \n\n\n    Rotating logs\n  \n\n\n    Logging and concurrency\n  \n\n\n    and more!\n  \n\nBook formats\n    The finished book will be made available in the following formats:\n  \n\n\n    paperback (at the appropriate reward level)\n  \n\n\n    PDF\n  \n\n\n    epub\n  \n\n\n    The paperback is a 6″ x 9″ book and is approximately 150 pages long.\n  \n    You can check out the book's campaign on Kickstarter.\n  \n    Let me know if you have any questions!"
},
{
    "title": "No title",
    "content": "Just came across this list and I think it's amazing: https://techtalksweekly.substack.com/p/all-pydata-2023-talks"
},
{
    "title": "No title",
    "content": "I have not seen updates in almost a year and there is no activity in the support forum? Is there a replacement forum? Thanks"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I have been programming for a few years now and have on and off had jobs in the industry. I used Jupyter Notebook in undergrad for a course almost a decade ago and I found it really cool. Back then I really didn’t know what I was doing and now I do. I think it’s cool how it makes it feel more like a TI calculator (I studied math originally)\n  \n    What are jobs that utilize this? What can I do or practice to put myself in a better position to land one?"
},
{
    "title": "No title",
    "content": "I made a detailed tutorial on how to work with missing data, outliers, and how to lean up data in Pandas Python.  This explains how to clean up your data so that your statistical results and trainin data is not skewed.\n  \n\nhttps://github.com/werowe/HypatiaAcademy/blob/master/pandas/pandas_missing_data.ipynb"
},
{
    "title": "No title",
    "content": "Tkinter is going to drive me crazy.\n  \n    Let me lay this out for you:\n  \n    Tkinter is the GUI interface I am using, which is a python frontend for a C library\n  \n    It holds \"widgets\" in a window.\n  \n    Some widgets have variables (StringVar, BooleanVar, etc - not \"variables\" like code variables but a class of updatable data holders for the widgets to link to the Tcl backend)\n  \n    The Vars have incremental names like PYVAR1, PYVAR93, etc by default.\n  \n    Names can be provided to the Vars via keywords upon initialization.  (Note I have not been able to find any reason to not do this - hence this post)\n  \n    Names should absolutely not be provided to the Vars unless they are unique.  Because on python garbage collection (or \"del Var\" command) the Vars are told to destroy() themselves and then this command is passed to C \"destroy this Var.\"\n  \n    But the garbage collection in C (or rather Tcl) is slightly delayed from Python...\n  \n    So if I immediately I instantiate a different Var with a different value (or same value, who knows) but reusing the same name, the Var has been deleted in Python (yey!) but it has not been deleted in C yet (uh oh).  C says to Python oh hey I got this Var already, use it, don't make a new one. So python goes to use it and then OOPS NEVERMIND LOL IT'S DELETED IN C HAHAHA\n  \n    So now I have this Var that I can't access in Python because it's a Schrodinger's Variable: it both exists and does not exist.  Wonky behavior too, like check boxes that are in no state at all, neither checked nor unchecked, and I can't access the variable to see what state it is in because the variable that existed before the one I'm using no longer actually exists.  So the Var I can access looks 100% fine!  But if I put a value checker in just the right place I can catch where the wonky behavior exists.\n  \n    This has been a frustrating several weeks.\n  \n    Anyway.  I have not been able to find any information about this and thought I'd share.  Hopefully this helps someone avoid the mistake I made.  I have since set all Var names to be the incremental default names.\n  \n    Edit: after quite a bit of experimentation I have discovered the issue is not in the Variable naming but in the fact that I had the Variables stored as attributes in my custom widget classes.  The attributes were not being cleaned up properly on widget destroy().  The simple solution is to pop the variable attribute from the widget dict before child.destroy()."
},
{
    "title": "No title",
    "content": "Hi, I'm Owen, and I've developed an AI tool for data processing.\n  \n    I spend a significant amount of time processing daily data from numerous Excel files, which requires me to write a lot of Python code.\n  \n    I have been using GitHub Copilot to assist with this, but I am exploring ways to eliminate the need to write code manually.\n  \n    The tool is called Tipis AI, and you can find it here: https://github.com/tipisai/tipis-fe\n\nWhat My Project Does\n\n    Therefore, I have developed an AI that generates Python code for data processing. I've also created a Python runtime environment to execute this code.\n  \n    You can use this tool to process any type of files.\n  \nTarget Audience\n\n    Any worker. Someone needs to process files.\n  \nComparison\n\n    You can perform complex calculations and summarizations on files using natural language descriptions, without needing to master complex Python knowledge\n  \n\n    If you have any suggestions for AI in data processing, please let me know, and I might include them in this product.I am very interested in code generation."
},
{
    "title": "No title",
    "content": "Golem Network has just released Ray on Golem MVP, a significant milestone in integrating Ray with Golem's decentralized infrastructure! 🎉Check out all the details about the release and next steps in our blog post:📝 https://blog.golem.network/announcing-ray-on-golem-mvp-launch-the-recommended-way-of-running-python-code-on-the-golem-network/👉 Golem is looking for engaged beta testers to run diverse applications and collaborate on refining the solution. Interested? Find all the information on the brand-new Ray on Golem site:🔗 https://www.golem.network/ray"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hello everyone., on today new scraper I created the python version for the zillow scraper.\n  \nhttps://github.com/johnbalvin/pyzill\n\nWhat My Project Does\n\n    The library will get zillow listings and details.I didn't created a defined structured like on the Go version just because it's not as easy to maintain this kind of projects on python like on Go.It is made on pure python with HTTP requests, so no selenium, puppeteer, playwright etc. or none of those automation libraries that I hate.\n  \nTarget Audience\n\n    This project target could be real state agents probably, so lets say you want to track the real price history of properties around an area, you can use it track it\n  \nComparison \n  \n    There are libraries similar outhere but they look outdated, most of the time, scraping projects need to ne on constant maintance due to changed on the page or api\n  \npip install pyzill\n\n    Let me know what ou think, thanks\n  \n    about me:I'm full stack developer specialized on web scraping and backend, with 6-7 years of experience"
},
{
    "title": "No title",
    "content": "What My Project Does?\n\n    Command-line tool to simplify the management of DNS records for domains hosted on Netlify, whether you want to migrate to Netlify or migrate away from Netlify.\n  \n\n\n    Import Zone File from Godaddy, NameCheap or others to Netlify - helpful in migrating nameservers.\n  \n\n\n    Export DNS records from Netlify as zonefile to be import it to Godaddy, Namecheap or other servers.\n  \n\n\nZonefile is a list of all the DNS records for a given domain\n  \n    Here is the url - https://github.com/sumansaurabh/netlify-dns-manager\n\nTarget Audience (e.g., Is it meant for production, just a toy project, etc.)\n\n    Anyone who is intended to use Netlify DNS.\n  \nComparison (A brief comparison explaining how it differs from existing alternatives.)\n\n    There is no such tool that can help in managing DNS in Netlify - hence I have created it."
},
{
    "title": "No title",
    "content": "Take the excellent nuitka, compile python code to C, turn it into web assembly, and you got Python in the browser, without the usual runtime overhead:https://wasmer.io/posts/py2wasm-a-python-to-wasm-compiler\n\n    While the doc states you can get this effect by doing:\n  pip install py2wasm\npy2wasm myprogram.py -o myprogram.wasm\n    wasmer run myprogram.wasm\n  \n    You still need the wasmer WASM runtime (curl https://get.wasmer.io -sSfL | sh for Unix users, iwr https://win.wasmer.io -useb | iex for Windows user with Powershell), however. But more than that, since you need nuikta, it means you need a C compiler installed. While in Ubuntu it's a just an sudo apt install build-essential, it will require a bit more work on Windows and Mac."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I'm trying to decide whether to use typeguard or stick to assert isinstance in the places where I care. Has anyone done benchmarking testing of the overhead of using type guards \"at\"typeguard decorator ?"
},
{
    "title": "No title",
    "content": "https://github.com/RadoTheProgrammer/pwdgen\nWhat my project does\n    My project generate simple, strong, memorable and easy-to-type passwords.\n  \n    The system is simple: it generate 2 pronounceable words separated by a special char, with a number at start or end.\n  \n    I tried creating a password generator that combines simplicity, security, memorability, and ease of type.\n  \n    This should be secure enough because it can generate 4e15 possibilities of passwords and uses the secrets module.\n  Target audience\n    For anyone who need to have passwords easily.\n  Comparison\n    Most passwords manager generate completely passwords with completely random characters that aren't very easy to memorize or tape.\n  \n    Examples include Dashlane, Norton, Avast.\n  \n    Or other like Bitwarden generate passwords that are not really fast-to-type.\n  \n    The mine generate sth like 7Xy-Bonuwucete 0Qubyby+Pomafy , or 7Zuxogu:Lebuwo .\n  Usage\n    You can install it with pip install pwd-generator and use the cli version:\n  pwdgen\n    To use it in a python code\n  import pwdgen\nprint(pwdgen.generate())Changes from last post\n    I already made another post for this, but this was not well received because my code use the random module.\n  \n    I updated it to use the secrets module."
},
{
    "title": "No title",
    "content": "Hi all,\n  \n    I used polars (a Rust-powered “Blazingly Fast DataFrame Library”) to analyze seven years of my own music royalties data. Vega-Altair (Python wrapper for Vega-Lite) powers the (often interactive) visualizations.\n  \n    Link to the article: https://osc.garden/blog/data-analysis-music-streaming/\n\n    It was a lot of fun learning polars and setting up the graphs with Vega-Altair.\n  \n    Would love to hear any comments and suggestions.\n  \n    Link to r/dataisbeautiful post: https://www.reddit.com/r/dataisbeautiful/comments/1ca7x6z/oc_my_music_needs_more_than_200k_plays_on/"
},
{
    "title": "No title",
    "content": "PyPiSource\n\nWhat My Project Doesfind_where is a Python package that provides a function to find values in dictionaries where a specified key matches a given value, similar to filtering in SQL.\n  \nTarget AudienceThis is my first attempt at creating a Python package so I would describe this as a toy project at this stage but am definitely looking for feedback from the wider community.\n  \nComparisonI mainly wrote this package because I kept on writing the same iterable based code when trying to find a value, given a key:\n  data = {\n\"people\": [\n    {\"first_name\": \"John\", \"last_name\": \"Smith\", \"age\": 25},\n    {\"first_name\": \"Alice\", \"last_name\": \"Jones\", \"age\": 32},\n    ]\n}\n\nfirst_name = None\n\nfor result in data[\"people\"]:\n    if result[\"age\"] == 32:\n        first_name = result[\"first_name\"]\n        break\n\nprint(first_name)\n    When using find_where, you can simply run:\n  from find_where import find_where\n\ndata = {\n    \"people\": [\n        {\"first_name\": \"John\", \"last_name\": \"Smith\", \"age\": 25},\n        {\"first_name\": \"Alice\", \"last_name\": \"Jones\", \"age\": 32},\n    ]\n}\n\nfirst_name = find_where(data[\"people\"], \"first_name\", age=32)\n\nprint(first_name)\n    Appreciate any feedback, constructive or otherwise!"
},
{
    "title": "No title",
    "content": "I have started at a new job and had the idea that it would probably be clever to set up my developing environment in exactly the same way as my predecessor did. Because:\n  \n\n\n    This should help resolving errors quicker in the transition period\n  \n\n\n    His code was good and clean and it appears that he knows what he is doing\n  \n\n\n    we were using mostly the same tools (VScode etc.) anyways.\n  \n\n\n    He set up his virtual environments (VE)s with conda/mamba. I vaguely remembered that I also used to do that but then stopped for some reason and switched to the virtualenv package. But I did not remember why anymore. So I just set up my VEs in the same way, it should not really make any difference anyways (so I thought). Well, fast forward about two weeks and now I have VEs that occasionally (but not always) exist twice in the same folders under the same name (according to mamba info --envs) and that are at the same time completely empty (according to mamba list) and contain all packages I have installed anywhere, ever (according to pip list). I usually install packages via pip and I assume this may have fucked things up in combination with mamba? I'll probably switch back to virtualenv again and add a \"do not use conda/mamba VEs !!!\" in my notes. I am working on Windows. Is mamba better on Linux?"
},
{
    "title": "No title",
    "content": "I'm sharing some of the Python tutorials I made.  I teach Ukrainian teenagers and university students Python programming and data science for free.\n  \nHere is an introduction to Pandas.\n  \n    It shows:\n  \n\n\n    How to read data into a Pandas dataframe from a CSV file.\n  \n\n\n    Show how to print out the column names.\n  \n\n\n    See and set the index.\n  \n\n\n    Print sections of the dataframe.\n  \n\n\n    Do basic statistics.\n  \n\n\n    Create a dataframe from a dictionary."
},
{
    "title": "No title",
    "content": "I was wondering if anyone has come across anything like https://github.com/a-h/templ or https://hono.dev/guides/jsx, but for python. For context, I am familiar with jinja2, mako, etc. but find them to be unintuitive due to the loose coupling of logic (ex: database calls) and templating (ex: generating a list from the database results). Therefore, I am looking for a \"inline\" templating solution."
},
{
    "title": "No title",
    "content": "https://github.com/leftmove/cria\n\n    My name is Anonyo, and I am a seventeen year old from Southeast Michigan. This is my second open source project. I built Cria, a Python library that allows you to run LLMs programmatically through Python. Cria is designed so there is as little friction as possible — getting started takes just five lines of code.\n  \n    I created this library because I was using OpenAI in my project, and kept running into rate limits. With local LLMs getting better and better, I sought to switch, but found command line configurations to be limited. Running and configuring LLMs can be trivial, but programs like ollama make it easier. The only problem I found with ollama though, was the lack of features in its Python client.\n  \n    To counteract this, I built Cria — a program that is concise, efficient, and most importantly programmable. The name is based off of Meta's llama, as baby llamas are called crias. Cria solves pain points of ollama - it manages the ollama server for you, saves your message history, streams responses by default, and allows easier management of multiple LLMs at once.\n  \n    The target audience for this project is anyone that uses AI. This library was built in a weekend, and is meant to make running LLMs easier for anyone that wants to go local. I built this for myself, but it could be used at scale as ollama runs its own server (there would of course have to be extra configuration though).\n  \n    In Python, while there are libraries to run LLMs, they are feature limited. Ollama provides the most extensive toolset, and so Cria uses that. Ollama has its own Python library, but I found that to be cumbersome when running multiple LLMs. There are similar libraries, but existing alternatives involve strictly the command line (from what I've seen).\n  \n    While this isn't my first open source project, it's my first Python library. The code may be a little rough around the edges, so I would appreciate any and all suggestions.\n  \n    Thank you!"
},
{
    "title": "No title",
    "content": "Hi there,\n  \n    My team at Bytewax and I have been working on a series of hands-on guides on streaming data and I am excited to share how one can handle missing values in real-time in Python.\n  \n    While some parts of the guide are simplified, for example, we use a random number generator as an input source, the algorithmic part is production-ready.\n  \n    We are taking advantage of the Numpy library + stateful operators in Bytewax.\n  \n    Other input sources are available, too.\n  \nhttps://bytewax.io/guides/handling-missing-values"
},
{
    "title": "No title",
    "content": "What My Project Does\n    Molly is designed to monitor time series data in a SQL database such as MySQL or PostgreSQL.\n  \n    Currently supported data quality feature:- staleness: when the data was last updated- completeness: whether any data is missing in a given date range\n  \n    Currently support messaging service:- Slack Web API\n  \n    Currently support scheduling service:- Apache Airflow- crontab\n  Target Audience\n    people in need of a plug-n-play dq monitoring service\n  Comparison\n    It's open source. It's free. It's written in pure python. And it's made for modern data stack so you can try it out with minimum effort.\n  \nI'd appreciate it if you could check it out on GitHub. All feedback and/or contributions are welcomed!\n\n    GitHub: https://github.com/flaviaouyang/molly"
},
{
    "title": "No title",
    "content": "For my application, Scipy's solvers are not fast enough so I am looking to speed up by using another package. These are some packages I have found so far:\n  \n\n\nNumbaLSODA\n\n\n\nDifferentialEquations.jl\n\n\n\nTorchquad(Although doesn't solve ODEs, just integrates quickly, but that could be wrapped I guess)\n  \n\n\nTorchdiffeq\n\n\n\nPyDSTool\n\n\n\n    I will be experimenting with these packages, but I was wondering whether anyone has experience with them and has done work on them before. Also, I am wondering whether there are any packages I should check out?"
},
{
    "title": "No title",
    "content": "New developers print out strings to their terminal. It’s how we learn! But printing out to the terminal isn’t what you do with most professional applications.\n  \n    In those cases, you log into files. Sometimes, you log into multiple locations at once. These logs may serve as an audit trail for compliance purposes or help the engineers debug what went wrong.\n  \nPython Logging teaches you how to log in the Python programming language. Python is one of the most popular programming languages in the world. Python comes with a logging module that makes logging easy.\n  What You’ll Learn\n    In this book, you will learn how about the following:\n  \n\n\n    Logger objects\n  \n\n\n    Log levels\n  \n\n\n    Log handlers\n  \n\n\n    Formatting your logs\n  \n\n\n    Log configuration\n  \n\n\n    Logging decorators\n  \n\n\n    Rotating logs\n  \n\n\n    Logging and concurrency\n  \n\n\n    and more!\n  \n\nBook formats\n    The finished book will be made available in the following formats:\n  \n\n\n    paperback (at the appropriate reward level)\n  \n\n\n    PDF\n  \n\n\n    epub\n  \n\n\n    The paperback is a 6″ x 9″ book and is approximately 150 pages long.\n  \n    You can check out the book's campaign on Kickstarter.\n  \n    Let me know if you have any questions!"
},
{
    "title": "No title",
    "content": "Just came across this list and I think it's amazing: https://techtalksweekly.substack.com/p/all-pydata-2023-talks"
},
{
    "title": "No title",
    "content": "I have not seen updates in almost a year and there is no activity in the support forum? Is there a replacement forum? Thanks"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I have been programming for a few years now and have on and off had jobs in the industry. I used Jupyter Notebook in undergrad for a course almost a decade ago and I found it really cool. Back then I really didn’t know what I was doing and now I do. I think it’s cool how it makes it feel more like a TI calculator (I studied math originally)\n  \n    What are jobs that utilize this? What can I do or practice to put myself in a better position to land one?"
},
{
    "title": "No title",
    "content": "I made a detailed tutorial on how to work with missing data, outliers, and how to lean up data in Pandas Python.  This explains how to clean up your data so that your statistical results and trainin data is not skewed.\n  \n\nhttps://github.com/werowe/HypatiaAcademy/blob/master/pandas/pandas_missing_data.ipynb"
},
{
    "title": "No title",
    "content": "Tkinter is going to drive me crazy.\n  \n    Let me lay this out for you:\n  \n    Tkinter is the GUI interface I am using, which is a python frontend for a C library\n  \n    It holds \"widgets\" in a window.\n  \n    Some widgets have variables (StringVar, BooleanVar, etc - not \"variables\" like code variables but a class of updatable data holders for the widgets to link to the Tcl backend)\n  \n    The Vars have incremental names like PYVAR1, PYVAR93, etc by default.\n  \n    Names can be provided to the Vars via keywords upon initialization.  (Note I have not been able to find any reason to not do this - hence this post)\n  \n    Names should absolutely not be provided to the Vars unless they are unique.  Because on python garbage collection (or \"del Var\" command) the Vars are told to destroy() themselves and then this command is passed to C \"destroy this Var.\"\n  \n    But the garbage collection in C (or rather Tcl) is slightly delayed from Python...\n  \n    So if I immediately I instantiate a different Var with a different value (or same value, who knows) but reusing the same name, the Var has been deleted in Python (yey!) but it has not been deleted in C yet (uh oh).  C says to Python oh hey I got this Var already, use it, don't make a new one. So python goes to use it and then OOPS NEVERMIND LOL IT'S DELETED IN C HAHAHA\n  \n    So now I have this Var that I can't access in Python because it's a Schrodinger's Variable: it both exists and does not exist.  Wonky behavior too, like check boxes that are in no state at all, neither checked nor unchecked, and I can't access the variable to see what state it is in because the variable that existed before the one I'm using no longer actually exists.  So the Var I can access looks 100% fine!  But if I put a value checker in just the right place I can catch where the wonky behavior exists.\n  \n    This has been a frustrating several weeks.\n  \n    Anyway.  I have not been able to find any information about this and thought I'd share.  Hopefully this helps someone avoid the mistake I made.  I have since set all Var names to be the incremental default names.\n  \n    Edit: after quite a bit of experimentation I have discovered the issue is not in the Variable naming but in the fact that I had the Variables stored as attributes in my custom widget classes.  The attributes were not being cleaned up properly on widget destroy().  The simple solution is to pop the variable attribute from the widget dict before child.destroy()."
},
{
    "title": "No title",
    "content": "Hi, I'm Owen, and I've developed an AI tool for data processing.\n  \n    I spend a significant amount of time processing daily data from numerous Excel files, which requires me to write a lot of Python code.\n  \n    I have been using GitHub Copilot to assist with this, but I am exploring ways to eliminate the need to write code manually.\n  \n    The tool is called Tipis AI, and you can find it here: https://github.com/tipisai/tipis-fe\n\nWhat My Project Does\n\n    Therefore, I have developed an AI that generates Python code for data processing. I've also created a Python runtime environment to execute this code.\n  \n    You can use this tool to process any type of files.\n  \nTarget Audience\n\n    Any worker. Someone needs to process files.\n  \nComparison\n\n    You can perform complex calculations and summarizations on files using natural language descriptions, without needing to master complex Python knowledge\n  \n\n    If you have any suggestions for AI in data processing, please let me know, and I might include them in this product.I am very interested in code generation."
},
{
    "title": "No title",
    "content": "Golem Network has just released Ray on Golem MVP, a significant milestone in integrating Ray with Golem's decentralized infrastructure! 🎉Check out all the details about the release and next steps in our blog post:📝 https://blog.golem.network/announcing-ray-on-golem-mvp-launch-the-recommended-way-of-running-python-code-on-the-golem-network/👉 Golem is looking for engaged beta testers to run diverse applications and collaborate on refining the solution. Interested? Find all the information on the brand-new Ray on Golem site:🔗 https://www.golem.network/ray"
},
{
    "title": "No title",
    "content": "Pydantic makes your code safer by making it strongly typed. You can no longer input a wrongly typed argument without getting an error (if pydantic can't convert it). This is great but to me it seems that sometimes standard python classes still seem preferable.\n  \n    Perhaps it's because I'm not using it correctly but my code for a pydantic class is much longer then for a normal class. Especially if you are working with computed attributes. Then you have to start using special decorators and for every computed attribute you have to declare a function with \"def ...\" Instead of in an init function just being able to write attribute_3 = attribute 1 + attribute 2.\n  \n    So I'm just wondering are you using pydantic for all your classes? And how do you handle computed fields in pydantic especially upon instantiation I find it hard to implement."
},
{
    "title": "No title",
    "content": "Interactive data visualization is a powerful tool that can significantly enhance the analysis and interpretation of complex datasets. With Python, the Plotly library offers various features that can be used to create interactive publication-quality graphs.\n  \n    This project demonstrates how to use 2 of these awesome features:\n  \n\n\nRange Slider\n\n\n\nDropdown Menu\n\n\n\n    A good dataset is essential to demonstrate this style of interactivity. For this project, we will use the UN population projection data to sort and visualize by country for various age groups.\n  \nFREE ARTICLE:\n\nhttps://johnloewen.substack.com/p/python-plotly-combining-dropdowns-and-range-sliders-for-user-interaction-658dc6fd9c71"
},
{
    "title": "No title",
    "content": "After 10+ years working with it, I keep discovering new features. This is a list of the most recent ones: https://jcarlosroldan.com/post/329"
},
{
    "title": "No title",
    "content": "A project to build a local server without any external dependencies with main features of hot reloading browser on source modification. The idea is to make it simple, standalone and work without any setup.\n  \n    The project uses regular sockets, websockets, inotify and a bit of javascript on the fly embedding to allow users to achieve automatic synchronization with browser. The article describes hot to monitor file changes with direct loading of libc and requesting kernel notify on change.\n  \n    Free article and code: https://hereket.com/posts/linux_live_reload_python_server/"
},
{
    "title": "No title",
    "content": "I found this awesome AI framework for python called \"Crew AI\", that allows us to create assistants, or agents in the CrewAI lingo, and assign specific tasks.\n  \n    I started my tiny youtube channel 3 months and for a youtuber, especially a novice one like myself, there are a bunch of tasks such as checking what other similar channels are doing, which topics are trending and if a video idea is good or not.\n  \n    There is also the part of coming up with the video content idea itself, create a catchy title, suitable youtube tags and finding the appropriate forum to talk about! You folks get the picture.\n  \n    It's a great project for anyone seeking to automate tasks not just related to a youtube channel, but this could easily be adapted to your daily job tasks, or even a startup idea.\n  \n    I have written a full article about how the project unfolds and also a youtube video for how I did it step by step, which you are welcome to check it out! The links are as follows:\n  \n    The medium article: https://medium.digitalmirror.uk/create-an-ai-team-to-manage-your-youtube-channel-5dc1e6c9b31b\n\n    The YouTube video: https://youtu.be/5JoVeYcxgpU\n\n    And of course the source code: https://github.com/fmiguelmmartins/crewaiyoutube.git\n\n    As always, if you would like to drop some feedback so I can improve with time I would be grateful! Cheers"
},
{
    "title": "No title",
    "content": "Link: https://github.com/marzooqy/anycrc\n\nWhat My Project Does\n\n    This is a Cython module with bindings to the crcany library. It supports calculating CRC hashes of arbitary sizes as well as updating a crc hash over time, and I believe that it is the fastest generic CRC library available for Python.\n  \nTarget Audience\n\n    Anyone that needs to compute CRCs.\n  \nComparison with Alternatives\n\ncrcmod-plus: A modernized version of the old crcmod library. It's highly customizable and fast.\n  \nfastcrc: A library with bindings to the crc-rs library. Has a limited selection of CRCs to use.\n  \nbinascii: The standard library module comes with support for only CRC32 and CRC-CCITT, and at least the CRC32 implementation used is pretty fast according to my testing.\n  \nCrcEngine: If you are looking for a pure Python implementation of CRC, then this is a good option. It is however one to two orders of magnitude slower than the libraries that use C/Rust bindings.\n  \ncrc: Popular but extremely inefficient CRC library. It does several things that severely slow it down, such as using an entire class to represent a byte. It's three orders of magnitude slower than the faster options."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hello!I work in big data space (data engineering), I mainly used Java, Scala and Python.I have been learning functional programming in greater depth and I found this Python library which seems pretty cool.https://github.com/dry-python/returnsI've used it at work for implementing an Either based error handling.It seems a great library.Any of you have used it?Any thoughts?For sure, I prefer doing FP in Scala but given the job market isn't too kind too Scala and FP languages in general. What are your thoughts to bring FP (at least parts of it) to the Python world?Some people in the TypeScript world seem to take that direction:https://github.com/Effect-TS/effect"
},
{
    "title": "No title",
    "content": "Greetings!\n  \n    I've previously introduced my Python package in this post. Since then, I've significantly enhanced its performance and expanded its capabilities.\n  What My Project Does\n    A lightweight python package designed for the efficient retrieval of financial data from various sources like Yahoo Finance, Nasdaq, Digrin and JustETF websites (somewhat similar to yfinance). This tool provides functionality akin to yfinance but with broader data access.\n  Main benefits of stockdex over yfinance\n\n\nFresh data: Yahoo Finance often delays updates to financial data by several days while Nasdaq and other sources typically update on the day reports are released. Stockdex enables access to this fresher data, such as quarterly earnings.\n  \n\n\nBroader Data Sources: Unlike yfinance which relies solely on the Yahoo Finance API, Stockdex aggregates data from multiple platforms including Digrin, JustETF, Nasdaq, and Yahoo Finance. For specific examples of data retrieval, refer to this readme.\n  \n\n\nAccess to Historical Data: Yahoo Finance limits access to the most recent five annual or four quarterly reports. Stockdex, however, taps into sources that maintain extensive historical archives not available through Yahoo Finance.\n  \n\nTarget Audience\n    The package is targeted at people who are interested in financial analysis using python.\n  \n    Explore more:\n  \n\n\nGithub Repo Link\n\n\n\nPypi link"
},
{
    "title": "No title",
    "content": "What My Project Does:pytablericons is a Python wrapper for the tabler-icons project, which provides a set of 5237 free MIT-licensed high-quality SVG icons for web projects with each icon being designed on a 24x24 grid and a 2px stroke.It allows you to load any of those SVG icons directly into a Pillow PNG Image in any size and color without losing quality and without having to download the icons manually or handling the SVG conversion yourself. For the outline icons, the stroke width is customizable as well. All of this is done in just a single line and since the icon is a Pillow Image, it can then be used easily with basically anything (e.g. PyQt5, PyQt6, PySide2, PySide6, Tkinter, etc.)\n  \nTarget Audience:This is useful for any Python developer who is working on UI projects and wants to easily display modern icons in any size and color without losing quality and without having to download each icon manually.\n  \nComparison: /\n  \nLinks:Preview: https://github.com/niklashenning/pytablericons/assets/58544929/e13fb020-4d5f-4e28-bd5f-0d5659bd6582GitHub: https://github.com/niklashenning/pytablericons"
},
{
    "title": "No title",
    "content": "https://github.com/RadoTheProgrammer/motsmeles/\nWhat my project does\n    It generate word search games with custom words, dimensions.\n  Target audience\n    For people who want to get fun with some word searching\n  Comparison\n    This is a python library, it's open source, and you can use it in a python code, or with the command-line.\n  \n    Also, you have freedom if you want to have diagonal or reversed words."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "difflib is popular. Most of the answers on Stack Overflow uses it. But, I think Google's diff-match-patch is better. See an example here. Or, am I missing something?"
},
{
    "title": "No title",
    "content": "What My Project Does:\n  \n    For everyone who says they use jupyter Notebook for Python to do chunks of code work… I developed my own data management system for that kind of stuff that provides a wrapper over serialization that essentially creates pythonic pointers to data on any network drive. I just grab my data that I want to grab. Provides autocomplete and C# like attributes. I can dynamically build any data storage class using UML formatted infrastructure-as-code to build the front end. The back end is completely dynamic. Here is the pypi link. Note that I developed this to handle genomic data that was generated over the course of a week of 24/7 computation, that I break into multiple logical chunks. You dont need to rely on Jupyter to hold your data and you can retrieve it months after generating it.\n  \n    This software routes the data to whatever means you use. It works with databases, pickles, yaml. You just have to add your interface in the data routing class. I just did yaml to start. This is just a wrapper around everything so you can declare your class like normal and at variable declaration you have data permanence instantly with no user interaction.\n  \n    If you pass Y=1 and power off your computer and turn it back on again Y still equals 1 when you call that variable instantly\n  \n    Target Audience (e.g., Is it meant for production, just a toy project, etc.)\n  \n    production as I use it for my genomic research on my project with Stanford\n  \n    Comparison (A brief comparison explaining how it differs from existing alternatives.):\n  \n    There is no alternatve to this project as far as i know. It replaces the need for Jupyter Notebook to hold your data as you can retrieve it months after generating it.\n  \n    PyPi:\n  \nhttps://pypi.org/project/data-nut-squirrel/\n\n    Source Code:\n  \nhttps://github.com/LunarFawn/rna_squirrel\n\n    Edit: Here is an example of implementation where I actually gave a shit about documentation.\n  \nhttps://pypi.org/project/serena-rna-tool/"
},
{
    "title": "No title",
    "content": "Release notes here, seems to be a 20-40% improvement around the board.\n  \n    This version features a new hand-written parser (rather than a generated one) that is much faster and offers better error messages. It also comes with a new rust-native language server inspired by rust-analyzer, that is multithreaded. I think they’re challenging Pylance’s throne, wouldn’t be surprised if the team goes after type checking next."
},
{
    "title": "No title",
    "content": "Hey everyone! I've written an article that simplifies the concept of state machines using Python, with a practical example related to order statuses. If you've ever been confused about state machines or just want a refresher with a real-world application, this might be just what you're looking for. Check it out and let me know what you think!Read the full article here\n\n    I'm here for any questions or discussions"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Python’s Plotly Dash is a powerful tool for creating interactive data visualizations. As a Comp Sci professor, I use it extensively for interactive dashboards.\n  \n    Its usefulness lies in its ability to create web-based applications directly from Python code, without the need for additional web development skills.Using UN food security data, let me show you an example of how you can create an interesting and useful Python Plotly dashboard that tells a data story using:\n  \n\n\nA bubble chart: with variable-sized markers, a bubble chart can represent the severity of undernourishment within a given country. Each bubble visually represents the scale of undernourishment.\n  \n\n\nA horizontal bar chart: with its rigid organized structure, a horizontal bar chart provides a clear ordered list that emphasizes the 10 most undernourished countries by percentage of population.\n  \n\n\nA dropdown menu: by Year, allowing the user to see the changing story over a period of time.\n  \n\n\n    These two data visualizations offer a dual perspective on the global picture of undernourishment: one that is geographically broad and another that is focused and comparative.\n  \nHere's a step-by-step project on how to put this all together.\n\n    FREE ARTICLE:\n  \nhttps://johnloewen.substack.com/p/combining-data-visuals-an-interactive"
},
{
    "title": "No title",
    "content": "https://github.com/devdave/pyminder\nWhat does it do?\n    The application is a very simple time tracker broken down by Client->Project->Task.  In addition it has a summary report view plus a simple CRUD like data manager.     The bigger goal was to see what it entailed to mix Python w/ReactJS and if it was easier using HTML+TypeScript over QT, TK, or some other unique API.\n  Target Audience\n    While I am using PyMinder to track my own work it wasn't meant for a general audience.   Instead I am sharing this in case anyone else has a need to make a desktop application and looking for alternatives.\n  Comparison\n    I have decades of experience with HTML & Ecma script plus I have found very little benefit is gained learning yet another way to make a user interface.   In addition there is more resources for HTML+JS tools and support.\n  \n    One thing to note, if the debug console is closed, the app generally consumes only 300MB.   I was expecting it to be more given the abstraction layers and behind the scenes it is an instrumented web browser instance.\n  Future sub project\n    90% of the code is type annotated but that 10% is a \"here be monsters\" component that scans a Python class and makes a Typescript API-Bridge.   Transformer works but it is very much hacked together with 2 unit tests that barely cover 5% of it. That said, I would like to rip that part out, clean it up, and make it its own thing as I could see some use for a Python to Typescript adapter library."
},
{
    "title": "No title",
    "content": "I'm interested to create a software where I can import some construction drawings and measure the distance within the PDF as a first step of my work. Tried a few open source pdf but none of them still allows me to have a correct measurement on the PDF, anyone have an idea?"
},
{
    "title": "No title",
    "content": "Hello ! Earlier this year I made a post about my open source project PyPDFForm and got some really nice feedbacks from you guys.\n  \n    I have been since then continuously working on it and I'd love to share you two really cool features that were newly added to the library.\n  \n    The first one is, like what the title says, the library finally supports creating a subset of widgets through code now. One of the previous hard dependency for PyPDFForm was that it requires a PDF template that was prepared using another tool, namely Adobe Acrobat or some web based ones like DocFly. Well now, at least for text field, checkbox, and dropdown, you can do it through plain Python code. I personally find this a huge milestone of the project and if you are interested in this new feature checkout the documentation here.\n  \n    The second new feature, which is actually not quite new because this was how PyPDFForm worked back in its ancestral stage, is that now you can fill a PDF form \"in place\", meaning when you fill it this way, the output PDF will look like as if it's filled manually. The reason why this got removed was because of a bug related to text field, where when filled this way the text that got put into a text field will not show up unless the text field is actually clicked by mouse and selected. Fortunately I was enlightened by another user from this sub last time I posted here and he/she gave me a solution on this weird text field bug. Thanks to that, I was able to bring this old but yet new feature back to the library for those who just wants to simply fill a PDF form. Again if you are interested, it's documented here.\n  \n    If you are interested in any of these new features or just the library in general, feel free to go checkout the newest release of the project, try it, test it, and leave comments or suggestions. And of course if you are willing, a star on GitHub is always kindly appreciated."
},
{
    "title": "No title",
    "content": "I recently open-sourced a package and the tooling was a bit of a struggle. I decided to write down the steps & all the tools needed to open-source a Python package in a scalable way that invite users and contributors.\n  \nhttps://jonathanadly.com/open-sourcing-a-python-project-the-right-way-in-2024\n\n    Happy to hear your feedback!"
},
{
    "title": "No title",
    "content": "Here is just a small wrapper to interact with the new MetaAI chat bot assistant with Python (https://www.meta.ai/), which is running the newly release Llama 3 model.\n  \n    Another nice thing is that its directly connected with Bing Search so you will be able to get the latest informations.\n  \nhttps://github.com/Strvm/meta-ai-api"
},
{
    "title": "No title",
    "content": "Does anyone know what’s going on behind the scenes of PyPI (who maintains `pip install`)? It seems like they’ve stopped processing support tickets over a month ago.\n  \n    I’ve hit the data limit for my package [PyBoy](https://github.com/baekalfen/pyboy), and I immediately posted a ticket to get the limit increased (as others have successfully done). But after more than a month, and several attempts at contacting the support team, I’ve heard nothing back, and I’ve run out of options.\n  \n    Does anyone know what’s happening, or how to get a comment from PyPI?\n  \n    My ticket is this one: https://github.com/pypi/support/issues/3757"
},
{
    "title": "No title",
    "content": "Article link: https://rishiraj.me/articles/2024-04/python_subinterpreter_parallelism\n\n    I have written an article, which should be helpful to folks at all experience levels, covering various multi-tasking paradigms in computers, and how they apply in CPython, with its unique limitations like the Global Interpreter Lock. Using this knowledge, we look at traditional ways to achieve \"true parallelism\" (i.e. multiple tasks running at the same time) in Python.\n  \n    Finally, we build a solution utilizing newer concepts in Python 3.12 to run any arbitrary pure Python code in parallel across multiple threads. All the code used to achieve this, along with the benchmarking code are available in the repository linked in the blog-post.\n  \n    This is my first time writing a technical post in Python. Any feedback would be really appreciated! 😊"
},
{
    "title": "No title",
    "content": "Pydantic makes your code safer by making it strongly typed. You can no longer input a wrongly typed argument without getting an error (if pydantic can't convert it). This is great but to me it seems that sometimes standard python classes still seem preferable.\n  \n    Perhaps it's because I'm not using it correctly but my code for a pydantic class is much longer then for a normal class. Especially if you are working with computed attributes. Then you have to start using special decorators and for every computed attribute you have to declare a function with \"def ...\" Instead of in an init function just being able to write attribute_3 = attribute 1 + attribute 2.\n  \n    So I'm just wondering are you using pydantic for all your classes? And how do you handle computed fields in pydantic especially upon instantiation I find it hard to implement."
},
{
    "title": "No title",
    "content": "Interactive data visualization is a powerful tool that can significantly enhance the analysis and interpretation of complex datasets. With Python, the Plotly library offers various features that can be used to create interactive publication-quality graphs.\n  \n    This project demonstrates how to use 2 of these awesome features:\n  \n\n\nRange Slider\n\n\n\nDropdown Menu\n\n\n\n    A good dataset is essential to demonstrate this style of interactivity. For this project, we will use the UN population projection data to sort and visualize by country for various age groups.\n  \nFREE ARTICLE:\n\nhttps://johnloewen.substack.com/p/python-plotly-combining-dropdowns-and-range-sliders-for-user-interaction-658dc6fd9c71"
},
{
    "title": "No title",
    "content": "After 10+ years working with it, I keep discovering new features. This is a list of the most recent ones: https://jcarlosroldan.com/post/329"
},
{
    "title": "No title",
    "content": "A project to build a local server without any external dependencies with main features of hot reloading browser on source modification. The idea is to make it simple, standalone and work without any setup.\n  \n    The project uses regular sockets, websockets, inotify and a bit of javascript on the fly embedding to allow users to achieve automatic synchronization with browser. The article describes hot to monitor file changes with direct loading of libc and requesting kernel notify on change.\n  \n    Free article and code: https://hereket.com/posts/linux_live_reload_python_server/"
},
{
    "title": "No title",
    "content": "I found this awesome AI framework for python called \"Crew AI\", that allows us to create assistants, or agents in the CrewAI lingo, and assign specific tasks.\n  \n    I started my tiny youtube channel 3 months and for a youtuber, especially a novice one like myself, there are a bunch of tasks such as checking what other similar channels are doing, which topics are trending and if a video idea is good or not.\n  \n    There is also the part of coming up with the video content idea itself, create a catchy title, suitable youtube tags and finding the appropriate forum to talk about! You folks get the picture.\n  \n    It's a great project for anyone seeking to automate tasks not just related to a youtube channel, but this could easily be adapted to your daily job tasks, or even a startup idea.\n  \n    I have written a full article about how the project unfolds and also a youtube video for how I did it step by step, which you are welcome to check it out! The links are as follows:\n  \n    The medium article: https://medium.digitalmirror.uk/create-an-ai-team-to-manage-your-youtube-channel-5dc1e6c9b31b\n\n    The YouTube video: https://youtu.be/5JoVeYcxgpU\n\n    And of course the source code: https://github.com/fmiguelmmartins/crewaiyoutube.git\n\n    As always, if you would like to drop some feedback so I can improve with time I would be grateful! Cheers"
},
{
    "title": "No title",
    "content": "Link: https://github.com/marzooqy/anycrc\n\nWhat My Project Does\n\n    This is a Cython module with bindings to the crcany library. It supports calculating CRC hashes of arbitary sizes as well as updating a crc hash over time, and I believe that it is the fastest generic CRC library available for Python.\n  \nTarget Audience\n\n    Anyone that needs to compute CRCs.\n  \nComparison with Alternatives\n\ncrcmod-plus: A modernized version of the old crcmod library. It's highly customizable and fast.\n  \nfastcrc: A library with bindings to the crc-rs library. Has a limited selection of CRCs to use.\n  \nbinascii: The standard library module comes with support for only CRC32 and CRC-CCITT, and at least the CRC32 implementation used is pretty fast according to my testing.\n  \nCrcEngine: If you are looking for a pure Python implementation of CRC, then this is a good option. It is however one to two orders of magnitude slower than the libraries that use C/Rust bindings.\n  \ncrc: Popular but extremely inefficient CRC library. It does several things that severely slow it down, such as using an entire class to represent a byte. It's three orders of magnitude slower than the faster options."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hello!I work in big data space (data engineering), I mainly used Java, Scala and Python.I have been learning functional programming in greater depth and I found this Python library which seems pretty cool.https://github.com/dry-python/returnsI've used it at work for implementing an Either based error handling.It seems a great library.Any of you have used it?Any thoughts?For sure, I prefer doing FP in Scala but given the job market isn't too kind too Scala and FP languages in general. What are your thoughts to bring FP (at least parts of it) to the Python world?Some people in the TypeScript world seem to take that direction:https://github.com/Effect-TS/effect"
},
{
    "title": "No title",
    "content": "Greetings!\n  \n    I've previously introduced my Python package in this post. Since then, I've significantly enhanced its performance and expanded its capabilities.\n  What My Project Does\n    A lightweight python package designed for the efficient retrieval of financial data from various sources like Yahoo Finance, Nasdaq, Digrin and JustETF websites (somewhat similar to yfinance). This tool provides functionality akin to yfinance but with broader data access.\n  Main benefits of stockdex over yfinance\n\n\nFresh data: Yahoo Finance often delays updates to financial data by several days while Nasdaq and other sources typically update on the day reports are released. Stockdex enables access to this fresher data, such as quarterly earnings.\n  \n\n\nBroader Data Sources: Unlike yfinance which relies solely on the Yahoo Finance API, Stockdex aggregates data from multiple platforms including Digrin, JustETF, Nasdaq, and Yahoo Finance. For specific examples of data retrieval, refer to this readme.\n  \n\n\nAccess to Historical Data: Yahoo Finance limits access to the most recent five annual or four quarterly reports. Stockdex, however, taps into sources that maintain extensive historical archives not available through Yahoo Finance.\n  \n\nTarget Audience\n    The package is targeted at people who are interested in financial analysis using python.\n  \n    Explore more:\n  \n\n\nGithub Repo Link\n\n\n\nPypi link"
},
{
    "title": "No title",
    "content": "What My Project Does:pytablericons is a Python wrapper for the tabler-icons project, which provides a set of 5237 free MIT-licensed high-quality SVG icons for web projects with each icon being designed on a 24x24 grid and a 2px stroke.It allows you to load any of those SVG icons directly into a Pillow PNG Image in any size and color without losing quality and without having to download the icons manually or handling the SVG conversion yourself. For the outline icons, the stroke width is customizable as well. All of this is done in just a single line and since the icon is a Pillow Image, it can then be used easily with basically anything (e.g. PyQt5, PyQt6, PySide2, PySide6, Tkinter, etc.)\n  \nTarget Audience:This is useful for any Python developer who is working on UI projects and wants to easily display modern icons in any size and color without losing quality and without having to download each icon manually.\n  \nComparison: /\n  \nLinks:Preview: https://github.com/niklashenning/pytablericons/assets/58544929/e13fb020-4d5f-4e28-bd5f-0d5659bd6582GitHub: https://github.com/niklashenning/pytablericons"
},
{
    "title": "No title",
    "content": "https://github.com/RadoTheProgrammer/motsmeles/\nWhat my project does\n    It generate word search games with custom words, dimensions.\n  Target audience\n    For people who want to get fun with some word searching\n  Comparison\n    This is a python library, it's open source, and you can use it in a python code, or with the command-line.\n  \n    Also, you have freedom if you want to have diagonal or reversed words."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "difflib is popular. Most of the answers on Stack Overflow uses it. But, I think Google's diff-match-patch is better. See an example here. Or, am I missing something?"
},
{
    "title": "No title",
    "content": "What My Project Does:\n  \n    For everyone who says they use jupyter Notebook for Python to do chunks of code work… I developed my own data management system for that kind of stuff that provides a wrapper over serialization that essentially creates pythonic pointers to data on any network drive. I just grab my data that I want to grab. Provides autocomplete and C# like attributes. I can dynamically build any data storage class using UML formatted infrastructure-as-code to build the front end. The back end is completely dynamic. Here is the pypi link. Note that I developed this to handle genomic data that was generated over the course of a week of 24/7 computation, that I break into multiple logical chunks. You dont need to rely on Jupyter to hold your data and you can retrieve it months after generating it.\n  \n    This software routes the data to whatever means you use. It works with databases, pickles, yaml. You just have to add your interface in the data routing class. I just did yaml to start. This is just a wrapper around everything so you can declare your class like normal and at variable declaration you have data permanence instantly with no user interaction.\n  \n    If you pass Y=1 and power off your computer and turn it back on again Y still equals 1 when you call that variable instantly\n  \n    Target Audience (e.g., Is it meant for production, just a toy project, etc.)\n  \n    production as I use it for my genomic research on my project with Stanford\n  \n    Comparison (A brief comparison explaining how it differs from existing alternatives.):\n  \n    There is no alternatve to this project as far as i know. It replaces the need for Jupyter Notebook to hold your data as you can retrieve it months after generating it.\n  \n    PyPi:\n  \nhttps://pypi.org/project/data-nut-squirrel/\n\n    Source Code:\n  \nhttps://github.com/LunarFawn/rna_squirrel\n\n    Edit: Here is an example of implementation where I actually gave a shit about documentation.\n  \nhttps://pypi.org/project/serena-rna-tool/"
},
{
    "title": "No title",
    "content": "Release notes here, seems to be a 20-40% improvement around the board.\n  \n    This version features a new hand-written parser (rather than a generated one) that is much faster and offers better error messages. It also comes with a new rust-native language server inspired by rust-analyzer, that is multithreaded. I think they’re challenging Pylance’s throne, wouldn’t be surprised if the team goes after type checking next."
},
{
    "title": "No title",
    "content": "Hey everyone! I've written an article that simplifies the concept of state machines using Python, with a practical example related to order statuses. If you've ever been confused about state machines or just want a refresher with a real-world application, this might be just what you're looking for. Check it out and let me know what you think!Read the full article here\n\n    I'm here for any questions or discussions"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Python’s Plotly Dash is a powerful tool for creating interactive data visualizations. As a Comp Sci professor, I use it extensively for interactive dashboards.\n  \n    Its usefulness lies in its ability to create web-based applications directly from Python code, without the need for additional web development skills.Using UN food security data, let me show you an example of how you can create an interesting and useful Python Plotly dashboard that tells a data story using:\n  \n\n\nA bubble chart: with variable-sized markers, a bubble chart can represent the severity of undernourishment within a given country. Each bubble visually represents the scale of undernourishment.\n  \n\n\nA horizontal bar chart: with its rigid organized structure, a horizontal bar chart provides a clear ordered list that emphasizes the 10 most undernourished countries by percentage of population.\n  \n\n\nA dropdown menu: by Year, allowing the user to see the changing story over a period of time.\n  \n\n\n    These two data visualizations offer a dual perspective on the global picture of undernourishment: one that is geographically broad and another that is focused and comparative.\n  \nHere's a step-by-step project on how to put this all together.\n\n    FREE ARTICLE:\n  \nhttps://johnloewen.substack.com/p/combining-data-visuals-an-interactive"
},
{
    "title": "No title",
    "content": "https://github.com/devdave/pyminder\nWhat does it do?\n    The application is a very simple time tracker broken down by Client->Project->Task.  In addition it has a summary report view plus a simple CRUD like data manager.     The bigger goal was to see what it entailed to mix Python w/ReactJS and if it was easier using HTML+TypeScript over QT, TK, or some other unique API.\n  Target Audience\n    While I am using PyMinder to track my own work it wasn't meant for a general audience.   Instead I am sharing this in case anyone else has a need to make a desktop application and looking for alternatives.\n  Comparison\n    I have decades of experience with HTML & Ecma script plus I have found very little benefit is gained learning yet another way to make a user interface.   In addition there is more resources for HTML+JS tools and support.\n  \n    One thing to note, if the debug console is closed, the app generally consumes only 300MB.   I was expecting it to be more given the abstraction layers and behind the scenes it is an instrumented web browser instance.\n  Future sub project\n    90% of the code is type annotated but that 10% is a \"here be monsters\" component that scans a Python class and makes a Typescript API-Bridge.   Transformer works but it is very much hacked together with 2 unit tests that barely cover 5% of it. That said, I would like to rip that part out, clean it up, and make it its own thing as I could see some use for a Python to Typescript adapter library."
},
{
    "title": "No title",
    "content": "I'm interested to create a software where I can import some construction drawings and measure the distance within the PDF as a first step of my work. Tried a few open source pdf but none of them still allows me to have a correct measurement on the PDF, anyone have an idea?"
},
{
    "title": "No title",
    "content": "Hello ! Earlier this year I made a post about my open source project PyPDFForm and got some really nice feedbacks from you guys.\n  \n    I have been since then continuously working on it and I'd love to share you two really cool features that were newly added to the library.\n  \n    The first one is, like what the title says, the library finally supports creating a subset of widgets through code now. One of the previous hard dependency for PyPDFForm was that it requires a PDF template that was prepared using another tool, namely Adobe Acrobat or some web based ones like DocFly. Well now, at least for text field, checkbox, and dropdown, you can do it through plain Python code. I personally find this a huge milestone of the project and if you are interested in this new feature checkout the documentation here.\n  \n    The second new feature, which is actually not quite new because this was how PyPDFForm worked back in its ancestral stage, is that now you can fill a PDF form \"in place\", meaning when you fill it this way, the output PDF will look like as if it's filled manually. The reason why this got removed was because of a bug related to text field, where when filled this way the text that got put into a text field will not show up unless the text field is actually clicked by mouse and selected. Fortunately I was enlightened by another user from this sub last time I posted here and he/she gave me a solution on this weird text field bug. Thanks to that, I was able to bring this old but yet new feature back to the library for those who just wants to simply fill a PDF form. Again if you are interested, it's documented here.\n  \n    If you are interested in any of these new features or just the library in general, feel free to go checkout the newest release of the project, try it, test it, and leave comments or suggestions. And of course if you are willing, a star on GitHub is always kindly appreciated."
},
{
    "title": "No title",
    "content": "I recently open-sourced a package and the tooling was a bit of a struggle. I decided to write down the steps & all the tools needed to open-source a Python package in a scalable way that invite users and contributors.\n  \nhttps://jonathanadly.com/open-sourcing-a-python-project-the-right-way-in-2024\n\n    Happy to hear your feedback!"
},
{
    "title": "No title",
    "content": "Here is just a small wrapper to interact with the new MetaAI chat bot assistant with Python (https://www.meta.ai/), which is running the newly release Llama 3 model.\n  \n    Another nice thing is that its directly connected with Bing Search so you will be able to get the latest informations.\n  \nhttps://github.com/Strvm/meta-ai-api"
},
{
    "title": "No title",
    "content": "Does anyone know what’s going on behind the scenes of PyPI (who maintains `pip install`)? It seems like they’ve stopped processing support tickets over a month ago.\n  \n    I’ve hit the data limit for my package [PyBoy](https://github.com/baekalfen/pyboy), and I immediately posted a ticket to get the limit increased (as others have successfully done). But after more than a month, and several attempts at contacting the support team, I’ve heard nothing back, and I’ve run out of options.\n  \n    Does anyone know what’s happening, or how to get a comment from PyPI?\n  \n    My ticket is this one: https://github.com/pypi/support/issues/3757"
},
{
    "title": "No title",
    "content": "Article link: https://rishiraj.me/articles/2024-04/python_subinterpreter_parallelism\n\n    I have written an article, which should be helpful to folks at all experience levels, covering various multi-tasking paradigms in computers, and how they apply in CPython, with its unique limitations like the Global Interpreter Lock. Using this knowledge, we look at traditional ways to achieve \"true parallelism\" (i.e. multiple tasks running at the same time) in Python.\n  \n    Finally, we build a solution utilizing newer concepts in Python 3.12 to run any arbitrary pure Python code in parallel across multiple threads. All the code used to achieve this, along with the benchmarking code are available in the repository linked in the blog-post.\n  \n    This is my first time writing a technical post in Python. Any feedback would be really appreciated! 😊"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "MPCode is a small open-source scripting programming language written in python 3.12\n  \n    The repository with the same name on github.com allows you to modify the language to your own needs, execute small scripts, write programmes and libraries.The language itself already has functions, object groups, logical operators, loops and variables.The current functionality has documentation in three languages (English, Italian and Russian) and some small code samples.It would be very nice to get feedback and ideas from you.You can also develop your own projects based on it, as the project is licensed by MIT.Link to the repository: https://github.com/weksoftware/MPCode"
},
{
    "title": "No title",
    "content": "Explaining one of the most common web API vulnerability classes - Broken Object Level Authorization in a practical manner. Providing a case study example based on the Damn Vulnerable RESTaurant API, including methods for identifying and preventing these vulnerabilities.\n  \nhttps://devsec-blog.com/2024/04/web-api-security-champion-broken-object-level-authorization-owasp-top-10/"
},
{
    "title": "No title",
    "content": "https://github.com/Never-Over/chatpdb\n\n    Do you ever copy code, errors, or stack traces into ChatGPT? We did, and found it frustrating to always have to manually find, copy, and paste each relevant piece of information. On top of that, being forced to change tools would switch our focus and cause us to lose our flow. That’s why we built chatpdb  - a python debugger with ChatGPT!\n  \n    Simply use chatpdb like you would use ipdb or pdb; it’s a drop in replacement with the exact same functionality. The only addition is the new y keyword, which will trigger a response from ChatGPT.\n  \n    Here’s how it works:\n  > /Programming/test-chatpdb/lib.py(2)echo_platform()\n      1 def echo_platform(platform: str):\n----> 2     print(\"You are running on:\" + platform)\n      3\nipdb> y \nThe exception occurred because the function `echo_platform` tries to concatenate the string \"You are running on:\" with the `platform` variable, which is `None`. [...]\n    In this example, chatpdb correctly diagnoses an error in the current function as being caused by a bug in the calling function. chatpdb automatically passes relevant context about your program, including the source code, stack trace, and error information if available to ChatGPT.\n  \n    If you have a more specific question you can also supply a prompt to y:\n  > /Programming/test-chatpdb/lib.py(2)echo_platform()\n      1 def echo_platform(platform: str):\n----> 2     print(\"You are running on:\" + platform)\n      3\nipdb> y \"Why is platform coming through as None?\"\nThe variable `platform` is coming through as `None` because the environment variable `\"PLATFORM\"` is not set in your system's environment variables. [...]\n    It’s easy to install and set up:\n  pip install chatpdb\nexport OPENAI_API_KEY=....\nimport chatpdb\nchatpdb.set_trace()\n    What my project does: A drop-in replacement for ipdb or pdb, with ChatGPT built in.\n  \n    Target audience: anyone who currently uses ChatGPT to debug or ask questions about their code.\n  \n    Comparison: Copilot offers similar functionality, but is a much heavier tool and harder to turn off/ignore when you don’t want it. stackexplain is probably the closest tool; however it lacks the ability to hook into your program in any other way than fully running the python process.\n  \nchatpdb meets you where you are – AI tooling that’s only invoked when you need it. We hope you give it a try! We’d love any feedback or suggestions.\n  \nDocs"
},
{
    "title": "No title",
    "content": "Groupby in itertools module of python works differently that you might have thought. In this video, I have tried to explain very clearly about this feature. You are requested to watch the video. If you like, you can share the video and subscribe my youtube channel for further hacks.\n  \nhttps://youtu.be/sX8G8qNwxjc?si=UTdHvbDKIMfOGhpr"
},
{
    "title": "No title",
    "content": "I run an API (Python + Flask + Gunicorn) which has an endpoint that's calls a nested processes, which can sometimes take 6 seconds, to sometimes 60+ seconds to complete. The time it takes isn't a measure of my API's performance really, but rather its bottlenecked by an external service that the process has to rely on. Im looking for any suggested libraries that can safely handle this secondary process asynchronously with a long timeout. Ideally the API endpoint is called, and it won't have to wait for this process to finish if it takes more than 30+ seconds.Here's a mockup of what the chain of commands look like.\n  \n    Process 1: core.slurm.SlurmUser - addSlurmUser took: 1.6083331108093262. This process then calls core.slurm.SlurmUser - addScratch (which takes sometimes 6 seconds, sometimes 30+ seconds)\n  \n    Child Process 2: core.slurm.SlurmUser - addScratch took: 29.492394208908081\n  \n    Any advice? Am I missing something that I can take advantage of already? The App is built using Flask + Gunicorn workers. Wondering If Celery is something that can nicely integrate with my current setup, or is it something that would replace Gunicorn in this scenario?"
},
{
    "title": "No title",
    "content": "I'm working on a project involving a Python application with multiple interacting modules and am looking for some advice on deploying it efficiently on Google Cloud Platform. Here's a brief overview of what the application entails:\n  \n    Functionality Overview:\n  \n    •\tData Fetching: One module fetches data from external websites.\n  \n    •\tData Extraction: Another module extracts data from another GCP project.\n  \n    •\tData Processing: Variables are processed across various modules.\n  \n    •\tCSV Output: The app generates and stores CSV files.\n  \n    •\tScheduled Running: The application should run automatically once daily.\n  \n    I would greatly appreciate your insights on the following questions:\n  \n\n\n    What are the best GCP services to use for this type of application?\n  \n\n\n    Recommended Python libraries for handling web data retrieval in this case?\n  \n\n\nHow to proceed step by step? I intend to first deploy my Python application using placeholder inputs and simple printed outputs. Then, I plan to integrate the data fetching component and implement the creation and storage of CSV files. Does this strategy seem logical to you?\n  \n\n\n    Currently, I only tried to deploy my Python code using Google Cloud Functions, but I find this method impractical because I must deploy each module separately. I would prefer a solution that allows me to deploy all my code at once, where the Python modules can smoothly pass variables among themselves.\n  \n    Thank you in advance for your help!"
},
{
    "title": "No title",
    "content": "I got grumpy with our Scrum process, and I thought about what kinda tool I'd love to work with... so I started making it in my beloved Python. I think it's mature enough I'd like to start giving it to other people.\n  \n    It's called ScrumMD. It's open source, and you can already install it with pip (pip install scrummd) if you've got Python 3.10+. Documented, with tutorials on https://scrummd.readthedocs.io/en/stable/ and source on https://github.com/lkingsford/scrummd\n\nWhat my project does\n\n    Short version is that it's some tools to support you storing all of your Scrum cards (or, I guess, other cards - tickets perhaps?) in markdown format on a local machine. There's intentionally a lot of flexibility - so, every card needs a summary, but everything else is fair game. You can configure to require fields for some collections (like needing status in stories), or limit fields (like requiring status be 'Done' or 'In Progress').\n  \nTarget Audience\n\n    Limited, but public. It's chief audience is software engineers who work in self-organising teams. It's for teams who use processes like Scrum, but don't need the bureaucracy layers.\n  \n    Honestly - I know it's niche. Heck, I won't even be using it at work myself. But, if I could, I would - because I actually like using it.\n  \nComparison\n\n    I'm not aware of any CLI tools that do this kinda thing - particularly with local markdown files. The chief tool it would replace is a project/scrum management tool like Jira, or an issue tracker like the one that comes with GitHub. Both of those are far more friendly to a non-technical audience than ScrumMD is.\n  \nTechnical things\n\n    Technically - it's pretty darned vanilla.\n  \n    The only non-included bits I'm using are a toml library on Python 3.10, Sphynx for Doco, Setup Tools and Pytest. I use Pylint and Mypy for my own sake. Wasn't anything technically novel going on - although it's the first time I've tried submitting anything to PyPi myself.\n  \n    It's been fun to try setting up CI in GitHub - I'd used a number of other CI/CD frameworks, but I think I like GitHub actions.\n  \n    Happy to talk through anything I've done here, and would really be genuinely pleased to be forced to talk my way through any design decisions or hear ways I could have structured it better."
},
{
    "title": "No title",
    "content": "I did a search and read previous discussions in this sub about why Jupyter labs exists. From what I understand it is for quick code to try things out especially in data science …. Thing is, can’t you get the same thing via a decent IDE with autocomplete and quality debug tools in some quick Python code like in VSCODE? I do that all the time….\n  \n    I develop RNA computational software in Python and I abandoned Jupyter labs very early on as it just was not well suited to the job of bespoke data science. So many limitations and don’t get me started with widgets… I’ve been  developing code for decades and I have spent 3 days trying to understand widgets and I think I finally understand how to modify a text box and get a value from it…. You have to observe (why?), then act on, then route through some output and then you can get the info? It’s like someone was drunk while trying to emulate visual C#.\n  \n    Edit: before anyone else tells me to read vscode docs like I’m an idiot… I am unable to remote into the repo I’m using now for reasons. Not everyone can remote in and the question is not about remoting in. It’s still garbage in VSCODE… just use regular python.\n  \n    Edit2: for everyone who says they use it to do chunks of code work… I developed my own data management system for that kind of stuff that provides a wrapper over serialization that essentially creates pythonic pointers to data on any network drive. I just grab my data that I want to grab. Provides autocomplete and C# like attributes. I can dynamically build any data storage class using UML formatted infrastructure-as-code to build the front end. The back end is completely dynamic. Here is the pypi link. Note that I developed this to handle genomic data that was generated over the course of a week of 24/7 computation.\n  \nhttps://pypi.org/project/data-nut-squirrel/"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I made a cheat sheet of all common operations on Python's many data structures. This include both the built-in data structures and all common standard library data structures.\n  \nThe time complexities of different data structures in Python\n\n    If you're unfamiliar with time complexity and Big O notation, be sure to read the first section and the last two sections. I also recommend Ned Batchelder's talk/article that explains this topic more deeply."
},
{
    "title": "No title",
    "content": "Hey Python community!\n  \n    I created a stable diffusion pipeline to convert reference images to prompt and used it along with text prompt to generate variations of reference image.\n  \n    Explainer video here:\n  \nhttps://www.youtube.com/watch?v=x9VryjEcxzk"
},
{
    "title": "No title",
    "content": "https://github.com/Never-Over/bridge\nThe Problem\n    We built bridge to solve the most frustrating part of any new project — infrastructure. Whenever you spin up a new Django project, you usually have to manually configure Postgres, background workers, a task queue, and more. The problem is amplified when you go to deploy your application — hosting providers don’t understand anything about what you’ve configured already, so you have to run through an even more complicated process to set up the same infrastructure in a deployed environment.\n  What My Project Does\nbridge is a pip-installable package that spins up all of the infrastructure you need, and automatically connects it to your Django project. By adding a single line to your Django project's settings.py file, bridge configures everything for you — this means you don’t need to mess with DATABASES, BROKER_URL, or other environment variables to connect to these services.\n  \nbridge also gives you the access you need to manage these services, including a database and Redis shell, as well as a Flower instance for monitoring background tasks.\n  \n    When you’re ready to deploy, bridge can handle that as well. By running bridge init render, bridge will write all of the configuration necessary to deploy your application on Render, including a button to trigger deploys straight from your README.\n  \n    If you don’t want all of these services, (say you already have a database, and just want to add background workers) bridge supports that too! It can automate everything you need and nothing you don’t.\n  Target Audience\n    This tool is intended for Django developers who are getting new projects off the ground.\n  Comparison\n    Compared to how Django functions on its own, bridge abstracts and manages all of the services you'd need to manually set up and configure.\n  \n    Compared to other boilerplate or starter pack repos, bridge is more lightweight and doesn't force you to change your application code. It is focused only on handling all of the required dependencies, startup, and teardown of your infrastructure.\n  How it Works\n    bridge is built on top of Docker, so you get fully isolated and up-to-date versions of Postgres and Redis from the beginning. Celery and Flower need to run on top of your app code, so we hook into runserver to spin these up as background processes. If you need to spin things down, bridge stop will conveniently shut down all services that it’s started.\n  Coming Soon\n    In the future, we want to add support for more services (jupyter, mail/mailhog etc), more hosting providers (Heroku, Railway, etc.), and more configuration (env vars, optional dependencies, etc).\n  \nbridge is and always will be fully open source. Please give it a try and we’d love any feedback!\n  \nGithub\n\nDocs\n\nPyPI"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Version 1.1.1 of UXsim is released, which improves performance significantly.\n  \nMain Changes in 1.1.1\n\n\n\n    Add setting to adjust vehicle logging time interval via World.vehicle_logging_timestep_interval\n  \n\n\n    By lowering the interval (e.g., World.vehicle_logging_timestep_interval=2), the simulation time can be reduced (~20% speed up), and we can obtain vehicle trajectory data with slightly less accuracy.\n  \n\n\n    The logging setting does not affect the internal simulation accuracy. Only the outputted trajectories are affected.\n  \n\n\n    By setting World.vehicle_logging_timestep_interval=-1, the record_log is turned off, and the simulation time can be significantly reduced (~40% speed up).\n  \n\n\n    This addresses Issue #58\n  \n\n\n\n\n    Correct route choice behavior\n  \n\n\n    Vehicle.links_prefer and Vehicle.links_avoid work correctly now.\n  \n\n\n\n\nUXsim\n\n    UXsim is a free, open-source macroscopic and mesoscopic network traffic flow simulator written in Python. It simulates the movements of car travelers and traffic congestion in road networks. It is suitable for simulating large-scale (e.g., city-scale) traffic phenomena. UXsim is especially useful for scientific and educational purposes because of its simple, lightweight, and customizable features, but users are free to use UXsim for any purpose."
},
{
    "title": "No title",
    "content": "I’m aware there is a big open source community aspect to the python community. So my question is: “would uploading commercial software as a pip package offend the community?” I’m hesitant if this is normal, and if users would consider trials for components that would then make their project ask for a license to work after some time. Just wondering if it’s worth the effort to make products more accessible, or if it would just rub users the wrong way."
},
{
    "title": "No title",
    "content": "So I'm trying to switch from MATLAB to Python for my numerical simulations.\n  \n    One of the things I like about MATLAB is the ability to edit figures by adding stuff (eg arrows, lines, text ... ) to the figure within the software itself. This is what I mean. It's a very handy tool in my line of work and I use it often.\n  \n    I was wondering if there's any similar tool for Python. I use matplotlib and seaborn. Any suggestions would be great. Thanks guys!"
},
{
    "title": "No title",
    "content": "https://www.youtube.com/watch?v=utTaPW32gKY\n\n    I made a video summarizing the top techniques used by the Python community in the recently popular One Billion Row Challenge (1brc, https://github.com/gunnarmorling/1brc).\n  \n    I adapted one of the top Python submissions into the fastest pure Python approach for the 1brc (using only built-in libraries). Also, I tested a few awesome libraries (polars, duckdb) to see how well they can carve through the challenge's 1 billion rows of input data.\n  \n    If anyone wants to try to speed up my solution, then feel free to fork this repo https://github.com/dougmercer-yt/1brc and give it a shot!"
},
{
    "title": "No title",
    "content": "I have a problem that I was terrible at listening to new music, everything I listen to is from around 2000 - 2010.\n  \n    There is a radio station here in the UK called BBC 6 music that has a decent playlist but since I only really listen in the car its not always appropriate to note the track names down - and my memory is terrible so I always forget by the time I get home.\n  \n    Luckily they put the playlist online but it's just text links so I can't add it to my music service of choice - YouTube Music.\n  \n    Thankfully the page is pretty simple so I wrote a basic ass scraper that will grab the HTML, parse through the tracks with beautful soup, then use the ytmusicapi package to remove the old tracks from my playlist and recreate it with the freshly scraped data, railway then makes it dead easy to deploy and schedule it to run once a day.\n  \n    This thing took me probably less than an hour and a half to create and will hopefully open me up to some quality new tunes!"
},
{
    "title": "No title",
    "content": "https://www.infoq.com/news/2024/04/meta-threads-instagram-5-months/\n\n    Zahan Malkani talked during QCon London 2024 about Meta's journey from identifying the opportunity in the market to shipping the Threads application only five months later. The company leveraged Instagram's existing monolithic architecture, written in Python and PHP, and quickly iterated to create a new text-first microblogging service in record time."
},
{
    "title": "No title",
    "content": "I don't know why float('inf') was chosen as the way the language accesses infinity since this is using a magic string. Why couldn't it just be float.inf? That way magic string is avoided. Feels too basic of a best practice to be simply passed up. Anyone know the reason behind this?"
},
{
    "title": "No title",
    "content": "Hey Folks,\n  \n    I am working on a project that uses a bit old pandas version (1.5.3). I am trying to update it and use a more recent version (2.2.2). Its the first time I use pandas 2 btw. I simply ran all my unit tests and got multiple and different errors all concerning some datetime aspects. I tried troubleshooting by searching the errors and look in pandas release note but I find really complicated. It feels like the documentation is really exhaustive but I could really use a note on principal errors one can get switching to pandas 2.\n  \n    Does anyone have any handy blogpost, article, documentation that specifies this ? My focus is mainly on datetime errors when differences computed or comparaisons\n  \n\n    Thanks"
},
{
    "title": "No title",
    "content": "Hello,\n  \n     \n  \n    I recently published my first Python package on PyPi called kmeans-tjdwill (source). This is a k-means clustering implementation I wrote as a toy project that eventually was used extensively throughout my Master's degree, especially in my thesis. After a refactor and some optimization, I wanted to post here to hopefully elicit feedback and commentary from more experienced, professional software engineers.\n  What My Project Does\n    The central function is kmeans.cluster which segments input data into groups based on proximity to centroids. Users may either specify the initial means themselves or allow them to be chosen from among the input data. There is also a function for viewing the clustering process for 2-D and 3-D data, and, finally, a function that uses kmeans.cluster to perform image segmentation. Visual examples of both are found on the landing page of the Github repo linked above.\n  Comparison\n    This k-means clustering implementation is different from others I've seen in that it maintains what I call—for want of a better term—data association. The example I use is with object detection data. Say we have object detection data where each detection is of form\n  [x_min, y_min, x_max, y_max, conf, label_id]\n    Normally, if we want to cluster based on the first four elements (the bounding box data), we would lose track of the remaining two elements as the data is shuffled around during clustering. If we were to instead cluster based on all of the elements, the centroid values would be affected by the extra elements because we'd be clustering 6-D data instead of 4-D. This package provides a way to cluster based on the first four elements while also maintaining the association with the remaining data through the ndim parameter.\n  \n    This means that once the data is done clustering, you can then use the remaining data for further processing. In this project's implementation of image segmentation, for example, I leverage the above such that each (R,G,B) pixel remembers its  original image coordinates. This was very useful for replacing each pixel in a given cluster with the centroidal color value.\n  Target Audience\n    While I did use the first iteration of this project through multiple projects of my Master's degree, it is certainly not on the level of industry. Hopefully, this project can be used for exploration by those learning the concept.\n  \n     \n  \n    I've never had a code review before, so I appreciate any feedback or suggestions for improvement. Thank you for your time."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hello!\n  What My Project Does\n    zpy is a set of Zsh functions, mostly wrapping pip-tools or uv, for the simple and practical management of Python virtual environments, dependency specifications, and isolated Python app installation.\n  \n    You may find it a suitable alternative to poetry, pipenv, pipx, or unaided pip-tools or uv.\n  \n    There's a focus on use of good old requirements.txt files, and extensive tab completion assistance.\n  \n    It's not new, but I just made a new release that can use uv as a backend, making it much faster (and hipper, obviously).\n  Target Audience (e.g., Is it meant for production, just a toy project, etc.)\n    I'd say it's for folks who enjoy Zsh and tab completion, and a preference for \"vanilla\" and standards-based Python environment definitions. It's more oriented toward personal developer environments, not deployments. But it should work with the same requirements and pyproject.toml files that are used in production, with whatever tools you want to use there (e.g. pip).\n  Comparison (A brief comparison explaining how it differs from existing alternatives.)\n    You may find poetry or pipenv a good fit, and that's ok. I have personally found them to conflict with my needs and preferences:\n  \n\n\n    both introduce new-syntax files\n  \n\n\n    neither can do what pipx does\n  \n\n\n    poetry is a bit rigid, only working with installable packages as projects (possibly outdated info, it was this way last time I tried)\n  \n\n\n    pipenv is problematically nosy and weird sometimes, e.g. using unrelated files above the current folder without consent\n  \n\n\n    neither fully embrace the rich completions and other features that Zsh offers\n  \n\n\n    I have had only good experience with pipx, and replacing its features here was not an initial goal. I just couldn't resist when the other functions provided components for a transparent pipx substitute, with excellent tab completion.\n  \n    One difference from using plain uv is that this manages venvs in an external directory, and not necessarily just one venv per project folder.\n  More Notes\n    If you have zpy installed, you can install uv with the pipz command, and from then on zpy will use uv instead of Python's venv module and pip-tools:\n  % pipz install uv\n    If you have any questions, please ask!\n  \n    I personally use it in combination with mise (for Python runtime management) and flit (for package publishing), but aim to keep it rather agnostic and interoperable.\n  \n    Here's some more explanation copied from the readme:\n  \n    Guiding Ideas:\n  \n\n\n    You should not have to manually specify the dependencies anywhere other than *requirements.in files\n  \n\n\n    Folks who want to use your code shouldn't have to install any new-fangled less-standard tools (pipenv, poetry, pip-tools, zpy, etc.); pip install -r *requirements.txt ought to be sufficient\n  \n\n\n    It's nice to keep the venv folder outside of the project itself\n  \n\n\n    Not every manageable project needs a pyproject.toml or to be packaged\n  \n\n\n    Lockfiles are good\n  \n\n\n    Tab completion is wonderful\n  \n\n\n    These functions don't:\n  \n\n\n    need to be used exclusively\n  \n\n\n    need to be used by everyone on the same project\n  \n\n\n    do what mise/pyenv/asdf-vm or flit do best (but do work with them if you choose)\n  \n\n\nconflict with anything else your team cares to do with your code; If they can be a friendlier neighbor to your workflows, file an issue\n  \n\n\n\n\noverview screenshot"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "MPCode is a small open-source scripting programming language written in python 3.12\n  \n    The repository with the same name on github.com allows you to modify the language to your own needs, execute small scripts, write programmes and libraries.The language itself already has functions, object groups, logical operators, loops and variables.The current functionality has documentation in three languages (English, Italian and Russian) and some small code samples.It would be very nice to get feedback and ideas from you.You can also develop your own projects based on it, as the project is licensed by MIT.Link to the repository: https://github.com/weksoftware/MPCode"
},
{
    "title": "No title",
    "content": "Explaining one of the most common web API vulnerability classes - Broken Object Level Authorization in a practical manner. Providing a case study example based on the Damn Vulnerable RESTaurant API, including methods for identifying and preventing these vulnerabilities.\n  \nhttps://devsec-blog.com/2024/04/web-api-security-champion-broken-object-level-authorization-owasp-top-10/"
},
{
    "title": "No title",
    "content": "https://github.com/Never-Over/chatpdb\n\n    Do you ever copy code, errors, or stack traces into ChatGPT? We did, and found it frustrating to always have to manually find, copy, and paste each relevant piece of information. On top of that, being forced to change tools would switch our focus and cause us to lose our flow. That’s why we built chatpdb  - a python debugger with ChatGPT!\n  \n    Simply use chatpdb like you would use ipdb or pdb; it’s a drop in replacement with the exact same functionality. The only addition is the new y keyword, which will trigger a response from ChatGPT.\n  \n    Here’s how it works:\n  > /Programming/test-chatpdb/lib.py(2)echo_platform()\n      1 def echo_platform(platform: str):\n----> 2     print(\"You are running on:\" + platform)\n      3\nipdb> y \nThe exception occurred because the function `echo_platform` tries to concatenate the string \"You are running on:\" with the `platform` variable, which is `None`. [...]\n    In this example, chatpdb correctly diagnoses an error in the current function as being caused by a bug in the calling function. chatpdb automatically passes relevant context about your program, including the source code, stack trace, and error information if available to ChatGPT.\n  \n    If you have a more specific question you can also supply a prompt to y:\n  > /Programming/test-chatpdb/lib.py(2)echo_platform()\n      1 def echo_platform(platform: str):\n----> 2     print(\"You are running on:\" + platform)\n      3\nipdb> y \"Why is platform coming through as None?\"\nThe variable `platform` is coming through as `None` because the environment variable `\"PLATFORM\"` is not set in your system's environment variables. [...]\n    It’s easy to install and set up:\n  pip install chatpdb\nexport OPENAI_API_KEY=....\nimport chatpdb\nchatpdb.set_trace()\n    What my project does: A drop-in replacement for ipdb or pdb, with ChatGPT built in.\n  \n    Target audience: anyone who currently uses ChatGPT to debug or ask questions about their code.\n  \n    Comparison: Copilot offers similar functionality, but is a much heavier tool and harder to turn off/ignore when you don’t want it. stackexplain is probably the closest tool; however it lacks the ability to hook into your program in any other way than fully running the python process.\n  \nchatpdb meets you where you are – AI tooling that’s only invoked when you need it. We hope you give it a try! We’d love any feedback or suggestions.\n  \nDocs"
},
{
    "title": "No title",
    "content": "Groupby in itertools module of python works differently that you might have thought. In this video, I have tried to explain very clearly about this feature. You are requested to watch the video. If you like, you can share the video and subscribe my youtube channel for further hacks.\n  \nhttps://youtu.be/sX8G8qNwxjc?si=UTdHvbDKIMfOGhpr"
},
{
    "title": "No title",
    "content": "I run an API (Python + Flask + Gunicorn) which has an endpoint that's calls a nested processes, which can sometimes take 6 seconds, to sometimes 60+ seconds to complete. The time it takes isn't a measure of my API's performance really, but rather its bottlenecked by an external service that the process has to rely on. Im looking for any suggested libraries that can safely handle this secondary process asynchronously with a long timeout. Ideally the API endpoint is called, and it won't have to wait for this process to finish if it takes more than 30+ seconds.Here's a mockup of what the chain of commands look like.\n  \n    Process 1: core.slurm.SlurmUser - addSlurmUser took: 1.6083331108093262. This process then calls core.slurm.SlurmUser - addScratch (which takes sometimes 6 seconds, sometimes 30+ seconds)\n  \n    Child Process 2: core.slurm.SlurmUser - addScratch took: 29.492394208908081\n  \n    Any advice? Am I missing something that I can take advantage of already? The App is built using Flask + Gunicorn workers. Wondering If Celery is something that can nicely integrate with my current setup, or is it something that would replace Gunicorn in this scenario?"
},
{
    "title": "No title",
    "content": "I'm working on a project involving a Python application with multiple interacting modules and am looking for some advice on deploying it efficiently on Google Cloud Platform. Here's a brief overview of what the application entails:\n  \n    Functionality Overview:\n  \n    •\tData Fetching: One module fetches data from external websites.\n  \n    •\tData Extraction: Another module extracts data from another GCP project.\n  \n    •\tData Processing: Variables are processed across various modules.\n  \n    •\tCSV Output: The app generates and stores CSV files.\n  \n    •\tScheduled Running: The application should run automatically once daily.\n  \n    I would greatly appreciate your insights on the following questions:\n  \n\n\n    What are the best GCP services to use for this type of application?\n  \n\n\n    Recommended Python libraries for handling web data retrieval in this case?\n  \n\n\nHow to proceed step by step? I intend to first deploy my Python application using placeholder inputs and simple printed outputs. Then, I plan to integrate the data fetching component and implement the creation and storage of CSV files. Does this strategy seem logical to you?\n  \n\n\n    Currently, I only tried to deploy my Python code using Google Cloud Functions, but I find this method impractical because I must deploy each module separately. I would prefer a solution that allows me to deploy all my code at once, where the Python modules can smoothly pass variables among themselves.\n  \n    Thank you in advance for your help!"
},
{
    "title": "No title",
    "content": "I got grumpy with our Scrum process, and I thought about what kinda tool I'd love to work with... so I started making it in my beloved Python. I think it's mature enough I'd like to start giving it to other people.\n  \n    It's called ScrumMD. It's open source, and you can already install it with pip (pip install scrummd) if you've got Python 3.10+. Documented, with tutorials on https://scrummd.readthedocs.io/en/stable/ and source on https://github.com/lkingsford/scrummd\n\nWhat my project does\n\n    Short version is that it's some tools to support you storing all of your Scrum cards (or, I guess, other cards - tickets perhaps?) in markdown format on a local machine. There's intentionally a lot of flexibility - so, every card needs a summary, but everything else is fair game. You can configure to require fields for some collections (like needing status in stories), or limit fields (like requiring status be 'Done' or 'In Progress').\n  \nTarget Audience\n\n    Limited, but public. It's chief audience is software engineers who work in self-organising teams. It's for teams who use processes like Scrum, but don't need the bureaucracy layers.\n  \n    Honestly - I know it's niche. Heck, I won't even be using it at work myself. But, if I could, I would - because I actually like using it.\n  \nComparison\n\n    I'm not aware of any CLI tools that do this kinda thing - particularly with local markdown files. The chief tool it would replace is a project/scrum management tool like Jira, or an issue tracker like the one that comes with GitHub. Both of those are far more friendly to a non-technical audience than ScrumMD is.\n  \nTechnical things\n\n    Technically - it's pretty darned vanilla.\n  \n    The only non-included bits I'm using are a toml library on Python 3.10, Sphynx for Doco, Setup Tools and Pytest. I use Pylint and Mypy for my own sake. Wasn't anything technically novel going on - although it's the first time I've tried submitting anything to PyPi myself.\n  \n    It's been fun to try setting up CI in GitHub - I'd used a number of other CI/CD frameworks, but I think I like GitHub actions.\n  \n    Happy to talk through anything I've done here, and would really be genuinely pleased to be forced to talk my way through any design decisions or hear ways I could have structured it better."
},
{
    "title": "No title",
    "content": "I did a search and read previous discussions in this sub about why Jupyter labs exists. From what I understand it is for quick code to try things out especially in data science …. Thing is, can’t you get the same thing via a decent IDE with autocomplete and quality debug tools in some quick Python code like in VSCODE? I do that all the time….\n  \n    I develop RNA computational software in Python and I abandoned Jupyter labs very early on as it just was not well suited to the job of bespoke data science. So many limitations and don’t get me started with widgets… I’ve been  developing code for decades and I have spent 3 days trying to understand widgets and I think I finally understand how to modify a text box and get a value from it…. You have to observe (why?), then act on, then route through some output and then you can get the info? It’s like someone was drunk while trying to emulate visual C#.\n  \n    Edit: before anyone else tells me to read vscode docs like I’m an idiot… I am unable to remote into the repo I’m using now for reasons. Not everyone can remote in and the question is not about remoting in. It’s still garbage in VSCODE… just use regular python.\n  \n    Edit2: for everyone who says they use it to do chunks of code work… I developed my own data management system for that kind of stuff that provides a wrapper over serialization that essentially creates pythonic pointers to data on any network drive. I just grab my data that I want to grab. Provides autocomplete and C# like attributes. I can dynamically build any data storage class using UML formatted infrastructure-as-code to build the front end. The back end is completely dynamic. Here is the pypi link. Note that I developed this to handle genomic data that was generated over the course of a week of 24/7 computation.\n  \nhttps://pypi.org/project/data-nut-squirrel/"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I made a cheat sheet of all common operations on Python's many data structures. This include both the built-in data structures and all common standard library data structures.\n  \nThe time complexities of different data structures in Python\n\n    If you're unfamiliar with time complexity and Big O notation, be sure to read the first section and the last two sections. I also recommend Ned Batchelder's talk/article that explains this topic more deeply."
},
{
    "title": "No title",
    "content": "Hey Python community!\n  \n    I created a stable diffusion pipeline to convert reference images to prompt and used it along with text prompt to generate variations of reference image.\n  \n    Explainer video here:\n  \nhttps://www.youtube.com/watch?v=x9VryjEcxzk"
},
{
    "title": "No title",
    "content": "https://github.com/Never-Over/bridge\nThe Problem\n    We built bridge to solve the most frustrating part of any new project — infrastructure. Whenever you spin up a new Django project, you usually have to manually configure Postgres, background workers, a task queue, and more. The problem is amplified when you go to deploy your application — hosting providers don’t understand anything about what you’ve configured already, so you have to run through an even more complicated process to set up the same infrastructure in a deployed environment.\n  What My Project Does\nbridge is a pip-installable package that spins up all of the infrastructure you need, and automatically connects it to your Django project. By adding a single line to your Django project's settings.py file, bridge configures everything for you — this means you don’t need to mess with DATABASES, BROKER_URL, or other environment variables to connect to these services.\n  \nbridge also gives you the access you need to manage these services, including a database and Redis shell, as well as a Flower instance for monitoring background tasks.\n  \n    When you’re ready to deploy, bridge can handle that as well. By running bridge init render, bridge will write all of the configuration necessary to deploy your application on Render, including a button to trigger deploys straight from your README.\n  \n    If you don’t want all of these services, (say you already have a database, and just want to add background workers) bridge supports that too! It can automate everything you need and nothing you don’t.\n  Target Audience\n    This tool is intended for Django developers who are getting new projects off the ground.\n  Comparison\n    Compared to how Django functions on its own, bridge abstracts and manages all of the services you'd need to manually set up and configure.\n  \n    Compared to other boilerplate or starter pack repos, bridge is more lightweight and doesn't force you to change your application code. It is focused only on handling all of the required dependencies, startup, and teardown of your infrastructure.\n  How it Works\n    bridge is built on top of Docker, so you get fully isolated and up-to-date versions of Postgres and Redis from the beginning. Celery and Flower need to run on top of your app code, so we hook into runserver to spin these up as background processes. If you need to spin things down, bridge stop will conveniently shut down all services that it’s started.\n  Coming Soon\n    In the future, we want to add support for more services (jupyter, mail/mailhog etc), more hosting providers (Heroku, Railway, etc.), and more configuration (env vars, optional dependencies, etc).\n  \nbridge is and always will be fully open source. Please give it a try and we’d love any feedback!\n  \nGithub\n\nDocs\n\nPyPI"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Version 1.1.1 of UXsim is released, which improves performance significantly.\n  \nMain Changes in 1.1.1\n\n\n\n    Add setting to adjust vehicle logging time interval via World.vehicle_logging_timestep_interval\n  \n\n\n    By lowering the interval (e.g., World.vehicle_logging_timestep_interval=2), the simulation time can be reduced (~20% speed up), and we can obtain vehicle trajectory data with slightly less accuracy.\n  \n\n\n    The logging setting does not affect the internal simulation accuracy. Only the outputted trajectories are affected.\n  \n\n\n    By setting World.vehicle_logging_timestep_interval=-1, the record_log is turned off, and the simulation time can be significantly reduced (~40% speed up).\n  \n\n\n    This addresses Issue #58\n  \n\n\n\n\n    Correct route choice behavior\n  \n\n\n    Vehicle.links_prefer and Vehicle.links_avoid work correctly now.\n  \n\n\n\n\nUXsim\n\n    UXsim is a free, open-source macroscopic and mesoscopic network traffic flow simulator written in Python. It simulates the movements of car travelers and traffic congestion in road networks. It is suitable for simulating large-scale (e.g., city-scale) traffic phenomena. UXsim is especially useful for scientific and educational purposes because of its simple, lightweight, and customizable features, but users are free to use UXsim for any purpose."
},
{
    "title": "No title",
    "content": "I’m aware there is a big open source community aspect to the python community. So my question is: “would uploading commercial software as a pip package offend the community?” I’m hesitant if this is normal, and if users would consider trials for components that would then make their project ask for a license to work after some time. Just wondering if it’s worth the effort to make products more accessible, or if it would just rub users the wrong way."
},
{
    "title": "No title",
    "content": "So I'm trying to switch from MATLAB to Python for my numerical simulations.\n  \n    One of the things I like about MATLAB is the ability to edit figures by adding stuff (eg arrows, lines, text ... ) to the figure within the software itself. This is what I mean. It's a very handy tool in my line of work and I use it often.\n  \n    I was wondering if there's any similar tool for Python. I use matplotlib and seaborn. Any suggestions would be great. Thanks guys!"
},
{
    "title": "No title",
    "content": "https://www.youtube.com/watch?v=utTaPW32gKY\n\n    I made a video summarizing the top techniques used by the Python community in the recently popular One Billion Row Challenge (1brc, https://github.com/gunnarmorling/1brc).\n  \n    I adapted one of the top Python submissions into the fastest pure Python approach for the 1brc (using only built-in libraries). Also, I tested a few awesome libraries (polars, duckdb) to see how well they can carve through the challenge's 1 billion rows of input data.\n  \n    If anyone wants to try to speed up my solution, then feel free to fork this repo https://github.com/dougmercer-yt/1brc and give it a shot!"
},
{
    "title": "No title",
    "content": "I have a problem that I was terrible at listening to new music, everything I listen to is from around 2000 - 2010.\n  \n    There is a radio station here in the UK called BBC 6 music that has a decent playlist but since I only really listen in the car its not always appropriate to note the track names down - and my memory is terrible so I always forget by the time I get home.\n  \n    Luckily they put the playlist online but it's just text links so I can't add it to my music service of choice - YouTube Music.\n  \n    Thankfully the page is pretty simple so I wrote a basic ass scraper that will grab the HTML, parse through the tracks with beautful soup, then use the ytmusicapi package to remove the old tracks from my playlist and recreate it with the freshly scraped data, railway then makes it dead easy to deploy and schedule it to run once a day.\n  \n    This thing took me probably less than an hour and a half to create and will hopefully open me up to some quality new tunes!"
},
{
    "title": "No title",
    "content": "https://www.infoq.com/news/2024/04/meta-threads-instagram-5-months/\n\n    Zahan Malkani talked during QCon London 2024 about Meta's journey from identifying the opportunity in the market to shipping the Threads application only five months later. The company leveraged Instagram's existing monolithic architecture, written in Python and PHP, and quickly iterated to create a new text-first microblogging service in record time."
},
{
    "title": "No title",
    "content": "I don't know why float('inf') was chosen as the way the language accesses infinity since this is using a magic string. Why couldn't it just be float.inf? That way magic string is avoided. Feels too basic of a best practice to be simply passed up. Anyone know the reason behind this?"
},
{
    "title": "No title",
    "content": "Hey Folks,\n  \n    I am working on a project that uses a bit old pandas version (1.5.3). I am trying to update it and use a more recent version (2.2.2). Its the first time I use pandas 2 btw. I simply ran all my unit tests and got multiple and different errors all concerning some datetime aspects. I tried troubleshooting by searching the errors and look in pandas release note but I find really complicated. It feels like the documentation is really exhaustive but I could really use a note on principal errors one can get switching to pandas 2.\n  \n    Does anyone have any handy blogpost, article, documentation that specifies this ? My focus is mainly on datetime errors when differences computed or comparaisons\n  \n\n    Thanks"
},
{
    "title": "No title",
    "content": "Hello,\n  \n     \n  \n    I recently published my first Python package on PyPi called kmeans-tjdwill (source). This is a k-means clustering implementation I wrote as a toy project that eventually was used extensively throughout my Master's degree, especially in my thesis. After a refactor and some optimization, I wanted to post here to hopefully elicit feedback and commentary from more experienced, professional software engineers.\n  What My Project Does\n    The central function is kmeans.cluster which segments input data into groups based on proximity to centroids. Users may either specify the initial means themselves or allow them to be chosen from among the input data. There is also a function for viewing the clustering process for 2-D and 3-D data, and, finally, a function that uses kmeans.cluster to perform image segmentation. Visual examples of both are found on the landing page of the Github repo linked above.\n  Comparison\n    This k-means clustering implementation is different from others I've seen in that it maintains what I call—for want of a better term—data association. The example I use is with object detection data. Say we have object detection data where each detection is of form\n  [x_min, y_min, x_max, y_max, conf, label_id]\n    Normally, if we want to cluster based on the first four elements (the bounding box data), we would lose track of the remaining two elements as the data is shuffled around during clustering. If we were to instead cluster based on all of the elements, the centroid values would be affected by the extra elements because we'd be clustering 6-D data instead of 4-D. This package provides a way to cluster based on the first four elements while also maintaining the association with the remaining data through the ndim parameter.\n  \n    This means that once the data is done clustering, you can then use the remaining data for further processing. In this project's implementation of image segmentation, for example, I leverage the above such that each (R,G,B) pixel remembers its  original image coordinates. This was very useful for replacing each pixel in a given cluster with the centroidal color value.\n  Target Audience\n    While I did use the first iteration of this project through multiple projects of my Master's degree, it is certainly not on the level of industry. Hopefully, this project can be used for exploration by those learning the concept.\n  \n     \n  \n    I've never had a code review before, so I appreciate any feedback or suggestions for improvement. Thank you for your time."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hello!\n  What My Project Does\n    zpy is a set of Zsh functions, mostly wrapping pip-tools or uv, for the simple and practical management of Python virtual environments, dependency specifications, and isolated Python app installation.\n  \n    You may find it a suitable alternative to poetry, pipenv, pipx, or unaided pip-tools or uv.\n  \n    There's a focus on use of good old requirements.txt files, and extensive tab completion assistance.\n  \n    It's not new, but I just made a new release that can use uv as a backend, making it much faster (and hipper, obviously).\n  Target Audience (e.g., Is it meant for production, just a toy project, etc.)\n    I'd say it's for folks who enjoy Zsh and tab completion, and a preference for \"vanilla\" and standards-based Python environment definitions. It's more oriented toward personal developer environments, not deployments. But it should work with the same requirements and pyproject.toml files that are used in production, with whatever tools you want to use there (e.g. pip).\n  Comparison (A brief comparison explaining how it differs from existing alternatives.)\n    You may find poetry or pipenv a good fit, and that's ok. I have personally found them to conflict with my needs and preferences:\n  \n\n\n    both introduce new-syntax files\n  \n\n\n    neither can do what pipx does\n  \n\n\n    poetry is a bit rigid, only working with installable packages as projects (possibly outdated info, it was this way last time I tried)\n  \n\n\n    pipenv is problematically nosy and weird sometimes, e.g. using unrelated files above the current folder without consent\n  \n\n\n    neither fully embrace the rich completions and other features that Zsh offers\n  \n\n\n    I have had only good experience with pipx, and replacing its features here was not an initial goal. I just couldn't resist when the other functions provided components for a transparent pipx substitute, with excellent tab completion.\n  \n    One difference from using plain uv is that this manages venvs in an external directory, and not necessarily just one venv per project folder.\n  More Notes\n    If you have zpy installed, you can install uv with the pipz command, and from then on zpy will use uv instead of Python's venv module and pip-tools:\n  % pipz install uv\n    If you have any questions, please ask!\n  \n    I personally use it in combination with mise (for Python runtime management) and flit (for package publishing), but aim to keep it rather agnostic and interoperable.\n  \n    Here's some more explanation copied from the readme:\n  \n    Guiding Ideas:\n  \n\n\n    You should not have to manually specify the dependencies anywhere other than *requirements.in files\n  \n\n\n    Folks who want to use your code shouldn't have to install any new-fangled less-standard tools (pipenv, poetry, pip-tools, zpy, etc.); pip install -r *requirements.txt ought to be sufficient\n  \n\n\n    It's nice to keep the venv folder outside of the project itself\n  \n\n\n    Not every manageable project needs a pyproject.toml or to be packaged\n  \n\n\n    Lockfiles are good\n  \n\n\n    Tab completion is wonderful\n  \n\n\n    These functions don't:\n  \n\n\n    need to be used exclusively\n  \n\n\n    need to be used by everyone on the same project\n  \n\n\n    do what mise/pyenv/asdf-vm or flit do best (but do work with them if you choose)\n  \n\n\nconflict with anything else your team cares to do with your code; If they can be a friendlier neighbor to your workflows, file an issue\n  \n\n\n\n\noverview screenshot"
},
{
    "title": "No title",
    "content": "What My Project Does?\n    Hello everyone, I'm proud to announce a brand-new Python library named pixi-kernel: https://github.com/renan-r-santos/pixi-kernel allowing you to run Jupyter kernels using Pixi for reproducible notebooks.\n  \n    By the way, if you haven't heard of the Pixi package manager, check it out at https://pixi.sh/latest/.It supports conda and PyPI (through uv) packages, pyproject.toml config, git, path and editable installs and it is really fast.\n  Target Audience\n    JupyterLab users, production-ready.\n  Comparison\n    The main differences compared to similar solutions such as poetry-kernel are the ability to install conda-forge packages and using kernels in languages other than Python. Compared to nb-conda-kernels, the difference is the use of the Pixi package manager which allows per-directory dependency locking."
},
{
    "title": "No title",
    "content": "https://github.com/RadoTheProgrammer/motsmeles/\n\n    Note: This is written in french.\n  What my project does\n    It generate word search games with custom words, dimensions.\n  Target audience\n    For people who want to get fun with some word searching\n  Comparison\n    This is a python library, it's open source, and you can use it in a python code, or with the command-line.\n  \n    Also, you have freedom if you want to have diagonal or reversed words.\n  Next steps\n    A GUI with pygame to generate and use it in a user-friendly interface."
},
{
    "title": "No title",
    "content": "Trying this again a third time because automod flagged it as a resource and the mods do not appear to be responding.https://github.com/heavy-resume/heavy-stack\n\nWhat it does: The Heavy Stack is a template web framework that is top to bottom Python using Sanic / (custom) ReactPy / Brython. It's a complete solution that just works out of the box.\n  \nTarget Audience: This is production capable but a little on the early side. Iterating is quite fast.\n  \nComparison:\n\n    Compared to other web frameworks:\n  \n\n\n    Just works out of the box, even has a docker image ready to go for the server and your postgres database that uses PG Vector.\n  \n\n\n    ReactPy is server side, so instead of creating API calls, you just use `heavy_use_effect` and `heavy_event` to respond to things, do async loads, etc. It uses a websocket. If you're familiar with ReactPy already, the difference is that there's `heavy` versions that inject a database connection to the async functions and let you stuff in user context.\n  \n\n\n    The custom version of ReactPy that ships with this reconnects the websocket and attempts to restore the client state. It also supports recording and replaying your actions and uses locust to do load testing with those actions.\n  \n\n\n    Hot reloading means if you change a code file you see your changes about 2 seconds after saving without having to refresh the page. For tests it means the test runs near instantly after the initial load.\n  \n\n\nDescription:\n\n    This is a dev container / GitHub Codespaces centric repo, so you'll be best served by using VS Code and either codespaces or something compatible the dev container standard. It's intended to be a template repo which you just run a script to rename to whatever project you want.\n  \n    Here are the features that work \"out of the box\" if you use the dev container or codespace:\n  \n\n\n    Pre-made dev container, docker files\n  \n\n\n    Top to Bottom Python (Sanic, custom ReactPy, Brython)\n  \n\n\n    PostgreSQL, PG Vector, CockroachDB\n  \n\n\n    SQL Model (SQLAlchemy + Pydantic)\n  \n\n\n    Hot reloading, both server and tests\n  \n\n\n    User action recording and playback for load testing\n  \n\n\n    Time tracking based on file changes\n  \n\n\n    Established patterns and examples, script to generate boilerplate around tables and their domain models\n  \n\n\n    There's some known issues and planned updates that will come at a later time, so for the sake of not overselling it, this should be treated like an alpha product. I'd love to see contributions though, especially when it comes to creating off-the-shelf components for ReactPy, or fixing the ReactPy bugs or getting that massive ReactPy PR mentioned in the readme piece-wise merged into the main project.\n  \n    Thanks for your time! I hope this is helpful for some people!"
},
{
    "title": "No title",
    "content": "I'm sure that many people have encountered the following (code for reproduction below) behavior:  if during the execution of `@property`, an `AttributeError` is raised and the same class implements `__getattr__` then `__getattr__` is invoked with the name of the property resulting in a confusing message: `AttributeError: 'Foo' object has no attribute 'something'`. If we remove `__getattr__` then we get a more meaningful and correct message: `AttributeError: 'Foo' object has no attribute 'bar'`.\n  from dataclasses import dataclass\n\n@dataclass\nclass Foo: val = 'foo_value'\n\nclass Test: \n    def __init__(self): \n        self.foo = Foo()\n\n    def __getattr__(self, name):\n        return getattr(self.foo, name)\n    \n    @property\n    def something(self):\n        return self.foo.bar\n\nt = Test() t.something\n    The error message is misleading and the behavior catches many people (myself including) in surprise and results in many wasted hours of debugging. I've seen people recommend using a custom decorator (instead of `@property` that catches attribute error) or wrapping your `@property` bodies in `try/except`. But it seems to me that this should be dealt with on language level.\n  \n    I'm trying to understand a few things:\n  \n\n\n    Is this behavior in Python intentional (i.e. is there a good reason) or just stems from the fact that `__getattr__` is invoked when `AttributeError` is raised when an instance attribute is accessed?\n  \n\n\n    If not intentional, is there a relatively easy way to detect `AttributeErrors` cause by `@property` execution in CPython? If so, why hasn't this been handled on language level?\n  \n\n\n    I've searched CPython issues on Github and I saw someone trying to correct documentation but couldn't find issues asking about this behavior or suggestions to fix - am I missing something?"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "At least a couple things that seems is that Litestar appears to be a fast  Django lite. Some batteries included, but not too many and opinionated. It's uses Rust so of course it's going to be fast, not that it really matters that much since IO > CPU for a website IMO. Which any async framework addresses this.\n  \n    I haven't heard many people using it though despite it having a lot of batteries included, but not dominating everything with first party support. Anyone using it in production in 2024?\n  \n    Edit:\n  \n    Fact check it appers it isn't written in Rust, I probably confused it with something else or it might just be data serialization"
},
{
    "title": "No title",
    "content": "In this article, I explain list comprehensions, as this is something people new to Python struggle with.\n  \nDemystifying list comprehensions in Python"
},
{
    "title": "No title",
    "content": "I'm working on an ETL job that reads in a JSONL file, splits it into multiple Polars LazyFrames, collects those to DataFrames, and then writes them to a Delta Table. Prior to doing this with Polars, I would generally complete a task like this with either Pandas or PySpark depending on the size of data I was dealing with. I'm looking for any suggestions or advice the community has on doing this as efficiently as possible using Polars."
},
{
    "title": "No title",
    "content": "A lot of people are starting to rely on ChatGPT to write code for them from time to time. What I have personally noticed is ChatGPT writes modules so heavy its ridiculous. Last night I was working on an issue a small but technical issue, and asked it for help, it spat out, I kid you not 200+ lines of code and I just thought to myself this cannot be right, it was so heavy I was shaking my head. After some trial and error I got a very elegant piece done in 15 lines of code. Way faster, cleaner and yes reusable. So, it's nice that you can ask CGPT a question from time to time but to rely on it for code structure at this point, in my opinion, no good. Nice to be here!"
},
{
    "title": "No title",
    "content": "Quick example:\n  from privates import private\n\n@private\nclass Hello:\n    __readonly__ = \"bar\",  # No need for @property!\n    __protected__ = \"foo\",\n\n    def __init__(self):\n        self.bar = \"hello world!\"\n        self.foo = \"foo\"\n\n# Hello is now only usable from this module\n    Repo: https://github.com/ZeroIntensity/privates.py"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I have searched for a workable solution for the marriage of FastAPI async REST with RabbitMQ async interop in one application. Here is a fully workable code I came up with (intentionally simplified).\n  class MessageBrokerManager:\n    async def consume(self, loop):\n        try:\n            pika.BlockingConnection(\n                parameters=pika.ConnectionParameters(\n                    host=self._host,\n                    port=self._port,\n                    credentials=pika.PlainCredentials(\n                        username=self._username,\n                        password=self._password,\n                    ),\n                )\n            )\n        except Exception as e:\n            log_info_fail(\"Failed to connect to RabbitMQ\")\n            # return empty task\n            return asyncio.create_task(asyncio.sleep(0))\n\n        self._connection = await connect_robust(\n            host=self._host,\n            port=self._port,\n            login=self._username,\n            password=self._password,\n            loop=loop,\n        )\n\n        self._channel = await self._connection.channel()\n        queue = await self._channel.declare_queue(self._queue_name, durable=True)\n        await queue.consume(callback=MessageBrokerManager.on_message, no_ack=True)\n\n        return self._connection\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    try:\n        loop = asyncio.get_running_loop()\n        task = loop.create_task(MessageBrokerManager().consume(loop))\n        await task\n    except Exception as e:\n        pass\n    yield\n\napp = FastAPI(lifespan=lifespan)\napp.include_router(some_rest_router)\n    Dependencies:\n  \n\n\n    pika>=1.3.2\n  \n\n\n    aio-pika>=9.4.1\n  \n\n\n    fastapi==0.109.2"
},
{
    "title": "No title",
    "content": "What My Project Does constable automatically injects print statements, during runtime, into your function code to give you a live step by step replay of each variable assignment.\n  \n    Github repo - https://github.com/saurabh0719/constable\n\nTarget Audience Can be used in testing/debugging environments. Do not leave this hanging around in production!\n  \nComparison Use pdb for all purposes that matter. Or an object state tracker.\n  \n    Example -\n  import constable\n\n@constable.trace('a', 'b')\ndef example(a, b):\n    a = a + b\n    c = a\n    a = \"Experimenting with the AST\"\n    b = c + b\n    a = c + b\n    return a\n\nexample(5, 6)\n    Output -\n  constable: example: line 5\n    a = a + b\n    a = 11\n    type(a) = <class 'int'>\n\nconstable: example: line 7\n    a = \"Experimenting with the AST\"\n    a = Experimenting with the AST\n    type(a) = <class 'str'>\n\nconstable: example: line 8\n    b = c + b\n    b = 17\n    type(b) = <class 'int'>\n\nconstable: example: line 9\n    a = c + b\n    a = 28\n    type(a) = <class 'int'>\n\nconstable: example: line 3 to 10\n    args: (5, 6)\n    kwargs: {}\n    returned: 28\n    execution time: 0.00018480 seconds"
},
{
    "title": "No title",
    "content": "For my machine learning project, I created an 'end to end' segmentation pipeline using MONAI and Pytorch for the deep learning portion, and the Optuna hyper parameter optimization library for hyperparam optimization / search. The entire project is fully encapsulated/packaged using Poetry, so its really easy to install and use. The pipeline is fully customizable in terms of which models to use, model parameters, optimizers, as well as hyperparameters via configuration files. It also takes advantage of automatic mixed-precision for accelerated compute. I also have it integrated with Aimstack for experiment tracking and visualization.\n  \nWhat My Project Does\n\n    Segmentation pipeline used for creating segmentation masks of diagnostic medical images in Nifti format. Streamlines the process of training, inference and hyperparameter search using Optuna and Aimstack integration. Modularized configuration files allow for specific use-case modification.\n  \nTarget Audience\n\n    Primarily meant for medical image and computer vision research & teaching. Not meant for clinical use.\n  \nComparison\n\n    Use a single pipeline for training, inference and visualization, quickly prototype models, and even benchmark HPC GPU performance.\n  \n    Overall, this was an amazing learning experience for me. I'm new to the world of ML, and I learned a lot while developing this. I've been running this on an HPC cluster with both A100 and H100 GPUs. What do you guy's think? https://github.com/adnan-umich/monai-train"
},
{
    "title": "No title",
    "content": "Hi all, here trying to replace values for a range of dates in a data frame copy I've made where DateTime is the index and the values I need to mass replace are in the 'Value' column.\n  \n    The reason why is I need to interpolate between two dates in a time series, where the data I have is incorrect due to measurement error.  I need to replace it with interpolated data to make a more representative data set for forecasting\n  \n    Can anyone advise?"
},
{
    "title": "No title",
    "content": "I’m thinking about giving a talk at my Python group but struggling to think of an interesting topic. Any suggestions or recommendations of ways of brainstorming? I like running so was thinking about a talk about using the Strava API, maybe get the top running shoes, but not sure if that’d interest other people. Topic ideas please!"
},
{
    "title": "No title",
    "content": "Hi everyone,\n  \n\n\nGithub\n\n\n\nDocumentation\n\n\nWhat my project does :\n    For a long time, i had a problem of rendering svg to png format. Specially after my project required opengraph image generation. Vercel's OG supports this functionality but that's JavaScript.\n  \n    So therefore i created this bindings to resvg library (same library used by vercel og)\n  Targer Audience\n    Developers\n  Usage\n    Install it like this:\n  pip install resvg_py\n    Then use it like this:\n  import resvg_py\n\nsvg_string = \"\"\"\n    <svg width=\"300\" height=\"130\" xmlns=\"http://www.w3.org/2000/svg\">\n      <rect width=\"200\" height=\"100\" x=\"10\" y=\"10\" rx=\"20\" ry=\"20\" fill=\"blue\" />\n    </svg>\n\"\"\"\n\nprint(resvg_py.svg_to_base64(svg_string))Goals:\n\n\n    To enable all the features available in resvg but don’t write to the disk, everything must be done in memory.\n  \n\n\n    Use the bare minimum amount of packages, in both python side and rust side\n  \n\n\n    Make the package as user friendly as possible\n  \n\nComparison\n    I don't think theres any project that has safe bindings for resvg\n  \n    Please do note that this is my first time writing a package in rust."
},
{
    "title": "No title",
    "content": "https://github.com/allmonday/pydantic-resolve\n\n    supports both pydantic v1 and v2.\n  \nWhat My Project Does: a declaractive, schema based way for fetching and reorgnizing and changing data, from simple to complicated.\n  \nTarget Audience: production, web development\n  \nComparison: graphql frameworks. much simple, define specific schemas at backend. and can adjust data in backward stage of traversal.\n  \nIntroduction: I've used FastAPI for around two years, and like the pydantic as well.\n  \n    The idea of generating openapi.json from pydantic (response_model) is facinating, it help frontend generate clients based on it and simpilify the integration.\n  \n    I also use strawberry with FastAPI in some scenario, and enjoy the benefits from dataloaders.\n  \n    with pydantic you can define nested data structures\n  \n    but usually we need to construct the fields manually, or with the help of ORM relationship.\n  \n    So one day it comes with an idea, what if we put pydantic and aiodataloader together? pydantic + aiodataloader ?\n  \n    starts from some root data, and then let resolve / dataloader fetching the descendants?\n  \n    apis from restful service can provide root data, or single field query\n  \n    dataloader from gql can provide batch query for children and descendants.\n  class MySite(BaseModel):\n    name: str\n\n    blogs: list[Blog] = []\n    async def resolve_blogs(self):\n        return await get_blogs()\n\n    comment_count: int = 0\n    def post_comment_count(self):\n        # >> it will wait until all blogs are resolved\n        return sum([b.comment_count for b in self.blogs])\n\nclass Blog(BaseModel):\n    id: int\n    title: str\n\n    comments: list[Comment] = []\n    async def resolve_comments(self):\n        return await query_comments(self.id)\n\n    comment_count: int = 0\n    def post_comment_count(self):\n        return len(self.comments)\n\nclass Comment(BaseModel):\n    id: int\n    content: str\n    looks pretty like graphql but absolutely in pydantic.\n  \n    executing is also very simple:\n  async def main():\n    my_blog_site = MySite(name: \"tangkikodo's blog\")\n    my_blog_site = await Resolver().resolve(my_blog_site)\n    print(my_blog_site.json(indent=2))\n    using resolve and contexts related params can handle 90% features in graphql, it can also handle self-referencing data, like calculating the full path of each nodes of a tree.\n  class Tree(BaseModel):\n    name: str\n    children: List[Tree] = []\n\n    path: str = ''\n    def resolve_path(self, parent):\n        if parent is not None:\n            return f'{parent.path}/{self.name}'\n        return self.name\n    The most interesting part is post methods.\n  \n    the shortage of graphql or orm relationship is that we can only read the data by the structure they provided. for the fetched nested result, there is lack of ability to modify it in scope of each node. so it's always difficult to transform it.\n  \n    in daily frontend requirements, we need to merge, pick, flat, transform all kinds of nested data from backend, which means quite a lot of works.\n  \n    with post method, this become very simple.\n  \n    take tree for example, we can declarativly calculate the sum of descendants of each node.\n  class Tree(BaseModel):\n    count: int\n    children: List[Tree] = []\n\n    total: int = 0\n    def post_total(self):\n        return self.count + sum([c.total for c in self.children])\n    take blog site for example, post_comments can calculate the comment count of each blog.\n  \n    with collector https://allmonday.github.io/pydantic-resolve/reference_api/#collectors , post field can collect data from it's deep descendants.\n  \n    this provide a huge flexibility for reorganizing the data structure.\n  \n    This project is already in production environment, and has been tested for 1 year.\n  \n    hope to be helpful for you and welcome your suggestions!"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I’m an OSS developer (primarily working on Dask) and lately I’ve been talking to users about how they’re using Dask for ETL-style production workflows and this inspired me to make something myself. I wanted a simple example that met the following criteria:\n  \n\n\nRun locally (optionally). Should be easy to try out locally and easily scalable.\n  \n\n\nScalable to cloud. I didn’t want to think hard about cloud deployment.\n  \n\n\nPython forward. I wanted to use tools familiar to Python users, not an ETL expert.\n  \n\n\n    The resulting data pipeline uses Prefect for workflow orchestration, Dask to scale the data processing across a cluster, Delta Lake for storage, and Coiled to deploy Dask on the cloud.\n  \n    I really like the outcome, but wanted to get more balanced feedback since lately I’ve been more on the side of building these tools rather than using them heavily for data engineering. Some questions I’ve had include:\n  \n\n\nPrefect vs. Airflow vs. Dagster? For the users I’ve been working with at Coiled, Prefect is the most commonly used tool. I also know Dagster is quite popular and could easily be swapped into this example.\n  \n\n\nDeltaLake or something else? To be honest I mostly see vanilla Parquet in the wild, but I’ve been curious about Delta for a while and mostly wanted an excuse to try it out (pandas and Dask support improved a lot with delta-rs).\n  \n\n\n    Anyway, if people have a chance to read things over and give feedback I’d welcome constructive critique.\n  \n    Code: https://github.com/coiled/etl-tpchBlog post: https://docs.coiled.io/blog/easy-scalable-production-etl.html"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hi, I just published my mini extension to VSCode with a command to convert a JSON object file to Python, something I often missed.\n  \n    I hope you will find it useful!\n  \nhttps://marketplace.visualstudio.com/items?itemName=BringFuture.json-to-python\n\n    What My Project Does: Converts JSON to PythonTarget Audience: VSCode codersComparison: Formatters, etc. I could not find a tool to do exactly that"
},
{
    "title": "No title",
    "content": "https://www.python.org/downloads/release/python-3123/\n\n\n    3.12.3 is the latest maintenance release, containing more than 300 bugfixes, build improvements and documentation changes since 3.12.2."
},
{
    "title": "No title",
    "content": "When writing code or tests in Python, one issue I had was when the code would fail due to random things like network issues or external peripherals not responding in time.\n  \n    So I had to add extra code to retry the code, but this added unnecessary complexity.\n  \n    Thats when I discovered the Tenacity library and it saved me hours and a lot of useless boilerplate code.\n  \n    Link: https://tenacity.readthedocs.io/en/latest/\n\n    I wrote a blog on it with some examples:\n  \nhttps://new.pythonforengineers.com/blog/awesome-python-library-tenacity/"
},
{
    "title": "No title",
    "content": "In another article, I outlined how to handle Python imports and dependencies in Poetry. Poetry has a lot to recommend it:\n  \n\n\n    writes a pyproject.toml file for you\n  \n\n\n    creates an environment for you\n  \n\n\n    pins top-level dependencies in pyproject.toml (nice if that’s what you want)\n  \n\n\n    automates a lot of adding and removing dependencies\n  \n\n\n    all of the documentation is in one place\n  \n\n\n    But there are a few minor cons:\n  \n\n\n    adds minor complexity to ci\n  \n\n\n    adds minor complexity to tox\n  \n\n\n    some tools have a hard time finding your environment\n  \n\n\n    no PEP 621 compliance\n  \n\n\n    anthonywritescode doesn’t use it, and he knows more than most of the rest of us combined.\n  \n\n\n    So, here is an alternative (with its own minor pros and cons). I intend this as a simple reference you can paste commands out of. It will get you through the usual workflow of creating a Python library and hosting it on PyPI.\n  \nhttps://shayallenhill.com/setuptools"
},
{
    "title": "No title",
    "content": "What My Project Does?\n    Hello everyone, I'm proud to announce a brand-new Python library named pixi-kernel: https://github.com/renan-r-santos/pixi-kernel allowing you to run Jupyter kernels using Pixi for reproducible notebooks.\n  \n    By the way, if you haven't heard of the Pixi package manager, check it out at https://pixi.sh/latest/.It supports conda and PyPI (through uv) packages, pyproject.toml config, git, path and editable installs and it is really fast.\n  Target Audience\n    JupyterLab users, production-ready.\n  Comparison\n    The main differences compared to similar solutions such as poetry-kernel are the ability to install conda-forge packages and using kernels in languages other than Python. Compared to nb-conda-kernels, the difference is the use of the Pixi package manager which allows per-directory dependency locking."
},
{
    "title": "No title",
    "content": "https://github.com/RadoTheProgrammer/motsmeles/\n\n    Note: This is written in french.\n  What my project does\n    It generate word search games with custom words, dimensions.\n  Target audience\n    For people who want to get fun with some word searching\n  Comparison\n    This is a python library, it's open source, and you can use it in a python code, or with the command-line.\n  \n    Also, you have freedom if you want to have diagonal or reversed words.\n  Next steps\n    A GUI with pygame to generate and use it in a user-friendly interface."
},
{
    "title": "No title",
    "content": "Trying this again a third time because automod flagged it as a resource and the mods do not appear to be responding.https://github.com/heavy-resume/heavy-stack\n\nWhat it does: The Heavy Stack is a template web framework that is top to bottom Python using Sanic / (custom) ReactPy / Brython. It's a complete solution that just works out of the box.\n  \nTarget Audience: This is production capable but a little on the early side. Iterating is quite fast.\n  \nComparison:\n\n    Compared to other web frameworks:\n  \n\n\n    Just works out of the box, even has a docker image ready to go for the server and your postgres database that uses PG Vector.\n  \n\n\n    ReactPy is server side, so instead of creating API calls, you just use `heavy_use_effect` and `heavy_event` to respond to things, do async loads, etc. It uses a websocket. If you're familiar with ReactPy already, the difference is that there's `heavy` versions that inject a database connection to the async functions and let you stuff in user context.\n  \n\n\n    The custom version of ReactPy that ships with this reconnects the websocket and attempts to restore the client state. It also supports recording and replaying your actions and uses locust to do load testing with those actions.\n  \n\n\n    Hot reloading means if you change a code file you see your changes about 2 seconds after saving without having to refresh the page. For tests it means the test runs near instantly after the initial load.\n  \n\n\nDescription:\n\n    This is a dev container / GitHub Codespaces centric repo, so you'll be best served by using VS Code and either codespaces or something compatible the dev container standard. It's intended to be a template repo which you just run a script to rename to whatever project you want.\n  \n    Here are the features that work \"out of the box\" if you use the dev container or codespace:\n  \n\n\n    Pre-made dev container, docker files\n  \n\n\n    Top to Bottom Python (Sanic, custom ReactPy, Brython)\n  \n\n\n    PostgreSQL, PG Vector, CockroachDB\n  \n\n\n    SQL Model (SQLAlchemy + Pydantic)\n  \n\n\n    Hot reloading, both server and tests\n  \n\n\n    User action recording and playback for load testing\n  \n\n\n    Time tracking based on file changes\n  \n\n\n    Established patterns and examples, script to generate boilerplate around tables and their domain models\n  \n\n\n    There's some known issues and planned updates that will come at a later time, so for the sake of not overselling it, this should be treated like an alpha product. I'd love to see contributions though, especially when it comes to creating off-the-shelf components for ReactPy, or fixing the ReactPy bugs or getting that massive ReactPy PR mentioned in the readme piece-wise merged into the main project.\n  \n    Thanks for your time! I hope this is helpful for some people!"
},
{
    "title": "No title",
    "content": "I'm sure that many people have encountered the following (code for reproduction below) behavior:  if during the execution of `@property`, an `AttributeError` is raised and the same class implements `__getattr__` then `__getattr__` is invoked with the name of the property resulting in a confusing message: `AttributeError: 'Foo' object has no attribute 'something'`. If we remove `__getattr__` then we get a more meaningful and correct message: `AttributeError: 'Foo' object has no attribute 'bar'`.\n  from dataclasses import dataclass\n\n@dataclass\nclass Foo: val = 'foo_value'\n\nclass Test: \n    def __init__(self): \n        self.foo = Foo()\n\n    def __getattr__(self, name):\n        return getattr(self.foo, name)\n    \n    @property\n    def something(self):\n        return self.foo.bar\n\nt = Test() t.something\n    The error message is misleading and the behavior catches many people (myself including) in surprise and results in many wasted hours of debugging. I've seen people recommend using a custom decorator (instead of `@property` that catches attribute error) or wrapping your `@property` bodies in `try/except`. But it seems to me that this should be dealt with on language level.\n  \n    I'm trying to understand a few things:\n  \n\n\n    Is this behavior in Python intentional (i.e. is there a good reason) or just stems from the fact that `__getattr__` is invoked when `AttributeError` is raised when an instance attribute is accessed?\n  \n\n\n    If not intentional, is there a relatively easy way to detect `AttributeErrors` cause by `@property` execution in CPython? If so, why hasn't this been handled on language level?\n  \n\n\n    I've searched CPython issues on Github and I saw someone trying to correct documentation but couldn't find issues asking about this behavior or suggestions to fix - am I missing something?"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "At least a couple things that seems is that Litestar appears to be a fast  Django lite. Some batteries included, but not too many and opinionated. It's uses Rust so of course it's going to be fast, not that it really matters that much since IO > CPU for a website IMO. Which any async framework addresses this.\n  \n    I haven't heard many people using it though despite it having a lot of batteries included, but not dominating everything with first party support. Anyone using it in production in 2024?\n  \n    Edit:\n  \n    Fact check it appers it isn't written in Rust, I probably confused it with something else or it might just be data serialization"
},
{
    "title": "No title",
    "content": "In this article, I explain list comprehensions, as this is something people new to Python struggle with.\n  \nDemystifying list comprehensions in Python"
},
{
    "title": "No title",
    "content": "I'm working on an ETL job that reads in a JSONL file, splits it into multiple Polars LazyFrames, collects those to DataFrames, and then writes them to a Delta Table. Prior to doing this with Polars, I would generally complete a task like this with either Pandas or PySpark depending on the size of data I was dealing with. I'm looking for any suggestions or advice the community has on doing this as efficiently as possible using Polars."
},
{
    "title": "No title",
    "content": "A lot of people are starting to rely on ChatGPT to write code for them from time to time. What I have personally noticed is ChatGPT writes modules so heavy its ridiculous. Last night I was working on an issue a small but technical issue, and asked it for help, it spat out, I kid you not 200+ lines of code and I just thought to myself this cannot be right, it was so heavy I was shaking my head. After some trial and error I got a very elegant piece done in 15 lines of code. Way faster, cleaner and yes reusable. So, it's nice that you can ask CGPT a question from time to time but to rely on it for code structure at this point, in my opinion, no good. Nice to be here!"
},
{
    "title": "No title",
    "content": "Quick example:\n  from privates import private\n\n@private\nclass Hello:\n    __readonly__ = \"bar\",  # No need for @property!\n    __protected__ = \"foo\",\n\n    def __init__(self):\n        self.bar = \"hello world!\"\n        self.foo = \"foo\"\n\n# Hello is now only usable from this module\n    Repo: https://github.com/ZeroIntensity/privates.py"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I have searched for a workable solution for the marriage of FastAPI async REST with RabbitMQ async interop in one application. Here is a fully workable code I came up with (intentionally simplified).\n  class MessageBrokerManager:\n    async def consume(self, loop):\n        try:\n            pika.BlockingConnection(\n                parameters=pika.ConnectionParameters(\n                    host=self._host,\n                    port=self._port,\n                    credentials=pika.PlainCredentials(\n                        username=self._username,\n                        password=self._password,\n                    ),\n                )\n            )\n        except Exception as e:\n            log_info_fail(\"Failed to connect to RabbitMQ\")\n            # return empty task\n            return asyncio.create_task(asyncio.sleep(0))\n\n        self._connection = await connect_robust(\n            host=self._host,\n            port=self._port,\n            login=self._username,\n            password=self._password,\n            loop=loop,\n        )\n\n        self._channel = await self._connection.channel()\n        queue = await self._channel.declare_queue(self._queue_name, durable=True)\n        await queue.consume(callback=MessageBrokerManager.on_message, no_ack=True)\n\n        return self._connection\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    try:\n        loop = asyncio.get_running_loop()\n        task = loop.create_task(MessageBrokerManager().consume(loop))\n        await task\n    except Exception as e:\n        pass\n    yield\n\napp = FastAPI(lifespan=lifespan)\napp.include_router(some_rest_router)\n    Dependencies:\n  \n\n\n    pika>=1.3.2\n  \n\n\n    aio-pika>=9.4.1\n  \n\n\n    fastapi==0.109.2"
},
{
    "title": "No title",
    "content": "What My Project Does constable automatically injects print statements, during runtime, into your function code to give you a live step by step replay of each variable assignment.\n  \n    Github repo - https://github.com/saurabh0719/constable\n\nTarget Audience Can be used in testing/debugging environments. Do not leave this hanging around in production!\n  \nComparison Use pdb for all purposes that matter. Or an object state tracker.\n  \n    Example -\n  import constable\n\n@constable.trace('a', 'b')\ndef example(a, b):\n    a = a + b\n    c = a\n    a = \"Experimenting with the AST\"\n    b = c + b\n    a = c + b\n    return a\n\nexample(5, 6)\n    Output -\n  constable: example: line 5\n    a = a + b\n    a = 11\n    type(a) = <class 'int'>\n\nconstable: example: line 7\n    a = \"Experimenting with the AST\"\n    a = Experimenting with the AST\n    type(a) = <class 'str'>\n\nconstable: example: line 8\n    b = c + b\n    b = 17\n    type(b) = <class 'int'>\n\nconstable: example: line 9\n    a = c + b\n    a = 28\n    type(a) = <class 'int'>\n\nconstable: example: line 3 to 10\n    args: (5, 6)\n    kwargs: {}\n    returned: 28\n    execution time: 0.00018480 seconds"
},
{
    "title": "No title",
    "content": "For my machine learning project, I created an 'end to end' segmentation pipeline using MONAI and Pytorch for the deep learning portion, and the Optuna hyper parameter optimization library for hyperparam optimization / search. The entire project is fully encapsulated/packaged using Poetry, so its really easy to install and use. The pipeline is fully customizable in terms of which models to use, model parameters, optimizers, as well as hyperparameters via configuration files. It also takes advantage of automatic mixed-precision for accelerated compute. I also have it integrated with Aimstack for experiment tracking and visualization.\n  \nWhat My Project Does\n\n    Segmentation pipeline used for creating segmentation masks of diagnostic medical images in Nifti format. Streamlines the process of training, inference and hyperparameter search using Optuna and Aimstack integration. Modularized configuration files allow for specific use-case modification.\n  \nTarget Audience\n\n    Primarily meant for medical image and computer vision research & teaching. Not meant for clinical use.\n  \nComparison\n\n    Use a single pipeline for training, inference and visualization, quickly prototype models, and even benchmark HPC GPU performance.\n  \n    Overall, this was an amazing learning experience for me. I'm new to the world of ML, and I learned a lot while developing this. I've been running this on an HPC cluster with both A100 and H100 GPUs. What do you guy's think? https://github.com/adnan-umich/monai-train"
},
{
    "title": "No title",
    "content": "Hi all, here trying to replace values for a range of dates in a data frame copy I've made where DateTime is the index and the values I need to mass replace are in the 'Value' column.\n  \n    The reason why is I need to interpolate between two dates in a time series, where the data I have is incorrect due to measurement error.  I need to replace it with interpolated data to make a more representative data set for forecasting\n  \n    Can anyone advise?"
},
{
    "title": "No title",
    "content": "I’m thinking about giving a talk at my Python group but struggling to think of an interesting topic. Any suggestions or recommendations of ways of brainstorming? I like running so was thinking about a talk about using the Strava API, maybe get the top running shoes, but not sure if that’d interest other people. Topic ideas please!"
},
{
    "title": "No title",
    "content": "Hi everyone,\n  \n\n\nGithub\n\n\n\nDocumentation\n\n\nWhat my project does :\n    For a long time, i had a problem of rendering svg to png format. Specially after my project required opengraph image generation. Vercel's OG supports this functionality but that's JavaScript.\n  \n    So therefore i created this bindings to resvg library (same library used by vercel og)\n  Targer Audience\n    Developers\n  Usage\n    Install it like this:\n  pip install resvg_py\n    Then use it like this:\n  import resvg_py\n\nsvg_string = \"\"\"\n    <svg width=\"300\" height=\"130\" xmlns=\"http://www.w3.org/2000/svg\">\n      <rect width=\"200\" height=\"100\" x=\"10\" y=\"10\" rx=\"20\" ry=\"20\" fill=\"blue\" />\n    </svg>\n\"\"\"\n\nprint(resvg_py.svg_to_base64(svg_string))Goals:\n\n\n    To enable all the features available in resvg but don’t write to the disk, everything must be done in memory.\n  \n\n\n    Use the bare minimum amount of packages, in both python side and rust side\n  \n\n\n    Make the package as user friendly as possible\n  \n\nComparison\n    I don't think theres any project that has safe bindings for resvg\n  \n    Please do note that this is my first time writing a package in rust."
},
{
    "title": "No title",
    "content": "https://github.com/allmonday/pydantic-resolve\n\n    supports both pydantic v1 and v2.\n  \nWhat My Project Does: a declaractive, schema based way for fetching and reorgnizing and changing data, from simple to complicated.\n  \nTarget Audience: production, web development\n  \nComparison: graphql frameworks. much simple, define specific schemas at backend. and can adjust data in backward stage of traversal.\n  \nIntroduction: I've used FastAPI for around two years, and like the pydantic as well.\n  \n    The idea of generating openapi.json from pydantic (response_model) is facinating, it help frontend generate clients based on it and simpilify the integration.\n  \n    I also use strawberry with FastAPI in some scenario, and enjoy the benefits from dataloaders.\n  \n    with pydantic you can define nested data structures\n  \n    but usually we need to construct the fields manually, or with the help of ORM relationship.\n  \n    So one day it comes with an idea, what if we put pydantic and aiodataloader together? pydantic + aiodataloader ?\n  \n    starts from some root data, and then let resolve / dataloader fetching the descendants?\n  \n    apis from restful service can provide root data, or single field query\n  \n    dataloader from gql can provide batch query for children and descendants.\n  class MySite(BaseModel):\n    name: str\n\n    blogs: list[Blog] = []\n    async def resolve_blogs(self):\n        return await get_blogs()\n\n    comment_count: int = 0\n    def post_comment_count(self):\n        # >> it will wait until all blogs are resolved\n        return sum([b.comment_count for b in self.blogs])\n\nclass Blog(BaseModel):\n    id: int\n    title: str\n\n    comments: list[Comment] = []\n    async def resolve_comments(self):\n        return await query_comments(self.id)\n\n    comment_count: int = 0\n    def post_comment_count(self):\n        return len(self.comments)\n\nclass Comment(BaseModel):\n    id: int\n    content: str\n    looks pretty like graphql but absolutely in pydantic.\n  \n    executing is also very simple:\n  async def main():\n    my_blog_site = MySite(name: \"tangkikodo's blog\")\n    my_blog_site = await Resolver().resolve(my_blog_site)\n    print(my_blog_site.json(indent=2))\n    using resolve and contexts related params can handle 90% features in graphql, it can also handle self-referencing data, like calculating the full path of each nodes of a tree.\n  class Tree(BaseModel):\n    name: str\n    children: List[Tree] = []\n\n    path: str = ''\n    def resolve_path(self, parent):\n        if parent is not None:\n            return f'{parent.path}/{self.name}'\n        return self.name\n    The most interesting part is post methods.\n  \n    the shortage of graphql or orm relationship is that we can only read the data by the structure they provided. for the fetched nested result, there is lack of ability to modify it in scope of each node. so it's always difficult to transform it.\n  \n    in daily frontend requirements, we need to merge, pick, flat, transform all kinds of nested data from backend, which means quite a lot of works.\n  \n    with post method, this become very simple.\n  \n    take tree for example, we can declarativly calculate the sum of descendants of each node.\n  class Tree(BaseModel):\n    count: int\n    children: List[Tree] = []\n\n    total: int = 0\n    def post_total(self):\n        return self.count + sum([c.total for c in self.children])\n    take blog site for example, post_comments can calculate the comment count of each blog.\n  \n    with collector https://allmonday.github.io/pydantic-resolve/reference_api/#collectors , post field can collect data from it's deep descendants.\n  \n    this provide a huge flexibility for reorganizing the data structure.\n  \n    This project is already in production environment, and has been tested for 1 year.\n  \n    hope to be helpful for you and welcome your suggestions!"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I’m an OSS developer (primarily working on Dask) and lately I’ve been talking to users about how they’re using Dask for ETL-style production workflows and this inspired me to make something myself. I wanted a simple example that met the following criteria:\n  \n\n\nRun locally (optionally). Should be easy to try out locally and easily scalable.\n  \n\n\nScalable to cloud. I didn’t want to think hard about cloud deployment.\n  \n\n\nPython forward. I wanted to use tools familiar to Python users, not an ETL expert.\n  \n\n\n    The resulting data pipeline uses Prefect for workflow orchestration, Dask to scale the data processing across a cluster, Delta Lake for storage, and Coiled to deploy Dask on the cloud.\n  \n    I really like the outcome, but wanted to get more balanced feedback since lately I’ve been more on the side of building these tools rather than using them heavily for data engineering. Some questions I’ve had include:\n  \n\n\nPrefect vs. Airflow vs. Dagster? For the users I’ve been working with at Coiled, Prefect is the most commonly used tool. I also know Dagster is quite popular and could easily be swapped into this example.\n  \n\n\nDeltaLake or something else? To be honest I mostly see vanilla Parquet in the wild, but I’ve been curious about Delta for a while and mostly wanted an excuse to try it out (pandas and Dask support improved a lot with delta-rs).\n  \n\n\n    Anyway, if people have a chance to read things over and give feedback I’d welcome constructive critique.\n  \n    Code: https://github.com/coiled/etl-tpchBlog post: https://docs.coiled.io/blog/easy-scalable-production-etl.html"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hi, I just published my mini extension to VSCode with a command to convert a JSON object file to Python, something I often missed.\n  \n    I hope you will find it useful!\n  \nhttps://marketplace.visualstudio.com/items?itemName=BringFuture.json-to-python\n\n    What My Project Does: Converts JSON to PythonTarget Audience: VSCode codersComparison: Formatters, etc. I could not find a tool to do exactly that"
},
{
    "title": "No title",
    "content": "https://www.python.org/downloads/release/python-3123/\n\n\n    3.12.3 is the latest maintenance release, containing more than 300 bugfixes, build improvements and documentation changes since 3.12.2."
},
{
    "title": "No title",
    "content": "When writing code or tests in Python, one issue I had was when the code would fail due to random things like network issues or external peripherals not responding in time.\n  \n    So I had to add extra code to retry the code, but this added unnecessary complexity.\n  \n    Thats when I discovered the Tenacity library and it saved me hours and a lot of useless boilerplate code.\n  \n    Link: https://tenacity.readthedocs.io/en/latest/\n\n    I wrote a blog on it with some examples:\n  \nhttps://new.pythonforengineers.com/blog/awesome-python-library-tenacity/"
},
{
    "title": "No title",
    "content": "In another article, I outlined how to handle Python imports and dependencies in Poetry. Poetry has a lot to recommend it:\n  \n\n\n    writes a pyproject.toml file for you\n  \n\n\n    creates an environment for you\n  \n\n\n    pins top-level dependencies in pyproject.toml (nice if that’s what you want)\n  \n\n\n    automates a lot of adding and removing dependencies\n  \n\n\n    all of the documentation is in one place\n  \n\n\n    But there are a few minor cons:\n  \n\n\n    adds minor complexity to ci\n  \n\n\n    adds minor complexity to tox\n  \n\n\n    some tools have a hard time finding your environment\n  \n\n\n    no PEP 621 compliance\n  \n\n\n    anthonywritescode doesn’t use it, and he knows more than most of the rest of us combined.\n  \n\n\n    So, here is an alternative (with its own minor pros and cons). I intend this as a simple reference you can paste commands out of. It will get you through the usual workflow of creating a Python library and hosting it on PyPI.\n  \nhttps://shayallenhill.com/setuptools"
},
{
    "title": "No title",
    "content": "OverviewWhen deploying a new version of an application, it is essential to ensure that no tasks are lost during the deployment process. This is especially important for applications that use WebSockets to maintain real-time connections with clients. In such cases, abruptly terminating the application can lead to data loss and client disconnections.\n  \nWhat My Project Does?\n\n    This project demonstrates how to implement graceful shutdowns using FastAPI and Kubernetes, that use WebSockets client connections and internal background queues.https://github.com/jainal09/fastapi-gracefulshutdown-websockets\n\n\nTarget Audience\n\n    Production Use Cases"
},
{
    "title": "No title",
    "content": "Hello r/python 👋! Back in January, I've compiled a list of the most watched PyCon talks from 2023. I've received tons of positive feedback via DM, upvotes, and comments, so I decided to put together another compilation.\n  \n    This time around, I've gathered not only PyCon, but all Python 2023 talks across +100 conferences (here's the list) that include PyCon (all locations), PyData (all locations), EuroPython, Conf42, and many more to give you a complete overview of the landscape. The list is gigantic and includes over 850 talks**!**\n  \n    What's more, I've created a Google Sheets version of this post that gives more convenient sorting and filtering options.\n  \n    If it turns to be useful, don't forget to share the list with friends / colleagues!\n  \n\n\n\"Tutorials - Mario Munoz: Web Development With A Python-backed Frontend: Featuring HTMX and Tailwind\" ⸱ +12k views ⸱ 02h 26m 00s\n  \n\n\n\"Thomas Bierhance: Polars - make the switch to lightning-fast dataframes\" ⸱ +11k views ⸱ 00h 29m 49s\n  \n\n\n\"Mariatta Wijaya: Welcome to PyCon US 2023\" ⸱ +11k views ⸱ 00h 29m 29s\n  \n\n\n\"Scaling Python for Machine Learning: Beyond Data Parallelism • Holden Karau • GOTO 2023\" ⸱ +10k views ⸱ 00h 39m 19s\n  \n\n\n\"Tutorials - Matt Harrison: Getting Started with Polars\" ⸱ +10k views ⸱ 02h 14m 28s\n  \n\n\n\"Keynote Speaker - Ned Batchelder\" ⸱ +7k views ⸱ 00h 47m 02s\n  \n\n\n\"Gábor Szárnyas - DuckDB: The Power of a Data Warehouse in your Python Process\" ⸱ +7k views ⸱ 00h 55m 27s\n  \n\n\n\"Talks - Samuel Colvin: How Pydantic V2 leverages Rust's Superpowers\" ⸱ +6k views ⸱ 00h 45m 55s\n  \n\n\n\"Delta-rs, Apache Arrow, Polars, WASM: Is Rust the Future of Analytics?\" ⸱ +6k views ⸱ 00h 31m 03s\n  \n\n\n\"Tutorials - Simon Willison: Data analysis with SQLite and Python\" ⸱ +5k views ⸱ 02h 45m 54s\n  \n\n\n\"Talks - Hynek Schlawack: Subclassing, Composition, Python, and You\" ⸱ +5k views ⸱ 00h 45m 42s\n  \n\n\n\"Writing Python Bindings for C++ Libraries: Easy-to-use Performance - Saksham Sharma - CppCon 2023\" ⸱ +5k views ⸱ 01h 01m 30s\n  \n\n\n\"Pedro Holanda - DuckDB: Bringing analytical SQL directly to your Python shell\" ⸱ +5k views ⸱ 00h 29m 31s\n  \n\n\n\"Tutorials - Lisa Carpenter: How to create beautiful interactive GUIs and web apps\" ⸱ +5k views ⸱ 01h 58m 05s\n  \n\n\n\"Tutorials - Reuven M. Lerner: Comprehending comprehensions\" ⸱ +5k views ⸱ 02h 17m 33s\n  \n\n\n\"Keynote Speaker - Guido van Rossum\" ⸱ +5k views ⸱ 00h 30m 51s\n  \n\n\n\"PyData Heidelberg #11 - TimeSeries Forecasting & LLM Langchain\" ⸱ +4k views ⸱ 02h 39m 06s\n  \n\n\n\"Keynote Speaker - James Powell\" ⸱ +4k views ⸱ 00h 25m 43s\n  \n\n\n\"Use Spark from anywhere: A Spark client in Python powered by Spark Connect\" ⸱ +4k views ⸱ 00h 56m 58s\n  \n\n\n\"Cython 3 – Python at the speed of C — Stefan Behnel\" ⸱ +4k views ⸱ 00h 31m 15s\n  \n\n\n\"Patrick Blöbaum: Performing Root Cause Analysis with DoWhy, a Causal Machine-Learning Library\" ⸱ +4k views ⸱ 00h 44m 38s\n  \n\n\n\"Max Mergenthaler and Fede Garza - Quantifying Uncertainty in Time Series Forecasting\" ⸱ +4k views ⸱ 00h 37m 25s\n  \n\n\n\"Talks - Bruce Eckel: Rethinking Objects\" ⸱ +4k views ⸱ 00h 32m 06s\n  \n\n\n\"Rust for Python data engineers — Karim Jedda\" ⸱ +4k views ⸱ 00h 27m 30s\n  \n\n\n\"Matt Harrison - An Introduction to Pandas 2, Polars, and DuckDB | PyData Global 2023\" ⸱ +4k views ⸱ 01h 22m 15s\n  \n\n\n\"Tutorials - Ted Patrick: Writing Serverless Python Web Apps with PyScript\" ⸱ +3k views ⸱ 02h 55m 29s\n  \n\n\n\"Tutorials - Trey Hunner: Intro to Python for Brand New Programmers\" ⸱ +3k views ⸱ 02h 16m 41s\n  \n\n\n\"Dr. Thomas Wiecki: Bayesian Marketing Science - Solving Marketing's 3 Biggest Problems\" ⸱ +3k views ⸱ 00h 30m 35s\n  \n\n\n    [**\"Gaël Varoquaux: Prepping Tables for Machine Learning Gets Easier PyData Südwest]\"** ⸱ +3k views ⸱ 00h 57m 54s\n  \n\n\n\"Talks - Reuven M. Lerner: Generators, coroutines and nanoservices\" ⸱ +3k views ⸱ 00h 26m 28s\n  \n\n\n\"Talks - Brett Cannon: Python's syntactic sugar\" ⸱ +3k views ⸱ 00h 31m 11s\n  \n\n\n\"Bruno Vollmer: BLE and Python - How to build a simple BLE project on Linux with Python\" ⸱ +3k views ⸱ 00h 29m 40s\n  \n\n\n\"Vasileios Mourtakos - A data engineering framework in Python\" ⸱ +3k views ⸱ 00h 35m 55s\n  \n\n\n\"Talks - Eric Snow: A Per-Interpreter GIL: Concurrency and Parallelism with Subinterpreters\" ⸱ +3k views ⸱ 00h 30m 29s\n  \n\n\n\"Talks - Łukasz Langa: Working Around the GIL with asyncio\" ⸱ +2k views ⸱ 00h 44m 26s\n  \n\n\n\"Hajime Takeda - Media Mix Modeling:How to Measure the Effectiveness of Advertising\" ⸱ +2k views ⸱ 00h 30m 37s\n  \n\n\n\"Tutorials - Mike Müller: The How and Why of Object-oriented Programming in Python\" ⸱ +2k views ⸱ 02h 45m 34s\n  \n\n\n\"Shahriyar Rzayev: Building Hexagonal Python Services\" ⸱ +2k views ⸱ 01h 29m 50s\n  \n\n\n\"Performance tips by the FastAPI Expert — Marcelo Trylesinski\" ⸱ +2k views ⸱ 00h 24m 59s\n  \n\n\n\"Solving Multi-Objective Constrained Optimisation Problems using Pymoo — Pranjal Biyani\" ⸱ +2k views ⸱ 00h 44m 23s\n  \n\n\n\"Carsten Binnig: Towards Learned Database Systems\" ⸱ +2k views ⸱ 00h 43m 34s\n  \n\n\n\"Talks - Brandt Bucher: Inside CPython 3.11's new specializing, adaptive interpreter\" ⸱ +2k views ⸱ 00h 23m 01s\n  \n\n\n\"Carl Kadie - A Perfect, Infinite-Precision, Game Physics in Python | PyData Seattle 2023\" ⸱ +2k views ⸱ 00h 36m 26s\n  \n\n\n\"From crontab to celery with no regrets by Marco Pavanelli\" ⸱ +2k views ⸱ 00h 32m 03s\n  \n\n\n\"Talks - Al Sweigart: An Overview of the Python Code Tool Landscape 2023\" ⸱ +2k views ⸱ 00h 26m 58s\n  \n\n\n\"Reinventing Machine Learning with Transformers and Hugging Face by Keynote speaker Julien Simon\" ⸱ +2k views ⸱ 01h 08m 52s\n  \n\n\n\"Empower your Spring Applications with Python Features on GraalVM by Johannes Link @ Spring I/O 2023\" ⸱ +2k views ⸱ 00h 53m 00s\n  \n\n\n\"Marysia Winkels - Data Storytelling through Visualization\" ⸱ +2k views ⸱ 00h 30m 57s\n  \n\n\n\"What polars does for you — Ritchie Vink\" ⸱ +2k views ⸱ 00h 27m 45s\n  \n\n\n\"Beyond Toy Datasets: Timeseries Forecasting for Real Business Problems - Robert Haase\" ⸱ +2k views ⸱ 00h 33m 55s\n  \n\n\n\"PyData Online - An AI assistant for football analytics - Petar Veličković (Google DeepMind)\" ⸱ +2k views ⸱ 01h 02m 40s\n  \n\n\n\"Talks - Mark Shannon: How we are making CPython faster. Past, present and future.\" ⸱ +2k views ⸱ 00h 29m 11s\n  \n\n\n\"JIm Dowling - Build a production ML system with only Python on free serverless services\" ⸱ +2k views ⸱ 01h 22m 35s\n  \n\n\n\"Writing a Python interpreter from scratch, in half an hour — Tushar Sadhwani\" ⸱ +2k views ⸱ 00h 43m 38s\n  \n\n\n\"Dominika Basaj & Barbara Rychalska - Creating behavioral profiles of your customer\" ⸱ +2k views ⸱ 00h 26m 55s\n  \n\n\n\"Stephan Sahm: Accelerate Python with Julia\" ⸱ +2k views ⸱ 01h 27m 14s\n  \n\n\n\"Why Delta Lake is the Best Storage Format for Pandas Analyses\" ⸱ +2k views ⸱ 00h 28m 25s\n  \n\n\n\"Tutorials - Geir Arne Hjelle: Introduction to Decorators: Power Up Your Python Code\" ⸱ +2k views ⸱ 02h 21m 20s\n  \n\n\n\"Jeroen Overschie - How to create a Devcontainer for your Python project 🐳\" ⸱ +2k views ⸱ 00h 36m 01s\n  \n\n\n\"Wesley Boelrijk - Lowering the barrier for ML monitoring\" ⸱ +2k views ⸱ 00h 36m 53s\n  \n\n\n\"Talks - Dan Craig: Testing Spacecraft with Pytest\" ⸱ +2k views ⸱ 00h 30m 12s\n  \n\n\n\"Keynote Speaker - Margaret Mitchell\" ⸱ +2k views ⸱ 00h 42m 29s\n  \n\n\n\"Talks - Moshe Zadka: pyproject.toml, packaging, and you\" ⸱ +2k views ⸱ 00h 30m 06s\n  \n\n\n\"Tutorials - Juhi, Dana: Intro to Hugging Face: Fine-tuning BERT for NLP tasks\" ⸱ +2k views ⸱ 02h 09m 52s\n  \n\n\n\"Tutorials - Patrick Arminio: Build a production ready GraphQL API using Python\" ⸱ +2k views ⸱ 02h 23m 14s\n  \n\n\n\"Ryan Curtin - Lightweight, low-overhead, high-performance: machine learning directly in C++\" ⸱ +2k views ⸱ 00h 43m 10s\n  \n\n\n\"Python with Spark Connect\" ⸱ +2k views ⸱ 00h 33m 27s\n  \n\n\n\"David Qiu - Jupyter AI — Bringing Generative AI to Jupyter | PyData Seattle 2023\" ⸱ +1k views ⸱ 00h 39m 29s\n  \n\n\n\"Polars: A highly optimized dataframe library | Matt Harrison | Conf42 Machine Learning 2023\" ⸱ +1k views ⸱ 00h 20m 31s\n  \n\n\n\"Nico Kreiling: Raised by Pandas, striving for more: An opinionated introduction to Polars\" ⸱ +1k views ⸱ 00h 29m 47s\n  \n\n\n\"Jiang et al. - Automated Machine Learning & Tuning with FLAML | PyData Seattle 2023\" ⸱ +1k views ⸱ 01h 21m 54s\n  \n\n\n\"Alexander CS Hendorf: 5 Things about fastAPI I wish we had known beforehand\" ⸱ +1k views ⸱ 00h 32m 31s\n  \n\n\n\"Duarte Carmo - MLOps for the rest of us- A poor man's guide to putting models in production\" ⸱ +1k views ⸱ 00h 26m 04s\n  \n\n\n    [**\"Ines Montani (spaCy) - Large Language Models from Prototype to Production PyData Südwest]\"** ⸱ +1k views ⸱ 00h 39m 55s\n  \n\n\n\"Robin Raymond: Rusty Python - A Case Study\" ⸱ +1k views ⸱ 00h 28m 47s\n  \n\n\n\"Tutorials - Leah Berg, Ray: Feature Engineering is for Everyone!\" ⸱ +1k views ⸱ 02h 16m 35s\n  \n\n\n\"Giles Weaver & Ian Ozsvald - Pandas 2, Dask or Polars? Tackling larger data on a single machine\" ⸱ +1k views ⸱ 00h 16m 22s\n  \n\n\n\"Talks - Glyph: How To Keep A Secret\" ⸱ +1k views ⸱ 00h 26m 19s\n  \n\n\n\"Maximilian M. - SHAPtivating Insights: unravelling blackbox AI models\" ⸱ +1k views ⸱ 00h 35m 02s\n  \n\n\n\"Vahan Huroyan - Recent Developments in Self-Supervised Learning for Computer Vision\" ⸱ +1k views ⸱ 00h 36m 25s\n  \n\n\n\"Lucas Durand - Building an Interactive Network Graph to Understand Communities | PyData Seattle 2023\" ⸱ +1k views ⸱ 01h 26m 14s\n  \n\n\n\"Big PyData BBQ #5: LLMs feat. Ines Montani (spaCy) & Alejandro Saucedo (Zalando)\" ⸱ +1k views ⸱ 03h 48m 21s\n  \n\n\n\"Stop using print! Understanding and using the \"logging\" module — Reuven M. Lerner\" ⸱ +1k views ⸱ 00h 29m 32s\n  \n\n\n\"Joris Van den Bossche & Patrick Hoefler: Pandas 2.0 and beyond\" ⸱ +1k views ⸱ 00h 37m 07s\n  \n\n\n\"Structured output with large language models / Uri Goren (Argmax)\" ⸱ +1k views ⸱ 00h 25m 09s\n  \n\n\n\"PyData Chicago April 2023 Meetup | Design and Analysis of (Backtest) Experiments with R and Python\" ⸱ +1k views ⸱ 00h 50m 19s\n  \n\n\n\"Sponsor Presentation - Python & Bloomberg: An Open Source Duo\" ⸱ +1k views ⸱ 00h 57m 08s\n  \n\n\n\"Soumith Chintala - Keynote: AI & the stuff built for AI - are they actually useful for data science?\" ⸱ +1k views ⸱ 00h 31m 54s\n  \n\n\n\"The Future of Microprocessors — Sophie Wilson\" ⸱ +1k views ⸱ 00h 56m 45s\n  \n\n\n\"Jia Yu - How Apache Sedona is Revolutionizing Geospatial Data Analysis | PyData Seattle 2023\" ⸱ +1k views ⸱ 00h 46m 12s\n  \n\n\n\"Lightning Talks - April 21, 5pm\" ⸱ +1k views ⸱ 01h 04m 16s\n  \n\n\n\"How to Leverage the Full Potential of LLMs for Your Business with Langchain - Leon Ruddat\" ⸱ +1k views ⸱ 00h 34m 40s\n  \n\n\n\"Jay Chia & Sammy Sidhu: Daft - The Distributed Python Dataframe for Complex Data\" ⸱ +1k views ⸱ 00h 23m 01s\n  \n\n\n\"Talks - A. Jesse Jiryu Davis: Consistency and isolation for Python programmers\" ⸱ +1k views ⸱ 00h 24m 23s\n  \n\n\n\"\"Python deployment with Docker and Poetry\" - Cristian Heredia (PyBay 2023)\" ⸱ +1k views ⸱ 00h 12m 24s\n  \n\n\n\"Peterson & Qin - Contextual Multi-Arm Bandit and its applications to digital experiments | PyData\" ⸱ +1k views ⸱ 00h 44m 54s\n  \n\n\n\"Tutorials - Ron Nathaniel: How To Troubleshoot and Monitor Applications using OpenTelemetry\" ⸱ +1k views ⸱ 02h 01m 57s\n  \n\n\n\"Keynote Speaker - Python Steering Council\" ⸱ +1k views ⸱ 00h 20m 42s\n  \n\n\n\"An unbiased evaluation of environment management and packaging tools — Anna-Lena Popkes\" ⸱ +1k views ⸱ 00h 43m 14s\n  \n\n\n\"Vincent Gosselin - Turning your Data/AI algorithms into full web apps in no time with Taipy\" ⸱ +1k views ⸱ 00h 38m 18s\n  \n\n\n\"Malte Tichy - Knowing what you don’t know matters- Uncertainty-aware model rating\" ⸱ +1k views ⸱ 00h 30m 05s\n  \n\n\n\"Talks - Andrew Godwin: Reconciling Everything\" ⸱ +1k views ⸱ 00h 30m 16s\n  \n\n\n\"Vincent Warmerdam - Bulk Labelling Techniques\" ⸱ +1k views ⸱ 00h 32m 55s\n  \n\n\n\"Hugo Bowne-Anderson - Full-stack Machine Learning and Generative AI for Data Scientists\" ⸱ +1k views ⸱ 01h 28m 30s\n  \n\n\n\"J.J. Allaire - Keynote: Dashboards with Jupyter and Quarto | PyData NYC 2023\" ⸱ +1k views ⸱ 00h 40m 15s\n  \n\n\n\"Alejandro Saucedo - Industrial Strength DALLE-E:Scaling Complex Large Text & Image Models\" ⸱ +1k views ⸱ 00h 28m 07s\n  \n\n\n\"Subclassing, Composition, Python, and You — Hynek Schlawack\" ⸱ +1k views ⸱ 00h 44m 59s\n  \n\n\n\"Tutorials -Zac Hatfield-Dodds, Ryan Soklaski: Introduction to Property-Based Testing\" ⸱ +1k views ⸱ 01h 31m 20s\n  \n\n\n    [**\"Furkan M. Torun - Become a Data Storyteller with Streamlit! PyData Prague 2023-11-20]\"** ⸱ +1k views ⸱ 00h 23m 26s\n  \n\n\n\"Anna-Lena Popkes: An unbiased evaluation of environment management and packaging tools\" ⸱ +1k views ⸱ 00h 43m 53s\n  \n\n\n\"PyData Chicago March 2023 Meetup | Monte Carlo with QMCPy for Vector Functions of Integrals\" ⸱ +1k views ⸱ 00h 46m 35s\n  \n\n\n\"James Powell - Simple Simulators with pandas and Generator Coroutines | PyData NYC 2023\" ⸱ +1k views ⸱ 01h 21m 43s\n  \n\n\n\"Polars is the Pandas killer / Igor Mintz (Viz.ai)\" ⸱ +1k views ⸱ 00h 21m 46s\n  \n\n\n\"Diving into Event-Driven Architectures with Python — Marc-André Lemburg\" ⸱ +1k views ⸱ 00h 30m 59s\n  \n\n\n\"Using Embeddings and Deep Neural Networks as a technique for AutoML Demand Forecasting PyData SW\" ⸱ +1k views ⸱ 00h 36m 06s\n  \n\n\n\"Leland McInnes - Data Mapping for Data Exploration | PyData Seattle 2023\" ⸱ +1k views ⸱ 00h 38m 16s\n  \n\n\n\"Eduardo Blancas - Using embedded SQL engines for plotting massive datasets on a laptop\" ⸱ +1k views ⸱ 00h 27m 00s\n  \n\n\n\"Large Language Models: From Prototype to Production — Ines Montani\" ⸱ +1k views ⸱ 00h 40m 20s\n  \n\n\n\"Tutorials - Pavithra Eswaramoorthy, Dharhas Pothina: Data of Unusual Size: Interactive Visualization\" ⸱ +1k views ⸱ 03h 06m 45s\n  \n\n\n\"Moritz Meister - Data Validation for Feature pipelines: Using Great Expectations and Hopsworks\" ⸱ +1k views ⸱ 00h 27m 40s\n  \n\n\n\"Inge van den Ende-Leveraging conformal prediction for calibrated probabilistic time series forecasts\" ⸱ +1k views ⸱ 00h 31m 33s\n  \n\n\n\"Nick Sorros - A Tour of Large Language Models\" ⸱ +1k views ⸱ 00h 46m 38s\n  \n\n\n\"Thomas Frauholz: From notebook to pipeline in no time with LineaPy\" ⸱ +1k views ⸱ 00h 43m 17s\n  \n\n\n\"EuroPython 2023 Opening Session\" ⸱ +1k views ⸱ 00h 20m 58s\n  \n\n\n\"Optimizing Ad Conversions with DS / Yael Kiselman (DigitalTurbine)\" ⸱ +1k views ⸱ 00h 51m 44s\n  \n\n\n\"Harizo Rajaona - A Tour of the Many DataFrame Frameworks\" ⸱ +1k views ⸱ 00h 34m 59s\n  \n\n\n\"Andrei Alekseev - Why does everyone need to develop a machine learning package?\" ⸱ +1k views ⸱ 00h 28m 07s\n  \n\n\n\"Ties de Kok - Going beyond ChatGPT: introduction to prompt engineering & LLMs | PyData Seattle 2023\" ⸱ +1k views ⸱ 00h 59m 56s\n  \n\n\n\"PyData Boston Sept session 1: Mike Woodward - Data sci done wrong: how/why scientists make mistakes.\" ⸱ +1k views ⸱ 00h 27m 14s\n  \n\n\n\"Joe Cheng - Shiny: Data-centric web applications in Python | PyData Seattle 2023\" ⸱ +1k views ⸱ 00h 43m 56s\n  \n\n\n\"Tutorials - Cheuk Ting Ho: Power up your work with compiling and profiling\" ⸱ +1k views ⸱ 01h 33m 02s\n  \n\n\n\"Talks -Algorithmic ideas, engineering tricks, and trivia behind CPython's new sorting algorithm\" ⸱ +1k views ⸱ 00h 29m 39s\n  \n\n\n\"Talks - Dawn Wages: Supercharge your Python Development Environment with VS Code + Dev Container\" ⸱ +1k views ⸱ 00h 31m 01s\n  \n\n\n\"Egor Romanov - Performance of Vector Databases\" ⸱ +1k views ⸱ 00h 43m 09s\n  \n\n\n\"Alexander CS Hendorf - Ten Years of Community Organizer | PyData NYC 2023\" ⸱ +1k views ⸱ 00h 42m 41s\n  \n\n\n\"Talks - Nicholas H.Tollervey, Paul Everitt: Build Yourself a PyScript\" ⸱ +1k views ⸱ 00h 39m 28s\n  \n\n\n\"Emil Rijcken - FuzzyTM: a Python package for fuzzy topic models\" ⸱ +1k views ⸱ 00h 28m 12s\n  \n\n\n\"Nikolas Markou - Artificial Intelligence for Vision: A walkthrough of recent breakthroughs\" ⸱ +1k views ⸱ 00h 42m 10s\n  \n\n\n\"Would Rust make you a better Pythonista? — Alexys Jacob\" ⸱ +1k views ⸱ 00h 47m 23s\n  \n\n\n\"Simon Pressler: Getting started with JAX\" ⸱ +1k views ⸱ 00h 29m 49s\n  \n\n\n\"Sponsor Presentation - How to build stunning Data Science Web applications in Python\" ⸱ +1k views ⸱ 00h 54m 10s\n  \n\n\n\"Pavel Pleskov - All about ML competitions!\" ⸱ +1k views ⸱ 01h 03m 13s\n  \n\n\n\"Laszlo Sragner - Code Smells in Data Science: What can we do about them? | PyData London 2023\" ⸱ +1k views ⸱ 00h 39m 19s\n  \n\n\n\"Talks - Jodie Burchell: Vectorize using linear algebra and NumPy to make your Python code fast\" ⸱ +1k views ⸱ 00h 28m 32s\n  \n\n\n\"Comparing Elixir and Python when working with Simple Neural Networks - A. Neto & L. C. Tavano\" ⸱ +1k views ⸱ 00h 38m 11s\n  \n\n\n\"Talks - Valerio Maggio: Pythonic functional (iter)tools for your data challenges\" ⸱ +1k views ⸱ 00h 32m 38s\n  \n\n\n\"Topaz Gilad - Classification Through Regression:Unlock the True Potential of Your Labels\" ⸱ +1k views ⸱ 00h 28m 13s\n  \n\n\n\"Running Python packages in the browser with Pyodide — Roman Yurchak\" ⸱ +1k views ⸱ 00h 27m 49s\n  \n\n\n\"J.J. Allaire - Publishing Jupyter Notebooks with Quarto | PyData Seattle 2023\" ⸱ +1k views ⸱ 00h 40m 47s\n  \n\n\n\"ipyvizzu-story- a open-source tool to build create+share animated data stories w/ python in jupyter\" ⸱ +1k views ⸱ 01h 25m 54s\n  \n\n\n    Due to Reddit post length limit, see the remaining talks in this post or google sheet."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "https://www.python.org/downloads/release/python-3123/"
},
{
    "title": "No title",
    "content": "Repo: https://github.com/Nictec/Adnexus\n\nWhat My Project Does: Adnexus is a declarative and fully typed DI/IoC framework for python. It is inspired by python-dependency-injector because this project is no longer maintained.\n  \nTarget Audience: Currently Adnexus is in heavy development. So expect heavy errors and breaking changes. Eventually if we reach 1.0.0 the project will be ready for production.\n  \nComparison: The main difference to existing (and maintained) projects is the \"declarative\" part of the framework. Injectables are declared direclty in the container class using Providers. This takes away the \"magic\" of IoC because it is clear where your dependencies come from. Furthermore all dependencies can be directly configured using the integrated config system (for details see the repo)\n  \n    Please try it and give me Feedback! Contributions are always welcome just open a pull request or an issue."
},
{
    "title": "No title",
    "content": "Hey Python enthusiasts! 👋\n    I'm thrilled to introduce PGCacheWatch, my recent project designed to enhance your PostgreSQL databases by bringing real-time event notifications right into your applications. Vastly improved cache invalidation efficiency, without the need for adding any new services or infrastructure.\n  What My Project Does\n    PGCacheWatch capitalizes on PostgreSQL's built-in NOTIFY/LISTEN features to deliver instant, real-time notifications about database events, ensuring your application's cache remains fresh and synchronized with your database.\n  Target Audience\n    This project is tailored for developers and architects striving to maintain high-performance and scalable Python applications with real-time data demands. Whether you're developing web applications using FastAPI, orchestrating data processing pipelines, or any application dependent on PostgreSQL for data storage, PGCacheWatch simplifies cache invalidation management.\n  Comparison\n    PGCacheWatch differentiates itself from other caching and notification mechanisms by:\n  \n\n\nZero Additional Infrastructure: Utilizing PostgreSQL's existing functionalities, it circumvents the necessity for external tools or services.\n  \n\nKey Features\n\n\nInstant Database Change Notifications: Ensures your application is immediately informed about database events as they occur, keeping data consistently updated.\n  \n\n\nAdaptable Cache Invalidation Strategies: Offers a range of strategies to precisely control the cache invalidation process, striking a balance between performance and data fidelity.\n  \n\n\nOptimized for Async Python: Conceived for the asynchronous Python ecosystem, making it a superb fit for applications utilizing asyncpg and FastAPI.\n  \n\nJoin Me on GitHub\n    I'm keen to see how PGCacheWatch can bolster your projects. Your feedback and contributions are greatly appreciated as I aspire to continually enhance and broaden the library's functionality.\n  \nExplore PGCacheWatch on GitHub"
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    A macro-nutrient and calorie calculator to help me achieve body composition goals.This calculator is compatible with metric and imperial units. It supports weight changes based on percentage and fixed values (lbs, kg). This is useful when losing weight in a fat loss phase as a percentage, and for gaining at a fixed value during a muscle growth phase.The recommendations are built using scientific research based on lean body mass. If the user's body fat percentage is unknown, then it will be calculated using BMI and the Deurenberg formula.General guidance is also available on the project page.\n  \nKey Features:\n\n\n\n    Compatible with metric and imperial units\n  \n\n\n    Weight change based on percentage or fixed values\n  \n\n\n    Recommendations based on scientific research\n  \n\n\n    Body Fat Percentage Calculator\n  \n\n\n    Choose where to place remaining calories (after minimum fat & protein targets reached)\n  \n\n\n    Safety Limits (Max Change & Caloric Intake Below BMR)\n  \n\n\n\nTarget Audience\n\n    This calculator is ready for production. Ideally, any end user should be able to use this program, not just developers. I hope that this calculator can help the general public.\n  \n\nComparison\n\n    There are many other macro calculators available. However, I feel that this one is a step beyond other calculators. This calculator will always make sure that the end user reaches their minimum protein and fat amounts. It also also allows the user to select where to place any remaining calories. Some may prefer to place remaining calories in carbs, while others may wish to stay in ketosis and place remaining calories in fat, or even a mix of all macro-nutrients. It also has built in safety mechanism to keep calories and macros adequate for health status.\n  \nProject Page:Advanced Macro Calculator on GitHub"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "https://flox.dev/blog/jupyter-remote-env\n\n    Hey everyone, we released Flox 1.0 a few weeks ago and one of the cool things you can do is set up environments with a bunch of dependencies already set up for running a Jupyter notebook. Take a look to see how it all works!"
},
{
    "title": "No title",
    "content": "What my project does\nautomathon is a Python library for simulating and visualizing finite automata, is easy to use and have a docs website to read more about the functions that are implemented for DFAs and NFAs.\n  \n    During my CS degree, I learned about automata and this became one of my favorite topics, that's why I started to work on this project, I wanted to create something simple and easy to use.\n  Target audience\n    Students, professors and any developer that could need to use automata.\n  Comparison\n    I don't know if there's any python library that let you simulate finite automata, but I'm sure you will love this one, there's a blog talking about automata and have examples using automathon, you can read it, it's written in French."
},
{
    "title": "No title",
    "content": "What my project does\n    My project generate simple, strong, memorable and easy-to-type passwords.\n  Target audience\n    For anyone who need to get passwords easily.\n  Comparison\n    Most passwords manager generate completely passwords with completely random characters that aren't very easy to memorize or tape.\n  \n    Examples include Dashlane, Norton, Avast.\n  \n    Or other like Bitwarden generate passwords that are not really fast-to-type.\n  \n    The mine generate sth like 6Nixe#Becokace 0Qubyby+Pomafy , or 7Zuxogu:Lebuwo . I wanted to make a password generator that combines simplicity, security, memorability, and ease of type.\n  Usage\n    You can install it with pip install pwd-generator and use the cli version:\n  pwdgen\n    To use it in a python code\n  import pwdgen\nprint(pwdgen.generate())Source code\n    The source code is on github"
},
{
    "title": "No title",
    "content": "It appeared too easy to me that we could create entire GUI from Python. I waited for the other shoe to drop, and it dropped. Now Anaconda and PySimpleGUI require payment if ever you want to use the libraries professionally.\n  \n    I wonder whether this will be the end of Python. It seems enshittification is finally reaching Python, where people previously used libraries for free and when they update them find out that they have to pay or stop using the libraries."
},
{
    "title": "No title",
    "content": "What My Project Does\n    Making asynchronous web API calls with asyncio and aiohttp can be complicated. This async_api_caller package abstracts away that complexity for a common case of needing to make multiple web API calls while varying query parameters.\n  \nInstallation\ngit clone git@github.com:amentumspace/async_api_caller.git\ncd async_api_caller/\npip install .\nUsage\nimport async_api_caller\nurl = \"https://ocean.amentum.io/gebco\"\nheaders = {'API-Key': API_KEY}\nparam_list = [\n    {\n        \"latitude\": 42,\n        \"longitude\": 42\n    },{\n        \"latitude\": 43,\n        \"longitude\": 43\n    }\n]\n\nresponses_json = async_api_caller.run(\n    url, headers, param_list\n)Target Audience\n    Python developers or data scientists or scientists or any Pythonista wanting a simple way to make asynchronous web API calls.\n  Comparison\n    Differs from existing alternatives in its simplicity. Simple like grequests without needing gevents. It really just abstracts the usual boilerplate needed to use asyncio with aiohttp."
},
{
    "title": "No title",
    "content": "I’m excited to share something we’ve been working on at Wolt: a new dependency injection library for Python “magic-di”. Born from our experience with a large service that has many components such as: API, event consumers, background workers, and cron jobs. magic-di aims to simplify dependency management without the hassle.\n  from magic_di.fastapi import inject_app, Provide\n\napp = inject_app(FastAPI())\n\n@app.get(path=\"/hello-reddit\") \ndef hello_reddit(service: Provide[Service]) -> dict: \n  return { \n     \"is_connected\": service.is_connected(),\n  }\nWhat Project Doesmagic-di cuts through the complexity of dependency management by adhering to a “zero-config” philosophy. It was developed to address the challenges we faced in managing code dependencies across various deployments, aiming for a straightforward solution.\n  \nCore Concepts\n\n\n\nZero Config: Setup is a breeze with magic-di. There’s no need to wrangle with injector configurations. All parameters in dependencies should either be injected or fetched from the environment, file, or any other source\n  \n\n\nConnectable Dependencies: Dependencies should implement a Connectable interface, which mandates __connect__ and __disconnect__ methods. This neat feature ensures that your dependencies are properly initialized and shut down.\n  \n\n\nDependency Order Resolution: magic-di ensures dependencies are injected in the correct order and ready to use right away once it’s injected\n  \n\n\nTarget Audiencemagic-di is ideal for Python developers looking for a straightforward dependency injection solution that helps to think only about business logic and not about boilerplate code. It’s designed to keep things simple, focusing on getting the job done without unnecessary configuration.\n  \nComparisonmagic-di differs from existing solutions by focusing on simplicity and practicality:\n  \nCompared to FastAPI’s built-in DI:FastAPI couples your logic to its ecosystem. magic-di offers a framework-agnostic approach, making it easier to integrate without tying your code to a specific framework.\n  \nCompared to python-dependency-injector: This library can be heavy on boilerplate. magic-di aims to reduce boilerplate to the bare minimum, advocating for a design where dependencies are self-configuring.\n  \nGoalsOur main goal with magic-di is to provide a DI tool that’s easy to integrate and simplifies the developer’s workflow. We wanted to address the common pain points in dependency management without introducing new complexities.\n  \n    In essence, magic-di is about making dependency injection straightforward and hassle-free for Python developers. It’s a tool born from practical needs, and while it may not revolutionize the field, it certainly aims to improve how we handle dependencies in our projects.\n  \n    We believe magic-di can make a positive impact on your projects, and we’re eager for you to try it out and share your thoughts. Your feedback is crucial as we continue to refine and improve this tool\n  \nRead more about magic-di: https://github.com/woltapp/magic-di"
},
{
    "title": "No title",
    "content": "Hello python community. I’m pretty new to scripting and come from a pure chemistry background. I’m working with 3-D MALDI-trapped ion mobility mass spectrometry data and I’ve been looking for a solid algorithm that can peak pick from two ‘time-resolved’ dimensions (mass and mobility), and the intensity arrays associated with those two dimensions. I need to peak pick from a heatmap essentially. Does anyone know of any good 3-D peak picking algorithms, or otherwise know how I can approach this? Thank you all!"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I think I see this a lot.\n  \n    I believe SQL, including like dataframe SQL packages, should always be used over Pandas when possible unless one is doing something Pandas is more suited to or unless someone requires broader Python control structures or features.\n  \n    I am a big believer in sticking to SQL as a baseline data manipulation framework and only using other frameworks if SQL is lacking. One of the biggest ways to accumulate technical debt is solving problems in multiple ways that are not standardized that not all people know."
},
{
    "title": "No title",
    "content": "Hey everyone!\n  \n    I wrote this quick Python script showing how you can automatically extract information from phone calls with LLMS. The call I use is a phone call received by a home building company where the caller is inquiring about an estimate for the cost to build a home on land he may purchase. Here's what the LLM extracted:\n  SUMMARY:\n- The caller is interested in getting an estimate for building a house on a property he is looking to purchase in Westchester.\n\nACTION ITEMS:\n- Have someone call the customer back today to discuss building estimate.\n- Set up time for builder to assess potential property site prior to purchase.\n\nCONTACT INFORMATION:\nName: Lindstrom, Kenny\nPhone number: 610-265-1715\n    If you skip to around 4:25 in this video you can see that all the information is extracted accurately! You can check out the code in this GitHub repository.\n  \n    It's really straightforward and only requires a handful of lines of code - feel free to drop any questions if you have any!"
},
{
    "title": "No title",
    "content": "I wrote my own Pyside6 alternative to Pysimple gui that I use in my projects starting some years back after some unsatisfactory interactions with the author of Pysimple gui.\n  \n    The GIT source for my GUI wrapper is in\n  \nhttps://github.com/David-Worboys/Black-DVD-Archiver/blob/main/qtgui.py\n\n    And the open source project it is used in is\n  \nhttps://github.com/David-Worboys/Black-DVD-Archiver\n\n    Just look for the layout methods in the .py files to see how it is used. https://github.com/David-Worboys/Black-DVD-Archiver/blob/main/video_cutter.py#L2391 is a good place to start, line 953,.\n  \n    Note size measurements are generally in chars, but pixels can be used\n  \n    Make no mistake there is a lot of work to produuctionise this and make a Python library, including, most importantly, doco of which there is none.\n  \n    If anyone is interested I will set up a new Git project for it..\n  \n    I am living on borrowed time and have to focus more on my other projectsWith the demise of open Pysimplegui is anyone interested in helping productionise this alternative?\n  \n    Help\n  \n    I wrote my own Pyside6 alternative to Pysimple gui that  I use in myprojects starting some years back after some unsatisfactoryinteractions with the author of Pysimple gui.\n  \n    The GIT source for my GUI wrapper is in\n  \nhttps://github.com/David-Worboys/Black-DVD-Archiver/blob/main/qtgui.py\n\n    And the open source project it is used in is\n  \nhttps://github.com/David-Worboys/Black-DVD-Archiver\n\n    Just look for the layout methods in the .py files to see how it is used. https://github.com/David-Worboys/Black-DVD-Archiver/blob/main/video_cutter.py#L2391 is a good place to start, line 953,.\n  \n    Note size measurements are generally in chars, but pixels can be used\n  \n    Make no mistake there is a lot of work to produuctionise this andmake a Python library, including, most importantly, doco of which thereis none.\n  \n    If anyone is interested I will set up a new Git project for it..\n  \n    I am living on borrowed time and have to focus more on my other projects"
},
{
    "title": "No title",
    "content": "Hello, everyone. I'm Kennedy Guerra, 33, open-source software maintainer. Just wanted to share a new feature I'm working on for the next version of Nodezator (GitHub | website), my public domain generalist Python node editor.\n  \n    In case you don't know Nodezator, the next few sections present it briefly. If you want to know about the new feature I'm working on, you can skip straight to the last subsection, which contains a detailed explanation and links to videos and extra info about the feature, called Automated System Testing.\n  What my project does\n    Nodezator is a generalist Python node editor (a desktop app) that allows developers to build and execute visual graphs whose nodes represent Python callables (your own and/or callables from Python standard or third-party libraries).\n  Target audience\n\n\n    Python developers in all areas. Intermediate Python knowledge is advised.\n  \n\n\n    You can also use it as a no-code interface, since people editing/executing the graph only have to deal with widgets and basic node editing operations like connecting/disconnecting sockets, creating or deleting nodes, etc.\n  \n\nComparison\n    Nodezator has actually many features that are either absent in other similar apps or are very rarely found:\n  \n\n\n    it is dedicated to the public domain with much love\n  \n\n\n    devs only need to define Python functions and the app automatically turns them into nodes (yes, no need to subclass anything)\n  \n\n\n    instead of writing your own functions, you can also feed existing functions from third-party libraries and the app will also turn them into nodes, virtually making Nodezator an \"everything nodes\" solution\n  \n\n\n    the graphs created can be converted back into Python code with the click of a button\n  \n\n\n    exporting to SVG, HTML+SVG and PNG is also available\n  \n\n\n    in the same way functions can receive other functions, nodes can receive other nodes as arguments, allowing the creation of powerful operations (functions are first-class citizens after all, so the nodes that represent them are as well)\n  \n\n\n    a comprehensive user manual available both inside the app and online.\n  \n\nNew feature on development branch: Automated System Testing\n    What I wanted to highlight today, though, is a feature I'm currently working on, called automated system testing, which is available in the development branch.\n  \n    Here's the link to a GitHub devlog post that contains a video demonstrating the feature in action: https://github.com/IndiePython/nodezator/discussions/72#discussioncomment-8997584\n\n    On the post you can also read more about the feature and how to reproduce the behaviour inside the app.\n  \n    Here's a brief explanation: once the user picks the tests to be performed in a dedicated form and clicks \"Start system testing session\", the app takes over and performs all the tests automatically by controlling the GUI. At the end of each test case, the app checks the state for expected values. After all test cases are performed, the app shows a report.\n  \n    I'm very relieved this feature is finally implemented and should soon be released (it needs a lot of polishing and is only available in the development branch, but is already functional). The reason for this is that as the project grows and becomes more and more complex, I needed to automated this kind of tests to ensure a more healthy/feasible development and more stable releases.\n  \n    That is, there are so many different tasks performed in the app that testing it manually takes a lot of time. As we all know, development requires a lot of iteration, so the enormous amount of time required to test everything manually would be spent many times over the implementation of even the simplest changes.\n  \n    Once this feature is released, we'll be able to automatically test everything and thus get feedback much earlier regarding the impact of our changes throughout the plethora of possible tasks that comprise Nodezator's workflow. Of course we also have unit tests, and they are important, but they are useful for testing units of behaviour, not their synergy when fully integrated in the working app as the user operates it.\n  \n    Once released, this feature will help me and other contributors check much sooner and with more precision whether our work is breaking any existing tasks/workflows/features. Sooner because the automated tests can be executed as fast as possible, as shown in the video, not only in normal time. With precision because the exact mouse and keyboard interactions are reproduced in the exact same way every time each test runs.\n  \n    This app is part of the Indie Python project, a personal project of mine to create and maintain completely free and open-source software (apps and games) and also release instructional/educational content related to them. Such software/content is created and maintained with much love, so please, consider supporting us if you have the means: https://patreon.com/KennedyRichard, https://github.com/sponsors/KennedyRichard, https://indiepython.com/donate.\n  \n    Thank you for your attention, let me know if you have any questions."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "In Gmail I realized that I had way too many filters that performed exactly the same action, but they simply had different \"from\" addresses. To solve this I created this little project that merges similar filters into a single one.\n  \n    It does require a lot of setup due to having to authenticate into Gmail, but if someone is on the same situation as me, it could be helpful.\n  \n    Link to the project: github\nWhat My Project Does\n    It connects into a Gmail account, reads their filters and squashes them into a single one.\n  \n    The program requires a manual setup of creating an OAuth credentials screen and downloading relevant credentials, which is cumbersome. But it's a one time effort as you can keep on using the application as long as you keep access to the GCP project.\n  \n    Note that it only merges filters if their criteria is a single \"from\" statement and they perform exactly the same action.\n  Target Audience\n    It's just a toy project of mine that I did for my sole benefit, but thought that it could be of use to anyone else in my same situation.\n  Comparison\n    I tried to look up for an already-existing solution for this issue before implementing it myself, but I couldn't find it. Relevant Gmail support post.\n  \n    Please do let me know if you have any suggestion or advice.\n  \n    Link to the project: github"
},
{
    "title": "No title",
    "content": "Hello everyone, I've been programming for about 4 years now and this is my first ever library that I created!\n  What My Project Does\n    It's called Reddit2Text, and it converts a reddit post (and all its comments) into a single, clean, easy to copy/paste string.\n  \n    I often like to ask ChatGPT about reddit posts, but copying all the relevant information among a large amount of comments is difficult/impossible. I searched for a tool or library that would help me do this and was astonished to find no such thing! I took it into my own hands and decided to make it myself.\n  Target Audience\n    This project is useable in its current state, and always looking for more feedback/features from the community!\n  Comparison\n    There are no other similar alternatives AFAIK\n  \n    Here is the GitHub repo: https://github.com/NFeruch/reddit2text\n\n    It's also available to download through pip/pypi :D\n  \n    Some basic features:\n  \n\n\n    Gathers the authors, upvotes, and text for the OP and every single comment\n  \n\n\n    Specify the max depth for how many comments you want\n  \n\n\n    Change the delimiter for the comment nesting\n  \n\n\n    Here is an example truncated output: https://pastebin.com/mmHFJtccUnder the hood, I relied heavily on the PRAW library (python reddit api wrapper) to do the actual interfacing with the Reddit API. I took it a step further though, by combining all these moving parts and raw outputs into something that's easily useable and very simple.Could you see yourself using something like this?"
},
{
    "title": "No title",
    "content": "Python has become a popular inclusion as a scripting language for many professional software packages. However, the APIs presented by these applications may be unsafe and/or do not include pythonic error handling. As someone working in the energy sector at my day job, I often find myself wrestling issues external to my python code in our power system simulator (usually memory related) that due to the implementation of the API can error at runtime below the exception handling mechanism of the interpreter. I will have have to implement a library for this purpose either internally or as an open-source side project.\n  \n    Traditionally, we handle these errors by isolating all potentially unsafe API calls in their own function and running that function in a separate process with multiprocessing and a handler function orchestrating the process and handling the exit codes. Although effective, this basic solution doesn't have the level of functionality needed (no logging, no stdout/stderr redirection, no exceptions, etc) and brings a decent amount of boilerplate to simple scripts.\n  \n    I wanted to post here and see if this is an issue others in the community face interacting with external code written in lower level languages they cannot freely modify. This isn't a detailed proposal more so trying to generate discussion around the idea, but essentially the module would provide decorators to wrap the external calls safely, with extra features like logging, custom exception classes and more to allow pythonic handling of errors.\n  \n    TLDR: Would there be interest in a library centered around pain-free handling of runtime errors that happen below the exception handling mechanism with extra convenience features?"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "OverviewWhen deploying a new version of an application, it is essential to ensure that no tasks are lost during the deployment process. This is especially important for applications that use WebSockets to maintain real-time connections with clients. In such cases, abruptly terminating the application can lead to data loss and client disconnections.\n  \nWhat My Project Does?\n\n    This project demonstrates how to implement graceful shutdowns using FastAPI and Kubernetes, that use WebSockets client connections and internal background queues.https://github.com/jainal09/fastapi-gracefulshutdown-websockets\n\n\nTarget Audience\n\n    Production Use Cases"
},
{
    "title": "No title",
    "content": "Hello r/python 👋! Back in January, I've compiled a list of the most watched PyCon talks from 2023. I've received tons of positive feedback via DM, upvotes, and comments, so I decided to put together another compilation.\n  \n    This time around, I've gathered not only PyCon, but all Python 2023 talks across +100 conferences (here's the list) that include PyCon (all locations), PyData (all locations), EuroPython, Conf42, and many more to give you a complete overview of the landscape. The list is gigantic and includes over 850 talks**!**\n  \n    What's more, I've created a Google Sheets version of this post that gives more convenient sorting and filtering options.\n  \n    If it turns to be useful, don't forget to share the list with friends / colleagues!\n  \n\n\n\"Tutorials - Mario Munoz: Web Development With A Python-backed Frontend: Featuring HTMX and Tailwind\" ⸱ +12k views ⸱ 02h 26m 00s\n  \n\n\n\"Thomas Bierhance: Polars - make the switch to lightning-fast dataframes\" ⸱ +11k views ⸱ 00h 29m 49s\n  \n\n\n\"Mariatta Wijaya: Welcome to PyCon US 2023\" ⸱ +11k views ⸱ 00h 29m 29s\n  \n\n\n\"Scaling Python for Machine Learning: Beyond Data Parallelism • Holden Karau • GOTO 2023\" ⸱ +10k views ⸱ 00h 39m 19s\n  \n\n\n\"Tutorials - Matt Harrison: Getting Started with Polars\" ⸱ +10k views ⸱ 02h 14m 28s\n  \n\n\n\"Keynote Speaker - Ned Batchelder\" ⸱ +7k views ⸱ 00h 47m 02s\n  \n\n\n\"Gábor Szárnyas - DuckDB: The Power of a Data Warehouse in your Python Process\" ⸱ +7k views ⸱ 00h 55m 27s\n  \n\n\n\"Talks - Samuel Colvin: How Pydantic V2 leverages Rust's Superpowers\" ⸱ +6k views ⸱ 00h 45m 55s\n  \n\n\n\"Delta-rs, Apache Arrow, Polars, WASM: Is Rust the Future of Analytics?\" ⸱ +6k views ⸱ 00h 31m 03s\n  \n\n\n\"Tutorials - Simon Willison: Data analysis with SQLite and Python\" ⸱ +5k views ⸱ 02h 45m 54s\n  \n\n\n\"Talks - Hynek Schlawack: Subclassing, Composition, Python, and You\" ⸱ +5k views ⸱ 00h 45m 42s\n  \n\n\n\"Writing Python Bindings for C++ Libraries: Easy-to-use Performance - Saksham Sharma - CppCon 2023\" ⸱ +5k views ⸱ 01h 01m 30s\n  \n\n\n\"Pedro Holanda - DuckDB: Bringing analytical SQL directly to your Python shell\" ⸱ +5k views ⸱ 00h 29m 31s\n  \n\n\n\"Tutorials - Lisa Carpenter: How to create beautiful interactive GUIs and web apps\" ⸱ +5k views ⸱ 01h 58m 05s\n  \n\n\n\"Tutorials - Reuven M. Lerner: Comprehending comprehensions\" ⸱ +5k views ⸱ 02h 17m 33s\n  \n\n\n\"Keynote Speaker - Guido van Rossum\" ⸱ +5k views ⸱ 00h 30m 51s\n  \n\n\n\"PyData Heidelberg #11 - TimeSeries Forecasting & LLM Langchain\" ⸱ +4k views ⸱ 02h 39m 06s\n  \n\n\n\"Keynote Speaker - James Powell\" ⸱ +4k views ⸱ 00h 25m 43s\n  \n\n\n\"Use Spark from anywhere: A Spark client in Python powered by Spark Connect\" ⸱ +4k views ⸱ 00h 56m 58s\n  \n\n\n\"Cython 3 – Python at the speed of C — Stefan Behnel\" ⸱ +4k views ⸱ 00h 31m 15s\n  \n\n\n\"Patrick Blöbaum: Performing Root Cause Analysis with DoWhy, a Causal Machine-Learning Library\" ⸱ +4k views ⸱ 00h 44m 38s\n  \n\n\n\"Max Mergenthaler and Fede Garza - Quantifying Uncertainty in Time Series Forecasting\" ⸱ +4k views ⸱ 00h 37m 25s\n  \n\n\n\"Talks - Bruce Eckel: Rethinking Objects\" ⸱ +4k views ⸱ 00h 32m 06s\n  \n\n\n\"Rust for Python data engineers — Karim Jedda\" ⸱ +4k views ⸱ 00h 27m 30s\n  \n\n\n\"Matt Harrison - An Introduction to Pandas 2, Polars, and DuckDB | PyData Global 2023\" ⸱ +4k views ⸱ 01h 22m 15s\n  \n\n\n\"Tutorials - Ted Patrick: Writing Serverless Python Web Apps with PyScript\" ⸱ +3k views ⸱ 02h 55m 29s\n  \n\n\n\"Tutorials - Trey Hunner: Intro to Python for Brand New Programmers\" ⸱ +3k views ⸱ 02h 16m 41s\n  \n\n\n\"Dr. Thomas Wiecki: Bayesian Marketing Science - Solving Marketing's 3 Biggest Problems\" ⸱ +3k views ⸱ 00h 30m 35s\n  \n\n\n    [**\"Gaël Varoquaux: Prepping Tables for Machine Learning Gets Easier PyData Südwest]\"** ⸱ +3k views ⸱ 00h 57m 54s\n  \n\n\n\"Talks - Reuven M. Lerner: Generators, coroutines and nanoservices\" ⸱ +3k views ⸱ 00h 26m 28s\n  \n\n\n\"Talks - Brett Cannon: Python's syntactic sugar\" ⸱ +3k views ⸱ 00h 31m 11s\n  \n\n\n\"Bruno Vollmer: BLE and Python - How to build a simple BLE project on Linux with Python\" ⸱ +3k views ⸱ 00h 29m 40s\n  \n\n\n\"Vasileios Mourtakos - A data engineering framework in Python\" ⸱ +3k views ⸱ 00h 35m 55s\n  \n\n\n\"Talks - Eric Snow: A Per-Interpreter GIL: Concurrency and Parallelism with Subinterpreters\" ⸱ +3k views ⸱ 00h 30m 29s\n  \n\n\n\"Talks - Łukasz Langa: Working Around the GIL with asyncio\" ⸱ +2k views ⸱ 00h 44m 26s\n  \n\n\n\"Hajime Takeda - Media Mix Modeling:How to Measure the Effectiveness of Advertising\" ⸱ +2k views ⸱ 00h 30m 37s\n  \n\n\n\"Tutorials - Mike Müller: The How and Why of Object-oriented Programming in Python\" ⸱ +2k views ⸱ 02h 45m 34s\n  \n\n\n\"Shahriyar Rzayev: Building Hexagonal Python Services\" ⸱ +2k views ⸱ 01h 29m 50s\n  \n\n\n\"Performance tips by the FastAPI Expert — Marcelo Trylesinski\" ⸱ +2k views ⸱ 00h 24m 59s\n  \n\n\n\"Solving Multi-Objective Constrained Optimisation Problems using Pymoo — Pranjal Biyani\" ⸱ +2k views ⸱ 00h 44m 23s\n  \n\n\n\"Carsten Binnig: Towards Learned Database Systems\" ⸱ +2k views ⸱ 00h 43m 34s\n  \n\n\n\"Talks - Brandt Bucher: Inside CPython 3.11's new specializing, adaptive interpreter\" ⸱ +2k views ⸱ 00h 23m 01s\n  \n\n\n\"Carl Kadie - A Perfect, Infinite-Precision, Game Physics in Python | PyData Seattle 2023\" ⸱ +2k views ⸱ 00h 36m 26s\n  \n\n\n\"From crontab to celery with no regrets by Marco Pavanelli\" ⸱ +2k views ⸱ 00h 32m 03s\n  \n\n\n\"Talks - Al Sweigart: An Overview of the Python Code Tool Landscape 2023\" ⸱ +2k views ⸱ 00h 26m 58s\n  \n\n\n\"Reinventing Machine Learning with Transformers and Hugging Face by Keynote speaker Julien Simon\" ⸱ +2k views ⸱ 01h 08m 52s\n  \n\n\n\"Empower your Spring Applications with Python Features on GraalVM by Johannes Link @ Spring I/O 2023\" ⸱ +2k views ⸱ 00h 53m 00s\n  \n\n\n\"Marysia Winkels - Data Storytelling through Visualization\" ⸱ +2k views ⸱ 00h 30m 57s\n  \n\n\n\"What polars does for you — Ritchie Vink\" ⸱ +2k views ⸱ 00h 27m 45s\n  \n\n\n\"Beyond Toy Datasets: Timeseries Forecasting for Real Business Problems - Robert Haase\" ⸱ +2k views ⸱ 00h 33m 55s\n  \n\n\n\"PyData Online - An AI assistant for football analytics - Petar Veličković (Google DeepMind)\" ⸱ +2k views ⸱ 01h 02m 40s\n  \n\n\n\"Talks - Mark Shannon: How we are making CPython faster. Past, present and future.\" ⸱ +2k views ⸱ 00h 29m 11s\n  \n\n\n\"JIm Dowling - Build a production ML system with only Python on free serverless services\" ⸱ +2k views ⸱ 01h 22m 35s\n  \n\n\n\"Writing a Python interpreter from scratch, in half an hour — Tushar Sadhwani\" ⸱ +2k views ⸱ 00h 43m 38s\n  \n\n\n\"Dominika Basaj & Barbara Rychalska - Creating behavioral profiles of your customer\" ⸱ +2k views ⸱ 00h 26m 55s\n  \n\n\n\"Stephan Sahm: Accelerate Python with Julia\" ⸱ +2k views ⸱ 01h 27m 14s\n  \n\n\n\"Why Delta Lake is the Best Storage Format for Pandas Analyses\" ⸱ +2k views ⸱ 00h 28m 25s\n  \n\n\n\"Tutorials - Geir Arne Hjelle: Introduction to Decorators: Power Up Your Python Code\" ⸱ +2k views ⸱ 02h 21m 20s\n  \n\n\n\"Jeroen Overschie - How to create a Devcontainer for your Python project 🐳\" ⸱ +2k views ⸱ 00h 36m 01s\n  \n\n\n\"Wesley Boelrijk - Lowering the barrier for ML monitoring\" ⸱ +2k views ⸱ 00h 36m 53s\n  \n\n\n\"Talks - Dan Craig: Testing Spacecraft with Pytest\" ⸱ +2k views ⸱ 00h 30m 12s\n  \n\n\n\"Keynote Speaker - Margaret Mitchell\" ⸱ +2k views ⸱ 00h 42m 29s\n  \n\n\n\"Talks - Moshe Zadka: pyproject.toml, packaging, and you\" ⸱ +2k views ⸱ 00h 30m 06s\n  \n\n\n\"Tutorials - Juhi, Dana: Intro to Hugging Face: Fine-tuning BERT for NLP tasks\" ⸱ +2k views ⸱ 02h 09m 52s\n  \n\n\n\"Tutorials - Patrick Arminio: Build a production ready GraphQL API using Python\" ⸱ +2k views ⸱ 02h 23m 14s\n  \n\n\n\"Ryan Curtin - Lightweight, low-overhead, high-performance: machine learning directly in C++\" ⸱ +2k views ⸱ 00h 43m 10s\n  \n\n\n\"Python with Spark Connect\" ⸱ +2k views ⸱ 00h 33m 27s\n  \n\n\n\"David Qiu - Jupyter AI — Bringing Generative AI to Jupyter | PyData Seattle 2023\" ⸱ +1k views ⸱ 00h 39m 29s\n  \n\n\n\"Polars: A highly optimized dataframe library | Matt Harrison | Conf42 Machine Learning 2023\" ⸱ +1k views ⸱ 00h 20m 31s\n  \n\n\n\"Nico Kreiling: Raised by Pandas, striving for more: An opinionated introduction to Polars\" ⸱ +1k views ⸱ 00h 29m 47s\n  \n\n\n\"Jiang et al. - Automated Machine Learning & Tuning with FLAML | PyData Seattle 2023\" ⸱ +1k views ⸱ 01h 21m 54s\n  \n\n\n\"Alexander CS Hendorf: 5 Things about fastAPI I wish we had known beforehand\" ⸱ +1k views ⸱ 00h 32m 31s\n  \n\n\n\"Duarte Carmo - MLOps for the rest of us- A poor man's guide to putting models in production\" ⸱ +1k views ⸱ 00h 26m 04s\n  \n\n\n    [**\"Ines Montani (spaCy) - Large Language Models from Prototype to Production PyData Südwest]\"** ⸱ +1k views ⸱ 00h 39m 55s\n  \n\n\n\"Robin Raymond: Rusty Python - A Case Study\" ⸱ +1k views ⸱ 00h 28m 47s\n  \n\n\n\"Tutorials - Leah Berg, Ray: Feature Engineering is for Everyone!\" ⸱ +1k views ⸱ 02h 16m 35s\n  \n\n\n\"Giles Weaver & Ian Ozsvald - Pandas 2, Dask or Polars? Tackling larger data on a single machine\" ⸱ +1k views ⸱ 00h 16m 22s\n  \n\n\n\"Talks - Glyph: How To Keep A Secret\" ⸱ +1k views ⸱ 00h 26m 19s\n  \n\n\n\"Maximilian M. - SHAPtivating Insights: unravelling blackbox AI models\" ⸱ +1k views ⸱ 00h 35m 02s\n  \n\n\n\"Vahan Huroyan - Recent Developments in Self-Supervised Learning for Computer Vision\" ⸱ +1k views ⸱ 00h 36m 25s\n  \n\n\n\"Lucas Durand - Building an Interactive Network Graph to Understand Communities | PyData Seattle 2023\" ⸱ +1k views ⸱ 01h 26m 14s\n  \n\n\n\"Big PyData BBQ #5: LLMs feat. Ines Montani (spaCy) & Alejandro Saucedo (Zalando)\" ⸱ +1k views ⸱ 03h 48m 21s\n  \n\n\n\"Stop using print! Understanding and using the \"logging\" module — Reuven M. Lerner\" ⸱ +1k views ⸱ 00h 29m 32s\n  \n\n\n\"Joris Van den Bossche & Patrick Hoefler: Pandas 2.0 and beyond\" ⸱ +1k views ⸱ 00h 37m 07s\n  \n\n\n\"Structured output with large language models / Uri Goren (Argmax)\" ⸱ +1k views ⸱ 00h 25m 09s\n  \n\n\n\"PyData Chicago April 2023 Meetup | Design and Analysis of (Backtest) Experiments with R and Python\" ⸱ +1k views ⸱ 00h 50m 19s\n  \n\n\n\"Sponsor Presentation - Python & Bloomberg: An Open Source Duo\" ⸱ +1k views ⸱ 00h 57m 08s\n  \n\n\n\"Soumith Chintala - Keynote: AI & the stuff built for AI - are they actually useful for data science?\" ⸱ +1k views ⸱ 00h 31m 54s\n  \n\n\n\"The Future of Microprocessors — Sophie Wilson\" ⸱ +1k views ⸱ 00h 56m 45s\n  \n\n\n\"Jia Yu - How Apache Sedona is Revolutionizing Geospatial Data Analysis | PyData Seattle 2023\" ⸱ +1k views ⸱ 00h 46m 12s\n  \n\n\n\"Lightning Talks - April 21, 5pm\" ⸱ +1k views ⸱ 01h 04m 16s\n  \n\n\n\"How to Leverage the Full Potential of LLMs for Your Business with Langchain - Leon Ruddat\" ⸱ +1k views ⸱ 00h 34m 40s\n  \n\n\n\"Jay Chia & Sammy Sidhu: Daft - The Distributed Python Dataframe for Complex Data\" ⸱ +1k views ⸱ 00h 23m 01s\n  \n\n\n\"Talks - A. Jesse Jiryu Davis: Consistency and isolation for Python programmers\" ⸱ +1k views ⸱ 00h 24m 23s\n  \n\n\n\"\"Python deployment with Docker and Poetry\" - Cristian Heredia (PyBay 2023)\" ⸱ +1k views ⸱ 00h 12m 24s\n  \n\n\n\"Peterson & Qin - Contextual Multi-Arm Bandit and its applications to digital experiments | PyData\" ⸱ +1k views ⸱ 00h 44m 54s\n  \n\n\n\"Tutorials - Ron Nathaniel: How To Troubleshoot and Monitor Applications using OpenTelemetry\" ⸱ +1k views ⸱ 02h 01m 57s\n  \n\n\n\"Keynote Speaker - Python Steering Council\" ⸱ +1k views ⸱ 00h 20m 42s\n  \n\n\n\"An unbiased evaluation of environment management and packaging tools — Anna-Lena Popkes\" ⸱ +1k views ⸱ 00h 43m 14s\n  \n\n\n\"Vincent Gosselin - Turning your Data/AI algorithms into full web apps in no time with Taipy\" ⸱ +1k views ⸱ 00h 38m 18s\n  \n\n\n\"Malte Tichy - Knowing what you don’t know matters- Uncertainty-aware model rating\" ⸱ +1k views ⸱ 00h 30m 05s\n  \n\n\n\"Talks - Andrew Godwin: Reconciling Everything\" ⸱ +1k views ⸱ 00h 30m 16s\n  \n\n\n\"Vincent Warmerdam - Bulk Labelling Techniques\" ⸱ +1k views ⸱ 00h 32m 55s\n  \n\n\n\"Hugo Bowne-Anderson - Full-stack Machine Learning and Generative AI for Data Scientists\" ⸱ +1k views ⸱ 01h 28m 30s\n  \n\n\n\"J.J. Allaire - Keynote: Dashboards with Jupyter and Quarto | PyData NYC 2023\" ⸱ +1k views ⸱ 00h 40m 15s\n  \n\n\n\"Alejandro Saucedo - Industrial Strength DALLE-E:Scaling Complex Large Text & Image Models\" ⸱ +1k views ⸱ 00h 28m 07s\n  \n\n\n\"Subclassing, Composition, Python, and You — Hynek Schlawack\" ⸱ +1k views ⸱ 00h 44m 59s\n  \n\n\n\"Tutorials -Zac Hatfield-Dodds, Ryan Soklaski: Introduction to Property-Based Testing\" ⸱ +1k views ⸱ 01h 31m 20s\n  \n\n\n    [**\"Furkan M. Torun - Become a Data Storyteller with Streamlit! PyData Prague 2023-11-20]\"** ⸱ +1k views ⸱ 00h 23m 26s\n  \n\n\n\"Anna-Lena Popkes: An unbiased evaluation of environment management and packaging tools\" ⸱ +1k views ⸱ 00h 43m 53s\n  \n\n\n\"PyData Chicago March 2023 Meetup | Monte Carlo with QMCPy for Vector Functions of Integrals\" ⸱ +1k views ⸱ 00h 46m 35s\n  \n\n\n\"James Powell - Simple Simulators with pandas and Generator Coroutines | PyData NYC 2023\" ⸱ +1k views ⸱ 01h 21m 43s\n  \n\n\n\"Polars is the Pandas killer / Igor Mintz (Viz.ai)\" ⸱ +1k views ⸱ 00h 21m 46s\n  \n\n\n\"Diving into Event-Driven Architectures with Python — Marc-André Lemburg\" ⸱ +1k views ⸱ 00h 30m 59s\n  \n\n\n\"Using Embeddings and Deep Neural Networks as a technique for AutoML Demand Forecasting PyData SW\" ⸱ +1k views ⸱ 00h 36m 06s\n  \n\n\n\"Leland McInnes - Data Mapping for Data Exploration | PyData Seattle 2023\" ⸱ +1k views ⸱ 00h 38m 16s\n  \n\n\n\"Eduardo Blancas - Using embedded SQL engines for plotting massive datasets on a laptop\" ⸱ +1k views ⸱ 00h 27m 00s\n  \n\n\n\"Large Language Models: From Prototype to Production — Ines Montani\" ⸱ +1k views ⸱ 00h 40m 20s\n  \n\n\n\"Tutorials - Pavithra Eswaramoorthy, Dharhas Pothina: Data of Unusual Size: Interactive Visualization\" ⸱ +1k views ⸱ 03h 06m 45s\n  \n\n\n\"Moritz Meister - Data Validation for Feature pipelines: Using Great Expectations and Hopsworks\" ⸱ +1k views ⸱ 00h 27m 40s\n  \n\n\n\"Inge van den Ende-Leveraging conformal prediction for calibrated probabilistic time series forecasts\" ⸱ +1k views ⸱ 00h 31m 33s\n  \n\n\n\"Nick Sorros - A Tour of Large Language Models\" ⸱ +1k views ⸱ 00h 46m 38s\n  \n\n\n\"Thomas Frauholz: From notebook to pipeline in no time with LineaPy\" ⸱ +1k views ⸱ 00h 43m 17s\n  \n\n\n\"EuroPython 2023 Opening Session\" ⸱ +1k views ⸱ 00h 20m 58s\n  \n\n\n\"Optimizing Ad Conversions with DS / Yael Kiselman (DigitalTurbine)\" ⸱ +1k views ⸱ 00h 51m 44s\n  \n\n\n\"Harizo Rajaona - A Tour of the Many DataFrame Frameworks\" ⸱ +1k views ⸱ 00h 34m 59s\n  \n\n\n\"Andrei Alekseev - Why does everyone need to develop a machine learning package?\" ⸱ +1k views ⸱ 00h 28m 07s\n  \n\n\n\"Ties de Kok - Going beyond ChatGPT: introduction to prompt engineering & LLMs | PyData Seattle 2023\" ⸱ +1k views ⸱ 00h 59m 56s\n  \n\n\n\"PyData Boston Sept session 1: Mike Woodward - Data sci done wrong: how/why scientists make mistakes.\" ⸱ +1k views ⸱ 00h 27m 14s\n  \n\n\n\"Joe Cheng - Shiny: Data-centric web applications in Python | PyData Seattle 2023\" ⸱ +1k views ⸱ 00h 43m 56s\n  \n\n\n\"Tutorials - Cheuk Ting Ho: Power up your work with compiling and profiling\" ⸱ +1k views ⸱ 01h 33m 02s\n  \n\n\n\"Talks -Algorithmic ideas, engineering tricks, and trivia behind CPython's new sorting algorithm\" ⸱ +1k views ⸱ 00h 29m 39s\n  \n\n\n\"Talks - Dawn Wages: Supercharge your Python Development Environment with VS Code + Dev Container\" ⸱ +1k views ⸱ 00h 31m 01s\n  \n\n\n\"Egor Romanov - Performance of Vector Databases\" ⸱ +1k views ⸱ 00h 43m 09s\n  \n\n\n\"Alexander CS Hendorf - Ten Years of Community Organizer | PyData NYC 2023\" ⸱ +1k views ⸱ 00h 42m 41s\n  \n\n\n\"Talks - Nicholas H.Tollervey, Paul Everitt: Build Yourself a PyScript\" ⸱ +1k views ⸱ 00h 39m 28s\n  \n\n\n\"Emil Rijcken - FuzzyTM: a Python package for fuzzy topic models\" ⸱ +1k views ⸱ 00h 28m 12s\n  \n\n\n\"Nikolas Markou - Artificial Intelligence for Vision: A walkthrough of recent breakthroughs\" ⸱ +1k views ⸱ 00h 42m 10s\n  \n\n\n\"Would Rust make you a better Pythonista? — Alexys Jacob\" ⸱ +1k views ⸱ 00h 47m 23s\n  \n\n\n\"Simon Pressler: Getting started with JAX\" ⸱ +1k views ⸱ 00h 29m 49s\n  \n\n\n\"Sponsor Presentation - How to build stunning Data Science Web applications in Python\" ⸱ +1k views ⸱ 00h 54m 10s\n  \n\n\n\"Pavel Pleskov - All about ML competitions!\" ⸱ +1k views ⸱ 01h 03m 13s\n  \n\n\n\"Laszlo Sragner - Code Smells in Data Science: What can we do about them? | PyData London 2023\" ⸱ +1k views ⸱ 00h 39m 19s\n  \n\n\n\"Talks - Jodie Burchell: Vectorize using linear algebra and NumPy to make your Python code fast\" ⸱ +1k views ⸱ 00h 28m 32s\n  \n\n\n\"Comparing Elixir and Python when working with Simple Neural Networks - A. Neto & L. C. Tavano\" ⸱ +1k views ⸱ 00h 38m 11s\n  \n\n\n\"Talks - Valerio Maggio: Pythonic functional (iter)tools for your data challenges\" ⸱ +1k views ⸱ 00h 32m 38s\n  \n\n\n\"Topaz Gilad - Classification Through Regression:Unlock the True Potential of Your Labels\" ⸱ +1k views ⸱ 00h 28m 13s\n  \n\n\n\"Running Python packages in the browser with Pyodide — Roman Yurchak\" ⸱ +1k views ⸱ 00h 27m 49s\n  \n\n\n\"J.J. Allaire - Publishing Jupyter Notebooks with Quarto | PyData Seattle 2023\" ⸱ +1k views ⸱ 00h 40m 47s\n  \n\n\n\"ipyvizzu-story- a open-source tool to build create+share animated data stories w/ python in jupyter\" ⸱ +1k views ⸱ 01h 25m 54s\n  \n\n\n    Due to Reddit post length limit, see the remaining talks in this post or google sheet."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "https://www.python.org/downloads/release/python-3123/"
},
{
    "title": "No title",
    "content": "Repo: https://github.com/Nictec/Adnexus\n\nWhat My Project Does: Adnexus is a declarative and fully typed DI/IoC framework for python. It is inspired by python-dependency-injector because this project is no longer maintained.\n  \nTarget Audience: Currently Adnexus is in heavy development. So expect heavy errors and breaking changes. Eventually if we reach 1.0.0 the project will be ready for production.\n  \nComparison: The main difference to existing (and maintained) projects is the \"declarative\" part of the framework. Injectables are declared direclty in the container class using Providers. This takes away the \"magic\" of IoC because it is clear where your dependencies come from. Furthermore all dependencies can be directly configured using the integrated config system (for details see the repo)\n  \n    Please try it and give me Feedback! Contributions are always welcome just open a pull request or an issue."
},
{
    "title": "No title",
    "content": "Hey Python enthusiasts! 👋\n    I'm thrilled to introduce PGCacheWatch, my recent project designed to enhance your PostgreSQL databases by bringing real-time event notifications right into your applications. Vastly improved cache invalidation efficiency, without the need for adding any new services or infrastructure.\n  What My Project Does\n    PGCacheWatch capitalizes on PostgreSQL's built-in NOTIFY/LISTEN features to deliver instant, real-time notifications about database events, ensuring your application's cache remains fresh and synchronized with your database.\n  Target Audience\n    This project is tailored for developers and architects striving to maintain high-performance and scalable Python applications with real-time data demands. Whether you're developing web applications using FastAPI, orchestrating data processing pipelines, or any application dependent on PostgreSQL for data storage, PGCacheWatch simplifies cache invalidation management.\n  Comparison\n    PGCacheWatch differentiates itself from other caching and notification mechanisms by:\n  \n\n\nZero Additional Infrastructure: Utilizing PostgreSQL's existing functionalities, it circumvents the necessity for external tools or services.\n  \n\nKey Features\n\n\nInstant Database Change Notifications: Ensures your application is immediately informed about database events as they occur, keeping data consistently updated.\n  \n\n\nAdaptable Cache Invalidation Strategies: Offers a range of strategies to precisely control the cache invalidation process, striking a balance between performance and data fidelity.\n  \n\n\nOptimized for Async Python: Conceived for the asynchronous Python ecosystem, making it a superb fit for applications utilizing asyncpg and FastAPI.\n  \n\nJoin Me on GitHub\n    I'm keen to see how PGCacheWatch can bolster your projects. Your feedback and contributions are greatly appreciated as I aspire to continually enhance and broaden the library's functionality.\n  \nExplore PGCacheWatch on GitHub"
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    A macro-nutrient and calorie calculator to help me achieve body composition goals.This calculator is compatible with metric and imperial units. It supports weight changes based on percentage and fixed values (lbs, kg). This is useful when losing weight in a fat loss phase as a percentage, and for gaining at a fixed value during a muscle growth phase.The recommendations are built using scientific research based on lean body mass. If the user's body fat percentage is unknown, then it will be calculated using BMI and the Deurenberg formula.General guidance is also available on the project page.\n  \nKey Features:\n\n\n\n    Compatible with metric and imperial units\n  \n\n\n    Weight change based on percentage or fixed values\n  \n\n\n    Recommendations based on scientific research\n  \n\n\n    Body Fat Percentage Calculator\n  \n\n\n    Choose where to place remaining calories (after minimum fat & protein targets reached)\n  \n\n\n    Safety Limits (Max Change & Caloric Intake Below BMR)\n  \n\n\n\nTarget Audience\n\n    This calculator is ready for production. Ideally, any end user should be able to use this program, not just developers. I hope that this calculator can help the general public.\n  \n\nComparison\n\n    There are many other macro calculators available. However, I feel that this one is a step beyond other calculators. This calculator will always make sure that the end user reaches their minimum protein and fat amounts. It also also allows the user to select where to place any remaining calories. Some may prefer to place remaining calories in carbs, while others may wish to stay in ketosis and place remaining calories in fat, or even a mix of all macro-nutrients. It also has built in safety mechanism to keep calories and macros adequate for health status.\n  \nProject Page:Advanced Macro Calculator on GitHub"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "https://flox.dev/blog/jupyter-remote-env\n\n    Hey everyone, we released Flox 1.0 a few weeks ago and one of the cool things you can do is set up environments with a bunch of dependencies already set up for running a Jupyter notebook. Take a look to see how it all works!"
},
{
    "title": "No title",
    "content": "What my project does\nautomathon is a Python library for simulating and visualizing finite automata, is easy to use and have a docs website to read more about the functions that are implemented for DFAs and NFAs.\n  \n    During my CS degree, I learned about automata and this became one of my favorite topics, that's why I started to work on this project, I wanted to create something simple and easy to use.\n  Target audience\n    Students, professors and any developer that could need to use automata.\n  Comparison\n    I don't know if there's any python library that let you simulate finite automata, but I'm sure you will love this one, there's a blog talking about automata and have examples using automathon, you can read it, it's written in French."
},
{
    "title": "No title",
    "content": "What my project does\n    My project generate simple, strong, memorable and easy-to-type passwords.\n  Target audience\n    For anyone who need to get passwords easily.\n  Comparison\n    Most passwords manager generate completely passwords with completely random characters that aren't very easy to memorize or tape.\n  \n    Examples include Dashlane, Norton, Avast.\n  \n    Or other like Bitwarden generate passwords that are not really fast-to-type.\n  \n    The mine generate sth like 6Nixe#Becokace 0Qubyby+Pomafy , or 7Zuxogu:Lebuwo . I wanted to make a password generator that combines simplicity, security, memorability, and ease of type.\n  Usage\n    You can install it with pip install pwd-generator and use the cli version:\n  pwdgen\n    To use it in a python code\n  import pwdgen\nprint(pwdgen.generate())Source code\n    The source code is on github"
},
{
    "title": "No title",
    "content": "It appeared too easy to me that we could create entire GUI from Python. I waited for the other shoe to drop, and it dropped. Now Anaconda and PySimpleGUI require payment if ever you want to use the libraries professionally.\n  \n    I wonder whether this will be the end of Python. It seems enshittification is finally reaching Python, where people previously used libraries for free and when they update them find out that they have to pay or stop using the libraries."
},
{
    "title": "No title",
    "content": "What My Project Does\n    Making asynchronous web API calls with asyncio and aiohttp can be complicated. This async_api_caller package abstracts away that complexity for a common case of needing to make multiple web API calls while varying query parameters.\n  \nInstallation\ngit clone git@github.com:amentumspace/async_api_caller.git\ncd async_api_caller/\npip install .\nUsage\nimport async_api_caller\nurl = \"https://ocean.amentum.io/gebco\"\nheaders = {'API-Key': API_KEY}\nparam_list = [\n    {\n        \"latitude\": 42,\n        \"longitude\": 42\n    },{\n        \"latitude\": 43,\n        \"longitude\": 43\n    }\n]\n\nresponses_json = async_api_caller.run(\n    url, headers, param_list\n)Target Audience\n    Python developers or data scientists or scientists or any Pythonista wanting a simple way to make asynchronous web API calls.\n  Comparison\n    Differs from existing alternatives in its simplicity. Simple like grequests without needing gevents. It really just abstracts the usual boilerplate needed to use asyncio with aiohttp."
},
{
    "title": "No title",
    "content": "I’m excited to share something we’ve been working on at Wolt: a new dependency injection library for Python “magic-di”. Born from our experience with a large service that has many components such as: API, event consumers, background workers, and cron jobs. magic-di aims to simplify dependency management without the hassle.\n  from magic_di.fastapi import inject_app, Provide\n\napp = inject_app(FastAPI())\n\n@app.get(path=\"/hello-reddit\") \ndef hello_reddit(service: Provide[Service]) -> dict: \n  return { \n     \"is_connected\": service.is_connected(),\n  }\nWhat Project Doesmagic-di cuts through the complexity of dependency management by adhering to a “zero-config” philosophy. It was developed to address the challenges we faced in managing code dependencies across various deployments, aiming for a straightforward solution.\n  \nCore Concepts\n\n\n\nZero Config: Setup is a breeze with magic-di. There’s no need to wrangle with injector configurations. All parameters in dependencies should either be injected or fetched from the environment, file, or any other source\n  \n\n\nConnectable Dependencies: Dependencies should implement a Connectable interface, which mandates __connect__ and __disconnect__ methods. This neat feature ensures that your dependencies are properly initialized and shut down.\n  \n\n\nDependency Order Resolution: magic-di ensures dependencies are injected in the correct order and ready to use right away once it’s injected\n  \n\n\nTarget Audiencemagic-di is ideal for Python developers looking for a straightforward dependency injection solution that helps to think only about business logic and not about boilerplate code. It’s designed to keep things simple, focusing on getting the job done without unnecessary configuration.\n  \nComparisonmagic-di differs from existing solutions by focusing on simplicity and practicality:\n  \nCompared to FastAPI’s built-in DI:FastAPI couples your logic to its ecosystem. magic-di offers a framework-agnostic approach, making it easier to integrate without tying your code to a specific framework.\n  \nCompared to python-dependency-injector: This library can be heavy on boilerplate. magic-di aims to reduce boilerplate to the bare minimum, advocating for a design where dependencies are self-configuring.\n  \nGoalsOur main goal with magic-di is to provide a DI tool that’s easy to integrate and simplifies the developer’s workflow. We wanted to address the common pain points in dependency management without introducing new complexities.\n  \n    In essence, magic-di is about making dependency injection straightforward and hassle-free for Python developers. It’s a tool born from practical needs, and while it may not revolutionize the field, it certainly aims to improve how we handle dependencies in our projects.\n  \n    We believe magic-di can make a positive impact on your projects, and we’re eager for you to try it out and share your thoughts. Your feedback is crucial as we continue to refine and improve this tool\n  \nRead more about magic-di: https://github.com/woltapp/magic-di"
},
{
    "title": "No title",
    "content": "Hello python community. I’m pretty new to scripting and come from a pure chemistry background. I’m working with 3-D MALDI-trapped ion mobility mass spectrometry data and I’ve been looking for a solid algorithm that can peak pick from two ‘time-resolved’ dimensions (mass and mobility), and the intensity arrays associated with those two dimensions. I need to peak pick from a heatmap essentially. Does anyone know of any good 3-D peak picking algorithms, or otherwise know how I can approach this? Thank you all!"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I think I see this a lot.\n  \n    I believe SQL, including like dataframe SQL packages, should always be used over Pandas when possible unless one is doing something Pandas is more suited to or unless someone requires broader Python control structures or features.\n  \n    I am a big believer in sticking to SQL as a baseline data manipulation framework and only using other frameworks if SQL is lacking. One of the biggest ways to accumulate technical debt is solving problems in multiple ways that are not standardized that not all people know."
},
{
    "title": "No title",
    "content": "Hey everyone!\n  \n    I wrote this quick Python script showing how you can automatically extract information from phone calls with LLMS. The call I use is a phone call received by a home building company where the caller is inquiring about an estimate for the cost to build a home on land he may purchase. Here's what the LLM extracted:\n  SUMMARY:\n- The caller is interested in getting an estimate for building a house on a property he is looking to purchase in Westchester.\n\nACTION ITEMS:\n- Have someone call the customer back today to discuss building estimate.\n- Set up time for builder to assess potential property site prior to purchase.\n\nCONTACT INFORMATION:\nName: Lindstrom, Kenny\nPhone number: 610-265-1715\n    If you skip to around 4:25 in this video you can see that all the information is extracted accurately! You can check out the code in this GitHub repository.\n  \n    It's really straightforward and only requires a handful of lines of code - feel free to drop any questions if you have any!"
},
{
    "title": "No title",
    "content": "I wrote my own Pyside6 alternative to Pysimple gui that I use in my projects starting some years back after some unsatisfactory interactions with the author of Pysimple gui.\n  \n    The GIT source for my GUI wrapper is in\n  \nhttps://github.com/David-Worboys/Black-DVD-Archiver/blob/main/qtgui.py\n\n    And the open source project it is used in is\n  \nhttps://github.com/David-Worboys/Black-DVD-Archiver\n\n    Just look for the layout methods in the .py files to see how it is used. https://github.com/David-Worboys/Black-DVD-Archiver/blob/main/video_cutter.py#L2391 is a good place to start, line 953,.\n  \n    Note size measurements are generally in chars, but pixels can be used\n  \n    Make no mistake there is a lot of work to produuctionise this and make a Python library, including, most importantly, doco of which there is none.\n  \n    If anyone is interested I will set up a new Git project for it..\n  \n    I am living on borrowed time and have to focus more on my other projectsWith the demise of open Pysimplegui is anyone interested in helping productionise this alternative?\n  \n    Help\n  \n    I wrote my own Pyside6 alternative to Pysimple gui that  I use in myprojects starting some years back after some unsatisfactoryinteractions with the author of Pysimple gui.\n  \n    The GIT source for my GUI wrapper is in\n  \nhttps://github.com/David-Worboys/Black-DVD-Archiver/blob/main/qtgui.py\n\n    And the open source project it is used in is\n  \nhttps://github.com/David-Worboys/Black-DVD-Archiver\n\n    Just look for the layout methods in the .py files to see how it is used. https://github.com/David-Worboys/Black-DVD-Archiver/blob/main/video_cutter.py#L2391 is a good place to start, line 953,.\n  \n    Note size measurements are generally in chars, but pixels can be used\n  \n    Make no mistake there is a lot of work to produuctionise this andmake a Python library, including, most importantly, doco of which thereis none.\n  \n    If anyone is interested I will set up a new Git project for it..\n  \n    I am living on borrowed time and have to focus more on my other projects"
},
{
    "title": "No title",
    "content": "Hello, everyone. I'm Kennedy Guerra, 33, open-source software maintainer. Just wanted to share a new feature I'm working on for the next version of Nodezator (GitHub | website), my public domain generalist Python node editor.\n  \n    In case you don't know Nodezator, the next few sections present it briefly. If you want to know about the new feature I'm working on, you can skip straight to the last subsection, which contains a detailed explanation and links to videos and extra info about the feature, called Automated System Testing.\n  What my project does\n    Nodezator is a generalist Python node editor (a desktop app) that allows developers to build and execute visual graphs whose nodes represent Python callables (your own and/or callables from Python standard or third-party libraries).\n  Target audience\n\n\n    Python developers in all areas. Intermediate Python knowledge is advised.\n  \n\n\n    You can also use it as a no-code interface, since people editing/executing the graph only have to deal with widgets and basic node editing operations like connecting/disconnecting sockets, creating or deleting nodes, etc.\n  \n\nComparison\n    Nodezator has actually many features that are either absent in other similar apps or are very rarely found:\n  \n\n\n    it is dedicated to the public domain with much love\n  \n\n\n    devs only need to define Python functions and the app automatically turns them into nodes (yes, no need to subclass anything)\n  \n\n\n    instead of writing your own functions, you can also feed existing functions from third-party libraries and the app will also turn them into nodes, virtually making Nodezator an \"everything nodes\" solution\n  \n\n\n    the graphs created can be converted back into Python code with the click of a button\n  \n\n\n    exporting to SVG, HTML+SVG and PNG is also available\n  \n\n\n    in the same way functions can receive other functions, nodes can receive other nodes as arguments, allowing the creation of powerful operations (functions are first-class citizens after all, so the nodes that represent them are as well)\n  \n\n\n    a comprehensive user manual available both inside the app and online.\n  \n\nNew feature on development branch: Automated System Testing\n    What I wanted to highlight today, though, is a feature I'm currently working on, called automated system testing, which is available in the development branch.\n  \n    Here's the link to a GitHub devlog post that contains a video demonstrating the feature in action: https://github.com/IndiePython/nodezator/discussions/72#discussioncomment-8997584\n\n    On the post you can also read more about the feature and how to reproduce the behaviour inside the app.\n  \n    Here's a brief explanation: once the user picks the tests to be performed in a dedicated form and clicks \"Start system testing session\", the app takes over and performs all the tests automatically by controlling the GUI. At the end of each test case, the app checks the state for expected values. After all test cases are performed, the app shows a report.\n  \n    I'm very relieved this feature is finally implemented and should soon be released (it needs a lot of polishing and is only available in the development branch, but is already functional). The reason for this is that as the project grows and becomes more and more complex, I needed to automated this kind of tests to ensure a more healthy/feasible development and more stable releases.\n  \n    That is, there are so many different tasks performed in the app that testing it manually takes a lot of time. As we all know, development requires a lot of iteration, so the enormous amount of time required to test everything manually would be spent many times over the implementation of even the simplest changes.\n  \n    Once this feature is released, we'll be able to automatically test everything and thus get feedback much earlier regarding the impact of our changes throughout the plethora of possible tasks that comprise Nodezator's workflow. Of course we also have unit tests, and they are important, but they are useful for testing units of behaviour, not their synergy when fully integrated in the working app as the user operates it.\n  \n    Once released, this feature will help me and other contributors check much sooner and with more precision whether our work is breaking any existing tasks/workflows/features. Sooner because the automated tests can be executed as fast as possible, as shown in the video, not only in normal time. With precision because the exact mouse and keyboard interactions are reproduced in the exact same way every time each test runs.\n  \n    This app is part of the Indie Python project, a personal project of mine to create and maintain completely free and open-source software (apps and games) and also release instructional/educational content related to them. Such software/content is created and maintained with much love, so please, consider supporting us if you have the means: https://patreon.com/KennedyRichard, https://github.com/sponsors/KennedyRichard, https://indiepython.com/donate.\n  \n    Thank you for your attention, let me know if you have any questions."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "In Gmail I realized that I had way too many filters that performed exactly the same action, but they simply had different \"from\" addresses. To solve this I created this little project that merges similar filters into a single one.\n  \n    It does require a lot of setup due to having to authenticate into Gmail, but if someone is on the same situation as me, it could be helpful.\n  \n    Link to the project: github\nWhat My Project Does\n    It connects into a Gmail account, reads their filters and squashes them into a single one.\n  \n    The program requires a manual setup of creating an OAuth credentials screen and downloading relevant credentials, which is cumbersome. But it's a one time effort as you can keep on using the application as long as you keep access to the GCP project.\n  \n    Note that it only merges filters if their criteria is a single \"from\" statement and they perform exactly the same action.\n  Target Audience\n    It's just a toy project of mine that I did for my sole benefit, but thought that it could be of use to anyone else in my same situation.\n  Comparison\n    I tried to look up for an already-existing solution for this issue before implementing it myself, but I couldn't find it. Relevant Gmail support post.\n  \n    Please do let me know if you have any suggestion or advice.\n  \n    Link to the project: github"
},
{
    "title": "No title",
    "content": "Hello everyone, I've been programming for about 4 years now and this is my first ever library that I created!\n  What My Project Does\n    It's called Reddit2Text, and it converts a reddit post (and all its comments) into a single, clean, easy to copy/paste string.\n  \n    I often like to ask ChatGPT about reddit posts, but copying all the relevant information among a large amount of comments is difficult/impossible. I searched for a tool or library that would help me do this and was astonished to find no such thing! I took it into my own hands and decided to make it myself.\n  Target Audience\n    This project is useable in its current state, and always looking for more feedback/features from the community!\n  Comparison\n    There are no other similar alternatives AFAIK\n  \n    Here is the GitHub repo: https://github.com/NFeruch/reddit2text\n\n    It's also available to download through pip/pypi :D\n  \n    Some basic features:\n  \n\n\n    Gathers the authors, upvotes, and text for the OP and every single comment\n  \n\n\n    Specify the max depth for how many comments you want\n  \n\n\n    Change the delimiter for the comment nesting\n  \n\n\n    Here is an example truncated output: https://pastebin.com/mmHFJtccUnder the hood, I relied heavily on the PRAW library (python reddit api wrapper) to do the actual interfacing with the Reddit API. I took it a step further though, by combining all these moving parts and raw outputs into something that's easily useable and very simple.Could you see yourself using something like this?"
},
{
    "title": "No title",
    "content": "Python has become a popular inclusion as a scripting language for many professional software packages. However, the APIs presented by these applications may be unsafe and/or do not include pythonic error handling. As someone working in the energy sector at my day job, I often find myself wrestling issues external to my python code in our power system simulator (usually memory related) that due to the implementation of the API can error at runtime below the exception handling mechanism of the interpreter. I will have have to implement a library for this purpose either internally or as an open-source side project.\n  \n    Traditionally, we handle these errors by isolating all potentially unsafe API calls in their own function and running that function in a separate process with multiprocessing and a handler function orchestrating the process and handling the exit codes. Although effective, this basic solution doesn't have the level of functionality needed (no logging, no stdout/stderr redirection, no exceptions, etc) and brings a decent amount of boilerplate to simple scripts.\n  \n    I wanted to post here and see if this is an issue others in the community face interacting with external code written in lower level languages they cannot freely modify. This isn't a detailed proposal more so trying to generate discussion around the idea, but essentially the module would provide decorators to wrap the external calls safely, with extra features like logging, custom exception classes and more to allow pythonic handling of errors.\n  \n    TLDR: Would there be interest in a library centered around pain-free handling of runtime errors that happen below the exception handling mechanism with extra convenience features?"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "First time poster here,\n  \n    Ill preface this post by saying I am not a software engineer by any metric. I am a hobbyist programmer and I mainly just create programs I find particularly useful in my day job as a designer.\n  \n    Figured I would share my handy PDF comparison tool, see if anyone else can find it useful as well. This tool is mostly designed around comparing CAD drawings. It probably isn't great for comparing text documents at the moment though I plan to add this functionality.\n  \n    It works by comparing the pdf's page by page in a raster image format. It creates several output files of different styles:\n  \n\n\n    Markup Page\n  \n\n\n    Differences Page\n  \n\n\n    Overlay Page\n  \n\n\n    The markup page is the main page (File 1) with outlines around any changes and some boxes to highlight larger areas.\n  \n    The differences page is essentially the main page - the secondary page. Any changes that were \"removed\" since the previous version end up in red and anything that was \"added\" is in blue.\n  \n    The overlay page is the secondary page in red with the main page slapped on top showing differences quite nicely in red.\n  \n    Don't judge too harshly :)\n  \nhttps://github.com/TMan9654/PyPDFCompare"
},
{
    "title": "No title",
    "content": "I recently developed an OpenSource solution that can auto-fix production exceptions for Python servers. Think of it as Datadog, but with the added capability to refactor your code. Currently, it's suitable for experimental use, and yes based on OpenAI API.\n  \n    The source code is available at https://github.com/CaptureFlow/captureflow-py.\n  \nWhat My Project Does:It incorporates a tracer client-side Python library and backend that accumulates such traces and is capable of proposing code improvements for your repository. It traverses the execution graph, extracts relevant parts of it, enriches it with implementation data from the GitHub API, and then generates suggestions using the OpenAI API.\n  \nTarget Audience:Python users who are interested in exploring code generation powered by production data (shoutout to DevinAI console.logging itself).\n  \nComparison:While there are not many direct comparisons, the POC use case for exception fixing is somewhat similar to Sentry's AI auto-fix feature that was released couple weeks ago. However, this library is not a replacement; it provides verbose tracing logs and will degrade your app's performance (for now).\n  \nExample:For a bugfix POC, check out this toy MR and explore the source code. Feedback and stars are welcome."
},
{
    "title": "No title",
    "content": "I recently develop an efficient modeling interface for optimization problems in Python called PyOptInterface.\n  \n\n\n    Source code is at https://github.com/metab0t/PyOptInterface\n\n\n\n    Documentation is at https://metab0t.github.io/PyOptInterface/\n\n\n\nWhat My Project Does\n\n    It is designed as a very thin wrapper of native C API of optimizers and attempts to provide common abstractions of an algebraic modelling environment including model, variable, constraint and expression with the least overhead of performance.\n  \nTarget Audience\n\n    Python users that want to build and solve an optimization model\n  \nComparison\n\n    The benchmark comparing PyOptInterface with some other modeling interfaces can be found here.\n  \nExample\n\n    This link shows an introductory example to use PyOptInterface and HiGHS to solve N-queens problem.\n  \n    Feedbacks and stars are welcome."
},
{
    "title": "No title",
    "content": "With the Python on Microcontrollers newsletter, you get all the latest information on Python running on hardware in one place! MicroPython, CircuitPython and Python on single Board Computers like Raspberry Pi & many more.\n  \n    The Python on Microcontrollers newsletter is the place for the latest news. It arrives Monday morning with all the week’s happenings. No advertising, no spam, easy to unsubscribe.\n  \n    10,921 subscribers - the largest Python on hardware newsletter out there.\n  \n    Catch all the weekly news on Python for Microcontrollers with adafruitdaily.com.\n  \n    This ad-free, spam-free weekly email is filled with CircuitPython, MicroPython, and Python information that you may have missed, all in one place!\n  \n    Ensure you catch the weekly Python on Hardware roundup– you can cancel anytime – try our spam-free newsletter today!\n  \nhttps://www.adafruitdaily.com/"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I'm seeking for python open source project where I can add things , colaborate with a community on building valuable stuff , Any good suggestions please ?"
},
{
    "title": "No title",
    "content": "Hello all.  I just saw this post about Pandas, which lead to a discussion of Polars being much faster.  Polars, which I had never heard about before this morning.\n\n    This leads me to wonder:  where are you all getting your Python news and other information?  r/Python obviously, but I fear there are some pipelines or subscriptions for Python news, information about must-have new packages, and the like that I'm currently missing."
},
{
    "title": "No title",
    "content": "The maintainer of `great_tables` released an article about the design philosophy of his package https://posit-dev.github.io/great-tables/blog/design-philosophy/\n\n    Curious about your thoughts. What tools do you prefer to use when you need to share a data table based on work you've done in python?"
},
{
    "title": "No title",
    "content": "Hi, I made a new project called dcdljeu (written in French), a solver for the games Le Compte Est Bon and Le Mot Le Plus Long from the TV Show Des Chiffres Et Des Lettres."
},
{
    "title": "No title",
    "content": "What my project does\npipxu installs Python applications, i.e. Python packages which have one or more executable programs, into independent isolated virtual environments on your system. Each package and it's dependencies are thus insulated from all other applications, and from the system Python. pipxu creates links to application executables in a common directory, which you have in your PATH. Packages are typically sourced from PyPI, the Python Package Index.\n  Target Audience\n    Python users and developers.\n  Comparison\npipxu is a re-implementation of most of the functionality of the popular pipx tool but is much faster because it uses uv to create and install application virtual environments instead of venv and pip as used by pipx.\n  \n    Read more at https://github.com/bulletmark/pipxu"
},
{
    "title": "No title",
    "content": "Version 1.1.0 of UXsim is released, which now supports modeling multilane ways.\n  \nMain Changes\n\n\n\n    Add support for multilane links. More technically, it is a multilane, single-pipe model where vehicles cannot overtake others. This allows us to set traffic capacity significantly larger while keeping consistency to KW theory.\n  \n\n\n    Separate Analyzer class from uxsim.py. This means that uxsim.py now contains only the essential codes for the simulation. It makes it easier for users to understand the simulation logic.\n  \n\n\nUXsim\n\n    UXsim is a free, open-source macroscopic and mesoscopic network traffic flow simulator written in Python. It simulates the movements of car travelers and traffic congestion in road networks. It is suitable for simulating large-scale (e.g., city-scale) traffic phenomena. UXsim is especially useful for scientific and educational purposes because of its simple, lightweight, and customizable features, but users are free to use UXsim for any purpose."
},
{
    "title": "No title",
    "content": "I am trying to understand why anyone would care whether you import math or numpy first. I am told it is good practise but it just seem pointless to care."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Is the xz library compromised along with the xz Linux utility?\n  \n    Anaconda seems to auto load this library upon creation of new venvs."
},
{
    "title": "No title",
    "content": "GitHub Repo: https://github.com/openscilab/nava\n\nWhat My Project Does:\n\n    Nava is a Python library that allows users to play sound in Python without any dependencies or platform restrictions. It is a cross-platform solution that runs on any operating system, including Windows, macOS, and Linux. Its lightweight and easy-to-use design makes Nava an ideal choice for developers looking to add sound functionality to their Python programs.\n  import time\nfrom nava import play, stop\nsound_id = play(\"alarm.wav\", async_mode=True, loop=False)\ntime.sleep(4)\nstop(sound_id)\nTarget Audience:\n\n    Developers who are looking to add sound functionality to their Python programs\n  \nComparison:\n\n\n\n    Light-weight and zero-dependency against pygame library. It can be inconvenient to install the pygame library just for playing sounds!\n  \n\n\nplaysound is deprecated and it's impossible to use it in modern environments. It also has some bugs on macOS and doesn't support the stop function."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "access comments with Docx2Python\n    You can access docx comments with the comments attribute of the output DocxContent object.\n  with docx2python('path/to/file.docx') as docx_content:\n    print(docx_content.comments)\n    For each comment, this will return a tuple:\n  (reference_text, author, date, comment_text)\nhttps://github.com/ShayHill/docx2python"
},
{
    "title": "No title",
    "content": "It's just around the corner. I'll be going on my own so it would be nice to meet up with some people potentially.\n  \n    Who plans to go and what speakers / sessions are you looking forward to?"
},
{
    "title": "No title",
    "content": "Today Cloudflare has announced support for Python in their Clouldflare Workers serverless environment. This allows Python to be run at the edge on their global network using a combination of Pyodine and the V8 runtime.\n  \n    Announcement can be found on the Cloudflare Blog: https://blog.cloudflare.com/python-workers"
},
{
    "title": "No title",
    "content": "Security Code Challenge for Developers & Ethical Hackers – Damn Vulnerable RESTaurant\n\n    A FastAPI based intentionally vulnerable web application teaching the most common security vulnerabilities in API through a dedicated game.\n  \n    Damn Vulnerable RESTaurant is operated by a mysterious Chef who has discovered that threat actors were able to compromise his restaurant’s API and the underlying system. He suspects that a competing restaurant located across the street might be involved in this attack! The goal of the challenge is to identify and fix vulnerabilities based on provided hints. During this adventure, you will have the opportunity to investigate how the attack was carried out and fix security issues to safeguard the application. Moreover, you can uncover the identity of the person behind the attack by the end of this adventure."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "As of PEP 4124 being accepted, the infamous pointers.py will be added to Python's standard library in 3.13! To quote Guido van Rossum's take on adding this, \"Why the hell not?\"\n  \n    This will also introduce pointer literals, the sizeof operator, and memory errors!\n  from pointers import malloc\n\nptr = &\"spam\"  # Pointer literal\nprint(*ptr)\nmem = malloc(?\"hello\")  # New sizeof operator\nprint(*mem)  # MemoryError: junk 13118820 6422376 4200155 at 0x7649f65a9670\n# MemoryWarning: leak at 0x7649f65a9670\n    However, it was decided in this discussion that segfaults would be added to the language for \"extra flavor\":\n  spam = *None\n# Segmentation fault, core dumped. Good luck, kiddo."
},
{
    "title": "No title",
    "content": "Ever wished your Python interpreter had the dramatic feeling of a 300 baud modem connection?\n  \n    Today there's a solution: pip install dramatic\n\ndramatic on PyPI\n\ndramatic on GitHub\nWhat My Project Does\n    All text output by Python will print character-by-character.\n  \n    It works as a context manager, as a decorator, or as a simple function call.\n  \n    Other features include a dramatic REPL, ability to run specific Python modules/scripts dramatically, and a --max-drama argument to make all Python programs dramatic all the time.\n  Target Audience\n    Those seeking amusement.\n  Comparison\n    Just like Python usually runs, but with the feeling that you're inside a text-based adventure game."
},
{
    "title": "No title",
    "content": "Source code: https://github.com/ben-n93/geojson-shave\n\nWhat my Project DoesReduces the file size of GeoJSON files by truncating coordinates to the specified decimal place, eliminating whitespace and (optionally) replacing the property key with an empty dictionary.\n  \nTarget AudienceFor anyone that works with geospatial data (and specifically GeoJSON files) - so data analysts, data engineers, data journalists, GIS professionals, etc.\n  \nComparison\n\n    There is a website that reduces the size of GeoJSON files, however it didn't work for me when I tested it. The advantage of my tool is that you can run it from the command-line and don't need an Internet connection."
},
{
    "title": "No title",
    "content": "This is a project that I initially created in a weekend for my own use after I was surprised to find out there was no easy way to generate Draw.io diagrams in Python. I'm a huge fan of Draw.io for documentation since it's free, lightweight, and the files are plaintext.\n  \n    After building basic initial functionality I decided to use this as a project to make a more mature Python library out of something I initially wrote for myself quick n dirty. I refactored (a lot), wrote docs, tests, build logic, etc. There's still a lot to do to make it as robust as it could be but I think it's good enough to share!\n  \nGitHub Page\n\nDocs\nWhat My Project Does:\n    There's some basic functionality that allows you to manually create Draw.io objects and edges, pre-format them from libraries, and lay them out. I've also started to implement more automated diagram types that extend that basic functionality for specific use cases. The ony currently released diagram type is a TreeDiagram, that allows you to define a tree structure with parent and children nodes, then auto generate a nice looking layout.\n  \n    For example, this code for a tree diagram generates this diagram! Or for something a little more manual, this code for a flowchart generates this diagram.\n  Target Audience:\n    I use Draw.io for everything so your imagination is the limit. But I think this would be the best fit for auto generating documentation, specifically that which needs to be read by non-technical or non-programmers.\n  Comparison:\n    I couldn't find any other Python libraries for creating Draw.io graphs. There are other graphing options, most based on Graphviz. The advantage of Draw.io here is the ability to have finer control over formatting and have a great desktop UI for working with the diagrams.\n  \n    edit:\n  \n    Drawpyo is now on PIP! Thanks for the push everyone."
},
{
    "title": "No title",
    "content": "First time poster here,\n  \n    Ill preface this post by saying I am not a software engineer by any metric. I am a hobbyist programmer and I mainly just create programs I find particularly useful in my day job as a designer.\n  \n    Figured I would share my handy PDF comparison tool, see if anyone else can find it useful as well. This tool is mostly designed around comparing CAD drawings. It probably isn't great for comparing text documents at the moment though I plan to add this functionality.\n  \n    It works by comparing the pdf's page by page in a raster image format. It creates several output files of different styles:\n  \n\n\n    Markup Page\n  \n\n\n    Differences Page\n  \n\n\n    Overlay Page\n  \n\n\n    The markup page is the main page (File 1) with outlines around any changes and some boxes to highlight larger areas.\n  \n    The differences page is essentially the main page - the secondary page. Any changes that were \"removed\" since the previous version end up in red and anything that was \"added\" is in blue.\n  \n    The overlay page is the secondary page in red with the main page slapped on top showing differences quite nicely in red.\n  \n    Don't judge too harshly :)\n  \nhttps://github.com/TMan9654/PyPDFCompare"
},
{
    "title": "No title",
    "content": "I recently developed an OpenSource solution that can auto-fix production exceptions for Python servers. Think of it as Datadog, but with the added capability to refactor your code. Currently, it's suitable for experimental use, and yes based on OpenAI API.\n  \n    The source code is available at https://github.com/CaptureFlow/captureflow-py.\n  \nWhat My Project Does:It incorporates a tracer client-side Python library and backend that accumulates such traces and is capable of proposing code improvements for your repository. It traverses the execution graph, extracts relevant parts of it, enriches it with implementation data from the GitHub API, and then generates suggestions using the OpenAI API.\n  \nTarget Audience:Python users who are interested in exploring code generation powered by production data (shoutout to DevinAI console.logging itself).\n  \nComparison:While there are not many direct comparisons, the POC use case for exception fixing is somewhat similar to Sentry's AI auto-fix feature that was released couple weeks ago. However, this library is not a replacement; it provides verbose tracing logs and will degrade your app's performance (for now).\n  \nExample:For a bugfix POC, check out this toy MR and explore the source code. Feedback and stars are welcome."
},
{
    "title": "No title",
    "content": "I recently develop an efficient modeling interface for optimization problems in Python called PyOptInterface.\n  \n\n\n    Source code is at https://github.com/metab0t/PyOptInterface\n\n\n\n    Documentation is at https://metab0t.github.io/PyOptInterface/\n\n\n\nWhat My Project Does\n\n    It is designed as a very thin wrapper of native C API of optimizers and attempts to provide common abstractions of an algebraic modelling environment including model, variable, constraint and expression with the least overhead of performance.\n  \nTarget Audience\n\n    Python users that want to build and solve an optimization model\n  \nComparison\n\n    The benchmark comparing PyOptInterface with some other modeling interfaces can be found here.\n  \nExample\n\n    This link shows an introductory example to use PyOptInterface and HiGHS to solve N-queens problem.\n  \n    Feedbacks and stars are welcome."
},
{
    "title": "No title",
    "content": "With the Python on Microcontrollers newsletter, you get all the latest information on Python running on hardware in one place! MicroPython, CircuitPython and Python on single Board Computers like Raspberry Pi & many more.\n  \n    The Python on Microcontrollers newsletter is the place for the latest news. It arrives Monday morning with all the week’s happenings. No advertising, no spam, easy to unsubscribe.\n  \n    10,921 subscribers - the largest Python on hardware newsletter out there.\n  \n    Catch all the weekly news on Python for Microcontrollers with adafruitdaily.com.\n  \n    This ad-free, spam-free weekly email is filled with CircuitPython, MicroPython, and Python information that you may have missed, all in one place!\n  \n    Ensure you catch the weekly Python on Hardware roundup– you can cancel anytime – try our spam-free newsletter today!\n  \nhttps://www.adafruitdaily.com/"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I'm seeking for python open source project where I can add things , colaborate with a community on building valuable stuff , Any good suggestions please ?"
},
{
    "title": "No title",
    "content": "Hello all.  I just saw this post about Pandas, which lead to a discussion of Polars being much faster.  Polars, which I had never heard about before this morning.\n\n    This leads me to wonder:  where are you all getting your Python news and other information?  r/Python obviously, but I fear there are some pipelines or subscriptions for Python news, information about must-have new packages, and the like that I'm currently missing."
},
{
    "title": "No title",
    "content": "The maintainer of `great_tables` released an article about the design philosophy of his package https://posit-dev.github.io/great-tables/blog/design-philosophy/\n\n    Curious about your thoughts. What tools do you prefer to use when you need to share a data table based on work you've done in python?"
},
{
    "title": "No title",
    "content": "Hi, I made a new project called dcdljeu (written in French), a solver for the games Le Compte Est Bon and Le Mot Le Plus Long from the TV Show Des Chiffres Et Des Lettres."
},
{
    "title": "No title",
    "content": "What my project does\npipxu installs Python applications, i.e. Python packages which have one or more executable programs, into independent isolated virtual environments on your system. Each package and it's dependencies are thus insulated from all other applications, and from the system Python. pipxu creates links to application executables in a common directory, which you have in your PATH. Packages are typically sourced from PyPI, the Python Package Index.\n  Target Audience\n    Python users and developers.\n  Comparison\npipxu is a re-implementation of most of the functionality of the popular pipx tool but is much faster because it uses uv to create and install application virtual environments instead of venv and pip as used by pipx.\n  \n    Read more at https://github.com/bulletmark/pipxu"
},
{
    "title": "No title",
    "content": "Version 1.1.0 of UXsim is released, which now supports modeling multilane ways.\n  \nMain Changes\n\n\n\n    Add support for multilane links. More technically, it is a multilane, single-pipe model where vehicles cannot overtake others. This allows us to set traffic capacity significantly larger while keeping consistency to KW theory.\n  \n\n\n    Separate Analyzer class from uxsim.py. This means that uxsim.py now contains only the essential codes for the simulation. It makes it easier for users to understand the simulation logic.\n  \n\n\nUXsim\n\n    UXsim is a free, open-source macroscopic and mesoscopic network traffic flow simulator written in Python. It simulates the movements of car travelers and traffic congestion in road networks. It is suitable for simulating large-scale (e.g., city-scale) traffic phenomena. UXsim is especially useful for scientific and educational purposes because of its simple, lightweight, and customizable features, but users are free to use UXsim for any purpose."
},
{
    "title": "No title",
    "content": "I am trying to understand why anyone would care whether you import math or numpy first. I am told it is good practise but it just seem pointless to care."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Is the xz library compromised along with the xz Linux utility?\n  \n    Anaconda seems to auto load this library upon creation of new venvs."
},
{
    "title": "No title",
    "content": "GitHub Repo: https://github.com/openscilab/nava\n\nWhat My Project Does:\n\n    Nava is a Python library that allows users to play sound in Python without any dependencies or platform restrictions. It is a cross-platform solution that runs on any operating system, including Windows, macOS, and Linux. Its lightweight and easy-to-use design makes Nava an ideal choice for developers looking to add sound functionality to their Python programs.\n  import time\nfrom nava import play, stop\nsound_id = play(\"alarm.wav\", async_mode=True, loop=False)\ntime.sleep(4)\nstop(sound_id)\nTarget Audience:\n\n    Developers who are looking to add sound functionality to their Python programs\n  \nComparison:\n\n\n\n    Light-weight and zero-dependency against pygame library. It can be inconvenient to install the pygame library just for playing sounds!\n  \n\n\nplaysound is deprecated and it's impossible to use it in modern environments. It also has some bugs on macOS and doesn't support the stop function."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "access comments with Docx2Python\n    You can access docx comments with the comments attribute of the output DocxContent object.\n  with docx2python('path/to/file.docx') as docx_content:\n    print(docx_content.comments)\n    For each comment, this will return a tuple:\n  (reference_text, author, date, comment_text)\nhttps://github.com/ShayHill/docx2python"
},
{
    "title": "No title",
    "content": "It's just around the corner. I'll be going on my own so it would be nice to meet up with some people potentially.\n  \n    Who plans to go and what speakers / sessions are you looking forward to?"
},
{
    "title": "No title",
    "content": "Today Cloudflare has announced support for Python in their Clouldflare Workers serverless environment. This allows Python to be run at the edge on their global network using a combination of Pyodine and the V8 runtime.\n  \n    Announcement can be found on the Cloudflare Blog: https://blog.cloudflare.com/python-workers"
},
{
    "title": "No title",
    "content": "Security Code Challenge for Developers & Ethical Hackers – Damn Vulnerable RESTaurant\n\n    A FastAPI based intentionally vulnerable web application teaching the most common security vulnerabilities in API through a dedicated game.\n  \n    Damn Vulnerable RESTaurant is operated by a mysterious Chef who has discovered that threat actors were able to compromise his restaurant’s API and the underlying system. He suspects that a competing restaurant located across the street might be involved in this attack! The goal of the challenge is to identify and fix vulnerabilities based on provided hints. During this adventure, you will have the opportunity to investigate how the attack was carried out and fix security issues to safeguard the application. Moreover, you can uncover the identity of the person behind the attack by the end of this adventure."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "As of PEP 4124 being accepted, the infamous pointers.py will be added to Python's standard library in 3.13! To quote Guido van Rossum's take on adding this, \"Why the hell not?\"\n  \n    This will also introduce pointer literals, the sizeof operator, and memory errors!\n  from pointers import malloc\n\nptr = &\"spam\"  # Pointer literal\nprint(*ptr)\nmem = malloc(?\"hello\")  # New sizeof operator\nprint(*mem)  # MemoryError: junk 13118820 6422376 4200155 at 0x7649f65a9670\n# MemoryWarning: leak at 0x7649f65a9670\n    However, it was decided in this discussion that segfaults would be added to the language for \"extra flavor\":\n  spam = *None\n# Segmentation fault, core dumped. Good luck, kiddo."
},
{
    "title": "No title",
    "content": "Ever wished your Python interpreter had the dramatic feeling of a 300 baud modem connection?\n  \n    Today there's a solution: pip install dramatic\n\ndramatic on PyPI\n\ndramatic on GitHub\nWhat My Project Does\n    All text output by Python will print character-by-character.\n  \n    It works as a context manager, as a decorator, or as a simple function call.\n  \n    Other features include a dramatic REPL, ability to run specific Python modules/scripts dramatically, and a --max-drama argument to make all Python programs dramatic all the time.\n  Target Audience\n    Those seeking amusement.\n  Comparison\n    Just like Python usually runs, but with the feeling that you're inside a text-based adventure game."
},
{
    "title": "No title",
    "content": "Source code: https://github.com/ben-n93/geojson-shave\n\nWhat my Project DoesReduces the file size of GeoJSON files by truncating coordinates to the specified decimal place, eliminating whitespace and (optionally) replacing the property key with an empty dictionary.\n  \nTarget AudienceFor anyone that works with geospatial data (and specifically GeoJSON files) - so data analysts, data engineers, data journalists, GIS professionals, etc.\n  \nComparison\n\n    There is a website that reduces the size of GeoJSON files, however it didn't work for me when I tested it. The advantage of my tool is that you can run it from the command-line and don't need an Internet connection."
},
{
    "title": "No title",
    "content": "This is a project that I initially created in a weekend for my own use after I was surprised to find out there was no easy way to generate Draw.io diagrams in Python. I'm a huge fan of Draw.io for documentation since it's free, lightweight, and the files are plaintext.\n  \n    After building basic initial functionality I decided to use this as a project to make a more mature Python library out of something I initially wrote for myself quick n dirty. I refactored (a lot), wrote docs, tests, build logic, etc. There's still a lot to do to make it as robust as it could be but I think it's good enough to share!\n  \nGitHub Page\n\nDocs\nWhat My Project Does:\n    There's some basic functionality that allows you to manually create Draw.io objects and edges, pre-format them from libraries, and lay them out. I've also started to implement more automated diagram types that extend that basic functionality for specific use cases. The ony currently released diagram type is a TreeDiagram, that allows you to define a tree structure with parent and children nodes, then auto generate a nice looking layout.\n  \n    For example, this code for a tree diagram generates this diagram! Or for something a little more manual, this code for a flowchart generates this diagram.\n  Target Audience:\n    I use Draw.io for everything so your imagination is the limit. But I think this would be the best fit for auto generating documentation, specifically that which needs to be read by non-technical or non-programmers.\n  Comparison:\n    I couldn't find any other Python libraries for creating Draw.io graphs. There are other graphing options, most based on Graphviz. The advantage of Draw.io here is the ability to have finer control over formatting and have a great desktop UI for working with the diagrams.\n  \n    edit:\n  \n    Drawpyo is now on PIP! Thanks for the push everyone."
},
{
    "title": "No title",
    "content": "A dense Python cheat sheet with just what you need.Design principles:• Focus on Python core• Comprehensive but selective (Just what you need)• Densely packed• Well-linked• Linkable• Responsive• PrintableIssues and feedback are tracked at the best-python-cheat-sheet repository.*It may not be the best Python cheat sheet, but it aspires to be."
},
{
    "title": "No title",
    "content": "test.py\nimport sys\nsys.setrecursionlimit(4)\nclass Foo:\n    class Foo:\n        foo = 1\npython test.py\n\n    Please be honest, write your guess in comments about the output of this code before opening the spoiler =)\n  \n    The answer: RecursionError: maximum recursion depth exceeded\n\n    Did you know?)"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hi everybody,\n  \n    I'm happy to share AnyPathLib 🛣️, a pip package I created to simplify access to different storage resources - S3, Azure, and local storage.\n  \nWhat My Project Does:\n\n    Basically, instead of writing custom code to handle the different storage resources using the SDK (boto3, azure-sdk), you can now just use AnyPathLib:\n  from anypathlib import AnyPath\n  \n\n  \n# Create an AnyPath instance for a local file\n  \n\n  \nlocal_file = AnyPath(\"/path/to/local/file.txt\")\n  \n\n  \n# Create an AnyPath instance for an S3 object\n  \n\n  \ns3_file = AnyPath(\"s3://bucket/path/to/object.txt\")\n  \n\n  \n# Copy a file from local to S3\n  \n\n  \nlocal_file.copy(s3_file)\n  \n\n  \n# Copy a directory from S3 to Azure\n  \n\n  \ns3_dir = AnyPath(\"s3://bucket/path/to/dir\") \n  \nazure_dir = AnyPath(\"https://account_name.blob.core.windows.net/container_name/path\") \n\ns3_dir.copy(azure_dir)\n    There are other cool features to save you some time, so check out this X thread for some more details or visit the Github project page.\n  \nTarget Audience:\n\n    Anybody which works with both a local environment and Azure or AWS, and could use a simplified API to access their storage\n  \nComparison:\n\n    I didn't find any similar library (that's why I wrote this one) - there are dedicated packages for S3 (boto3) and the Azure SDK, but they come with different, and somewhat unintuitive APIs.\n  \n    I'd love to hear your feedback, issues, and of course - PRs :)"
},
{
    "title": "No title",
    "content": "https://github.com/amalshaji/portr\nWhat my project does?\n    Portr is a tunnelling solution that let's you expose local http/tcp connections to the public internet.\n  Target audience\n    Developers that work with webhooks or callbacks. Or anyone who want to quickly share their dev server with others.\n  Comparison\n\n\n    Monitor connections, and manage teams and members via the admin dashboard. Watch video\n\n\n\n    Inspect and replay HTTP requests using portr inspector. Watch video.\n  \n\n\n    The admin backend is built using Python. I would appreciate a feedback."
},
{
    "title": "No title",
    "content": "What's the first step to building 90% of AI applications?Chunking your documents! Unfortunately there's very few libraries for this and most rely on naively splitting text into N characters. You lose so much valuable information hidden in the layout of the document when you do this.  If you give an LLM bad inputs, you're going to get bad results.\n\nhttps://github.com/Filimoa/open-parse/\n\nWhat it does:\n\n    The library can detect the layout of a PDF, convert to markdown and extract tables into html in one line of code. You can then feed this directly into your vector database or something like LlamaIndex.  It does all this while preserving bounding boxes so you can display citations directly to the user.\n  \nTarget Audience:\n\n    Anyone building a RAG AI app.  I run a talk-to-your-file website that's seen millions of documents and this library represents a giant chunk of what I know about feeding documents to LLM's.\n  \nComparison:\n\n    Open source libraries like LlamaIndex chunk your documents into strings of fixed length.  There's a couple commercial solutions that do this but they're expensive (typically 100 pages / dollar) and honestly have less than ideal results (AWS Texttract, Adobe, Google).\n  \n    Facebook has nougat for converting files to markdown but it's incredibly slow on anything without a massive GPU."
},
{
    "title": "No title",
    "content": "I recently posted my opinions on the import system in Python and how it can be made more intuitive and straightforward.\n  \n    I got some comments, some of them are just snappy as usual and some of them had some suggestions and workarounds.\n  \n    Either way the post got removed, citing that it is not suitable for this sub-reddit. I mean, come on, I cannot talk about Python features in Python sub-reddit?\n  \nUpdate:\n\n    To mods: Thanks for letting me have this conversation.\n  \n    To others who made helpful comments, thank you.\n  \nOriginal post:\n\n    I don't want to recreate the post verbatim here, I don't remember it exactly too. It was basically something like this,\n  \n    *requiring* or *loading* code, which is more or less importing can be much more straight forward if we can directly load files.\n  \n    Instead of the concept of packages, each file is a self contained module which can import and export classes, variables and functions.\n  \n    For example,\n  \nsrc/constants.py\n\nADMIN_ROLE_NAME = 'admin'\n\nsrc/user.py\n\nfrom constants import ADMIN_ROLE_NAME\n\ntests/user_test.py\n\nfrom ../../src/constants.py import ADMIN_ROLE_NAME\n\n    Here there would be no __init__.py files, just straightforward imports directly from files.\n  \n    I would refrain from giving example from Ruby or JS, like I did in the original post, because people perceive it as a fan boy post, while my intention is not that.\n  \n    I'll give a real example,\n  \n    Here is my repo, which https://github.com/rajaravivarma-r/python-robyn-react\n\n    I tried to put the contents of the `constants.py` file into `src/__init__.py` but there is no way of me importing them from within the child directory package, namely `src/api/__init__.py`.\n  \nBackward compatibility:\n\n    Lot of you have mentioned about how this would break existing code, but C++ standard had a recommendation to introduce module systems to new code which goes like this. Add a pragma like #pragma C++14_Modules at the top of the file, or something like that, but you get the idea.\n  \n    Ruby has a # frozen_string_literal: true\n\n    So Python could introduce something like that for new code, while treating the rest of the old code as it is.\n  \n    I believe this is constructive enough.\n  \n    I know this is not a PEP, but I want to show the community that my intention was genuine and not a rant.\n  \nOutro:Assume ignorance rather than malevolence.\n\n    Your snarky comments or aggressive down voting doesn't improve anyone's life. Last time when I posted about some surprising behaviour of Python in this community (from my old account), I got a comment about my daughter and wife. It wasn't even deleted despite reporting."
},
{
    "title": "No title",
    "content": "What my Project Does\n\n\n\n    It's a program that searches through posts in a specified subreddit and downloads images, gifs etc.\n  \n\n\nTarget Audience\n\n\n\n    It's a toy project at this point, but I might extend its functionality if I can find good resources. Maybe even add support for different websites (imgur et al.).\n  \n\n\nComparison\n\n\n\n    I found a few websites for downloading images from reddit but they claimed they didn't work because of the Reddit API changes, I was sceptical and wrote my own script.\n  \n    Anyhow,\n  \n    For the last few days I was trying to find good wallpapers for my system. I wrote a Python script for getting images and gifs from subreddits, here is the project repo. Please read the instructions before using it.\n  \n    It's pretty basic but gets the job done. I am still working on the project.\n  \n    I also wrote a flake.nix for my NixOS bros. Didn't test the project on Windows but I don't see why it wouldn't work.\n  \n    Lastly, here is my NixOS config in case  you are interested. Enjoy"
},
{
    "title": "No title",
    "content": "Discover the power of Modin in overcoming the performance bottlenecks of Pandas. We'll walk you through how a simple switch can reduce your waiting times and make your workflow smooth and speedy, all without leaving the comfort of Pandas behind.\n  \n    Modin is an active  open source project. Take a look to this blog post to see if it is applicable for your code: https://dchigarev.github.io/modin_perf_examples/"
},
{
    "title": "No title",
    "content": "I have been reading a lot about these tools recently and wondered if they are used in production, especially when building a backend with Python. As far as I know, PyPy is not ideal because of its lack of package support, but Cython seems pretty neat."
},
{
    "title": "No title",
    "content": "I've created a simple API service https://github.com/koldakov/futuramaapi to access Futurama units with async Python + FastAPI + SQLAlchemy (PostgreSQL).\n  \n    I've chosen Hypercorn for a web server to achieve HTTP/2 support - considering I didn't find yet cheap hosting that supports HTTP/2. Heroku doesn't support, render doesn't support.\n  \n    Also it has some code related to GraphQL (I used strawberry).\n  \n    + I didn't find a lot of discussions about Heroku + FastAPI - the problem will be with redirect to https. As default fastapi (starlette) middleware doesn't work with proxies there will be an infinite loop if you try to use fastapi.middleware.httpsredirect.HTTPSRedirectMiddleware. Chain will be: Client https-> Proxy(Hypercorn, for example) http-> fastapi, in that case as you can see the request is secure, but fastapi (starlette) won't understand this and will redirect to https. Here is a simple workaround to support https under proxy: https://github.com/koldakov/futuramaapi/blob/main/app/middlewares/secure.py\n\n\n    Constructive feedback is greatly appreciated.\n  \n\n    Source code: https://github.com/koldakov/futuramaapi\n\n    Working example: https://futuramaapi.com"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I created a python library \"dis-bg-remover\" based off the \"Highly Accurate Dichotomous Image Segmentation (https://arxiv.org/pdf/2203.03041.pdf), whose results are comparable, if not better, to the premium offerings in the market.\n  \n    Explainer video here\n  \nhttps://www.youtube.com/watch?v=js7AYKkZvFI"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hi thereI was tasked with migrating from mongoDb to PostgreSQL due to the performance issues.The backend is FastAPI-based web server for a mobile APP with a decent amount if CRUD. We also heavily use Pydantic for data validation and the primary carried of data across the service layers\n  \n    Would like to hear opinions from SQLModel users as to how easy it is to use or what kind of pitfalls am i setting myself up by using it.  Last post that I could find was years old and even that was not really conclusive. Now SQLAlchemy 2.0 is released and even SQLModel has support for Pydantic V2, so would like some newer thoughts in this topic.\n  \n    Our datamodel includes quite a bit of top level nestings, which I would prefer to be stored as `JSON` columns (Similar SO Question). SQLModel doesn't seem to have any documentation for such cases.\n  \n    If possible I would prefer avoiding to write and maintain our own internal library with SQLAlchemy ORM to perform these conversions. But maybe just using plain SQLAlchemy and Pydantic might be a good solution too.\n  \n    Seeking inputs from more experienced users. Thank you.\n  \n    Edit:\n  \n    In the end I gave up on using orm to query data and just used plain SQLAlchemy queries.\n  \n    The reason was that the ORM layer already abstract lot of stuff from us and SQLModel makes it more abstracted.\n  \n    This is fine if you know all the details of the abstractions. But at the point I am in, where I am new to sqlalchemy, it just adds lots more complexity and gotchas which can just avoid with plain queries.\n  \n    Maybe better documentation might help down the line but as of now, I am leaning towards just plain sqlqlchemy  core.\n  \n    Also using the ORM layer to just auto generate revisions is a nice feature."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I just published a tutorial series on how to automate a Python script in Google Cloud using Cloud Functions and/or Cloud Run. Feedback would be great. Thanks!\n  \n\n\nAutomating Python with Google Cloud\n\n\n\nAutomating Python with Google Cloud Functions\n\n\n\nAutomating Python with Google Cloud Run"
},
{
    "title": "No title",
    "content": "Hey all,\n  \n    I love what Django has with django-fsm. I require something but without Django, as there is no user interaction with the workflow. All inputs and outputs are either rabbitmq or another api. This is to be a workflow management system.\n  \n    I am looking for state management backed up by database so we can not suffer if a k8s pod dies with all the states in memory. Some of our workflows could take weeks.\n  \n    Is it still best to make this in Django or is there a database backed state management module available?\n  \n    I see pytransitions but I would have to add database logging to it."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Creating a GeoTIFF raster XYZ tile service in python with caching capability"
},
{
    "title": "No title",
    "content": "Hello everyone!\n  \n    I am proud to introduce a Makefile Parser for Python that I think will be useful!\n  \n    the link is Thanatisia/makefile-parser-python\n\n    As an introduction, I have been using python and been following this subreddit for quite awhile now, but this is my first post\n  \n    Recently, I've been planning a side-project involving the use of Makefiles in Python, and I required the use of a Makefile parser ala json or pyyaml - whereby you would import a Makefile into python as objects/dictionary/lists. Hence, I started searching for a Makefile Parser.\n  \n    The only parser I've found is PyMake(2) which is cool but it hasnt been updated since 2017 from what I recall and that it is more of a CLI utility, so with that in mind, I went about to make it\n  \n    I hope that this will be as useful as it is for me, currently I am using this in a side project and i'm able to format it such that its printing out a typical Makefile structure right off the bat, which is pretty nice.\n  \n    Additional information to the project\n  \n\n\nWhat My Project Does\n\n\n\n    This is a Makefile Parser, made in Python. The operational workflow is basically\n  Start --> Import File into dictionary --> Manipulation and Usage --> Processing --> Output --> Export\n\n\nTarget Audience (e.g., Is it meant for production, just a toy project, etc.)\n  \n\n\n    This is a library/package/module by design, much like json or pyyaml as mentioned but a smaller scale at the moment as its just started.\n  \n    I'm not sure if it applies for you but its a parser/importer\n  \n\n\nComparison (A brief comparison explaining how it differs from existing alternatives.)\n  \n\n\n    I'm not sure if there are any other Makefile Parsers other than Pymake2, but the idea is similar to the aforementioned ideas"
},
{
    "title": "No title",
    "content": "I recently ran into some challenges while trying to work with Apple's .numbers files on Linux. After a bit of experimentation, I figured out a workflow that simplifies the process. If you're planning to use .numbers files and need to load them into pandas, I've created a tutorial that covers the required dependencies and the steps to follow: https://nezhar.com/blog/load-apple-numbers-files-python-pandas-using-containers/.\n  \n    Has anyone else here worked with .numbers files in Python? I’d love to hear about your experiences or any tips you might have."
},
{
    "title": "No title",
    "content": "I was trying to tackle the problem of queue fairness and here's how I solved it."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "While Python doesn't explicitly support Type-Level Programming, I've had some success with it.\n  \n    I think this may be of interest to the Python community, so I'm sharing a POC (Proof Of Concept) I've written.\n  \n    This POC is a statically typed list that encodes its length in its type so that, for instance, when you concatenate two lists, the result has the correct length, all done at type-checking time.\n  \nRead more on GitHub"
},
{
    "title": "No title",
    "content": "A dense Python cheat sheet with just what you need.Design principles:• Focus on Python core• Comprehensive but selective (Just what you need)• Densely packed• Well-linked• Linkable• Responsive• PrintableIssues and feedback are tracked at the best-python-cheat-sheet repository.*It may not be the best Python cheat sheet, but it aspires to be."
},
{
    "title": "No title",
    "content": "test.py\nimport sys\nsys.setrecursionlimit(4)\nclass Foo:\n    class Foo:\n        foo = 1\npython test.py\n\n    Please be honest, write your guess in comments about the output of this code before opening the spoiler =)\n  \n    The answer: RecursionError: maximum recursion depth exceeded\n\n    Did you know?)"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hi everybody,\n  \n    I'm happy to share AnyPathLib 🛣️, a pip package I created to simplify access to different storage resources - S3, Azure, and local storage.\n  \nWhat My Project Does:\n\n    Basically, instead of writing custom code to handle the different storage resources using the SDK (boto3, azure-sdk), you can now just use AnyPathLib:\n  from anypathlib import AnyPath\n  \n\n  \n# Create an AnyPath instance for a local file\n  \n\n  \nlocal_file = AnyPath(\"/path/to/local/file.txt\")\n  \n\n  \n# Create an AnyPath instance for an S3 object\n  \n\n  \ns3_file = AnyPath(\"s3://bucket/path/to/object.txt\")\n  \n\n  \n# Copy a file from local to S3\n  \n\n  \nlocal_file.copy(s3_file)\n  \n\n  \n# Copy a directory from S3 to Azure\n  \n\n  \ns3_dir = AnyPath(\"s3://bucket/path/to/dir\") \n  \nazure_dir = AnyPath(\"https://account_name.blob.core.windows.net/container_name/path\") \n\ns3_dir.copy(azure_dir)\n    There are other cool features to save you some time, so check out this X thread for some more details or visit the Github project page.\n  \nTarget Audience:\n\n    Anybody which works with both a local environment and Azure or AWS, and could use a simplified API to access their storage\n  \nComparison:\n\n    I didn't find any similar library (that's why I wrote this one) - there are dedicated packages for S3 (boto3) and the Azure SDK, but they come with different, and somewhat unintuitive APIs.\n  \n    I'd love to hear your feedback, issues, and of course - PRs :)"
},
{
    "title": "No title",
    "content": "https://github.com/amalshaji/portr\nWhat my project does?\n    Portr is a tunnelling solution that let's you expose local http/tcp connections to the public internet.\n  Target audience\n    Developers that work with webhooks or callbacks. Or anyone who want to quickly share their dev server with others.\n  Comparison\n\n\n    Monitor connections, and manage teams and members via the admin dashboard. Watch video\n\n\n\n    Inspect and replay HTTP requests using portr inspector. Watch video.\n  \n\n\n    The admin backend is built using Python. I would appreciate a feedback."
},
{
    "title": "No title",
    "content": "What's the first step to building 90% of AI applications?Chunking your documents! Unfortunately there's very few libraries for this and most rely on naively splitting text into N characters. You lose so much valuable information hidden in the layout of the document when you do this.  If you give an LLM bad inputs, you're going to get bad results.\n\nhttps://github.com/Filimoa/open-parse/\n\nWhat it does:\n\n    The library can detect the layout of a PDF, convert to markdown and extract tables into html in one line of code. You can then feed this directly into your vector database or something like LlamaIndex.  It does all this while preserving bounding boxes so you can display citations directly to the user.\n  \nTarget Audience:\n\n    Anyone building a RAG AI app.  I run a talk-to-your-file website that's seen millions of documents and this library represents a giant chunk of what I know about feeding documents to LLM's.\n  \nComparison:\n\n    Open source libraries like LlamaIndex chunk your documents into strings of fixed length.  There's a couple commercial solutions that do this but they're expensive (typically 100 pages / dollar) and honestly have less than ideal results (AWS Texttract, Adobe, Google).\n  \n    Facebook has nougat for converting files to markdown but it's incredibly slow on anything without a massive GPU."
},
{
    "title": "No title",
    "content": "I recently posted my opinions on the import system in Python and how it can be made more intuitive and straightforward.\n  \n    I got some comments, some of them are just snappy as usual and some of them had some suggestions and workarounds.\n  \n    Either way the post got removed, citing that it is not suitable for this sub-reddit. I mean, come on, I cannot talk about Python features in Python sub-reddit?\n  \nUpdate:\n\n    To mods: Thanks for letting me have this conversation.\n  \n    To others who made helpful comments, thank you.\n  \nOriginal post:\n\n    I don't want to recreate the post verbatim here, I don't remember it exactly too. It was basically something like this,\n  \n    *requiring* or *loading* code, which is more or less importing can be much more straight forward if we can directly load files.\n  \n    Instead of the concept of packages, each file is a self contained module which can import and export classes, variables and functions.\n  \n    For example,\n  \nsrc/constants.py\n\nADMIN_ROLE_NAME = 'admin'\n\nsrc/user.py\n\nfrom constants import ADMIN_ROLE_NAME\n\ntests/user_test.py\n\nfrom ../../src/constants.py import ADMIN_ROLE_NAME\n\n    Here there would be no __init__.py files, just straightforward imports directly from files.\n  \n    I would refrain from giving example from Ruby or JS, like I did in the original post, because people perceive it as a fan boy post, while my intention is not that.\n  \n    I'll give a real example,\n  \n    Here is my repo, which https://github.com/rajaravivarma-r/python-robyn-react\n\n    I tried to put the contents of the `constants.py` file into `src/__init__.py` but there is no way of me importing them from within the child directory package, namely `src/api/__init__.py`.\n  \nBackward compatibility:\n\n    Lot of you have mentioned about how this would break existing code, but C++ standard had a recommendation to introduce module systems to new code which goes like this. Add a pragma like #pragma C++14_Modules at the top of the file, or something like that, but you get the idea.\n  \n    Ruby has a # frozen_string_literal: true\n\n    So Python could introduce something like that for new code, while treating the rest of the old code as it is.\n  \n    I believe this is constructive enough.\n  \n    I know this is not a PEP, but I want to show the community that my intention was genuine and not a rant.\n  \nOutro:Assume ignorance rather than malevolence.\n\n    Your snarky comments or aggressive down voting doesn't improve anyone's life. Last time when I posted about some surprising behaviour of Python in this community (from my old account), I got a comment about my daughter and wife. It wasn't even deleted despite reporting."
},
{
    "title": "No title",
    "content": "What my Project Does\n\n\n\n    It's a program that searches through posts in a specified subreddit and downloads images, gifs etc.\n  \n\n\nTarget Audience\n\n\n\n    It's a toy project at this point, but I might extend its functionality if I can find good resources. Maybe even add support for different websites (imgur et al.).\n  \n\n\nComparison\n\n\n\n    I found a few websites for downloading images from reddit but they claimed they didn't work because of the Reddit API changes, I was sceptical and wrote my own script.\n  \n    Anyhow,\n  \n    For the last few days I was trying to find good wallpapers for my system. I wrote a Python script for getting images and gifs from subreddits, here is the project repo. Please read the instructions before using it.\n  \n    It's pretty basic but gets the job done. I am still working on the project.\n  \n    I also wrote a flake.nix for my NixOS bros. Didn't test the project on Windows but I don't see why it wouldn't work.\n  \n    Lastly, here is my NixOS config in case  you are interested. Enjoy"
},
{
    "title": "No title",
    "content": "Discover the power of Modin in overcoming the performance bottlenecks of Pandas. We'll walk you through how a simple switch can reduce your waiting times and make your workflow smooth and speedy, all without leaving the comfort of Pandas behind.\n  \n    Modin is an active  open source project. Take a look to this blog post to see if it is applicable for your code: https://dchigarev.github.io/modin_perf_examples/"
},
{
    "title": "No title",
    "content": "I have been reading a lot about these tools recently and wondered if they are used in production, especially when building a backend with Python. As far as I know, PyPy is not ideal because of its lack of package support, but Cython seems pretty neat."
},
{
    "title": "No title",
    "content": "I've created a simple API service https://github.com/koldakov/futuramaapi to access Futurama units with async Python + FastAPI + SQLAlchemy (PostgreSQL).\n  \n    I've chosen Hypercorn for a web server to achieve HTTP/2 support - considering I didn't find yet cheap hosting that supports HTTP/2. Heroku doesn't support, render doesn't support.\n  \n    Also it has some code related to GraphQL (I used strawberry).\n  \n    + I didn't find a lot of discussions about Heroku + FastAPI - the problem will be with redirect to https. As default fastapi (starlette) middleware doesn't work with proxies there will be an infinite loop if you try to use fastapi.middleware.httpsredirect.HTTPSRedirectMiddleware. Chain will be: Client https-> Proxy(Hypercorn, for example) http-> fastapi, in that case as you can see the request is secure, but fastapi (starlette) won't understand this and will redirect to https. Here is a simple workaround to support https under proxy: https://github.com/koldakov/futuramaapi/blob/main/app/middlewares/secure.py\n\n\n    Constructive feedback is greatly appreciated.\n  \n\n    Source code: https://github.com/koldakov/futuramaapi\n\n    Working example: https://futuramaapi.com"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I created a python library \"dis-bg-remover\" based off the \"Highly Accurate Dichotomous Image Segmentation (https://arxiv.org/pdf/2203.03041.pdf), whose results are comparable, if not better, to the premium offerings in the market.\n  \n    Explainer video here\n  \nhttps://www.youtube.com/watch?v=js7AYKkZvFI"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hi thereI was tasked with migrating from mongoDb to PostgreSQL due to the performance issues.The backend is FastAPI-based web server for a mobile APP with a decent amount if CRUD. We also heavily use Pydantic for data validation and the primary carried of data across the service layers\n  \n    Would like to hear opinions from SQLModel users as to how easy it is to use or what kind of pitfalls am i setting myself up by using it.  Last post that I could find was years old and even that was not really conclusive. Now SQLAlchemy 2.0 is released and even SQLModel has support for Pydantic V2, so would like some newer thoughts in this topic.\n  \n    Our datamodel includes quite a bit of top level nestings, which I would prefer to be stored as `JSON` columns (Similar SO Question). SQLModel doesn't seem to have any documentation for such cases.\n  \n    If possible I would prefer avoiding to write and maintain our own internal library with SQLAlchemy ORM to perform these conversions. But maybe just using plain SQLAlchemy and Pydantic might be a good solution too.\n  \n    Seeking inputs from more experienced users. Thank you.\n  \n    Edit:\n  \n    In the end I gave up on using orm to query data and just used plain SQLAlchemy queries.\n  \n    The reason was that the ORM layer already abstract lot of stuff from us and SQLModel makes it more abstracted.\n  \n    This is fine if you know all the details of the abstractions. But at the point I am in, where I am new to sqlalchemy, it just adds lots more complexity and gotchas which can just avoid with plain queries.\n  \n    Maybe better documentation might help down the line but as of now, I am leaning towards just plain sqlqlchemy  core.\n  \n    Also using the ORM layer to just auto generate revisions is a nice feature."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I just published a tutorial series on how to automate a Python script in Google Cloud using Cloud Functions and/or Cloud Run. Feedback would be great. Thanks!\n  \n\n\nAutomating Python with Google Cloud\n\n\n\nAutomating Python with Google Cloud Functions\n\n\n\nAutomating Python with Google Cloud Run"
},
{
    "title": "No title",
    "content": "Hey all,\n  \n    I love what Django has with django-fsm. I require something but without Django, as there is no user interaction with the workflow. All inputs and outputs are either rabbitmq or another api. This is to be a workflow management system.\n  \n    I am looking for state management backed up by database so we can not suffer if a k8s pod dies with all the states in memory. Some of our workflows could take weeks.\n  \n    Is it still best to make this in Django or is there a database backed state management module available?\n  \n    I see pytransitions but I would have to add database logging to it."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Creating a GeoTIFF raster XYZ tile service in python with caching capability"
},
{
    "title": "No title",
    "content": "Hello everyone!\n  \n    I am proud to introduce a Makefile Parser for Python that I think will be useful!\n  \n    the link is Thanatisia/makefile-parser-python\n\n    As an introduction, I have been using python and been following this subreddit for quite awhile now, but this is my first post\n  \n    Recently, I've been planning a side-project involving the use of Makefiles in Python, and I required the use of a Makefile parser ala json or pyyaml - whereby you would import a Makefile into python as objects/dictionary/lists. Hence, I started searching for a Makefile Parser.\n  \n    The only parser I've found is PyMake(2) which is cool but it hasnt been updated since 2017 from what I recall and that it is more of a CLI utility, so with that in mind, I went about to make it\n  \n    I hope that this will be as useful as it is for me, currently I am using this in a side project and i'm able to format it such that its printing out a typical Makefile structure right off the bat, which is pretty nice.\n  \n    Additional information to the project\n  \n\n\nWhat My Project Does\n\n\n\n    This is a Makefile Parser, made in Python. The operational workflow is basically\n  Start --> Import File into dictionary --> Manipulation and Usage --> Processing --> Output --> Export\n\n\nTarget Audience (e.g., Is it meant for production, just a toy project, etc.)\n  \n\n\n    This is a library/package/module by design, much like json or pyyaml as mentioned but a smaller scale at the moment as its just started.\n  \n    I'm not sure if it applies for you but its a parser/importer\n  \n\n\nComparison (A brief comparison explaining how it differs from existing alternatives.)\n  \n\n\n    I'm not sure if there are any other Makefile Parsers other than Pymake2, but the idea is similar to the aforementioned ideas"
},
{
    "title": "No title",
    "content": "I recently ran into some challenges while trying to work with Apple's .numbers files on Linux. After a bit of experimentation, I figured out a workflow that simplifies the process. If you're planning to use .numbers files and need to load them into pandas, I've created a tutorial that covers the required dependencies and the steps to follow: https://nezhar.com/blog/load-apple-numbers-files-python-pandas-using-containers/.\n  \n    Has anyone else here worked with .numbers files in Python? I’d love to hear about your experiences or any tips you might have."
},
{
    "title": "No title",
    "content": "I was trying to tackle the problem of queue fairness and here's how I solved it."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "While Python doesn't explicitly support Type-Level Programming, I've had some success with it.\n  \n    I think this may be of interest to the Python community, so I'm sharing a POC (Proof Of Concept) I've written.\n  \n    This POC is a statically typed list that encodes its length in its type so that, for instance, when you concatenate two lists, the result has the correct length, all done at type-checking time.\n  \nRead more on GitHub"
},
{
    "title": "No title",
    "content": "I have a repo in GitHub with Jupyter Notebooks for students. On each notebook I have a link where the student can open the notebook in Colab (by changing the URL from https://github.com/ to https://githubtocolab.com/). The problem is that some school boards have Colab blocked.\n  \n    Is there a similar technique I can use to open the notebooks in Kaggle?\n  \n    Are there other sites I should be considering?\n  \n    Thanks!"
},
{
    "title": "No title",
    "content": "i need some python library that can automatically generate a synchronization map between a list of text fragments and an audio file containing the narration of the text. aeneas does exactly that except it doesnt work on higher than 2.7 with windows."
},
{
    "title": "No title",
    "content": "Call for Proposals open through May 20\n  \n    PyOhio is a fun, friendly, free general Python conference now in its Nth year (N is large).  This year it will be Sat & Sun Jul 27-28, 2024 at The Westin Cleveland Downtown in Cleveland, OH (its first year outside Columbus)!\n  \n    Hope to see you there!"
},
{
    "title": "No title",
    "content": "Hi everyone, I developed a simple plugin for mkdocs-material.\n  \nWhat My Project Does:\n\n    In simple terms, this plugin allows users to mark pages as read and it shows a checkmark icon in navigation bar for the pages that was marked as read.The plugin adds a button under the page content and when users clicks, it stores read date in localStorage. It shows a checkmark icon in the navigation panels for the pages that marked as read. It also shows a \"document updated\" icon if the pages that marked as read got an update.\n  \nTarget Audience:\n\n    Anyone who has a website built with Material for MkDocs (a.k.a. mkdocs-material).This plugin could be useful for the pages that user read by an order and wants to continue from where it left. I guess the Learn page of FastAPI documentation could be a great example to that.\n  \nComparison: No alternative plugin exist for Material for MkDocs afaik\n  \nProject repo: github.com/berk-karaal/mkdocs-material-mark-as-read\n\n    You can try this plugin on the documentation website. Let me know what you think about this plugin. Also please share if you have a feature request or an idea to improve this plugin."
},
{
    "title": "No title",
    "content": "Hello r/Python,\n  \nWhat My Project Does:\n\n    This script is designed to automate the management of Python virtual environments and dependencies, streamlining the setup process for Python projects. It facilitates a more efficient workflow for managing project-specific environments and package installations, especially in a professional development setting.\n  \nTarget Audience: This tool is particularly beneficial for developers and IT professionals looking for a systematic approach to environment management. It is designed to integrate into existing workflows, providing a reliable and consistent method for managing Python environments and dependencies.\n  \nFeatures include:\n\n\n\n    Automated creation of Python virtual environments.\n  \n\n\n    Batch installation of packages from a predefined array or a requirements.txt file.\n  \n\n\n    Can import, update, upgrade, and remove packages within the virtual environment.\n  \n\n\n    Functionalities for listing all installed packages for transparency and audit purposes.\n  \n\n\nBenefits:\n\n\n\nEfficiency: Reduces manual setup and management of virtual environments.\n  \n\n\nConsistency: Ensures uniform environments across development stages and projects.\n  \n\n\nFlexibility: Supports custom package lists and requirements, adaptable to project-specific needs.\n  \n\n\nComparison:\n  \n\n\n    Fast setup , updates, and removal.\n  \n\n\n    Set custom paths to store your files\n  \n\n\n    Everyone is invited to download, implement, and provide feedback on this script to further refine its capabilities to meet professional standards and requirements (AKA just be really useful).\n  \n    For access and further details, please visit: GitHub"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hi all,\n  \n    I've recently found a Python Malware in a FOSS tool that is currently available on GitHub. I've written about how I found it, what it does and who the author is. The whole malware analysis is available in form of an article.\n  \n    I would appreciate any and all feedback."
},
{
    "title": "No title",
    "content": "Hi r/Python,\n  \n    In case you didn't know, ipython-sql has been forked. The new project has fixed some long-standing issues and added a bunch of new capapabilities:\n  \n\n\n    Splitting long SQL queries in multiple cells\n  \n\n\n    Plotting large-scale datasets\n  \n\n\n    More flexibility to open database connections\n  \n\n\n    The API remains the same, you can replace projects that depend on ipython-sql with jupysql: pip install jupysql\n\n    You can read more about the project here. There's also a post in DuckDB's blog that you might want to check out, too."
},
{
    "title": "No title",
    "content": "What My Project Does: Goprox is a Python module that revolutionizes Google searches by automatically checking and using proxies, eliminating the need for user input. With Goprox, users can enjoy seamless searching without worrying about proxy configuration or getting blocked.\n  \nTarget Audience: Goprox is perfect for developers seeking to automate Google searches for web scraping, automation, or data collection tasks. It's also ideal for anyone who desires a hassle-free search experience without the hassle of manual proxy management.\n  \nComparison: Compared to existing alternatives, Goprox stands out with its focus on automatic proxy handling. Unlike other solutions that require manual proxy input, Goprox streamlines the process by autonomously managing proxies for each search query.\n  \n    Experience the power of Goprox on GitHub today! Your feedback and contributions are greatly appreciated."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hello Python community. My name is Alexander – I am the author of Centrifugo project. It's a self-hosted real-time messaging server (alternative to Ably, Pusher, Pubnub services). It was written in Python originally, but then migrated to Go. But it's fully language-agnostic and helps many projects written in Python (and in Django in particular) to add real-time updates to the application. Centrifugo is quite fast, scales well, has super-efficient integration with Redis (~million of publications per second, and more with Redis sharding/Redis Cluster).\n  What My Project Does\n    Any kind of real-time messaging apps may be built with the help of Centrifugo. Chat/messenger apps, real-time multiplayer games, turn-based games in particular. Streaming metrics. The best thing is that Centrifugo is a separate language-agnostic service which provides API for client connections (WebSocket, EventSource, HTTP-streaming, experimental Webtransport, GRPC) and for backend communication (over HTTP or GRPC). So it may be used as a universal real-time component throughout different tech stacks. Including Python. Centrifugo is used in many projects, for example, our core WebSocket library is part of Grafana.\n  Target Audience\n    Software engineers, startups and mature projects that require real-time updates in the application. Centrifugo gives answers to some problems developers may come across when building real-time app in scale. See our blog post: Scaling WebSocket in Go and beyond.\n  Comparison\n    There is no direct analogue, but many projects exist in the area. Some of them cloud-based - like pusher.com, ably.com, pubnub.com. Some are self-hosted - like Mercure. We have comparison with similar technologies on Centrifugo site. I'd say Protobuf protocol, transport selection, both bidirectional and unidirectional approaches, super-efficient built-in Redis integration for scalability are some selling points of Centrifugo when comparing to other self-hosted solutions.\n  The actual update\n    During last month Centrifugal ecosystem got several Python updates, and I'd like to share this with you:\n  \n\n\n    We've released Python real-time SDK for Centrifugo. See centrifuge-python. This is a WebSocket client, uses JSON or Protobuf for communicating with Centrifugo. Real-time SDKs usually used on client-side of app - it's possible to subscribe/unsubscribe on channels, receive online presence data, communication with the backend over RPC calls through WebSocket.\n  \n\n\n    Next library we just released is pycent v5, HTTP SDK for Centrifugo server API. Most of the time you publish real-time data to Centrifugo channels you are using server API, and this is a small lib that simplifies integration with Centrifugo. It has both sync and async clients, uses Pydantic for DTO.\n  \n\n\n    Finally, not exactly generic Python related, but I'd like to mention it also because we've put a lot of effort into it. We've released a Grand Tutorial for Centrifugo which shows how to build scalable chat/messenger application on top of Django and Centrifugo. From scratch. It covers some aspects of application building other tutorials never mention - delivery guarantees, approaches for reliable delivery and idempotent processing, shows some numbers.\n  \n\n\n    Hope this may be useful to someone in the community. Since Centrifugo has roots in Python a good integration with the ecosystem is very important for us. If you have any questions about a project – will be happy to answer."
},
{
    "title": "No title",
    "content": "Hi all!\n  \n    The Newspaper3k is abandoned (latest release in 2018) without any upgrades and bugfixing.\n  \n    I forked it, and imported all open Issues into my repo. The first two releases (0.9.0 and 0.9.1) were mainly bugfixes and bringing the project more up to date and compatible with python > 3.6  (I started from version 0.9.0 😁). In the latest version, 0.9.3 I not only almost reworked the whole News article parsing process, but also added a lot of new supported languages (around 40 new languages)\n  \n\nRepository: https://github.com/AndyTheFactory/newspaper4k\n\nDocumentation: https://newspaper4k.readthedocs.io/\n\nWhat My Project Does\n    Newspaper4k helps you in extracting and curating articles from news websites. Leveraging automatic parsers and natural language processing (NLP) techniques, it aims to extract significant details such as: Title, Authors, Article Content, Images, Keywords, Summaries, and other relevant information and metadata from newspaper articles and web pages. The primary goal is to efficiently extract the main textual content of articles while eliminating any unnecessary elements or \"boilerplate\" text that doesn't contribute to the core information.\n  Target Audience\n    Newspaper4k is built for developers, researchers, and content creators who need to process and analyze news content at scale, providing them with powerful tools to automate the extraction and evaluation of news articles.\n  Comparisons\n    As of the 0.9.3 version, the library can also parse the Google News results based on keyword search, topic, country, etc\n  \n    The documentation is expanded and I added a series of usage examples. The integration with Playwright  is possible (for websites that generate the content with javascript), and since 0.9.3 I integrated cloudscraper that attempts to circumvent Cloudflair protections.\n  \n    Also, compared with the latest release of newspaper3k (0.2.8), the results on the Scraperhub Article Extraction Benchmark are much improved and the multithreaded news retrieval is now stable.\n  \n    Please don't hesitate to provide your feedback and make use of it! I highly value your input and encourage you to play around with the  project."
},
{
    "title": "No title",
    "content": "Hey guys! My friend and I are building Nelima. It's basically a Large Language Model designed to take actions on your behalf with natural language prompts and theoretically automate anything. For example, it can schedule appointments, send emails, check the weather, and even connect to IoT devices to let you command it – you can ask it to publish a website or call an Uber for you!\n  \n    You can integrate your own custom actions, written in Python, to suit your specific needs, and layer multiple actions to perform more complex tasks. When you create these actions or functions, it contributes to the overall capabilities of Nelima, and everyone can now invoke the same action. Right now, it's a quite limited in terms of the # of actions it can do but we're having fun building a few :)\n  \n    Nelima can see the outcomes of each micro-action undertaken to achieve the overarching goal. The potential for reasoning is very much possible and doesn't shy away from taking measures – for example, if it sees your grocery list from a sub-action on fulfilling an action and realizes that a certain item has allergens which might be harmful to the user, it puts a warning label, even though the user didn't ask for this.\n  \n    I thought the community here might find it useful. It uses Python 3 (Version 3.11), and the environment includes the following packages: BeautifulSoup, urllib3, requests, pyyaml. We’ll try to include more if people need those.\n  \n    Give it a try and let me know what you think! :)"
},
{
    "title": "No title",
    "content": "EDIT: Since there are a lot of upvotes on that comment, yes, this is about seeking strangers' validations. But we're all validation seekers. For instance, we all need someone's validation to make a living (boss, customers, etc.) Your boss ain't gonna tell you \"god damn boy, you suck at your job, here take my money\". But in this case, validation seeking is kind of a broad term, let's use something more specialized like Market Research 😉. Also, thanks for all your comments and inputs.\n  \n    Hello everyone\n  \n    I've been working on this project for the past 3 weeks, and I want people's opinions to determine how much time I should invest in it.\n  \n    Project Link: https://github.com/mnvoh/cameratokeyboard\nWhat My Project Does\n    It's a computer vision project (python, YOLOv8) which essentially allows you to use your camera as a keyboard by detecting your fingers and analyzing their movements. The only requirement (besides having a camera) is that you have to print the keyboard (on an A4 paper, for example). The keyboard is for you and there are 4 markers identifying the boundaries of the keyboard which are for the program.\n  Target Audience\n    Currently, it's a PoC, but the goal is to develop it into a fully functioning virtual keyboard with a desirable accuracy.\n  \n    The initial idea came to me while thinking that it would be actually cool if you could print your keyboard design on your desk mat, and then just use that as a keyboard, but it could also have substantial applications in cell phones.\n  Comparison\n    I have searched quite a bit, but haven't found any similar solutions.\n  \nEDIT:\n\n    Thanks to u/avaqueue for finding these articles:\n  \n\n\nhttps://www.academia.edu/105250798/Paper_Keyboard_Using_Image_Processing?uc-sb-sw=5982163\n\n\n\nhttps://ieeexplore.ieee.org/document/6377072\n\n\n\n    Also, u/HobblingCobbler has mentioned that they've had a phone with this feature. waiting on more info\n  The Main Question\n    Now I have had an enormously gigantic amount of fun working on this and will continue for sure, but how much time depends on its potential. That's why I'm asking for your opinions:\n  \nIs it actually worth it? Or am I imagining its potentials?"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Mine is a web scraper. It’s only like 50 lines of code.\n  \n    It takes in a link, pulls all the hyperlinks and then does some basic regex to pull out the info I want. Then it spits out a file with all the links.\n  \n    Took me like 20 minutes to code, but I feel like I use it every other week to pull a bunch of links for files I might want to download quickly or to pull data from sites to model."
},
{
    "title": "No title",
    "content": "Hello!\n  \n    It's still early in development, but I just want to know if people are interested.\n  \nWhat My Project Does\n\n    I created a library to integrate Pydantic type checking for handwritten SQL queries. It also allows you to test the types, and create nested queries easily.\n  \nAlternative Queries\n\nTarget Audience\n\n    We had a project recently were we had to use handwritten SQL and managing the parameters and reusing queries was quite a hassle. So I'm planning to use this on my production projects moving forward.\n  \nComparison\n\n    I would say in terms of usage, it's like Pydantic + SqlParams, but only with the python default formatting.\n  \nSqlParams"
},
{
    "title": "No title",
    "content": "Hi,\n  \n    I'd like to introduce to you my open-source FEA package FElupe, available on GitHub. Its target audience is typically an engineer, e.g. in the field of mechanical or biomedical engineering. It is designed to be both flexible for scientifc research as well as easy-to-use for industry-related problems. Packages with similar scopes are e.g. scikit-fem and Fenics(x).\n  \n    FElupe is a Python 3.8+ 🐍 finite element analysis package 📦 focussing on the formulation and numerical solution of nonlinear problems in continuum mechanics 🔧 of solid bodies 🚂. Its name is a combination of FE (finite element) and the german word Lupe 🔍 (magnifying glass) as a synonym for getting an insight 📖 how a finite element analysis code 🧮 looks like under the hood 🕳️.\n  \n    FElupe has minimal requirements, all available at PyPI supporting all platforms.\n  pip install felupe[all]\n    It's pure Python but the assembly performance is well suited for mid-sized problems. It is possible to assemble up to 130000 degrees of freedom for linear elasticity on a modern notebook in one second runtime (results may vary).\n  import felupe as fem\n\nmesh = fem.Cube(n=6)\nregion = fem.RegionHexahedron(mesh)\nfield = fem.FieldContainer([fem.Field(region, dim=3)])\n\nboundaries, loadcase = fem.dof.uniaxial(field, clamped=True)\n\numat = fem.OgdenRoxburgh(material=fem.NeoHooke(mu=1), r=3, m=1, beta=0)\nsolid = fem.SolidBodyNearlyIncompressible(umat, field, bulk=5000)\n\nmove = fem.math.linsteps([0, 1, 0, 1, 2, 1], num=5)\nstep = fem.Step(items=[solid], ramp={boundaries[\"move\"]: move}, boundaries=boundaries)\n\njob = fem.CharacteristicCurve(steps=[step], boundary=boundaries[\"move\"])\njob.evaluate(filename=\"result.xdmf\")\nfig, ax = job.plot(\n    xlabel=\"Displacement $u$ in mm $\\longrightarrow$\",\n    ylabel=\"Normal Force $F$ in N $\\longrightarrow$\",\n)\n\nsolid.plot(\"Principal Values of Cauchy Stress\").show()\n    The above minmal code-block covers the essential high-level parts of creating and solving problems with FElupe.\n  \n    There is also a Gallery of Examples.\n  \n    Please let me know what you think about it. If you like it - I'd be glad if you share it with your friends, star it on GitHub, whatever you like. Thanks!\n  \nhttps://github.com/adtzlr/felupe"
},
{
    "title": "No title",
    "content": "I created  a simple tool for extracting text from PDF, EPUB, TXT, and DOCX files.It is mainly for personal use, but I would really appreciate a feedback\n  \nhttps://github.com/KirillAn/extractText/tree/main"
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    PyBackport holds serves three purposes:\n  \n\n\n    Use instances from newer python releases in older python versions. For example, using StrEnum (new in python 3.11) in python 3.9\n  \n\n\n    Enable experimental unreleased classes. For example, define new enums where members are tuples with TupleEnum\n\n\n\n    Provide support to commonly used constants. For example, PyBackport allows to import and use colors.RGB to avoid re-defining constant color codes in every new project.\n  \n\n\nTarget Audience\n\n    The audience is developers that must create scripts for older python versions, such as new packages which must support python 3.8+. With PyBackport these instances from newer python versions can be used.\n  \nComparison\n\n    I've not encountered any similar released package with good documentation.\n  \n    I also choose not to set a release V1.0 for my package, as I backported only some enums functionality. Anyone feel free to open a new issue, contribute with new pull requests or to notify me any other useful instances to backport. I intend to keep expanding the package as much as possible.\n  \nLinks\n\nGitHub repo\n\nPyPI release\n\nDocumentation"
},
{
    "title": "No title",
    "content": "In short: here is the hacky 2-line header that makes any python script a clickable batch file(assuming bundled python in a subfolder)\n  @classmethod # 2>nul & (if not exist \"%~dp0\\python64-win\\python.exe\" (echo Fatal python64-win\\python.exe not found & pause) else (title %~f0 & \"%~dp0\\python64-win\\python.exe\" \"%~f0\" %*)) & exit /B & # noqa: E501\ndef __unused(): \"fake function to help writing header that allows executing same file as python and batch\"\n\n# Normal script starts here\n    Its usage is either double-click to run without args or \"drag a file on it\" if the script processes a file (passed as an argument). Its installation is as easy as .zip unpacking\n  \n    More detailed:\n  \n    From time to time I get in touch with small handy one-file python scripts useful for some simple but very handy automations for a some niche use case on the windows platform.\n  \n    This post is not about tasks themselves - there may be really anything, but about distributing them. Such scripts are typically has no any repo/site and just attached to a message or similar, since the author has no any plans for further development, but still wants to share.\n  \n    90% of target users are not developers, and just can use the \"double-clickable\" application. The other 10% are developers who may want to perform some script enhancements.\n  \n    Non-developers needs \"look at max 1-2 sentence documentation + several clicks to install + several to launch\". Developers need the ability to enhance the script. How to get both satisfied?\n  \n    Often such scripts are packed & bundled with a python into a single .exe - this is not editable, the other variant is distributing .py with the too large doc \"download python installer from official site, install it, then click the .py file\" and without an easy way to distribute dependency packages.\n  \n    As an author of some such scripts after several improve iterations I found a solution that satisfy both needs: distribute .zip containing bundled portable python interpreter with preinstalled dependencies in a subdirectory and a clickable file being simultaneously a .bat file and a python script - the extension is .py.bat or just .bat\n  \n    Getting portable python is a bit tricky, but embeddable official image is mostly ok."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Toschat !What My Project Does?\n    a lightweight python chat app within your terminal with simple UI and it's built on top of Textual framework. What you can do:\n  \n    - Create a new account\n  \n    - Add your friends to your contact\n  \n    - and chat!\n  Target Audience\n    It's my hobby project to improve my python knowledge but everyone can install and use it. All data store in a live server with Django framework.\n  Comparison\n    Instead of texting with your colleagues or your friends on your phone or a web browser, .. why not your terminal ? 😁\n  \n    I'm happy for advices on code improvements, features recommendations and feedbacks. 😁😁\n  \n    Github Link: https://github.com/MuongKimhong/toschat\n\n    If you find it interesting and useful, give it a star I would appreciate it very much.Have a good weekend programmers!"
},
{
    "title": "No title",
    "content": "From the Article:This provides a good overview of how Reflex works under the hood.\n  \nTLDR:Under the hood, Reflex apps compile down to a React frontend app and a FastAPI backend app. Only the UI is compiled to Javascript; all the app logic and state management stays in Python and is run on the server. Reflex uses WebSockets to send events from the frontend to the backend, and to send state updates from the backend to the frontend.\n  \nFull post: https://reflex.dev/blog/2024-03-21-reflex-architecture/#designing-a-pure-python-web-framework"
},
{
    "title": "No title",
    "content": "I shared a \"monads in python library\" a little while back on here- since then I've been toying with and expanding things out into a more general project for helping functional programming patterns in python.\n  \n    [I had (am having) a bunch of fun making this - thought I'd share here!](https://github.com/benrutter/ufo-tools)\n  \n    What my project does: A simple library with utilities for functional programming in Python.Target Audience: It's meant for use in actual production contexts, but it's still in a very early and experimental phase. It's probably most helpful right now as a learning resource.\n  \n    Comparison: Most similar project I know of is the Returns library, which is great, but a lot more heavy weight. It has more of an expectation of being used for everything, rather than a drop in as-and-when tool.\n  \n    I'd love any feedback in general (as well as any healthy flamewars around the herecy or one-true-way of functional programming)"
},
{
    "title": "No title",
    "content": "I have a repo in GitHub with Jupyter Notebooks for students. On each notebook I have a link where the student can open the notebook in Colab (by changing the URL from https://github.com/ to https://githubtocolab.com/). The problem is that some school boards have Colab blocked.\n  \n    Is there a similar technique I can use to open the notebooks in Kaggle?\n  \n    Are there other sites I should be considering?\n  \n    Thanks!"
},
{
    "title": "No title",
    "content": "i need some python library that can automatically generate a synchronization map between a list of text fragments and an audio file containing the narration of the text. aeneas does exactly that except it doesnt work on higher than 2.7 with windows."
},
{
    "title": "No title",
    "content": "Call for Proposals open through May 20\n  \n    PyOhio is a fun, friendly, free general Python conference now in its Nth year (N is large).  This year it will be Sat & Sun Jul 27-28, 2024 at The Westin Cleveland Downtown in Cleveland, OH (its first year outside Columbus)!\n  \n    Hope to see you there!"
},
{
    "title": "No title",
    "content": "Hi everyone, I developed a simple plugin for mkdocs-material.\n  \nWhat My Project Does:\n\n    In simple terms, this plugin allows users to mark pages as read and it shows a checkmark icon in navigation bar for the pages that was marked as read.The plugin adds a button under the page content and when users clicks, it stores read date in localStorage. It shows a checkmark icon in the navigation panels for the pages that marked as read. It also shows a \"document updated\" icon if the pages that marked as read got an update.\n  \nTarget Audience:\n\n    Anyone who has a website built with Material for MkDocs (a.k.a. mkdocs-material).This plugin could be useful for the pages that user read by an order and wants to continue from where it left. I guess the Learn page of FastAPI documentation could be a great example to that.\n  \nComparison: No alternative plugin exist for Material for MkDocs afaik\n  \nProject repo: github.com/berk-karaal/mkdocs-material-mark-as-read\n\n    You can try this plugin on the documentation website. Let me know what you think about this plugin. Also please share if you have a feature request or an idea to improve this plugin."
},
{
    "title": "No title",
    "content": "Hello r/Python,\n  \nWhat My Project Does:\n\n    This script is designed to automate the management of Python virtual environments and dependencies, streamlining the setup process for Python projects. It facilitates a more efficient workflow for managing project-specific environments and package installations, especially in a professional development setting.\n  \nTarget Audience: This tool is particularly beneficial for developers and IT professionals looking for a systematic approach to environment management. It is designed to integrate into existing workflows, providing a reliable and consistent method for managing Python environments and dependencies.\n  \nFeatures include:\n\n\n\n    Automated creation of Python virtual environments.\n  \n\n\n    Batch installation of packages from a predefined array or a requirements.txt file.\n  \n\n\n    Can import, update, upgrade, and remove packages within the virtual environment.\n  \n\n\n    Functionalities for listing all installed packages for transparency and audit purposes.\n  \n\n\nBenefits:\n\n\n\nEfficiency: Reduces manual setup and management of virtual environments.\n  \n\n\nConsistency: Ensures uniform environments across development stages and projects.\n  \n\n\nFlexibility: Supports custom package lists and requirements, adaptable to project-specific needs.\n  \n\n\nComparison:\n  \n\n\n    Fast setup , updates, and removal.\n  \n\n\n    Set custom paths to store your files\n  \n\n\n    Everyone is invited to download, implement, and provide feedback on this script to further refine its capabilities to meet professional standards and requirements (AKA just be really useful).\n  \n    For access and further details, please visit: GitHub"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hi all,\n  \n    I've recently found a Python Malware in a FOSS tool that is currently available on GitHub. I've written about how I found it, what it does and who the author is. The whole malware analysis is available in form of an article.\n  \n    I would appreciate any and all feedback."
},
{
    "title": "No title",
    "content": "Hi r/Python,\n  \n    In case you didn't know, ipython-sql has been forked. The new project has fixed some long-standing issues and added a bunch of new capapabilities:\n  \n\n\n    Splitting long SQL queries in multiple cells\n  \n\n\n    Plotting large-scale datasets\n  \n\n\n    More flexibility to open database connections\n  \n\n\n    The API remains the same, you can replace projects that depend on ipython-sql with jupysql: pip install jupysql\n\n    You can read more about the project here. There's also a post in DuckDB's blog that you might want to check out, too."
},
{
    "title": "No title",
    "content": "What My Project Does: Goprox is a Python module that revolutionizes Google searches by automatically checking and using proxies, eliminating the need for user input. With Goprox, users can enjoy seamless searching without worrying about proxy configuration or getting blocked.\n  \nTarget Audience: Goprox is perfect for developers seeking to automate Google searches for web scraping, automation, or data collection tasks. It's also ideal for anyone who desires a hassle-free search experience without the hassle of manual proxy management.\n  \nComparison: Compared to existing alternatives, Goprox stands out with its focus on automatic proxy handling. Unlike other solutions that require manual proxy input, Goprox streamlines the process by autonomously managing proxies for each search query.\n  \n    Experience the power of Goprox on GitHub today! Your feedback and contributions are greatly appreciated."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hello Python community. My name is Alexander – I am the author of Centrifugo project. It's a self-hosted real-time messaging server (alternative to Ably, Pusher, Pubnub services). It was written in Python originally, but then migrated to Go. But it's fully language-agnostic and helps many projects written in Python (and in Django in particular) to add real-time updates to the application. Centrifugo is quite fast, scales well, has super-efficient integration with Redis (~million of publications per second, and more with Redis sharding/Redis Cluster).\n  What My Project Does\n    Any kind of real-time messaging apps may be built with the help of Centrifugo. Chat/messenger apps, real-time multiplayer games, turn-based games in particular. Streaming metrics. The best thing is that Centrifugo is a separate language-agnostic service which provides API for client connections (WebSocket, EventSource, HTTP-streaming, experimental Webtransport, GRPC) and for backend communication (over HTTP or GRPC). So it may be used as a universal real-time component throughout different tech stacks. Including Python. Centrifugo is used in many projects, for example, our core WebSocket library is part of Grafana.\n  Target Audience\n    Software engineers, startups and mature projects that require real-time updates in the application. Centrifugo gives answers to some problems developers may come across when building real-time app in scale. See our blog post: Scaling WebSocket in Go and beyond.\n  Comparison\n    There is no direct analogue, but many projects exist in the area. Some of them cloud-based - like pusher.com, ably.com, pubnub.com. Some are self-hosted - like Mercure. We have comparison with similar technologies on Centrifugo site. I'd say Protobuf protocol, transport selection, both bidirectional and unidirectional approaches, super-efficient built-in Redis integration for scalability are some selling points of Centrifugo when comparing to other self-hosted solutions.\n  The actual update\n    During last month Centrifugal ecosystem got several Python updates, and I'd like to share this with you:\n  \n\n\n    We've released Python real-time SDK for Centrifugo. See centrifuge-python. This is a WebSocket client, uses JSON or Protobuf for communicating with Centrifugo. Real-time SDKs usually used on client-side of app - it's possible to subscribe/unsubscribe on channels, receive online presence data, communication with the backend over RPC calls through WebSocket.\n  \n\n\n    Next library we just released is pycent v5, HTTP SDK for Centrifugo server API. Most of the time you publish real-time data to Centrifugo channels you are using server API, and this is a small lib that simplifies integration with Centrifugo. It has both sync and async clients, uses Pydantic for DTO.\n  \n\n\n    Finally, not exactly generic Python related, but I'd like to mention it also because we've put a lot of effort into it. We've released a Grand Tutorial for Centrifugo which shows how to build scalable chat/messenger application on top of Django and Centrifugo. From scratch. It covers some aspects of application building other tutorials never mention - delivery guarantees, approaches for reliable delivery and idempotent processing, shows some numbers.\n  \n\n\n    Hope this may be useful to someone in the community. Since Centrifugo has roots in Python a good integration with the ecosystem is very important for us. If you have any questions about a project – will be happy to answer."
},
{
    "title": "No title",
    "content": "Hi all!\n  \n    The Newspaper3k is abandoned (latest release in 2018) without any upgrades and bugfixing.\n  \n    I forked it, and imported all open Issues into my repo. The first two releases (0.9.0 and 0.9.1) were mainly bugfixes and bringing the project more up to date and compatible with python > 3.6  (I started from version 0.9.0 😁). In the latest version, 0.9.3 I not only almost reworked the whole News article parsing process, but also added a lot of new supported languages (around 40 new languages)\n  \n\nRepository: https://github.com/AndyTheFactory/newspaper4k\n\nDocumentation: https://newspaper4k.readthedocs.io/\n\nWhat My Project Does\n    Newspaper4k helps you in extracting and curating articles from news websites. Leveraging automatic parsers and natural language processing (NLP) techniques, it aims to extract significant details such as: Title, Authors, Article Content, Images, Keywords, Summaries, and other relevant information and metadata from newspaper articles and web pages. The primary goal is to efficiently extract the main textual content of articles while eliminating any unnecessary elements or \"boilerplate\" text that doesn't contribute to the core information.\n  Target Audience\n    Newspaper4k is built for developers, researchers, and content creators who need to process and analyze news content at scale, providing them with powerful tools to automate the extraction and evaluation of news articles.\n  Comparisons\n    As of the 0.9.3 version, the library can also parse the Google News results based on keyword search, topic, country, etc\n  \n    The documentation is expanded and I added a series of usage examples. The integration with Playwright  is possible (for websites that generate the content with javascript), and since 0.9.3 I integrated cloudscraper that attempts to circumvent Cloudflair protections.\n  \n    Also, compared with the latest release of newspaper3k (0.2.8), the results on the Scraperhub Article Extraction Benchmark are much improved and the multithreaded news retrieval is now stable.\n  \n    Please don't hesitate to provide your feedback and make use of it! I highly value your input and encourage you to play around with the  project."
},
{
    "title": "No title",
    "content": "Hey guys! My friend and I are building Nelima. It's basically a Large Language Model designed to take actions on your behalf with natural language prompts and theoretically automate anything. For example, it can schedule appointments, send emails, check the weather, and even connect to IoT devices to let you command it – you can ask it to publish a website or call an Uber for you!\n  \n    You can integrate your own custom actions, written in Python, to suit your specific needs, and layer multiple actions to perform more complex tasks. When you create these actions or functions, it contributes to the overall capabilities of Nelima, and everyone can now invoke the same action. Right now, it's a quite limited in terms of the # of actions it can do but we're having fun building a few :)\n  \n    Nelima can see the outcomes of each micro-action undertaken to achieve the overarching goal. The potential for reasoning is very much possible and doesn't shy away from taking measures – for example, if it sees your grocery list from a sub-action on fulfilling an action and realizes that a certain item has allergens which might be harmful to the user, it puts a warning label, even though the user didn't ask for this.\n  \n    I thought the community here might find it useful. It uses Python 3 (Version 3.11), and the environment includes the following packages: BeautifulSoup, urllib3, requests, pyyaml. We’ll try to include more if people need those.\n  \n    Give it a try and let me know what you think! :)"
},
{
    "title": "No title",
    "content": "EDIT: Since there are a lot of upvotes on that comment, yes, this is about seeking strangers' validations. But we're all validation seekers. For instance, we all need someone's validation to make a living (boss, customers, etc.) Your boss ain't gonna tell you \"god damn boy, you suck at your job, here take my money\". But in this case, validation seeking is kind of a broad term, let's use something more specialized like Market Research 😉. Also, thanks for all your comments and inputs.\n  \n    Hello everyone\n  \n    I've been working on this project for the past 3 weeks, and I want people's opinions to determine how much time I should invest in it.\n  \n    Project Link: https://github.com/mnvoh/cameratokeyboard\nWhat My Project Does\n    It's a computer vision project (python, YOLOv8) which essentially allows you to use your camera as a keyboard by detecting your fingers and analyzing their movements. The only requirement (besides having a camera) is that you have to print the keyboard (on an A4 paper, for example). The keyboard is for you and there are 4 markers identifying the boundaries of the keyboard which are for the program.\n  Target Audience\n    Currently, it's a PoC, but the goal is to develop it into a fully functioning virtual keyboard with a desirable accuracy.\n  \n    The initial idea came to me while thinking that it would be actually cool if you could print your keyboard design on your desk mat, and then just use that as a keyboard, but it could also have substantial applications in cell phones.\n  Comparison\n    I have searched quite a bit, but haven't found any similar solutions.\n  \nEDIT:\n\n    Thanks to u/avaqueue for finding these articles:\n  \n\n\nhttps://www.academia.edu/105250798/Paper_Keyboard_Using_Image_Processing?uc-sb-sw=5982163\n\n\n\nhttps://ieeexplore.ieee.org/document/6377072\n\n\n\n    Also, u/HobblingCobbler has mentioned that they've had a phone with this feature. waiting on more info\n  The Main Question\n    Now I have had an enormously gigantic amount of fun working on this and will continue for sure, but how much time depends on its potential. That's why I'm asking for your opinions:\n  \nIs it actually worth it? Or am I imagining its potentials?"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Mine is a web scraper. It’s only like 50 lines of code.\n  \n    It takes in a link, pulls all the hyperlinks and then does some basic regex to pull out the info I want. Then it spits out a file with all the links.\n  \n    Took me like 20 minutes to code, but I feel like I use it every other week to pull a bunch of links for files I might want to download quickly or to pull data from sites to model."
},
{
    "title": "No title",
    "content": "Hello!\n  \n    It's still early in development, but I just want to know if people are interested.\n  \nWhat My Project Does\n\n    I created a library to integrate Pydantic type checking for handwritten SQL queries. It also allows you to test the types, and create nested queries easily.\n  \nAlternative Queries\n\nTarget Audience\n\n    We had a project recently were we had to use handwritten SQL and managing the parameters and reusing queries was quite a hassle. So I'm planning to use this on my production projects moving forward.\n  \nComparison\n\n    I would say in terms of usage, it's like Pydantic + SqlParams, but only with the python default formatting.\n  \nSqlParams"
},
{
    "title": "No title",
    "content": "Hi,\n  \n    I'd like to introduce to you my open-source FEA package FElupe, available on GitHub. Its target audience is typically an engineer, e.g. in the field of mechanical or biomedical engineering. It is designed to be both flexible for scientifc research as well as easy-to-use for industry-related problems. Packages with similar scopes are e.g. scikit-fem and Fenics(x).\n  \n    FElupe is a Python 3.8+ 🐍 finite element analysis package 📦 focussing on the formulation and numerical solution of nonlinear problems in continuum mechanics 🔧 of solid bodies 🚂. Its name is a combination of FE (finite element) and the german word Lupe 🔍 (magnifying glass) as a synonym for getting an insight 📖 how a finite element analysis code 🧮 looks like under the hood 🕳️.\n  \n    FElupe has minimal requirements, all available at PyPI supporting all platforms.\n  pip install felupe[all]\n    It's pure Python but the assembly performance is well suited for mid-sized problems. It is possible to assemble up to 130000 degrees of freedom for linear elasticity on a modern notebook in one second runtime (results may vary).\n  import felupe as fem\n\nmesh = fem.Cube(n=6)\nregion = fem.RegionHexahedron(mesh)\nfield = fem.FieldContainer([fem.Field(region, dim=3)])\n\nboundaries, loadcase = fem.dof.uniaxial(field, clamped=True)\n\numat = fem.OgdenRoxburgh(material=fem.NeoHooke(mu=1), r=3, m=1, beta=0)\nsolid = fem.SolidBodyNearlyIncompressible(umat, field, bulk=5000)\n\nmove = fem.math.linsteps([0, 1, 0, 1, 2, 1], num=5)\nstep = fem.Step(items=[solid], ramp={boundaries[\"move\"]: move}, boundaries=boundaries)\n\njob = fem.CharacteristicCurve(steps=[step], boundary=boundaries[\"move\"])\njob.evaluate(filename=\"result.xdmf\")\nfig, ax = job.plot(\n    xlabel=\"Displacement $u$ in mm $\\longrightarrow$\",\n    ylabel=\"Normal Force $F$ in N $\\longrightarrow$\",\n)\n\nsolid.plot(\"Principal Values of Cauchy Stress\").show()\n    The above minmal code-block covers the essential high-level parts of creating and solving problems with FElupe.\n  \n    There is also a Gallery of Examples.\n  \n    Please let me know what you think about it. If you like it - I'd be glad if you share it with your friends, star it on GitHub, whatever you like. Thanks!\n  \nhttps://github.com/adtzlr/felupe"
},
{
    "title": "No title",
    "content": "I created  a simple tool for extracting text from PDF, EPUB, TXT, and DOCX files.It is mainly for personal use, but I would really appreciate a feedback\n  \nhttps://github.com/KirillAn/extractText/tree/main"
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    PyBackport holds serves three purposes:\n  \n\n\n    Use instances from newer python releases in older python versions. For example, using StrEnum (new in python 3.11) in python 3.9\n  \n\n\n    Enable experimental unreleased classes. For example, define new enums where members are tuples with TupleEnum\n\n\n\n    Provide support to commonly used constants. For example, PyBackport allows to import and use colors.RGB to avoid re-defining constant color codes in every new project.\n  \n\n\nTarget Audience\n\n    The audience is developers that must create scripts for older python versions, such as new packages which must support python 3.8+. With PyBackport these instances from newer python versions can be used.\n  \nComparison\n\n    I've not encountered any similar released package with good documentation.\n  \n    I also choose not to set a release V1.0 for my package, as I backported only some enums functionality. Anyone feel free to open a new issue, contribute with new pull requests or to notify me any other useful instances to backport. I intend to keep expanding the package as much as possible.\n  \nLinks\n\nGitHub repo\n\nPyPI release\n\nDocumentation"
},
{
    "title": "No title",
    "content": "In short: here is the hacky 2-line header that makes any python script a clickable batch file(assuming bundled python in a subfolder)\n  @classmethod # 2>nul & (if not exist \"%~dp0\\python64-win\\python.exe\" (echo Fatal python64-win\\python.exe not found & pause) else (title %~f0 & \"%~dp0\\python64-win\\python.exe\" \"%~f0\" %*)) & exit /B & # noqa: E501\ndef __unused(): \"fake function to help writing header that allows executing same file as python and batch\"\n\n# Normal script starts here\n    Its usage is either double-click to run without args or \"drag a file on it\" if the script processes a file (passed as an argument). Its installation is as easy as .zip unpacking\n  \n    More detailed:\n  \n    From time to time I get in touch with small handy one-file python scripts useful for some simple but very handy automations for a some niche use case on the windows platform.\n  \n    This post is not about tasks themselves - there may be really anything, but about distributing them. Such scripts are typically has no any repo/site and just attached to a message or similar, since the author has no any plans for further development, but still wants to share.\n  \n    90% of target users are not developers, and just can use the \"double-clickable\" application. The other 10% are developers who may want to perform some script enhancements.\n  \n    Non-developers needs \"look at max 1-2 sentence documentation + several clicks to install + several to launch\". Developers need the ability to enhance the script. How to get both satisfied?\n  \n    Often such scripts are packed & bundled with a python into a single .exe - this is not editable, the other variant is distributing .py with the too large doc \"download python installer from official site, install it, then click the .py file\" and without an easy way to distribute dependency packages.\n  \n    As an author of some such scripts after several improve iterations I found a solution that satisfy both needs: distribute .zip containing bundled portable python interpreter with preinstalled dependencies in a subdirectory and a clickable file being simultaneously a .bat file and a python script - the extension is .py.bat or just .bat\n  \n    Getting portable python is a bit tricky, but embeddable official image is mostly ok."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Toschat !What My Project Does?\n    a lightweight python chat app within your terminal with simple UI and it's built on top of Textual framework. What you can do:\n  \n    - Create a new account\n  \n    - Add your friends to your contact\n  \n    - and chat!\n  Target Audience\n    It's my hobby project to improve my python knowledge but everyone can install and use it. All data store in a live server with Django framework.\n  Comparison\n    Instead of texting with your colleagues or your friends on your phone or a web browser, .. why not your terminal ? 😁\n  \n    I'm happy for advices on code improvements, features recommendations and feedbacks. 😁😁\n  \n    Github Link: https://github.com/MuongKimhong/toschat\n\n    If you find it interesting and useful, give it a star I would appreciate it very much.Have a good weekend programmers!"
},
{
    "title": "No title",
    "content": "From the Article:This provides a good overview of how Reflex works under the hood.\n  \nTLDR:Under the hood, Reflex apps compile down to a React frontend app and a FastAPI backend app. Only the UI is compiled to Javascript; all the app logic and state management stays in Python and is run on the server. Reflex uses WebSockets to send events from the frontend to the backend, and to send state updates from the backend to the frontend.\n  \nFull post: https://reflex.dev/blog/2024-03-21-reflex-architecture/#designing-a-pure-python-web-framework"
},
{
    "title": "No title",
    "content": "I shared a \"monads in python library\" a little while back on here- since then I've been toying with and expanding things out into a more general project for helping functional programming patterns in python.\n  \n    [I had (am having) a bunch of fun making this - thought I'd share here!](https://github.com/benrutter/ufo-tools)\n  \n    What my project does: A simple library with utilities for functional programming in Python.Target Audience: It's meant for use in actual production contexts, but it's still in a very early and experimental phase. It's probably most helpful right now as a learning resource.\n  \n    Comparison: Most similar project I know of is the Returns library, which is great, but a lot more heavy weight. It has more of an expectation of being used for everything, rather than a drop in as-and-when tool.\n  \n    I'd love any feedback in general (as well as any healthy flamewars around the herecy or one-true-way of functional programming)"
},
{
    "title": "No title",
    "content": "Have you noticed that Python's static type system is unsound? Look at this code:\n  from typing import overload\n\n@overload\ndef f(x: int) -> str: ...\n@overload\ndef f(x: str) -> int: ...\ndef f(x: int | str) -> int | str:\n    match x:\n        case int():\n            return 1\n        case str():\n            return 'a'\n\nprint('a' + f(1))\nprint(1 + f('a'))\n    The code above type checks, but raises TypeError when executed. With proper overloading this would never happen.\n\n    Can you think of other cases?\n  \n    You're not allowed to use cast, Any, to omit type hints, to use stubs, or to cheat in similar ways."
},
{
    "title": "No title",
    "content": "Hey guys,\n  \n    since I couldn't find any good libraries for showing modern-looking toast notifications in PyQt, I made one myself.\n  \nWhat My Project Does:\n\n    It supports showing multiple toasts at the same time, queueing of toasts, 6 different positions, multiple screens and much more. Since it's developed with QtPy, an abstraction layer for multiple versions of PyQt and PySide, you can use it with PyQt5, PyQt6, PySide2, and PySide6. Also, basically anything can be completely customized and it's extremely easy to use.\n  \nTarget Audience:\n\n    This is useful for any Python developer who is working with PyQt or PySide and wants to display clean and modern-looking toast nofifications easily.\n  \nComparison:\n\n    Since I wasn't able to find any real libraries for PyQt toasts, I can't really give a comparison.\n  \n    Preview: https://github.com/niklashenning/pyqt-toast/assets/58544929/f4d7f4a4-6d69-4087-ae19-da54b6da499d\n\n    Github: https://github.com/niklashenning/pyqt-toast\n\n    Hope this helps :)"
},
{
    "title": "No title",
    "content": "Hey folks!\n  \n    Is anyone of you attending this year PyCon Italia?\n  \n    I'm one of the organisers and I'd love to connect to new people this year 😊\n  \n    See you there! 🐍✨"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I moved from C to python and absolutely loving it.\n  \n    While looking into code of available face swappers I realized they are mostly wrapper on insightface and inswapper. So i thought I'll strip it all down to bare minimum, and the results were better than expected.\n  \n    I've captured my learnings in this video, if you're interested.\n  \nhttps://www.youtube.com/watch?v=Ju_reA3zQso"
},
{
    "title": "No title",
    "content": "Link to Repo: https://github.com/leapingio/leaping\n\nWhat My Project DoesWe’ve all been in hell when you have no idea why a test might be failing. You set a breakpoint, add print statements, and re-run the code, all to realize that you added them in the wrong spot or need to step backward.\n  \n    Leaping is a simple, fast and lightweight omniscient debugger for Python tests. Leaping traces the execution of your code and allows you to retroactively inspect the state of your program at any time, using an LLM-based debugger with natural language.\n  \n    Using Leaping, you can quickly get the answer to questions like:\n  \n\n\n    What was the value of variable x at this point?\n  \n\n\n    What was variable y set to this value?\n  \n\n\n    Why am I not hitting function x?\n  \n\n\n    What changes can I make to this test/code to make it pass?\n  \n\n\n    Here’s a brief demo of it in action: https://www.loom.com/share/94ebe34097a343c39876d7109f2a1428\n\nTarget AudienceThis is useful for any Python developer who is struggling with a test or even just an issue from production\n  \nComparisonThere’s just one alternative  https://github.com/gleb-sevruk/pycrunch-trace. It’s different because it requires you to meticulously instrument your code, is quite difficult to set up (we built this because it took us almost a day to do so!) and requires uploading your trace data to a somewhat janky web portal.\n  \nHere’s a link to the repo and we’d love it if you played around with it. We’re committed to being open-source and welcome all issues, feature requests or even contributions!"
},
{
    "title": "No title",
    "content": "So, I wanted to get a general idea about how people feel about giving return type hint of None for a function that doesn't return anything.\n  \n    With the introduction of PEP 484, type hints were introduced and we all rejoiced. Lot of my coworkers just don't get the importance of type hints and I worked way too hard to get everyone onboarded so they can see how incredibly useful it is! After some time I met a coworker who is a fan of typing and use it well... except they write -> None everywhere!\n  \n    Now this might be my personal opinion, but I hate this because it's redundant and not to mention ugly (at least to me). It is implicit and by default, functions return None in python, and I just don't see why -> None should be used. We have been arguing a lot over this since we are building a style guide for the team and I wanted to understand what the general consensus is about this. Even in PEP 484, they have mentioned that -> None should be used for __init__ functions and I just find that crazy.\n  \n    Am I in the wrong here? Is this fight pointless? What are your opinions on the matter?"
},
{
    "title": "No title",
    "content": "Hey all!Wanted to share opensource project for all interested in building interfaces based on eyetracking.\n  \n    What My Project Does: EyeGestures is python library offering e2e eyetracking with blinking detection, and gaze fixation. It provides eye-motion-controlled cursor.\n  \n    Comparison: I have not found other e2e eye tracking library with focus on building interfaces. We offer eye-motion following cursor, with built-in switchable configuration, gaze detection and blinking. Furthermore webAPI provides activation zones, and eye-magnetic buttons to ease user control.\n  \n    Target Audience: GameDev, OS, App and Frontend Engineers wanting utilize eyetracking capabilities in their apps.\n  \n    BROADER DESCRIPTION: EyeGestures is python library, desktop app, and paid web API (only webAPI is paid if you want to use our servers, nothing stops you from adding library to your backend and have it for free :) ). It is not perfect yet, but we work towards making it smoother and easier experience for both users and developers.\n  \n    The idea of the project is to bring inexpensive eyetracking to everyone having device armed with native webcam or phone cam. Most of OSs support eye-tracking but only when having additional quite pricey hardware, we want to change it with opensource!\n  \n    Bare with us, it is still early stage project and team (basically me, but having some help from time to time) is working on bringing more documentation and making tracker better.\n  \n    For now we have cursor following motion of eye, calibration for it, blinking detection and gaze fixation detection.\n  \n    Feel free to check our [repo](https://github.com/NativeSensors/EyeGestures/tree/main), and [main_demo1](https://eyegestures.com/) [demo2](https://eyegestures.com/game) [demo3](https://eyegestures.com/cinema):\n  \n    You can contact us/me under: contact@eyegestures.com"
},
{
    "title": "No title",
    "content": "Packt has published \"Python Real-World Projects\"\n  \n\n\n    As part of our marketing activities, we are offering free digital copies of the book in return for unbiased feedback in the form of a reader review.\n  \n    Here is what you will learn from the book:\n  \n\n\n    Explore core deliverables for an application including documentation and test cases\n  \n\n\n    Discover approaches to data acquisition such as file processing, RESTful APIs, and SQL queries\n  \n\n\n    Create a data inspection notebook to establish properties of source data\n  \n\n\n    Write applications to validate, clean, convert, and normalize source data\n  \n\n\n    Use foundational graphical analysis techniques to visualize data\n  \n\n\n    Build basic univariate and multivariate statistical analysis tools\n  \n\n\n    Create reports from raw data using JupyterLab publication tools\n  \n\n\n    If you feel you might be interested in this opportunity please comment below on or before 31st March 2024\n  \n    Amazon Link"
},
{
    "title": "No title",
    "content": "If your NumPy-based code is too slow, you can sometimes use Numba to speed it up. Numba is a compiled language that uses the same syntax as Python, and it compiles at runtime, so it’s very easy to write. And because it re-implements a large part of the NumPy APIs, it can also easily be used with existing NumPy-based code.However, Numba’s NumPy support can be a trap: it can lead you to missing huge optimization opportunities by sticking to NumPy-style code. In this article I show examples of:\n  \n\n\n    The wrong way to use Numba, writing NumPy-style full array transforms.\n  \n\n\n    The right way to use Numba, namely for loops."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Disclaimer: Not my project but a very interesting one I encountered recently\n\n    UXsim is a free, open-source traffic flow simulator designed for large-scale vehicular transportation simulations, developed purely in Python. Whether you're a researcher, student, or enthusiast in transportation, UXsim offers a comprehensive toolkit for macroscopic and mesoscopic traffic flow simulations.\n  \nWhat My Project Does\n\n    UXsim stands out by providing an easy-to-use Python implementation of standard models for dynamic network traffic flow. It's capable of simulating traffic dynamics in large networks, incorporating elements such as traffic signals, inflow control, route guidance, and congestion pricing. The simulator supports dynamic traffic assignments and comes with features for analyzing and visualizing simulation results, making it highly useful for scientific and educational purposes.\n  \nTarget Audience\n\n    This project is aimed at both scientific and educational communities. While UXsim's primary goal is to support academic research and teaching in transportation and traffic flow modeling, its open-source nature and Python implementation make it accessible for anyone interested in traffic simulation. It is not just a toy project but a robust tool that can be used for serious research and educational purposes, potentially even in production scenarios with further customization.\n  \nComparison\n\n    Compared to existing alternatives, UXsim differentiates itself in several key ways:\n  \n\n\nPure Python Implementation: This makes UXsim easily integratable with other Python-based frameworks, such as PyTorch for deep reinforcement learning in traffic control. It offers a high degree of customization and flexibility not always available in other simulators.\n  \n\n\nComprehensive Modeling and Visualization: UXsim combines Newell's simplified car-following model, Lagrangian Incremental Node Model, and a Dynamic User Optimum-type Route Choice Model. This comprehensive approach, coupled with its visualization capabilities, provides a deep understanding of traffic flow dynamics.\n  \n\n\nEducational Value: Thanks to its simplicity and extensive documentation, UXsim serves as an excellent educational tool, helping students and newcomers understand the complexities of traffic flow and management.\n  \n\n\n    UXsim has already shown its capabilities through examples like large-scale city simulations and deep reinforcement learning for traffic signal control. For anyone looking to dive into the details of traffic flow simulation or seeking a platform for transportation research, UXsim is a versatile and powerful tool.\n  \nLinks\n\n\n\n    GitHub: UXsim Repository\n\n\n\n    Docs: https://toruseo.jp/UXsim/docs/\n\n\n\nImages\n\n\n\nhttps://github.com/toruseo/UXsim/blob/images/gridnetwork_macro.gif\n\n\n\nhttps://github.com/toruseo/UXsim/blob/images/gridnetwork_fancy.gif\n\n\n\nhttps://github.com/toruseo/UXsim/blob/images/tsd_traj_links_grid.png\n\n\n\nhttps://github.com/toruseo/UXsim/blob/images/anim_network1_0.22_nocontrol.gif"
},
{
    "title": "No title",
    "content": "Docs here: https://ahuang11.github.io/streamjoy/\n\n    Repo here: https://github.com/ahuang11/streamjoy/tree/main\n\nWhat My Project Does\n\n    Streamjoy turns your images into animations using sensible defaults for fun, hassle-free creation. It cuts down the boilerplate and time to work on animations, and it's simple to start with just a few lines of code.\n  \nTarget Audience\n\n    For scientists or hobbyists that want to see something in motion and analyze it! It's still in beta, but would love feedback!\n  \nComparison\n\n    Unlike other libraries, this runs in parallel, and I really like how it can add intros and pauses by specifying keywords!\n  \n    I also like how simple it is to use with pandas and xarray--just invoke the streamjoy method after import.\n  \nExample\nimport xarray as xr\nimport streamjoy.xarray\n\nds = xr.tutorial.open_dataset(\"air_temperature\") \n\nds.streamjoy(\"air_temperature.mp4\")\n    StreamJoy also supports custom renderers.\n  import matplotlib.pyplot as plt\nimport numpy as np\nfrom streamjoy import stream, wrap_matplotlib\n\n@wrap_matplotlib()\ndef plot_frame(i):\n    x = np.linspace(0, 2, 1000)\n    y = np.sin(2 * np.pi * (x - 0.01 * i))\n    fig, ax = plt.subplots() ax.plot(x, y)\n    return fig\n\nif __name__ == \"__main__\":\n    stream(list(range(10)), uri=\"sine_wave.gif\", renderer=plot_frame)\n\n    Install it with just pip to start: pip install streamjoy"
},
{
    "title": "No title",
    "content": "Anyone going to pyconf from Philly?Looking to carpool and possibly get a Airbnb near the spot!lmk if anyone wants to join."
},
{
    "title": "No title",
    "content": "PSF Official"
},
{
    "title": "No title",
    "content": "Hi all!\n  \n    I've had an idea for a while around creating a library that would allow you to quickly build new API clients that are fully typed. Last week I finally got around to building it and am keen to see what everyone thinks.\n  \nWhat My Project Does\n\n    A library for creating fully typed declarative API clients quickly and easily. So you could build an API integration with other services very quickly, and in a declarative way.\n  \n    And it would ensure that your request/query params are valid/correct before sending them (avoiding potential errors). As well as making the API response easy to work with, matching your own models rather than trying to work with JSON blobs.\n  \nTarget Audience\n\n    It's still in early development but should work fine for a lot of use cases. I'm keen to hear if people find something like this useful and whether it works for your use cases or not so that I can continue to expand it.\n  \nComparison\n\n    There's a few libraries that allow you to create API clients (and even some that auto generate API clients based on an OpenAPI spec). But their interfaces are (IMO) a bit more clunky and most don't seem to support serialization/deserialization or typing very well.\n  \nWhat would an API client with this library look like?\n\n    Glad you asked. Currently, it would look something like this (For a single API endpoint over HTTP GET):\n  import attrs\nimport quickapi\n\n\n# An example type that will be part of the API response\n@attrs.define\nclass Fact:\n    fact: str\n    length: int\n\n# What the API response should look like\n@attrs.define\nclass ResponseBody(quickapi.BaseResponseBody):\n    current_page: int\n    data: list[Fact]\n\n# Now we can define our API\nclass MyApi(quickapi.BaseApi[ResponseBody]):\n    url = \"https://catfact.ninja/facts\"\n    response_body = ResponseBody\n    And you would use it like this:\n  api_client = MyApi()\nresponse = api_client.execute()\n\n# That's it! Now response is fully typed and conforms to our ResponseBody definition\nassert isinstance(response.body, ResponseBody)\nassert isinstance(response.body.data[0], Fact)\n    But my goal is to eventually be able to define it all in a single class and allow using different serialization/deserialization libraries.\n  \n    So it could end up looking something closer to this (For a single endpoint over HTTP POST):\n  import quickapi\n\n@quickapi.api\nclass MyApi:\n    url = \"https://catfact.ninja/facts\"\n    method = quickapi.ApiMethod.POST\n\n    class RequestBody:\n        required_input: str\n        optional_input: str | None = None    \n\n    class ResponseBody:\n        current_page: int\n        data: list[Fact]\n    Github: https://github.com/martinn/quickapiclient\n\n    What do you guys think?"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "For years my training students have been asking me for a list of all the dunder methods. The Python docs don't have such a list, so I compiled my own... after having on my to-do list for years.\n  \nEvery dunder method in Python\n\n    I realized why it took me so long during when I finally finished compiling the table of all of them... there are over 100 dunder methods in Python! 💯\n  \n    Edit: I should have said \"the Python docs don't have such a list in a single row-by-row table\". The Data Model page does indeed include a giant \"Special Names\" section and a \"Coroutines\" section which document nearly every special method, but it's quite challenging to skim and not *quite* complete."
},
{
    "title": "No title",
    "content": "Hope that sharing this is allowed here, let me know otherwise.I just published a new post on our engineering blog on why and how we’re using Python at Picnic (an online supermarket in the EU). I think it might be interesting for this sub as well.\n  \nhttps://picnic.app/careers/python-picnic"
},
{
    "title": "No title",
    "content": "Hey all, I'm excited to announce that EOmaps v8.0 is now released and that it has been accepted into the PyOpenSci ecosystem!\n  \nWhat is it?\n\n    EOmaps is a pyhton package to visualize and analyze geographical datasets. It's 100% free and open-source! You can find all information you need to install (or contribute 🚀) in the documentation and on  GitHub!\n  \n    Check it out and let me know what you think!\n  \n\nEOmaps v8.0 brings a lot of updates, fixes and improvements like:\n  \n\n\n    A switch to BSD-3-Clause License\n  \n\n\n    A much improved documentation (new API docs, contribution guide etc.)\n  \n\n\n    Better management of optional dependencies with pip install\n\n\n\n    Pre-configured Jupyter Widgets for Jupyter Notebooks\n  \n\n\n    ... and much more!"
},
{
    "title": "No title",
    "content": "Hello! It has been a while since I have made a post about Stockstir, the tool to gather stock information from any script at no cost, written in Python. After a couple months from my last post, I have updated and refined bugs and other issues, as well as added new functions for enhanced usability. The latest version is now 2.1.4.\n  \nWhat My Project Does\n\n    Stockstir is an easy way to instantly gather stock data from any of your Python scripts. Not only that, but it includes other features, such as multi data gathering, anti ban, a fail-safe mechanism, random user agents, and much more.\n  \nTarget Audience\n\n    Stockstir is for everyone that needs to gather realtime company stock info from any of their scripts. It mostly differs from any other stock related project in the way that it is simple, and doesn't rely on apis.\n  \nComparison\n\n    Stockstir differs from other methods of gathering stock data in that it is has a very simple concept behind it. It is largely a GET wrapper in the Tools class, but initial API support such as Alpha Vantage, as well as gathering much more data of a Company stock through cnbc's JSON api, under the API class. It is mostly a quick way to gather stock data through simple use.\n  \n    You can find installation instructions and other information under the project link provided below:\n  \n    Link: Stockstir Project Link\n\n    To those of you that are new to Stockstir, V2 added a plethora of new features including a fail-safe mechanism (more on that further down), and V2.1.0 reconstructed the entire codebase to further match PEP guidelines upon the request and suggestion from many of you.\n  \n    To see the latest Changelog information, visit the CHANGELOG.md file located in the project files hosted on Github.\n  \n    Here are a few examples of the different usages of Stockstir:\n  Quick Usage:\n    To easily gather a single price of a company's stock, you can do it in one line.\n  from stockstir import Stockstir\nprice = Stockstir().tools.get_single_price(\"ticker/stockSymbol\")\nprint(price)\n    The above Stockstir method get_single_price is one of the most basic of the functions provided.\n  Stockstir Object Instantiation\n    Since update V2, you can instantiate Stockstir as an object, and customize certain parameters:\n  from stockstir import Stockstir\ns = Stockstir() # Instantiate the Stockstir object, like so.\n# We can also create a new Stockstir object, if for example you need certain options toggled:\ns2 = Stockstir(print_output=True, random_user_agent=True, provider='cnbc')Stockstir Functionality, the Fail-Safe mechanism, and Providers:\n    I am not going to cover the entirety of Stockstir functionality here, which is why Stockstir has a readthedocs.io documentation:\n  \nStockstir Documentation\n\n    However, basic Stockstir functionality can be described as a GET wrapper. It has providers, or, in other words, a website, and a regex pattern to find the price based the request made. Providers are a large part of Stockstir. The fail-safe mechanism chooses a new provider that works, in case it fails. A provider fails if the provider has changed their code base and the regex can't find anything, or, if the website the request is made to has changed or is down. To counter provider failure in case a prpovider fails, automated checks on my side run five times a day. If one of them doesn't work, I get notified within a short period of time.\n  \n    You can choose between 'cnbc', 'insiders', or 'zacks' for the providers. 'cnbc' is the default. To view working providers, you can do so like this:\n  from stockstir import Stockstir\ns = Stockstir(provider='cnbc') #You can set the provider via the provider option in the Stockstir instantiation. Default will always be cnbc.\ns.providers.list_available_providers() # list the available providers.Many Thanks\n    Thank you for trying out Stockstir, or even just looking into trying it!"
},
{
    "title": "No title",
    "content": "I wanted to share the talks from last month’s Dask Demo Day, where folks from the Dask community give short demos to show off ongoing work. Hopefully this helps elevate some of the great work people are doing.Last month’s talks:- One trillion row challenge- Deploy Dask on Databricks with dask-databricks- Deploy Prefect workflows on the cloud with Coiled- Scale embedding pipelines (LlamaIndex + Dask)- Use AWS Cost Explorer to see the cost of public IPv4 addressesRecording on YouTube: https://www.youtube.com/watch?v=07e1JL83ur8Join the next one this Thursday, March 21st, 11am ET https://github.com/dask/community/issues/307"
},
{
    "title": "No title",
    "content": "I built this after having conceptually designed it in an interview but failed to get the job. Figured I might as well build it. Any thoughts or advice welcome!\n  \nHow My Project Works\n\n    This progress bar connects with your Slack Bot account via tokens to send you a message as something is loading. It updates the progress bar as called by the user to edit the message on Slack, always keeping you up to date on how progress is going (ex. Used in an application that's loading something, running tests, etc.)\n  \nTarget Audience\n\n    This can be used in a professional setting where you want to notify employees of certain progress. Also can be used for personal projects and / or design teams in universities that use Slack.\n  \nComparison\n\nslack-progress: This project exists but has not been supported for a long time, as it's an old version of Python and the slacker api has been deprecated.\n  \n    Github: https://github.com/mlizzi/slack-progress-bar\n\n    pypi: https://pypi.org/project/slack-progress-bar/"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Created an article and a video that will help beginners to NiceGUI add multiple pages to NiceGUI to better structure the app.https://www.bitdoze.com/nicegui-pages/"
},
{
    "title": "No title",
    "content": "Have you noticed that Python's static type system is unsound? Look at this code:\n  from typing import overload\n\n@overload\ndef f(x: int) -> str: ...\n@overload\ndef f(x: str) -> int: ...\ndef f(x: int | str) -> int | str:\n    match x:\n        case int():\n            return 1\n        case str():\n            return 'a'\n\nprint('a' + f(1))\nprint(1 + f('a'))\n    The code above type checks, but raises TypeError when executed. With proper overloading this would never happen.\n\n    Can you think of other cases?\n  \n    You're not allowed to use cast, Any, to omit type hints, to use stubs, or to cheat in similar ways."
},
{
    "title": "No title",
    "content": "Hey guys,\n  \n    since I couldn't find any good libraries for showing modern-looking toast notifications in PyQt, I made one myself.\n  \nWhat My Project Does:\n\n    It supports showing multiple toasts at the same time, queueing of toasts, 6 different positions, multiple screens and much more. Since it's developed with QtPy, an abstraction layer for multiple versions of PyQt and PySide, you can use it with PyQt5, PyQt6, PySide2, and PySide6. Also, basically anything can be completely customized and it's extremely easy to use.\n  \nTarget Audience:\n\n    This is useful for any Python developer who is working with PyQt or PySide and wants to display clean and modern-looking toast nofifications easily.\n  \nComparison:\n\n    Since I wasn't able to find any real libraries for PyQt toasts, I can't really give a comparison.\n  \n    Preview: https://github.com/niklashenning/pyqt-toast/assets/58544929/f4d7f4a4-6d69-4087-ae19-da54b6da499d\n\n    Github: https://github.com/niklashenning/pyqt-toast\n\n    Hope this helps :)"
},
{
    "title": "No title",
    "content": "Hey folks!\n  \n    Is anyone of you attending this year PyCon Italia?\n  \n    I'm one of the organisers and I'd love to connect to new people this year 😊\n  \n    See you there! 🐍✨"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I moved from C to python and absolutely loving it.\n  \n    While looking into code of available face swappers I realized they are mostly wrapper on insightface and inswapper. So i thought I'll strip it all down to bare minimum, and the results were better than expected.\n  \n    I've captured my learnings in this video, if you're interested.\n  \nhttps://www.youtube.com/watch?v=Ju_reA3zQso"
},
{
    "title": "No title",
    "content": "Link to Repo: https://github.com/leapingio/leaping\n\nWhat My Project DoesWe’ve all been in hell when you have no idea why a test might be failing. You set a breakpoint, add print statements, and re-run the code, all to realize that you added them in the wrong spot or need to step backward.\n  \n    Leaping is a simple, fast and lightweight omniscient debugger for Python tests. Leaping traces the execution of your code and allows you to retroactively inspect the state of your program at any time, using an LLM-based debugger with natural language.\n  \n    Using Leaping, you can quickly get the answer to questions like:\n  \n\n\n    What was the value of variable x at this point?\n  \n\n\n    What was variable y set to this value?\n  \n\n\n    Why am I not hitting function x?\n  \n\n\n    What changes can I make to this test/code to make it pass?\n  \n\n\n    Here’s a brief demo of it in action: https://www.loom.com/share/94ebe34097a343c39876d7109f2a1428\n\nTarget AudienceThis is useful for any Python developer who is struggling with a test or even just an issue from production\n  \nComparisonThere’s just one alternative  https://github.com/gleb-sevruk/pycrunch-trace. It’s different because it requires you to meticulously instrument your code, is quite difficult to set up (we built this because it took us almost a day to do so!) and requires uploading your trace data to a somewhat janky web portal.\n  \nHere’s a link to the repo and we’d love it if you played around with it. We’re committed to being open-source and welcome all issues, feature requests or even contributions!"
},
{
    "title": "No title",
    "content": "So, I wanted to get a general idea about how people feel about giving return type hint of None for a function that doesn't return anything.\n  \n    With the introduction of PEP 484, type hints were introduced and we all rejoiced. Lot of my coworkers just don't get the importance of type hints and I worked way too hard to get everyone onboarded so they can see how incredibly useful it is! After some time I met a coworker who is a fan of typing and use it well... except they write -> None everywhere!\n  \n    Now this might be my personal opinion, but I hate this because it's redundant and not to mention ugly (at least to me). It is implicit and by default, functions return None in python, and I just don't see why -> None should be used. We have been arguing a lot over this since we are building a style guide for the team and I wanted to understand what the general consensus is about this. Even in PEP 484, they have mentioned that -> None should be used for __init__ functions and I just find that crazy.\n  \n    Am I in the wrong here? Is this fight pointless? What are your opinions on the matter?"
},
{
    "title": "No title",
    "content": "Hey all!Wanted to share opensource project for all interested in building interfaces based on eyetracking.\n  \n    What My Project Does: EyeGestures is python library offering e2e eyetracking with blinking detection, and gaze fixation. It provides eye-motion-controlled cursor.\n  \n    Comparison: I have not found other e2e eye tracking library with focus on building interfaces. We offer eye-motion following cursor, with built-in switchable configuration, gaze detection and blinking. Furthermore webAPI provides activation zones, and eye-magnetic buttons to ease user control.\n  \n    Target Audience: GameDev, OS, App and Frontend Engineers wanting utilize eyetracking capabilities in their apps.\n  \n    BROADER DESCRIPTION: EyeGestures is python library, desktop app, and paid web API (only webAPI is paid if you want to use our servers, nothing stops you from adding library to your backend and have it for free :) ). It is not perfect yet, but we work towards making it smoother and easier experience for both users and developers.\n  \n    The idea of the project is to bring inexpensive eyetracking to everyone having device armed with native webcam or phone cam. Most of OSs support eye-tracking but only when having additional quite pricey hardware, we want to change it with opensource!\n  \n    Bare with us, it is still early stage project and team (basically me, but having some help from time to time) is working on bringing more documentation and making tracker better.\n  \n    For now we have cursor following motion of eye, calibration for it, blinking detection and gaze fixation detection.\n  \n    Feel free to check our [repo](https://github.com/NativeSensors/EyeGestures/tree/main), and [main_demo1](https://eyegestures.com/) [demo2](https://eyegestures.com/game) [demo3](https://eyegestures.com/cinema):\n  \n    You can contact us/me under: contact@eyegestures.com"
},
{
    "title": "No title",
    "content": "Packt has published \"Python Real-World Projects\"\n  \n\n\n    As part of our marketing activities, we are offering free digital copies of the book in return for unbiased feedback in the form of a reader review.\n  \n    Here is what you will learn from the book:\n  \n\n\n    Explore core deliverables for an application including documentation and test cases\n  \n\n\n    Discover approaches to data acquisition such as file processing, RESTful APIs, and SQL queries\n  \n\n\n    Create a data inspection notebook to establish properties of source data\n  \n\n\n    Write applications to validate, clean, convert, and normalize source data\n  \n\n\n    Use foundational graphical analysis techniques to visualize data\n  \n\n\n    Build basic univariate and multivariate statistical analysis tools\n  \n\n\n    Create reports from raw data using JupyterLab publication tools\n  \n\n\n    If you feel you might be interested in this opportunity please comment below on or before 31st March 2024\n  \n    Amazon Link"
},
{
    "title": "No title",
    "content": "If your NumPy-based code is too slow, you can sometimes use Numba to speed it up. Numba is a compiled language that uses the same syntax as Python, and it compiles at runtime, so it’s very easy to write. And because it re-implements a large part of the NumPy APIs, it can also easily be used with existing NumPy-based code.However, Numba’s NumPy support can be a trap: it can lead you to missing huge optimization opportunities by sticking to NumPy-style code. In this article I show examples of:\n  \n\n\n    The wrong way to use Numba, writing NumPy-style full array transforms.\n  \n\n\n    The right way to use Numba, namely for loops."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Disclaimer: Not my project but a very interesting one I encountered recently\n\n    UXsim is a free, open-source traffic flow simulator designed for large-scale vehicular transportation simulations, developed purely in Python. Whether you're a researcher, student, or enthusiast in transportation, UXsim offers a comprehensive toolkit for macroscopic and mesoscopic traffic flow simulations.\n  \nWhat My Project Does\n\n    UXsim stands out by providing an easy-to-use Python implementation of standard models for dynamic network traffic flow. It's capable of simulating traffic dynamics in large networks, incorporating elements such as traffic signals, inflow control, route guidance, and congestion pricing. The simulator supports dynamic traffic assignments and comes with features for analyzing and visualizing simulation results, making it highly useful for scientific and educational purposes.\n  \nTarget Audience\n\n    This project is aimed at both scientific and educational communities. While UXsim's primary goal is to support academic research and teaching in transportation and traffic flow modeling, its open-source nature and Python implementation make it accessible for anyone interested in traffic simulation. It is not just a toy project but a robust tool that can be used for serious research and educational purposes, potentially even in production scenarios with further customization.\n  \nComparison\n\n    Compared to existing alternatives, UXsim differentiates itself in several key ways:\n  \n\n\nPure Python Implementation: This makes UXsim easily integratable with other Python-based frameworks, such as PyTorch for deep reinforcement learning in traffic control. It offers a high degree of customization and flexibility not always available in other simulators.\n  \n\n\nComprehensive Modeling and Visualization: UXsim combines Newell's simplified car-following model, Lagrangian Incremental Node Model, and a Dynamic User Optimum-type Route Choice Model. This comprehensive approach, coupled with its visualization capabilities, provides a deep understanding of traffic flow dynamics.\n  \n\n\nEducational Value: Thanks to its simplicity and extensive documentation, UXsim serves as an excellent educational tool, helping students and newcomers understand the complexities of traffic flow and management.\n  \n\n\n    UXsim has already shown its capabilities through examples like large-scale city simulations and deep reinforcement learning for traffic signal control. For anyone looking to dive into the details of traffic flow simulation or seeking a platform for transportation research, UXsim is a versatile and powerful tool.\n  \nLinks\n\n\n\n    GitHub: UXsim Repository\n\n\n\n    Docs: https://toruseo.jp/UXsim/docs/\n\n\n\nImages\n\n\n\nhttps://github.com/toruseo/UXsim/blob/images/gridnetwork_macro.gif\n\n\n\nhttps://github.com/toruseo/UXsim/blob/images/gridnetwork_fancy.gif\n\n\n\nhttps://github.com/toruseo/UXsim/blob/images/tsd_traj_links_grid.png\n\n\n\nhttps://github.com/toruseo/UXsim/blob/images/anim_network1_0.22_nocontrol.gif"
},
{
    "title": "No title",
    "content": "Docs here: https://ahuang11.github.io/streamjoy/\n\n    Repo here: https://github.com/ahuang11/streamjoy/tree/main\n\nWhat My Project Does\n\n    Streamjoy turns your images into animations using sensible defaults for fun, hassle-free creation. It cuts down the boilerplate and time to work on animations, and it's simple to start with just a few lines of code.\n  \nTarget Audience\n\n    For scientists or hobbyists that want to see something in motion and analyze it! It's still in beta, but would love feedback!\n  \nComparison\n\n    Unlike other libraries, this runs in parallel, and I really like how it can add intros and pauses by specifying keywords!\n  \n    I also like how simple it is to use with pandas and xarray--just invoke the streamjoy method after import.\n  \nExample\nimport xarray as xr\nimport streamjoy.xarray\n\nds = xr.tutorial.open_dataset(\"air_temperature\") \n\nds.streamjoy(\"air_temperature.mp4\")\n    StreamJoy also supports custom renderers.\n  import matplotlib.pyplot as plt\nimport numpy as np\nfrom streamjoy import stream, wrap_matplotlib\n\n@wrap_matplotlib()\ndef plot_frame(i):\n    x = np.linspace(0, 2, 1000)\n    y = np.sin(2 * np.pi * (x - 0.01 * i))\n    fig, ax = plt.subplots() ax.plot(x, y)\n    return fig\n\nif __name__ == \"__main__\":\n    stream(list(range(10)), uri=\"sine_wave.gif\", renderer=plot_frame)\n\n    Install it with just pip to start: pip install streamjoy"
},
{
    "title": "No title",
    "content": "Anyone going to pyconf from Philly?Looking to carpool and possibly get a Airbnb near the spot!lmk if anyone wants to join."
},
{
    "title": "No title",
    "content": "PSF Official"
},
{
    "title": "No title",
    "content": "Hi all!\n  \n    I've had an idea for a while around creating a library that would allow you to quickly build new API clients that are fully typed. Last week I finally got around to building it and am keen to see what everyone thinks.\n  \nWhat My Project Does\n\n    A library for creating fully typed declarative API clients quickly and easily. So you could build an API integration with other services very quickly, and in a declarative way.\n  \n    And it would ensure that your request/query params are valid/correct before sending them (avoiding potential errors). As well as making the API response easy to work with, matching your own models rather than trying to work with JSON blobs.\n  \nTarget Audience\n\n    It's still in early development but should work fine for a lot of use cases. I'm keen to hear if people find something like this useful and whether it works for your use cases or not so that I can continue to expand it.\n  \nComparison\n\n    There's a few libraries that allow you to create API clients (and even some that auto generate API clients based on an OpenAPI spec). But their interfaces are (IMO) a bit more clunky and most don't seem to support serialization/deserialization or typing very well.\n  \nWhat would an API client with this library look like?\n\n    Glad you asked. Currently, it would look something like this (For a single API endpoint over HTTP GET):\n  import attrs\nimport quickapi\n\n\n# An example type that will be part of the API response\n@attrs.define\nclass Fact:\n    fact: str\n    length: int\n\n# What the API response should look like\n@attrs.define\nclass ResponseBody(quickapi.BaseResponseBody):\n    current_page: int\n    data: list[Fact]\n\n# Now we can define our API\nclass MyApi(quickapi.BaseApi[ResponseBody]):\n    url = \"https://catfact.ninja/facts\"\n    response_body = ResponseBody\n    And you would use it like this:\n  api_client = MyApi()\nresponse = api_client.execute()\n\n# That's it! Now response is fully typed and conforms to our ResponseBody definition\nassert isinstance(response.body, ResponseBody)\nassert isinstance(response.body.data[0], Fact)\n    But my goal is to eventually be able to define it all in a single class and allow using different serialization/deserialization libraries.\n  \n    So it could end up looking something closer to this (For a single endpoint over HTTP POST):\n  import quickapi\n\n@quickapi.api\nclass MyApi:\n    url = \"https://catfact.ninja/facts\"\n    method = quickapi.ApiMethod.POST\n\n    class RequestBody:\n        required_input: str\n        optional_input: str | None = None    \n\n    class ResponseBody:\n        current_page: int\n        data: list[Fact]\n    Github: https://github.com/martinn/quickapiclient\n\n    What do you guys think?"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "For years my training students have been asking me for a list of all the dunder methods. The Python docs don't have such a list, so I compiled my own... after having on my to-do list for years.\n  \nEvery dunder method in Python\n\n    I realized why it took me so long during when I finally finished compiling the table of all of them... there are over 100 dunder methods in Python! 💯\n  \n    Edit: I should have said \"the Python docs don't have such a list in a single row-by-row table\". The Data Model page does indeed include a giant \"Special Names\" section and a \"Coroutines\" section which document nearly every special method, but it's quite challenging to skim and not *quite* complete."
},
{
    "title": "No title",
    "content": "Hope that sharing this is allowed here, let me know otherwise.I just published a new post on our engineering blog on why and how we’re using Python at Picnic (an online supermarket in the EU). I think it might be interesting for this sub as well.\n  \nhttps://picnic.app/careers/python-picnic"
},
{
    "title": "No title",
    "content": "Hey all, I'm excited to announce that EOmaps v8.0 is now released and that it has been accepted into the PyOpenSci ecosystem!\n  \nWhat is it?\n\n    EOmaps is a pyhton package to visualize and analyze geographical datasets. It's 100% free and open-source! You can find all information you need to install (or contribute 🚀) in the documentation and on  GitHub!\n  \n    Check it out and let me know what you think!\n  \n\nEOmaps v8.0 brings a lot of updates, fixes and improvements like:\n  \n\n\n    A switch to BSD-3-Clause License\n  \n\n\n    A much improved documentation (new API docs, contribution guide etc.)\n  \n\n\n    Better management of optional dependencies with pip install\n\n\n\n    Pre-configured Jupyter Widgets for Jupyter Notebooks\n  \n\n\n    ... and much more!"
},
{
    "title": "No title",
    "content": "Hello! It has been a while since I have made a post about Stockstir, the tool to gather stock information from any script at no cost, written in Python. After a couple months from my last post, I have updated and refined bugs and other issues, as well as added new functions for enhanced usability. The latest version is now 2.1.4.\n  \nWhat My Project Does\n\n    Stockstir is an easy way to instantly gather stock data from any of your Python scripts. Not only that, but it includes other features, such as multi data gathering, anti ban, a fail-safe mechanism, random user agents, and much more.\n  \nTarget Audience\n\n    Stockstir is for everyone that needs to gather realtime company stock info from any of their scripts. It mostly differs from any other stock related project in the way that it is simple, and doesn't rely on apis.\n  \nComparison\n\n    Stockstir differs from other methods of gathering stock data in that it is has a very simple concept behind it. It is largely a GET wrapper in the Tools class, but initial API support such as Alpha Vantage, as well as gathering much more data of a Company stock through cnbc's JSON api, under the API class. It is mostly a quick way to gather stock data through simple use.\n  \n    You can find installation instructions and other information under the project link provided below:\n  \n    Link: Stockstir Project Link\n\n    To those of you that are new to Stockstir, V2 added a plethora of new features including a fail-safe mechanism (more on that further down), and V2.1.0 reconstructed the entire codebase to further match PEP guidelines upon the request and suggestion from many of you.\n  \n    To see the latest Changelog information, visit the CHANGELOG.md file located in the project files hosted on Github.\n  \n    Here are a few examples of the different usages of Stockstir:\n  Quick Usage:\n    To easily gather a single price of a company's stock, you can do it in one line.\n  from stockstir import Stockstir\nprice = Stockstir().tools.get_single_price(\"ticker/stockSymbol\")\nprint(price)\n    The above Stockstir method get_single_price is one of the most basic of the functions provided.\n  Stockstir Object Instantiation\n    Since update V2, you can instantiate Stockstir as an object, and customize certain parameters:\n  from stockstir import Stockstir\ns = Stockstir() # Instantiate the Stockstir object, like so.\n# We can also create a new Stockstir object, if for example you need certain options toggled:\ns2 = Stockstir(print_output=True, random_user_agent=True, provider='cnbc')Stockstir Functionality, the Fail-Safe mechanism, and Providers:\n    I am not going to cover the entirety of Stockstir functionality here, which is why Stockstir has a readthedocs.io documentation:\n  \nStockstir Documentation\n\n    However, basic Stockstir functionality can be described as a GET wrapper. It has providers, or, in other words, a website, and a regex pattern to find the price based the request made. Providers are a large part of Stockstir. The fail-safe mechanism chooses a new provider that works, in case it fails. A provider fails if the provider has changed their code base and the regex can't find anything, or, if the website the request is made to has changed or is down. To counter provider failure in case a prpovider fails, automated checks on my side run five times a day. If one of them doesn't work, I get notified within a short period of time.\n  \n    You can choose between 'cnbc', 'insiders', or 'zacks' for the providers. 'cnbc' is the default. To view working providers, you can do so like this:\n  from stockstir import Stockstir\ns = Stockstir(provider='cnbc') #You can set the provider via the provider option in the Stockstir instantiation. Default will always be cnbc.\ns.providers.list_available_providers() # list the available providers.Many Thanks\n    Thank you for trying out Stockstir, or even just looking into trying it!"
},
{
    "title": "No title",
    "content": "I wanted to share the talks from last month’s Dask Demo Day, where folks from the Dask community give short demos to show off ongoing work. Hopefully this helps elevate some of the great work people are doing.Last month’s talks:- One trillion row challenge- Deploy Dask on Databricks with dask-databricks- Deploy Prefect workflows on the cloud with Coiled- Scale embedding pipelines (LlamaIndex + Dask)- Use AWS Cost Explorer to see the cost of public IPv4 addressesRecording on YouTube: https://www.youtube.com/watch?v=07e1JL83ur8Join the next one this Thursday, March 21st, 11am ET https://github.com/dask/community/issues/307"
},
{
    "title": "No title",
    "content": "I built this after having conceptually designed it in an interview but failed to get the job. Figured I might as well build it. Any thoughts or advice welcome!\n  \nHow My Project Works\n\n    This progress bar connects with your Slack Bot account via tokens to send you a message as something is loading. It updates the progress bar as called by the user to edit the message on Slack, always keeping you up to date on how progress is going (ex. Used in an application that's loading something, running tests, etc.)\n  \nTarget Audience\n\n    This can be used in a professional setting where you want to notify employees of certain progress. Also can be used for personal projects and / or design teams in universities that use Slack.\n  \nComparison\n\nslack-progress: This project exists but has not been supported for a long time, as it's an old version of Python and the slacker api has been deprecated.\n  \n    Github: https://github.com/mlizzi/slack-progress-bar\n\n    pypi: https://pypi.org/project/slack-progress-bar/"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Created an article and a video that will help beginners to NiceGUI add multiple pages to NiceGUI to better structure the app.https://www.bitdoze.com/nicegui-pages/"
},
{
    "title": "No title",
    "content": "I'll disclaim: I'm not a programmer but a litterateur, my code may be quite bad and in some (some!) places written with LLMs - but it works as should be and its simplicity makes me admire Python.\n  // The beginning of lyric digression\n    For the last 7 years I've been writing scripts in bash. I've accumulated quite a lot of them, I would say that it became a kind of escapism for me, in anxious moments of my life, when I was too lazy to do something on my own, I prefer to spend half a night writing a script, so that I could press one button and - bang - everything would work by itself. At one point, Bash became not enough. Despite the fact that it became convenient and fast to write in it, in fact, it works slowly enough to create some serious projects. And at one point, my friend encouraged me to use Python, luckily he knows Python and I had an idea that came to life. So now I use my Python program literally every day.\n  // End of lyric digression\n    I watch movies and shows often (don't blame me for piracy, of course it's Big Buck Bunny every time) and since I don't like to click a lot of buttons, I wrote a 500 line script (hopefully someone will recognize themselves in this :D).\n  \nhttps://github.com/asakura42/btstrm // https://pypi.org/project/btstrm/\n\n\nWhat My Project Does\n\n    This program can:\n  \n\n\n    Search for movies using TMDB in a given language and show the list of found movies along with posters (without API key!)\n  \n\n\n    Search torrents by name using Jackett (local torrent aggregator).\n  \n\n\n    Stream them with your favorite player (personally I use mpv).\n  \n\n\n    Stream any torrent or magnet that contain video files\n  \n\n\n    Screenshots:\n  \n\n\n    Searching movie: https://files.catbox.moe/3ine82.png\n\n\n\n    List of torrents: https://files.catbox.moe/u3i9va.png\n\n\n\n    Torrent selected, 10 seconds passed: https://files.catbox.moe/eyr3uz.png\n\n\n\n    And finally: https://files.catbox.moe/7x3sss.png\n\n\n\n\nTarget Audience\n\n    Literally every movie/show enjoyer.\n  \n\nComparison\n\n    You may ask - \"why? There are peerflix and webtorrent\".\n  \n    These programs may be enough for some people. But, first of all, there is no search for torrents in them, which is quite important and saves a lot of time. Secondly, btstrm uses btfs to work with bittorrent network. Unlike webtorrent (javascript, meh), btfs is written in C and uses libtorrent, and it \"mounts\" the torrent as a file system, which makes it easier to work with the files themselves if you need to, and it doesn't clog up your /tmp with multi-gigabyte files.\n  \n    Other programs it uses are chafa for showing movie posters, fzf for selecting a movie or torrent, and a few Python libraries. Also, if you are learning languages, and listen to condensed audio with impd, the program has a flag to easily add audio to your collection.\n  \n    At first time this program was just a product of simple merging of btfsplay script and some functions to search in Jackett. But now it is program that shows progress, uses many arguments and... just does its job."
},
{
    "title": "No title",
    "content": "Hey all!\n  \n    In this past few months, I've been working on OasysDB, an embedded vector database inspired by SQLite. OasysDB is written in Rust with interoperability to Python which offers high memory and type safety with high performance.\n  \nWhat My Project Does\n\n    OasysDB helps perform RAG with local AI models or any other small to medium size vector search operation . One of the perfect use case to use OasysDB is if you're building a LLM-based desktop app that doesn't require network connection.\n  \nWhy use OasysDB\n\n\n\n    Requires almost zero setup with pip install oasysdb.\n  \n\n\n    Easy to use and straightforward API.\n  \n\n\n    Embedded database with optional persistence and high performance.\n  \n\n\n    Type and memory safe; less prone to unexpected errors.\n  \n\n\nHere's what comes with v0.3.0\n\n\n\n    Storing vectors with a custom and flexible metadata in a Collection. You can modify, delete, or insert new vector records without the collection rebuilding the entire index.\n  \n\n\n    Optional persistence mode with Database class. By default, Collection runs in memory to make sure the high performance. But if you need to persist it for later use, simply call the Database.save_collection method.\n  \n\n\nTarget Audience\n\n    Currently, OasysDB can be considered as in-beta which means there might be things that we change before we reach v1.0.0.\n  \nComparison\n\n    OasysDB is fully-embedded inside the application instead of running separately. If most of the current vector database such as Qdrant is like Postgres, OasysDB is like SQLite.\n  \nBenchmarking\n\n    I'm still working on adding more benchmarks but we do have search performance benchmark which I will list below. The result below is taken using 16GB memory and Apple M2 CPU machine/my personal computer. Note that depending on different machine, the benchmark might differs. So take it with a grain of salt.\n  \n\n\n    Collection of 10,000 vectors with 128 dimensions: 0.15 ms\n  \n\n\n    Collection of 1,000,000 vectors with 128 dimensions: 1.5 ms\n  \n\n\n    Anyway, thank you so much for coming by. Please let me know if you decide to give it a go. I'll be happy to hear any feedback or question. I'll be happy to help too if you need any assistance.\n  \nhttps://github.com/oasysai/oasysdb"
},
{
    "title": "No title",
    "content": "What is your biggest hurdle in learning the Python programming language? What specific area is hard for you to understand?\n  \n    Edit:\n  \n    Thank you to all the people who commented and discussed various challenges. Here are the obvious ones:\n  \n\n\n    Installation on various OS, along with which packages to use for installation (Pip, conda).\n  \n\n\n    Bootcamp tutorials seem to be boring and repetitive. There is hardly a resource available that mimics real-world scenarios.\n  \n\n\n    Type hinting can be challenging at first.\n  \n\n\n    Module and file structure - Navigate through the various sundirectory"
},
{
    "title": "No title",
    "content": "Hey Reddit community!\n  \n    I'm excited to present ocrtoolkit, a powerful OCR package designed to simplify your workflow and elevate your OCR tasks!\n  What My Project Does\n    If you've ever found yourself grappling with complex boilerplate code while tackling OCR-related challenges, you're in luck. ocrtoolkit streamlines the entire OCR process, offering intuitive wrappers for tasks such as image file handling, model execution, result parsing, and much more. Let's delve into the core features:\n  \n\n\nDatasets Module: Need to effortlessly load image files or directories? Look no further than the ocrtoolkit.datasets module.\n  \n\n\nModels Module: Seamlessly integrate with popular OCR frameworks like paddleOCR, ultralytics, and doctr through the ocrtoolkit.models module. Leverage sophisticated object detection models from ultralytics to pinpoint regions of interest before running OCR.\n  \n\n\nWrappers Module: Utilize wrappers for object detection, word detection, and recognition results with ease, courtesy of the ocrtoolkit.wrappers module. This standalone module ensures quick installation via pip install ocrtoolkit.\n  \n\n\nUtilities Module: Access a plethora of utilities for tasks like word-to-line merging, geometry operations, file I/O, and beyond with the ocrtoolkit.utilities module.\n  \n\nTarget Audience\n    Whether you're a researcher, developer, or data scientist embarking on OCR-related projects, ocrtoolkit caters to your needs. This package is your go-to solution for streamlining workflows, experimenting with different models and frameworks, and simplifying the inference process.\n  Comparison\n    Let's discuss how ocrtoolkit sets itself apart from existing alternatives:\n  \n\n\nComprehensive Support: Unlike packages solely focused on inference, ocrtoolkit offers comprehensive support for a myriad of OCR-related tasks, from parsing and processing ocr results, saving/loading and easy visualizations.\n  \n\n\nSeamless Integration: Experience seamless integration with popular OCR and object detection frameworks, facilitating effortless experimentation within a unified environment.\n  \n\n\nUser-Friendly Design: Designed with ease of use in mind, ocrtoolkit ensures swift setup and configuration, enabling users to dive into OCR tasks without hassle.\n  \n\nWhat ocrtoolkit is NOT for\n\n\nTraining Models: ocrtoolkit is not designed for training new OCR models. Instead, its primary focus lies in utilizing pretrained or fine-tuned models for inference.\n  \n\n\nHigh-Performance Applications: While ocrtoolkit boasts successful usage in production environments, it may not be the ideal choice for applications requiring maximum performance optimization.\n  \n\nAdditional Resources\n    Explore the comprehensive documentation and discover more about ocrtoolkit on its PyPi page. Dive into the notebooks folder within the repository for insightful examples, and don't hesitate to share your feedback and suggestions!\n  \n    Thank you for your time, and I eagerly await your valuable insights! ^_^"
},
{
    "title": "No title",
    "content": "I've been using Ruby with Sorbet for a long time.  There are some pain points, and some errors that it misses, but in generally it's a really nice development tool.  I can immediately look up the types of different variables, or method signatures.  I also get (nearly) immediate feedback in Vim, if I write a function with an obvious type error.  Even though typing is \"gradual\", it catches a lot more problems than you would expect.  It also is pretty easy to migrate a file to be type-checked, as long as your code isn't too magical.  I like it because, tbh, if you can't encode your idea into the type system, you're probably doing something that another person will struggle to understand.\n  \n\n    Anyway I have been seeing that Python has type hints, and some libraries for runtime validation.  I was wondering if there are any static analysis tools that are widely used."
},
{
    "title": "No title",
    "content": "Taipy-ChessWhat my project does\n    Taipy-Chess, is a chess visualization tool, based on 20,000 games.You can see all the games, the openings they played, opponents, top played openings and most successful openings. You can see heatmaps and charts on the data. This app was built using the taipy framework for easy data analysis and visualization.\n  Target Audience\n    The target audience is set for people in chess data visualization. I created this a part of a competition\nComparison\n    To compare to other data analysis chess tools, I would say, this is just a fun and easy to use chess visualization app. Compared to the projects posted on this competition, I would say, chess :).\n  More\n    I hope you guys enjoy. You should try creator quests. A fun way to challenge yourself. Star/Upvote if you like :).\n  Demo\nhttps://github.com/KorieDrakeChaney/taipy-chess/assets/92071726/c25fb773-124c-4836-bf0a-8bd80b2d5d14\nLinks\nGithubquine.sh"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Link to Blog post: https://prefix.dev/blog/introducing_multi_env_pixi\n\n    Pixi is a package manager that allows development using the conda & pypi ecosystem.\n  \n    Effortlessly switch between CUDA, CPU and other specialized software environments for maximum native performance and test against multiple versions of Python🐍."
},
{
    "title": "No title",
    "content": "Hello r/python!I've been compiling a list of Python libraries and resources that I've found useful across various projects. I thought it would be beneficial to share this collection with the community, so I've put together a repository for easy access and reference.You're welcome to check it out here: https://github.com/geru-scotland/pylib-atlas. If you find it useful, please consider starring it for future reference!Your feedback and contributions to enhance this collection are also highly appreciated.Thank you!"
},
{
    "title": "No title",
    "content": "I was working on a project that needed to send an email for confirmation. I didn't want to wait for the function to complete and retry if it failed, since I could verify from the database later and retry again.\n  \n    To solve this issue, I created a decorator for my personal use but then I decided to create a small library out of it in case it can help others.\n  \n    The decorator @retry_later() when used with your function, will retry your function in the background without stopping the flow of execution. Only use it with functions for which you don't need an immediate return value. In case your function stores the state in the database or somewhere else, this is for you!\n  \n    The library is called retry-later. It's already on PyPI.\n  \n    If you want to see some features that are not implemented, please leave a comment - I will implement it! Since it's at a fairly early stage, the library is quite rudimentary, and I am willing to add some features as needed.\n  \nTarget Audience:\n\nDevelopers. If you have a long-running/infinite event loop and you have error-prone functions (like sending an email or calling a callback URL), and you want to retry the operations without interrupting your current flow of execution. This library supports both synchronous (without async keyword) and async functions\n  \nComparison:\n  \n    It's easy to use! Simply add @retry_later() to your function :)\n  \n    I looked at other libraries like retry but it retries synchronously, and your flow has to wait for it to complete. For some operations, I just don't wanna wait!\n  \n    To use, simply add the `@retry_later()` decorator to your function.\n  \n    @retry_later() async def send_email(email: str, body: str): await send_email_to_friend(email, body)\n  \n    This will retry your function asynchronously without stopping the flow of execution.\n  \n    Git Repo: https://github.com/krishnasism/retry-later/\n\n    Take a look inside the `examples` folder inside the repo for usage examples.\n  \n    Please leave a⭐on the repo if you think it can help you!\n  \n    And again, open to suggestions!"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I tried to visualize the prime numbers in polar coordinates. This project was inspired by 3Blue1Brown channel to understand the relations.\n  \nhttps://github.com/Aaris-Kazi/prime-visualization/blob/main/Snapshots/whole.gif\n\n    To check more try to hit the link source code"
},
{
    "title": "No title",
    "content": "Almost 5 years ago, I released PyBoy v1.0.0 to the public, and it's been incredible to see what people have used it for.\n  \n    Things have changed a lot since then, and you might even have seen the viral video by Peter Whidden, who created an AI to play Pokémon using PyBoy: https://www.youtube.com/watch?v=DcYLT37ImBY\n\n    I've kept the API stable for all this time, and piled up improvements that I wanted to make. So the time has come for PyBoy v2.0.0 with a new and improved API. The hope is that the new API will be much more ergonomic to use, and still feel familiar to existing users.\n  \nWhat is PyBoyIn short, it’s a Game Boy emulator written from scratch in pure Python, with additional support for scripting through an API. PyBoy is as fast as comparable emulators written in C and C++ (or even better?). Benchmarked performance can get as high as 400 times faster than real-time. Meaning you can run 400 hours of game time in 1 hour.\n  \n    You can find the code and how to get started on the GitHub repo: https://github.com/Baekalfen/PyBoy\n\nThe new APIThe changelog is long, but of the major features, I want to highlight:\n  \n    * Significant improvements to documentation* `pyboy.tick()` now supports frame-skipping which dramatically improves AI/RL performance* `pyboy.button('left')` a simplified way to send input, which automatically releases buttons* `pyboy.memory[bank, address]` an intuitive way to read/write memory of the emulator* `pyboy.hook_register(bank, address, callback, context)` register callbacks when the emulators hits certain parts of the ROM* `pyboy.memory_scanner` a tool to isolate addresses of interest\n  \nFeedback, Contribute, Learn\n\n    We’d love to hear your feedback, and see the projects you wish to use PyBoy for! We will do our best to make it happen.\n  \n    Please give any feedback in the comments below, on our Discord server or create issues/pull-requests on GitHub if you wish.\n  \n    And a special thanks to the people on Discord, who were a great motivational factor in all of this (in no specific order): krs013, thatguy, NicoleFaye, pdubs, jan0809, Lyfe, capnspacehook, kr1tzy, Nico, MLGxPwnentz, mr_seeker, Sky, Travis Scott Burger, Whippersnatch Pumpkinpatch, AutoMathis"
},
{
    "title": "No title",
    "content": "Welcome to Brain tumor beginner tutorial, where we delve into world of CNNs (Convolutional Neural Networks) and their groundbreaking applications in image classification and brain tumor detection.\n  \n    This is a simple tutorial convolutional neural network tutorial that demonstrates how to brain tumor in a dataset of images.\n  \n    We will build and train a model using CNN and see the model accuracy & loss, and then we will test and predict a tumor using new images.\n  \n    Here is a link to the Github Repo: https://youtu.be/-147KGbGI3g\n\n    Enjoy\n  \n    Eran\n  \n    #cnnforimageclassification #cnnmachinelearningmodel #cnnml #deeplearningbraintumorclassification #aidetectbraintumor"
},
{
    "title": "No title",
    "content": "Without a doubt, the match statement is very powerful, but there are a few annoyances or missing features, IMO.\n  Type Aliases\n    Type aliases are not supported, so this won't work:\n  NewType1: TypeAlias = ...\ntype NewType2 = ...\n\nmatch x:\n    case NewType1():\n        ...\n    case NewType2():\n        ...Fallthrough\n    The following is not possible:\n  match x:\n    case A() if cond1(x): fallthrough\n    case B() if cond2(x):\n        code1\n    The alternatives are\n  match x:\n    case A() if cond1(x):\n        code1\n    case B() if cond2(x):\n        code1\n    which usually forces one to define a function for code1, or\n  match x:\n    case _ if ((isinstance(x, A) and cond1(x)) or\n               (isinstance(x, B) and cond2(x))):\n        code1\n    which is much more verbose.\n  Chained Member Accesses\n    I'd like to be able to write\n  match x:\n    case A(b.c.d.x = 4):\n        ...\n    instead of\n  match x:\n    case A(b = object(c = object(d = object(x = 4)))):\n        ...Double Indentation\n    The match statement wastes horizontal space. Why not just\n  match x:\ncase ...:\n    ...\ncase ...:\n    ...\n    I've been doing a lot of parsing lately and I find myself going for the if isinstance pattern in place of a second, nested match. Most of the time, I like to have two code views one next to the other, so all my code has a maximum row length of 81 (79 + 2 extra chars for some leeway)."
},
{
    "title": "No title",
    "content": "What is flect?\n    flect is a Python framework designed for building full-stack web applications. By leveraging Pydantic models in the backend to correspond with React components in the frontend, flect enables developers to quickly craft interactive and visually appealing user interfaces using Python.\n  Key Features\n\n\nFast Development: Empowers writing your entire application in Python, offering a seamless integration between backend logic and frontend UI.\n  \n\n\nEasy Form Validation: Utilizes a single Pydantic model for consistent form validation throughout your application, speeding up development and minimizing errors.\n  \n\n\nFolder-Based Routing: Simplifies route management with an intuitive folder structure.\n  \n\n\nClient-Side Routing: Ensures smooth and rapid page transitions without reloads.\n  \n\n\nSEO Friendly: Supports server-side rendering to enhance search engine visibility.\n  \n\n\nCustom Components: Allows the use of custom-built React components within flect.\n  \n\n\n    View the documentation website, built entirely with flect and deployed on Vercel, for more insights. Source code is available here.\n  What My Project Does\n    flect bridges the gap between Python backend development and React frontend design, enabling the creation of full-stack web applications without the need for JavaScript. This framework is perfect for rapidly developing feature-rich applications, offering out-of-the-box support for form validation, SEO, and custom components.\n  Target Audience\n\n\nPython Developers: For those looking to build responsive web applications leveraging their Python skills, without the necessity to write JavaScript or interact with npm.\n  \n\n\nFrontend Developers: For developers focused on crafting unique, reusable components without duplicating effort across views.\n  \n\n\nGeneral Audience: Anyone interested in a framework that offers a true separation of concerns, with backend logic defining the application and the frontend focusing solely on UI implementation.\n  \n\nComparison\n    flect draws inspiration from FastUI but differentiates itself in several key aspects:\n  \n    Frontend Foundation: flect leverages react-router, ShadcnUI, and TailwindCSS, creating a more dynamic and customizable user interface than FastUI.\n  \n    Routing Capabilities: It introduces Folder-Based Routing and Client-Side Routing, making navigation management both intuitive and efficient.\n  \n    SEO Optimization: With support for server-side rendering, flect enhances the search engine visibility of web applications, an area where traditional Python frameworks might lag.\n  \n    This streamlined approach, combining modern frontend technologies with Python's backend, sets flect apart as a more versatile and user-friendly framework for full-stack development.\n  Learn More\n\n\nDocumentation\n\n\nThanks for Reading"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I have been using more and more of Python over the past few weeks because I got sick of JS. When I asked a programmer friend of mine who uses JS religiously what he thought about using Python for web backends, he told me that it would be slow because of JSON serialization.\n  \n    I spent some time researching this, but couldn’t find anything decisive that explained why JSON serialization would be faster in JS. The answers I found said a few things:\n  \n\n\n    V8 is optimized for this because it’s part of the JS standard\n  \n\n\n    it’s slow in Python because of an implementation of how the JSON to Python representation (JSON dump) function is represented in Python.\n  \n\n\n    Does anyone else have any further insight on this topic? I am trying to understand this at a fundamental level and cut through any noise / wrong understanding.\n  \n    Thanks so much!"
},
{
    "title": "No title",
    "content": "Maybe i worded my title wrong. Taking a look at job listings involving python, backend development seems to be the only listing I come across. Why is that the case ? Are they no other career paths outside backend dev for python developers ?"
},
{
    "title": "No title",
    "content": "We spoke with Ralf about the upcoming 2.0 release, the first major release in 16 years. Ralf has a great story as to how he got into the open source community in general, and NumPy and SciPy in particular.\n  \n    You can check it out here: https://open.substack.com/pub/onceamaintainer/p/once-a-maintainer-ralf-gommers?r=2773u5&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "FlowQ is a Distributed Computing system API, Which aims to make Distributed Computing Free and Simple to use!\nFeatures of FlowQ:\n\n\nEffortless Setup: Ditch the complicated configurations! FlowQ runs right out of the box, no ssh headaches or pre-installation required.\n  \n\n\nSimple and Secure Connection: Leverages the Hack.Chat  and FileBin platforms to establish secure, base-64 encrypted and anonymous connections with your computing cluster.\n  \n\n\nParallel task execution: FlowQ maximizes your network by executing tasks in parallel across your machines(with multi-threading), significantly boosting your processing speed.\n  \n\nYou can add computers to your cluster just running by:pip install FlowQ\npython -m FlowQ.cluster -c <your-channel-name>\n\n\n    Do not worry, if you dont have any extra computers!, you can just run the above code in a Google Colab Instances, to add computers in your cluster!\n  \n\nWhat my project does:\n    Helps to complete tasks which are time and memory intensive, faster by splitting the work between computers. It can speedup your program task by X times( X is number of computers in cluster)\n  Target Audience:\n    Anyone, who wants to speedup thier program tasks. Eg: Data Extraction, Data Transformation, or any kind of repetative task :)\n  \n    Github: FlowQ GitHub Repo (Has more informations!)\n  \n    PyPi page: https://pypi.org/project/FlowQ/\nSome Points I wanted to say:\n\n\n    The project is in very early development stage. So, if you get any issues, Please report it. It would be a great help!\n  \n\n\n    Your Contributions are Happily Welcomed! I would like to see new ideas from people!\n  \n\n\n    Leave feedbacks on anything you would wanna see it Fixed or Improved! I would love to read your opinions!\n\n\n\n    Thanks for Reading :)"
},
{
    "title": "No title",
    "content": "Hey everyone,\n  \n    Today, I wanted to search for some documentation about PyYAML for a personal project. However, I saw that their documentation and GitHub pages all give 404's. The package is still in PyPi.\n  \n    Does anyone know why? Did they go private?\n  \n\n\nhttps://pyyaml.org\n\n\n\nhttps://github.com/yaml/pyyaml\n\n\n\nhttps://pypi.org/project/PyYAML/"
},
{
    "title": "No title",
    "content": "Repo: https://github.com/genomoncology/FuzzTypes\n\n    FuzzTypes is a library for defining custom annotation types that auto-correct data.\n  \n    This can be used to clean the structured data generated or extracted using OpenAI Function Calling.\n  \n    Since it is built on Pydantic it will work with FastAPI Custom GPT Actions, Instructor, Marvin, DSPy Typed Predictors and any other library currently using Pydantic to define and validate function calling tools.\n  \n    FuzzTypes comes with a variety of \"usable types\" that can be used immediately such as Fuzzy Dates/Times, Email, Zipcode, Integer Words, Emojis.\n  \n    It's also easy to create your own custom annotation types. Simply:- Provide a function that converts an \"bad\" input value to a \"good\" output value.- Provide a regex for matching desired patterns (e.g. zipcode, email)- Provide a list of \"named entities\" (name + aliases) that can be matched to either exactly, case insensitively, fuzzily or semantically (e.g. country, currency)\n  \n    For more information, check out the README:https://github.com/genomoncology/FuzzTypes"
},
{
    "title": "No title",
    "content": "What it Does:\n    Schnoz is an advanced packet sniffer/analyzer and IDS with several options to conduct network monitoring and threat analysis. My script supports both Windows and Linux and is written in Python.\n  Features:\n\n\n    Actively sniffs local traffic on specified interfaces\n  \n\n\n    Passively sniffs traffic from pcap files\n  \n\n\n    Can either actively sniff an interface or file for suspicious activity, alerting on potential malicious traffic. Malicious terms can come from a user-inputted wordlist or use the Schnozlist, which are terms that I've encountered through CTFs\n  \n\n\n    Analyzes HTTP requests and responses on either an active interface or file\n  \n\n\n    Windows and Linux support\n  \n\n\n    Very, very user friendly\n  \n\n\n    Customizable results with the use of arguments\n  \n\n\nTarget Audience: Anyone working in a blue team environment and anyone working on a network, defensive CTF. This tool may also be useful for someone trying to get a better understanding of network traffic\n  \nComparisons: My script is comparable to Wireshark and tcpdump. These tools, though, can be harder to use for someone who's just getting into packet analysis. I've included the option of using arguments for more advanced analysis.\n  \n\n    I just published the code today, but I'm going to try and improve it consistently for the community. I'm very open to feedback about its functionality and source code. Thanks!\n  \n    Repo: https://github.com/abelreqma/schnoz\n\n    Source code: https://github.com/abelreqma/schnoz/blob/main/schnoz.py"
},
{
    "title": "No title",
    "content": "I'll disclaim: I'm not a programmer but a litterateur, my code may be quite bad and in some (some!) places written with LLMs - but it works as should be and its simplicity makes me admire Python.\n  // The beginning of lyric digression\n    For the last 7 years I've been writing scripts in bash. I've accumulated quite a lot of them, I would say that it became a kind of escapism for me, in anxious moments of my life, when I was too lazy to do something on my own, I prefer to spend half a night writing a script, so that I could press one button and - bang - everything would work by itself. At one point, Bash became not enough. Despite the fact that it became convenient and fast to write in it, in fact, it works slowly enough to create some serious projects. And at one point, my friend encouraged me to use Python, luckily he knows Python and I had an idea that came to life. So now I use my Python program literally every day.\n  // End of lyric digression\n    I watch movies and shows often (don't blame me for piracy, of course it's Big Buck Bunny every time) and since I don't like to click a lot of buttons, I wrote a 500 line script (hopefully someone will recognize themselves in this :D).\n  \nhttps://github.com/asakura42/btstrm // https://pypi.org/project/btstrm/\n\n\nWhat My Project Does\n\n    This program can:\n  \n\n\n    Search for movies using TMDB in a given language and show the list of found movies along with posters (without API key!)\n  \n\n\n    Search torrents by name using Jackett (local torrent aggregator).\n  \n\n\n    Stream them with your favorite player (personally I use mpv).\n  \n\n\n    Stream any torrent or magnet that contain video files\n  \n\n\n    Screenshots:\n  \n\n\n    Searching movie: https://files.catbox.moe/3ine82.png\n\n\n\n    List of torrents: https://files.catbox.moe/u3i9va.png\n\n\n\n    Torrent selected, 10 seconds passed: https://files.catbox.moe/eyr3uz.png\n\n\n\n    And finally: https://files.catbox.moe/7x3sss.png\n\n\n\n\nTarget Audience\n\n    Literally every movie/show enjoyer.\n  \n\nComparison\n\n    You may ask - \"why? There are peerflix and webtorrent\".\n  \n    These programs may be enough for some people. But, first of all, there is no search for torrents in them, which is quite important and saves a lot of time. Secondly, btstrm uses btfs to work with bittorrent network. Unlike webtorrent (javascript, meh), btfs is written in C and uses libtorrent, and it \"mounts\" the torrent as a file system, which makes it easier to work with the files themselves if you need to, and it doesn't clog up your /tmp with multi-gigabyte files.\n  \n    Other programs it uses are chafa for showing movie posters, fzf for selecting a movie or torrent, and a few Python libraries. Also, if you are learning languages, and listen to condensed audio with impd, the program has a flag to easily add audio to your collection.\n  \n    At first time this program was just a product of simple merging of btfsplay script and some functions to search in Jackett. But now it is program that shows progress, uses many arguments and... just does its job."
},
{
    "title": "No title",
    "content": "Hey all!\n  \n    In this past few months, I've been working on OasysDB, an embedded vector database inspired by SQLite. OasysDB is written in Rust with interoperability to Python which offers high memory and type safety with high performance.\n  \nWhat My Project Does\n\n    OasysDB helps perform RAG with local AI models or any other small to medium size vector search operation . One of the perfect use case to use OasysDB is if you're building a LLM-based desktop app that doesn't require network connection.\n  \nWhy use OasysDB\n\n\n\n    Requires almost zero setup with pip install oasysdb.\n  \n\n\n    Easy to use and straightforward API.\n  \n\n\n    Embedded database with optional persistence and high performance.\n  \n\n\n    Type and memory safe; less prone to unexpected errors.\n  \n\n\nHere's what comes with v0.3.0\n\n\n\n    Storing vectors with a custom and flexible metadata in a Collection. You can modify, delete, or insert new vector records without the collection rebuilding the entire index.\n  \n\n\n    Optional persistence mode with Database class. By default, Collection runs in memory to make sure the high performance. But if you need to persist it for later use, simply call the Database.save_collection method.\n  \n\n\nTarget Audience\n\n    Currently, OasysDB can be considered as in-beta which means there might be things that we change before we reach v1.0.0.\n  \nComparison\n\n    OasysDB is fully-embedded inside the application instead of running separately. If most of the current vector database such as Qdrant is like Postgres, OasysDB is like SQLite.\n  \nBenchmarking\n\n    I'm still working on adding more benchmarks but we do have search performance benchmark which I will list below. The result below is taken using 16GB memory and Apple M2 CPU machine/my personal computer. Note that depending on different machine, the benchmark might differs. So take it with a grain of salt.\n  \n\n\n    Collection of 10,000 vectors with 128 dimensions: 0.15 ms\n  \n\n\n    Collection of 1,000,000 vectors with 128 dimensions: 1.5 ms\n  \n\n\n    Anyway, thank you so much for coming by. Please let me know if you decide to give it a go. I'll be happy to hear any feedback or question. I'll be happy to help too if you need any assistance.\n  \nhttps://github.com/oasysai/oasysdb"
},
{
    "title": "No title",
    "content": "What is your biggest hurdle in learning the Python programming language? What specific area is hard for you to understand?\n  \n    Edit:\n  \n    Thank you to all the people who commented and discussed various challenges. Here are the obvious ones:\n  \n\n\n    Installation on various OS, along with which packages to use for installation (Pip, conda).\n  \n\n\n    Bootcamp tutorials seem to be boring and repetitive. There is hardly a resource available that mimics real-world scenarios.\n  \n\n\n    Type hinting can be challenging at first.\n  \n\n\n    Module and file structure - Navigate through the various sundirectory"
},
{
    "title": "No title",
    "content": "Hey Reddit community!\n  \n    I'm excited to present ocrtoolkit, a powerful OCR package designed to simplify your workflow and elevate your OCR tasks!\n  What My Project Does\n    If you've ever found yourself grappling with complex boilerplate code while tackling OCR-related challenges, you're in luck. ocrtoolkit streamlines the entire OCR process, offering intuitive wrappers for tasks such as image file handling, model execution, result parsing, and much more. Let's delve into the core features:\n  \n\n\nDatasets Module: Need to effortlessly load image files or directories? Look no further than the ocrtoolkit.datasets module.\n  \n\n\nModels Module: Seamlessly integrate with popular OCR frameworks like paddleOCR, ultralytics, and doctr through the ocrtoolkit.models module. Leverage sophisticated object detection models from ultralytics to pinpoint regions of interest before running OCR.\n  \n\n\nWrappers Module: Utilize wrappers for object detection, word detection, and recognition results with ease, courtesy of the ocrtoolkit.wrappers module. This standalone module ensures quick installation via pip install ocrtoolkit.\n  \n\n\nUtilities Module: Access a plethora of utilities for tasks like word-to-line merging, geometry operations, file I/O, and beyond with the ocrtoolkit.utilities module.\n  \n\nTarget Audience\n    Whether you're a researcher, developer, or data scientist embarking on OCR-related projects, ocrtoolkit caters to your needs. This package is your go-to solution for streamlining workflows, experimenting with different models and frameworks, and simplifying the inference process.\n  Comparison\n    Let's discuss how ocrtoolkit sets itself apart from existing alternatives:\n  \n\n\nComprehensive Support: Unlike packages solely focused on inference, ocrtoolkit offers comprehensive support for a myriad of OCR-related tasks, from parsing and processing ocr results, saving/loading and easy visualizations.\n  \n\n\nSeamless Integration: Experience seamless integration with popular OCR and object detection frameworks, facilitating effortless experimentation within a unified environment.\n  \n\n\nUser-Friendly Design: Designed with ease of use in mind, ocrtoolkit ensures swift setup and configuration, enabling users to dive into OCR tasks without hassle.\n  \n\nWhat ocrtoolkit is NOT for\n\n\nTraining Models: ocrtoolkit is not designed for training new OCR models. Instead, its primary focus lies in utilizing pretrained or fine-tuned models for inference.\n  \n\n\nHigh-Performance Applications: While ocrtoolkit boasts successful usage in production environments, it may not be the ideal choice for applications requiring maximum performance optimization.\n  \n\nAdditional Resources\n    Explore the comprehensive documentation and discover more about ocrtoolkit on its PyPi page. Dive into the notebooks folder within the repository for insightful examples, and don't hesitate to share your feedback and suggestions!\n  \n    Thank you for your time, and I eagerly await your valuable insights! ^_^"
},
{
    "title": "No title",
    "content": "I've been using Ruby with Sorbet for a long time.  There are some pain points, and some errors that it misses, but in generally it's a really nice development tool.  I can immediately look up the types of different variables, or method signatures.  I also get (nearly) immediate feedback in Vim, if I write a function with an obvious type error.  Even though typing is \"gradual\", it catches a lot more problems than you would expect.  It also is pretty easy to migrate a file to be type-checked, as long as your code isn't too magical.  I like it because, tbh, if you can't encode your idea into the type system, you're probably doing something that another person will struggle to understand.\n  \n\n    Anyway I have been seeing that Python has type hints, and some libraries for runtime validation.  I was wondering if there are any static analysis tools that are widely used."
},
{
    "title": "No title",
    "content": "Taipy-ChessWhat my project does\n    Taipy-Chess, is a chess visualization tool, based on 20,000 games.You can see all the games, the openings they played, opponents, top played openings and most successful openings. You can see heatmaps and charts on the data. This app was built using the taipy framework for easy data analysis and visualization.\n  Target Audience\n    The target audience is set for people in chess data visualization. I created this a part of a competition\nComparison\n    To compare to other data analysis chess tools, I would say, this is just a fun and easy to use chess visualization app. Compared to the projects posted on this competition, I would say, chess :).\n  More\n    I hope you guys enjoy. You should try creator quests. A fun way to challenge yourself. Star/Upvote if you like :).\n  Demo\nhttps://github.com/KorieDrakeChaney/taipy-chess/assets/92071726/c25fb773-124c-4836-bf0a-8bd80b2d5d14\nLinks\nGithubquine.sh"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Link to Blog post: https://prefix.dev/blog/introducing_multi_env_pixi\n\n    Pixi is a package manager that allows development using the conda & pypi ecosystem.\n  \n    Effortlessly switch between CUDA, CPU and other specialized software environments for maximum native performance and test against multiple versions of Python🐍."
},
{
    "title": "No title",
    "content": "Hello r/python!I've been compiling a list of Python libraries and resources that I've found useful across various projects. I thought it would be beneficial to share this collection with the community, so I've put together a repository for easy access and reference.You're welcome to check it out here: https://github.com/geru-scotland/pylib-atlas. If you find it useful, please consider starring it for future reference!Your feedback and contributions to enhance this collection are also highly appreciated.Thank you!"
},
{
    "title": "No title",
    "content": "I was working on a project that needed to send an email for confirmation. I didn't want to wait for the function to complete and retry if it failed, since I could verify from the database later and retry again.\n  \n    To solve this issue, I created a decorator for my personal use but then I decided to create a small library out of it in case it can help others.\n  \n    The decorator @retry_later() when used with your function, will retry your function in the background without stopping the flow of execution. Only use it with functions for which you don't need an immediate return value. In case your function stores the state in the database or somewhere else, this is for you!\n  \n    The library is called retry-later. It's already on PyPI.\n  \n    If you want to see some features that are not implemented, please leave a comment - I will implement it! Since it's at a fairly early stage, the library is quite rudimentary, and I am willing to add some features as needed.\n  \nTarget Audience:\n\nDevelopers. If you have a long-running/infinite event loop and you have error-prone functions (like sending an email or calling a callback URL), and you want to retry the operations without interrupting your current flow of execution. This library supports both synchronous (without async keyword) and async functions\n  \nComparison:\n  \n    It's easy to use! Simply add @retry_later() to your function :)\n  \n    I looked at other libraries like retry but it retries synchronously, and your flow has to wait for it to complete. For some operations, I just don't wanna wait!\n  \n    To use, simply add the `@retry_later()` decorator to your function.\n  \n    @retry_later() async def send_email(email: str, body: str): await send_email_to_friend(email, body)\n  \n    This will retry your function asynchronously without stopping the flow of execution.\n  \n    Git Repo: https://github.com/krishnasism/retry-later/\n\n    Take a look inside the `examples` folder inside the repo for usage examples.\n  \n    Please leave a⭐on the repo if you think it can help you!\n  \n    And again, open to suggestions!"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I tried to visualize the prime numbers in polar coordinates. This project was inspired by 3Blue1Brown channel to understand the relations.\n  \nhttps://github.com/Aaris-Kazi/prime-visualization/blob/main/Snapshots/whole.gif\n\n    To check more try to hit the link source code"
},
{
    "title": "No title",
    "content": "Almost 5 years ago, I released PyBoy v1.0.0 to the public, and it's been incredible to see what people have used it for.\n  \n    Things have changed a lot since then, and you might even have seen the viral video by Peter Whidden, who created an AI to play Pokémon using PyBoy: https://www.youtube.com/watch?v=DcYLT37ImBY\n\n    I've kept the API stable for all this time, and piled up improvements that I wanted to make. So the time has come for PyBoy v2.0.0 with a new and improved API. The hope is that the new API will be much more ergonomic to use, and still feel familiar to existing users.\n  \nWhat is PyBoyIn short, it’s a Game Boy emulator written from scratch in pure Python, with additional support for scripting through an API. PyBoy is as fast as comparable emulators written in C and C++ (or even better?). Benchmarked performance can get as high as 400 times faster than real-time. Meaning you can run 400 hours of game time in 1 hour.\n  \n    You can find the code and how to get started on the GitHub repo: https://github.com/Baekalfen/PyBoy\n\nThe new APIThe changelog is long, but of the major features, I want to highlight:\n  \n    * Significant improvements to documentation* `pyboy.tick()` now supports frame-skipping which dramatically improves AI/RL performance* `pyboy.button('left')` a simplified way to send input, which automatically releases buttons* `pyboy.memory[bank, address]` an intuitive way to read/write memory of the emulator* `pyboy.hook_register(bank, address, callback, context)` register callbacks when the emulators hits certain parts of the ROM* `pyboy.memory_scanner` a tool to isolate addresses of interest\n  \nFeedback, Contribute, Learn\n\n    We’d love to hear your feedback, and see the projects you wish to use PyBoy for! We will do our best to make it happen.\n  \n    Please give any feedback in the comments below, on our Discord server or create issues/pull-requests on GitHub if you wish.\n  \n    And a special thanks to the people on Discord, who were a great motivational factor in all of this (in no specific order): krs013, thatguy, NicoleFaye, pdubs, jan0809, Lyfe, capnspacehook, kr1tzy, Nico, MLGxPwnentz, mr_seeker, Sky, Travis Scott Burger, Whippersnatch Pumpkinpatch, AutoMathis"
},
{
    "title": "No title",
    "content": "Welcome to Brain tumor beginner tutorial, where we delve into world of CNNs (Convolutional Neural Networks) and their groundbreaking applications in image classification and brain tumor detection.\n  \n    This is a simple tutorial convolutional neural network tutorial that demonstrates how to brain tumor in a dataset of images.\n  \n    We will build and train a model using CNN and see the model accuracy & loss, and then we will test and predict a tumor using new images.\n  \n    Here is a link to the Github Repo: https://youtu.be/-147KGbGI3g\n\n    Enjoy\n  \n    Eran\n  \n    #cnnforimageclassification #cnnmachinelearningmodel #cnnml #deeplearningbraintumorclassification #aidetectbraintumor"
},
{
    "title": "No title",
    "content": "Without a doubt, the match statement is very powerful, but there are a few annoyances or missing features, IMO.\n  Type Aliases\n    Type aliases are not supported, so this won't work:\n  NewType1: TypeAlias = ...\ntype NewType2 = ...\n\nmatch x:\n    case NewType1():\n        ...\n    case NewType2():\n        ...Fallthrough\n    The following is not possible:\n  match x:\n    case A() if cond1(x): fallthrough\n    case B() if cond2(x):\n        code1\n    The alternatives are\n  match x:\n    case A() if cond1(x):\n        code1\n    case B() if cond2(x):\n        code1\n    which usually forces one to define a function for code1, or\n  match x:\n    case _ if ((isinstance(x, A) and cond1(x)) or\n               (isinstance(x, B) and cond2(x))):\n        code1\n    which is much more verbose.\n  Chained Member Accesses\n    I'd like to be able to write\n  match x:\n    case A(b.c.d.x = 4):\n        ...\n    instead of\n  match x:\n    case A(b = object(c = object(d = object(x = 4)))):\n        ...Double Indentation\n    The match statement wastes horizontal space. Why not just\n  match x:\ncase ...:\n    ...\ncase ...:\n    ...\n    I've been doing a lot of parsing lately and I find myself going for the if isinstance pattern in place of a second, nested match. Most of the time, I like to have two code views one next to the other, so all my code has a maximum row length of 81 (79 + 2 extra chars for some leeway)."
},
{
    "title": "No title",
    "content": "What is flect?\n    flect is a Python framework designed for building full-stack web applications. By leveraging Pydantic models in the backend to correspond with React components in the frontend, flect enables developers to quickly craft interactive and visually appealing user interfaces using Python.\n  Key Features\n\n\nFast Development: Empowers writing your entire application in Python, offering a seamless integration between backend logic and frontend UI.\n  \n\n\nEasy Form Validation: Utilizes a single Pydantic model for consistent form validation throughout your application, speeding up development and minimizing errors.\n  \n\n\nFolder-Based Routing: Simplifies route management with an intuitive folder structure.\n  \n\n\nClient-Side Routing: Ensures smooth and rapid page transitions without reloads.\n  \n\n\nSEO Friendly: Supports server-side rendering to enhance search engine visibility.\n  \n\n\nCustom Components: Allows the use of custom-built React components within flect.\n  \n\n\n    View the documentation website, built entirely with flect and deployed on Vercel, for more insights. Source code is available here.\n  What My Project Does\n    flect bridges the gap between Python backend development and React frontend design, enabling the creation of full-stack web applications without the need for JavaScript. This framework is perfect for rapidly developing feature-rich applications, offering out-of-the-box support for form validation, SEO, and custom components.\n  Target Audience\n\n\nPython Developers: For those looking to build responsive web applications leveraging their Python skills, without the necessity to write JavaScript or interact with npm.\n  \n\n\nFrontend Developers: For developers focused on crafting unique, reusable components without duplicating effort across views.\n  \n\n\nGeneral Audience: Anyone interested in a framework that offers a true separation of concerns, with backend logic defining the application and the frontend focusing solely on UI implementation.\n  \n\nComparison\n    flect draws inspiration from FastUI but differentiates itself in several key aspects:\n  \n    Frontend Foundation: flect leverages react-router, ShadcnUI, and TailwindCSS, creating a more dynamic and customizable user interface than FastUI.\n  \n    Routing Capabilities: It introduces Folder-Based Routing and Client-Side Routing, making navigation management both intuitive and efficient.\n  \n    SEO Optimization: With support for server-side rendering, flect enhances the search engine visibility of web applications, an area where traditional Python frameworks might lag.\n  \n    This streamlined approach, combining modern frontend technologies with Python's backend, sets flect apart as a more versatile and user-friendly framework for full-stack development.\n  Learn More\n\n\nDocumentation\n\n\nThanks for Reading"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I have been using more and more of Python over the past few weeks because I got sick of JS. When I asked a programmer friend of mine who uses JS religiously what he thought about using Python for web backends, he told me that it would be slow because of JSON serialization.\n  \n    I spent some time researching this, but couldn’t find anything decisive that explained why JSON serialization would be faster in JS. The answers I found said a few things:\n  \n\n\n    V8 is optimized for this because it’s part of the JS standard\n  \n\n\n    it’s slow in Python because of an implementation of how the JSON to Python representation (JSON dump) function is represented in Python.\n  \n\n\n    Does anyone else have any further insight on this topic? I am trying to understand this at a fundamental level and cut through any noise / wrong understanding.\n  \n    Thanks so much!"
},
{
    "title": "No title",
    "content": "Maybe i worded my title wrong. Taking a look at job listings involving python, backend development seems to be the only listing I come across. Why is that the case ? Are they no other career paths outside backend dev for python developers ?"
},
{
    "title": "No title",
    "content": "We spoke with Ralf about the upcoming 2.0 release, the first major release in 16 years. Ralf has a great story as to how he got into the open source community in general, and NumPy and SciPy in particular.\n  \n    You can check it out here: https://open.substack.com/pub/onceamaintainer/p/once-a-maintainer-ralf-gommers?r=2773u5&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "FlowQ is a Distributed Computing system API, Which aims to make Distributed Computing Free and Simple to use!\nFeatures of FlowQ:\n\n\nEffortless Setup: Ditch the complicated configurations! FlowQ runs right out of the box, no ssh headaches or pre-installation required.\n  \n\n\nSimple and Secure Connection: Leverages the Hack.Chat  and FileBin platforms to establish secure, base-64 encrypted and anonymous connections with your computing cluster.\n  \n\n\nParallel task execution: FlowQ maximizes your network by executing tasks in parallel across your machines(with multi-threading), significantly boosting your processing speed.\n  \n\nYou can add computers to your cluster just running by:pip install FlowQ\npython -m FlowQ.cluster -c <your-channel-name>\n\n\n    Do not worry, if you dont have any extra computers!, you can just run the above code in a Google Colab Instances, to add computers in your cluster!\n  \n\nWhat my project does:\n    Helps to complete tasks which are time and memory intensive, faster by splitting the work between computers. It can speedup your program task by X times( X is number of computers in cluster)\n  Target Audience:\n    Anyone, who wants to speedup thier program tasks. Eg: Data Extraction, Data Transformation, or any kind of repetative task :)\n  \n    Github: FlowQ GitHub Repo (Has more informations!)\n  \n    PyPi page: https://pypi.org/project/FlowQ/\nSome Points I wanted to say:\n\n\n    The project is in very early development stage. So, if you get any issues, Please report it. It would be a great help!\n  \n\n\n    Your Contributions are Happily Welcomed! I would like to see new ideas from people!\n  \n\n\n    Leave feedbacks on anything you would wanna see it Fixed or Improved! I would love to read your opinions!\n\n\n\n    Thanks for Reading :)"
},
{
    "title": "No title",
    "content": "Hey everyone,\n  \n    Today, I wanted to search for some documentation about PyYAML for a personal project. However, I saw that their documentation and GitHub pages all give 404's. The package is still in PyPi.\n  \n    Does anyone know why? Did they go private?\n  \n\n\nhttps://pyyaml.org\n\n\n\nhttps://github.com/yaml/pyyaml\n\n\n\nhttps://pypi.org/project/PyYAML/"
},
{
    "title": "No title",
    "content": "Repo: https://github.com/genomoncology/FuzzTypes\n\n    FuzzTypes is a library for defining custom annotation types that auto-correct data.\n  \n    This can be used to clean the structured data generated or extracted using OpenAI Function Calling.\n  \n    Since it is built on Pydantic it will work with FastAPI Custom GPT Actions, Instructor, Marvin, DSPy Typed Predictors and any other library currently using Pydantic to define and validate function calling tools.\n  \n    FuzzTypes comes with a variety of \"usable types\" that can be used immediately such as Fuzzy Dates/Times, Email, Zipcode, Integer Words, Emojis.\n  \n    It's also easy to create your own custom annotation types. Simply:- Provide a function that converts an \"bad\" input value to a \"good\" output value.- Provide a regex for matching desired patterns (e.g. zipcode, email)- Provide a list of \"named entities\" (name + aliases) that can be matched to either exactly, case insensitively, fuzzily or semantically (e.g. country, currency)\n  \n    For more information, check out the README:https://github.com/genomoncology/FuzzTypes"
},
{
    "title": "No title",
    "content": "What it Does:\n    Schnoz is an advanced packet sniffer/analyzer and IDS with several options to conduct network monitoring and threat analysis. My script supports both Windows and Linux and is written in Python.\n  Features:\n\n\n    Actively sniffs local traffic on specified interfaces\n  \n\n\n    Passively sniffs traffic from pcap files\n  \n\n\n    Can either actively sniff an interface or file for suspicious activity, alerting on potential malicious traffic. Malicious terms can come from a user-inputted wordlist or use the Schnozlist, which are terms that I've encountered through CTFs\n  \n\n\n    Analyzes HTTP requests and responses on either an active interface or file\n  \n\n\n    Windows and Linux support\n  \n\n\n    Very, very user friendly\n  \n\n\n    Customizable results with the use of arguments\n  \n\n\nTarget Audience: Anyone working in a blue team environment and anyone working on a network, defensive CTF. This tool may also be useful for someone trying to get a better understanding of network traffic\n  \nComparisons: My script is comparable to Wireshark and tcpdump. These tools, though, can be harder to use for someone who's just getting into packet analysis. I've included the option of using arguments for more advanced analysis.\n  \n\n    I just published the code today, but I'm going to try and improve it consistently for the community. I'm very open to feedback about its functionality and source code. Thanks!\n  \n    Repo: https://github.com/abelreqma/schnoz\n\n    Source code: https://github.com/abelreqma/schnoz/blob/main/schnoz.py"
},
{
    "title": "No title",
    "content": "Hey Everybody, I have seen Python used for many things and I am just wondering, for those who work with Python and another language, what is the best complimentary language for your area (or just in general in your opinion) and why?\n  \n    Is the language used to make faster libraries (like making a C/C++ library for a CPU intensive task)? Maybe you use a higher level language like C# or Java for an application and Python for some DS, AI/ML section? I am curious which languages work well with Python and why? Thanks!\n  \n    Edit: Thanks everyone for all of this info about languages that are useful with Python. It has been very informative and I will definitely be checking out some of these suggested companion languages. Thanks!"
},
{
    "title": "No title",
    "content": "deptry 0.14.0 was just released, bringing significant speed improvements: It is now up to 10 times faster than the previous release! 🚀\n  \n    For those unfamiliar with deptry; deptry is a command line tool to check for issues with dependencies in a Python project, such as unused or missing dependencies. [GitHub]\n  \n    For some benchmarks of the new release, see the release notes here.\n  \n    The performance improvement was achieved by leveraging Rust to parse the AST and extract the import statements from .py files, rather than using Python's ast module.\n  \n    The addition of Rust to the project also opens up doors for more optimizations in the future, so stay tuned!"
},
{
    "title": "No title",
    "content": "DSPy is a framework that aims to solve the fragility problem in language model (LM)-based applications by prioritizing programming over prompting. It allows you to recompile the entire pipeline to optimize it to your specific task — instead of repeating manual rounds of prompt engineering — whenever you change a component.\n  \nhttps://github.com/stanfordnlp/dspy"
},
{
    "title": "No title",
    "content": "Hello! Been working on this for awhile and excited to finally drop it.\n  \n    I forked Pyppeteer (no longer maintained) and revamped it, doing a good amount of refactoring and adding features along the way.\n  \n    Project link: https://github.com/michaeleveringham/mokr Documentation link: https://mokr.readthedocs.io/en/latest/index.html\nWhat My Project Does\n    Offers automated web browsing for both Chrome and Firefox.\n  Target Audience\n    Currently enthusiasts seeking to automate tests or web scrape. Eventually I’d like to offer this as production-ready but my test suite isn’t finished yet.\n  Comparisons\n    I won’t regurgitate the entire readme but some notable changes from Pyppeteer or Playwright include an overhauled network manager, Firefox (partial) support, an httpx session that shares browser state, and proxy support, including SOCKS proxies. There are loads of other small changes too, like making names more “pythonic” and restructuring the project.\n  \n    Feel free to give feedback and use it! I noted in the documentation some potential advantages over other solutions like Playwright (though I’m not deluded, Playwright is far superior still)."
},
{
    "title": "No title",
    "content": "Healthy-API\nWhat it does\n    Provide an easy interface for defining health checks for web applications written in Flask and FastAPI\n  Target Audience\n    For Python developers who are using Flask and FastAPI\n  Similar Packages\n\n\nhttps://pypi.org/project/fastapi-healthchecks/ - This looks more Object/class based, but appears to only handles FastAPI. Also allows you to put up a maintenance page\n  \n\n\nhttps://pypi.org/project/Flask-Meter/ - My older package which only handles Flask\n  \n\nBackground\n    When I was a lesser experienced developer, I ended up writing my own solutions for health checks as a  tour of the language ecosystem. It also gave me insight on how to write a package which could be shared & maintained. I recently updated an older package of mine which only worked for Flask apps. These days I've been writing more FastAPI. Enjoy!"
},
{
    "title": "No title",
    "content": "Slint is a declarative GUI toolkit to build native user interfaces for desktop and embedded applications. Sometime ago, we had announced that Slint will be supporting Python https://www.reddit.com/r/Python/comments/18mdpig/declarative_gui_for_python/\n\n    Today we are excited to share that we released Slint v1.5 introducing Pythonic Slint. Check out all the details at https://slint.dev/blog/slint-1.5-released.html 🚀\n  \n    Check out the repo at https://github.com/slint-ui/slint/tree/master/api/python#slint-python-alpha"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What it does:\n    BrowserForge is a smart browser header and fingerprint generator that mimics the frequency if different browsers, operating systems, and devices found in the wild.\n  Features\n\n\n    Uses a Bayesian generative network to mimic actual web traffic\n  \n\n\n    Extremely fast runtime (0.1-0.2 miliseconds)\n  \n\n\n    Easy and simple for humans to use\n  \n\n\n    Extensive customization options for browsers, operating systems, devices, locales, and HTTP version\n  \n\n\n    Injectors for Playwright and Pyppeteer\n  \n\n\n    Written with type safety\n  \n\n\nTarget audience: Anyone interested in webscraping\n  \nComparison: Other popular libraries such as fake-headers do not consider the frequencies of header values in the real world, and are often flagged by bot detectors for unusual traffic.\n  \n    See it here: https://github.com/daijro/browserforge\n\n    Credit to Apify's nodejs fingerprint-suite for the original logic!\n  \n    Hope you guys find it useful!"
},
{
    "title": "No title",
    "content": "Hello everyone,\n  \n    I built a V1 for a SaaS platform to create and manage OpenAI assistants. It is made to be white-labeled. The back-end is in Python (FastAPI).\n  \n    I decided to work on other projects. If the code can be helpful to you, please feel free to use it the way you want 🤝\n  \n    Link to the repo here 👉 github[dot]com/HenryObj/assistants"
},
{
    "title": "No title",
    "content": "Hello,I'm organizing a workshop next Tuesday, March 19, 2024, at 4 PM Pacific Time. During this workshop, instructors will build a real-time dashboard for analyzing pizza shop orders using Bytewax, Pinot, and Streamlit.For more details and to RSVP (attendance is free), please visit:https://bytewax.io/events/real-time-pizza-analyticsI believe this workshop is especially noteworthy for those interested, as I've personally enjoyed migrating the previous solution from Kafka Streams to Bytewax. The solution is now more Python-friendly, and the complexity of the codebase is significantly reduced. For me, it felt truly refreshing to rewrite it :DI am still in the process of updating the repository and will share it later, but here is the previous version:https://dev.startree.ai/docs/pinot/demo-apps/pizza-shop"
},
{
    "title": "No title",
    "content": "So I'm updating the dask.org homepage and want to include user quotes of people who Dask has had some positive impact.  If you have ...\n  \n\n\n    Like Dask or found Dask it easy or helpful in some way\n  \n\n\n    don't mind having your name and face on the dask.org website\n  \n\n\n    ... then can you leave a comment here with a quote that you wouldn't mind me putting online?  Bonus points if you can link to something like a personal homepage or linkedin page where I can scrape a tiny headshot.  For long quotes I may tighten up language a bit (let me know if you mind).\n  \n    And hey, if you want to say negative things here that's ok too!  (although I probably won't put them on the homepage 🙂)\n  \n    Thanks all,\n  \n    -matt!"
},
{
    "title": "No title",
    "content": "I was going crazy trying to compare Python cloud function hosts and started taking notes ... hope this helps someone, feedback welcome!\n  \nhttps://github.com/hbmartin/comparison-hosts-serverless-cloud-function-faas-for-python"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Created  a beginner guide to NiceGUI and how you can get started, NiceGUI can help you build UIs to Python apps and offers performance and flexibility, in case someone is interested there is an article and a video:https://www.bitdoze.com/nicegui-get-started/"
},
{
    "title": "No title",
    "content": "Official Event"
},
{
    "title": "No title",
    "content": "Python is packed with numerous command-line modules, one of them being smtpd. It allows you to run your own local SMTP server for testing emails.\n  \n    You can read more about it here in my blog post: Email Testing with Python's smtpd Module"
},
{
    "title": "No title",
    "content": "Hey everyone!\n  \n    I'm excited to share a simple yet effective tool I've developed that's all about enhancing your Python coding experience in VSCode. We all know how Docstrings are crucial for understanding and documenting our Python code, but they often blend into the background, treated as standard comments in VSCode. This can make them harder to read and differentiate from the rest of your code.\n  \n    That's why I created a Python Docstring Highlighter for VSCode that recognizes the main styles (Google, NumPy, and Sphinx). This extension not only makes your code more readable but also allows you to customize the highlighting to fit your theme, making your coding environment as comfortable and productive as possible.\n  \n    Whether you're documenting your own project or navigating through others', this extension is designed to make your life easier and your code more accessible. Give it a try and see the difference for yourself!\n  \nSource code\n\nInstall extension (VSCode)\n\n    Looking forward to your feedback and suggestions. Happy coding!"
},
{
    "title": "No title",
    "content": "Link\nGithub - learncpp_pdf\nWhat My Project Does\n    scrape htmls from learncpp.com and make it a pdf book\n  Target Audience\n    people who wants to learn cpp and would like to have a pdf copy of learncpp.com as well as people who wants to learn how to use asyncio and multiprocessing\n  Comparison\n\n\namalrajan/learncpp-download Too slow, since it does not make use of asyncio and multiprocessing\n  \n\n\nLearnCpp converter Difficult to use, you have to run different scripts manually\n  \n\nIntroduction\n    For thoese who might not familiar with learncpp.com, it is one of the most recommended and highly rated free online source for anyone who wants to learn cpp, but since it is hosted online and requires internet to access, it might not be as convenient as a pdf book for reading.\n  \n    the author specifically said that a PDF version of the site is not planned, and people should create the PDF version on their own, so I created this tool to help you easily make the PDF version on your own.\n  Quote from the author\n\n    Q: Is there a PDF version of this site available for offline viewing?\n  \n\n\n    Unfortunately, there is not. The site is able to stay free for everyone because we’re ad-sponsored -- that model simply doesn’t work in PDF format. You are welcome to convert pages from this website into PDF (or any other) format for your own private use, so long as you do not distribute them.\n  \nFeatures\n\n\n    Ultra fast, utilize concurrency for scraping and parallel for making PDF, the whole process is expected to finish within a few minutes.\n  \n\n\n    Rich cli interface showing realtime progress of the application\n  \n\n\n    Cached on fail, you can just re-run the application without worrying about redundant IO or calcualtion.\n  \n\n\n    This project is also meant to be a demonstration on usage of libs like pixi, multiprocessing and asyncio. Please share your thoughts on this, I'm all ears!"
},
{
    "title": "No title",
    "content": "While it is easy to use Python to turn an idea into a program, one will quickly run into bottlenecks that make their code less performant than they might want it to be. One such bottleneck is memory, of which Python consumes a lot compared to statically typed languages. Indeed, someone asking for advice on how to optimize their Python application online will likely receive the following advice: \"Rewrite it in Rust\". For obvious reasons, this is not very practical advice most of the time. Thus, we must make do with what we have: Python, and libraries written for Python.\n  \n    What follows is an exhibition of the memory model behind your Python application: How objects are allocated, where they are stored, and how they are eventually cleaned up.\n  \nhttps://codebeez.nl/blogs/the-memory-footprint-of-your-python-application/"
},
{
    "title": "No title",
    "content": "Exciting to see, after many years, serious work in enabling multithreading that takes advantage of multiple CPUs in a more effective way in Python. One step at a time: https://github.com/python/cpython/pull/116338"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hello, I made a Sudoku game in python (not really original). The difficult part for me was the user interface with curses library. A diagram is available to explain the algorithm used. The fame is compatible Linux, Mac and Windows.\n  \n    Link to the project here\n\nHow to install and play?\n$ pip install play-sudoku\n$ sudoku\nWhat My Project Does\n\n    Sudoku game, made with python & curses library. Play in the terminal. For Linux, Mac OS and Windows.\n  \nTarget Audience\n\n    Geek and people who don't want to work. Linux, Mac and Windows. Doesn't work on virtual terminal.\n  \nComparison\n\n    I just did it for fun. I try to make several games in python in terminal.\n  \n    Don't hesitate to let me know if you come across any errors or bugs. Thanks."
},
{
    "title": "No title",
    "content": "I'm currently building a program for internal use only to perform some calculations after receiving input from a Tkinter interface, loading and saving data locally. Matplotlib is used to create a graph for better visualisation of the output.\n  \n    libraries used: tkinter, json, os, math, matplotlib\n\n    From what I understand based on the PSFL licence for tkinter, os, json, math there should be no problem. I think matplotlib BSD licence should also be ok.\n  \n    Libraries haven't been changed at all but only used inside the program by calling functions. Software is for internal use only and not for sell.\n  \n    What are the limitations and what do I have to indicate other and the PSFL licence and Matplotlib BSD?"
},
{
    "title": "No title",
    "content": "Haystack 2.0 stable is live! Try it out for yourself: https://haystack.deepset.ai/blog/haystack-2-release/\n\n    Haystack is an open-source AI framework in Python for creating production-ready apps using LLMs and other language models. It's nearly 4 years old - we've been doing NLP and LLM engineering since before it was cool. 😎 and is model and database agnostic — you can use whatever tools make the most sense for your use case. These tools are combined into a full AI app with the use of Haystack's components and pipelines. The aim of the framework is to provide the structure for you to compose your own custom app.\n  \n\n    🚀 The pipeline architecture is a true graph, and I'll be presenting just that in this years PyCon USA too: Everything is a graph, including LLM Applications (and that's handy)\n\n\nContext about the 2.0 release:\n\n    Haystack was first officially released in 2020, when the forefront of NLP was mostly semantic search, retrieval, and extractive question-answering. Haystack 2.0 is a complete rewrite, but the underlying principle of composing components into flexible pipelines remains the same.The release has quite a few model providers, tracing and monitoring capabilities, and databases supported out of the box:For models (generative and embedding): OpenAI, Mistral, Cohere, Jina AI, Google AI, Vertex AI, Optimum (by hugging face), sentence transformers, Amazon Bedrock, Azure, Fast Embed, OllamaFor databases: Weaviate, Pinecone, Qdrant, Mongo DB, Astra DB, Neo4j, pgvector, Chroma, Elastic Search, OpenSearch...\n  \n\n    Hope you give it a try and let us know what your think! We have a quick start guide to get going: https://haystack.deepset.ai/overview/quick-start"
},
{
    "title": "No title",
    "content": "I am trying to build my own CMS in Python. It is supposed to be simple and useful for developers who like to tinker with the inner workings of their websites. Most mainstream content management systems like WordPress seem too complicated to play around with to me.\n  \n\nhttps://pypi.org/project/sapphirecms/\n\nhttps://github.com/SapphireCMS/SapphireCMS/\n\n\n    I am asking for help in the sense that I need to know the pain points and issues other developers face with the CMSs they use.\n  \n    This is also an open invitation for collaborators on the project. Your efforts will be credited."
},
{
    "title": "No title",
    "content": "Hey Everybody, I have seen Python used for many things and I am just wondering, for those who work with Python and another language, what is the best complimentary language for your area (or just in general in your opinion) and why?\n  \n    Is the language used to make faster libraries (like making a C/C++ library for a CPU intensive task)? Maybe you use a higher level language like C# or Java for an application and Python for some DS, AI/ML section? I am curious which languages work well with Python and why? Thanks!\n  \n    Edit: Thanks everyone for all of this info about languages that are useful with Python. It has been very informative and I will definitely be checking out some of these suggested companion languages. Thanks!"
},
{
    "title": "No title",
    "content": "deptry 0.14.0 was just released, bringing significant speed improvements: It is now up to 10 times faster than the previous release! 🚀\n  \n    For those unfamiliar with deptry; deptry is a command line tool to check for issues with dependencies in a Python project, such as unused or missing dependencies. [GitHub]\n  \n    For some benchmarks of the new release, see the release notes here.\n  \n    The performance improvement was achieved by leveraging Rust to parse the AST and extract the import statements from .py files, rather than using Python's ast module.\n  \n    The addition of Rust to the project also opens up doors for more optimizations in the future, so stay tuned!"
},
{
    "title": "No title",
    "content": "DSPy is a framework that aims to solve the fragility problem in language model (LM)-based applications by prioritizing programming over prompting. It allows you to recompile the entire pipeline to optimize it to your specific task — instead of repeating manual rounds of prompt engineering — whenever you change a component.\n  \nhttps://github.com/stanfordnlp/dspy"
},
{
    "title": "No title",
    "content": "Hello! Been working on this for awhile and excited to finally drop it.\n  \n    I forked Pyppeteer (no longer maintained) and revamped it, doing a good amount of refactoring and adding features along the way.\n  \n    Project link: https://github.com/michaeleveringham/mokr Documentation link: https://mokr.readthedocs.io/en/latest/index.html\nWhat My Project Does\n    Offers automated web browsing for both Chrome and Firefox.\n  Target Audience\n    Currently enthusiasts seeking to automate tests or web scrape. Eventually I’d like to offer this as production-ready but my test suite isn’t finished yet.\n  Comparisons\n    I won’t regurgitate the entire readme but some notable changes from Pyppeteer or Playwright include an overhauled network manager, Firefox (partial) support, an httpx session that shares browser state, and proxy support, including SOCKS proxies. There are loads of other small changes too, like making names more “pythonic” and restructuring the project.\n  \n    Feel free to give feedback and use it! I noted in the documentation some potential advantages over other solutions like Playwright (though I’m not deluded, Playwright is far superior still)."
},
{
    "title": "No title",
    "content": "Healthy-API\nWhat it does\n    Provide an easy interface for defining health checks for web applications written in Flask and FastAPI\n  Target Audience\n    For Python developers who are using Flask and FastAPI\n  Similar Packages\n\n\nhttps://pypi.org/project/fastapi-healthchecks/ - This looks more Object/class based, but appears to only handles FastAPI. Also allows you to put up a maintenance page\n  \n\n\nhttps://pypi.org/project/Flask-Meter/ - My older package which only handles Flask\n  \n\nBackground\n    When I was a lesser experienced developer, I ended up writing my own solutions for health checks as a  tour of the language ecosystem. It also gave me insight on how to write a package which could be shared & maintained. I recently updated an older package of mine which only worked for Flask apps. These days I've been writing more FastAPI. Enjoy!"
},
{
    "title": "No title",
    "content": "Slint is a declarative GUI toolkit to build native user interfaces for desktop and embedded applications. Sometime ago, we had announced that Slint will be supporting Python https://www.reddit.com/r/Python/comments/18mdpig/declarative_gui_for_python/\n\n    Today we are excited to share that we released Slint v1.5 introducing Pythonic Slint. Check out all the details at https://slint.dev/blog/slint-1.5-released.html 🚀\n  \n    Check out the repo at https://github.com/slint-ui/slint/tree/master/api/python#slint-python-alpha"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What it does:\n    BrowserForge is a smart browser header and fingerprint generator that mimics the frequency if different browsers, operating systems, and devices found in the wild.\n  Features\n\n\n    Uses a Bayesian generative network to mimic actual web traffic\n  \n\n\n    Extremely fast runtime (0.1-0.2 miliseconds)\n  \n\n\n    Easy and simple for humans to use\n  \n\n\n    Extensive customization options for browsers, operating systems, devices, locales, and HTTP version\n  \n\n\n    Injectors for Playwright and Pyppeteer\n  \n\n\n    Written with type safety\n  \n\n\nTarget audience: Anyone interested in webscraping\n  \nComparison: Other popular libraries such as fake-headers do not consider the frequencies of header values in the real world, and are often flagged by bot detectors for unusual traffic.\n  \n    See it here: https://github.com/daijro/browserforge\n\n    Credit to Apify's nodejs fingerprint-suite for the original logic!\n  \n    Hope you guys find it useful!"
},
{
    "title": "No title",
    "content": "Hello everyone,\n  \n    I built a V1 for a SaaS platform to create and manage OpenAI assistants. It is made to be white-labeled. The back-end is in Python (FastAPI).\n  \n    I decided to work on other projects. If the code can be helpful to you, please feel free to use it the way you want 🤝\n  \n    Link to the repo here 👉 github[dot]com/HenryObj/assistants"
},
{
    "title": "No title",
    "content": "Hello,I'm organizing a workshop next Tuesday, March 19, 2024, at 4 PM Pacific Time. During this workshop, instructors will build a real-time dashboard for analyzing pizza shop orders using Bytewax, Pinot, and Streamlit.For more details and to RSVP (attendance is free), please visit:https://bytewax.io/events/real-time-pizza-analyticsI believe this workshop is especially noteworthy for those interested, as I've personally enjoyed migrating the previous solution from Kafka Streams to Bytewax. The solution is now more Python-friendly, and the complexity of the codebase is significantly reduced. For me, it felt truly refreshing to rewrite it :DI am still in the process of updating the repository and will share it later, but here is the previous version:https://dev.startree.ai/docs/pinot/demo-apps/pizza-shop"
},
{
    "title": "No title",
    "content": "So I'm updating the dask.org homepage and want to include user quotes of people who Dask has had some positive impact.  If you have ...\n  \n\n\n    Like Dask or found Dask it easy or helpful in some way\n  \n\n\n    don't mind having your name and face on the dask.org website\n  \n\n\n    ... then can you leave a comment here with a quote that you wouldn't mind me putting online?  Bonus points if you can link to something like a personal homepage or linkedin page where I can scrape a tiny headshot.  For long quotes I may tighten up language a bit (let me know if you mind).\n  \n    And hey, if you want to say negative things here that's ok too!  (although I probably won't put them on the homepage 🙂)\n  \n    Thanks all,\n  \n    -matt!"
},
{
    "title": "No title",
    "content": "I was going crazy trying to compare Python cloud function hosts and started taking notes ... hope this helps someone, feedback welcome!\n  \nhttps://github.com/hbmartin/comparison-hosts-serverless-cloud-function-faas-for-python"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Created  a beginner guide to NiceGUI and how you can get started, NiceGUI can help you build UIs to Python apps and offers performance and flexibility, in case someone is interested there is an article and a video:https://www.bitdoze.com/nicegui-get-started/"
},
{
    "title": "No title",
    "content": "Official Event"
},
{
    "title": "No title",
    "content": "Python is packed with numerous command-line modules, one of them being smtpd. It allows you to run your own local SMTP server for testing emails.\n  \n    You can read more about it here in my blog post: Email Testing with Python's smtpd Module"
},
{
    "title": "No title",
    "content": "Hey everyone!\n  \n    I'm excited to share a simple yet effective tool I've developed that's all about enhancing your Python coding experience in VSCode. We all know how Docstrings are crucial for understanding and documenting our Python code, but they often blend into the background, treated as standard comments in VSCode. This can make them harder to read and differentiate from the rest of your code.\n  \n    That's why I created a Python Docstring Highlighter for VSCode that recognizes the main styles (Google, NumPy, and Sphinx). This extension not only makes your code more readable but also allows you to customize the highlighting to fit your theme, making your coding environment as comfortable and productive as possible.\n  \n    Whether you're documenting your own project or navigating through others', this extension is designed to make your life easier and your code more accessible. Give it a try and see the difference for yourself!\n  \nSource code\n\nInstall extension (VSCode)\n\n    Looking forward to your feedback and suggestions. Happy coding!"
},
{
    "title": "No title",
    "content": "Link\nGithub - learncpp_pdf\nWhat My Project Does\n    scrape htmls from learncpp.com and make it a pdf book\n  Target Audience\n    people who wants to learn cpp and would like to have a pdf copy of learncpp.com as well as people who wants to learn how to use asyncio and multiprocessing\n  Comparison\n\n\namalrajan/learncpp-download Too slow, since it does not make use of asyncio and multiprocessing\n  \n\n\nLearnCpp converter Difficult to use, you have to run different scripts manually\n  \n\nIntroduction\n    For thoese who might not familiar with learncpp.com, it is one of the most recommended and highly rated free online source for anyone who wants to learn cpp, but since it is hosted online and requires internet to access, it might not be as convenient as a pdf book for reading.\n  \n    the author specifically said that a PDF version of the site is not planned, and people should create the PDF version on their own, so I created this tool to help you easily make the PDF version on your own.\n  Quote from the author\n\n    Q: Is there a PDF version of this site available for offline viewing?\n  \n\n\n    Unfortunately, there is not. The site is able to stay free for everyone because we’re ad-sponsored -- that model simply doesn’t work in PDF format. You are welcome to convert pages from this website into PDF (or any other) format for your own private use, so long as you do not distribute them.\n  \nFeatures\n\n\n    Ultra fast, utilize concurrency for scraping and parallel for making PDF, the whole process is expected to finish within a few minutes.\n  \n\n\n    Rich cli interface showing realtime progress of the application\n  \n\n\n    Cached on fail, you can just re-run the application without worrying about redundant IO or calcualtion.\n  \n\n\n    This project is also meant to be a demonstration on usage of libs like pixi, multiprocessing and asyncio. Please share your thoughts on this, I'm all ears!"
},
{
    "title": "No title",
    "content": "While it is easy to use Python to turn an idea into a program, one will quickly run into bottlenecks that make their code less performant than they might want it to be. One such bottleneck is memory, of which Python consumes a lot compared to statically typed languages. Indeed, someone asking for advice on how to optimize their Python application online will likely receive the following advice: \"Rewrite it in Rust\". For obvious reasons, this is not very practical advice most of the time. Thus, we must make do with what we have: Python, and libraries written for Python.\n  \n    What follows is an exhibition of the memory model behind your Python application: How objects are allocated, where they are stored, and how they are eventually cleaned up.\n  \nhttps://codebeez.nl/blogs/the-memory-footprint-of-your-python-application/"
},
{
    "title": "No title",
    "content": "Exciting to see, after many years, serious work in enabling multithreading that takes advantage of multiple CPUs in a more effective way in Python. One step at a time: https://github.com/python/cpython/pull/116338"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hello, I made a Sudoku game in python (not really original). The difficult part for me was the user interface with curses library. A diagram is available to explain the algorithm used. The fame is compatible Linux, Mac and Windows.\n  \n    Link to the project here\n\nHow to install and play?\n$ pip install play-sudoku\n$ sudoku\nWhat My Project Does\n\n    Sudoku game, made with python & curses library. Play in the terminal. For Linux, Mac OS and Windows.\n  \nTarget Audience\n\n    Geek and people who don't want to work. Linux, Mac and Windows. Doesn't work on virtual terminal.\n  \nComparison\n\n    I just did it for fun. I try to make several games in python in terminal.\n  \n    Don't hesitate to let me know if you come across any errors or bugs. Thanks."
},
{
    "title": "No title",
    "content": "I'm currently building a program for internal use only to perform some calculations after receiving input from a Tkinter interface, loading and saving data locally. Matplotlib is used to create a graph for better visualisation of the output.\n  \n    libraries used: tkinter, json, os, math, matplotlib\n\n    From what I understand based on the PSFL licence for tkinter, os, json, math there should be no problem. I think matplotlib BSD licence should also be ok.\n  \n    Libraries haven't been changed at all but only used inside the program by calling functions. Software is for internal use only and not for sell.\n  \n    What are the limitations and what do I have to indicate other and the PSFL licence and Matplotlib BSD?"
},
{
    "title": "No title",
    "content": "Haystack 2.0 stable is live! Try it out for yourself: https://haystack.deepset.ai/blog/haystack-2-release/\n\n    Haystack is an open-source AI framework in Python for creating production-ready apps using LLMs and other language models. It's nearly 4 years old - we've been doing NLP and LLM engineering since before it was cool. 😎 and is model and database agnostic — you can use whatever tools make the most sense for your use case. These tools are combined into a full AI app with the use of Haystack's components and pipelines. The aim of the framework is to provide the structure for you to compose your own custom app.\n  \n\n    🚀 The pipeline architecture is a true graph, and I'll be presenting just that in this years PyCon USA too: Everything is a graph, including LLM Applications (and that's handy)\n\n\nContext about the 2.0 release:\n\n    Haystack was first officially released in 2020, when the forefront of NLP was mostly semantic search, retrieval, and extractive question-answering. Haystack 2.0 is a complete rewrite, but the underlying principle of composing components into flexible pipelines remains the same.The release has quite a few model providers, tracing and monitoring capabilities, and databases supported out of the box:For models (generative and embedding): OpenAI, Mistral, Cohere, Jina AI, Google AI, Vertex AI, Optimum (by hugging face), sentence transformers, Amazon Bedrock, Azure, Fast Embed, OllamaFor databases: Weaviate, Pinecone, Qdrant, Mongo DB, Astra DB, Neo4j, pgvector, Chroma, Elastic Search, OpenSearch...\n  \n\n    Hope you give it a try and let us know what your think! We have a quick start guide to get going: https://haystack.deepset.ai/overview/quick-start"
},
{
    "title": "No title",
    "content": "I am trying to build my own CMS in Python. It is supposed to be simple and useful for developers who like to tinker with the inner workings of their websites. Most mainstream content management systems like WordPress seem too complicated to play around with to me.\n  \n\nhttps://pypi.org/project/sapphirecms/\n\nhttps://github.com/SapphireCMS/SapphireCMS/\n\n\n    I am asking for help in the sense that I need to know the pain points and issues other developers face with the CMSs they use.\n  \n    This is also an open invitation for collaborators on the project. Your efforts will be credited."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hello everyone,\n  \n    I enjoy solving CTFs, and I often have thought it would be interesting to have CTF events in other domain besides security. Last year, I organized a small Python-themed CTF event for learners at a Korean conference. The response from participants was positive, so I decided to open it for everyone.\n  \nhttps://pyctf.seung.de\n\n    Anyone can read the challenges, but registration is required to submit flags. Since there's no email verification, feel free to try it without concerns.\n  \n    I made it for beginners and learners, so the challenges are generally easy, but there are also challenges for experienced people. I hope you all have fun. There may be mistakes in the moving to English ver., so I would greatly appreciate bug report or any feedback to help improve, and I will be more happier if I get to see more Python CTF events in the future."
},
{
    "title": "No title",
    "content": "https://www.youtube.com/watch?v=VtHtAbOSp8E\n\n    Hey everyone 👋\n  \n    I’m trying to make a habit out of finishing projects and posting the progress on YouTube\n  \n    Any feedback is more than welcome 🙏\n  \n    What my project does\n  \n\n\n    reads on screen\n  \n\n\n    mocks mouse input\n  \n\n\n    finds all words in a squaredle based on a word dictionary\n  \n\n\n    Target audience\n  \n\n\n    it’s s just a toy project\n  \n\n\n    Link to the bot written in Python\n  \n\n\nhttps://github.com/2BytesGoat/squaredle-solver"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "GitLab snippet\n\n\"\"\"v2 Exception instance is a part of the Result\"\"\"\n\nfrom typing import Any, Optional, Tuple\nfrom os import path\n\nResult = Tuple[Optional[Any], Optional[Exception]]\n\n\ndef try_result(result: Result) -> Optional[Any]:\n    # Pre-condition\n    if result is None:\n        raise ValueError('result is None')\n\n    value, error = result\n    if error is not None:\n        raise error\n    return value\n\n\ndef summarize_file_ex(file_path: str) -> Result:\n    # Pre-conditions\n    if file_path is None:\n        return None, ValueError('file_path is None')\n    if file_path.strip() == '':\n        return None, ValueError('file path is blank')\n    if not path.exists(file_path):\n        return None, FileExistsError(f\"file_path \\\"{file_path}\\\" not found\")\n\n    total = 0.0\n    with open(file_path, 'r', encoding='utf-8') as file:\n        for line in file:\n            try:\n                stripped_line: str = line.strip()\n                num = float(stripped_line)\n            except ValueError:\n                return None, RuntimeError(f\"error converting line {line} value \\\"{stripped_line}\\\" to float in file \\\"{file_path}\\\".\")\n            total += num\n    return total, None\n\n\nif __name__ == '__main__':\n    file_path: str = path.join('data', 'file_001.txt')\n    total: float = try_result(summarize_file_ex(file_path=file_path))\n    print(f\"total = {total}\")\n\n    ChatGPT4 had a mixed response:\n  \n\n\n    This approach can make your code more verbose, and it's somewhat unconventional in the Python community because it goes against the typical idiomatic way of using exceptions for error handling. Python's exceptions are powerful and are the standard way of handling errors, so using Go-style error handling may not leverage the full capabilities and syntactical sugar of Python, like the try-except blocks.\n  \n    However, if you prefer this explicit style of error handling for certain cases, or if you're working in an environment where the explicitness and clarity of error handling are prioritized, this pattern can be quite useful. Just keep in mind that it can lead to more boilerplate code and might be unfamiliar to other Python developers."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Good day everyone!\n  \n    I'm here to share a project I have been working on: thread.\n  What is thread?\n    Thread is a lightweight and performant library which extends the built-in threading module, enabling you to fetch return values, decorate functions and run functions concurrently.\n  Comparison\n    9 times out of 10, what you are looking for are other built-in modules like multiprocessing, asyncio or concurrent.futures. Thread is not a replacement for these, but is for improving your experience for that 1 time.\n  Target audience\n    Thread is targeted at developers who want low-level control over threads without giving up features of other similar libraries.\n  Thanks for your time!\n    If you'd like to contribute or check out the project, you can do so at these links: Source code, Documentation, PyPI"
},
{
    "title": "No title",
    "content": "I started writing some code to interact with Salesforce and pagerduty. So I found simple-salesforce and pypdras and set to work.\n  \n    After what felt like both too long and not long at all I started to get annoyed by both libraries. They (reasonably enough) worked totally different to each other and documentation would be poor.\n  \n    Alternatively they both wrap an HTTP REST API using the requests module, which has excellent documentation as do their official APIs. Well, relatively speaking of course.\n  \n    Whilst I head to dog a little into he authentication mechanisms and such it was very quickly massively easier to do the API calls myself and be able to program against them in very similar ways. I even decided to use requests_cache to add implicit caching to these backends and a couple of others I also started to use.\n  \n    Left me me wondering why anyone would bother making some of these sorts of wrapper libraries in the first place. So often they seem to be nothing but a hindrance.\n  \n    I'm sure I've had similar experiences with other non API libraries too, a library might make something 5 lines simpler but at the cost of another import, another pip installation on deployment... Often they seem to be more hassle than they're worth!"
},
{
    "title": "No title",
    "content": "What is the combined size of your Python codebase?\n  \n\n\n    1k-50k lines (Small)\n  \n\n\n    50k-300k lines (Medium)\n  \n\n\n    301k-1M lines (Large)\n  \n\n\n    1M+ lines (Extra Large)\n  \n\n\n    For the reference:- Requests (https://github.com/psf/requests) is 17k lines- Flask (https://github.com/pallets/flask) is 34k lines- urllib3 (https://github.com/urllib3/urllib3) is 35k lines- boto3 (https://github.com/boto/boto3) is 70k lines- black (https://github.com/psf/black) is 131k lines- pip (https://github.com/pypa/pip) is 240k lines- Ansible (https://github.com/ansible/ansible) is 366k lines- SciKit Learn (https://github.com/scikit-learn/scikit-learn) is 471k lines- Django (https://github.com/django/django) is 668k lines- NumPy (https://github.com/numpy/numpy) is 757k lines- Airflow (https://github.com/apache/airflow) is 1.2M linesShould the bucket sizes be adjusted?"
},
{
    "title": "No title",
    "content": "My first attempt to get some help from Github Copilot:\n  \n\n    Me: write a match case statement in python to match the first character of a string is a semicolon\n  \n\n\n    Copilot: Python doesn't have a built-in match or case statement like some other languages (e.g., JavaScript, Swift, Rust). However, you can achieve similar functionality using...\n  \n\n    Strangely, it finally gave me an answer when I made a small change to my prompt:\n  \n\n    Me: write a match statement in python to match the first character of a string is a semicolon\n  \n\n\n    Copilot: Python 3.10 introduces the structural pattern matching in the form of the match statement. Here's how you can use it to check if the first character of a string is a semicolon:...\n  \n\n    I almost gave up after the first response that was blatantly false. Fortunately I something that looks like reasonable syntax...but then I haven't tried it yet, so that might be a lie, too."
},
{
    "title": "No title",
    "content": "Happy weekend every one!\n  \n    A while ago I showcased stockdex in this community. Since then, I've improved data retrieval performance by integrating yahoo finance API besides web scarping from sources. These tweaks have made more detailed data available in returned pandas dataframes while making the queries 5 times faster in average.\n  What My Project Does\n    A lightweight python package designed for the efficient retrieval of financial data from various sources like Yahoo Finance, Nasdaq, and Digrin. It is similar to yfinance python package.\n  Comparison with yfinance\n    It allows data retrieval from more sources, not just yahoo finance API. It also provides more data, for example yfinance returns dividends data for the last 5 years at max, but stockdex returns the entire dividends history.\n  Target Audience\n    The package is targeted at people who are interested in financial analysis using python.\n  \n    For more details or to contribute, feel free to visit the links below:\n  \nGithub Repo Link\n\nPypi link"
},
{
    "title": "No title",
    "content": "Official Event"
},
{
    "title": "No title",
    "content": "MIT's Data-to-AI lab created sdv (synthetic data vault) to generate _realistic_ fake data from real, production data.\n  \n    - CTGAN paper: https://dspace.mit.edu/handle/1721.1/128349\n\n    - Vine Copula Models paper: https://dai.lids.mit.edu/wp-content/uploads/2019/01/1812.01226.pdf\n\n\n    Code example:\n  from sdv.lite import SingleTablePreset\nfrom sdv.datasets import demo\nimport pandas as pd\n\nreal_data, metadata = download_demo('single_table', 'adult')\nsynthesizer = SingleTablePreset(metadata, name='FAST_ML')\n\nsynthetic_data = synthesizer.sample(num_rows=500)\n# Once the model is trained, you can generate as many \n# look-alike rows of data as you'd like even if original data is smaller\nsynthetic_data_more = synthesizer.sample(num_rows=10_000)\n    Link to library: https://github.com/sdv-dev/SDV"
},
{
    "title": "No title",
    "content": "A while ago I started experimenting with f-strings a bit, how to make them safe to use for HTML rendering without safety issues, basically I tried to make p(f\"Hello {b(\"World\")}! work. I also experimented with HTMX along the way. It ended as an open-source library.\n  \n    So today I'm releasing Ludic 0.1.0: https://github.com/paveldedik/ludic\n\n    I'm looking for any feedback, is it something you would find useful?\n  \n    I'm still not sure how flexible this approach is. I know there are quite a lot of similar libraries, like reflex (I started researching when I already had quite a bit of code written as it was quite fun to write it).\n  \n    Nevertheless, I'm not done experimenting with it, I'm now trying to implement examples from htmx.org to write them in pure Python using the library and see how it goes.\n  \nTL;DR looking for feedback for my library."
},
{
    "title": "No title",
    "content": "URL: https://github.com/Shakakai/typedtemplate\n\n    I'm in the process of releasing a few of my internal libraries for building LLM-based apps. The first release is typedtemplate, a library that allows you to use Pydantic syntax to describe the data required by a template to render.\n  What does it currently do?\n\n\n    Documentation: Provides a class that you can easily generate documentation for. This means all your normal python documentation generators and tooling can provide useful docs for your templates that are easily consumable by your team.\n  \n\n\n    IDE/Copilot Support: Pydantic classes provide an great developer experience in the IDE with helpful autocomplete and a structure that Copilot understands well.\n  \n\n\n    Validation: Runtime validation that the data conforms to the type definitions.\n  \n\nWhy did I create this?\n    I'm working with LLMs everyday now. I have hundreds of LLM prompts (and countless variants) in my project. If you are like me, it doesn't take long after creating a template to forget the shape of the data required to make it render properly. I'm lazy. This library lets me forget more things and lean on my tooling.\n  Target Audience\n    I'm currently using this in production and my company is supporting it.\n  Comparison\n    I actually don't know of a library that provides a type interface around string templates. I'm sure one exists.\n  Notes\n\n\n    Currently works with Jinja2 and Django templating engines. If you need a different engine, just ask (quick to implement).\n  \n\n\n    Was built to play nice with any framework.\n  \n\n\n    Next library coming is a Non-OpenAI interface to LLMs that feels good. Uses LiteLLM under the hood to support almost all available LLMs."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Today I felt bored, so I made a program to do FizzBuzz in the most obfuscated way possible:\n  def d(i,m):return lambda x:m if x%i==0else\"\"\ndef f(n,a):return str(n)if len((m:=(''.join([l(n)for l in a]))))==0else m\ndef r(n):return f(n,[d(3,'\\x46\\x69\\x7a\\x7a'),d(5,'\\x42\\x75\\x7a\\x7a')])\ndef p(l):print(r(l[0]));return None if len(l)==1else p(l[1:])\np([n for n in range(1, 101)]) # Runs it\n    And here is the deobfuscated version:\n  def divisibility_lambda_creator(divisor, message):\n    return lambda num: message if num % divisor == 0 else \"\"\n    \ndef fizzbuzzify(num, lambdas):\n    out = ''.join([l(num)for l in lambdas])\n    if len(out) == 0:\n        return str(num)\n    else:\n        return out\n    \ndef run(num):\n    return fizzbuzzify(num,[divisibility_lambda_creator(3, \"Fizz\"),divisibility_lambda_creator(5,'Buzz')])\n    \ndef printer(nums):\n    print(run(nums[0]))\n    if len(nums) != 1:\n        printer(nums[1:])\n    \nprinter([n for n in range(1, 101)])\n    It is also really extendible: You can add  7, \"Bazz\" by doing:\n  - def r(n):return f(n,[d(3,'\\x46\\x69\\x7a\\x7a'),d(5,'\\x42\\x75\\x7a\\x7a')])\n+ def r(n):return f(n,[d(3,'\\x46\\x69\\x7a\\x7a'),d(5,'\\x42\\x75\\x7a\\x7a'),d(7,'\\x42\\x61\\x7a\\x7a')])\n    If anyone has any way to obfuscate it further that'd be cool!"
},
{
    "title": "No title",
    "content": "What my project doesUiWizard allows any python developer to create a website using python.\n  \n    I wanted to have a tool that I could use for my own personal projects with minimal exposure to the underlying technologies like JavaScript, CSS and HTML.\n  \n    This however does not mean that it is not possible to use these tools.\n  \n    UiWizard is made using FastAPI, Tailwind CSS, DaisyUI, HTMX as the major libraries.\n  \nTarget audience\n\n    Anyone interested in creating a website. I currently would not recommend it for a production use-case in the current state but as it is with anything I'm not going to stop you.\n  \n    There are still some minor inconsistencies in the library API.\n  \nComparison\n\n    This project mostly resembles NiceGUI as it was inspired by it. The feature set provided by NiceGUI is a lot bigger but the main difference is the approach to handling interactivity. NiceGUI uses websockets and client side to render its interface. UiWizard uses server side rendering and HTMX for interactivity instead.\n  \n    One difference that I think is important is NiceGUI uses websockets and any change or interactive element on the site triggers the user defined function in the code using events. This means there is only one request associated with the initial websocket connection. When a user interacts with UiWizard it triggers a new request each time, which means each function is an endpoint with access to the new request if needed.\n  \nSource code\n\n    This repository contains the source and some examples\n  \nhttps://github.com/Declow/uiwiz\n\nDocs\n\n    Some docs can be found here https://ui-wizard.comThis website is made using UiWizard\n  \n    I started this project because I wanted to learn more about web development and have a tool I liked to work with."
},
{
    "title": "No title",
    "content": "DukPy is a lightweight and easy to use JavaScript interpreter for Python. It comes with builtin support for various transpilers like SCSS, Less and React JSX thus allowing to build web apps in pure python without the need for NodeJS\n  \n    Release 0.3.1 finally added support for Python 3.12 and provides prebuilt wheels."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hello, Python community! I'm excited to share a project I've been working on called PyPixelStream. This is an open-source live streaming software, specifically designed for low-resolution LED panels. Inspired by OBS (Open Broadcaster Software), PyPixelStream aims to simplify the management and broadcasting of content to LED screens, addressing the unique challenges presented by low-resolution visual media.What Does PyPixelStream Do?PyPixelStream enables live streaming to low-resolution LED screens, providing a cross-platform (Windows, Linux, macOS) environment to manage and broadcast visual content in a way that's optimized for these displays. It offers a variety of media sources (images, videos, webcam, screen capture, GIFs) and customizable text sources (clocks, timers, dates, cryptocurrency prices), along with dynamic effects and extendable filters to adjust brightness, contrast, apply chroma key, and more.Target AudienceThis project is aimed at technology enthusiasts, developers, visual artists, and anyone interested in exploring new ways to interact with low-resolution visual media. PyPixelStream is perfect for use in personal projects, artistic experimentation, or even in professional applications where LED displays play a central role.Comparison With Existing AlternativesUnlike other streaming solutions, PyPixelStream is specifically optimized for low-resolution LED panels, offering detailed customization and control over the broadcast content. While OBS is a powerful tool for live streaming in general, PyPixelStream focuses on the efficiency and adaptability needed for low-resolution media, ensuring that performance and quality are not compromised, even on limited-resource devices like the Raspberry Pi.This is an open-source project, and I'm eager to make it as accessible and versatile as possible! Whether you're interested in contributing code, suggesting features, or just providing feedback, your input would be greatly appreciated. Check out the project on GitHub here: https://github.com/offerrall/PyPixelStream"
},
{
    "title": "No title",
    "content": "I'm active in a SaaS community, so there are a lot of conversations about building multi-tenant apps. And FastAPI is super popular.  I didn't see a lot of examples though...\n  \n    So I created one:\n  \n    Example: https://github.com/niledatabase/niledatabase/tree/main/examples/quickstart/pythonVideo tutorial: https://youtu.be/Axl63TUf2bcWritten tutorial: https://www.thenile.dev/docs/getting-started/languages/python\n\n    (Disclaimer: The example uses Nile, a serverless Postgres that I'm working on. But it can be modified to work on any Postgres).\n  \n    Feedback is welcome!"
},
{
    "title": "No title",
    "content": "No, I'm not saying what you probably think I am. Keep reading: I'll be brief.\n  \n    Here's what I found today in one of the tests:\n  for bad_fp in _bad_fps:\n    with pytest.raises(BadArgumentError):\n        follow_path(bad_fp, '.')\n        follow_path('.', bad_fp)\n    The correct code should've been\n  for bad_fp in _bad_fps:\n    with pytest.raises(BadArgumentError):\n        follow_path(bad_fp, '.')\n    with pytest.raises(BadArgumentError):\n        follow_path('.', bad_fp)\n    Is that a stupid mistake? Yes, it is, but stupid mistakes do happen, unfortunately.\n  \n    How did I catch it? The coverage of the testing file wasn't 100% like it usually is! I got suspicious and I saw that there was a glaring red line (vscode, coverage gutter) on the second call to follow_path.\n  \n    P.S. In case you're wondering, the code uses a custom hierarchy of exceptions instead of reusing the standard ones. The code is internal and we like it that way :)"
},
{
    "title": "No title",
    "content": "Hello r/Python community!\n  \n    I'm excited to share with you XposedOrNot, an open-source project built using Python to improve how we monitor data breaches. This project isn't just about enhancing security; it's about utilizing the power and flexibility of Python to address one of the most pressing issues in the digital world today.\n  What My Project Does:\n    XposedOrNot is an open-source tool built using Python, designed to enhance data breach monitoring. By leveraging the simplicity and versatility of Python, it offers a set of comprehensive features aimed at improving security practices and data breach awareness. The tool includes a dashboard with a risk score for emails, data breach impact analysis and analytics, visualization tools, and categorized data for better navigation and understanding of breach data.\n  Target Audience:\n    This project is geared towards developers, security professionals, and anyone interested in contributing to or utilizing security tools. It's suitable for both educational purposes and as a practical tool for monitoring data breaches in a production environment.\n  Comparison:\n    Unlike many commercial solutions that may be closed source and require payment for full features, XposedOrNot stands out because:\n  \n\n\nBuilt with Python: It leverages Python for its development, making it easily accessible for contributions and customization by developers familiar with Python.\n  \n\n\n100% Open Source: The project invites open collaboration and contribution, emphasizing transparency and community-driven development.\n  \n\n\nFree API Access & API Playground: It provides open API access and a playground for users to experiment and integrate the tool's capabilities into their projects.\n  \n\n\nComprehensive Features: Offers in-depth insights with its dashboard, analytics, and visualization tools specifically designed to navigate and understand data breaches effectively.\n  \n\n\nCommunity-Driven Development: Encourages contributions and feedback with an open roadmap, recognizing contributors to foster a collaborative development environment.\n  \n\nWhat’s Next & How to Support:\n    I'm inviting the community to help shape the future of data breach monitoring by contributing ideas, code, or feedback.\n  \n    Your support is crucial. Explore the project on GitHub, where your stars help increase visibility and encourage more contributions.\n  \n    Let's use the power of Python to improve data breach monitoring together!"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hello everyone,\n  \n    I enjoy solving CTFs, and I often have thought it would be interesting to have CTF events in other domain besides security. Last year, I organized a small Python-themed CTF event for learners at a Korean conference. The response from participants was positive, so I decided to open it for everyone.\n  \nhttps://pyctf.seung.de\n\n    Anyone can read the challenges, but registration is required to submit flags. Since there's no email verification, feel free to try it without concerns.\n  \n    I made it for beginners and learners, so the challenges are generally easy, but there are also challenges for experienced people. I hope you all have fun. There may be mistakes in the moving to English ver., so I would greatly appreciate bug report or any feedback to help improve, and I will be more happier if I get to see more Python CTF events in the future."
},
{
    "title": "No title",
    "content": "https://www.youtube.com/watch?v=VtHtAbOSp8E\n\n    Hey everyone 👋\n  \n    I’m trying to make a habit out of finishing projects and posting the progress on YouTube\n  \n    Any feedback is more than welcome 🙏\n  \n    What my project does\n  \n\n\n    reads on screen\n  \n\n\n    mocks mouse input\n  \n\n\n    finds all words in a squaredle based on a word dictionary\n  \n\n\n    Target audience\n  \n\n\n    it’s s just a toy project\n  \n\n\n    Link to the bot written in Python\n  \n\n\nhttps://github.com/2BytesGoat/squaredle-solver"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "GitLab snippet\n\n\"\"\"v2 Exception instance is a part of the Result\"\"\"\n\nfrom typing import Any, Optional, Tuple\nfrom os import path\n\nResult = Tuple[Optional[Any], Optional[Exception]]\n\n\ndef try_result(result: Result) -> Optional[Any]:\n    # Pre-condition\n    if result is None:\n        raise ValueError('result is None')\n\n    value, error = result\n    if error is not None:\n        raise error\n    return value\n\n\ndef summarize_file_ex(file_path: str) -> Result:\n    # Pre-conditions\n    if file_path is None:\n        return None, ValueError('file_path is None')\n    if file_path.strip() == '':\n        return None, ValueError('file path is blank')\n    if not path.exists(file_path):\n        return None, FileExistsError(f\"file_path \\\"{file_path}\\\" not found\")\n\n    total = 0.0\n    with open(file_path, 'r', encoding='utf-8') as file:\n        for line in file:\n            try:\n                stripped_line: str = line.strip()\n                num = float(stripped_line)\n            except ValueError:\n                return None, RuntimeError(f\"error converting line {line} value \\\"{stripped_line}\\\" to float in file \\\"{file_path}\\\".\")\n            total += num\n    return total, None\n\n\nif __name__ == '__main__':\n    file_path: str = path.join('data', 'file_001.txt')\n    total: float = try_result(summarize_file_ex(file_path=file_path))\n    print(f\"total = {total}\")\n\n    ChatGPT4 had a mixed response:\n  \n\n\n    This approach can make your code more verbose, and it's somewhat unconventional in the Python community because it goes against the typical idiomatic way of using exceptions for error handling. Python's exceptions are powerful and are the standard way of handling errors, so using Go-style error handling may not leverage the full capabilities and syntactical sugar of Python, like the try-except blocks.\n  \n    However, if you prefer this explicit style of error handling for certain cases, or if you're working in an environment where the explicitness and clarity of error handling are prioritized, this pattern can be quite useful. Just keep in mind that it can lead to more boilerplate code and might be unfamiliar to other Python developers."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Good day everyone!\n  \n    I'm here to share a project I have been working on: thread.\n  What is thread?\n    Thread is a lightweight and performant library which extends the built-in threading module, enabling you to fetch return values, decorate functions and run functions concurrently.\n  Comparison\n    9 times out of 10, what you are looking for are other built-in modules like multiprocessing, asyncio or concurrent.futures. Thread is not a replacement for these, but is for improving your experience for that 1 time.\n  Target audience\n    Thread is targeted at developers who want low-level control over threads without giving up features of other similar libraries.\n  Thanks for your time!\n    If you'd like to contribute or check out the project, you can do so at these links: Source code, Documentation, PyPI"
},
{
    "title": "No title",
    "content": "I started writing some code to interact with Salesforce and pagerduty. So I found simple-salesforce and pypdras and set to work.\n  \n    After what felt like both too long and not long at all I started to get annoyed by both libraries. They (reasonably enough) worked totally different to each other and documentation would be poor.\n  \n    Alternatively they both wrap an HTTP REST API using the requests module, which has excellent documentation as do their official APIs. Well, relatively speaking of course.\n  \n    Whilst I head to dog a little into he authentication mechanisms and such it was very quickly massively easier to do the API calls myself and be able to program against them in very similar ways. I even decided to use requests_cache to add implicit caching to these backends and a couple of others I also started to use.\n  \n    Left me me wondering why anyone would bother making some of these sorts of wrapper libraries in the first place. So often they seem to be nothing but a hindrance.\n  \n    I'm sure I've had similar experiences with other non API libraries too, a library might make something 5 lines simpler but at the cost of another import, another pip installation on deployment... Often they seem to be more hassle than they're worth!"
},
{
    "title": "No title",
    "content": "What is the combined size of your Python codebase?\n  \n\n\n    1k-50k lines (Small)\n  \n\n\n    50k-300k lines (Medium)\n  \n\n\n    301k-1M lines (Large)\n  \n\n\n    1M+ lines (Extra Large)\n  \n\n\n    For the reference:- Requests (https://github.com/psf/requests) is 17k lines- Flask (https://github.com/pallets/flask) is 34k lines- urllib3 (https://github.com/urllib3/urllib3) is 35k lines- boto3 (https://github.com/boto/boto3) is 70k lines- black (https://github.com/psf/black) is 131k lines- pip (https://github.com/pypa/pip) is 240k lines- Ansible (https://github.com/ansible/ansible) is 366k lines- SciKit Learn (https://github.com/scikit-learn/scikit-learn) is 471k lines- Django (https://github.com/django/django) is 668k lines- NumPy (https://github.com/numpy/numpy) is 757k lines- Airflow (https://github.com/apache/airflow) is 1.2M linesShould the bucket sizes be adjusted?"
},
{
    "title": "No title",
    "content": "My first attempt to get some help from Github Copilot:\n  \n\n    Me: write a match case statement in python to match the first character of a string is a semicolon\n  \n\n\n    Copilot: Python doesn't have a built-in match or case statement like some other languages (e.g., JavaScript, Swift, Rust). However, you can achieve similar functionality using...\n  \n\n    Strangely, it finally gave me an answer when I made a small change to my prompt:\n  \n\n    Me: write a match statement in python to match the first character of a string is a semicolon\n  \n\n\n    Copilot: Python 3.10 introduces the structural pattern matching in the form of the match statement. Here's how you can use it to check if the first character of a string is a semicolon:...\n  \n\n    I almost gave up after the first response that was blatantly false. Fortunately I something that looks like reasonable syntax...but then I haven't tried it yet, so that might be a lie, too."
},
{
    "title": "No title",
    "content": "Happy weekend every one!\n  \n    A while ago I showcased stockdex in this community. Since then, I've improved data retrieval performance by integrating yahoo finance API besides web scarping from sources. These tweaks have made more detailed data available in returned pandas dataframes while making the queries 5 times faster in average.\n  What My Project Does\n    A lightweight python package designed for the efficient retrieval of financial data from various sources like Yahoo Finance, Nasdaq, and Digrin. It is similar to yfinance python package.\n  Comparison with yfinance\n    It allows data retrieval from more sources, not just yahoo finance API. It also provides more data, for example yfinance returns dividends data for the last 5 years at max, but stockdex returns the entire dividends history.\n  Target Audience\n    The package is targeted at people who are interested in financial analysis using python.\n  \n    For more details or to contribute, feel free to visit the links below:\n  \nGithub Repo Link\n\nPypi link"
},
{
    "title": "No title",
    "content": "Official Event"
},
{
    "title": "No title",
    "content": "MIT's Data-to-AI lab created sdv (synthetic data vault) to generate _realistic_ fake data from real, production data.\n  \n    - CTGAN paper: https://dspace.mit.edu/handle/1721.1/128349\n\n    - Vine Copula Models paper: https://dai.lids.mit.edu/wp-content/uploads/2019/01/1812.01226.pdf\n\n\n    Code example:\n  from sdv.lite import SingleTablePreset\nfrom sdv.datasets import demo\nimport pandas as pd\n\nreal_data, metadata = download_demo('single_table', 'adult')\nsynthesizer = SingleTablePreset(metadata, name='FAST_ML')\n\nsynthetic_data = synthesizer.sample(num_rows=500)\n# Once the model is trained, you can generate as many \n# look-alike rows of data as you'd like even if original data is smaller\nsynthetic_data_more = synthesizer.sample(num_rows=10_000)\n    Link to library: https://github.com/sdv-dev/SDV"
},
{
    "title": "No title",
    "content": "A while ago I started experimenting with f-strings a bit, how to make them safe to use for HTML rendering without safety issues, basically I tried to make p(f\"Hello {b(\"World\")}! work. I also experimented with HTMX along the way. It ended as an open-source library.\n  \n    So today I'm releasing Ludic 0.1.0: https://github.com/paveldedik/ludic\n\n    I'm looking for any feedback, is it something you would find useful?\n  \n    I'm still not sure how flexible this approach is. I know there are quite a lot of similar libraries, like reflex (I started researching when I already had quite a bit of code written as it was quite fun to write it).\n  \n    Nevertheless, I'm not done experimenting with it, I'm now trying to implement examples from htmx.org to write them in pure Python using the library and see how it goes.\n  \nTL;DR looking for feedback for my library."
},
{
    "title": "No title",
    "content": "URL: https://github.com/Shakakai/typedtemplate\n\n    I'm in the process of releasing a few of my internal libraries for building LLM-based apps. The first release is typedtemplate, a library that allows you to use Pydantic syntax to describe the data required by a template to render.\n  What does it currently do?\n\n\n    Documentation: Provides a class that you can easily generate documentation for. This means all your normal python documentation generators and tooling can provide useful docs for your templates that are easily consumable by your team.\n  \n\n\n    IDE/Copilot Support: Pydantic classes provide an great developer experience in the IDE with helpful autocomplete and a structure that Copilot understands well.\n  \n\n\n    Validation: Runtime validation that the data conforms to the type definitions.\n  \n\nWhy did I create this?\n    I'm working with LLMs everyday now. I have hundreds of LLM prompts (and countless variants) in my project. If you are like me, it doesn't take long after creating a template to forget the shape of the data required to make it render properly. I'm lazy. This library lets me forget more things and lean on my tooling.\n  Target Audience\n    I'm currently using this in production and my company is supporting it.\n  Comparison\n    I actually don't know of a library that provides a type interface around string templates. I'm sure one exists.\n  Notes\n\n\n    Currently works with Jinja2 and Django templating engines. If you need a different engine, just ask (quick to implement).\n  \n\n\n    Was built to play nice with any framework.\n  \n\n\n    Next library coming is a Non-OpenAI interface to LLMs that feels good. Uses LiteLLM under the hood to support almost all available LLMs."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Today I felt bored, so I made a program to do FizzBuzz in the most obfuscated way possible:\n  def d(i,m):return lambda x:m if x%i==0else\"\"\ndef f(n,a):return str(n)if len((m:=(''.join([l(n)for l in a]))))==0else m\ndef r(n):return f(n,[d(3,'\\x46\\x69\\x7a\\x7a'),d(5,'\\x42\\x75\\x7a\\x7a')])\ndef p(l):print(r(l[0]));return None if len(l)==1else p(l[1:])\np([n for n in range(1, 101)]) # Runs it\n    And here is the deobfuscated version:\n  def divisibility_lambda_creator(divisor, message):\n    return lambda num: message if num % divisor == 0 else \"\"\n    \ndef fizzbuzzify(num, lambdas):\n    out = ''.join([l(num)for l in lambdas])\n    if len(out) == 0:\n        return str(num)\n    else:\n        return out\n    \ndef run(num):\n    return fizzbuzzify(num,[divisibility_lambda_creator(3, \"Fizz\"),divisibility_lambda_creator(5,'Buzz')])\n    \ndef printer(nums):\n    print(run(nums[0]))\n    if len(nums) != 1:\n        printer(nums[1:])\n    \nprinter([n for n in range(1, 101)])\n    It is also really extendible: You can add  7, \"Bazz\" by doing:\n  - def r(n):return f(n,[d(3,'\\x46\\x69\\x7a\\x7a'),d(5,'\\x42\\x75\\x7a\\x7a')])\n+ def r(n):return f(n,[d(3,'\\x46\\x69\\x7a\\x7a'),d(5,'\\x42\\x75\\x7a\\x7a'),d(7,'\\x42\\x61\\x7a\\x7a')])\n    If anyone has any way to obfuscate it further that'd be cool!"
},
{
    "title": "No title",
    "content": "What my project doesUiWizard allows any python developer to create a website using python.\n  \n    I wanted to have a tool that I could use for my own personal projects with minimal exposure to the underlying technologies like JavaScript, CSS and HTML.\n  \n    This however does not mean that it is not possible to use these tools.\n  \n    UiWizard is made using FastAPI, Tailwind CSS, DaisyUI, HTMX as the major libraries.\n  \nTarget audience\n\n    Anyone interested in creating a website. I currently would not recommend it for a production use-case in the current state but as it is with anything I'm not going to stop you.\n  \n    There are still some minor inconsistencies in the library API.\n  \nComparison\n\n    This project mostly resembles NiceGUI as it was inspired by it. The feature set provided by NiceGUI is a lot bigger but the main difference is the approach to handling interactivity. NiceGUI uses websockets and client side to render its interface. UiWizard uses server side rendering and HTMX for interactivity instead.\n  \n    One difference that I think is important is NiceGUI uses websockets and any change or interactive element on the site triggers the user defined function in the code using events. This means there is only one request associated with the initial websocket connection. When a user interacts with UiWizard it triggers a new request each time, which means each function is an endpoint with access to the new request if needed.\n  \nSource code\n\n    This repository contains the source and some examples\n  \nhttps://github.com/Declow/uiwiz\n\nDocs\n\n    Some docs can be found here https://ui-wizard.comThis website is made using UiWizard\n  \n    I started this project because I wanted to learn more about web development and have a tool I liked to work with."
},
{
    "title": "No title",
    "content": "DukPy is a lightweight and easy to use JavaScript interpreter for Python. It comes with builtin support for various transpilers like SCSS, Less and React JSX thus allowing to build web apps in pure python without the need for NodeJS\n  \n    Release 0.3.1 finally added support for Python 3.12 and provides prebuilt wheels."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hello, Python community! I'm excited to share a project I've been working on called PyPixelStream. This is an open-source live streaming software, specifically designed for low-resolution LED panels. Inspired by OBS (Open Broadcaster Software), PyPixelStream aims to simplify the management and broadcasting of content to LED screens, addressing the unique challenges presented by low-resolution visual media.What Does PyPixelStream Do?PyPixelStream enables live streaming to low-resolution LED screens, providing a cross-platform (Windows, Linux, macOS) environment to manage and broadcast visual content in a way that's optimized for these displays. It offers a variety of media sources (images, videos, webcam, screen capture, GIFs) and customizable text sources (clocks, timers, dates, cryptocurrency prices), along with dynamic effects and extendable filters to adjust brightness, contrast, apply chroma key, and more.Target AudienceThis project is aimed at technology enthusiasts, developers, visual artists, and anyone interested in exploring new ways to interact with low-resolution visual media. PyPixelStream is perfect for use in personal projects, artistic experimentation, or even in professional applications where LED displays play a central role.Comparison With Existing AlternativesUnlike other streaming solutions, PyPixelStream is specifically optimized for low-resolution LED panels, offering detailed customization and control over the broadcast content. While OBS is a powerful tool for live streaming in general, PyPixelStream focuses on the efficiency and adaptability needed for low-resolution media, ensuring that performance and quality are not compromised, even on limited-resource devices like the Raspberry Pi.This is an open-source project, and I'm eager to make it as accessible and versatile as possible! Whether you're interested in contributing code, suggesting features, or just providing feedback, your input would be greatly appreciated. Check out the project on GitHub here: https://github.com/offerrall/PyPixelStream"
},
{
    "title": "No title",
    "content": "I'm active in a SaaS community, so there are a lot of conversations about building multi-tenant apps. And FastAPI is super popular.  I didn't see a lot of examples though...\n  \n    So I created one:\n  \n    Example: https://github.com/niledatabase/niledatabase/tree/main/examples/quickstart/pythonVideo tutorial: https://youtu.be/Axl63TUf2bcWritten tutorial: https://www.thenile.dev/docs/getting-started/languages/python\n\n    (Disclaimer: The example uses Nile, a serverless Postgres that I'm working on. But it can be modified to work on any Postgres).\n  \n    Feedback is welcome!"
},
{
    "title": "No title",
    "content": "No, I'm not saying what you probably think I am. Keep reading: I'll be brief.\n  \n    Here's what I found today in one of the tests:\n  for bad_fp in _bad_fps:\n    with pytest.raises(BadArgumentError):\n        follow_path(bad_fp, '.')\n        follow_path('.', bad_fp)\n    The correct code should've been\n  for bad_fp in _bad_fps:\n    with pytest.raises(BadArgumentError):\n        follow_path(bad_fp, '.')\n    with pytest.raises(BadArgumentError):\n        follow_path('.', bad_fp)\n    Is that a stupid mistake? Yes, it is, but stupid mistakes do happen, unfortunately.\n  \n    How did I catch it? The coverage of the testing file wasn't 100% like it usually is! I got suspicious and I saw that there was a glaring red line (vscode, coverage gutter) on the second call to follow_path.\n  \n    P.S. In case you're wondering, the code uses a custom hierarchy of exceptions instead of reusing the standard ones. The code is internal and we like it that way :)"
},
{
    "title": "No title",
    "content": "Hello r/Python community!\n  \n    I'm excited to share with you XposedOrNot, an open-source project built using Python to improve how we monitor data breaches. This project isn't just about enhancing security; it's about utilizing the power and flexibility of Python to address one of the most pressing issues in the digital world today.\n  What My Project Does:\n    XposedOrNot is an open-source tool built using Python, designed to enhance data breach monitoring. By leveraging the simplicity and versatility of Python, it offers a set of comprehensive features aimed at improving security practices and data breach awareness. The tool includes a dashboard with a risk score for emails, data breach impact analysis and analytics, visualization tools, and categorized data for better navigation and understanding of breach data.\n  Target Audience:\n    This project is geared towards developers, security professionals, and anyone interested in contributing to or utilizing security tools. It's suitable for both educational purposes and as a practical tool for monitoring data breaches in a production environment.\n  Comparison:\n    Unlike many commercial solutions that may be closed source and require payment for full features, XposedOrNot stands out because:\n  \n\n\nBuilt with Python: It leverages Python for its development, making it easily accessible for contributions and customization by developers familiar with Python.\n  \n\n\n100% Open Source: The project invites open collaboration and contribution, emphasizing transparency and community-driven development.\n  \n\n\nFree API Access & API Playground: It provides open API access and a playground for users to experiment and integrate the tool's capabilities into their projects.\n  \n\n\nComprehensive Features: Offers in-depth insights with its dashboard, analytics, and visualization tools specifically designed to navigate and understand data breaches effectively.\n  \n\n\nCommunity-Driven Development: Encourages contributions and feedback with an open roadmap, recognizing contributors to foster a collaborative development environment.\n  \n\nWhat’s Next & How to Support:\n    I'm inviting the community to help shape the future of data breach monitoring by contributing ideas, code, or feedback.\n  \n    Your support is crucial. Explore the project on GitHub, where your stars help increase visibility and encourage more contributions.\n  \n    Let's use the power of Python to improve data breach monitoring together!"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What My Project Does\n    My friend and I recently developed an open-source NLP Library based on LLMs in Python, accessible via the pip install command. This library is capable of solving a wide array of NLP tasks for developers with minimal involvement. It contains over 30 features, covering everything from pattern extraction to OCR text extraction and beyond. Currently, it operates with the Gemini 1.0 Pro Model, allowing it to function without the need to run an LLM on your local environment. All that's required is a Gemini API key, which is available for free.\n  Target audience\n    Over the past few years, developers have encountered significant challenges when dealing with text data, despite the availability of rich resources such as SpaCy and other NLP libraries. These challenges persist due to the constantly increasing volume of data, which demands considerable human involvement, thought, and effort. Our target audience includes anyone who works with text data using Python.\n  \n    Comparison\n  \n    Consider one of the most stressful scenarios where you use regex to extract patterns. You define pattern formats and still remain unsure whether it completely yields accurate results. However, within our library, in one of our features, we attempt to solve this issue. You just need to pass the pattern name in a comma-separated string, such as \"email, person name, phone number\", and the backend prompt template will handle the effort for you, returning the extracted patterns in a list. The output format can be a list, dictionary, string, or more, depending on the feature you utilize. The same applies to cleaning, intent detection, and many other features that you will explore in our library.\n  GitHub\n    Code, documentation, and example can all be found on GitHub:\n  \nhttps://github.com/FareedKhan-dev/basiclingua-LLM-Based-NLP"
},
{
    "title": "No title",
    "content": "I've just come across an article where a Haskell, a Python, and a JS implementation of the Sieve of Eratosthenes are shown.\n  \n    Here's the Haskell one:\n  primes = sieve [ 2.. ]\n  where\n    sieve (p:x) = p : sieve [ n | n <- x, n `mod` p > 0 ]\n    And here's the Python one:\n  def eratosthenes(n):\n    sieve = [ True for i in range(n+1) ]\n    def markOff(pv):\n        for i in range(pv+pv, n+1, pv):\n            sieve[i] = False\n    markOff(2)\n    for i in range(3, n+1):\n        if sieve[i]:\n            markOff(i)\n    return [ i for i in range(1, n+1) if sieve[i] ]\n    I don't understand why markOff(2) is singled out and not handled in the loop. Why compare a clever Haskell implementation with a stupid Python one?\n  \n    Here's my version:\n  def eratosthenes(n):\n    sieve = [True] * (n + 1)\n    for i, x in enumerate(sieve):\n        if i < 2 or not x:\n            continue\n        for j in range(2 * i, n + 1, i):\n            sieve[j] = False\n    return [i for i, x in enumerate(sieve) if x][2:]\n    We can also start from 2:\n  def eratosthenes2(n):\n    sieve_from2 = [True] * (n - 1)      # for ints 2, 3, ..., n\n    for i, x in enumerate(sieve_from2):\n        if not x:\n            continue\n        for j in range(i + (i + 2), n - 1, i + 2):\n            sieve_from2[j] = False\n    return [i + 2 for i, x in enumerate(sieve_from2) if x]\n    But next time someone shows you the Haskell code above, please show them this:\n  def sieve(xs):\n    yield (p := next(it := iter(xs)))\n    yield from sieve(x for x in it if x % p > 0)\n\nprimes = sieve(count(2))\n    We're just some syntactic sugar away from the Haskell implementation!This proves that we can if we want to. We just don't see the point in being that clever all the time :)\n  EDIT:\n    Here's the recursive version without using any division:\n  def sieve2(xs):\n    yield (p := next(it := dropwhile(lambda x: not x[1], xs))[0])\n    yield from sieve2((x, b and r != p - 1) for (x, b), r in zip(it, cycle(range(p))))\n\nprimes2 = sieve2(zip(count(2), repeat(True)))\n    Not so elegant anymore..."
},
{
    "title": "No title",
    "content": "Original Posts\n  \n\n\nGitHub Discussion\n\n\n\nPython.org Discussion\n\n\n\nPEP 649\n\n\n\n    As a user of strong typing in Python, and maintainer of libraries that utilize them greatly with plans of further work on a runtime type-using library, I found this quite an interesting read. Basically seeks to bring up implications of Python 3.13's  updates on runtime type checkers via PEP 649...\n  \n    This comes from the maintainer of Beartype - if you aren't familiar with beartype's documentation then it's worth a read even if you never use the library - a runtime-static type-checker.\n  \n    It brings in some highly thought after individuals in the Python typing community including the Anyio, Pydantic, Msgspec maintainers... certainly worth a read through if this (typing) interests you."
},
{
    "title": "No title",
    "content": "Hey everyone, we just released a new version of reflex and wanted to share some updates.\n  \n    For those who don’t know about Reflex (we used to be called Pynecone), it’s a framework to build web apps in pure Python. We wanted to make it easy for Python developers to share their ideas without having to use Javascript and traditional frontend tools, while still being as flexible enough to create any type of web app.\n  \n    Since our last post, we’ve made many improvements including:\n  \n\n\n    We’ve released our hosting service . Just type reflex deploy and we will set up your app, and give you a URL back to share with others. During our alpha we’re giving free hosting for all apps (and always plan to have a free tier).\n  \n\n\n    A tutorial on building a ChatGPT clone using Reflex. See the final app https://chat.reflex.run\n\n\n\n    New core components based on Radix UI, with a unified theming system.\n  \n\n\n    More guides on how to wrap custom React components. We’re working now on building out our 3rd party component ecosystem.\n  \n\n\n    Our key focuses going forward are on making the framework stable, speed improvements, and growing out the ecosystem of 3rd party components. We’ve published our roadmap here.\n  \n    Let us know what you think - we’re fully open source and welcome contributions!\n  \n    We also have a Reddit where we post updates: https://www.reddit.com/r/reflex/"
},
{
    "title": "No title",
    "content": "What My Project Does\n    Cadwyn allows you to support a single version of your code while auto-generating the schemas and routes for older versions. You keep REST API versioning encapsulated in small and independent \"version change\" modules while your business logic stays simple and knows nothing about versioning.\n  \n    It is heavily inspired by Stripe's approach to API versioning but it is much more sophisticated and allows you to reach the true \"zero-duplication\" versioning.\n  \n    We have recently discussed it on TalkPython podcast.\n  Target audience\n    Cadwyn is made for FastAPI so any FastAPI developer could benefit from it immediately if they need versioning. However, Cadwyn is also a huge case study: its documentation contains a lot of guides and research on how to build any kind of API Versioning and Cadwyn in itself can serve as a guide for making it in another web framework or language.\n  Comparison\n    There does not exist any tool (especially open-source and especially in python) for API versioning of this quality and scalability.\n  GitHub\nhttps://github.com/zmievsa/cadwyn"
},
{
    "title": "No title",
    "content": "Anaconda 2024-02 is released:\n  \n\n\n    python 3.11.7\n  \n\n\n    conda 24.1.2\n  \n\n\n    numpy 1.26.3\n  \n\n\n    pandas 2.1.4\n  \n\n\n    matplotlib 3.8.0\n  \n\n\n    jupyterlab 4.0.11\n  \n\n\n    spyder 5.4.3\n  \n\n\n    The JupyterLab Variable Inspector extension is also preinstalled.\n  \nAnaconda Release Notes\n\n    Miniconda 24.1.2-0 (February 27, 2024) is released:\n  \n\n\n    Python 3.12.1\n  \n\n\n    conda 24.1.2\n  \n\n\nMiniconda Release Notes\n\n    The new version of conda uses the libmamba solver by default which has a number of performance improvements over the classic solver.\n  \n\n    The conda team is pleased to announce the availability of ‘libmamba’ as a new, much faster dependency solver for conda!\n  \n    Three different companies worked to make this release possible: QuantStack, developing mamba and libmamba; Quansight, integrating libmamba into conda; and Anaconda, developing conda and managing the overall effort.\n  \n\nA faster conda\n\n\n    We are changing the default solver of conda to conda-libmamba-solverFirst, complex solves will run noticeably faster. Benchmarking predicts a 50 to 80% improvement in run times.\n  \n    Second, you will notice improved error messages when conda encounters problems. libmamba's error messages give you more insight into what is triggering the problem.\n  \n    Third, conda-libmamba-solver outputs more details about the channels in use and target platform at the beginning of the process. Mind these differences if you are parsing stdout (although we definitely recommend enabling the --json mode for programmatic usage!).\n  \n\nconda 23.10.0+ uses libmamba by default\n\n    Originally libmamba was supposed to be rolled out for the 2023-09 release but this came with conda=23.7.4 which included the libmamba solver but wasn't enabled by default:\n  \nconda libmamba rollout"
},
{
    "title": "No title",
    "content": "What My Project Does\n    Sitcom Simulator is a python/CLI tool that takes any text prompt (for example, \"Joe Biden vs. Donald Trump: ultimate weightlifting competition\") and turns it into a bizarre meme video with realistic images and voices. The tool is essentially duct tape that combines various AI tools into one unholy abomination:\n  \n\n\n    ChatGPT generates the video script.\n  \n\n\n    FakeYou generates voices for the characters.\n  \n\n\n    Stable Diffusion generates images of the characters.\n  \n\n\n    Freepd provides the background music.\n  \n\n\n    FFmpeg connects the images and voices into a movie.\n  \n\nTarget Audience\n    People who like making memes, funny videos, or weird AI \"art\" but are too lazy to do it manually.\n  \n    The code is fairly customizable and extendable; it could probably be used as a base for many types of AI video generation projects even if your use case is somewhat different.\n  Comparison\n    There are many AI video editing tools out there (e.g., Kapwing), almost all of which are complicated commercial products with a vague notion of improving \"productivity\" or whatever. In contrast, Sitcom Simulator is simple, open source, and the only AI video tool focused on humor, memes, and wasting your time with terrible brain rot.\n  GitHub\n    Code, documentation, and example videos can all be found on GitHub:\n  \nhttps://github.com/joshmoody24/sitcom-simulator"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Idea\n    The below must be enough to define an API:\n  class UserParams(pydantic.BaseModel):\n    uid: str\n\nclass GetUsers(AbstractProcedure):\n    def call(self, in_: UserParams, context) -> List[UserDetails]:\n        ...\n    to then do the following somewhere in your TypeScript code:\n  import { callGetUsers } from \"./src/out\";\n\nexpect(callGetUsers(userParams).$promise).resolves.toEqual(listOfUserDetails);Desired benefits\nAutomated typescript client generationOf course, it's possible to annotate your API, export an OpenAPI schema and generate a typescript client from it. I just want it out of the box.\n  \nBrowser Dates done rightJavascript doesn't have a separate date type, so it uses Date for both python's date and datetime.\n  \n    Hence when you pass 2000-01-01 to a browser in New York, the browser will read it as UTC datetime and then convert it to the local timezone, so it will give you Dec 31, 1991 7:00PM, which is fine if you wanted to work with a particular moment in time, but what if you wanted to display someone's date of birth? That's why lacking the date type is a problem.\n  \n    The library should see that you want to pass python's date to the browser and automatically prepare it in the browser, so that  Jan 1st is still Jan 1st.\n  \nBrowser friendly types onlysynclane raises an exception if you use types, which browser won't be able to understand.\n  \nNo need to define URLsA procedure name should be enough. e.g. you name it AddUser, you should be able to just run callAddUser function in the typescript client. I don't want to define any other identifier like path per API endpoint.\n  \nEnumsIf your procedure in/out types include enums, they should become available in the typescript client.\n  What My Project Does\n    It does all the above: checks types, handles RPC requests, generates TypeScript client code, including all the above benefits.\n  Target Audience\n    It is in currently in beta. If it doesn't require breaking changes within next 6-12 months, I'll make it stable.\n  Comparison\n    It is like FastAPI + Pydantic, but with:\n  \n\n\n    out of the box TypeScript client generation\n  \n\n\n    benefits described above\n  \n\n\n    should be easily embeddable to any framework, because it only requires a single API endpoint (all the routing happens inside of RPC)\n  \n\nLinks\n    docs & examples: https://synclane.readthedocs.io/en/latest/github: https://github.com/westandskif/synclanepypi: https://pypi.org/project/synclane/\n\n    What do you think?Would you use it? Any feedback is much appreciated!"
},
{
    "title": "No title",
    "content": "What My Project Does\n    Open Interface is a desktop app (MacOS/Windows/Linux) that can control your mouse and keyboard to perform any human-computer task you ask of it by using LLMs as a backend to break down the steps required.\n  Target Audience\n    Since most LLMs are not production reliable, this is still a toy project but anyone can use it.\n  Comparison\n    I haven't much looked at alternatives since this is an application not a library.\n  \n    Libraries used\n  - Tkinter\n- PyAutoGUI\n- Pyinstaller \n- The official OpenAI module \nGithub: https://github.com/AmberSahdev/Open-Interface/\n\nDemo: https://github.com/AmberSahdev/Open-Interface/?tab=readme-ov-file#demo\n\nInstall: https://github.com/AmberSahdev/Open-Interface/?tab=readme-ov-file#install"
},
{
    "title": "No title",
    "content": "Hello everyone,\n  \n    We're happy to announce that our Stratosphere Laboratory has been accepted as a mentor organization in the Google Summer of Code 2024!\n  \n    Projects you can contribute to are our Intrusion Prevention System and our AI VPN:\n  \nhttps://github.com/stratosphereips/StratosphereLinuxIPS\n\nhttps://github.com/stratosphereips/AIVPN\n\n    Check out our ideas list here:https://github.com/stratosphereips/Google-Summer-of-Code/blob/main/list_of_ideas.md\n\n    If you think you have the required skills to contribute, you can check the contributor guide here:https://github.com/stratosphereips/Google-Summer-of-Code/blob/main/contributor_guide.md\n\n    You can join our discord if you have more questionshttps://discord.gg/zu5HwMFy5C\n\n    Let's make this summer a memorable one!  🚀 🚀"
},
{
    "title": "No title",
    "content": "What my Project Does?\n    Youtility helps you to download YouTube content locally. With Youtility, you can download:\n  \n\n\n    Single videos with captions file\n  \n\n\n    Playlists (also as audio-only files)\n  \n\n\n    Video to Mp3\n  \n\nTarget Audience\n    People who want to save YouTube playlists/videos locally who don't wanna use command line tools like PyTube.\n  Comparison\n    Unlike existing alternatives, Youtility helps you to download even an entire playlist as audio files. It can also download XML captions for you. Plus, it also has a great UI.\n  GitHub\n    GitHub Link: https://github.com/rohankishore/Youtility"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "class Movable:\ndef __init__(self, x, y, dx, dy, worldwidth, worldheight):\n    \"\"\"automatically sets the given arguments. Can be reused with any class that has an order of named args.\"\"\"\n    \n    nonmembers = [] #populate with names that should not become members and will be used later. In many simple classes, this can be left empty.\n    \n    for key, value in list(locals().items())[1:]: #exclude 'self', which is the first entry.\n        if not key in nonmembers:\n            setattr(self, key, value)\n\n    #handle all nonmembers and assign other members:\n\n    return\n    I always hate how redundant and bothersome it is to type \"self.member = member\" 10+ times, and this code does work the way I want it to. It's pretty readable in my opinion, especially with the documentation. That aside, is it considered acceptable practice in python? Will other developers get annoyed if I use it?\n  \n    Edit:  Thanks for the very fast replies. Data classes it is! I meant for this to be a discussion of code conventions, but since I learned about a completely new feature to me, I guess this post belongs in r/learpython."
},
{
    "title": "No title",
    "content": "https://github.com/kernel137/shavisInstall with\n  \npip install shavisshavis is (secure hash algorithm visualization), is a CLI tool that can hash any file (Only through SHA256) and create an 8x8 pixel image displaying the hash through themed colours.\n  \nWhat My Project Does:\n  \n    Main functionality of this CLI tool is taking the 64 digit hex number (SHA256 hash) and, through 16 hex colors in ascending order, turning it into a 8x8 pixel image that can then be scaled to powers of two.Shavis also supports piping: echo -n \"Hello World!\" | shavis\n\n    This also works for git hashes, since git hashes are SHA1, images for this hash are 8x5, I think with some clever integration, its possible to integrate a small res picture of a git commit hash  to quickly visualize the hash within vscode or some other platform.A quick way to get a visual of the last git commit while within a local git repository is:\n  \ngit rev-parse HEAD | shavis -g\n\n    What do you guys think? Is it a worthwhile investment to implement other hashes? Is it an interesting project? Do you guys have any ideas about what to add to it?"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What My Project Does\nRob's Awesome Python Template is a cookiecutter template meant to bootstrap python projects using modern best practices.\n  \n    At the very basic level it includes:\n  \n\n\n    Modern pyproject.toml without any legacy files (no setup.py or setup.cfg).\n  \n\n\n    Development Management using Makefiles.\n  \n\n\n    Configuration Management with Pydantic.\n  \n\n\n    PyPI Publishing from Git Tags using setuptools-scm.\n  \n\n\n    Formatting and Linting with Ruff.\n  \n\n\n    Typing with mypy.\n  \n\n\n    Lockfiles (requirements.txt, requirements-dev.txt) with uv.\n  \n\n\n    Testing with pytest.\n  \n\n\n    CI/CD using Github Actions.\n  \n\n\n    Precommit Hooks using the precommit framework.\n  \n\n\n    It also has a ton of optional features:\n  \n\n\n    Github Actions for CI\n  \n\n\n    Cross Platform (arm, arm64, amd64) Docker containers using the Multi-Py project.\n  \n\n\n    Optionally use any combination of FastAPI, Click/Typer, Celery, and Sqlalchemy.\n  \n\n\n    I've used this template for a number of projects- QuasiQueue, Paracelsus, and Fedimapper being some nice examples. If you want to see exactly what the project would generate today you can review the examples repository which builds a few projects using different options to give people a feel for what things can look like.\n  Target Audience\n    Any developer looking to bootstrap their projects can use this! What makes it really helpful is that you get a modern, high quality project with tools already configured before you even write a single line of code. Even if all you're doing is creating a POC this template can help you do it with style.\n  Comparison\n    Although there are other templates out there, this project is unique in a few ways:\n  \n\n\n    It is extremely modern, using tools such as Ruff and UV while also removing legacy fluff such as setup.py.\n  \n\n\n    It isn't targeted towards a specific type of software (library, cli, app) but can be used for a variety of project types, or even for projects with multiple entry points.\n  \n\n\n    It uses Cookiecutter, rather than direct git forking, to fill in fields and customize the project so you can get started right away after running it.\n  \n\nTry it now!\n    If you haven't installed Cookiecutter yet it's pretty easy on most platforms. With homebrew, for instance, just run brew install cookiecutter.\n  \n    From there-\n  cookiecutter gh:tedivm/robs_awesome_python_template"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "https://github.com/icyveins7/sew\nMotivation\n    This project started because at work I realised I was copying / editing code to create tables / insert rows etc in many different scripts. Oftentimes the structure of the tables, and the way I inserted the data, was the same; just inserting all columns.\n  \n    So at first I started auto-generating my CREATE TABLE and INSERT statements, but then it eventually grew into this.\n  What This Project Does\n    The goal of this package is essentially to reduce the code typed for simple CRUD operations.\n  \n    I pretty much only use this now because I can always dip down to make custom statements if I want to anyway (which still happens for more complex things like inner joins).\n  \n    Here's an example:\n  \n    Before:\n  import sqlite3 as sq\ncon = sq.connect(\"path/to/db\")\ncur = con.cursor()\ncur.execute(\"select * from mytable where ...\")\nresults = cur.fetchall()\n    After:\n  import sew\ndb = sew.Database(\"path/to/db\")\ndb['mytable'].select(\"*\", ...)\nresults = db.fetchall()Target Audience\n    This is really for anyone who uses SQLite from python’s in built library.\n  \n    Hopefully someone else will find this useful; it’s my first ever package uploaded to PyPI so things may not be as crisp as they should be, but constructive feedback is welcome. Otherwise, ask me anything here!\n  Comparisons\n    I think there are other libraries out there that attempt to optimize database interaction speed but none that I found that do what I described above. Might be wrong though."
},
{
    "title": "No title",
    "content": "https://github.com/final-israel/vmn\n\n    Hello r/Python community!\n  \n    I’m excited to share a project I’ve developed called vmn. It’s a git tag based versioning system that we’ve been using extensively at my company to manage our application versions. Here’s a quick overview:\n  \n    What vmn Does: vmn is a tool that simplifies version management by leveraging git tags. It’s designed to automate and streamline the process of versioning your applications, making it easier to maintain a clear and consistent version history.\n  \n    Target Audience: This project is aimed at developers and teams who use git for version control and are looking for a straightforward way to manage their application versions. It’s suitable for both production use and development environments.\n  \n    Comparison: Unlike other versioning systems that might rely on manual updates or complex configurations, vmn keeps things simple and git-centric. It’s a lightweight and easy-to-use solution that integrates seamlessly with your existing git workflow.\n  \n    I would LOVE to hear your feedback and suggestions! If you have any ideas for features or improvements, I’m open to adding them.\n  \n    Check it out and give it a star ⭐️⭐️⭐️\n  \n    Thanks😁"
},
{
    "title": "No title",
    "content": "I’m wondering if anyone has ever seen a case of code using eval() and thought to themselves “yeah actually that’s probably the right way to do it”?\n  \n    My understanding has always been that it’s a huge security risk and generally a recipe for disaster.\n  \n    But I was just working on a task where I couldn’t really figure out any other way to achieve the dynamic functionality I was looking for, so I wrote code that assembles a string to do what I need, and then runs eval() on that string. Pretty sure this is the first time I’ve ever used eval() at all.\n  \n    It’s a low-stakes proof of concept for a totally internal tool, so I’m not hugely worried about security at the moment, but it just feels so icky to do something like that. I’m curious if in others’ experience there’s always a better way than using eval(), or if sometimes it’s legit."
},
{
    "title": "No title",
    "content": "Greetings pythonistas,\n  \n    I'm thrilled to share with you my latest article which delves into the amazing python package, pyttsx3. Explore the capabilities and how interesting the Text-to-Speech(TTS) package is\n  \n    Read the full blog post here: [https://jeffmint.hashnode.dev/code-that-speaks-a-beginners-guide-to-pyttsx3-text-to-speech-tts-in-python ]\n  \n    If you find the content intriguing and would like to actively contribute to its development, I'm excited to announce that it's an open-source initiative! Feel free to explore the repository, and let's join forces to enhance and elevate the usage of the package together. Your collaboration and ideas are invaluable in making it even more remarkable. [https://github.com/Minty-cyber/CodeThatSpeaks]\n\n    Cheers to building something great together! 🚀"
},
{
    "title": "No title",
    "content": "Hello all,\n  \n    After having published a double entry accounting package in php a couple of years ago, I hereby present the python version to the community. Please take a look here, all and any feedback (and criticism) is most welcome.\n  \nWhat My Project DoesThe library provides a fully featued double entry accounting backend with support for postgres and mysql, allowing you to develop python accounting applications that can produce finacial reports compatible with the IFRS and GAAP standards.\n  \nTarget AudienceMay be used for production/commercial use or just for writing a private bookkeeping system for your business\n  \nComparisonMost other libraries either focus on personal finance or keeping a simple ledger with which one can keep track of expenses and incomes. This library represents a fully featured accounting software kernel that can handle the accounting needs of any size of business."
},
{
    "title": "No title",
    "content": "Hey Pythonistas,\n  \n    We are working on extending the Oxen.ai toolchain to support diffs of different data types. For example, quickly finding changes in large csv or parquet files. Oxen identifies the data schema and performs hashing under the hood to find changes in the rows and columns.\n  \n    Docs 👉 https://docs.oxen.ai/concepts/diffs\n\n    Eventually we want to extend support to diffs of many data types (directories, images, videos, audio, etc).\n  \n    If you aren’t familiar with Oxen.ai it is an Open Source CLI, Server and Python Library for versioning and iterating on large machine learning datasets that would be too large for git.\n  \n    Source code 👨‍💻 https://github.com/Oxen-AI/oxen-release\n\n    The first two diff types we are supporting are a TextDiff and a TabularDiff. The TabularDiff is useful for finding changes in rows and columns of tabular datasets of csv, parquet, jsonl etc. The TextDiff is like your standard git diff that finds added and removed lines in a text file.\n  \n    We also rolled out a Web UI to help visualize the diffs beyond a simple text diff like GitHub.\n  \n    Check it out here: https://oxen.ai\n\n    Let us know if you find these features helpful or if there are any other data types you think would be helpful. The next one on the roadmap is DirDiff that will summarize changes in large directories and sub directories of data. Contributors welcome!\n  \n    Best & Moo,\n  \n    The Oxen.ai Herd"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Blog - https://astral.sh/blog/ruff-v0.3.0\n\n    Changes:\n  \n    - The Ruff 2024.2 style guide- Range Formatting- f-string placeholder formatting- Lint for invalid formatter suppression comments- Multiple new rules - both stable and in preview"
},
{
    "title": "No title",
    "content": "This document provides some fundamental and comparisons about virtual environment tools. The readers are expected to use one of similar tools once. If not, the readers should read 12. Virtual Environments and Packages — Python 3.12.2 documentation. Some concepts might be oversimplified.\n  The fundamental\n    Every beginner tutorial about creating virtual environment starts with venv or virtualenv .\n  \nvenv is a standard module for creating lightweight environment.\n  \nvirtualenv is a tool to create isolated environments. Since Python 3.3, a subset of it has been integrated into the standard library under the venv module.\n  \n    Eventually, the developers build management tools.\n  \nvirtualenvwrapper is a useful set of scripts for creating and deleting virtual environments.\n  \npyenv is a tool to install, isolate, manage, Python versions per user and per project.\n  \npyenv-virtualenv is a plugin of pyenv to manage virtualenv .\n  \npipenv is a Python virtualenv management tool that supports a multitude of systems and nicely bridges the gaps between pip, python (using system python, pyenv or asdf) and virtualenv.\n  The combinations\n    Black: virtual environment creation tools\n  \n    Red: virtual environment management tools\n  \n    Green: Python versions management tools\n  \nHere is the image\n\n    Here are some of the tool sets that people commonly use.\n  The comparisons\n\n\n\n\n            virtualenvwrapper\n          \n            pipenv pyenv\n          \n            pyenv pyenv-virtualenv\n          \n\n\n\n\n\n\n              Manage multiple Python versions\n            \n              No\n            \n              Yes\n            \n              Yes\n            \n\n\n              Manage per project\n            \n              No, you can have many venv per project and many projects per venv\n            \n              Yes\n            \n              No, like virtualenvwrapper.\n            \n\n\nThe holdup\n    The readers may ask what conda / mamaba and poetry are all about.\n  Conda-related\nconda is a system-level, binary package and environment manager running on all major operating systems and platforms.\n  \nmamba is conda implemented in c++.\n  \n    Many people might be more familiar with miniconda , micromamba which are minimal CLI versions of aforementioned tools.\n  \n    Conda is similar to pyenv pyenv-virtualenv but not limited to Python. One can install C++ packages such as Nvidia drivers.\n  \nImportant use case: people often use this for machine learning projects.\n  Poetry\npoetry is a tool for dependency management and packaging in Python. It allows you to declare the libraries your project depends on and it will manage (install/update) them for you. Poetry offers a lockfile to ensure repeatable installs, and can build your project for distribution. Poetry either uses your configured virtualenvs or creates its own to always be isolated from your system.\n  Reading list\npython - What is the difference between venv, pyvenv, pyenv, virtualenv, virtualenvwrapper, pipenv, etc? - Stack Overflow\n\nPython Virtual Environments: A Primer – Real Python\nReferences\nvenv — Creation of virtual environments — Python 3.12.2 documentation\n\nvirtualenv (pypa.io)\n\nvirtualenvwrapper 6.0.1.dev12+gb1aa495 — virtualenvwrapper 6.0.1.dev12+gb1aa495 documentation\n\nhttps://github.com/pyenv/pyenv\n\nhttps://github.com/pyenv/pyenv-virtualenv\n\nhttps://github.com/pypa/pipenv\n\nMiniconda — Anaconda documentation\n\nMicromamba Installation — documentation\n\nIntroduction | Documentation | Poetry - Python dependency management and packaging made easy (python-poetry.org)"
},
{
    "title": "No title",
    "content": "Hey everyone - really excited to showcase Hatchet, an OSS project I've been working on for the past few months: https://github.com/hatchet-dev/hatchet\nWhat My Project Does\n    Hatchet is a Celery alternative built for scale and observability. Specifically, it supports:\n  \nLow Latency and High Throughput Scheduling: Hatchet is built on a low-latency queue (25ms average start) so it can support real-time user interaction even while running from an async worker.\n  \nConcurrency, Fairness, and Rate Limiting: implements FIFO, LIFO, Round Robin, and Priority Queues with built-in strategies for limiting concurrency.\n  \nDAG workflows: Hatchet lets you declare tasks which are dependent on the execution of other tasks for full DAG-style execution.\n  \nDurability and error handling: all events and executions are persisted to Postgres. When a worker fails, tasks automatically get reassigned to new workers, and the workflow will pick up where it left off.\n  \nWeb UI and API for observability: visualize events, logs and workflows within the dashboard.\n  \nWorkflow replay: replay tasks, workflows, and events right from the UI or via the API.\n  Target Audience\n    This is being actively used in production at 5 companies, the largest of which is executing 50k tasks per day. It's ideal for companies looking to scale their async tasks or want more visibility into workflow progression.\n  Comparison\n    Celery is a clear alternative - and while it's a great framework, there are a few reasons to favor Hatchet:\n  \n    - Better observability - I've spent a lot of time in the Celery Flower UI or building Grafana views for exported prom metrics. We wanted to build a modern, dev-friendly dashboard. There's still a long way to go, but a core focus of the platform is dev experience after deployment to production.\n  \n    - Postgres-backed - when I started to build this, I wanted a transactional database that's easy to horizontally scale and can handle high volumes of writes. We are working on batching execution data and forwarding it to Clickhouse, so Postgres is a natural choice.\n  \n    - Networking - when deploying with Celery, each worker manages its own connection to the underlying broker using redis or amqp. With Hatchet, each worker connects via a long-lived gRPC connection, which makes it easier to distribute workers across different networks or clusters, as there is widespread support for proxying HTTP 2.0 rather than forwarding TLS which can get tricky.\n  FAQs\nHow can I get started?\n\n    Here's a quickstart repository for Python.\n  \nThis is written in Go, are you in the wrong place?\n\n    After the original engine was deployed, we spent the next month building our Python SDK. I made the decision to use a lower-level language for more control over the underlying engine runtime which makes it easier to optimize latency.\n  \nWhat's next on the roadmap?\n\n    Support for logging from a task execution and giving a great logs API + view on our dashboard. This also can be difficult to integrate well with Celery AFAIK.\n  Feedback\n    Would love to hear what you think - also feel free to join our Discord and share your thoughts there."
},
{
    "title": "No title",
    "content": "Hey, heres a tutorial I made that shows how to use Firebase with Tkinter to create a simple application that has authentication and which uses the Firestore database: https://youtu.be/hxuiRPUJMwU?si=QF0zevMvOmq6BN1K\n\n    Using Firebase is much easier than setting up your own backend and you can start using it for free!"
},
{
    "title": "No title",
    "content": "Hi, I am working on a stream processing framework called Bytewax and I recently gave a talk where I read live data from HackerNews API, cleaned it with unstructured.io, created embeddings with a hugging face model, and stored everything in Milvus.\n  \n    I find a lot of devs are struggling with real-time streaming. The steps are easy to follow, and it is an end-to-end Python: https://github.com/bytewax/real-time-milvusI also added diagrams and summarized it in a blog https://bytewax.io/blog/stream-process-embed-repeat"
},
{
    "title": "No title",
    "content": "https://github.com/parkervg/blendsql\n\nWhat my Project Does\n\n    BlendSQL is a superset of SQLite for blending together complex reasoning between vanilla SQL and LLM calls. It's got a bunch of optimizations implemented using the amazing pure Python parser sqlglot to minimize the number of external LLM calls made. It allows you to efficiently perform tasks ranging from simple table question-answering:\n  {{  \n    LLMQA(  \n        'Give me a short summary of my spending.',  \n        context=(SELECT * FROM spending)  \n    )  \n}}\n    ...to complex question-answering with unstructured data using constrained decoding:\n  \n(BlendSQL corresponding to 'List the term lengths of all U.S. presidents from the 3rd most populous city in Ohio')\nSELECT {{LLMMap('How long did they serve?', 'presidents::term')}} FROM presidents  \n    WHERE birthplace = {{  \n        LLMQA(  \n            'Which city is the 3rd most populous city in Ohio?',  \n            (SELECT * FROM documents WHERE documents MATCH 'ohio'),  \n            options='presidents::birthplace'  \n        )  \n    }}\nTarget Audience\n\n    BlendSQL can be used in a couple ways - by directly writing the queries yourself as you would normal SQL to operate over a database, or as an intermediate representation that is taught to a LLM via few-shot learning. In our research, we show that using BlendSQL as an intermediate representation for complex question answering tasks boosts performance over the traditional end-to-end approach while using substantially fewer tokens.\n  \nComparison\n\n    While other existing tools enable text-to-SQL using LLMs to write queries, this tool enables the user to oversee all calls (LLM + SQL) within a unified query language.\n  \n    Appreciate any feedback folks have! Thanks."
},
{
    "title": "No title",
    "content": "I used jupyterlite, pyodide and the PyArma module to implement the method described in this stackexchange question.\n  \n    The jupyterlite site with the notebooks is: http://juangburgos.github.io/FitSumExponentials/"
},
{
    "title": "No title",
    "content": "What my project does\n\nThis project is a Discord bot that lets you talk with document collections stored in the Qdrant vector database.\n  \n    The project comes with a Discord bot written using discord.py, and a backend written in FastAPI. I work for Hasura as a Community Engineer and we are in the Developer tooling space and so we get lots of support requests from developers. I built this bot to help answer questions that should be covered in our technical documentation. There is a blog post written in the README, and setup instructions in SETUP.md. This bot lives in our Discord server and automatically responds to requests in our help forum.\n  \nTarget Audience We are running this in production, and you probably could too. I won’t say it’s perfect, but it would be a really great project for someone wanting to get started with event-driven programming, interested in building a Discord bot, or who was curious about Hasura. I would definitely say this is something you want to be relatively comfortable with Python for it to be approachable. It uses a decent handful of some of my favorite python libraries like FastAPI, selenium, beautifulsoup4, and others.\n  \nComparison There are lots of chatbots coming out right now, and some might be better than this. This isn’t anything more than a simple case of RAG. The specific use case we wanted was a bot that would automatically try to answer help-forum questions and provide links to documentation. I’m in a handful of programming related Discord servers, and but just because I haven’t seen a bot like this in the ones I am in doesn’t mean there aren’t others. I think what’s maybe most useful is how easy it is to run this yourself because of the Docker-compose."
},
{
    "title": "No title",
    "content": "What Does My Project Do?\n  \npyCage is a VSCode extension powered by astral.sh's uv package manager and pip. It enables users to search for and download Python packages directly from the command palette, similar to Dart's package manager.\n  \n    Target Audience\n  \n    This extension is designed for individuals who frequently use venvs and struggle with remembering package names.\n  \n    Comparison\n  \n    The installation speed is notably fast due to the option to download using uv.It might be faster to download packages using pyCage than using the terminal in some cases.\n  \n    GitHub\n  \n    GitHub Link: https://github.com/qKitNp/pyCage"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "A few days ago I published the first version of my python library for simple usage and training of DeepMind's AlphaZero and MuZero algorithms. The library is designed for easy usage and extensibility, allowing you to define custom environments, memory buffers, networks, etc. while also providing default implementations for all of these.\n  What my project does\n    Allows for simple training and usage of the MuZero and AlphaZero models with focus on extensibility.\n  Target audience\n    Human-like Inhabitants of the planet earth.\n  Comparison\n    There are a couple of projects implementing one of the algorithms, but my project offers both algorithms in one easy to use library.\n  \nGithub link: https://github.com/Skirlax/MuAlphaZeroLibrary"
},
{
    "title": "No title",
    "content": "What my Project Does?\ncvGen is a CV generator made using PyQt6 and Python. It collects user data via inputs given by the user and then crafts a beautiful CV using built-in HTML and CSS templates.\n  Target Audience\n    People who want to make good looking CVs without using any command line tools / YAML formatting manually.\n  Comparison\n    For comparison, I don't know what to compare it to since there are no other UI-based CV generators. If you know any,  please let me know.\n  GitHub\n    GitHub Link: https://github.com/rohankishore/cvGen\nKnown Issues\n\n\n    Only one template as of now (looking forward to expanding it)\n  \n\n\n    Only support input via the UI (coding support for JSON/YAML)"
},
{
    "title": "No title",
    "content": "I've been building a lot of projects lately and got tired of upgrading pip each time I create a new venv (virtual environment), so I decided to look up solutions and found that I can use a script to run the steps I usually do manually.\n  \n    This is obviously not something new or that necessary but thought it would be fun and save a few seconds especially when am building new projects everyday.\n  \n    I created a short tutorial on how to do this: https://youtu.be/xMDh4TYoIB4?si=iEziqP5YQFj2wIHW"
},
{
    "title": "No title",
    "content": "Hi guys\n  \n\n    I made a starter template repo for creating Python bindings from a C library.. maybe someone else might find this useful - https://github.com/joegasewicz/cython-starter-template\n\n\n    thanks for looking"
},
{
    "title": "No title",
    "content": "Pandas is Great, But...\n    Pandas excels when your dataset is manageable in size, and you adhere strictly to its built-in functions. However, when either of these conditions is not met, performance drastically declines. Relying solely on custom, albeit user-friendly, built-in functions for decent performance seems suboptimal. Despite this, for many operations, learning an array of built-ins isn't necessary; you can accomplish a great deal using just a few key methods like apply, aggregate, groupby, and rolling (not addressing I/O here). However, the apply is known to be one of the worst in terms of performance, that's why I wanted to make it the best..!\n  \n    Nonetheless, pandas shines for several reasons, including its user-friendly API and widespread adoption.\n  Why I'm Skeptical of Alternatives\n    In the quest for better performance, you might consider options like Modin or Polars. However, my experience with them has been mixed: they either introduce a steep learning curve or fail to function in 90% of the environments I've tested.\n  The Essence of Pandopt\n    Pandopt aims to enhance basic pandas operations, such as apply and rolling, by implementing dynamic code transformation and JIT compilation via Numba. This approach significantly boosts performance without altering other aspects of pandas functionality.\n  Target Audience\n    This message is intended for data science enthusiasts, optimization fans, and those who appreciate elegant coding.\n  Performance Comparison\n    Pandopt dramatically outperforms pandas in executing apply functions on large datasets (over 1 million rows), offering speed improvements of approximately 10,000 to 20,000 times post-initial compilation (which is cached). Even for standard operations like sum, it outpaces numpy or Polars by roughly 8 times. While gains in rolling functions are less pronounced, Pandopt still enhances flexibility, allowing the application of custom functions across multiple columns and functions natively.\n  \n    Project is public on git and available on Pypi even if unstable: https://github.com/remigenet/pandopt"
},
{
    "title": "No title",
    "content": "While working at my current company, I developed a PDF text search and extraction library based on pdfminer.six. We open-sourced the library, and would love to know if this could help you! :)\n  \nWhat My Project Does:hotpdf was developed as an alternative to libraries like pdfquery with a focus on text search and extraction. The project was built to be memory efficient and have fast text searching and extraction on PDF documents.\n  \nTarget Audience:If you work with PDFs, especially performing text operations search and extraction by bounding box, then this library is for you. In my current company, we use it for this exact use case.\n  \n    For example: We want to extract the value associated with IBAN: in a document. We search for the \"IBAN\" and extract text around the surrounding coordinates.\n  \nComparison:For search, it implements a Trie, and for coordinate-based extraction, it implements a Sparse Matrix. These 2 data structures are created on each page.\n  \n    This helps the loading of PDFs into the memory fast. And since every page has its data structure implementation, it's easy to parallelize the whole process and merge the objects into one.\n  \nAgainst pdfquery, it's around 5-8x times faster and uses 5-10x less memory.\n  \n    All benchmarks were performed on test files that are found on the repo itself :) (plus some confidential internal files to test our whole flow)\n  \n    Git Repo: https://github.com/weareprestatech/hotpdf"
},
{
    "title": "No title",
    "content": "Hello All, SpeechBrain 1.0 has just come out, and I wanted to post a project showcase to give the project some more exposure. There is so much you can do with SpeechBrain and it's completely free and open source!\n  What The Project Does\n    SpeechBrain is a completely free and open-source PyTorch toolkit for Conversational AI development, the technology behind things like speech assistants, chatbots, and large language models. With the toolkit you can perform tasks like speech recognition, speaker recognition, speech enhancement, speech separation, language modeling, dialogue, and much much more! There are over 200+ pre-made recipes you can use to train your own models for all kinds of tasks!\n  \n    Sometimes training models is extremely expensive though, and isn't possible for the average person. That's why the project also features 100+ pretrained models uploaded to hugging face that are free to download and use! By just creating an instance of a class and calling a function, you can use these models yourself. An example is transcribing speech in languages like English, French, Italian, and Mandarin to text! This is just a small taste of what you can do with SpeechBrain!\n  Target Audience\n    The project is intended for anyone interested in speech processing, natural language processing, and machine learning.\n  \n    Users of SpeechBrain range from those looking to build a product, researchers looking to discover something new, users looking to build their own tools/scripts and anything else you can think of!\n  Comparison\n    There are some alternative like NeMO and ESPNET. They are all good toolkits. SpeechBrain stands for its simplicity and flexibility that make it suitable for things like research and fast development of Conversational AI technologies.\n  \n    Thank you to the community for all the support!\n  \n    Useful Links:\n  \nWebsite | Tutorials | Twitter | HuggingFace | Contributing"
},
{
    "title": "No title",
    "content": "I have been using python to code for almost 2 years and wanted to know what all IDEs people use ? So I can make a wise choice. TIA"
},
{
    "title": "No title",
    "content": "Now you can use Airbyte source connectors to process data in memory with Python.\n  \nhttps://pathway.com/developers/showcases/etl-python-airbyte\n\nWhat My Project Does\n\n    We integrated Airbyte connectors with Pathway, a Python stream processing framework, using the airbyte-serverless project. ETL pipelines are coming back with many use cases in AI (RAG pipelines), ETL for unstructured data and pipelines that deal with PII data.In this article, we show how to stream data from Github using Airbyte and remove PII data with Pathway.\n  \nTarget Audience\n\n    This is a production-ready approach, to be used as a template for streaming ETL production settings.\n  \nComparison\n\n    The setup is meant as an alternative to ELT setups (like Fivetran/Airbyte + dbt + warehouse), applying transform-before-load with Python. We are curious on your feedback on the implementation and other use cases you may think of from decoupling the extract and load steps.For the brave who want to see how it's done: https://github.com/pathwaycom/pathway/blob/main/python/pathway/io/airbyte/ + https://github.com/unytics/airbyte_serverless"
},
{
    "title": "No title",
    "content": "clifunction\n\n    I just published the first production quality version of my library clifunction!\n  \n    Code: https://github.com/Isaak-Malers/clifunction\n\n    PyPi: https://pypi.org/project/clifunction/\n\n    Documentation: https://isaak-malers.github.io/clifunction/\n\nWhat my project does\n\n    Builds a fully featured CLI from your python functions.  All you need to do is annotate your functions fully, put the @cli_function decorator on the function, and then call your new CLI from the terminal!\n  \nTarget audience\n\n    Anyone making small audience deployments of python code.  Are you writing scripts to distribute to 10's of collaborators?  This is a really fast way to reach MVP for shipping something out that can be run by technical people without an IDE/Editor.  Its also great for transitioning from running on a local machine, to running on a cloud process.  Academics, Data Scientists, Infastructure/Devops will find this is a great way to quickly share projects with their peers.\n  \nComparison\n\n    This fills a similar need to the ubiquitous \"argparse\", but this is a lot easier to throw on existing code.  When developing it I asked myself \"How can this make it as easy as possible to go from a script running in my IDE, to a script running in a docker container somewhere\".  It has fewer features than argparse, but it leverages pythons built in declarative annotations to maximize how quickly you can get something useable."
},
{
    "title": "No title",
    "content": "Thank you for the all the great feedbacks on my last post.\n  \n    Context:\n  \nWhat My Project Does?\n\n    Penify is a GitHub App that auto-generates and dynamically updates documentation for your repo. It supports various styles like Google, Numpy, reST, and Epydoc, ensuring your documentation stays current with every change.\n  \n    Check it out: [Penify GitHub App](https://github.com/apps/penify-dev).\n  \n    Here's how it works in action: [Sample PR](https://github.com/NVIDIA/trt-llm-rag-windows/pull/44/files).\n  \nUpdates:\n\n\n\n    Generates consistent style docstrings: Google, reST, Numpy, Epydoc.\n  \n\n\n    Reduces hallucination through improved prompts and more context.\n  \n\n\n    Offers Full Repo documentation generation - previously it used to generate docs for only code changes.\n  \n\n\nWorking on\n\n\n\n    ignore-private\n  \n\n\n    ignore-semiprivate\n  \n\n\n    ignore-init-method\n  \n\n\n    ignore-setters-getters\n  \n\n\nTarget Audience:\n\n\n\n    Ideal for freelance developers requiring high-quality project documentation.\n  \n\n\n    Perfect for project managers desiring updated codebase docs for smooth onboarding of new developers.\n  \n\n\nAlternatives: Copilot & Tabnine Cons:\n  \n\n\n    In consistent style docstring style - sometimes it generates Google,Numpy....\n  \n\n\n    Context selection is very limited in Co-pilot\n  \n\n\n    Need to manually do doc updates for each modified code; Penify does it automatically in the background(it detects code changes and generates docs)\n  \n\n\nPros:\n\n\n\n    You can update the modified docstring before pushing it.\n  \n\n\n    I know the community believes that Docstring generation shouldn't be done by LLMs - but I am seeing some good results. Hence, I am giving my effort. Please try and let me know your feedback."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hey!\n  \n    I've recently started to work on this new project, once I met the Ann Campbell's work just feel in love, really liked this way to measure understandability, cognitive complexity is a measurement of how difficult it is to understand a specific piece of code.\n  \n    Unlike cyclomatic complexity, which primarily focuses on control flow based on branches and decisions, cognitive complexity takes into account the cognitive load required to comprehend the code.\n  \ncomplexipy is extremely fast, and you can use it in your local files and directories or in your git repos to calculate the cognitive complexity, checkout the docs.\n  \n    As you can see, I was inspired also by the astral-sh way to describe their projects, really admire their work.\n  What my project does\n    I've created complexipy an extremely fast Python library to calculate the cognitive complexity of python files, written in Rust. This metric is measured by Sonar and I find it really useful when working on huge projects.\n  Target audience\n    Any developer that cares about understandability.\n  Comparison\n    I don't know if there's any python library that let you calculate the cognitive complexity, but this one is powered by Rust."
},
{
    "title": "No title",
    "content": "Check out my article on advanced web scraping techniques:\n  \n\n\n\n    Get and manage cookies and custom headers\n  \n\n\n    Avoid TLS fingerprinting\n  \n\n\n    Recognize important HTTP headers to send in requests\n  \n\n\n    Implement exponential-backoff HTTP request retrying.\n  \n\n\nhttps://jacobpadilla.com/articles/advanced-web-scraping-techniques"
},
{
    "title": "No title",
    "content": "Hey r/Python enthusiasts!I've recently wrapped up a fascinating project where I benchmark the performance of RSA and AES key generation using Python's cryptography library. The goal was to understand the time dynamics and security implications behind these two cryptographic heavyweights.\n  \nI did it to understand why we need both\n\n    Here's what went down:\n  \n\n\n    RSA (4089 bits) vs. AES (256 bits): I generated keys for both, focusing on RSA's asymmetric capabilities for secure messaging and AES's symmetric speed for bulk data encryption.\n  \n\n\n    Performance Measurement: I recorded the time taken to generate each key type, providing real-time insights into their operational efficiency.\n  \n\n\n    Output Documentation: Generated keys were saved in PEM format, alongside a detailed log of the time metrics in a 'result.txt' file.\n  \n\n\n    Load RSA Keys: You've already generated RSA keys in your project. Let's assume you have them ready. 5. Create a Signature: Use your private RSA key to create a signature for your message. The signature is created by encrypting a hash of the message with your private key. 6. Verify Signature: Anyone with your public key can verify the signature by decrypting it with the public key and comparing the hash value with their own hash of the original message. This project not only offered a hands-on experience with cryptographic key generation but also shed light on the practical aspects of implementing RSA and AES in Python. Would love to hear your thoughts, experiences, or any insights on improving the benchmarks or understanding the nuances better. Let's decrypt the complexities of cryptography together! GitHub Link: https://github.com/BDR-Pro/Comparing_RSA_AES Happy Encrypting! #python #cryptography #RSA #AES #security\n  \n\n\nWhat My Project Does\n\n    The project benchmarks the performance of RSA and AES key generation using Python's cryptography library. It focuses on analyzing the time required to generate cryptographic keys, evaluating the efficiency and practicality of RSA's asymmetric encryption against AES's symmetric encryption. The project extends beyond key generation, incorporating a demonstration of digital signature creation and verification with RSA, showcasing a real-world application of these cryptographic principles.\n  \nTarget Audience\n\n    This project is designed for cybersecurity enthusiasts, cryptography students, and developers interested in understanding and implementing encryption in their applications. It serves as an educational tool for those new to cryptography and as a benchmarking utility for more experienced users evaluating the performance and applicability of RSA and AES in their projects. While it can be used as a reference for production environments, its primary intent is educational.\n  \nComparison\n\n    Unlike many existing alternatives that focus solely on theoretical aspects or provide isolated examples of cryptographic functions, this project delivers a hands-on, comparative analysis between two pivotal cryptographic standards. It uniquely bridges the gap between theory and practice by providing a real-world application context, complete with performance metrics and operational insights. Furthermore, the inclusion of signature generation and verification with RSA provides a comprehensive overview that many tutorials or benchmarks lack, offering users a broader understanding of cryptographic applications beyond mere key generation."
},
{
    "title": "No title",
    "content": "Blog - https://astral.sh/blog/ruff-v0.3.0\n\n    Changes:\n  \n    - The Ruff 2024.2 style guide- Range Formatting- f-string placeholder formatting- Lint for invalid formatter suppression comments- Multiple new rules - both stable and in preview"
},
{
    "title": "No title",
    "content": "This document provides some fundamental and comparisons about virtual environment tools. The readers are expected to use one of similar tools once. If not, the readers should read 12. Virtual Environments and Packages — Python 3.12.2 documentation. Some concepts might be oversimplified.\n  The fundamental\n    Every beginner tutorial about creating virtual environment starts with venv or virtualenv .\n  \nvenv is a standard module for creating lightweight environment.\n  \nvirtualenv is a tool to create isolated environments. Since Python 3.3, a subset of it has been integrated into the standard library under the venv module.\n  \n    Eventually, the developers build management tools.\n  \nvirtualenvwrapper is a useful set of scripts for creating and deleting virtual environments.\n  \npyenv is a tool to install, isolate, manage, Python versions per user and per project.\n  \npyenv-virtualenv is a plugin of pyenv to manage virtualenv .\n  \npipenv is a Python virtualenv management tool that supports a multitude of systems and nicely bridges the gaps between pip, python (using system python, pyenv or asdf) and virtualenv.\n  The combinations\n    Black: virtual environment creation tools\n  \n    Red: virtual environment management tools\n  \n    Green: Python versions management tools\n  \nHere is the image\n\n    Here are some of the tool sets that people commonly use.\n  The comparisons\n\n\n\n\n            virtualenvwrapper\n          \n            pipenv pyenv\n          \n            pyenv pyenv-virtualenv\n          \n\n\n\n\n\n\n              Manage multiple Python versions\n            \n              No\n            \n              Yes\n            \n              Yes\n            \n\n\n              Manage per project\n            \n              No, you can have many venv per project and many projects per venv\n            \n              Yes\n            \n              No, like virtualenvwrapper.\n            \n\n\nThe holdup\n    The readers may ask what conda / mamaba and poetry are all about.\n  Conda-related\nconda is a system-level, binary package and environment manager running on all major operating systems and platforms.\n  \nmamba is conda implemented in c++.\n  \n    Many people might be more familiar with miniconda , micromamba which are minimal CLI versions of aforementioned tools.\n  \n    Conda is similar to pyenv pyenv-virtualenv but not limited to Python. One can install C++ packages such as Nvidia drivers.\n  \nImportant use case: people often use this for machine learning projects.\n  Poetry\npoetry is a tool for dependency management and packaging in Python. It allows you to declare the libraries your project depends on and it will manage (install/update) them for you. Poetry offers a lockfile to ensure repeatable installs, and can build your project for distribution. Poetry either uses your configured virtualenvs or creates its own to always be isolated from your system.\n  Reading list\npython - What is the difference between venv, pyvenv, pyenv, virtualenv, virtualenvwrapper, pipenv, etc? - Stack Overflow\n\nPython Virtual Environments: A Primer – Real Python\nReferences\nvenv — Creation of virtual environments — Python 3.12.2 documentation\n\nvirtualenv (pypa.io)\n\nvirtualenvwrapper 6.0.1.dev12+gb1aa495 — virtualenvwrapper 6.0.1.dev12+gb1aa495 documentation\n\nhttps://github.com/pyenv/pyenv\n\nhttps://github.com/pyenv/pyenv-virtualenv\n\nhttps://github.com/pypa/pipenv\n\nMiniconda — Anaconda documentation\n\nMicromamba Installation — documentation\n\nIntroduction | Documentation | Poetry - Python dependency management and packaging made easy (python-poetry.org)"
},
{
    "title": "No title",
    "content": "Hey everyone - really excited to showcase Hatchet, an OSS project I've been working on for the past few months: https://github.com/hatchet-dev/hatchet\nWhat My Project Does\n    Hatchet is a Celery alternative built for scale and observability. Specifically, it supports:\n  \nLow Latency and High Throughput Scheduling: Hatchet is built on a low-latency queue (25ms average start) so it can support real-time user interaction even while running from an async worker.\n  \nConcurrency, Fairness, and Rate Limiting: implements FIFO, LIFO, Round Robin, and Priority Queues with built-in strategies for limiting concurrency.\n  \nDAG workflows: Hatchet lets you declare tasks which are dependent on the execution of other tasks for full DAG-style execution.\n  \nDurability and error handling: all events and executions are persisted to Postgres. When a worker fails, tasks automatically get reassigned to new workers, and the workflow will pick up where it left off.\n  \nWeb UI and API for observability: visualize events, logs and workflows within the dashboard.\n  \nWorkflow replay: replay tasks, workflows, and events right from the UI or via the API.\n  Target Audience\n    This is being actively used in production at 5 companies, the largest of which is executing 50k tasks per day. It's ideal for companies looking to scale their async tasks or want more visibility into workflow progression.\n  Comparison\n    Celery is a clear alternative - and while it's a great framework, there are a few reasons to favor Hatchet:\n  \n    - Better observability - I've spent a lot of time in the Celery Flower UI or building Grafana views for exported prom metrics. We wanted to build a modern, dev-friendly dashboard. There's still a long way to go, but a core focus of the platform is dev experience after deployment to production.\n  \n    - Postgres-backed - when I started to build this, I wanted a transactional database that's easy to horizontally scale and can handle high volumes of writes. We are working on batching execution data and forwarding it to Clickhouse, so Postgres is a natural choice.\n  \n    - Networking - when deploying with Celery, each worker manages its own connection to the underlying broker using redis or amqp. With Hatchet, each worker connects via a long-lived gRPC connection, which makes it easier to distribute workers across different networks or clusters, as there is widespread support for proxying HTTP 2.0 rather than forwarding TLS which can get tricky.\n  FAQs\nHow can I get started?\n\n    Here's a quickstart repository for Python.\n  \nThis is written in Go, are you in the wrong place?\n\n    After the original engine was deployed, we spent the next month building our Python SDK. I made the decision to use a lower-level language for more control over the underlying engine runtime which makes it easier to optimize latency.\n  \nWhat's next on the roadmap?\n\n    Support for logging from a task execution and giving a great logs API + view on our dashboard. This also can be difficult to integrate well with Celery AFAIK.\n  Feedback\n    Would love to hear what you think - also feel free to join our Discord and share your thoughts there."
},
{
    "title": "No title",
    "content": "Hey, heres a tutorial I made that shows how to use Firebase with Tkinter to create a simple application that has authentication and which uses the Firestore database: https://youtu.be/hxuiRPUJMwU?si=QF0zevMvOmq6BN1K\n\n    Using Firebase is much easier than setting up your own backend and you can start using it for free!"
},
{
    "title": "No title",
    "content": "Hi, I am working on a stream processing framework called Bytewax and I recently gave a talk where I read live data from HackerNews API, cleaned it with unstructured.io, created embeddings with a hugging face model, and stored everything in Milvus.\n  \n    I find a lot of devs are struggling with real-time streaming. The steps are easy to follow, and it is an end-to-end Python: https://github.com/bytewax/real-time-milvusI also added diagrams and summarized it in a blog https://bytewax.io/blog/stream-process-embed-repeat"
},
{
    "title": "No title",
    "content": "https://github.com/parkervg/blendsql\n\nWhat my Project Does\n\n    BlendSQL is a superset of SQLite for blending together complex reasoning between vanilla SQL and LLM calls. It's got a bunch of optimizations implemented using the amazing pure Python parser sqlglot to minimize the number of external LLM calls made. It allows you to efficiently perform tasks ranging from simple table question-answering:\n  {{  \n    LLMQA(  \n        'Give me a short summary of my spending.',  \n        context=(SELECT * FROM spending)  \n    )  \n}}\n    ...to complex question-answering with unstructured data using constrained decoding:\n  \n(BlendSQL corresponding to 'List the term lengths of all U.S. presidents from the 3rd most populous city in Ohio')\nSELECT {{LLMMap('How long did they serve?', 'presidents::term')}} FROM presidents  \n    WHERE birthplace = {{  \n        LLMQA(  \n            'Which city is the 3rd most populous city in Ohio?',  \n            (SELECT * FROM documents WHERE documents MATCH 'ohio'),  \n            options='presidents::birthplace'  \n        )  \n    }}\nTarget Audience\n\n    BlendSQL can be used in a couple ways - by directly writing the queries yourself as you would normal SQL to operate over a database, or as an intermediate representation that is taught to a LLM via few-shot learning. In our research, we show that using BlendSQL as an intermediate representation for complex question answering tasks boosts performance over the traditional end-to-end approach while using substantially fewer tokens.\n  \nComparison\n\n    While other existing tools enable text-to-SQL using LLMs to write queries, this tool enables the user to oversee all calls (LLM + SQL) within a unified query language.\n  \n    Appreciate any feedback folks have! Thanks."
},
{
    "title": "No title",
    "content": "I used jupyterlite, pyodide and the PyArma module to implement the method described in this stackexchange question.\n  \n    The jupyterlite site with the notebooks is: http://juangburgos.github.io/FitSumExponentials/"
},
{
    "title": "No title",
    "content": "What my project does\n\nThis project is a Discord bot that lets you talk with document collections stored in the Qdrant vector database.\n  \n    The project comes with a Discord bot written using discord.py, and a backend written in FastAPI. I work for Hasura as a Community Engineer and we are in the Developer tooling space and so we get lots of support requests from developers. I built this bot to help answer questions that should be covered in our technical documentation. There is a blog post written in the README, and setup instructions in SETUP.md. This bot lives in our Discord server and automatically responds to requests in our help forum.\n  \nTarget Audience We are running this in production, and you probably could too. I won’t say it’s perfect, but it would be a really great project for someone wanting to get started with event-driven programming, interested in building a Discord bot, or who was curious about Hasura. I would definitely say this is something you want to be relatively comfortable with Python for it to be approachable. It uses a decent handful of some of my favorite python libraries like FastAPI, selenium, beautifulsoup4, and others.\n  \nComparison There are lots of chatbots coming out right now, and some might be better than this. This isn’t anything more than a simple case of RAG. The specific use case we wanted was a bot that would automatically try to answer help-forum questions and provide links to documentation. I’m in a handful of programming related Discord servers, and but just because I haven’t seen a bot like this in the ones I am in doesn’t mean there aren’t others. I think what’s maybe most useful is how easy it is to run this yourself because of the Docker-compose."
},
{
    "title": "No title",
    "content": "What Does My Project Do?\n  \npyCage is a VSCode extension powered by astral.sh's uv package manager and pip. It enables users to search for and download Python packages directly from the command palette, similar to Dart's package manager.\n  \n    Target Audience\n  \n    This extension is designed for individuals who frequently use venvs and struggle with remembering package names.\n  \n    Comparison\n  \n    The installation speed is notably fast due to the option to download using uv.It might be faster to download packages using pyCage than using the terminal in some cases.\n  \n    GitHub\n  \n    GitHub Link: https://github.com/qKitNp/pyCage"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "A few days ago I published the first version of my python library for simple usage and training of DeepMind's AlphaZero and MuZero algorithms. The library is designed for easy usage and extensibility, allowing you to define custom environments, memory buffers, networks, etc. while also providing default implementations for all of these.\n  What my project does\n    Allows for simple training and usage of the MuZero and AlphaZero models with focus on extensibility.\n  Target audience\n    Human-like Inhabitants of the planet earth.\n  Comparison\n    There are a couple of projects implementing one of the algorithms, but my project offers both algorithms in one easy to use library.\n  \nGithub link: https://github.com/Skirlax/MuAlphaZeroLibrary"
},
{
    "title": "No title",
    "content": "What my Project Does?\ncvGen is a CV generator made using PyQt6 and Python. It collects user data via inputs given by the user and then crafts a beautiful CV using built-in HTML and CSS templates.\n  Target Audience\n    People who want to make good looking CVs without using any command line tools / YAML formatting manually.\n  Comparison\n    For comparison, I don't know what to compare it to since there are no other UI-based CV generators. If you know any,  please let me know.\n  GitHub\n    GitHub Link: https://github.com/rohankishore/cvGen\nKnown Issues\n\n\n    Only one template as of now (looking forward to expanding it)\n  \n\n\n    Only support input via the UI (coding support for JSON/YAML)"
},
{
    "title": "No title",
    "content": "I've been building a lot of projects lately and got tired of upgrading pip each time I create a new venv (virtual environment), so I decided to look up solutions and found that I can use a script to run the steps I usually do manually.\n  \n    This is obviously not something new or that necessary but thought it would be fun and save a few seconds especially when am building new projects everyday.\n  \n    I created a short tutorial on how to do this: https://youtu.be/xMDh4TYoIB4?si=iEziqP5YQFj2wIHW"
},
{
    "title": "No title",
    "content": "Hi guys\n  \n\n    I made a starter template repo for creating Python bindings from a C library.. maybe someone else might find this useful - https://github.com/joegasewicz/cython-starter-template\n\n\n    thanks for looking"
},
{
    "title": "No title",
    "content": "Pandas is Great, But...\n    Pandas excels when your dataset is manageable in size, and you adhere strictly to its built-in functions. However, when either of these conditions is not met, performance drastically declines. Relying solely on custom, albeit user-friendly, built-in functions for decent performance seems suboptimal. Despite this, for many operations, learning an array of built-ins isn't necessary; you can accomplish a great deal using just a few key methods like apply, aggregate, groupby, and rolling (not addressing I/O here). However, the apply is known to be one of the worst in terms of performance, that's why I wanted to make it the best..!\n  \n    Nonetheless, pandas shines for several reasons, including its user-friendly API and widespread adoption.\n  Why I'm Skeptical of Alternatives\n    In the quest for better performance, you might consider options like Modin or Polars. However, my experience with them has been mixed: they either introduce a steep learning curve or fail to function in 90% of the environments I've tested.\n  The Essence of Pandopt\n    Pandopt aims to enhance basic pandas operations, such as apply and rolling, by implementing dynamic code transformation and JIT compilation via Numba. This approach significantly boosts performance without altering other aspects of pandas functionality.\n  Target Audience\n    This message is intended for data science enthusiasts, optimization fans, and those who appreciate elegant coding.\n  Performance Comparison\n    Pandopt dramatically outperforms pandas in executing apply functions on large datasets (over 1 million rows), offering speed improvements of approximately 10,000 to 20,000 times post-initial compilation (which is cached). Even for standard operations like sum, it outpaces numpy or Polars by roughly 8 times. While gains in rolling functions are less pronounced, Pandopt still enhances flexibility, allowing the application of custom functions across multiple columns and functions natively.\n  \n    Project is public on git and available on Pypi even if unstable: https://github.com/remigenet/pandopt"
},
{
    "title": "No title",
    "content": "While working at my current company, I developed a PDF text search and extraction library based on pdfminer.six. We open-sourced the library, and would love to know if this could help you! :)\n  \nWhat My Project Does:hotpdf was developed as an alternative to libraries like pdfquery with a focus on text search and extraction. The project was built to be memory efficient and have fast text searching and extraction on PDF documents.\n  \nTarget Audience:If you work with PDFs, especially performing text operations search and extraction by bounding box, then this library is for you. In my current company, we use it for this exact use case.\n  \n    For example: We want to extract the value associated with IBAN: in a document. We search for the \"IBAN\" and extract text around the surrounding coordinates.\n  \nComparison:For search, it implements a Trie, and for coordinate-based extraction, it implements a Sparse Matrix. These 2 data structures are created on each page.\n  \n    This helps the loading of PDFs into the memory fast. And since every page has its data structure implementation, it's easy to parallelize the whole process and merge the objects into one.\n  \nAgainst pdfquery, it's around 5-8x times faster and uses 5-10x less memory.\n  \n    All benchmarks were performed on test files that are found on the repo itself :) (plus some confidential internal files to test our whole flow)\n  \n    Git Repo: https://github.com/weareprestatech/hotpdf"
},
{
    "title": "No title",
    "content": "Hello All, SpeechBrain 1.0 has just come out, and I wanted to post a project showcase to give the project some more exposure. There is so much you can do with SpeechBrain and it's completely free and open source!\n  What The Project Does\n    SpeechBrain is a completely free and open-source PyTorch toolkit for Conversational AI development, the technology behind things like speech assistants, chatbots, and large language models. With the toolkit you can perform tasks like speech recognition, speaker recognition, speech enhancement, speech separation, language modeling, dialogue, and much much more! There are over 200+ pre-made recipes you can use to train your own models for all kinds of tasks!\n  \n    Sometimes training models is extremely expensive though, and isn't possible for the average person. That's why the project also features 100+ pretrained models uploaded to hugging face that are free to download and use! By just creating an instance of a class and calling a function, you can use these models yourself. An example is transcribing speech in languages like English, French, Italian, and Mandarin to text! This is just a small taste of what you can do with SpeechBrain!\n  Target Audience\n    The project is intended for anyone interested in speech processing, natural language processing, and machine learning.\n  \n    Users of SpeechBrain range from those looking to build a product, researchers looking to discover something new, users looking to build their own tools/scripts and anything else you can think of!\n  Comparison\n    There are some alternative like NeMO and ESPNET. They are all good toolkits. SpeechBrain stands for its simplicity and flexibility that make it suitable for things like research and fast development of Conversational AI technologies.\n  \n    Thank you to the community for all the support!\n  \n    Useful Links:\n  \nWebsite | Tutorials | Twitter | HuggingFace | Contributing"
},
{
    "title": "No title",
    "content": "I have been using python to code for almost 2 years and wanted to know what all IDEs people use ? So I can make a wise choice. TIA"
},
{
    "title": "No title",
    "content": "Now you can use Airbyte source connectors to process data in memory with Python.\n  \nhttps://pathway.com/developers/showcases/etl-python-airbyte\n\nWhat My Project Does\n\n    We integrated Airbyte connectors with Pathway, a Python stream processing framework, using the airbyte-serverless project. ETL pipelines are coming back with many use cases in AI (RAG pipelines), ETL for unstructured data and pipelines that deal with PII data.In this article, we show how to stream data from Github using Airbyte and remove PII data with Pathway.\n  \nTarget Audience\n\n    This is a production-ready approach, to be used as a template for streaming ETL production settings.\n  \nComparison\n\n    The setup is meant as an alternative to ELT setups (like Fivetran/Airbyte + dbt + warehouse), applying transform-before-load with Python. We are curious on your feedback on the implementation and other use cases you may think of from decoupling the extract and load steps.For the brave who want to see how it's done: https://github.com/pathwaycom/pathway/blob/main/python/pathway/io/airbyte/ + https://github.com/unytics/airbyte_serverless"
},
{
    "title": "No title",
    "content": "clifunction\n\n    I just published the first production quality version of my library clifunction!\n  \n    Code: https://github.com/Isaak-Malers/clifunction\n\n    PyPi: https://pypi.org/project/clifunction/\n\n    Documentation: https://isaak-malers.github.io/clifunction/\n\nWhat my project does\n\n    Builds a fully featured CLI from your python functions.  All you need to do is annotate your functions fully, put the @cli_function decorator on the function, and then call your new CLI from the terminal!\n  \nTarget audience\n\n    Anyone making small audience deployments of python code.  Are you writing scripts to distribute to 10's of collaborators?  This is a really fast way to reach MVP for shipping something out that can be run by technical people without an IDE/Editor.  Its also great for transitioning from running on a local machine, to running on a cloud process.  Academics, Data Scientists, Infastructure/Devops will find this is a great way to quickly share projects with their peers.\n  \nComparison\n\n    This fills a similar need to the ubiquitous \"argparse\", but this is a lot easier to throw on existing code.  When developing it I asked myself \"How can this make it as easy as possible to go from a script running in my IDE, to a script running in a docker container somewhere\".  It has fewer features than argparse, but it leverages pythons built in declarative annotations to maximize how quickly you can get something useable."
},
{
    "title": "No title",
    "content": "Thank you for the all the great feedbacks on my last post.\n  \n    Context:\n  \nWhat My Project Does?\n\n    Penify is a GitHub App that auto-generates and dynamically updates documentation for your repo. It supports various styles like Google, Numpy, reST, and Epydoc, ensuring your documentation stays current with every change.\n  \n    Check it out: [Penify GitHub App](https://github.com/apps/penify-dev).\n  \n    Here's how it works in action: [Sample PR](https://github.com/NVIDIA/trt-llm-rag-windows/pull/44/files).\n  \nUpdates:\n\n\n\n    Generates consistent style docstrings: Google, reST, Numpy, Epydoc.\n  \n\n\n    Reduces hallucination through improved prompts and more context.\n  \n\n\n    Offers Full Repo documentation generation - previously it used to generate docs for only code changes.\n  \n\n\nWorking on\n\n\n\n    ignore-private\n  \n\n\n    ignore-semiprivate\n  \n\n\n    ignore-init-method\n  \n\n\n    ignore-setters-getters\n  \n\n\nTarget Audience:\n\n\n\n    Ideal for freelance developers requiring high-quality project documentation.\n  \n\n\n    Perfect for project managers desiring updated codebase docs for smooth onboarding of new developers.\n  \n\n\nAlternatives: Copilot & Tabnine Cons:\n  \n\n\n    In consistent style docstring style - sometimes it generates Google,Numpy....\n  \n\n\n    Context selection is very limited in Co-pilot\n  \n\n\n    Need to manually do doc updates for each modified code; Penify does it automatically in the background(it detects code changes and generates docs)\n  \n\n\nPros:\n\n\n\n    You can update the modified docstring before pushing it.\n  \n\n\n    I know the community believes that Docstring generation shouldn't be done by LLMs - but I am seeing some good results. Hence, I am giving my effort. Please try and let me know your feedback."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hey!\n  \n    I've recently started to work on this new project, once I met the Ann Campbell's work just feel in love, really liked this way to measure understandability, cognitive complexity is a measurement of how difficult it is to understand a specific piece of code.\n  \n    Unlike cyclomatic complexity, which primarily focuses on control flow based on branches and decisions, cognitive complexity takes into account the cognitive load required to comprehend the code.\n  \ncomplexipy is extremely fast, and you can use it in your local files and directories or in your git repos to calculate the cognitive complexity, checkout the docs.\n  \n    As you can see, I was inspired also by the astral-sh way to describe their projects, really admire their work.\n  What my project does\n    I've created complexipy an extremely fast Python library to calculate the cognitive complexity of python files, written in Rust. This metric is measured by Sonar and I find it really useful when working on huge projects.\n  Target audience\n    Any developer that cares about understandability.\n  Comparison\n    I don't know if there's any python library that let you calculate the cognitive complexity, but this one is powered by Rust."
},
{
    "title": "No title",
    "content": "Check out my article on advanced web scraping techniques:\n  \n\n\n\n    Get and manage cookies and custom headers\n  \n\n\n    Avoid TLS fingerprinting\n  \n\n\n    Recognize important HTTP headers to send in requests\n  \n\n\n    Implement exponential-backoff HTTP request retrying.\n  \n\n\nhttps://jacobpadilla.com/articles/advanced-web-scraping-techniques"
},
{
    "title": "No title",
    "content": "Hey r/Python enthusiasts!I've recently wrapped up a fascinating project where I benchmark the performance of RSA and AES key generation using Python's cryptography library. The goal was to understand the time dynamics and security implications behind these two cryptographic heavyweights.\n  \nI did it to understand why we need both\n\n    Here's what went down:\n  \n\n\n    RSA (4089 bits) vs. AES (256 bits): I generated keys for both, focusing on RSA's asymmetric capabilities for secure messaging and AES's symmetric speed for bulk data encryption.\n  \n\n\n    Performance Measurement: I recorded the time taken to generate each key type, providing real-time insights into their operational efficiency.\n  \n\n\n    Output Documentation: Generated keys were saved in PEM format, alongside a detailed log of the time metrics in a 'result.txt' file.\n  \n\n\n    Load RSA Keys: You've already generated RSA keys in your project. Let's assume you have them ready. 5. Create a Signature: Use your private RSA key to create a signature for your message. The signature is created by encrypting a hash of the message with your private key. 6. Verify Signature: Anyone with your public key can verify the signature by decrypting it with the public key and comparing the hash value with their own hash of the original message. This project not only offered a hands-on experience with cryptographic key generation but also shed light on the practical aspects of implementing RSA and AES in Python. Would love to hear your thoughts, experiences, or any insights on improving the benchmarks or understanding the nuances better. Let's decrypt the complexities of cryptography together! GitHub Link: https://github.com/BDR-Pro/Comparing_RSA_AES Happy Encrypting! #python #cryptography #RSA #AES #security\n  \n\n\nWhat My Project Does\n\n    The project benchmarks the performance of RSA and AES key generation using Python's cryptography library. It focuses on analyzing the time required to generate cryptographic keys, evaluating the efficiency and practicality of RSA's asymmetric encryption against AES's symmetric encryption. The project extends beyond key generation, incorporating a demonstration of digital signature creation and verification with RSA, showcasing a real-world application of these cryptographic principles.\n  \nTarget Audience\n\n    This project is designed for cybersecurity enthusiasts, cryptography students, and developers interested in understanding and implementing encryption in their applications. It serves as an educational tool for those new to cryptography and as a benchmarking utility for more experienced users evaluating the performance and applicability of RSA and AES in their projects. While it can be used as a reference for production environments, its primary intent is educational.\n  \nComparison\n\n    Unlike many existing alternatives that focus solely on theoretical aspects or provide isolated examples of cryptographic functions, this project delivers a hands-on, comparative analysis between two pivotal cryptographic standards. It uniquely bridges the gap between theory and practice by providing a real-world application context, complete with performance metrics and operational insights. Furthermore, the inclusion of signature generation and verification with RSA provides a comprehensive overview that many tutorials or benchmarks lack, offering users a broader understanding of cryptographic applications beyond mere key generation."
},
{
    "title": "No title",
    "content": "Hey everyone!\n  \n    I wrote a Flask application that transcribes phone calls in real-time, feel free to check out if you're looking to do something similar:\n  \n\n\nGitHub repo\n\n\n\nYouTube video tutorial\n\n\n\nBlog tutorial"
},
{
    "title": "No title",
    "content": "Hi! I am Saurabh. I love writing fast programs and I've always hated how slow Python code can sometimes be. To solve this problem, I have created Codeflash.\n  What My Project Does\ncodeflash is a Python package that uses AI to figure out the most performant way to rewrite a Python code. It not only optimizes the performance but also verifies the correctness of the new code, i.e. makes sure that the new code follows exactly the same behavior as your original code. This automates the manual optimization process.\n  \n    It can improve algorithms, data structures, fix logic, use better optimized libraries etc to speed up your code.\n  \n    Website - https://www.codeflash.ai/ , get started here.\n  \n    PyPi - https://pypi.org/project/codeflash/\n\n    If you have a Python project, it should take you less than 5 minutes to setup codeflash - pip install codeflash and codeflash init\n\n    Codeflash can also optimize your entire project! Run codeflash --all after setting up codeflash, and codeflash will optimize your project, function by function, and create PRs on GitHub when it finds an optimization. This is super powerful.You can also install codeflash as a GitHub actions check that runs on every new PR you create, to ensure that all new code is performant. If codeflash finds that a code can be made more performant, it will create a PR comment with the new optimized code. This ensures that your project stays at peak performance everytime.\n  How it works\n    Codeflash works by optimizing the code path under a function. So if there is a function foo(a, b): , codeflash finds the fastest implementation of the function foo and all the other functions it calls. The optimization procedure preserves the signature of the function foo and then figures out a new optimized implementation that results in exactly the same return values as the original foo. The behavior of the new function is verified to be correct by running your unit tests and generating a bunch of new regression tests. The runtime of the new code is measured and the fastest one is recommended.\n  Target Audience\n    Codeflash is currently the best at optimizing pure-functions without side effects. You can use codeflash to improve performance of any custom algorithm, numpy code, pandas code, data processing code etc. It is very general purpose. You should be able to optimize anything that can be unit-tested.\n  \n    You can also try to optimize non-pure functions but you should review the new code. We are improving support to more types of functions. Would love to know about your use case and how we can support it!\n  Comparison\n    I am currently unaware of any direct comparison with codeflash on optimizing performance of user level code.\n  \n    Codeflash is still early but has gotten great results already by optimizing open source projects like Langchain and many others. I would love you to try codeflash to optimize your code and let me know how you use it and how we can improve it for you!\n  \n    Thank you."
},
{
    "title": "No title",
    "content": "This has been a long standing issue on Pycharm and nobody on the company seems interested in it. Looks like somebody went ahead and created a plugin that from a quick check looks like it works fine:\n  \n    I did not make this.\n  \nhttps://github.com/souperk/pycharm-sqlalchemy-plugin"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "If you work with spatio temporal data like satellite imagery, SAR, Point Clouds, etc. Try to use Spatio Temporal Data Catalog (STAC). Take a look this tutorial that will explain step by step approach how to use STAC in Python from searching data, display it and do analysis the data in a row.\n  \nhttps://www.geodose.com/2024/02/pystac-decoded-step-by-step-tutorial.html"
},
{
    "title": "No title",
    "content": "Hello, I'd like to share a recent project of mine. infinite Sides! Screenshots\n\n    Project Repo: https://github.com/Fus3n/infinite-sides\nWhat my project does\n    I recently played the game called Infinite Craft and it was quite fun to see how you can craft anything from any word you can check out the original game here.\n  \n    This game uses LLM to generate words based on a combination of other words or two words, you get a few words that you start with and you can go with it and come up with infinite combinations (well as far as an LLM can go in theory, its not perfect).\n  \n    So I decided to make it in Python and PySide6 right after discovering Ollama, which is a way to run local LLMs pretty fast.\n  \n    So you can just boot up Ollama and my app will use its local API  to communicate with LLMs to generate texts in your computer, so no need for the internet, but its main feature is that you can just create your OWN infinitely crafting game, for example, a Minecraft themed one, Example Here\n\n    So you can basically create any kind of game!\n  \n    You can also forward the port Ollama is hosted on by default using ngrok and share the link with other people which they can use to play the game in infinite-sides, all they have to do is change the base URL in settings, so you can give access to your model to your friends if they don't have a beefy computer to run LLMs.\n  Target audience\n    If you enjoy games like this you also might enjoy this one!\n  Comparison\n    I don't know If there are already any apps/games like this either way this is my version just a project I did and hope people can enjoy it!\n  \n\n\n    All instructions to run and use it are in the README, there is currently no binaries but will be in the future.\n  \n\n\n    Also, I am not good with \"prompt engineering\" so the current default system prompt is pretty bad and might not give the best experience, So I suggest you edit it in the settings and try different variations if you want to give the game a try."
},
{
    "title": "No title",
    "content": "Our company is currently developing a book/guide for the PCEP certification. While there are numerous resources available, we aim to create a comprehensive book covering essential aspects such as tips and tricks, example questions, brain teasers, and case studies.\n  \n    I'd love to hear your suggestions for additional content that would be particularly valuable and not commonly found in existing resources on this certification."
},
{
    "title": "No title",
    "content": "Recently, I wrote a blog post on ChemPy, a Python package, for balancing chemical equations. I hope you find it useful.\n  \nhttps://chemistryprogramming.xyz/2024/02/chempy-balancing-equations-python/"
},
{
    "title": "No title",
    "content": "Why do, garbage collected languages like python, not implement resource management concepts in a sane way?C++ and Rust have RAII, which is awesome.Python has __enter__, __exit__, with ... as ..., try: ... finally:.But that is extremely limited:Returning a resource prevents the user to use the safe with statement, the resource is closed.And even if it would work, how should the caller know?Since python never cared, this would be an improvement.\n  \n    On top writing with for all resources, especially when working with resources, creating resources, is very cumbersome. The code is blown up by with or try ... finally/catch blocks. And it is still unsafe because you will forget to use these wherever they are required.\n  \n    The best solution to this would be, to clean up all uncatched resources automatically on scope exit.Python already knows that they are resources, since they implement __enter__ and __exit__.Python just has to prevent the creation of strong references from resources. Weak references should still creatable with weakref. In the end this would be only syntactic sugar, but it would make the resource management way safer and on top more convenient:\n          f = open(__file__, \"r\")  \n        some_code_may_throw(f)  \n        return f  \n    would compile to\n          f = open(__file__, \"r\")  \n        try:  \n            some_code_may_throw(f)  \n            return f  \n        except:\n            f.close()\n            raise"
},
{
    "title": "No title",
    "content": "It highlights all matches including groups so you can quickly test regex patterns: https://python-fiddle.com/tools/regex"
},
{
    "title": "No title",
    "content": "What my project does\nsuitespotauth provides a simple wrapper for the authentication flow required by SuiteSpot construction management software. It exposes a secure access_token attribute which you need (for Bearer Authorization) to make API calls to get your construction data from SuiteSpot.\n  \n    This cuts out all the boilerplate flow required of creating Basic Authorization to generate an API token and exchange it for the required access token.\n  \n    GitHub: https://github.com/yhavin/suitespotauth\n\n    Homepage: https://pypi.org/project/suitespotauth\nTarget audience\n    Available for use in production. The target audience is for developers who want to make API calls to SuiteSpot analytics to retrieve their construction data, but do not want to write 100 lines of code just to get the access token.\n  Comparison\n    There is no known other package providing a wrapper to the authentication flow. The official authentication docs are available at https://auth.suitespot.io/api#/.\n  Development\n    This package is being actively developed, and all replies/issued will be read.\n  \n    Thank you!"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Note, Tyche is an active research project and we're looking for users to help us do a formal evaluation study. If you're interested in helping out and giving feedback on the tool, please fill out this Google form.\n  \n\n    First of all, if you haven't heard of Hypothesis, check it out. It's a \"library for creating unit tests which are simpler to write and more powerful when run, finding edge cases in your code you wouldn’t have thought to look for\" (from the docs) and it's my favorite way to test Python code.\n  \n    But if you're a long-time user of Hypothesis, you've likely run into a situation where a Hypothesis test wasn't actually doing what you thought it was doing. Maybe you thought you were testing on positive integers, but you were actually wasting time generating lots of negative ones too. Or maybe a from_type generator just didn't produce the values you thought it would. Sometimes Hypothesis tells you about these mistakes, but often you don't notice until a bug made it into production.\n  \nWhat My Project Does Tyche is an open source (source) Visual Studio Code extension that helps you analyze your Hypothesis tests and make sure they're exactly as thorough as you expect. The interface shows you charts that summarize the distribution of data used to test your code and helps to warn you if things seem off. The extension has already helped the Hypothesis team to fix a couple of bugs in the Hypothesis framework itself, and it may be able to help you improve your own test suites too.\n  \nTarget Audience Users of Hypothesis. Even if you're pretty confident that your tests are already as effective as they can be, it's worth using Tyche to double check.\n  \nComparison As far as I know, there aren't other tools like Tyche out there yet. This project is part of cutting-edge research at a major university."
},
{
    "title": "No title",
    "content": "Hi guys!I created my very first python lib. It's a server/client solution that allows to control  GPIO pins of a remote Raspberry Pi.\n  \n    GitHub: https://github.com/schech1/remoteioHomepage: https://pypi.org/project/remoteio/\n\nWhat My Project Doesremoteio creates a socket and can run as a deamon at startup. The remoteio python lib provides gpiozero-like commands to control outputs on the remote Raspberry Pi.\n  \nTarget AudienceMakers who want to control the GPIO outputs of one or more remote Raspberry Pis.\n  \nComparison\n\n    The previously known remote GPIO option of Raspberry Pi OS is provided by pigpiod , which does not work on the Raspberry Pi 5 anymore.\n  \n    The project is still small and currently only offers to toggle outputs.I plan to add also inputs and I2c/SPI in future updates.I appreciate your support!"
},
{
    "title": "No title",
    "content": "I released RenderCV a while ago with this post. Today, I released v1 of RenderCV, and it's much more capable now. I hope it will help people to automate their CV generation process and version-control their CVs.\n  What My Project Does\n    RenderCV is a LaTeX CV/resume generator from a JSON/YAML input file. The primary motivation behind the RenderCV is to allow the separation between the content and design of a CV.\n  \n    It takes a YAML file that looks like this:\n  cv:\n  name: John Doe\n  location: Your Location\n  email: youremail@yourdomain.com\n  phone: tel:+90-541-999-99-99\n  website: https://yourwebsite.com/\n  social_networks:\n    - network: LinkedIn\n      username: yourusername\n    - network: GitHub\n      username: yourusername\n  sections:\n    summary:\n      - This is an example resume to showcase the capabilities\n        of the open-source LaTeX CV generator, [RenderCV](https://github.com/sinaatalay/rendercv).\n        A substantial part of the content is taken from [here](https://www.careercup.com/resume),\n        where a *clean and tidy CV* pattern is proposed by **Gayle\n        L. McDowell**.\n    education:\n    ...\n    And then produces these PDFs and their LaTeX code:\n  \n\n\n\nclassic theme\n          \nsb2nov theme\n          \nmoderncv theme\n          \nengineeringresumes theme\n          \n\n\n\n\n\n\nExample PDF,\n            \nExample PDF\n\nExample PDF\n\nExample PDF\n\n\n\nCorresponding YAML\n\nCorresponding YAML\n\nCorresponding YAML\n\nCorresponding YAML\n\n\n\n\n    It also generates an HTML file so that the content can be pasted into Grammarly for spell-checking. See README.md of the repository.\n  \n    RenderCV also validates the input file, and if there are any problems, it tells users where the issues are and how they can fix them.\n  \n    I recorded a short video to introduce RenderCV and its capabilities:\n  \nhttps://youtu.be/0aXEArrN-_c\nTarget Audience\n    Anyone who would like to generate an elegant CV from a YAML input.\n  Comparison\n    I don't know of any other LaTeX CV generator tools implemented with Python."
},
{
    "title": "No title",
    "content": "I just wrote a plugin that runs Pyright on-the-fly as you code in PyCharm.\n  \n    This is just an MVP and hasn't been battle-tested, so basically I'm asking for help. Please try it out and let me know what can be improved. Version 0.1.0 will be released on the Marketplace once PyCharm 2024.1 is released. Until then I'll try to make it as good as possible.\n  Target audience\n    PyCharm users who also use Pyright.\n  Comparison\n    Instead of arguing with your colleagues about whether you should switching to VSCode for better type checking, now you can enjoy the power of Pyright directly in PyCharm.\n  What it does\n    More or less the same thing as running pyright path/to/your-file.py in the terminal, but automatically."
},
{
    "title": "No title",
    "content": "PyPI: https://pypi.org/project/sqlcipher3-wheels/\n\n    GitHub: https://github.com/laggykiller/sqlcipher3\n\n\nWhat My Project Does\n\nsqlcipher3 is a python3 binding for SQLCipher maintained by coleifer. Officially it only provides manylinux wheels for x86_64 and bundles with SQLCipher version 3 (From sqlcipher3-binary). sdist does not work if sqlcipher is not installed in system, as it does not contain source of sqlcipher. It is quite a pain to compile, especially on Windows due to dependency to openssl.\n  \n\n    I decided to create my own fork that build sqlcipher3 wheels with cibuildwheel. The wheels are compiled with sqlcipher version 4. Wheels are available for all platforms (Windows, MacOS, manylinux, musllinux) and all supported architectures (x86_64, i686, aarch64, ppc64le, s390x). openssl is installed with conan, and sdist now bundles with source of sqlcipher so it could be built even if the system does not have openssl and sqlcipher installed.\n  \n\n    You can download the wheels by pip install sqlcipher3-wheels. Hope this reduces your pain and suffering!\n  \n\nTarget Audience\n\n    Those who want to use sqlcipher3 without compiling by themselves\n  \n\nComparison\n\nsqlcipher3 is where I have forked from, which only provides manylinux x86_64 wheels built with sqlcipher version 3. The sdist does not contain source of sqlcipher and cannot compile if openssl is absent."
},
{
    "title": "No title",
    "content": "Hey all,\n  \n    I am making this post to highlight a video I just published on YouTube about a Python CLI (command-line interface) financial analyzer application I have developed.\n  What my project does\n    The project in question is meant to analyze spending, record transactions, and view balances/investments. I think it's a great alternative to something like Mint (especially because Mint is shutting down!) Yes, you will likely need programming experience to use it cleanly but hey that's why I'm posting in r/Python :)\n  Target Audience\n    Anyone who wants to track their spending would find value in this project. If you like applications like Mint, you will like this project.\n  Comparison\n    One similar project is LedgerCLI. However this project is more for the pure accounting side and doesn't involve the budgeting / investment aspect that I think of when I think about examining personal finance.\n  References\n    Please checkout this recent post and this post on the GUI version\nAnd checkout the YouTube video here!GitHub Repo\nGithub Link"
},
{
    "title": "No title",
    "content": "Hey Redditors,\n  \nWhat My Project DoesI'm excited to introduce you to LIVA (Local Intelligent Voice Assistant), a side project I've been working on that brings the power of voice assistants right to your terminal! It uses LLMs, Text to Speech and Speech to Text to create a voice assistant\n  \nTarget AudienceThe project is intended for developers, tech enthusiasts who are working with Large Language Models would quickly like to run a voice chat with their favorite Large Language Model\n  \nComparisonSo far I have not found anything that does speech to text in almost real time combined with LLM and a Speech Generator. These were available as separate projects but did not quite found a project that combines all three (I did find voice assistants which use LLMs and Speech Generators but they were lacking near real-time speech to text capabilities)\n  \nKey Features:Here's what you can expect from LIVA:\n  \n    🎤 Speech Recognition: LIVA accurately transcribes your spoken words into text, making interaction seamless and intuitive. By default whisper-base.en is being used for speech recognition\n  \n    💡 Powered by LLM: Leveraging advanced Large Language Models, LIVA understands context and provides intelligent responses to your queries. By default the parameters are set to Mistral:Instruct with Ollama Endpoint, but the model can be easily changed, you can use any OpenAI compatible endpoint\n  \n    🔊 Text-to-Speech Synthesis: LIVA doesn't just understand – it speaks back to you! With natural-sounding text-to-speech synthesis, LIVA's responses are clear and human-like. For the TTS, I'm going with the SpeechT5\n  \n    🛠️ Customizable and User-Friendly: With customizable settings and an intuitive interface, LIVA adapts to your preferences and needs, making it easy to use for everyone. Right now, you can customize the LLM and the TTS model (It accepts the variants of Whisper)\n  \n    Let's say you want to use the openhermes from ollama with whisper-small.en. Then you just simply run\n  python --model-id openhermes --stt-model openai/whisper-small.enmain.py  \n    Running the python main.py will look for the whisper-base.en and will download if it isn't present. And coming to the model, by default it looks for the Mistral:Instruct on the Ollama Endpoint\n  \nContributionsBut here's where you come in – I want your input! Your feedback, suggestions, and ideas are invaluable in making LIVA even better. Whether you're a developer, a tech enthusiast, or simply curious to try it out, your voice matters.Here's how you can get involved:\n  \n\n\nTry It Out: Head over to GitHub to check out the project code. Install it, give it a try, and let me know what you think.\n  \n\n\nFeedback and Suggestions: Have ideas for new features or improvements? Found a bug? Share your thoughts by submitting feedback on GitHub. Your input helps shape the future of LIVA.\n  \n\n\nSpread the Word: Know someone who might benefit from LIVA? Share it with them! The more people who use and contribute to LIVA, the stronger the community becomes.\n  \n\n\nCollaborate: Interested in contributing code, documentation, or ideas? Fork the repository, make your changes, and submit a pull request. Let's collaborate and make LIVA the best it can be.\n  \n\n\n    I'm excited about the potential of LIVA, and I can't wait to see where this journey takes us. Together, let's create a voice assistant that's intelligent, accessible, and tailored to our needs.\n  \n    Got questions, ideas, or just want to chat about LIVA? Drop a comment below or reach out to me directly. Your input is what makes LIVA great!\n  \n    (P.S. If you're interested in joining the LIVA project and contributing, check out the suggestions above!)"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I'd like to present my simple GoComics API wrapper.\n  \nWhat My Project Does\n\n    This PyPI package allows for easy searching, downloading, and displaying of comic strips directly from GoComics.\n  \nTarget Audience\n\n    This package is intended for developers and comic enthusiasts looking to integrate comic strips into their websites or projects.\n  \nComparison\n\n    The only other comparison I can find is a repository to download Marvel comics, last updated 8 years ago.\n  \nKey Features:\n\n\n\n    Simple Installation: Get started with pip install comics.\n  \n\n\n    Versatile Usage: Find specific comics by date or randomly explore the large collection from GoComics, including classics like Calvin and Hobbes.\n  \n\n\n    Easy Integration: Add comic strips to your personal website, create a curated collection, or enjoy them in Jupyter notebooks.\n  \n\n\n    This wrapper supports Python 3.7+, is open for contributions on GitHub, and is licensed under MIT. For more details, check out the GitHub repository!"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "https://github.com/python-lapidary/rybak\nWhat my project does\n    Rybak is a directory-tree generator library.\n  \n    It accepts directory-tree structure, where file content as well as file and directory names can be templates (Jinja or Mako), applies client provided data, and generates a directory structure.\n  Target audience\n    It's intended for use in any project that needs to generate complex directory structures from templates. I keep it as a helper library for my OpenAPI client generator.\n  Comparison\n    It's inspired by Cookiecutter and Copier. Compared to them:\n  \n\n\n    focuses on delivering a clean public API and not a CLI or interacting with user\n  \n\n\n    can generate more complex directory structures, where a single template file can be used to generate multiple output files\n  \n\n\n    Reads templates from importlib.resources\n  \n\n\n    works with Jinja as well as Mako templates (only one engine for a template directory)"
},
{
    "title": "No title",
    "content": "Wannabe Security Researcher!?!? Is the title of my very first blog post of my very first blog, I hope it to be informative for who is interested in Security and more specifically about an home assignment I received for a position of Sr. Security Researcher and how I approached it. Thanks for readinghttps://cyberroute.github.io/post/2024-02-23-rdpscan/"
},
{
    "title": "No title",
    "content": "GitHub: DataLoader\nWhat My Project Does\n    DataLoader is a comprehensive utility that facilitates the efficient loading and processing of data from a specified path (directory) or all data from multiple directories. This project is designed to be user-friendly and easy to integrate into your projects.\n  \n\n\nPlease note this project may not be suited for everyone as most users do not prefer dynamically loading their files/data but rather traditionally. Along with users not prefer having fixed loading methods for their files (which can be changed using a specified parameter for user preference).\n\n\nTarget Audience\n    This module is tailored for users who seek for loading all files in a given path/directories on the go. Reducing the use for context managers when wanting to simply retrieve some data from the file.\n  \n    I mainly built this project because I tend to load files all the time mainly just for retrieval purposes rather writing or manipulating the data itself. I definitely know I am not the only one so thought I would publish it here.\n  \n    It was definitely fun and also fairly challenging as I had to take in account for handling many use cases that may occur all while giving the user an easy experience.\n  \n    I understand many might not be a fan of this considering no project related to this has been made for obvious reasons but is very handy and has many benefits to it towards efficiency.\n  Comparison\n    This module will dynamically obtain all the files in a given path or directories (while automatically filtering out hidden files) and load them with their respective loading method.\n  \n    You can also specify exactly what extensions you'd like to load and skip all other files. If any files fails to load, it will be returned as a TextWrapperIO instance.\n  \n    For the reason that directories will contain many or long files, I tried my best to efficiently write my code so that everything will be fast, smooth and simple.\n  \n    Therefore, I purposely implemented a parameter whether you'd like it as a generator or if set to False, as a dictionary (Custom made dictionary for displaying the data purposes).\n  \n    The ext_loaders parameter allows you to customize the loading methods for your file extensions. You can also pass in keyword arguments for the specified loaders.\n  # Structure: {extension: {<loading method>: {<loading method kwargs>}}}\next_loaders = {\"csv\": {pd.read_csv: {\"header\": 10}}}\n\nPlease note, if you prefer all files to not be mapped with any loading method but rather just returned as a TextWrapperIO object, you can pass in the no_method parameter.\n\nThe class was built primarily focused on memory. The speed results are promising depending on file size being loaded as well as computer specifications of course.\nUse Cases\n    For example, if the specified path or directories contains the specified file extensions of (csv, pdf, txt), the class will return a generator (or a dictionary if set to false).\n  \n    The csv files will be loaded with pandas.read_csv, the pdf files will be a generator from pdf.high_level.extract_pages, and the txt files will be opened and read using the tradition open method.\n  \n    If the object is being returned as a dictionary (when generator is set to False), all the keys will be the full posix path by default and the values will be masked by its objects loaded type. This was implemented to prevent overflow in the terminal as all the values would contain all loaded data.\n  - Load all files with a specified path (directory) as a Generatordl_gen = DataLoader(path=Path(__file__).parent)\ndl_files = dl_gen.files\nprint(dl_files) # Output: <generator object DataLoader.files.<key-value> at 0x1163f4ba0>- Load all files with a specified path (directory) as a Dictionary  (Custom-Repr)# Disabling 'generator' and 'full_posix'(Otherwise will return the keys as the full posix path) for displaying purposes.\ndl_dict = DataLoader(path=Path(__file__).parent,\n                    generator=False,\n                    full_posix=False)\n\ndl_files = dl_dict.files\nprint(dl_files)\n# Output:\nDataLoader((LICENSE.md, <TextIOWrapper>),\n       (requirements.txt, <Str>),\n       (Makefile, <Str>),\n       (pyblack.toml, <Str>),\n       (README.md, <TextIOWrapper>),\n       (setup.py, <Str>),\n       (MANIFEST.ini, <TextIOWrapper>),\n       (tox.ini, <ConfigParser>),\n       (setup.cfg, <ConfigParser>))- Load all files from multiple directories\n# Disabling 'generator' and 'full_posix'(Otherwise will return the keys as the full posix path) for displaying purposes.\n\ndl = DataLoader(directories=Path(<directory_with_directories>).glob(\"*\"), \n            generator=False,\n                full_posix=False\n                )\n\ndl_dir_files = dl.dir_fiiles\nprint(dl_fir_files)\n# Output:\nDataLoader((technologie_3.txt, <Str>),\n       (technologie_2.txt, <Str>),\n       (technologie_1.txt, <Str>),\n       (technologie_5.txt, <Str>),\n       (technologie_4.txt, <Str>),\n       (sport_1.txt, <Str>),\n       (sport_2.txt, <Str>),\n       (sport_3.txt, <Str>),\n       (8_00083.csv, <DataFrame>),\n       (8_00082.csv, <DataFrame>),\n       (8_00085.csv, <DataFrame>),\n       (descriptor.json, <Dict>),\n       (vehicles.csv, <DataFrame>),\n       (bart-43253423.pdf, <TextIOWrapper>),\n       (caltrain-425345423423.pdf, <TextIOWrapper>),\n       (bart_20180908_007.pdf, <TextIOWrapper>),\n       (space_5.txt, <Str>),\n       (space_4.txt, <Str>),- Load all files with default extensions\ndl_dict = DataLoader(path=Path(__file__).parent, default_extensions=(\"csv\"), generator=False, full_posix=False)\ndl_files = dl_dict.files print(dl_files)Output:DataLoader((8_00083.csv, <DataFrame>),\n       (8_00082.csv, <DataFrame>),\n       (8_00085.csv, <DataFrame>),\n       (vehicles.csv, <DataFrame>))- Retrieve the files datadl_vehicles = dl_files[\"vehicles.csv\"]\n# Output: <DataFrame>\n   CNR Manufacturer    Model      Fuel  Power (kW) Transmision  Weight (kg)0    2        Skoda    Fabia  gasoline          81      Manual         1312 1    3         Seat    Ibiza  gasoline         110      Manual         1376 2    4        Skoda  Octavia  gasoline         140   Automatic         1728 3    5        Skoda  Octavia  gasoline         110      Manual         1376 4    6        Skoda    Karoq  gasoline         110      Manual         1632- Using DataMetrics Class\n\n    This class was mainly built for fun and can seamlessly be used with DataLoader. It takes in an iterable of valid file paths and returns a dictionary of all the OS stats results for each file.\n  \n    The dictionary will also have 2 custom made keys called st_vsize and st_fsize.\n  \nst_vsize: The value will contain a dictionary of the disk volume size (shutil.disk_usage)\n  \nst_fsize: The value will be a NamedTuple instance called Stats which will contain 3 values.\n  Stats(symbolic='0.26 KB', calculated_size=0.2607421875, bytes_size=267)\nPlease note, if passing in a DataLoader.files/dir_files instance, full_posix must be enabled to ensure all the paths are absolute and no errors are raised. full_posix is enabled by default, but can be disabled with DataMetrics for displaying purposes (disabled by default).\n\n- Load All File Statsdl_gen = DataLoader(path=Path(__file__).parent)\ndl_files = dl_gen.files\n\ndm = DataMetrics(files=dl_files, full_posix=False)\n\ndm_stats = dm.all_stats\nprint(dm_stats)\n# Output:\nDataMetrics((8_00083.csv, <Dict>), (8_00082.csv, <Dict>), (8_00085.csv, <Dict>), (vehicles.csv, <Dict>))\ndm_vehicles = dm_stats[\"vehicles.csv\"]\nprint(dm_vehicles)\n<dictionary of all stats. For representation purposes, going to pass the custom dictionary to it.>\n\nprint(DataLoader.special_repr(dm_vehicles, module=\"DataMetrics\"))\n# Output:\nDataMetrics((st_atime, <Stats>),\n        (st_ctime, <Stats>),\n        (st_dev, <Stats>),\n        (st_gid, <Stats>),\n        (st_ino, <Stats>),\n        (st_mode, <Stats>),\n        (st_mtime, <Stats>),\n        (st_nlink, <Stats>),\n        (st_size, <Stats>),\n        (st_uid, <Stats>),\n        (st_fsize, <Stats>),\n        (st_vsize, <Dict>))\n\nprint(dm_vehicles[\"st_fsize\"])\n# Output: NamedTuple Instance\nStats(symbolic='0.26 KB', calculated_size=0.2607421875, bytes_size=267)- Export All Files Statsdm_stats.export_stats() # A JSON file containing the stats for each file.- Retrieve the Calculated Total Size# This method calculates the total size by summing the st_size bytes size for all specified files\nprint(dm_stats.total_stats)\n# Output:\nStats(symbolic='165.93 KB (Kilobytes)', calculated_size=165.9345703125, bytes_size=169917)Project Limitations and Downsides\nWhile the project offers valuable features, it's essential to highlight a few limitations for a comprehensive understanding.\n\n1. Path and Directories Redundancy\n\n    - The inclusion of both `path` and `directories` parameters may seem redundant and could be streamlined into a single parameter. Presently, users need to use the `files` property method when specifying the path or the `dir_files` property method when directories are specified.\n  \n2. Dynamic File Mapping\n\n    -  The current implementation does not allow users to explicitly specify which files to map the loaders with. The mapping is dynamic, meaning that the loader specified for a particular extension will be applied universally. This project is meant to be quick and easy for data retrieval keep in mind.\n\n3. Limited Handling of Duplicate Filenames\n\n    - When full_posix is disabled, and multiple files share the same name, only one of the files will be retrieved. Users should be aware of this behavior and consider passing in full_posix or simply load the file using the classes load_file method. If expectations are not meant, consider loading the file(s) using the traditional way for now."
},
{
    "title": "No title",
    "content": "Hey everyone!\n  \n    I wrote a Flask application that transcribes phone calls in real-time, feel free to check out if you're looking to do something similar:\n  \n\n\nGitHub repo\n\n\n\nYouTube video tutorial\n\n\n\nBlog tutorial"
},
{
    "title": "No title",
    "content": "Hi! I am Saurabh. I love writing fast programs and I've always hated how slow Python code can sometimes be. To solve this problem, I have created Codeflash.\n  What My Project Does\ncodeflash is a Python package that uses AI to figure out the most performant way to rewrite a Python code. It not only optimizes the performance but also verifies the correctness of the new code, i.e. makes sure that the new code follows exactly the same behavior as your original code. This automates the manual optimization process.\n  \n    It can improve algorithms, data structures, fix logic, use better optimized libraries etc to speed up your code.\n  \n    Website - https://www.codeflash.ai/ , get started here.\n  \n    PyPi - https://pypi.org/project/codeflash/\n\n    If you have a Python project, it should take you less than 5 minutes to setup codeflash - pip install codeflash and codeflash init\n\n    Codeflash can also optimize your entire project! Run codeflash --all after setting up codeflash, and codeflash will optimize your project, function by function, and create PRs on GitHub when it finds an optimization. This is super powerful.You can also install codeflash as a GitHub actions check that runs on every new PR you create, to ensure that all new code is performant. If codeflash finds that a code can be made more performant, it will create a PR comment with the new optimized code. This ensures that your project stays at peak performance everytime.\n  How it works\n    Codeflash works by optimizing the code path under a function. So if there is a function foo(a, b): , codeflash finds the fastest implementation of the function foo and all the other functions it calls. The optimization procedure preserves the signature of the function foo and then figures out a new optimized implementation that results in exactly the same return values as the original foo. The behavior of the new function is verified to be correct by running your unit tests and generating a bunch of new regression tests. The runtime of the new code is measured and the fastest one is recommended.\n  Target Audience\n    Codeflash is currently the best at optimizing pure-functions without side effects. You can use codeflash to improve performance of any custom algorithm, numpy code, pandas code, data processing code etc. It is very general purpose. You should be able to optimize anything that can be unit-tested.\n  \n    You can also try to optimize non-pure functions but you should review the new code. We are improving support to more types of functions. Would love to know about your use case and how we can support it!\n  Comparison\n    I am currently unaware of any direct comparison with codeflash on optimizing performance of user level code.\n  \n    Codeflash is still early but has gotten great results already by optimizing open source projects like Langchain and many others. I would love you to try codeflash to optimize your code and let me know how you use it and how we can improve it for you!\n  \n    Thank you."
},
{
    "title": "No title",
    "content": "This has been a long standing issue on Pycharm and nobody on the company seems interested in it. Looks like somebody went ahead and created a plugin that from a quick check looks like it works fine:\n  \n    I did not make this.\n  \nhttps://github.com/souperk/pycharm-sqlalchemy-plugin"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "If you work with spatio temporal data like satellite imagery, SAR, Point Clouds, etc. Try to use Spatio Temporal Data Catalog (STAC). Take a look this tutorial that will explain step by step approach how to use STAC in Python from searching data, display it and do analysis the data in a row.\n  \nhttps://www.geodose.com/2024/02/pystac-decoded-step-by-step-tutorial.html"
},
{
    "title": "No title",
    "content": "Hello, I'd like to share a recent project of mine. infinite Sides! Screenshots\n\n    Project Repo: https://github.com/Fus3n/infinite-sides\nWhat my project does\n    I recently played the game called Infinite Craft and it was quite fun to see how you can craft anything from any word you can check out the original game here.\n  \n    This game uses LLM to generate words based on a combination of other words or two words, you get a few words that you start with and you can go with it and come up with infinite combinations (well as far as an LLM can go in theory, its not perfect).\n  \n    So I decided to make it in Python and PySide6 right after discovering Ollama, which is a way to run local LLMs pretty fast.\n  \n    So you can just boot up Ollama and my app will use its local API  to communicate with LLMs to generate texts in your computer, so no need for the internet, but its main feature is that you can just create your OWN infinitely crafting game, for example, a Minecraft themed one, Example Here\n\n    So you can basically create any kind of game!\n  \n    You can also forward the port Ollama is hosted on by default using ngrok and share the link with other people which they can use to play the game in infinite-sides, all they have to do is change the base URL in settings, so you can give access to your model to your friends if they don't have a beefy computer to run LLMs.\n  Target audience\n    If you enjoy games like this you also might enjoy this one!\n  Comparison\n    I don't know If there are already any apps/games like this either way this is my version just a project I did and hope people can enjoy it!\n  \n\n\n    All instructions to run and use it are in the README, there is currently no binaries but will be in the future.\n  \n\n\n    Also, I am not good with \"prompt engineering\" so the current default system prompt is pretty bad and might not give the best experience, So I suggest you edit it in the settings and try different variations if you want to give the game a try."
},
{
    "title": "No title",
    "content": "Our company is currently developing a book/guide for the PCEP certification. While there are numerous resources available, we aim to create a comprehensive book covering essential aspects such as tips and tricks, example questions, brain teasers, and case studies.\n  \n    I'd love to hear your suggestions for additional content that would be particularly valuable and not commonly found in existing resources on this certification."
},
{
    "title": "No title",
    "content": "Recently, I wrote a blog post on ChemPy, a Python package, for balancing chemical equations. I hope you find it useful.\n  \nhttps://chemistryprogramming.xyz/2024/02/chempy-balancing-equations-python/"
},
{
    "title": "No title",
    "content": "Why do, garbage collected languages like python, not implement resource management concepts in a sane way?C++ and Rust have RAII, which is awesome.Python has __enter__, __exit__, with ... as ..., try: ... finally:.But that is extremely limited:Returning a resource prevents the user to use the safe with statement, the resource is closed.And even if it would work, how should the caller know?Since python never cared, this would be an improvement.\n  \n    On top writing with for all resources, especially when working with resources, creating resources, is very cumbersome. The code is blown up by with or try ... finally/catch blocks. And it is still unsafe because you will forget to use these wherever they are required.\n  \n    The best solution to this would be, to clean up all uncatched resources automatically on scope exit.Python already knows that they are resources, since they implement __enter__ and __exit__.Python just has to prevent the creation of strong references from resources. Weak references should still creatable with weakref. In the end this would be only syntactic sugar, but it would make the resource management way safer and on top more convenient:\n          f = open(__file__, \"r\")  \n        some_code_may_throw(f)  \n        return f  \n    would compile to\n          f = open(__file__, \"r\")  \n        try:  \n            some_code_may_throw(f)  \n            return f  \n        except:\n            f.close()\n            raise"
},
{
    "title": "No title",
    "content": "It highlights all matches including groups so you can quickly test regex patterns: https://python-fiddle.com/tools/regex"
},
{
    "title": "No title",
    "content": "What my project does\nsuitespotauth provides a simple wrapper for the authentication flow required by SuiteSpot construction management software. It exposes a secure access_token attribute which you need (for Bearer Authorization) to make API calls to get your construction data from SuiteSpot.\n  \n    This cuts out all the boilerplate flow required of creating Basic Authorization to generate an API token and exchange it for the required access token.\n  \n    GitHub: https://github.com/yhavin/suitespotauth\n\n    Homepage: https://pypi.org/project/suitespotauth\nTarget audience\n    Available for use in production. The target audience is for developers who want to make API calls to SuiteSpot analytics to retrieve their construction data, but do not want to write 100 lines of code just to get the access token.\n  Comparison\n    There is no known other package providing a wrapper to the authentication flow. The official authentication docs are available at https://auth.suitespot.io/api#/.\n  Development\n    This package is being actively developed, and all replies/issued will be read.\n  \n    Thank you!"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Note, Tyche is an active research project and we're looking for users to help us do a formal evaluation study. If you're interested in helping out and giving feedback on the tool, please fill out this Google form.\n  \n\n    First of all, if you haven't heard of Hypothesis, check it out. It's a \"library for creating unit tests which are simpler to write and more powerful when run, finding edge cases in your code you wouldn’t have thought to look for\" (from the docs) and it's my favorite way to test Python code.\n  \n    But if you're a long-time user of Hypothesis, you've likely run into a situation where a Hypothesis test wasn't actually doing what you thought it was doing. Maybe you thought you were testing on positive integers, but you were actually wasting time generating lots of negative ones too. Or maybe a from_type generator just didn't produce the values you thought it would. Sometimes Hypothesis tells you about these mistakes, but often you don't notice until a bug made it into production.\n  \nWhat My Project Does Tyche is an open source (source) Visual Studio Code extension that helps you analyze your Hypothesis tests and make sure they're exactly as thorough as you expect. The interface shows you charts that summarize the distribution of data used to test your code and helps to warn you if things seem off. The extension has already helped the Hypothesis team to fix a couple of bugs in the Hypothesis framework itself, and it may be able to help you improve your own test suites too.\n  \nTarget Audience Users of Hypothesis. Even if you're pretty confident that your tests are already as effective as they can be, it's worth using Tyche to double check.\n  \nComparison As far as I know, there aren't other tools like Tyche out there yet. This project is part of cutting-edge research at a major university."
},
{
    "title": "No title",
    "content": "Hi guys!I created my very first python lib. It's a server/client solution that allows to control  GPIO pins of a remote Raspberry Pi.\n  \n    GitHub: https://github.com/schech1/remoteioHomepage: https://pypi.org/project/remoteio/\n\nWhat My Project Doesremoteio creates a socket and can run as a deamon at startup. The remoteio python lib provides gpiozero-like commands to control outputs on the remote Raspberry Pi.\n  \nTarget AudienceMakers who want to control the GPIO outputs of one or more remote Raspberry Pis.\n  \nComparison\n\n    The previously known remote GPIO option of Raspberry Pi OS is provided by pigpiod , which does not work on the Raspberry Pi 5 anymore.\n  \n    The project is still small and currently only offers to toggle outputs.I plan to add also inputs and I2c/SPI in future updates.I appreciate your support!"
},
{
    "title": "No title",
    "content": "I released RenderCV a while ago with this post. Today, I released v1 of RenderCV, and it's much more capable now. I hope it will help people to automate their CV generation process and version-control their CVs.\n  What My Project Does\n    RenderCV is a LaTeX CV/resume generator from a JSON/YAML input file. The primary motivation behind the RenderCV is to allow the separation between the content and design of a CV.\n  \n    It takes a YAML file that looks like this:\n  cv:\n  name: John Doe\n  location: Your Location\n  email: youremail@yourdomain.com\n  phone: tel:+90-541-999-99-99\n  website: https://yourwebsite.com/\n  social_networks:\n    - network: LinkedIn\n      username: yourusername\n    - network: GitHub\n      username: yourusername\n  sections:\n    summary:\n      - This is an example resume to showcase the capabilities\n        of the open-source LaTeX CV generator, [RenderCV](https://github.com/sinaatalay/rendercv).\n        A substantial part of the content is taken from [here](https://www.careercup.com/resume),\n        where a *clean and tidy CV* pattern is proposed by **Gayle\n        L. McDowell**.\n    education:\n    ...\n    And then produces these PDFs and their LaTeX code:\n  \n\n\n\nclassic theme\n          \nsb2nov theme\n          \nmoderncv theme\n          \nengineeringresumes theme\n          \n\n\n\n\n\n\nExample PDF,\n            \nExample PDF\n\nExample PDF\n\nExample PDF\n\n\n\nCorresponding YAML\n\nCorresponding YAML\n\nCorresponding YAML\n\nCorresponding YAML\n\n\n\n\n    It also generates an HTML file so that the content can be pasted into Grammarly for spell-checking. See README.md of the repository.\n  \n    RenderCV also validates the input file, and if there are any problems, it tells users where the issues are and how they can fix them.\n  \n    I recorded a short video to introduce RenderCV and its capabilities:\n  \nhttps://youtu.be/0aXEArrN-_c\nTarget Audience\n    Anyone who would like to generate an elegant CV from a YAML input.\n  Comparison\n    I don't know of any other LaTeX CV generator tools implemented with Python."
},
{
    "title": "No title",
    "content": "I just wrote a plugin that runs Pyright on-the-fly as you code in PyCharm.\n  \n    This is just an MVP and hasn't been battle-tested, so basically I'm asking for help. Please try it out and let me know what can be improved. Version 0.1.0 will be released on the Marketplace once PyCharm 2024.1 is released. Until then I'll try to make it as good as possible.\n  Target audience\n    PyCharm users who also use Pyright.\n  Comparison\n    Instead of arguing with your colleagues about whether you should switching to VSCode for better type checking, now you can enjoy the power of Pyright directly in PyCharm.\n  What it does\n    More or less the same thing as running pyright path/to/your-file.py in the terminal, but automatically."
},
{
    "title": "No title",
    "content": "PyPI: https://pypi.org/project/sqlcipher3-wheels/\n\n    GitHub: https://github.com/laggykiller/sqlcipher3\n\n\nWhat My Project Does\n\nsqlcipher3 is a python3 binding for SQLCipher maintained by coleifer. Officially it only provides manylinux wheels for x86_64 and bundles with SQLCipher version 3 (From sqlcipher3-binary). sdist does not work if sqlcipher is not installed in system, as it does not contain source of sqlcipher. It is quite a pain to compile, especially on Windows due to dependency to openssl.\n  \n\n    I decided to create my own fork that build sqlcipher3 wheels with cibuildwheel. The wheels are compiled with sqlcipher version 4. Wheels are available for all platforms (Windows, MacOS, manylinux, musllinux) and all supported architectures (x86_64, i686, aarch64, ppc64le, s390x). openssl is installed with conan, and sdist now bundles with source of sqlcipher so it could be built even if the system does not have openssl and sqlcipher installed.\n  \n\n    You can download the wheels by pip install sqlcipher3-wheels. Hope this reduces your pain and suffering!\n  \n\nTarget Audience\n\n    Those who want to use sqlcipher3 without compiling by themselves\n  \n\nComparison\n\nsqlcipher3 is where I have forked from, which only provides manylinux x86_64 wheels built with sqlcipher version 3. The sdist does not contain source of sqlcipher and cannot compile if openssl is absent."
},
{
    "title": "No title",
    "content": "Hey all,\n  \n    I am making this post to highlight a video I just published on YouTube about a Python CLI (command-line interface) financial analyzer application I have developed.\n  What my project does\n    The project in question is meant to analyze spending, record transactions, and view balances/investments. I think it's a great alternative to something like Mint (especially because Mint is shutting down!) Yes, you will likely need programming experience to use it cleanly but hey that's why I'm posting in r/Python :)\n  Target Audience\n    Anyone who wants to track their spending would find value in this project. If you like applications like Mint, you will like this project.\n  Comparison\n    One similar project is LedgerCLI. However this project is more for the pure accounting side and doesn't involve the budgeting / investment aspect that I think of when I think about examining personal finance.\n  References\n    Please checkout this recent post and this post on the GUI version\nAnd checkout the YouTube video here!GitHub Repo\nGithub Link"
},
{
    "title": "No title",
    "content": "Hey Redditors,\n  \nWhat My Project DoesI'm excited to introduce you to LIVA (Local Intelligent Voice Assistant), a side project I've been working on that brings the power of voice assistants right to your terminal! It uses LLMs, Text to Speech and Speech to Text to create a voice assistant\n  \nTarget AudienceThe project is intended for developers, tech enthusiasts who are working with Large Language Models would quickly like to run a voice chat with their favorite Large Language Model\n  \nComparisonSo far I have not found anything that does speech to text in almost real time combined with LLM and a Speech Generator. These were available as separate projects but did not quite found a project that combines all three (I did find voice assistants which use LLMs and Speech Generators but they were lacking near real-time speech to text capabilities)\n  \nKey Features:Here's what you can expect from LIVA:\n  \n    🎤 Speech Recognition: LIVA accurately transcribes your spoken words into text, making interaction seamless and intuitive. By default whisper-base.en is being used for speech recognition\n  \n    💡 Powered by LLM: Leveraging advanced Large Language Models, LIVA understands context and provides intelligent responses to your queries. By default the parameters are set to Mistral:Instruct with Ollama Endpoint, but the model can be easily changed, you can use any OpenAI compatible endpoint\n  \n    🔊 Text-to-Speech Synthesis: LIVA doesn't just understand – it speaks back to you! With natural-sounding text-to-speech synthesis, LIVA's responses are clear and human-like. For the TTS, I'm going with the SpeechT5\n  \n    🛠️ Customizable and User-Friendly: With customizable settings and an intuitive interface, LIVA adapts to your preferences and needs, making it easy to use for everyone. Right now, you can customize the LLM and the TTS model (It accepts the variants of Whisper)\n  \n    Let's say you want to use the openhermes from ollama with whisper-small.en. Then you just simply run\n  python --model-id openhermes --stt-model openai/whisper-small.enmain.py  \n    Running the python main.py will look for the whisper-base.en and will download if it isn't present. And coming to the model, by default it looks for the Mistral:Instruct on the Ollama Endpoint\n  \nContributionsBut here's where you come in – I want your input! Your feedback, suggestions, and ideas are invaluable in making LIVA even better. Whether you're a developer, a tech enthusiast, or simply curious to try it out, your voice matters.Here's how you can get involved:\n  \n\n\nTry It Out: Head over to GitHub to check out the project code. Install it, give it a try, and let me know what you think.\n  \n\n\nFeedback and Suggestions: Have ideas for new features or improvements? Found a bug? Share your thoughts by submitting feedback on GitHub. Your input helps shape the future of LIVA.\n  \n\n\nSpread the Word: Know someone who might benefit from LIVA? Share it with them! The more people who use and contribute to LIVA, the stronger the community becomes.\n  \n\n\nCollaborate: Interested in contributing code, documentation, or ideas? Fork the repository, make your changes, and submit a pull request. Let's collaborate and make LIVA the best it can be.\n  \n\n\n    I'm excited about the potential of LIVA, and I can't wait to see where this journey takes us. Together, let's create a voice assistant that's intelligent, accessible, and tailored to our needs.\n  \n    Got questions, ideas, or just want to chat about LIVA? Drop a comment below or reach out to me directly. Your input is what makes LIVA great!\n  \n    (P.S. If you're interested in joining the LIVA project and contributing, check out the suggestions above!)"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I'd like to present my simple GoComics API wrapper.\n  \nWhat My Project Does\n\n    This PyPI package allows for easy searching, downloading, and displaying of comic strips directly from GoComics.\n  \nTarget Audience\n\n    This package is intended for developers and comic enthusiasts looking to integrate comic strips into their websites or projects.\n  \nComparison\n\n    The only other comparison I can find is a repository to download Marvel comics, last updated 8 years ago.\n  \nKey Features:\n\n\n\n    Simple Installation: Get started with pip install comics.\n  \n\n\n    Versatile Usage: Find specific comics by date or randomly explore the large collection from GoComics, including classics like Calvin and Hobbes.\n  \n\n\n    Easy Integration: Add comic strips to your personal website, create a curated collection, or enjoy them in Jupyter notebooks.\n  \n\n\n    This wrapper supports Python 3.7+, is open for contributions on GitHub, and is licensed under MIT. For more details, check out the GitHub repository!"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "https://github.com/python-lapidary/rybak\nWhat my project does\n    Rybak is a directory-tree generator library.\n  \n    It accepts directory-tree structure, where file content as well as file and directory names can be templates (Jinja or Mako), applies client provided data, and generates a directory structure.\n  Target audience\n    It's intended for use in any project that needs to generate complex directory structures from templates. I keep it as a helper library for my OpenAPI client generator.\n  Comparison\n    It's inspired by Cookiecutter and Copier. Compared to them:\n  \n\n\n    focuses on delivering a clean public API and not a CLI or interacting with user\n  \n\n\n    can generate more complex directory structures, where a single template file can be used to generate multiple output files\n  \n\n\n    Reads templates from importlib.resources\n  \n\n\n    works with Jinja as well as Mako templates (only one engine for a template directory)"
},
{
    "title": "No title",
    "content": "Wannabe Security Researcher!?!? Is the title of my very first blog post of my very first blog, I hope it to be informative for who is interested in Security and more specifically about an home assignment I received for a position of Sr. Security Researcher and how I approached it. Thanks for readinghttps://cyberroute.github.io/post/2024-02-23-rdpscan/"
},
{
    "title": "No title",
    "content": "GitHub: DataLoader\nWhat My Project Does\n    DataLoader is a comprehensive utility that facilitates the efficient loading and processing of data from a specified path (directory) or all data from multiple directories. This project is designed to be user-friendly and easy to integrate into your projects.\n  \n\n\nPlease note this project may not be suited for everyone as most users do not prefer dynamically loading their files/data but rather traditionally. Along with users not prefer having fixed loading methods for their files (which can be changed using a specified parameter for user preference).\n\n\nTarget Audience\n    This module is tailored for users who seek for loading all files in a given path/directories on the go. Reducing the use for context managers when wanting to simply retrieve some data from the file.\n  \n    I mainly built this project because I tend to load files all the time mainly just for retrieval purposes rather writing or manipulating the data itself. I definitely know I am not the only one so thought I would publish it here.\n  \n    It was definitely fun and also fairly challenging as I had to take in account for handling many use cases that may occur all while giving the user an easy experience.\n  \n    I understand many might not be a fan of this considering no project related to this has been made for obvious reasons but is very handy and has many benefits to it towards efficiency.\n  Comparison\n    This module will dynamically obtain all the files in a given path or directories (while automatically filtering out hidden files) and load them with their respective loading method.\n  \n    You can also specify exactly what extensions you'd like to load and skip all other files. If any files fails to load, it will be returned as a TextWrapperIO instance.\n  \n    For the reason that directories will contain many or long files, I tried my best to efficiently write my code so that everything will be fast, smooth and simple.\n  \n    Therefore, I purposely implemented a parameter whether you'd like it as a generator or if set to False, as a dictionary (Custom made dictionary for displaying the data purposes).\n  \n    The ext_loaders parameter allows you to customize the loading methods for your file extensions. You can also pass in keyword arguments for the specified loaders.\n  # Structure: {extension: {<loading method>: {<loading method kwargs>}}}\next_loaders = {\"csv\": {pd.read_csv: {\"header\": 10}}}\n\nPlease note, if you prefer all files to not be mapped with any loading method but rather just returned as a TextWrapperIO object, you can pass in the no_method parameter.\n\nThe class was built primarily focused on memory. The speed results are promising depending on file size being loaded as well as computer specifications of course.\nUse Cases\n    For example, if the specified path or directories contains the specified file extensions of (csv, pdf, txt), the class will return a generator (or a dictionary if set to false).\n  \n    The csv files will be loaded with pandas.read_csv, the pdf files will be a generator from pdf.high_level.extract_pages, and the txt files will be opened and read using the tradition open method.\n  \n    If the object is being returned as a dictionary (when generator is set to False), all the keys will be the full posix path by default and the values will be masked by its objects loaded type. This was implemented to prevent overflow in the terminal as all the values would contain all loaded data.\n  - Load all files with a specified path (directory) as a Generatordl_gen = DataLoader(path=Path(__file__).parent)\ndl_files = dl_gen.files\nprint(dl_files) # Output: <generator object DataLoader.files.<key-value> at 0x1163f4ba0>- Load all files with a specified path (directory) as a Dictionary  (Custom-Repr)# Disabling 'generator' and 'full_posix'(Otherwise will return the keys as the full posix path) for displaying purposes.\ndl_dict = DataLoader(path=Path(__file__).parent,\n                    generator=False,\n                    full_posix=False)\n\ndl_files = dl_dict.files\nprint(dl_files)\n# Output:\nDataLoader((LICENSE.md, <TextIOWrapper>),\n       (requirements.txt, <Str>),\n       (Makefile, <Str>),\n       (pyblack.toml, <Str>),\n       (README.md, <TextIOWrapper>),\n       (setup.py, <Str>),\n       (MANIFEST.ini, <TextIOWrapper>),\n       (tox.ini, <ConfigParser>),\n       (setup.cfg, <ConfigParser>))- Load all files from multiple directories\n# Disabling 'generator' and 'full_posix'(Otherwise will return the keys as the full posix path) for displaying purposes.\n\ndl = DataLoader(directories=Path(<directory_with_directories>).glob(\"*\"), \n            generator=False,\n                full_posix=False\n                )\n\ndl_dir_files = dl.dir_fiiles\nprint(dl_fir_files)\n# Output:\nDataLoader((technologie_3.txt, <Str>),\n       (technologie_2.txt, <Str>),\n       (technologie_1.txt, <Str>),\n       (technologie_5.txt, <Str>),\n       (technologie_4.txt, <Str>),\n       (sport_1.txt, <Str>),\n       (sport_2.txt, <Str>),\n       (sport_3.txt, <Str>),\n       (8_00083.csv, <DataFrame>),\n       (8_00082.csv, <DataFrame>),\n       (8_00085.csv, <DataFrame>),\n       (descriptor.json, <Dict>),\n       (vehicles.csv, <DataFrame>),\n       (bart-43253423.pdf, <TextIOWrapper>),\n       (caltrain-425345423423.pdf, <TextIOWrapper>),\n       (bart_20180908_007.pdf, <TextIOWrapper>),\n       (space_5.txt, <Str>),\n       (space_4.txt, <Str>),- Load all files with default extensions\ndl_dict = DataLoader(path=Path(__file__).parent, default_extensions=(\"csv\"), generator=False, full_posix=False)\ndl_files = dl_dict.files print(dl_files)Output:DataLoader((8_00083.csv, <DataFrame>),\n       (8_00082.csv, <DataFrame>),\n       (8_00085.csv, <DataFrame>),\n       (vehicles.csv, <DataFrame>))- Retrieve the files datadl_vehicles = dl_files[\"vehicles.csv\"]\n# Output: <DataFrame>\n   CNR Manufacturer    Model      Fuel  Power (kW) Transmision  Weight (kg)0    2        Skoda    Fabia  gasoline          81      Manual         1312 1    3         Seat    Ibiza  gasoline         110      Manual         1376 2    4        Skoda  Octavia  gasoline         140   Automatic         1728 3    5        Skoda  Octavia  gasoline         110      Manual         1376 4    6        Skoda    Karoq  gasoline         110      Manual         1632- Using DataMetrics Class\n\n    This class was mainly built for fun and can seamlessly be used with DataLoader. It takes in an iterable of valid file paths and returns a dictionary of all the OS stats results for each file.\n  \n    The dictionary will also have 2 custom made keys called st_vsize and st_fsize.\n  \nst_vsize: The value will contain a dictionary of the disk volume size (shutil.disk_usage)\n  \nst_fsize: The value will be a NamedTuple instance called Stats which will contain 3 values.\n  Stats(symbolic='0.26 KB', calculated_size=0.2607421875, bytes_size=267)\nPlease note, if passing in a DataLoader.files/dir_files instance, full_posix must be enabled to ensure all the paths are absolute and no errors are raised. full_posix is enabled by default, but can be disabled with DataMetrics for displaying purposes (disabled by default).\n\n- Load All File Statsdl_gen = DataLoader(path=Path(__file__).parent)\ndl_files = dl_gen.files\n\ndm = DataMetrics(files=dl_files, full_posix=False)\n\ndm_stats = dm.all_stats\nprint(dm_stats)\n# Output:\nDataMetrics((8_00083.csv, <Dict>), (8_00082.csv, <Dict>), (8_00085.csv, <Dict>), (vehicles.csv, <Dict>))\ndm_vehicles = dm_stats[\"vehicles.csv\"]\nprint(dm_vehicles)\n<dictionary of all stats. For representation purposes, going to pass the custom dictionary to it.>\n\nprint(DataLoader.special_repr(dm_vehicles, module=\"DataMetrics\"))\n# Output:\nDataMetrics((st_atime, <Stats>),\n        (st_ctime, <Stats>),\n        (st_dev, <Stats>),\n        (st_gid, <Stats>),\n        (st_ino, <Stats>),\n        (st_mode, <Stats>),\n        (st_mtime, <Stats>),\n        (st_nlink, <Stats>),\n        (st_size, <Stats>),\n        (st_uid, <Stats>),\n        (st_fsize, <Stats>),\n        (st_vsize, <Dict>))\n\nprint(dm_vehicles[\"st_fsize\"])\n# Output: NamedTuple Instance\nStats(symbolic='0.26 KB', calculated_size=0.2607421875, bytes_size=267)- Export All Files Statsdm_stats.export_stats() # A JSON file containing the stats for each file.- Retrieve the Calculated Total Size# This method calculates the total size by summing the st_size bytes size for all specified files\nprint(dm_stats.total_stats)\n# Output:\nStats(symbolic='165.93 KB (Kilobytes)', calculated_size=165.9345703125, bytes_size=169917)Project Limitations and Downsides\nWhile the project offers valuable features, it's essential to highlight a few limitations for a comprehensive understanding.\n\n1. Path and Directories Redundancy\n\n    - The inclusion of both `path` and `directories` parameters may seem redundant and could be streamlined into a single parameter. Presently, users need to use the `files` property method when specifying the path or the `dir_files` property method when directories are specified.\n  \n2. Dynamic File Mapping\n\n    -  The current implementation does not allow users to explicitly specify which files to map the loaders with. The mapping is dynamic, meaning that the loader specified for a particular extension will be applied universally. This project is meant to be quick and easy for data retrieval keep in mind.\n\n3. Limited Handling of Duplicate Filenames\n\n    - When full_posix is disabled, and multiple files share the same name, only one of the files will be retrieved. Users should be aware of this behavior and consider passing in full_posix or simply load the file using the classes load_file method. If expectations are not meant, consider loading the file(s) using the traditional way for now."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I’ve just created an official release of a VBA precompiler written in python.\n  What my Project Does\n    For those who don’t know, in VBA you can have precompiler blocks to change things for different OSs or VBA versions. For example, to change the function signature for a new windows version:\n  #if Win64 Then\nFunction foo(Bar, Baz)\n#Else\nFunction Foo(Bar)\n#Endif\n    The problem with this is, if you want to scan raw source code, you can’t just ignore the precompiler lines, because now the code looks like you’ve tried to define the same function twice, and one was never ended.\n  \n    This tool takes environment variables that the user provides, and will comment out any lines that need to be skipped, creating 100% valid precompiled code. It even performs the comparisons, and arithmetic operations specified in the VBA specification.\n  Target Audience\n    People interested in malware prevention, static analysis, and Linting may find it helpful. Also, useful if you are interested in learning about compilers, ANTLR, and code parsing.\n  Limitations\n    It is currently missing the standard library functions, like Cbool(), Abs(), etc. I’m guessing these are never called by users in the precompiler phase."
},
{
    "title": "No title",
    "content": "Hi all!\n  \n    I've made another Python cheat sheet tutorial. Yeah yeah, nothing new, I know. But here's the thing:\n  What My Project Does\n    The main idea was not just to write a wall of text telling about everything, but to make it interactive. So that everything would have its example code snippet, which you could change, run, and see how it worked. And not somewhere in a web version, but on your own computer, in your own environment.\n  \n    Fortunately, Python has the perfect tool for this - the Jupyter Notebook. That's why all chapters are written as separate notebooks and there is an example for each point (well, almost).\n  Target Audience\n    I originally aimed at complete beginners and tried to go from simple to complex, but I think I overdid it at some point. So it might be just as suitable to refresh knowledge and fill in the gaps for anyone.\n  What else\n    It also has some useful information for Python-developer interview preparation and a link to the absolutely wonderful Tech Interview Handbook for Coding Interview preparation in case anyone missed it.\n  \n    I would very welcome any constructive criticism, any advice, and of course contributions and GitHub stars :)\n  \n    A little less, but still I will be glad to unreasonable hatred and comments that nobody needs it and that there are hundreds of similar projects on the Internet.\n  \n    Check out the project on GitHub: PyQuest"
},
{
    "title": "No title",
    "content": "Hi guys! I'd like to share a reactive web UI framework I've been working on for a while that I made public a couple of days ago.\n  \n\n\n    GitHub: https://github.com/hyperdiv/hyperdiv\n\n\n\n    Homepage: https://hyperdiv.io\n\n\n\n    There is a short coding demo video and intro article on the website.\n  What My Project Does\n    Hyperdiv is a way to build reactive UIs in pure Python quickly, with a built-in UI component system based on Shoelace (https://shoelace.style), markdown, and charts based on Chart.js (https://chartjs.org). It uses immediate-mode syntax which enables seamlessly blending declarative UI code with Python logic and event handling.\n  Target Audience\n    The aim of Hyperdiv is to reduce tool and language complexity when building full stack apps, and enable people to get to a working UI very quickly. I think it is a good fit for adding browser UIs to CLI tools, prototyping UIs, and internal tools. You can also put it behind Nginx and deploy it on the internet.\n  Comparison\n    Hyperdiv adds to a niche currently occupied by Streamlit, Reflex.dev, PyWebIO, PyJS, etc. -- frameworks that let you build web apps in pure Python.\n  \n    Hyperdiv stands apart with a unique blend of immediate-mode UI + reactive state, and letting you build fairly unrestricted, arbitrarily nested UI layouts with terse syntax.\n  \n    I appreciate your support!"
},
{
    "title": "No title",
    "content": "Hello everyone!\n  \n    We (4 people startup) open-sourced an SDK to create Stateful Functions in Python!\n  \n\n\n    Github: https://github.com/stealthrocket/dispatch-sdk-python\n\n\n\n    Docs: https://docs.stealthrocket.cloud\n\n\n\n    Package: https://pypi.org/project/dispatch-functions\n\n\nWhat My Project Does\n    We're developing Dispatch, a cloud service that helps developers create reliable systems. It can be used to build real-time and durable workflows, data pipelines, highly parallel jobs, and much more.\n  \n    Say you have a background job that fails and needs to be retried; getting the retry logic right is difficult, and it gets even harder when the retries need to happen across a fleet of instances and in a way that doesn’t take down the application. However, the biggest pain point is managing the state of the system, which often requires adding queues, databases, or workflow engines.\n  \n    With the SDK we released, we are giving Python developers a simple solution to these problems: Stateful Functions.\n  Audience\n    This SDK is useful to Python developers who build production services and need solutions for state management of their background jobs, data streams, workflows, etc...\n  Comparison\n    Dispatch addresses technical problems that are sometimes solved with cloud products like AWS Step Functions, Azure Durable Functions, or Temporal.\n  \n    Dispatch focuses on being lightweight and simple: there is no heavy framework to bring into the application, no cloud component to deploy, no workflow language to learn. The code can run in a serverless deployment, a container, or really anywhere. Python developers can add it to an existing application without having to change anything about the rest of the code.\n  Technical Details\n    At the core of the SDK is an event loop integrating with native Python coroutines (async/await) that supports pausing execution and serializing the state. On resumption, the coroutine state is recreated to continue execution, which could happen in a different instance of the application!\n  \n    Capturing stack frames was the most difficult part: even if Python has native support for pickling values, some objects like generators or coroutines cannot be serialized by default. To work around this limitation, we developed a native extension for CPython that allows us to save and reload the internal interpreter state.\n  Feedback\n    The project is young, but we're really excited to share it. Check it out and let us know what you think!"
},
{
    "title": "No title",
    "content": "I've published a new package for people interested in working with mathematical objects called continued fractions in an object-oriented way with Python.\n  \nhttps://pypi.org/project/continuedfractions/\nWhat My Project Does?\n    The package has been designed to take advantage of an existing Python standard library for rational numbers (`fractions`), and provides a way of computing continued fraction representations for any real number that can be defined in Python, and inspecting their properties (elements, convergents, segments, remainders), and also performing operations with them as normal rational numbers. Continued fractions are not part of the `fractions` standard library.\n  Background\n    I know it's a rather specialised topic, but I guess the first question is \"What is a continued fraction?\", and the next might be, \"What can you do with them?\". A continued fraction is a way of representing real numbers as the sum of an integer part + a nested fraction, where the denominators contain sums of integers and fractions, and those fractions then contain sums of integers and fractions, and so on.\n  \n    Since integers and fractions add up to rational numbers, continued fractions are ways of representing real numbers as the sum of (possibly, infinitely many) rational numbers.\n  \n    They are nested, hierarchical structures, which can be finite or infinite depending on the type of number being represented. For irrational numbers like pi the continued fraction representation will always be infinite, but for rational numbers (ratios of integers) it will always be finite.\n  \n    I don't want to go into the full details, but to answer the next question: continued fractions are really quite beautiful, interesting and useful in many ways. They are very important for the approximation of irrational numbers, which have infinite decimal expansions, with rational numbers, which have a finite decimal expansion. They are also connected to the study of special sequences and orderings of rational numbers - see the Stern-Brocot tree.\n  Target Audience\n    I guess people interested in continued fractions and, more generally, perhaps, number theory.\n  Comparison\n    I don't know of any other Python package that deals with continued fractions in the same way - I did come across one called `contfrac`, but that is not object-oriented, and when I compared the performance of its main function for computing continued fraction representations with mine it was much slower.\n  \n    There are other non-Python packages that can do continued fractions, obviously, like Mathematica, and who knows what else. But this has been designed for Python users in mind.\n  Limitations\n    Obviously the package cannot give you a complete continued fraction representation for pi or any irrational number - it gives you a finite representation that is the best possible, given the limitations of binary floating point arithmetic, as implemented in Python. For irrational numbers the representation is approximate, not exact - this will be improved in the future to allow the representations to be as exact as possible.\n  Documentation\n    At the moment the documentation is confined to the GitHub project README, but it is quite detailed, with lots of examples. I do plan to publish the docs separately, e.g. GitHub Pages or something like that, in later releases.\n  Feedback\n    All/any feedback is welcome, and so are contributions from any one interested in contributing to adding more cool features for studying continued fractions with Python."
},
{
    "title": "No title",
    "content": "GitHub: KeyCraftsman\nWhat My Project Does\nKeyCraftsman is an innovative Python class designed to generate passcodes to your own liking. Offering an array of features such as key length specification, character exclusion, inclusion of all characters, URL-safe encoding, and exportation of the generated passkey(s).\n  \nTarget Audience\n    This module is tailored for users who seek for flexibility in generating custom randomized password key(s).\n  Comparison\n    Many existing modules lack the flexibility needed in terms of features and often include deprecated elements. In contrast, this module stands out by offering a rich set of features compared to traditional Python password-generating modules. While the demand for such features may not be exceptionally high, I embarked on creating a modernized, medium/heavyweight version of key generation for the sheer enjoyment of exploring new possibilities.\n  \nFeatures\n- Exclude Characters: Tailor your keys by excluding specific characters.\n  \n\n- Include All Characters: Embrace diversity by including all ASCII letters, digits, and punctuation.\n  \n\n- Unique Characters: Ensure uniqueness in generated keys or words. If words is specified, it will generate words with only unique number of letters in them.\n  \n\n- Custom Text Wrapping: Wrap your keys with a custom separator and width for a personalized touch.\n  \n\n- Multiple Key Generation: Efficiently generate multiple keys with a single instance.\n  \n\n- Word Generation: Explore creative possibilities with word generation using random.SystemRandom().\n\n\n    - Exclusion Chart: Simplify character exclusion with the provided exclusion chart, available for printing and export.\n  \n\nFor a comprehensive overview of all features and methods, please refer to the documentation. We invite you to explore the capabilities of KeyCraftsman and hope you find joy in utilizing this modernized approach to key generation."
},
{
    "title": "No title",
    "content": "Showcasing marimo-wasm\n    Hey everyone! We (2 developers) made marimo compatible with WebAssembly (WASM), so you can run it entirely in the browser thanks to Pyodide\n  \nhttps://marimo.app\nWhat My Project Does\n    Background: marimo is an open-source reactive notebook for Python, that aims to solve well-known problems with traditional Python notebooks[1].\n  \n    This showcase is for marimo-wasm: a WASM build of marimo allowing the notebook to run entirely in the browser.\n  \n    This is done through WebAssembly and standing on the shoulder of Pyodide (h/t!)\n  \n    You can try out the playground: https://marimo.app/\n\n    This tool has been great for educating others through interactive examples. You can share snippets of code with just a URL. For example, here is a notebook on Bayes' Theorem.\n  \n    While the installable marimo library is hardened and used in production, the WASM build may have some rough edges. Still, the playground is great for sharing notebooks, data applications[2], or explorations with the community without the cost of servers to you or your audience.\n  \n    [1] https://docs.marimo.io/faq.html#faq-problems\n\n    [2] You can hide the code and turn your notebook into an app\n  Target Audience\n    Data Engineers, Data Scientists, Story Tellers, Educators, and Researchers.\n  Comparison\nJupyterLite - a WASM powered Jupyter running in the browser. However, it is not reactive like marimo.\n  \nIPyflow - a reactive notebook for Python implemented as a Jupyter kernel. However, it is not WASM compatible.\n  \nJupyter - marimo is a reinvention of the Python notebook as a reproducible, interactive, and shareable Python program that can be executed as scripts or deployed as interactive web apps - without the need of extensions or additional infrastructure. More here."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What My Project Does\ncopykitten is a clipboard library with support for text and images and a simple API. It's built around Rust arboard library. Thanks to this, it's multiplatform and doesn't require any dependencies.\n  Target Audience\n    Developers building CLI/GUI/TUI applications. The library has beta status on PyPI, but the underlying Rust library is pretty stable, being used in commercial projects like Bitwarden and 1Password.\n  Comparison\n    There are lots of other clipboard libraries for Python: pyperclip, jaraco.clipboard, pyclip, just to name a few. However, most of them are not maintained for years and require the presence of additional libraries or tools in the operating system. copykitten doesn't suffer from these shortcomings.\n  A bit of history\n    Throughout my years with Python there were several times when I needed to use the clipboard in my applications and every time I had to fall back to some shaky methods like asking the end user to install xclip and calling subprocess.run. This never felt great.\n  \n    Right now I'm making a multiplayer TUI game (maybe I’ll showcase it later too :) ), where users can copy join game codes into the clipboard to easily share it (much like Among Us). This is how I came to the idea of making such a library. I also wanted to try Rust for a long time, and so this all just clicked in my head instantly.\n  \n    I had fun building it and definitely had some pain too and learned a bit of nitty-gritty details about how clipboards work in different operating systems. Now I hate Windows.\n\n    With this post I hope to gain some attention to the project so that I can receive feedback about the issues and maybe feature requests and spread the word that there's a modern, convenient alternative to the existing packages.\n  \n    Feel free to try it out: https://github.com/Klavionik/copykitten"
},
{
    "title": "No title",
    "content": "Hi all!\n  \n    Introducing version 0.2.0 of the Squirrels Python library! Squirrels is a dbt-like framework that allows users to create REST API endpoints for dynamic data analytics. Feel free to read more about it on our website!\n  \n    Following our version 0.1.0 release in our previous post here, we've recreated the framework to bring you an improved experience! Although this introduces many breaking changes, we intend to prevent breaking changes as much as possible for subsequent minor version increments (i.e. until 1.0.0). Compared to 0.1.0, the new 0.2.0 version has incorporated the following features:\n  \n\n\nFlexible Lineage for Data Models\n\n\n\n    Create data models (in SQL or Python) as database views or federates. Federate models provide a \"ref\" function/macro to refer to other models just like dbt! Specify a target data model for your dataset API, and Squirrels takes care of the model dependencies and execution order for you.\n  \n\n\n    You can query different database systems with different database views and join the results together in a federate model!\n  \n\n\n\n\nConsolidated Parameters\n\n\n\n    Multiple dataset APIs can share common parameters. You can specify a pool of parameter properties in one place (in Python or YAML) and specify the list of parameters that each dataset uses.\n  \n\n\n\n\nAuthentication\n\n\n\n    You can specify a custom User model and an authentication method that integrates with your authentication source in a Python file. The user attributes can be used the affect the widget parameters or models in real time!\n  \n\n\n    We now support three different levels of access for datasets, with public datasets open to all, protected datasets open to both internal and external users, and private datasets only for private users.\n  \n\n\n\n\n    Feel free to give Squirrels a whirl by following our simple tutorial listed on our tutorial page! And here's the link to our GitHub repo: https://github.com/squirrels-nest/squirrels\n\n    Below are the sections specified in Rule 10:\n  \nWhat My Project Does:\n\n    Squirrels is a dbt-like framework that allows users to create REST API endpoints for dynamic data analytics. Again, feel free to read more about it on our website here!\n  \nTarget Audience:\n\n    Data Engineers, Analytics Engineers, Data Architects, Data Analysts, Data Scientists, Business Analysts, API Engineers, etc.\n  \nComparisons:\n\n    We've come across several questions regarding \"What's the difference between Squirrels and <some other data tool>?\". Below are a few of the points on some commonly asked about comparisons.\n  \n\n\n    dbt:\n  \n\n\n    Used for data transformations only (Squirrels is largely inspired by dbt, but made for the real-time analytics or semantic layer use case instead)\n  \n\n\n    Does not support widget parameters (and does not need to for non-customer-facing data transformations)\n  \n\n\n\n\n    cube.js:\n  \n\n\n    Data models are created in YAML (for Squirrels, they're created in SQL or Python)\n  \n\n\n    Cannot create a lineage of multiple data models under one API endpoint\n  \n\n\n    No support for specifying widget parameters properties to dynamically change dataset behavior\n  \n\n\n\n\n    VulcanSQL\n  \n\n\n    No support for Python models and cannot create a lineage of multiple data models under one API endpoint\n  \n\n\n    Can specify parameters, but it's very \"free-form\" with limited control over the typing of parameters for the client (\"typing\" such as dropdown parameter, date parameter, etc.)\n  \n\n\n    Similar to Squirrels, it can have authenticated users affect the model behavior, but cannot integrate with your authentication source.\n  \n\n\n\n\n    Thus, we are not aware of existing tools that have the same capabilities as Squirrel does. We have tried to use a dbt-like project structure to make it easier for people to pick up."
},
{
    "title": "No title",
    "content": "Hi all, long-time reader and first-time poster. I recently had my 1st kid, have some time off, and built Cry Baby\nWhat My Project Does\n    Cry Baby provides a probability that your baby is crying by continuously recording audio, chunking it into 4-second clips, and feeding these clips into a Convolutional Neural Network (CNN).\n  \n    Cry Baby is currently compatible with MAC and Linux, and you can find the setup instructions in the README.\n  Target Audience\n    People with babies with too much time on their hands. I envisioned this tool as a high-tech baby monitor that could send notifications and allow live audio streaming. However, my partner opted for a traditional baby monitor instead. 😅\n  Comparison\n    I know baby monitors exist that claim to notify you when a baby is crying, but the ones I've seen are only based on decibels. Then Amazon's Alexa seems to work based on crying...but I REALLY don't like the idea of having that in my house.\n  \n    I couldn't find an open source model that detected baby crying so I decided to make one myself. The model alone may be useful for someone, I'm happy to clean up the training code and publish that if anyone is interested.\n  \n    I'm taking a break from the project, but I'm eager to hear your thoughts, especially if you see potential uses or improvements. If there's interest, I'd love to collaborate further—I still have four weeks of paternity leave to dive back in!\n  \n    Update:I've noticed his poops are loud, which is one predictor of his crying. Have any other parents experienced this of 1 week-olds? I assume it's going to end once he starts eating solids. But it would be funny to try and train another model on the sound of babies pooping so I change his diaper before he starts crying."
},
{
    "title": "No title",
    "content": "I've received an assignment whereby I am required to extract texts, tables, layouts, headers, titles, etc from PDFs (Multi-page).\n  \n    These PDFs have actual text on them and not images.\n  \n    So far I've tried using Camelot, PyMuPDF, and Nougat. Unfortunately, none of these modules are able to meet my client's expectations.\n  \n    Due to this, I've tried AWS Textract. I've showed a sample result of Textract and they immediately loved it. However, only then they mentioned that the PDFs have sensitive data and cannot be exposed via the internet.\n  \n    Now, they are looking to find an on-prem solution to get similar results as AWS Textract.\n  \n    Anyone know any kind of software/tool/python module that can be self-hosted and able to get similar results as AWS Textract?\n  \n    Thanks in advance."
},
{
    "title": "No title",
    "content": "I'm excited to share my latest open-source project: Typed FFmpeg, a Python wrapper for FFmpeg enhanced with type hints for better code predictability and IDE support.\n  \nhttps://github.com/livingbio/typed-ffmpeg\nKey Features:\n\n\n    Type-hinted for improved developer experience.\n  \n\n\n    Simplified FFmpeg operations in Python.\n  \n\n\n    Detailed docs with examples for easy start.\n  \n\nWhat My Project Does:\n    Typed FFmpeg is an open-source Python wrapper for FFmpeg, designed to integrate multimedia processing capabilities into Python applications more seamlessly. It offers type hints throughout, improving code reliability and developer experience by providing better IDE support and error checking before runtime. Whether you're resizing videos, converting file formats, or extracting metadata, Typed FFmpeg simplifies these tasks with an intuitive, Pythonic interface.\n  Target Audience:\n    This project is aimed at developers working on multimedia applications, educational content, or data analysis involving video and audio processing. It's suitable for both production and experimental projects, offering a balance between ease of use and robust functionality.\n  Comparison:\n    Unlike existing high level FFmpeg wrappers for Python, Typed FFmpeg places a strong emphasis on type hints, which sets it apart by enhancing code quality and development speed. While other tools may provide similar functionalities, Typed FFmpeg aims to reduce the learning curve and improve code safety, making multimedia processing accessible to a broader range of Python developers, from beginners to seasoned professionals.\n  \nPyAV provides detailed control over media through direct FFmpeg library bindings, catering to advanced users for in-depth manipulation. It demands a greater understanding of FFmpeg’s architecture, presenting a steeper learning curve compared to Typed FFmpeg, which focuses on ease of use and type safety.\n  How to Help:\n\n\n    Check out the project and star it if you like it.\n  \n\n\n    Try it out, report issues, or contribute.\n  \n\n\n    Share any feedback or suggestions for features.\n  \n\n\n    Appreciate your support and feedback!"
},
{
    "title": "No title",
    "content": "What my project does\n    This project will use docker to compile your one file python app into a binary using nuitka that's runnable on many linux systems. You'll need to have docker desktop installed.\n  \nhttps://pypi.org/project/python-compile/\nTarget Audience\n    Developers trying to target production services with optimized python binaries.\n  \n    To use this you'll need a requirements.txt file and a python file entry point that runs your app.\n  \n    For example:\n  \n    pip install python-compile\n  \n    python-compile --os debian --input demo\\http\\server.py --requirements requirements.txt\n  \n    I use nuitka under the hood.\n  \n    Github link:https://github.com/zackees/python-compile"
},
{
    "title": "No title",
    "content": "Hey all! I recently wrote a review comparing different hosting platforms for Python applications. In it, I take a look at Koyeb, Render, pythonanywhere, Fly.io, DigitalOcean App Platform, and Heroku: https://git-push-to-production.hashnode.dev/5-top-free-hosting-platforms-for-python-apps-2024\n\n    I'd love to hear about your experiences with these platforms. Which ones have you used and would recommend? Also, please feel free to share any managed hosting solutions not featured in the article. While I focused on platforms that do a lot of the heavy lifting for you, I'm open to hearing about more self-managed solutions too."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Having almost 10 YOE in Python, it took me this long to understand \"project\" in pyproject.toml means package (in JS,  package.json name is more explicit):\n  \nhttps://packaging.python.org/en/latest/glossary/#term-Project\n\n\n    I've always thought `pyproject.toml` was intended to manage any sort of Python project especially those I don't intended to package and distribute (e.g., some script/notebook and requirements file)."
},
{
    "title": "No title",
    "content": "What my project does:\n    Agent reinforcement learning by interacting with a grid-base environment. This is achieved by implementing the bellman's equation.\n  Target Audience:\n\n\n    Initially, the intended audience for this application consisted of my fellow classmates from the    mathematical modeling course at my university.\n  \n\n\n    Toy or sandbox project.\n  \n\nComparison:\n\n\n    User-friendly interface that allows you to create and manipulate grid-based environments effortlessly.\n  \n\n\n    The power to define and modify most parameters of your grid worlds at the click of a button!\n  \n\n\n\nhttps://github.com/bwe587/GridWorld/"
},
{
    "title": "No title",
    "content": "Hi everyone!I wanted to share a side project of mine that grew into something that, to my knowledge, doesn't exist yet. My high school does basic python, and some of it involves Turtle. My friends were messing around with how to do 3D rendering in turtle, which inspired me to try the same but go full out. Over the course of 5 days, I went from rendering wireframe cubes to what eventually became a 3D engine in Turtle, with way too many features for it's own good.\n  Project Overview\n    3D engine module built exclusively with turtle and mathematical imports such as math or numpy.\n  \n    Features include:\n  \n\n\n    A scene system that is similar to modern game engines (a lot simpler of course)\n  \n\n\n    A physical camera system, with gimbal lock prevention features\n  \n\n\n    A robust object system, with full transformation support and vertex management\n  \n\n\n    Lighting/Material framework allowing for physical light objects and materials applied to objects (experimental)\n  \n\n\n    Loading OBJ files directly\n  \n\n\n    Realtime positioning in 3D space using vectors\n  \n\nMissing Features\n    However, I never considered the scale of this project, so I started getting burnt out quickly, and there are several features that I would love to do, but honestly I don't have the motivation to, so I decided to share the project in it's current state. Some of the things I wish to have added are:\n  \n\n\n    A raycasted lighting system with shadows\n  \n\n\n    Proper object clipping management\n  \n\n\n    Parent/child system for objects alongside object groups\n  \n\n\n    Improved turtle management/optimisation\n  \n\n\n    Verifying function parameters and proper documentation (beyond docstrings)\n  \n\n\n    Vector3 classes instead of lists\n  \n\n\n    A lot more flexible vertex management/manipulation.\n  \n\nTarget Audience\n    This project is a complete gimmick, just seeing how far the Turtle module can be taken despite it's primitive nature and extremely poor performance. Regardless, I'm proud of how far I managed to push Turtle (even if it runs at 0.3 FPS), and I would love to see anyone interested to push this gimmick as far as possible. One of my friends is even working on implementing a physics system using Turtle3D as a renderer.\n  Comparison\n    If this module were to somehow be taken seriously, the only partially fair comparison would be Panda3D or PyOpenGL, and in that regard Turtle3D is nothing. While Turtle3D does allow for a good range of flexibility, it's absolutely nothing like Panda3D or PyOpenGL as they are capable of so much more at a much better rate of performance and code quality.\n  Closing Thoughts\n    Of course, it isn't anywhere near high level or complex, let alone incredibly impressive, and I'm sure there are people in this subreddit who are far smarter than me that will see an incredible amount of miscalculations or atrocities in my code than I have already noticed.All my code is open source and on GitHub here.Feel free to improve it as you will!"
},
{
    "title": "No title",
    "content": "Hello ! Long time reader, first time poster! I began coding in 2023 and I tend to work on GIS related topics. I have to edit a lot of maps for work, and I automated the process by rendering them directly from a python script. One issue I have is that I sometimes have a lot of style information to 'translate' from my normal QGis environment to matplotlib properties, and that is a unpractical process.\n  \n    I started to write a little package to help myself translating QGis layer style files (.qml) into a list of dict of kwargs for matplotlib. It is very basic for now, but it is already working for my use cases, and I would be interested in making it more useful for the community. Here is the git repo : https://github.com/PiouPiou974/qgis_style_files_extractor\n\n    NB that GIS softwares like qgis are able to render more complex styles thant matplotlib, by example a polygon with 2 different hatches + markers, by drawing as many times as needed the same polygon with different parameters. I do the same thing in matplotlib, therefore I use lists of style kwargs dict instead of a unique dict for each style.\n  \n    If you are interested in this package, I would love to discuss about it with you !"
},
{
    "title": "No title",
    "content": "typing-exProject Overview\n    I made this for fun, and also as a dependency for my other project docker-playbook (working in progress).\n  \n    If you found any bug, or have any suggestions, please let me know.\n  \nGitHub link https://github.com/yusing/typing-ex\n\nDocs https://github.com/yusing/typing-ex/tree/main/docs\n\n    This module provides four main sub-modules: Frozen, TypeInfo, EnumEx and TypedDefaultDict, simple description below (check docs above for more):\n  Target Audience\n    People who want more strict type checking and less runtime errors / bugs.\n  Installation\npip install typing-ex\nComparison\ntypeguard provides runtime type checking for functions and class methods with @typeguard decorator. This project aims to enforce type annotation instead.\n  \nmypy provides static type checking, a good companion for this project to provide both static and runtime type checking.\n  ClassesTypedDefaultDict (Similar to pydantic's BaseModel)\nTypedDefaultDict combines features of TypedDict and defaultdict, with type checking upon init, set item, set attribute, and update()\n\n    You can use it to make new classes too, no more __init__()!\n  \n    It allows you to specify @factorymethod in order to convert specific properties to fit target type. (i.e. read json data and convert some properties to other classes)\n  class OtherClass:\n    def __self__(self, s_list: List[str]):\n        ...\n\nclass MyModel(TypedDefaultDict):\n    key1: str = \"default value\"\n    key2: Optional[int] \n    complex_key: OtherClass\n\n    @factorymethod(\"complex_key\")\n    def complex_key_factory(value: List[str]) -> OtherClass:\n        return OtherClass(value)\n\nmy_dict = MyModel(key1=\"value1\", complex_key=[\"1\", \"2\", \"3\"])\n# or dict way\nmy_dict = MyModel({\n    \"key1\": \"value1\",\n    \"complex_key\": [\"1\", \"2\", \"3\"]\n})\nprint(my_dict[\"key1\"])  # Output: value1\nprint(my_dict.complex_key)  # Output: By definition of OtherClassTypeInfo\nTypeInfo provides type information and run-time type checking, no more isinstance, issubclass, is, == mess.\n  \n    Useful when you want to check / assert variable type in runtime.\n  \n\n\nTypeInfo.get(t): alias of TypeInfo[t]\n\n\n\nTypeInfo.of(value): return a TypeInfo object that represents the type of value\n\n\n\nTypeInfo.check_union(t1, t2): check if union type t1 fulfill union type t2\n  \n\n\nTypeInfo[t].type: type\n  \n\n\nTypeInfo[t].origin: unsubscripted type\n  \n\n\nTypeInfo[t].args: type arguments\n  \n\n\nTypeInfo[t].name: name of type (including arguments)\n  \n\n\nTypeInfo[t].check_value(value): check if value matches the type in type info\n  \n\nassert TypeInfo.of([1, 2.0, 3]).type == list[Union[int, float]] # runtime resolving\nt_test = Optional[Dict[Optional[str], Optional[int]]]\nassert TypeInfo[t_test].check_value(None)\nassert TypeInfo[t_test].check_value({\"1\": 1})\nassert TypeInfo[t_test].check_value({\"1\": 1, \"2\": 2})\nassert TypeInfo[t_test].check_value({None: 1, \"2\": None})\nassert TypeInfo[t_test].check_value({None: None, \"2\": None})\nassert not TypeInfo[t_test].check_value({\"1\": 1, 2: 2})\nassert not TypeInfo[t_test].check_value({1: None})EnumEx\nEnumEx is a builtin Enum like class, but allow member aliasing (same value but different output name). Useful if you want to output a different name but with same internal implementation.\n  class ActionEnum(EnumEx):\n    __value_type__ = int  # Optional: Specify value type (defaults to `int`)\n    STOP = 0\n    CREATE = 1\n    START = CREATE # Alias for CREATE\n    UPDATE = 2\n    RESTART = UPDATE # Alias for RESTART\nprint(Action.CREATE)  # Output: CREATE \nprint(Action.START)  # Output: START\nprint(Action.START.orig_name)  # Output: CREATE\nprint(MyEnum.START.value)  # Output: 1\n\nclass Action:\n    action :ActionEnum\n    def start(self): ...\n    def stop(self): ...\n    def restart(self): ...\n    def execute(self):\n        with Progress() as progress:\n            progress.text = f\"{action.name.lower()}ing...\"\n            getattr(self, action.orig_name.lower())()\n            progress.text = \"Done\"Frozen: (raises errors on change)\n    Useful when you want to lock data after processing, without static type checker complaining (i.e. when casting it to tuple)\n  \n\n\nFrozenList: Immutable list\n\n\n\nFrozenDict: Immutable dict\n\n\n\nfrozen_copy: Create frozen copy of supported types (Sequence, Set, Mapping)"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I'm excited to share my Bachelor's degree project: pySSV. It allows you to write and interact with shaders in a Jupyter Notebook.\n  \n    Source code: https://github.com/space928/Shaders-For-Scientific-VisualisationDocumentation: https://pyssv.readthedocs.io/en/latest/\n\n    If you're interested in the project I'd greatly appreciate if you help with the evaluation of this project by filling out my survey: https://forms.gle/rX7uPxaxQ1xcNVQB8\nProject overview\n    This is a library which allows you to write shaders in your Jupyter notebooks to create interactive data visualisations.\n  \n    It provides the following features:\n  \n\n\n    Custom Jupyter Widget\n  \n\n\n    Shader templating and preprocessor\n  \n\n\nShaderToy shader template, allowing ShaderToy shaders to be very easily used in pySSV\n\n\n\n\n\n    Vertex/Pixel/Geometry shader support\n  \n\n\n    Vertex buffer input from NumPy arrays\n  \n\n\n    Texture input from NumPy arrays and Pillow Images\n  \n\n\n    Simple to use IMGUI library\n  \n\n\n    Multipass rendering\n  \n\n\n    Renderdoc support\n  \n\n\nAnd more!\n\n\n\n    A simple SDF shader example in pySSV, utilising the sdf shader template:\n  import pySSV as ssv\n\ncanvas = ssv.canvas()\ncanvas.shader(\"\"\"\n#pragma SSV sdf sdf_main --camera_distance 2. --rotate_speed 1.5 --render_mode SOLID\n\n// SDF taken from: https://iquilezles.org/articles/distfunctions/\nfloat sdCappedTorus(vec3 p, vec2 sc, float ra, float rb) {\n  p.x = abs(p.x);\n  float k = (sc.y*p.x>sc.x*p.y) ? dot(p.xy,sc) : length(p.xy);\n  return sqrt( dot(p,p) + ra*ra - 2.0*ra*k ) - rb;\n}\n\nfloat sdf_main(vec3 p) {\n    float t = 2.*(sin(uTime)*0.5+0.5)+0.2;\n    return sdCappedTorus(p, vec2(sin(t), cos(t)), 0.5, 0.2);\n}\n\"\"\")\ncanvas.run()Target audience\npySSV is targeted towards shader/computer graphics enthusiasts and data scientists. A basic understanding of shaders is recommended, but through the use of it's templating system, pySSV aims to be very approachable for people new to shaders and graphics programming.\n  Comparison\n    Compared to dedicated visualisation libraries such as pyVista or VTK, pySSV aims to give users more control over how their data is rendered and can provide more flexibility for creative visualisation by allowing direct access to shaders.\n  \n    Compared to other graphics libraries such as pythreejs or Glumpy, pySSV places the focus on shaders, and attempts to streamline the API needed to write highly flexible shaders."
},
{
    "title": "No title",
    "content": "https://github.com/KlemenS189/import-embargo\n\nWhat the project does\n\n    In bigger codebases with multiple people/teams it is usually a good architecture pattern to expose certain packages as public and others as private. Python does not enforce this by default.\n  \n    This is why I have created this tool, to add these constraints if needed. You can check more docs in github repo.\n  \n    Code is pure python without any dependencies apart from dev deps. It uses AST under the hood.\n  \nTarget audience\n\n    Usage in production at your own discretion for now.\n  \nComparison\n\n    Simpler alternative to https://github.com/seddonym/import-linter/\n\n    NOTE: This is only a 0.0.1 version so bugs are of course possible. Collaborators welcome :)"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I’ve just created an official release of a VBA precompiler written in python.\n  What my Project Does\n    For those who don’t know, in VBA you can have precompiler blocks to change things for different OSs or VBA versions. For example, to change the function signature for a new windows version:\n  #if Win64 Then\nFunction foo(Bar, Baz)\n#Else\nFunction Foo(Bar)\n#Endif\n    The problem with this is, if you want to scan raw source code, you can’t just ignore the precompiler lines, because now the code looks like you’ve tried to define the same function twice, and one was never ended.\n  \n    This tool takes environment variables that the user provides, and will comment out any lines that need to be skipped, creating 100% valid precompiled code. It even performs the comparisons, and arithmetic operations specified in the VBA specification.\n  Target Audience\n    People interested in malware prevention, static analysis, and Linting may find it helpful. Also, useful if you are interested in learning about compilers, ANTLR, and code parsing.\n  Limitations\n    It is currently missing the standard library functions, like Cbool(), Abs(), etc. I’m guessing these are never called by users in the precompiler phase."
},
{
    "title": "No title",
    "content": "Hi all!\n  \n    I've made another Python cheat sheet tutorial. Yeah yeah, nothing new, I know. But here's the thing:\n  What My Project Does\n    The main idea was not just to write a wall of text telling about everything, but to make it interactive. So that everything would have its example code snippet, which you could change, run, and see how it worked. And not somewhere in a web version, but on your own computer, in your own environment.\n  \n    Fortunately, Python has the perfect tool for this - the Jupyter Notebook. That's why all chapters are written as separate notebooks and there is an example for each point (well, almost).\n  Target Audience\n    I originally aimed at complete beginners and tried to go from simple to complex, but I think I overdid it at some point. So it might be just as suitable to refresh knowledge and fill in the gaps for anyone.\n  What else\n    It also has some useful information for Python-developer interview preparation and a link to the absolutely wonderful Tech Interview Handbook for Coding Interview preparation in case anyone missed it.\n  \n    I would very welcome any constructive criticism, any advice, and of course contributions and GitHub stars :)\n  \n    A little less, but still I will be glad to unreasonable hatred and comments that nobody needs it and that there are hundreds of similar projects on the Internet.\n  \n    Check out the project on GitHub: PyQuest"
},
{
    "title": "No title",
    "content": "Hi guys! I'd like to share a reactive web UI framework I've been working on for a while that I made public a couple of days ago.\n  \n\n\n    GitHub: https://github.com/hyperdiv/hyperdiv\n\n\n\n    Homepage: https://hyperdiv.io\n\n\n\n    There is a short coding demo video and intro article on the website.\n  What My Project Does\n    Hyperdiv is a way to build reactive UIs in pure Python quickly, with a built-in UI component system based on Shoelace (https://shoelace.style), markdown, and charts based on Chart.js (https://chartjs.org). It uses immediate-mode syntax which enables seamlessly blending declarative UI code with Python logic and event handling.\n  Target Audience\n    The aim of Hyperdiv is to reduce tool and language complexity when building full stack apps, and enable people to get to a working UI very quickly. I think it is a good fit for adding browser UIs to CLI tools, prototyping UIs, and internal tools. You can also put it behind Nginx and deploy it on the internet.\n  Comparison\n    Hyperdiv adds to a niche currently occupied by Streamlit, Reflex.dev, PyWebIO, PyJS, etc. -- frameworks that let you build web apps in pure Python.\n  \n    Hyperdiv stands apart with a unique blend of immediate-mode UI + reactive state, and letting you build fairly unrestricted, arbitrarily nested UI layouts with terse syntax.\n  \n    I appreciate your support!"
},
{
    "title": "No title",
    "content": "Hello everyone!\n  \n    We (4 people startup) open-sourced an SDK to create Stateful Functions in Python!\n  \n\n\n    Github: https://github.com/stealthrocket/dispatch-sdk-python\n\n\n\n    Docs: https://docs.stealthrocket.cloud\n\n\n\n    Package: https://pypi.org/project/dispatch-functions\n\n\nWhat My Project Does\n    We're developing Dispatch, a cloud service that helps developers create reliable systems. It can be used to build real-time and durable workflows, data pipelines, highly parallel jobs, and much more.\n  \n    Say you have a background job that fails and needs to be retried; getting the retry logic right is difficult, and it gets even harder when the retries need to happen across a fleet of instances and in a way that doesn’t take down the application. However, the biggest pain point is managing the state of the system, which often requires adding queues, databases, or workflow engines.\n  \n    With the SDK we released, we are giving Python developers a simple solution to these problems: Stateful Functions.\n  Audience\n    This SDK is useful to Python developers who build production services and need solutions for state management of their background jobs, data streams, workflows, etc...\n  Comparison\n    Dispatch addresses technical problems that are sometimes solved with cloud products like AWS Step Functions, Azure Durable Functions, or Temporal.\n  \n    Dispatch focuses on being lightweight and simple: there is no heavy framework to bring into the application, no cloud component to deploy, no workflow language to learn. The code can run in a serverless deployment, a container, or really anywhere. Python developers can add it to an existing application without having to change anything about the rest of the code.\n  Technical Details\n    At the core of the SDK is an event loop integrating with native Python coroutines (async/await) that supports pausing execution and serializing the state. On resumption, the coroutine state is recreated to continue execution, which could happen in a different instance of the application!\n  \n    Capturing stack frames was the most difficult part: even if Python has native support for pickling values, some objects like generators or coroutines cannot be serialized by default. To work around this limitation, we developed a native extension for CPython that allows us to save and reload the internal interpreter state.\n  Feedback\n    The project is young, but we're really excited to share it. Check it out and let us know what you think!"
},
{
    "title": "No title",
    "content": "I've published a new package for people interested in working with mathematical objects called continued fractions in an object-oriented way with Python.\n  \nhttps://pypi.org/project/continuedfractions/\nWhat My Project Does?\n    The package has been designed to take advantage of an existing Python standard library for rational numbers (`fractions`), and provides a way of computing continued fraction representations for any real number that can be defined in Python, and inspecting their properties (elements, convergents, segments, remainders), and also performing operations with them as normal rational numbers. Continued fractions are not part of the `fractions` standard library.\n  Background\n    I know it's a rather specialised topic, but I guess the first question is \"What is a continued fraction?\", and the next might be, \"What can you do with them?\". A continued fraction is a way of representing real numbers as the sum of an integer part + a nested fraction, where the denominators contain sums of integers and fractions, and those fractions then contain sums of integers and fractions, and so on.\n  \n    Since integers and fractions add up to rational numbers, continued fractions are ways of representing real numbers as the sum of (possibly, infinitely many) rational numbers.\n  \n    They are nested, hierarchical structures, which can be finite or infinite depending on the type of number being represented. For irrational numbers like pi the continued fraction representation will always be infinite, but for rational numbers (ratios of integers) it will always be finite.\n  \n    I don't want to go into the full details, but to answer the next question: continued fractions are really quite beautiful, interesting and useful in many ways. They are very important for the approximation of irrational numbers, which have infinite decimal expansions, with rational numbers, which have a finite decimal expansion. They are also connected to the study of special sequences and orderings of rational numbers - see the Stern-Brocot tree.\n  Target Audience\n    I guess people interested in continued fractions and, more generally, perhaps, number theory.\n  Comparison\n    I don't know of any other Python package that deals with continued fractions in the same way - I did come across one called `contfrac`, but that is not object-oriented, and when I compared the performance of its main function for computing continued fraction representations with mine it was much slower.\n  \n    There are other non-Python packages that can do continued fractions, obviously, like Mathematica, and who knows what else. But this has been designed for Python users in mind.\n  Limitations\n    Obviously the package cannot give you a complete continued fraction representation for pi or any irrational number - it gives you a finite representation that is the best possible, given the limitations of binary floating point arithmetic, as implemented in Python. For irrational numbers the representation is approximate, not exact - this will be improved in the future to allow the representations to be as exact as possible.\n  Documentation\n    At the moment the documentation is confined to the GitHub project README, but it is quite detailed, with lots of examples. I do plan to publish the docs separately, e.g. GitHub Pages or something like that, in later releases.\n  Feedback\n    All/any feedback is welcome, and so are contributions from any one interested in contributing to adding more cool features for studying continued fractions with Python."
},
{
    "title": "No title",
    "content": "GitHub: KeyCraftsman\nWhat My Project Does\nKeyCraftsman is an innovative Python class designed to generate passcodes to your own liking. Offering an array of features such as key length specification, character exclusion, inclusion of all characters, URL-safe encoding, and exportation of the generated passkey(s).\n  \nTarget Audience\n    This module is tailored for users who seek for flexibility in generating custom randomized password key(s).\n  Comparison\n    Many existing modules lack the flexibility needed in terms of features and often include deprecated elements. In contrast, this module stands out by offering a rich set of features compared to traditional Python password-generating modules. While the demand for such features may not be exceptionally high, I embarked on creating a modernized, medium/heavyweight version of key generation for the sheer enjoyment of exploring new possibilities.\n  \nFeatures\n- Exclude Characters: Tailor your keys by excluding specific characters.\n  \n\n- Include All Characters: Embrace diversity by including all ASCII letters, digits, and punctuation.\n  \n\n- Unique Characters: Ensure uniqueness in generated keys or words. If words is specified, it will generate words with only unique number of letters in them.\n  \n\n- Custom Text Wrapping: Wrap your keys with a custom separator and width for a personalized touch.\n  \n\n- Multiple Key Generation: Efficiently generate multiple keys with a single instance.\n  \n\n- Word Generation: Explore creative possibilities with word generation using random.SystemRandom().\n\n\n    - Exclusion Chart: Simplify character exclusion with the provided exclusion chart, available for printing and export.\n  \n\nFor a comprehensive overview of all features and methods, please refer to the documentation. We invite you to explore the capabilities of KeyCraftsman and hope you find joy in utilizing this modernized approach to key generation."
},
{
    "title": "No title",
    "content": "Showcasing marimo-wasm\n    Hey everyone! We (2 developers) made marimo compatible with WebAssembly (WASM), so you can run it entirely in the browser thanks to Pyodide\n  \nhttps://marimo.app\nWhat My Project Does\n    Background: marimo is an open-source reactive notebook for Python, that aims to solve well-known problems with traditional Python notebooks[1].\n  \n    This showcase is for marimo-wasm: a WASM build of marimo allowing the notebook to run entirely in the browser.\n  \n    This is done through WebAssembly and standing on the shoulder of Pyodide (h/t!)\n  \n    You can try out the playground: https://marimo.app/\n\n    This tool has been great for educating others through interactive examples. You can share snippets of code with just a URL. For example, here is a notebook on Bayes' Theorem.\n  \n    While the installable marimo library is hardened and used in production, the WASM build may have some rough edges. Still, the playground is great for sharing notebooks, data applications[2], or explorations with the community without the cost of servers to you or your audience.\n  \n    [1] https://docs.marimo.io/faq.html#faq-problems\n\n    [2] You can hide the code and turn your notebook into an app\n  Target Audience\n    Data Engineers, Data Scientists, Story Tellers, Educators, and Researchers.\n  Comparison\nJupyterLite - a WASM powered Jupyter running in the browser. However, it is not reactive like marimo.\n  \nIPyflow - a reactive notebook for Python implemented as a Jupyter kernel. However, it is not WASM compatible.\n  \nJupyter - marimo is a reinvention of the Python notebook as a reproducible, interactive, and shareable Python program that can be executed as scripts or deployed as interactive web apps - without the need of extensions or additional infrastructure. More here."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What My Project Does\ncopykitten is a clipboard library with support for text and images and a simple API. It's built around Rust arboard library. Thanks to this, it's multiplatform and doesn't require any dependencies.\n  Target Audience\n    Developers building CLI/GUI/TUI applications. The library has beta status on PyPI, but the underlying Rust library is pretty stable, being used in commercial projects like Bitwarden and 1Password.\n  Comparison\n    There are lots of other clipboard libraries for Python: pyperclip, jaraco.clipboard, pyclip, just to name a few. However, most of them are not maintained for years and require the presence of additional libraries or tools in the operating system. copykitten doesn't suffer from these shortcomings.\n  A bit of history\n    Throughout my years with Python there were several times when I needed to use the clipboard in my applications and every time I had to fall back to some shaky methods like asking the end user to install xclip and calling subprocess.run. This never felt great.\n  \n    Right now I'm making a multiplayer TUI game (maybe I’ll showcase it later too :) ), where users can copy join game codes into the clipboard to easily share it (much like Among Us). This is how I came to the idea of making such a library. I also wanted to try Rust for a long time, and so this all just clicked in my head instantly.\n  \n    I had fun building it and definitely had some pain too and learned a bit of nitty-gritty details about how clipboards work in different operating systems. Now I hate Windows.\n\n    With this post I hope to gain some attention to the project so that I can receive feedback about the issues and maybe feature requests and spread the word that there's a modern, convenient alternative to the existing packages.\n  \n    Feel free to try it out: https://github.com/Klavionik/copykitten"
},
{
    "title": "No title",
    "content": "Hi all!\n  \n    Introducing version 0.2.0 of the Squirrels Python library! Squirrels is a dbt-like framework that allows users to create REST API endpoints for dynamic data analytics. Feel free to read more about it on our website!\n  \n    Following our version 0.1.0 release in our previous post here, we've recreated the framework to bring you an improved experience! Although this introduces many breaking changes, we intend to prevent breaking changes as much as possible for subsequent minor version increments (i.e. until 1.0.0). Compared to 0.1.0, the new 0.2.0 version has incorporated the following features:\n  \n\n\nFlexible Lineage for Data Models\n\n\n\n    Create data models (in SQL or Python) as database views or federates. Federate models provide a \"ref\" function/macro to refer to other models just like dbt! Specify a target data model for your dataset API, and Squirrels takes care of the model dependencies and execution order for you.\n  \n\n\n    You can query different database systems with different database views and join the results together in a federate model!\n  \n\n\n\n\nConsolidated Parameters\n\n\n\n    Multiple dataset APIs can share common parameters. You can specify a pool of parameter properties in one place (in Python or YAML) and specify the list of parameters that each dataset uses.\n  \n\n\n\n\nAuthentication\n\n\n\n    You can specify a custom User model and an authentication method that integrates with your authentication source in a Python file. The user attributes can be used the affect the widget parameters or models in real time!\n  \n\n\n    We now support three different levels of access for datasets, with public datasets open to all, protected datasets open to both internal and external users, and private datasets only for private users.\n  \n\n\n\n\n    Feel free to give Squirrels a whirl by following our simple tutorial listed on our tutorial page! And here's the link to our GitHub repo: https://github.com/squirrels-nest/squirrels\n\n    Below are the sections specified in Rule 10:\n  \nWhat My Project Does:\n\n    Squirrels is a dbt-like framework that allows users to create REST API endpoints for dynamic data analytics. Again, feel free to read more about it on our website here!\n  \nTarget Audience:\n\n    Data Engineers, Analytics Engineers, Data Architects, Data Analysts, Data Scientists, Business Analysts, API Engineers, etc.\n  \nComparisons:\n\n    We've come across several questions regarding \"What's the difference between Squirrels and <some other data tool>?\". Below are a few of the points on some commonly asked about comparisons.\n  \n\n\n    dbt:\n  \n\n\n    Used for data transformations only (Squirrels is largely inspired by dbt, but made for the real-time analytics or semantic layer use case instead)\n  \n\n\n    Does not support widget parameters (and does not need to for non-customer-facing data transformations)\n  \n\n\n\n\n    cube.js:\n  \n\n\n    Data models are created in YAML (for Squirrels, they're created in SQL or Python)\n  \n\n\n    Cannot create a lineage of multiple data models under one API endpoint\n  \n\n\n    No support for specifying widget parameters properties to dynamically change dataset behavior\n  \n\n\n\n\n    VulcanSQL\n  \n\n\n    No support for Python models and cannot create a lineage of multiple data models under one API endpoint\n  \n\n\n    Can specify parameters, but it's very \"free-form\" with limited control over the typing of parameters for the client (\"typing\" such as dropdown parameter, date parameter, etc.)\n  \n\n\n    Similar to Squirrels, it can have authenticated users affect the model behavior, but cannot integrate with your authentication source.\n  \n\n\n\n\n    Thus, we are not aware of existing tools that have the same capabilities as Squirrel does. We have tried to use a dbt-like project structure to make it easier for people to pick up."
},
{
    "title": "No title",
    "content": "Hi all, long-time reader and first-time poster. I recently had my 1st kid, have some time off, and built Cry Baby\nWhat My Project Does\n    Cry Baby provides a probability that your baby is crying by continuously recording audio, chunking it into 4-second clips, and feeding these clips into a Convolutional Neural Network (CNN).\n  \n    Cry Baby is currently compatible with MAC and Linux, and you can find the setup instructions in the README.\n  Target Audience\n    People with babies with too much time on their hands. I envisioned this tool as a high-tech baby monitor that could send notifications and allow live audio streaming. However, my partner opted for a traditional baby monitor instead. 😅\n  Comparison\n    I know baby monitors exist that claim to notify you when a baby is crying, but the ones I've seen are only based on decibels. Then Amazon's Alexa seems to work based on crying...but I REALLY don't like the idea of having that in my house.\n  \n    I couldn't find an open source model that detected baby crying so I decided to make one myself. The model alone may be useful for someone, I'm happy to clean up the training code and publish that if anyone is interested.\n  \n    I'm taking a break from the project, but I'm eager to hear your thoughts, especially if you see potential uses or improvements. If there's interest, I'd love to collaborate further—I still have four weeks of paternity leave to dive back in!\n  \n    Update:I've noticed his poops are loud, which is one predictor of his crying. Have any other parents experienced this of 1 week-olds? I assume it's going to end once he starts eating solids. But it would be funny to try and train another model on the sound of babies pooping so I change his diaper before he starts crying."
},
{
    "title": "No title",
    "content": "I've received an assignment whereby I am required to extract texts, tables, layouts, headers, titles, etc from PDFs (Multi-page).\n  \n    These PDFs have actual text on them and not images.\n  \n    So far I've tried using Camelot, PyMuPDF, and Nougat. Unfortunately, none of these modules are able to meet my client's expectations.\n  \n    Due to this, I've tried AWS Textract. I've showed a sample result of Textract and they immediately loved it. However, only then they mentioned that the PDFs have sensitive data and cannot be exposed via the internet.\n  \n    Now, they are looking to find an on-prem solution to get similar results as AWS Textract.\n  \n    Anyone know any kind of software/tool/python module that can be self-hosted and able to get similar results as AWS Textract?\n  \n    Thanks in advance."
},
{
    "title": "No title",
    "content": "I'm excited to share my latest open-source project: Typed FFmpeg, a Python wrapper for FFmpeg enhanced with type hints for better code predictability and IDE support.\n  \nhttps://github.com/livingbio/typed-ffmpeg\nKey Features:\n\n\n    Type-hinted for improved developer experience.\n  \n\n\n    Simplified FFmpeg operations in Python.\n  \n\n\n    Detailed docs with examples for easy start.\n  \n\nWhat My Project Does:\n    Typed FFmpeg is an open-source Python wrapper for FFmpeg, designed to integrate multimedia processing capabilities into Python applications more seamlessly. It offers type hints throughout, improving code reliability and developer experience by providing better IDE support and error checking before runtime. Whether you're resizing videos, converting file formats, or extracting metadata, Typed FFmpeg simplifies these tasks with an intuitive, Pythonic interface.\n  Target Audience:\n    This project is aimed at developers working on multimedia applications, educational content, or data analysis involving video and audio processing. It's suitable for both production and experimental projects, offering a balance between ease of use and robust functionality.\n  Comparison:\n    Unlike existing high level FFmpeg wrappers for Python, Typed FFmpeg places a strong emphasis on type hints, which sets it apart by enhancing code quality and development speed. While other tools may provide similar functionalities, Typed FFmpeg aims to reduce the learning curve and improve code safety, making multimedia processing accessible to a broader range of Python developers, from beginners to seasoned professionals.\n  \nPyAV provides detailed control over media through direct FFmpeg library bindings, catering to advanced users for in-depth manipulation. It demands a greater understanding of FFmpeg’s architecture, presenting a steeper learning curve compared to Typed FFmpeg, which focuses on ease of use and type safety.\n  How to Help:\n\n\n    Check out the project and star it if you like it.\n  \n\n\n    Try it out, report issues, or contribute.\n  \n\n\n    Share any feedback or suggestions for features.\n  \n\n\n    Appreciate your support and feedback!"
},
{
    "title": "No title",
    "content": "What my project does\n    This project will use docker to compile your one file python app into a binary using nuitka that's runnable on many linux systems. You'll need to have docker desktop installed.\n  \nhttps://pypi.org/project/python-compile/\nTarget Audience\n    Developers trying to target production services with optimized python binaries.\n  \n    To use this you'll need a requirements.txt file and a python file entry point that runs your app.\n  \n    For example:\n  \n    pip install python-compile\n  \n    python-compile --os debian --input demo\\http\\server.py --requirements requirements.txt\n  \n    I use nuitka under the hood.\n  \n    Github link:https://github.com/zackees/python-compile"
},
{
    "title": "No title",
    "content": "Hey all! I recently wrote a review comparing different hosting platforms for Python applications. In it, I take a look at Koyeb, Render, pythonanywhere, Fly.io, DigitalOcean App Platform, and Heroku: https://git-push-to-production.hashnode.dev/5-top-free-hosting-platforms-for-python-apps-2024\n\n    I'd love to hear about your experiences with these platforms. Which ones have you used and would recommend? Also, please feel free to share any managed hosting solutions not featured in the article. While I focused on platforms that do a lot of the heavy lifting for you, I'm open to hearing about more self-managed solutions too."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Having almost 10 YOE in Python, it took me this long to understand \"project\" in pyproject.toml means package (in JS,  package.json name is more explicit):\n  \nhttps://packaging.python.org/en/latest/glossary/#term-Project\n\n\n    I've always thought `pyproject.toml` was intended to manage any sort of Python project especially those I don't intended to package and distribute (e.g., some script/notebook and requirements file)."
},
{
    "title": "No title",
    "content": "What my project does:\n    Agent reinforcement learning by interacting with a grid-base environment. This is achieved by implementing the bellman's equation.\n  Target Audience:\n\n\n    Initially, the intended audience for this application consisted of my fellow classmates from the    mathematical modeling course at my university.\n  \n\n\n    Toy or sandbox project.\n  \n\nComparison:\n\n\n    User-friendly interface that allows you to create and manipulate grid-based environments effortlessly.\n  \n\n\n    The power to define and modify most parameters of your grid worlds at the click of a button!\n  \n\n\n\nhttps://github.com/bwe587/GridWorld/"
},
{
    "title": "No title",
    "content": "Hi everyone!I wanted to share a side project of mine that grew into something that, to my knowledge, doesn't exist yet. My high school does basic python, and some of it involves Turtle. My friends were messing around with how to do 3D rendering in turtle, which inspired me to try the same but go full out. Over the course of 5 days, I went from rendering wireframe cubes to what eventually became a 3D engine in Turtle, with way too many features for it's own good.\n  Project Overview\n    3D engine module built exclusively with turtle and mathematical imports such as math or numpy.\n  \n    Features include:\n  \n\n\n    A scene system that is similar to modern game engines (a lot simpler of course)\n  \n\n\n    A physical camera system, with gimbal lock prevention features\n  \n\n\n    A robust object system, with full transformation support and vertex management\n  \n\n\n    Lighting/Material framework allowing for physical light objects and materials applied to objects (experimental)\n  \n\n\n    Loading OBJ files directly\n  \n\n\n    Realtime positioning in 3D space using vectors\n  \n\nMissing Features\n    However, I never considered the scale of this project, so I started getting burnt out quickly, and there are several features that I would love to do, but honestly I don't have the motivation to, so I decided to share the project in it's current state. Some of the things I wish to have added are:\n  \n\n\n    A raycasted lighting system with shadows\n  \n\n\n    Proper object clipping management\n  \n\n\n    Parent/child system for objects alongside object groups\n  \n\n\n    Improved turtle management/optimisation\n  \n\n\n    Verifying function parameters and proper documentation (beyond docstrings)\n  \n\n\n    Vector3 classes instead of lists\n  \n\n\n    A lot more flexible vertex management/manipulation.\n  \n\nTarget Audience\n    This project is a complete gimmick, just seeing how far the Turtle module can be taken despite it's primitive nature and extremely poor performance. Regardless, I'm proud of how far I managed to push Turtle (even if it runs at 0.3 FPS), and I would love to see anyone interested to push this gimmick as far as possible. One of my friends is even working on implementing a physics system using Turtle3D as a renderer.\n  Comparison\n    If this module were to somehow be taken seriously, the only partially fair comparison would be Panda3D or PyOpenGL, and in that regard Turtle3D is nothing. While Turtle3D does allow for a good range of flexibility, it's absolutely nothing like Panda3D or PyOpenGL as they are capable of so much more at a much better rate of performance and code quality.\n  Closing Thoughts\n    Of course, it isn't anywhere near high level or complex, let alone incredibly impressive, and I'm sure there are people in this subreddit who are far smarter than me that will see an incredible amount of miscalculations or atrocities in my code than I have already noticed.All my code is open source and on GitHub here.Feel free to improve it as you will!"
},
{
    "title": "No title",
    "content": "Hello ! Long time reader, first time poster! I began coding in 2023 and I tend to work on GIS related topics. I have to edit a lot of maps for work, and I automated the process by rendering them directly from a python script. One issue I have is that I sometimes have a lot of style information to 'translate' from my normal QGis environment to matplotlib properties, and that is a unpractical process.\n  \n    I started to write a little package to help myself translating QGis layer style files (.qml) into a list of dict of kwargs for matplotlib. It is very basic for now, but it is already working for my use cases, and I would be interested in making it more useful for the community. Here is the git repo : https://github.com/PiouPiou974/qgis_style_files_extractor\n\n    NB that GIS softwares like qgis are able to render more complex styles thant matplotlib, by example a polygon with 2 different hatches + markers, by drawing as many times as needed the same polygon with different parameters. I do the same thing in matplotlib, therefore I use lists of style kwargs dict instead of a unique dict for each style.\n  \n    If you are interested in this package, I would love to discuss about it with you !"
},
{
    "title": "No title",
    "content": "typing-exProject Overview\n    I made this for fun, and also as a dependency for my other project docker-playbook (working in progress).\n  \n    If you found any bug, or have any suggestions, please let me know.\n  \nGitHub link https://github.com/yusing/typing-ex\n\nDocs https://github.com/yusing/typing-ex/tree/main/docs\n\n    This module provides four main sub-modules: Frozen, TypeInfo, EnumEx and TypedDefaultDict, simple description below (check docs above for more):\n  Target Audience\n    People who want more strict type checking and less runtime errors / bugs.\n  Installation\npip install typing-ex\nComparison\ntypeguard provides runtime type checking for functions and class methods with @typeguard decorator. This project aims to enforce type annotation instead.\n  \nmypy provides static type checking, a good companion for this project to provide both static and runtime type checking.\n  ClassesTypedDefaultDict (Similar to pydantic's BaseModel)\nTypedDefaultDict combines features of TypedDict and defaultdict, with type checking upon init, set item, set attribute, and update()\n\n    You can use it to make new classes too, no more __init__()!\n  \n    It allows you to specify @factorymethod in order to convert specific properties to fit target type. (i.e. read json data and convert some properties to other classes)\n  class OtherClass:\n    def __self__(self, s_list: List[str]):\n        ...\n\nclass MyModel(TypedDefaultDict):\n    key1: str = \"default value\"\n    key2: Optional[int] \n    complex_key: OtherClass\n\n    @factorymethod(\"complex_key\")\n    def complex_key_factory(value: List[str]) -> OtherClass:\n        return OtherClass(value)\n\nmy_dict = MyModel(key1=\"value1\", complex_key=[\"1\", \"2\", \"3\"])\n# or dict way\nmy_dict = MyModel({\n    \"key1\": \"value1\",\n    \"complex_key\": [\"1\", \"2\", \"3\"]\n})\nprint(my_dict[\"key1\"])  # Output: value1\nprint(my_dict.complex_key)  # Output: By definition of OtherClassTypeInfo\nTypeInfo provides type information and run-time type checking, no more isinstance, issubclass, is, == mess.\n  \n    Useful when you want to check / assert variable type in runtime.\n  \n\n\nTypeInfo.get(t): alias of TypeInfo[t]\n\n\n\nTypeInfo.of(value): return a TypeInfo object that represents the type of value\n\n\n\nTypeInfo.check_union(t1, t2): check if union type t1 fulfill union type t2\n  \n\n\nTypeInfo[t].type: type\n  \n\n\nTypeInfo[t].origin: unsubscripted type\n  \n\n\nTypeInfo[t].args: type arguments\n  \n\n\nTypeInfo[t].name: name of type (including arguments)\n  \n\n\nTypeInfo[t].check_value(value): check if value matches the type in type info\n  \n\nassert TypeInfo.of([1, 2.0, 3]).type == list[Union[int, float]] # runtime resolving\nt_test = Optional[Dict[Optional[str], Optional[int]]]\nassert TypeInfo[t_test].check_value(None)\nassert TypeInfo[t_test].check_value({\"1\": 1})\nassert TypeInfo[t_test].check_value({\"1\": 1, \"2\": 2})\nassert TypeInfo[t_test].check_value({None: 1, \"2\": None})\nassert TypeInfo[t_test].check_value({None: None, \"2\": None})\nassert not TypeInfo[t_test].check_value({\"1\": 1, 2: 2})\nassert not TypeInfo[t_test].check_value({1: None})EnumEx\nEnumEx is a builtin Enum like class, but allow member aliasing (same value but different output name). Useful if you want to output a different name but with same internal implementation.\n  class ActionEnum(EnumEx):\n    __value_type__ = int  # Optional: Specify value type (defaults to `int`)\n    STOP = 0\n    CREATE = 1\n    START = CREATE # Alias for CREATE\n    UPDATE = 2\n    RESTART = UPDATE # Alias for RESTART\nprint(Action.CREATE)  # Output: CREATE \nprint(Action.START)  # Output: START\nprint(Action.START.orig_name)  # Output: CREATE\nprint(MyEnum.START.value)  # Output: 1\n\nclass Action:\n    action :ActionEnum\n    def start(self): ...\n    def stop(self): ...\n    def restart(self): ...\n    def execute(self):\n        with Progress() as progress:\n            progress.text = f\"{action.name.lower()}ing...\"\n            getattr(self, action.orig_name.lower())()\n            progress.text = \"Done\"Frozen: (raises errors on change)\n    Useful when you want to lock data after processing, without static type checker complaining (i.e. when casting it to tuple)\n  \n\n\nFrozenList: Immutable list\n\n\n\nFrozenDict: Immutable dict\n\n\n\nfrozen_copy: Create frozen copy of supported types (Sequence, Set, Mapping)"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I'm excited to share my Bachelor's degree project: pySSV. It allows you to write and interact with shaders in a Jupyter Notebook.\n  \n    Source code: https://github.com/space928/Shaders-For-Scientific-VisualisationDocumentation: https://pyssv.readthedocs.io/en/latest/\n\n    If you're interested in the project I'd greatly appreciate if you help with the evaluation of this project by filling out my survey: https://forms.gle/rX7uPxaxQ1xcNVQB8\nProject overview\n    This is a library which allows you to write shaders in your Jupyter notebooks to create interactive data visualisations.\n  \n    It provides the following features:\n  \n\n\n    Custom Jupyter Widget\n  \n\n\n    Shader templating and preprocessor\n  \n\n\nShaderToy shader template, allowing ShaderToy shaders to be very easily used in pySSV\n\n\n\n\n\n    Vertex/Pixel/Geometry shader support\n  \n\n\n    Vertex buffer input from NumPy arrays\n  \n\n\n    Texture input from NumPy arrays and Pillow Images\n  \n\n\n    Simple to use IMGUI library\n  \n\n\n    Multipass rendering\n  \n\n\n    Renderdoc support\n  \n\n\nAnd more!\n\n\n\n    A simple SDF shader example in pySSV, utilising the sdf shader template:\n  import pySSV as ssv\n\ncanvas = ssv.canvas()\ncanvas.shader(\"\"\"\n#pragma SSV sdf sdf_main --camera_distance 2. --rotate_speed 1.5 --render_mode SOLID\n\n// SDF taken from: https://iquilezles.org/articles/distfunctions/\nfloat sdCappedTorus(vec3 p, vec2 sc, float ra, float rb) {\n  p.x = abs(p.x);\n  float k = (sc.y*p.x>sc.x*p.y) ? dot(p.xy,sc) : length(p.xy);\n  return sqrt( dot(p,p) + ra*ra - 2.0*ra*k ) - rb;\n}\n\nfloat sdf_main(vec3 p) {\n    float t = 2.*(sin(uTime)*0.5+0.5)+0.2;\n    return sdCappedTorus(p, vec2(sin(t), cos(t)), 0.5, 0.2);\n}\n\"\"\")\ncanvas.run()Target audience\npySSV is targeted towards shader/computer graphics enthusiasts and data scientists. A basic understanding of shaders is recommended, but through the use of it's templating system, pySSV aims to be very approachable for people new to shaders and graphics programming.\n  Comparison\n    Compared to dedicated visualisation libraries such as pyVista or VTK, pySSV aims to give users more control over how their data is rendered and can provide more flexibility for creative visualisation by allowing direct access to shaders.\n  \n    Compared to other graphics libraries such as pythreejs or Glumpy, pySSV places the focus on shaders, and attempts to streamline the API needed to write highly flexible shaders."
},
{
    "title": "No title",
    "content": "https://github.com/KlemenS189/import-embargo\n\nWhat the project does\n\n    In bigger codebases with multiple people/teams it is usually a good architecture pattern to expose certain packages as public and others as private. Python does not enforce this by default.\n  \n    This is why I have created this tool, to add these constraints if needed. You can check more docs in github repo.\n  \n    Code is pure python without any dependencies apart from dev deps. It uses AST under the hood.\n  \nTarget audience\n\n    Usage in production at your own discretion for now.\n  \nComparison\n\n    Simpler alternative to https://github.com/seddonym/import-linter/\n\n    NOTE: This is only a 0.0.1 version so bugs are of course possible. Collaborators welcome :)"
},
{
    "title": "No title",
    "content": "Hi there!\n  \nWhat My Project Does\n\n    This is a simple, yet powerful, script in Python (3.8 - 3.12+ without python deps) for which packs existent docker-compose project in self-executable encrypted archive.To restore, you don't need to install DKP or remember the details: just run archive as a script (help included!).\n  \n    It can backup images, mounted files, directories, volumes, as well as project env files.\n  \n    Can be used as CLI or as library.\n  \nTarget Audience\n\n    Created to help other people, since I believe it's not only me who setups once backup routine and after a years forgot how to recover.\n  \n    Primarily for sysadmins, for self-hosters, for SMB (for large corporation it's probably not very useful).\n  \nComparison\n\n    The closest possible solution:\n  \n\n\n    restic - is not compose-aware\n  \n\n\nInstallation:\n  pip install dkp\nSource: https://github.com/reddec/dkp\n\nUsage:\nusage: dkp [-h] [--output OUTPUT] [--skip-images] [--passphrase PASSPHRASE] [project]\n    \nDocker Compose packer - pack compose project with all batteries included\n    \npositional arguments:\n  project               Compose project name\n    \noptions:\n  -h, --help            show this help message and exit\n  --output OUTPUT, -o OUTPUT\n                        Output file\n  --skip-images, -S     Do not archive images\n  --passphrase PASSPHRASE, -p PASSPHRASE\n                        Passphrase to encrypt backup. Can be set via env PASSPHRASE"
},
{
    "title": "No title",
    "content": "👋 Greetings and Salutations all!\n  \n    I wanted to share a new way of embedding interactive Python code cells into websites, slides, and books through the {quarto-pyodide} extension for Quarto\n\n\n\n    See the documentation: https://quarto.thecoatlessprofessor.com/pyodide\n\n\n\n    Check it out on GitHub: https://github.com/coatless-quarto/pyodide\n\n\nProject Overview\n    This project introduces a new custom code cell engine for Quarto that allows for interactive Python cells into HTML documents. Leveraging the capabilities of Pyodide's WebAssembly distribution for Python, the Python code is run directly in the user's web browser. Moreover, the custom engine removes the need for any expertise in HTML or JavaScript on the author's side.\n  Target Audience\n    The primary audience for this project includes instructors teaching Python who wish to simplify the setup process for their students, making it ideal for introductory Computer Science and Data Science courses. Additionally, it offers a swift and reproducible way for data scientists and others interested in Python scripts to analyze data interactively.\n  \nNote: While this provides a user-friendly approach, it does not eliminate the eventual need for exposure to more comprehensive IDEs like VS Code, PyCharm, or even Posit Workbench.\n  Comparison\n    Compared to existing approaches like JupyterLite, PyScript, and Thebe, the {quarto-pyodide} extension adopts a more user-friendly stance by abstracting away complex interfaces and setup for both visitors to the web page and authors.\n  Demos\n\n\n    README:\n  \n\n\n    Live: https://quarto.thecoatlessprofessor.com/pyodide/examples/readme/\n\n\n\n    Source: https://github.com/coatless-quarto/pyodide/tree/main/examples/readme\n\n\n\n\n\n    RevealJS:\n  \n\n\n    Live: https://quarto.thecoatlessprofessor.com/pyodide/examples/revealjs/#/title-slide\n\n\n\n    Source: https://github.com/coatless-quarto/pyodide/tree/main/examples/revealjs\n\n\n\n\n\n    Blog:\n  \n\n\n    Live: https://quarto.thecoatlessprofessor.com/pyodide/examples/blog/\n\n\n\n    Source: https://github.com/coatless-quarto/pyodide/tree/main/examples/blog"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "For those who work with Qt, I'm sure some of you have seen Megasolid Idiom, a simple, rich-text editor. However, it's written with PyQt5.\n  \n    If you're learning graphics programming in Python, that's disappointing since PyQt6 has been around since January of 2021 and appears to be stable. I wanted that editor in PyQt6, so I updated it and shared it on github. It has an MIT license, like the original."
},
{
    "title": "No title",
    "content": "Hey everyone! I want to share with everyone the project I have been working on called Alto. It's a Python CLI tool to run functions, scripts, and Jupyter notebooks in the cloud.\n  \n\n\n    Github: https://github.com/runprism/alto\n\n\n\n    Documentation: https://alto.mintlify.app/welcome/v0.0.8/welcome\n\n\nWhat My Project Does\n    Alto allows data practitioners to effortlessly run functions, scripts, and Jupyter notebooks in the cloud. Currently, Alto supports running code in EC2 instances. However, we're working on adding more infrastructure types (e.g., EMR clusters) and cloud providers (GCP, Azure, etc.)\n  Target Audience\n    For data teams, building and maintaining infrastructure is difficult. It's also not exactly within their wheelhouse. I built Alto for data analysts / data scientists (like myself) who wish to run data workloads in the cloud without having to deal with provisioning the infrastructure to do so.\n  \n    Alto is an early-stage project, so use in production at your own risk.\n  Comparison\n    Alto allows users to perform serverless data execution, similar to AWS Lambda. There are a few features that make Alto distinct:\n  \n\n\n    Code format. Alto supports running functions, scripts, and Jupyter notebooks, whereas Lambda is primarily used for executing functions.\n  \n\n\n    Local development. Alto allows users to run their code in the cloud as is. In other words, users can develop and test their code on their local machine and then deploy to the cloud when they're ready. They could also choose to develop in the cloud itself."
},
{
    "title": "No title",
    "content": "Found a cool resource which explains the CLI tools hidden in the Python Standard Library.\n  \n    Link : https://til.simonwillison.net/python/stdlib-cli-tools"
},
{
    "title": "No title",
    "content": "Hi everyone, I have released a beta version of ADIX - a new Python package for Data Scientists and anyone interested in data analysis.What My Project DoesADIX works in Jupyter notebooks or Google colab and is specifically focused on streamlining the exploratory data analysis process, making it incredibly easy and fast to gain insights from your dataset with just one command: ix.eda(). This allows you to quickly understand your data and be more productive in your research.Target AudienceData scientists, ML engineers and anyone interested in data analysis / exploration.PyPi : https://pypi.org/project/adix/\n\n    I'd greatly appreciate any feedback you may have. And if you like the project please consider giving a star on GitHub. You can find the documentation here.\n  \nMain features🎨 Customizable Themes: Personalize your analysis environment with your own unique style using customizable color schemes! \n  \n    ⚡ Efficient Cache Utilization: Say goodbye to long loading times! ADIX leverages optimized caching mechanisms for lightning-fast performance.\n  \n     🔍 Rapid Data Insight: ADIX prioritizes speed, delivering crucial insights at your fingertips for quick decision-making. \n  \n    🔢 Automatic Type Detection: Let ADIX do the heavy lifting! It automatically detects numerical, categorical, and text features, with the option for manual overrides when needed.\n  \n    📊 Statistically Rich Summary Information: Unveil the intricate details of your data with comprehensive summaries, including type identification, unique values, missing values, and more. Dive deep into numerical data with properties like min-max range, quartiles, and skewness. \n  \n    📈 Univariate and Bivariate Statistics Unveiled: Explore univariate and bivariate insights with ADIX's versatile visualization options. From bar charts to matrices, and box plots, uncover a multitude of ways to interpret and analyze your data effectively.\n  \nComparisonAutomated data exploration is a popular topic and there are some alternatives that work similarly, for example sweetviz or lux."
},
{
    "title": "No title",
    "content": "What My Project Does: It's a lightweight and fun Periodic Table of Elements app for the console. Supports Browse and Search for all 118 elements, and 20 Display Modes.  Interact with the periodic table like never before as you command the elements with just a few keystrokes the cool way, in the console.  A few months ago, I built this app for NodeJS. After receiving lots of positive reviews and feedback, I decided to rebuild it again for Python!\n  \nPYPI: periodic-table-cli\n  \n    The controls are the same. Use Arrows for navigation. Use Slash/BackSlash to toggle 20 display modes. Use Letters/Numbers to search and Enter to select. Use ESC or CTRL+C to quit.\n  \n    Just make sure to have Python 3.8+ installed and use a terminal that supports 256 colors. Tested on MacOS (Terminal + iTerm2), Ubuntu (Terminal + XTerm), and Windows Command Prompt (requires windows-curses).\n  \nTarget Audience: Scientists, chemists, developers, terminal enthusiasts, and anyone who just likes cool tools.\n  \nComparison: It's not the first time this idea has been done, but this one has more browse features, display modes, and an overall cool minimalist design.\n  \n    More directions available in the links below. Enjoy!\n  \nHomepage: https://spirometaxas.com/projects/periodic-table-cli\n\nPYPI Page: https://pypi.org/project/periodic-table-cli/\n\nSource Code: https://github.com/spirometaxas/periodic-table-cli-py"
},
{
    "title": "No title",
    "content": "What My Project Does: A library and CLI that reads a stream of data and generates Python types describing it. See below for a detailed description and an example.\n  \nTarget Audience: Primarily web developers, but also anyone who works with structured data. Stability-wise, it is in beta, but fairly well-tested and I haven't encountered major problems using it regularly for the past year.\n  \nComparison: There are libraries that do the same, but output JSON Schema (e.g. genson). To the best of my knowledge, no other library generates Python types directly from data.\n  \n    A while ago I found myself working on a large distributed backend codebase with almost no types / schemas available for inter-service RPCs. Everything was just pure dicts! I hated it, so I decided to make a \"type learner\" that would consume a stream of data and generate Python types. Initially I built it for myself and it may reflect my specific use-case, but I hope that I was able to make it quite generic so it can be useful for others.\n  \n    For example, let's generate types for GitHub REST API\n  # fetch GitHub releases list\ngh api \\\n  -H \"Accept: application/vnd.github+json\" \\\n  -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n  /repos/facebook/react/releases --paginate |> data.json\n\nslow-learner learn --spread --type-name Release data.json\n    The result is\n  \"\"\"\nThis file contains Python 3.8+ type definitions generated by TypeLearner from 99 observed value(s)\n\nSource JSON files:\n- /Users/njvh/Documents/Personal/slow-learner/data.json\n\"\"\"\n\nfrom typing import List\nfrom typing import Literal\nfrom typing_extensions import NotRequired\nfrom typing import Optional\nfrom typing import TypedDict\nfrom typing import Union\n\n\nclass ReleaseAuthor(TypedDict):\n    login: str\n    id: int\n    node_id: str\n    avatar_url: str\n    gravatar_id: Literal[\"\"]\n    url: str\n    html_url: str\n    followers_url: str\n    following_url: str\n    gists_url: str\n    starred_url: str\n    subscriptions_url: str\n    organizations_url: str\n    repos_url: str\n    events_url: str\n    received_events_url: str\n    type: Literal[\"User\"]\n    site_admin: Literal[False]\n\n\nclass ReleaseAssetsItemUploader(TypedDict):\n    login: str\n    id: int\n    node_id: str\n    avatar_url: str\n    gravatar_id: Literal[\"\"]\n    url: str\n    html_url: str\n    followers_url: str\n    following_url: str\n    gists_url: str\n    starred_url: str\n    subscriptions_url: str\n    organizations_url: str\n    repos_url: str\n    events_url: str\n    received_events_url: str\n    type: Literal[\"User\"]\n    site_admin: Literal[False]\n\n\nclass ReleaseAssetsItem(TypedDict):\n    url: str\n    id: int\n    node_id: str\n    name: str\n    label: Optional[str]\n    uploader: ReleaseAssetsItemUploader\n    content_type: Union[\n        Literal[\"text/javascript\"],\n        Literal[\"application/javascript\"],\n        Literal[\"application/x-javascript\"],\n        Literal[\"application/zip\"],\n    ]\n    state: Literal[\"uploaded\"]\n    size: int\n    download_count: int\n    created_at: str\n    updated_at: str\n    browser_download_url: str\n\n\nReleaseReactions = TypedDict(\n    \"ReleaseReactions\",\n    {\n        \"url\": str,\n        \"total_count\": int,\n        \"+1\": int,\n        \"-1\": Literal[0],\n        \"laugh\": int,\n        \"hooray\": int,\n        \"confused\": Literal[0],\n        \"heart\": int,\n        \"rocket\": int,\n        \"eyes\": int,\n    },\n)\n\n\nclass Release(TypedDict):\n    url: str\n    assets_url: str\n    upload_url: str\n    html_url: str\n    id: int\n    author: ReleaseAuthor\n    node_id: str\n    tag_name: str\n    target_commitish: str\n    name: str\n    draft: Literal[False]\n    prerelease: bool\n    created_at: str\n    published_at: str\n    assets: List[ReleaseAssetsItem]\n    tarball_url: str\n    zipball_url: str\n    body: str\n    reactions: NotRequired[ReleaseReactions]\n\nGithub | PyPI | Read more"
},
{
    "title": "No title",
    "content": "What My Project Does\n    Ever wanted to have a unittest compare results to reference data from a text file, and fail if they don't match? So did we! We have just opened sourced our Python package that extends Python unittest to enable comparison to data from a reference text file containing data.\n  Target Audience\n    Python developers, data scientists, looking to include file comparison in their unit testing.\n  Comparison\n    Not the first time this has been done. This just packages functionality into an easy to install and use Python package.\n  Clone the projectgit clone https://github.com/amentumspace/file_unittestInstallation\n    The package can be installed into any active environment by running:\n  cd file_unittest/   \npip install .file_unittest.TestCase\n    This class extends the unittest.TestCase class to allow the user to run unit tests by writing data to a file and verifying the output has not changed since the last time the unit test was run.\n  \n    Once the user is satisfied that the output data is correct, these unit test simply ensure that the data does not change with changes in the code base.\n  Usage\n    Rather than inherit from unittest.TestCase the user should derive a class from file_unittest.TestCase.\n  \n    Like the normal unit test, individual test functions are appended with test_.\n  \n    Any desired output data is written to file by calling self.output\n\n    Example unit test file:\n  import unittest\nimport file_unittest\n\nclass MyTest(file_unittest.TestCase):\n    def test_case(self):\n        self.output(\"hello\")\n\nif __name__ == '__main__':\n    unittest.main()\n\n\n    Unlike the normal unittest.TestCase the user does not need to make any assertion calls eg. self.assertTrue, self.assertFalse etc.\n  \n\n\n    This class will take care of warning about any differences in output.\n  \n\nDefault output location\n    The default location to output the test results will be:\n      {derived_class_filepath}/test_results/\n      {derived_class_filename}.{derived_class_name}.{test_name}.txtMissing or different results\n    If the expected output file is missing, or differences are detected, the output data will be written to the same file, but with .new postfix:\n      {derived_class_filepath}/test_results/\n      {derived_class_filename}.{derived_class_name}.{test_name}.new\n\n\n    On the first run, the user will need to inspect the .new file for expected results;\n  \n\n\n    Otherwise, if differences are detected, the user can diff the .txt and .new files to investigate and resolve any differences;\n  \n\n\n    In both cases once the user is satisfied with the results, the .new files can be renamed with .txt  extension and the .txt files can be checked into the git repo\n  \n\nCommitting test result files to source control\n    Once the output .txt files have been satisfactorily generated as in the previous step, they should be checked into source control so that they can be used as the benchmark for future runs."
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    I just dropped my first open source project for python on github! Its a speeech recognition program written 100% in python. I made it to help me while I play star citizens by rebinding the keys too speech recognition commands, that way I dont need to remember over 100 keybinds or accidently click the wrong one.\n  \nComparison (A brief comparison explaining how it differs from existing alternatives.)\n\n    Some features that I have not seen in other similar programs. Such as A topmost/ Picture in picture Browser function so that users can simply call a voice command and it will open a Picture in picture browser over top of the game, that they can interact with and use so they do not have to switch to desktop open a browser type in the url, and the switch back. They simply can interact with a browser on top of their game. and close it without ever having to leave i think its pretty neat idea.\n  \nTarget Audience (e.g., Is it meant for production, just a toy project, etc.\n\n    So far im pretty pleased, and would like anyone thats intersted in checking out other peoples side/hobby projects to check it out! Ive gone through tons of offline speech recognition librarys like vosk and sphinx but none of them can compete in the terms of speed, and accuracy like the windows speech recognition uses. Im using a python libary called Dragonfly2. So far it is working great on windows 10 and windows 11 computers. Its extremely fast from voice command to keypress. and im pretty happy so far with my side project and the results!\n  \n    I figured I would post it here and see what everyone here thinks about it so far? This is my first time releaseing a python script I've created, Ive made a few small things here and there with python but never released anything so I decided I might as well start\n  \nhttps://github.com/techsavvy42/Captains_Voice_Commander"
},
{
    "title": "No title",
    "content": "I've finally implemented my \"ideal state\" for Python release management on GitHub. Focused around these three tenets:\n  \n\n\nMust be idiot proof, meaning: all checks are automated and operations work is automated.\n  \n\n\nMust be runnable from a mobile browser. If your publish operation requires a laptop, you are probably doing it wrong.\n  \n\n\nMust decouple marketing work from engineering work, while not duplicating efforts.\n  \n\n\n    And here's how I got there:\n  \n\n\nSemantic pull request titles. Basically, take a conventional commits approach but apply them to your PR titles (which are trivial to edit+groom) instead of commit history (which is frustrating to edit+groom).\n  \n\n\nNo changelogs. If you use Semantic PRs, and if you default to those PR titles as your commit message when merging, then your main branch commit history is your changelog.\n  \n\n\nNo version bumps. Instead use dynamic versioning, such as provided by the poetry-dynamic-versioning plugin.\n  \n\n\nAutomatically-drafted release notes. If we've followed all the above, then a machine can create and continually edit a release notes draft (including calculating the correct version bump and the ability to generate and categorize changelog entries), which means at any moment we are ready to...\n  \n\n\nOne-click publish. Optionally groom before you do, but if you've followed the above four steps, it's just a single click to \"publish\" the release in GitHub. A GitHub Action can listen for the new release tag and respond by publishing to PyPi.\n  \n\n\nMy tools:\n\n\n\n    Semantic PR checker: https://github.com/zeke/semantic-pull-requests\n\n\n\n    Poetry dynamic versions plugin: https://github.com/mtkennerly/poetry-dynamic-versioning\n\n\n\n    Release drafter: https://github.com/marketplace/actions/release-drafter\n\n\n\nLet me know what you think! Would you adopt this? What would you change?"
},
{
    "title": "No title",
    "content": "Since threads in Python share the memory space of their parent process, we might need to define thread-specific variables for specific use cases to avoid unintended side effects.\n  \n    In this post, we will:\n  \n\n\n    Explore Thread local storage: Python threading module's solution for thread-specific/thread-private values.\n  \n\n\n    See its real-world example from Open-Source ( usage in the Peewee ORM library).\n  \n\n\n    Look at the CPython source code to see how thread local storage is implemented under the hood.\n  \n\nBackground\n    Threads share the memory space of their parent process. This will allow us to seamlessly access and share variables, data structures, etc., across threads. But this comes with its own challenges. There may be scenarios where we need to isolate variables and might need to store data specific to each thread. Thread local storage can be leveraged in this case.\n  \n    We can use local() found in the threading module to define thread-local variables.\n  import threading\nimport time\n\n# Create a thread-local storage object (1)\nthread_local = threading.local()\n\n\ndef init_data(number):\n    thread_local.number = number * 100\n\n\ndef show_data():\n    print(f\"Thread {threading.current_thread().name} has number {thread_local.number}\")\n\n\ndef worker(number):\n    init_data(number)\n\n    for _ in range(3):\n        time.sleep(1)\n        show_data()\n\n\nthread1 = threading.Thread(target=worker, name=\"A\", kwargs={\"number\": 1})\nthread2 = threading.Thread(target=worker, name=\"B\", kwargs={\"number\": 2})\n\nthread1.start()\nthread2.start()\n\nthread1.join()\nthread2.join()\n    The program output will be:\n  Thread A has number 100\nThread B has number 200\nThread A has number 100\nThread B has number 200\nThread A has number 100\nThread B has number 200\n    As seen in comment #(1), a thread-local storage object was created and assigned to the variable thread_local. Arbitrary attributes can be assigned to this variable, which are specific to the thread that performs the assignment and is isolated from others.\n  \n    In the example, each thread stores its own number attribute to the thread_local object and accesses the thread-specific value during their concurrent execution.\n  Usage in the Wild\n    Peewee, a Python ORM, utilizes thread-local data in its ThreadSafeDatabaseMetadata to support dynamic database switches at runtime in multithreaded applications.\n  \n    The source code can be found here.\n# File: peewee/playhouse/shortcuts.py\n\nclass ThreadSafeDatabaseMetadata(Metadata):\n    \"\"\"\n    Metadata class to allow swapping database at run-time in a multi-threaded\n    application. To use:\n\n    class Base(Model):\n        class Meta:\n            model_metadata_class = ThreadSafeDatabaseMetadata\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        # The database attribute is stored in a thread-local.\n        self._database = None\n        self._local = threading.local()\n        super(ThreadSafeDatabaseMetadata, self).__init__(*args, **kwargs)\n\n    def _get_db(self):\n        return getattr(self._local, \"database\", self._database)\n\n    def _set_db(self, db):\n        if self._database is None:\n            self._database = db\n        self._local.database = db\n\n    database = property(_get_db, _set_db)\n    In multithreaded applications using peewee ORM, database switching at runtime without using ThreadSafeDatabaseMetada can lead to errors. If multiple threads work in parallel and one thread changes the connection parameters, this can lead to errors such as writing to a wrong DB, inconsistent writes (in case of non-atomic DB operations), etc.\n  \nThreadSafeDatabaseMetada solves this by keeping the database attributes in a thread-local object (self._local). In this way, dynamic changes to the database will only affect the thread that made the change. Other threads will keep working with their existing databases.\n  Should I use it?\n    Thread-local storage should be used when:\n  \n    You are writing a multi-threaded application (obviously) and:\n  \n\n\n    If some variables are used by the current thread only and are not relevant to the main thread/other threads.\n  \n\n\n    You find out that changes made by one thread can lead to unintended side effects in other concurrent threads.\n  \n\n\n    Generally, if you are working with multiple threads and there is shared mutable data:\n  \n    You should check if sharing these data across threads is actually needed.\n  \n\n\n    If sharing can be avoided, use thread-local storage to make the data specific to each thread.\n  \n\n\n    Otherwise, implement locks or other synchronization primitives to enforce thread safety.\n  \n\n\nNote: Context variables from the contextvars standard library module can be used as an alternative to threading.local(). They work with multithreading as well as asyncio to store context-specific information. In the case of asyncio programs, context variables allow each coroutine task to have its own set of variables isolated from other (asyncio) tasks.\n  Behind the scenes\n    The Python implementation of threading.local can be found in the /Lib/_threading_local.py path in CPython source code.\n\n    The _localimpl class is used to store thread-local values.\n  class _localimpl:\n    \"\"\"A class managing thread-local dicts\"\"\"\n    __slots__ = 'key', 'dicts', 'localargs', 'locallock', '__weakref__'\n\n    def __init__(self):\n        # The key used in the Thread objects' attribute dicts.\n        # We keep it a string for speed but make it unlikely to clash with\n        # a \"real\" attribute.\n        self.key = '_threading_local._localimpl.' + str(id(self))\n        # { id(Thread) -> (ref(Thread), thread-local dict) }\n        self.dicts = {}\n    The dicts attribute maps the id of a thread to a tuple. The tuple is two-membered, containing a reference to the thread and the actual dictionary storing thread local values (# { id(Thread) -> (ref(Thread), thread-local dict) }).\n  \n    Looking at a few other methods of the class:\n  class _localimpl:\n    \"\"\"A class managing thread-local dicts\"\"\"\n    def get_dict(self):\n        \"\"\"Return the dict for the current thread. Raises KeyError if none\n        defined.\"\"\"\n        thread = current_thread()\n        return self.dicts[id(thread)][1] # (1) returning the local dict of current thread\n\n    def create_dict(self):\n        \"\"\"Create a new dict for the current thread, and return it.\"\"\"\n        localdict = {}\n        key = self.key\n        thread = current_thread()\n        idt = id(thread)\n        ...\n        wrthread = ref(thread, thread_deleted)\n        thread.__dict__[key] = wrlocal\n        self.dicts[idt] = wrthread, localdict  # (2) Populating the `dicts` with a new thread\n        return localdict\n    The comments #(1) & #(2) illustrate operations on the dicts attribute discussed previously.\n  \n(1): Returns the local data dictionary corresponding to the current thread accessing it.\n  \n(2): This part initializes the local dict for a new thread.\n  \n    Then, we have the actual local callable, which we call as threading.local() to initialize a thread-local object.\n  @contextmanager\ndef _patch(self):\n    impl = object.__getattribute__(self, '_local__impl')\n    try:\n        dct = impl.get_dict()  # (3) this will return local dict of current thread -/\n                               # see its definition in the above snippet\n    except KeyError:\n        ...\n        # calls _localimpl`s create_dict to create & init a new dict\n    with impl.locallock:\n        object.__setattr__(self, '__dict__', dct) # (4) <The magic> Temporarily replaces -/\n        # the instance's __dict__ attribute with the thread-specific dictionary.\n        \n        yield\n\n\nclass local:\n    __slots__ = '_local__impl', '__dict__'\n\n    def __new__(cls, /, *args, **kw):\n        ...\n\n        impl = _localimpl()  # (1) - wraps the _localimpl object in an attribute\n        impl.localargs = (args, kw)\n        impl.locallock = RLock()  # (2) - Lock for thread safety\n        object.__setattr__(self, '_local__impl', impl)\n        # We need to create the thread dict in anticipation of\n        # __init__ being called, to make sure we don't call it\n        # again ourselves.\n        impl.create_dict()\n        return self\n\n    def __getattribute__(self, name):\n        with _patch(self):\n            return object.__getattribute__(self, name)\n\n    def __setattr__(self, name, value):\n        if name == '__dict__':\n            raise AttributeError(\n                \"%r object attribute '__dict__' is read-only\"\n                % self.__class__.__name__)\n        with _patch(self):\n            return object.__setattr__(self, name, value)\n\n    ...\n    Let's look at various parts of the code one by one.\n  \n# (1):  An object of _localimpl class is created and later stored in _local__impl attribute\n  \n# (2): RLock() is used on dict operations - Concurrent writes by multiple threads may cause 'lost writes' since dict is not thread-safe.\n  \n# (3): In the try/except block, the current thread's local data is fetched and assigned to the variable named dct.\n  \n# (4): The magic happens here. Here is a quick refresher on class attributes before we delve further:\n  \n\n    A class has a namespace implemented by a dictionary object. Class attribute references are translated to lookups in this dictionary, e.g., C.x is translated to C.dict[\"x\"] (although there are a number of hooks which allow for other means of locating attributes).\n  \n\n    The _patch function we are currently in is a context manager. Using the line marked as # (4), it patches the namespace dictionary of the local class with the dot variable (which stores the current-thread specific data).\n  \n    Since the __getattribute__ and __setattr__ dunders of the class local use the _patch context manager, attribute access performed inside the context will use the thread-local dictionary (dct) replacing the class's actual namespace dictionary.\n  \nNote: The Lib/_threading_local.py starts with the below note:\n  \n\n    Note that this module provides a Python version of the threading.local class. Depending on the version of Python you're using, there may be a faster one available. You should always import the local class from threading.\n  \n\n    The code we looked at might not be the one running in our Python installations. I think newer Pythons versions are using C implementations of the thread local functionality for efficiency.\n  \n    ...\n  \n    I share interesting Python snippets 🐍 like this from open-source projects illustrating Python language features in my newsletter, \"Python in the Wild\".\n  \n    Subscribe to the newsletter on Substack or Linkedin to receive new Pythonic posts to your email 💌🚀."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Happy weekend every one!\n  \n    I have recently created my first python package.\n  What My Project Does\n    The package helps with extracting financial data of companies from Yahoo into pandas dataframes using companies stocks symbol(like yfinance package)\n  Target Audience\n    The package is targeted at people who are interested in financial data of companies and want to use it for analysis or other purposes.\n  \n    Feel free to give you opinion or open issues on the Github project:\n  \nGithub Repo Link\n\nPypi link"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hi there!\n  \n    I'm working in ugropy, a Python library for molecule fragmentation to get the UNIFAC/Joback subgroups from the molecule's name or SMILES.\n  \nGitHubPyPi\n\nWhat My Project Does\n\n    By now ugropy can detect the subgroups for classic liquid-vapor UNIFAC, Predictive Soave-Redlich-Kwong (PSRK), and Joback (and estimate properties).\n  \nTarget Audience\n\n    Chemical engineers or scientists that work in thermodynamics or process simulation.\n  \nExample of use\nfrom ugropy import Groups\n\n\nhexane = Groups(\"hexane\")\n\nprint(hexane.unifac_groups)\nprint(hexane.psrk_groups)\nprint(hexane.joback.groups)\n    You will obtain:\n  {'CH3': 2, 'CH2': 4}\n{'CH3': 2, 'CH2': 4}\n{'-CH3': 2, '-CH2-': 4}\n    Any help and recommendations are welcome! I hope that the project may be helpful to someone :)"
},
{
    "title": "No title",
    "content": "Last time I showed how to count how many CPU instructions it takes to print(\"Hello\") and import seaborn.\n  \n    Here's a new post on how to record and visualise system calls that your Python code makes.\n  \n    Spoiler: 1 for print(\"Hello\"), about 20k for import seaborn, including an execve for lscpu!"
},
{
    "title": "No title",
    "content": "Hi,\n  \n    🎨 Discover how easy it is to transform your own phots into beautiful paintings\n  \n    🖼️ This is a cool effect based on Stylized Neural Painting library. Simple to use , and the outcome is impressive,\n  \n    You can find instructions here : https://github.com/feitgemel/Python-Code-Cool-Stuff/tree/master/How%20to%20make%20photos%20look%20like%20paintings\n\n    The link for the tutorial video :  https://youtu.be/m1QhxOWeeRc\n\n    Enjoy\n  \n    Eran\n  \n    #convertphototodigitalart #makephotolooklikepainting #makephotoslooklikepaintings #makepicturelooklikepainting"
},
{
    "title": "No title",
    "content": "From the makers of ruff comes uv\n\n\n    TL;DR: uv is an extremely fast Python package installer and resolver, written in Rust, and designed as a drop-in replacement for pip and pip-tools workflows.\n  \n\n    It is also capable of replacing virtualenv.\n  \n    With this announcement, the rye project and package management solution created by u/mitsuhiko (creator of Flask, minijinja, and so much more) in Rust, will be maintained by the astral team.\n  \n    This \"merger\" and announcement is all working toward the goal of a Cargo-type project and package management experience, but for Python.\n  \n    For those of you who have big problems with the state of Python's package and project management, this is a great set of announcements...\n  \n    For everyone else, there is https://xkcd.com/927/.\n  \n\n\nTwitter Announcement\n\n\n\nPyPI\n\n\n\nGitHub\n\n\n\n    Install it today:\n  pip install uv\n# or\npipx install uv\n# or\ncurl -LsSf https://astral.sh/uv/install.sh | sh"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hi people, after teaching Python applied to Data Science more than 800 people (the majority 1:1) and producing many courses, I’ve decided to put them on YouTube.\n  \n    They are mostly applied practical cases. For example, this tutorial to preprocess and visualize solar energy generation (Time Series).\n  \n    On top of that, I’m hosting a weekly live streaming to solve doubts on the tutorials I publish.\n  \n    If you are interested in stopping by and get some doubts solved, feel free to join here."
},
{
    "title": "No title",
    "content": "A GitHub repository of Python Tutorials in markdown and notebook format:\n  \nhttps://github.com/PhilipYip1988/python-notebooks\n\n    Covering:\n  \n\n\n    Python Installation using Anaconda/Miniconda\n  \n\n\n    Python Environments\n  \n\n\n    Markdown and TeX Syntax\n  \n\n\n    OOP Fundamentals and classes in the Builtins Module\n  \n\n\n    IPython Magics\n  \n\n\n    Formatters (AutoPEP8, ISort, Black and Ruff)\n  \n\n\n    Code Blocks and Comprehensions\n  \n\n\n    Collections Module\n  \n\n\n    Itertools Module\n  \n\n\n    Math Module\n  \n\n\n    Statistics Module\n  \n\n\n    Random Module\n  \n\n\n    Datetime Module\n  \n\n\n    File Formats (IO, CSV, Pickle and Shelve Modules)\n  \n\n\n    OS Module\n  \n\n\n    PathLib Module\n  \n\n\n    System Module\n  \n\n\n    NumPy Library\n  \n\n\n    Pandas Library\n  \n\n\n    Matplotlib Library\n  \n\n\n    Seaborn Library\n  \n\n\n    Plotly Library\n  \n\n\n    Pillow Library\n  \n\n\n    The tutorials are mainly focused on JupyterLab 4 but also mention how to setup VSCode.\n  \n    These are fairly detailed tutorials and it took me a long time to put together, hopefully it will help some beginners, particularly those looking to use Python for datascience."
},
{
    "title": "No title",
    "content": "I'm excited to announce the beta release of BlackMarblePy - a new Python package designed to retrieve NASA Black Marble data. For those unfamiliar, NASA Black Marble imagery provides stunning views of Earth at night, capturing the lights from cities and other human activity.\n  \n    This package aims to make accessing this data easier for researchers, developers, and anyone interested in exploring our planet's nighttime lights. Whether you're studying urbanization, monitoring light pollution, or simply fascinated by Earth's beauty after dark, this package is for you.\n  \n    Key Features:\n  \n\n\n    Simple Python interface for accessing NASA Black Marble data.\n  \n\n\n    Download daily, monthly, and yearly nighttime lights data for user-specified region of interest and time.\n  \n\n\n    Parallel downloading for faster data retrieval and automatic retry mechanism for handling network errors.\n  \n\n\n    Access NASA Black Marble as a Xarray Dataset\n  \n\n\n    Comprehensive documentation and examples to get you started quickly.\n  \n\n\n    How You Can Help:\n  \n    I'm reaching out to the community to gather feedback and suggestions for improvement. Whether you encounter any bugs, have ideas for additional features, or just want to share your experience using the package, your input is invaluable.\n  \n    Blog post: https://blogs.worldbank.org/opendata/illuminating-insights-harnessing-nasas-black-marble-r-and-python-packages\n\n    Repository: https://github.com/worldbank/blackmarblepy"
},
{
    "title": "No title",
    "content": "Hi there, I hope you guys are doing well. I have some amazing news to share with you! I have created a new python library called better_bing_image_downloader.\n  \n    It is a powerful and convenient tool to download bulk images from Bing without any API keys. It is compatible with python 3.x and above and it is hosted on both pip and github.\n  \n    You can install it easily with pip install better-bing-image-downloader.\n  \n    The README.md file on github gives you a comprehensive idea of how to use this package to its fullest potential in any project that requires web image scraping.\n  \n    You can check it out here:  Pypi Link or  GitHub Repo .\n  \n    I hope this library will be helpful for your data and machine learning projects. I would love to hear your feedback and suggestions!"
},
{
    "title": "No title",
    "content": "Hi there!\n  \nWhat My Project Does\n\n    This is a simple, yet powerful, script in Python (3.8 - 3.12+ without python deps) for which packs existent docker-compose project in self-executable encrypted archive.To restore, you don't need to install DKP or remember the details: just run archive as a script (help included!).\n  \n    It can backup images, mounted files, directories, volumes, as well as project env files.\n  \n    Can be used as CLI or as library.\n  \nTarget Audience\n\n    Created to help other people, since I believe it's not only me who setups once backup routine and after a years forgot how to recover.\n  \n    Primarily for sysadmins, for self-hosters, for SMB (for large corporation it's probably not very useful).\n  \nComparison\n\n    The closest possible solution:\n  \n\n\n    restic - is not compose-aware\n  \n\n\nInstallation:\n  pip install dkp\nSource: https://github.com/reddec/dkp\n\nUsage:\nusage: dkp [-h] [--output OUTPUT] [--skip-images] [--passphrase PASSPHRASE] [project]\n    \nDocker Compose packer - pack compose project with all batteries included\n    \npositional arguments:\n  project               Compose project name\n    \noptions:\n  -h, --help            show this help message and exit\n  --output OUTPUT, -o OUTPUT\n                        Output file\n  --skip-images, -S     Do not archive images\n  --passphrase PASSPHRASE, -p PASSPHRASE\n                        Passphrase to encrypt backup. Can be set via env PASSPHRASE"
},
{
    "title": "No title",
    "content": "👋 Greetings and Salutations all!\n  \n    I wanted to share a new way of embedding interactive Python code cells into websites, slides, and books through the {quarto-pyodide} extension for Quarto\n\n\n\n    See the documentation: https://quarto.thecoatlessprofessor.com/pyodide\n\n\n\n    Check it out on GitHub: https://github.com/coatless-quarto/pyodide\n\n\nProject Overview\n    This project introduces a new custom code cell engine for Quarto that allows for interactive Python cells into HTML documents. Leveraging the capabilities of Pyodide's WebAssembly distribution for Python, the Python code is run directly in the user's web browser. Moreover, the custom engine removes the need for any expertise in HTML or JavaScript on the author's side.\n  Target Audience\n    The primary audience for this project includes instructors teaching Python who wish to simplify the setup process for their students, making it ideal for introductory Computer Science and Data Science courses. Additionally, it offers a swift and reproducible way for data scientists and others interested in Python scripts to analyze data interactively.\n  \nNote: While this provides a user-friendly approach, it does not eliminate the eventual need for exposure to more comprehensive IDEs like VS Code, PyCharm, or even Posit Workbench.\n  Comparison\n    Compared to existing approaches like JupyterLite, PyScript, and Thebe, the {quarto-pyodide} extension adopts a more user-friendly stance by abstracting away complex interfaces and setup for both visitors to the web page and authors.\n  Demos\n\n\n    README:\n  \n\n\n    Live: https://quarto.thecoatlessprofessor.com/pyodide/examples/readme/\n\n\n\n    Source: https://github.com/coatless-quarto/pyodide/tree/main/examples/readme\n\n\n\n\n\n    RevealJS:\n  \n\n\n    Live: https://quarto.thecoatlessprofessor.com/pyodide/examples/revealjs/#/title-slide\n\n\n\n    Source: https://github.com/coatless-quarto/pyodide/tree/main/examples/revealjs\n\n\n\n\n\n    Blog:\n  \n\n\n    Live: https://quarto.thecoatlessprofessor.com/pyodide/examples/blog/\n\n\n\n    Source: https://github.com/coatless-quarto/pyodide/tree/main/examples/blog"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "For those who work with Qt, I'm sure some of you have seen Megasolid Idiom, a simple, rich-text editor. However, it's written with PyQt5.\n  \n    If you're learning graphics programming in Python, that's disappointing since PyQt6 has been around since January of 2021 and appears to be stable. I wanted that editor in PyQt6, so I updated it and shared it on github. It has an MIT license, like the original."
},
{
    "title": "No title",
    "content": "Hey everyone! I want to share with everyone the project I have been working on called Alto. It's a Python CLI tool to run functions, scripts, and Jupyter notebooks in the cloud.\n  \n\n\n    Github: https://github.com/runprism/alto\n\n\n\n    Documentation: https://alto.mintlify.app/welcome/v0.0.8/welcome\n\n\nWhat My Project Does\n    Alto allows data practitioners to effortlessly run functions, scripts, and Jupyter notebooks in the cloud. Currently, Alto supports running code in EC2 instances. However, we're working on adding more infrastructure types (e.g., EMR clusters) and cloud providers (GCP, Azure, etc.)\n  Target Audience\n    For data teams, building and maintaining infrastructure is difficult. It's also not exactly within their wheelhouse. I built Alto for data analysts / data scientists (like myself) who wish to run data workloads in the cloud without having to deal with provisioning the infrastructure to do so.\n  \n    Alto is an early-stage project, so use in production at your own risk.\n  Comparison\n    Alto allows users to perform serverless data execution, similar to AWS Lambda. There are a few features that make Alto distinct:\n  \n\n\n    Code format. Alto supports running functions, scripts, and Jupyter notebooks, whereas Lambda is primarily used for executing functions.\n  \n\n\n    Local development. Alto allows users to run their code in the cloud as is. In other words, users can develop and test their code on their local machine and then deploy to the cloud when they're ready. They could also choose to develop in the cloud itself."
},
{
    "title": "No title",
    "content": "Found a cool resource which explains the CLI tools hidden in the Python Standard Library.\n  \n    Link : https://til.simonwillison.net/python/stdlib-cli-tools"
},
{
    "title": "No title",
    "content": "Hi everyone, I have released a beta version of ADIX - a new Python package for Data Scientists and anyone interested in data analysis.What My Project DoesADIX works in Jupyter notebooks or Google colab and is specifically focused on streamlining the exploratory data analysis process, making it incredibly easy and fast to gain insights from your dataset with just one command: ix.eda(). This allows you to quickly understand your data and be more productive in your research.Target AudienceData scientists, ML engineers and anyone interested in data analysis / exploration.PyPi : https://pypi.org/project/adix/\n\n    I'd greatly appreciate any feedback you may have. And if you like the project please consider giving a star on GitHub. You can find the documentation here.\n  \nMain features🎨 Customizable Themes: Personalize your analysis environment with your own unique style using customizable color schemes! \n  \n    ⚡ Efficient Cache Utilization: Say goodbye to long loading times! ADIX leverages optimized caching mechanisms for lightning-fast performance.\n  \n     🔍 Rapid Data Insight: ADIX prioritizes speed, delivering crucial insights at your fingertips for quick decision-making. \n  \n    🔢 Automatic Type Detection: Let ADIX do the heavy lifting! It automatically detects numerical, categorical, and text features, with the option for manual overrides when needed.\n  \n    📊 Statistically Rich Summary Information: Unveil the intricate details of your data with comprehensive summaries, including type identification, unique values, missing values, and more. Dive deep into numerical data with properties like min-max range, quartiles, and skewness. \n  \n    📈 Univariate and Bivariate Statistics Unveiled: Explore univariate and bivariate insights with ADIX's versatile visualization options. From bar charts to matrices, and box plots, uncover a multitude of ways to interpret and analyze your data effectively.\n  \nComparisonAutomated data exploration is a popular topic and there are some alternatives that work similarly, for example sweetviz or lux."
},
{
    "title": "No title",
    "content": "What My Project Does: It's a lightweight and fun Periodic Table of Elements app for the console. Supports Browse and Search for all 118 elements, and 20 Display Modes.  Interact with the periodic table like never before as you command the elements with just a few keystrokes the cool way, in the console.  A few months ago, I built this app for NodeJS. After receiving lots of positive reviews and feedback, I decided to rebuild it again for Python!\n  \nPYPI: periodic-table-cli\n  \n    The controls are the same. Use Arrows for navigation. Use Slash/BackSlash to toggle 20 display modes. Use Letters/Numbers to search and Enter to select. Use ESC or CTRL+C to quit.\n  \n    Just make sure to have Python 3.8+ installed and use a terminal that supports 256 colors. Tested on MacOS (Terminal + iTerm2), Ubuntu (Terminal + XTerm), and Windows Command Prompt (requires windows-curses).\n  \nTarget Audience: Scientists, chemists, developers, terminal enthusiasts, and anyone who just likes cool tools.\n  \nComparison: It's not the first time this idea has been done, but this one has more browse features, display modes, and an overall cool minimalist design.\n  \n    More directions available in the links below. Enjoy!\n  \nHomepage: https://spirometaxas.com/projects/periodic-table-cli\n\nPYPI Page: https://pypi.org/project/periodic-table-cli/\n\nSource Code: https://github.com/spirometaxas/periodic-table-cli-py"
},
{
    "title": "No title",
    "content": "What My Project Does: A library and CLI that reads a stream of data and generates Python types describing it. See below for a detailed description and an example.\n  \nTarget Audience: Primarily web developers, but also anyone who works with structured data. Stability-wise, it is in beta, but fairly well-tested and I haven't encountered major problems using it regularly for the past year.\n  \nComparison: There are libraries that do the same, but output JSON Schema (e.g. genson). To the best of my knowledge, no other library generates Python types directly from data.\n  \n    A while ago I found myself working on a large distributed backend codebase with almost no types / schemas available for inter-service RPCs. Everything was just pure dicts! I hated it, so I decided to make a \"type learner\" that would consume a stream of data and generate Python types. Initially I built it for myself and it may reflect my specific use-case, but I hope that I was able to make it quite generic so it can be useful for others.\n  \n    For example, let's generate types for GitHub REST API\n  # fetch GitHub releases list\ngh api \\\n  -H \"Accept: application/vnd.github+json\" \\\n  -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n  /repos/facebook/react/releases --paginate |> data.json\n\nslow-learner learn --spread --type-name Release data.json\n    The result is\n  \"\"\"\nThis file contains Python 3.8+ type definitions generated by TypeLearner from 99 observed value(s)\n\nSource JSON files:\n- /Users/njvh/Documents/Personal/slow-learner/data.json\n\"\"\"\n\nfrom typing import List\nfrom typing import Literal\nfrom typing_extensions import NotRequired\nfrom typing import Optional\nfrom typing import TypedDict\nfrom typing import Union\n\n\nclass ReleaseAuthor(TypedDict):\n    login: str\n    id: int\n    node_id: str\n    avatar_url: str\n    gravatar_id: Literal[\"\"]\n    url: str\n    html_url: str\n    followers_url: str\n    following_url: str\n    gists_url: str\n    starred_url: str\n    subscriptions_url: str\n    organizations_url: str\n    repos_url: str\n    events_url: str\n    received_events_url: str\n    type: Literal[\"User\"]\n    site_admin: Literal[False]\n\n\nclass ReleaseAssetsItemUploader(TypedDict):\n    login: str\n    id: int\n    node_id: str\n    avatar_url: str\n    gravatar_id: Literal[\"\"]\n    url: str\n    html_url: str\n    followers_url: str\n    following_url: str\n    gists_url: str\n    starred_url: str\n    subscriptions_url: str\n    organizations_url: str\n    repos_url: str\n    events_url: str\n    received_events_url: str\n    type: Literal[\"User\"]\n    site_admin: Literal[False]\n\n\nclass ReleaseAssetsItem(TypedDict):\n    url: str\n    id: int\n    node_id: str\n    name: str\n    label: Optional[str]\n    uploader: ReleaseAssetsItemUploader\n    content_type: Union[\n        Literal[\"text/javascript\"],\n        Literal[\"application/javascript\"],\n        Literal[\"application/x-javascript\"],\n        Literal[\"application/zip\"],\n    ]\n    state: Literal[\"uploaded\"]\n    size: int\n    download_count: int\n    created_at: str\n    updated_at: str\n    browser_download_url: str\n\n\nReleaseReactions = TypedDict(\n    \"ReleaseReactions\",\n    {\n        \"url\": str,\n        \"total_count\": int,\n        \"+1\": int,\n        \"-1\": Literal[0],\n        \"laugh\": int,\n        \"hooray\": int,\n        \"confused\": Literal[0],\n        \"heart\": int,\n        \"rocket\": int,\n        \"eyes\": int,\n    },\n)\n\n\nclass Release(TypedDict):\n    url: str\n    assets_url: str\n    upload_url: str\n    html_url: str\n    id: int\n    author: ReleaseAuthor\n    node_id: str\n    tag_name: str\n    target_commitish: str\n    name: str\n    draft: Literal[False]\n    prerelease: bool\n    created_at: str\n    published_at: str\n    assets: List[ReleaseAssetsItem]\n    tarball_url: str\n    zipball_url: str\n    body: str\n    reactions: NotRequired[ReleaseReactions]\n\nGithub | PyPI | Read more"
},
{
    "title": "No title",
    "content": "What My Project Does\n    Ever wanted to have a unittest compare results to reference data from a text file, and fail if they don't match? So did we! We have just opened sourced our Python package that extends Python unittest to enable comparison to data from a reference text file containing data.\n  Target Audience\n    Python developers, data scientists, looking to include file comparison in their unit testing.\n  Comparison\n    Not the first time this has been done. This just packages functionality into an easy to install and use Python package.\n  Clone the projectgit clone https://github.com/amentumspace/file_unittestInstallation\n    The package can be installed into any active environment by running:\n  cd file_unittest/   \npip install .file_unittest.TestCase\n    This class extends the unittest.TestCase class to allow the user to run unit tests by writing data to a file and verifying the output has not changed since the last time the unit test was run.\n  \n    Once the user is satisfied that the output data is correct, these unit test simply ensure that the data does not change with changes in the code base.\n  Usage\n    Rather than inherit from unittest.TestCase the user should derive a class from file_unittest.TestCase.\n  \n    Like the normal unit test, individual test functions are appended with test_.\n  \n    Any desired output data is written to file by calling self.output\n\n    Example unit test file:\n  import unittest\nimport file_unittest\n\nclass MyTest(file_unittest.TestCase):\n    def test_case(self):\n        self.output(\"hello\")\n\nif __name__ == '__main__':\n    unittest.main()\n\n\n    Unlike the normal unittest.TestCase the user does not need to make any assertion calls eg. self.assertTrue, self.assertFalse etc.\n  \n\n\n    This class will take care of warning about any differences in output.\n  \n\nDefault output location\n    The default location to output the test results will be:\n      {derived_class_filepath}/test_results/\n      {derived_class_filename}.{derived_class_name}.{test_name}.txtMissing or different results\n    If the expected output file is missing, or differences are detected, the output data will be written to the same file, but with .new postfix:\n      {derived_class_filepath}/test_results/\n      {derived_class_filename}.{derived_class_name}.{test_name}.new\n\n\n    On the first run, the user will need to inspect the .new file for expected results;\n  \n\n\n    Otherwise, if differences are detected, the user can diff the .txt and .new files to investigate and resolve any differences;\n  \n\n\n    In both cases once the user is satisfied with the results, the .new files can be renamed with .txt  extension and the .txt files can be checked into the git repo\n  \n\nCommitting test result files to source control\n    Once the output .txt files have been satisfactorily generated as in the previous step, they should be checked into source control so that they can be used as the benchmark for future runs."
},
{
    "title": "No title",
    "content": "What My Project Does\n\n    I just dropped my first open source project for python on github! Its a speeech recognition program written 100% in python. I made it to help me while I play star citizens by rebinding the keys too speech recognition commands, that way I dont need to remember over 100 keybinds or accidently click the wrong one.\n  \nComparison (A brief comparison explaining how it differs from existing alternatives.)\n\n    Some features that I have not seen in other similar programs. Such as A topmost/ Picture in picture Browser function so that users can simply call a voice command and it will open a Picture in picture browser over top of the game, that they can interact with and use so they do not have to switch to desktop open a browser type in the url, and the switch back. They simply can interact with a browser on top of their game. and close it without ever having to leave i think its pretty neat idea.\n  \nTarget Audience (e.g., Is it meant for production, just a toy project, etc.\n\n    So far im pretty pleased, and would like anyone thats intersted in checking out other peoples side/hobby projects to check it out! Ive gone through tons of offline speech recognition librarys like vosk and sphinx but none of them can compete in the terms of speed, and accuracy like the windows speech recognition uses. Im using a python libary called Dragonfly2. So far it is working great on windows 10 and windows 11 computers. Its extremely fast from voice command to keypress. and im pretty happy so far with my side project and the results!\n  \n    I figured I would post it here and see what everyone here thinks about it so far? This is my first time releaseing a python script I've created, Ive made a few small things here and there with python but never released anything so I decided I might as well start\n  \nhttps://github.com/techsavvy42/Captains_Voice_Commander"
},
{
    "title": "No title",
    "content": "I've finally implemented my \"ideal state\" for Python release management on GitHub. Focused around these three tenets:\n  \n\n\nMust be idiot proof, meaning: all checks are automated and operations work is automated.\n  \n\n\nMust be runnable from a mobile browser. If your publish operation requires a laptop, you are probably doing it wrong.\n  \n\n\nMust decouple marketing work from engineering work, while not duplicating efforts.\n  \n\n\n    And here's how I got there:\n  \n\n\nSemantic pull request titles. Basically, take a conventional commits approach but apply them to your PR titles (which are trivial to edit+groom) instead of commit history (which is frustrating to edit+groom).\n  \n\n\nNo changelogs. If you use Semantic PRs, and if you default to those PR titles as your commit message when merging, then your main branch commit history is your changelog.\n  \n\n\nNo version bumps. Instead use dynamic versioning, such as provided by the poetry-dynamic-versioning plugin.\n  \n\n\nAutomatically-drafted release notes. If we've followed all the above, then a machine can create and continually edit a release notes draft (including calculating the correct version bump and the ability to generate and categorize changelog entries), which means at any moment we are ready to...\n  \n\n\nOne-click publish. Optionally groom before you do, but if you've followed the above four steps, it's just a single click to \"publish\" the release in GitHub. A GitHub Action can listen for the new release tag and respond by publishing to PyPi.\n  \n\n\nMy tools:\n\n\n\n    Semantic PR checker: https://github.com/zeke/semantic-pull-requests\n\n\n\n    Poetry dynamic versions plugin: https://github.com/mtkennerly/poetry-dynamic-versioning\n\n\n\n    Release drafter: https://github.com/marketplace/actions/release-drafter\n\n\n\nLet me know what you think! Would you adopt this? What would you change?"
},
{
    "title": "No title",
    "content": "Since threads in Python share the memory space of their parent process, we might need to define thread-specific variables for specific use cases to avoid unintended side effects.\n  \n    In this post, we will:\n  \n\n\n    Explore Thread local storage: Python threading module's solution for thread-specific/thread-private values.\n  \n\n\n    See its real-world example from Open-Source ( usage in the Peewee ORM library).\n  \n\n\n    Look at the CPython source code to see how thread local storage is implemented under the hood.\n  \n\nBackground\n    Threads share the memory space of their parent process. This will allow us to seamlessly access and share variables, data structures, etc., across threads. But this comes with its own challenges. There may be scenarios where we need to isolate variables and might need to store data specific to each thread. Thread local storage can be leveraged in this case.\n  \n    We can use local() found in the threading module to define thread-local variables.\n  import threading\nimport time\n\n# Create a thread-local storage object (1)\nthread_local = threading.local()\n\n\ndef init_data(number):\n    thread_local.number = number * 100\n\n\ndef show_data():\n    print(f\"Thread {threading.current_thread().name} has number {thread_local.number}\")\n\n\ndef worker(number):\n    init_data(number)\n\n    for _ in range(3):\n        time.sleep(1)\n        show_data()\n\n\nthread1 = threading.Thread(target=worker, name=\"A\", kwargs={\"number\": 1})\nthread2 = threading.Thread(target=worker, name=\"B\", kwargs={\"number\": 2})\n\nthread1.start()\nthread2.start()\n\nthread1.join()\nthread2.join()\n    The program output will be:\n  Thread A has number 100\nThread B has number 200\nThread A has number 100\nThread B has number 200\nThread A has number 100\nThread B has number 200\n    As seen in comment #(1), a thread-local storage object was created and assigned to the variable thread_local. Arbitrary attributes can be assigned to this variable, which are specific to the thread that performs the assignment and is isolated from others.\n  \n    In the example, each thread stores its own number attribute to the thread_local object and accesses the thread-specific value during their concurrent execution.\n  Usage in the Wild\n    Peewee, a Python ORM, utilizes thread-local data in its ThreadSafeDatabaseMetadata to support dynamic database switches at runtime in multithreaded applications.\n  \n    The source code can be found here.\n# File: peewee/playhouse/shortcuts.py\n\nclass ThreadSafeDatabaseMetadata(Metadata):\n    \"\"\"\n    Metadata class to allow swapping database at run-time in a multi-threaded\n    application. To use:\n\n    class Base(Model):\n        class Meta:\n            model_metadata_class = ThreadSafeDatabaseMetadata\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        # The database attribute is stored in a thread-local.\n        self._database = None\n        self._local = threading.local()\n        super(ThreadSafeDatabaseMetadata, self).__init__(*args, **kwargs)\n\n    def _get_db(self):\n        return getattr(self._local, \"database\", self._database)\n\n    def _set_db(self, db):\n        if self._database is None:\n            self._database = db\n        self._local.database = db\n\n    database = property(_get_db, _set_db)\n    In multithreaded applications using peewee ORM, database switching at runtime without using ThreadSafeDatabaseMetada can lead to errors. If multiple threads work in parallel and one thread changes the connection parameters, this can lead to errors such as writing to a wrong DB, inconsistent writes (in case of non-atomic DB operations), etc.\n  \nThreadSafeDatabaseMetada solves this by keeping the database attributes in a thread-local object (self._local). In this way, dynamic changes to the database will only affect the thread that made the change. Other threads will keep working with their existing databases.\n  Should I use it?\n    Thread-local storage should be used when:\n  \n    You are writing a multi-threaded application (obviously) and:\n  \n\n\n    If some variables are used by the current thread only and are not relevant to the main thread/other threads.\n  \n\n\n    You find out that changes made by one thread can lead to unintended side effects in other concurrent threads.\n  \n\n\n    Generally, if you are working with multiple threads and there is shared mutable data:\n  \n    You should check if sharing these data across threads is actually needed.\n  \n\n\n    If sharing can be avoided, use thread-local storage to make the data specific to each thread.\n  \n\n\n    Otherwise, implement locks or other synchronization primitives to enforce thread safety.\n  \n\n\nNote: Context variables from the contextvars standard library module can be used as an alternative to threading.local(). They work with multithreading as well as asyncio to store context-specific information. In the case of asyncio programs, context variables allow each coroutine task to have its own set of variables isolated from other (asyncio) tasks.\n  Behind the scenes\n    The Python implementation of threading.local can be found in the /Lib/_threading_local.py path in CPython source code.\n\n    The _localimpl class is used to store thread-local values.\n  class _localimpl:\n    \"\"\"A class managing thread-local dicts\"\"\"\n    __slots__ = 'key', 'dicts', 'localargs', 'locallock', '__weakref__'\n\n    def __init__(self):\n        # The key used in the Thread objects' attribute dicts.\n        # We keep it a string for speed but make it unlikely to clash with\n        # a \"real\" attribute.\n        self.key = '_threading_local._localimpl.' + str(id(self))\n        # { id(Thread) -> (ref(Thread), thread-local dict) }\n        self.dicts = {}\n    The dicts attribute maps the id of a thread to a tuple. The tuple is two-membered, containing a reference to the thread and the actual dictionary storing thread local values (# { id(Thread) -> (ref(Thread), thread-local dict) }).\n  \n    Looking at a few other methods of the class:\n  class _localimpl:\n    \"\"\"A class managing thread-local dicts\"\"\"\n    def get_dict(self):\n        \"\"\"Return the dict for the current thread. Raises KeyError if none\n        defined.\"\"\"\n        thread = current_thread()\n        return self.dicts[id(thread)][1] # (1) returning the local dict of current thread\n\n    def create_dict(self):\n        \"\"\"Create a new dict for the current thread, and return it.\"\"\"\n        localdict = {}\n        key = self.key\n        thread = current_thread()\n        idt = id(thread)\n        ...\n        wrthread = ref(thread, thread_deleted)\n        thread.__dict__[key] = wrlocal\n        self.dicts[idt] = wrthread, localdict  # (2) Populating the `dicts` with a new thread\n        return localdict\n    The comments #(1) & #(2) illustrate operations on the dicts attribute discussed previously.\n  \n(1): Returns the local data dictionary corresponding to the current thread accessing it.\n  \n(2): This part initializes the local dict for a new thread.\n  \n    Then, we have the actual local callable, which we call as threading.local() to initialize a thread-local object.\n  @contextmanager\ndef _patch(self):\n    impl = object.__getattribute__(self, '_local__impl')\n    try:\n        dct = impl.get_dict()  # (3) this will return local dict of current thread -/\n                               # see its definition in the above snippet\n    except KeyError:\n        ...\n        # calls _localimpl`s create_dict to create & init a new dict\n    with impl.locallock:\n        object.__setattr__(self, '__dict__', dct) # (4) <The magic> Temporarily replaces -/\n        # the instance's __dict__ attribute with the thread-specific dictionary.\n        \n        yield\n\n\nclass local:\n    __slots__ = '_local__impl', '__dict__'\n\n    def __new__(cls, /, *args, **kw):\n        ...\n\n        impl = _localimpl()  # (1) - wraps the _localimpl object in an attribute\n        impl.localargs = (args, kw)\n        impl.locallock = RLock()  # (2) - Lock for thread safety\n        object.__setattr__(self, '_local__impl', impl)\n        # We need to create the thread dict in anticipation of\n        # __init__ being called, to make sure we don't call it\n        # again ourselves.\n        impl.create_dict()\n        return self\n\n    def __getattribute__(self, name):\n        with _patch(self):\n            return object.__getattribute__(self, name)\n\n    def __setattr__(self, name, value):\n        if name == '__dict__':\n            raise AttributeError(\n                \"%r object attribute '__dict__' is read-only\"\n                % self.__class__.__name__)\n        with _patch(self):\n            return object.__setattr__(self, name, value)\n\n    ...\n    Let's look at various parts of the code one by one.\n  \n# (1):  An object of _localimpl class is created and later stored in _local__impl attribute\n  \n# (2): RLock() is used on dict operations - Concurrent writes by multiple threads may cause 'lost writes' since dict is not thread-safe.\n  \n# (3): In the try/except block, the current thread's local data is fetched and assigned to the variable named dct.\n  \n# (4): The magic happens here. Here is a quick refresher on class attributes before we delve further:\n  \n\n    A class has a namespace implemented by a dictionary object. Class attribute references are translated to lookups in this dictionary, e.g., C.x is translated to C.dict[\"x\"] (although there are a number of hooks which allow for other means of locating attributes).\n  \n\n    The _patch function we are currently in is a context manager. Using the line marked as # (4), it patches the namespace dictionary of the local class with the dot variable (which stores the current-thread specific data).\n  \n    Since the __getattribute__ and __setattr__ dunders of the class local use the _patch context manager, attribute access performed inside the context will use the thread-local dictionary (dct) replacing the class's actual namespace dictionary.\n  \nNote: The Lib/_threading_local.py starts with the below note:\n  \n\n    Note that this module provides a Python version of the threading.local class. Depending on the version of Python you're using, there may be a faster one available. You should always import the local class from threading.\n  \n\n    The code we looked at might not be the one running in our Python installations. I think newer Pythons versions are using C implementations of the thread local functionality for efficiency.\n  \n    ...\n  \n    I share interesting Python snippets 🐍 like this from open-source projects illustrating Python language features in my newsletter, \"Python in the Wild\".\n  \n    Subscribe to the newsletter on Substack or Linkedin to receive new Pythonic posts to your email 💌🚀."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Happy weekend every one!\n  \n    I have recently created my first python package.\n  What My Project Does\n    The package helps with extracting financial data of companies from Yahoo into pandas dataframes using companies stocks symbol(like yfinance package)\n  Target Audience\n    The package is targeted at people who are interested in financial data of companies and want to use it for analysis or other purposes.\n  \n    Feel free to give you opinion or open issues on the Github project:\n  \nGithub Repo Link\n\nPypi link"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hi there!\n  \n    I'm working in ugropy, a Python library for molecule fragmentation to get the UNIFAC/Joback subgroups from the molecule's name or SMILES.\n  \nGitHubPyPi\n\nWhat My Project Does\n\n    By now ugropy can detect the subgroups for classic liquid-vapor UNIFAC, Predictive Soave-Redlich-Kwong (PSRK), and Joback (and estimate properties).\n  \nTarget Audience\n\n    Chemical engineers or scientists that work in thermodynamics or process simulation.\n  \nExample of use\nfrom ugropy import Groups\n\n\nhexane = Groups(\"hexane\")\n\nprint(hexane.unifac_groups)\nprint(hexane.psrk_groups)\nprint(hexane.joback.groups)\n    You will obtain:\n  {'CH3': 2, 'CH2': 4}\n{'CH3': 2, 'CH2': 4}\n{'-CH3': 2, '-CH2-': 4}\n    Any help and recommendations are welcome! I hope that the project may be helpful to someone :)"
},
{
    "title": "No title",
    "content": "Last time I showed how to count how many CPU instructions it takes to print(\"Hello\") and import seaborn.\n  \n    Here's a new post on how to record and visualise system calls that your Python code makes.\n  \n    Spoiler: 1 for print(\"Hello\"), about 20k for import seaborn, including an execve for lscpu!"
},
{
    "title": "No title",
    "content": "Hi,\n  \n    🎨 Discover how easy it is to transform your own phots into beautiful paintings\n  \n    🖼️ This is a cool effect based on Stylized Neural Painting library. Simple to use , and the outcome is impressive,\n  \n    You can find instructions here : https://github.com/feitgemel/Python-Code-Cool-Stuff/tree/master/How%20to%20make%20photos%20look%20like%20paintings\n\n    The link for the tutorial video :  https://youtu.be/m1QhxOWeeRc\n\n    Enjoy\n  \n    Eran\n  \n    #convertphototodigitalart #makephotolooklikepainting #makephotoslooklikepaintings #makepicturelooklikepainting"
},
{
    "title": "No title",
    "content": "From the makers of ruff comes uv\n\n\n    TL;DR: uv is an extremely fast Python package installer and resolver, written in Rust, and designed as a drop-in replacement for pip and pip-tools workflows.\n  \n\n    It is also capable of replacing virtualenv.\n  \n    With this announcement, the rye project and package management solution created by u/mitsuhiko (creator of Flask, minijinja, and so much more) in Rust, will be maintained by the astral team.\n  \n    This \"merger\" and announcement is all working toward the goal of a Cargo-type project and package management experience, but for Python.\n  \n    For those of you who have big problems with the state of Python's package and project management, this is a great set of announcements...\n  \n    For everyone else, there is https://xkcd.com/927/.\n  \n\n\nTwitter Announcement\n\n\n\nPyPI\n\n\n\nGitHub\n\n\n\n    Install it today:\n  pip install uv\n# or\npipx install uv\n# or\ncurl -LsSf https://astral.sh/uv/install.sh | sh"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hi people, after teaching Python applied to Data Science more than 800 people (the majority 1:1) and producing many courses, I’ve decided to put them on YouTube.\n  \n    They are mostly applied practical cases. For example, this tutorial to preprocess and visualize solar energy generation (Time Series).\n  \n    On top of that, I’m hosting a weekly live streaming to solve doubts on the tutorials I publish.\n  \n    If you are interested in stopping by and get some doubts solved, feel free to join here."
},
{
    "title": "No title",
    "content": "A GitHub repository of Python Tutorials in markdown and notebook format:\n  \nhttps://github.com/PhilipYip1988/python-notebooks\n\n    Covering:\n  \n\n\n    Python Installation using Anaconda/Miniconda\n  \n\n\n    Python Environments\n  \n\n\n    Markdown and TeX Syntax\n  \n\n\n    OOP Fundamentals and classes in the Builtins Module\n  \n\n\n    IPython Magics\n  \n\n\n    Formatters (AutoPEP8, ISort, Black and Ruff)\n  \n\n\n    Code Blocks and Comprehensions\n  \n\n\n    Collections Module\n  \n\n\n    Itertools Module\n  \n\n\n    Math Module\n  \n\n\n    Statistics Module\n  \n\n\n    Random Module\n  \n\n\n    Datetime Module\n  \n\n\n    File Formats (IO, CSV, Pickle and Shelve Modules)\n  \n\n\n    OS Module\n  \n\n\n    PathLib Module\n  \n\n\n    System Module\n  \n\n\n    NumPy Library\n  \n\n\n    Pandas Library\n  \n\n\n    Matplotlib Library\n  \n\n\n    Seaborn Library\n  \n\n\n    Plotly Library\n  \n\n\n    Pillow Library\n  \n\n\n    The tutorials are mainly focused on JupyterLab 4 but also mention how to setup VSCode.\n  \n    These are fairly detailed tutorials and it took me a long time to put together, hopefully it will help some beginners, particularly those looking to use Python for datascience."
},
{
    "title": "No title",
    "content": "I'm excited to announce the beta release of BlackMarblePy - a new Python package designed to retrieve NASA Black Marble data. For those unfamiliar, NASA Black Marble imagery provides stunning views of Earth at night, capturing the lights from cities and other human activity.\n  \n    This package aims to make accessing this data easier for researchers, developers, and anyone interested in exploring our planet's nighttime lights. Whether you're studying urbanization, monitoring light pollution, or simply fascinated by Earth's beauty after dark, this package is for you.\n  \n    Key Features:\n  \n\n\n    Simple Python interface for accessing NASA Black Marble data.\n  \n\n\n    Download daily, monthly, and yearly nighttime lights data for user-specified region of interest and time.\n  \n\n\n    Parallel downloading for faster data retrieval and automatic retry mechanism for handling network errors.\n  \n\n\n    Access NASA Black Marble as a Xarray Dataset\n  \n\n\n    Comprehensive documentation and examples to get you started quickly.\n  \n\n\n    How You Can Help:\n  \n    I'm reaching out to the community to gather feedback and suggestions for improvement. Whether you encounter any bugs, have ideas for additional features, or just want to share your experience using the package, your input is invaluable.\n  \n    Blog post: https://blogs.worldbank.org/opendata/illuminating-insights-harnessing-nasas-black-marble-r-and-python-packages\n\n    Repository: https://github.com/worldbank/blackmarblepy"
},
{
    "title": "No title",
    "content": "Hi there, I hope you guys are doing well. I have some amazing news to share with you! I have created a new python library called better_bing_image_downloader.\n  \n    It is a powerful and convenient tool to download bulk images from Bing without any API keys. It is compatible with python 3.x and above and it is hosted on both pip and github.\n  \n    You can install it easily with pip install better-bing-image-downloader.\n  \n    The README.md file on github gives you a comprehensive idea of how to use this package to its fullest potential in any project that requires web image scraping.\n  \n    You can check it out here:  Pypi Link or  GitHub Repo .\n  \n    I hope this library will be helpful for your data and machine learning projects. I would love to hear your feedback and suggestions!"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What’s the value proposition of Pycharm, compared with VS Vode + copilot suscription? Both will cost about the same yearly. Why would you keep your development in Pycharm?\n  \n    In the medium run, do you see Pycharm pro stay attractive?\n  \n    I’ve been using Pycharm pro for years, and recently tried using VS Code because of copilot. VS Code seems to have better integration of LLM code assistance (and faster development here), and a more modular design which seems promising for future improvements. I am considering to totally shift to VS Code."
},
{
    "title": "No title",
    "content": "https://github.com/Never-Over/modguard\n\n    We built modguard to solve a recurring problem that we've experienced on software teams -- code sprawl. Unintended cross-module imports would tightly couple together what used to be independent domains, and eventually create \"balls of mud\". This made it harder to test, and harder to make changes. Mis-use of modules which were intended to be private would then degrade performance and even cause security incidents.\n  \n    This would happen for a variety of reasons:\n  \n\n\n    Junior developers had a limited understanding of the existing architecture and/or frameworks being used\n  \n\n\n    It's significantly easier to add to an existing service than to create a new one\n  \n\n\n    Python doesn't stop you from importing any code living anywhere\n  \n\n\n    When changes are in a 'gray area', social desire to not block others would let changes through code review\n  \n\n\n    External deadlines and management pressure would result in \"doing it properly\" getting punted and/or never done\n  \n\n\n    The attempts to fix this problem almost always came up short. Inevitably, standards guides would be written and stricter and stricter attempts would be made to enforce style guides, lead developer education efforts, and restrict code review. However, each of these approaches had their own flaws.\n  \n    The solution was to explicitly define a module's boundary and public interface in code, and enforce those domain boundaries through CI. This meant that no developer could introduce a new cross-module dependency without explicitly changing the public interface or the boundary itself. This was a significantly smaller and well-scoped set of changes that could be maintained and managed by those who understood the intended design of the system.\n  \n    With modguard set up, you can collaborate on your codebase with confidence that the intentional design of your modules will always be preserved.\n  \nmodguard is:\n  \n\n\n    fully open source\n  \n\n\n    able to be adopted incrementally\n  \n\n\n    implemented with no runtime footprint\n  \n\n\n    a standalone library with no external dependencies\n  \n\n\n    interoperable with your existing system (cli, generated config)\n  \n\n\n    We hope you give it a try! Would love any feedback."
},
{
    "title": "No title",
    "content": "Libraries such as django-silk are excellent for profiling the queries executed by your Django application. We have found, however, that they do not provide a completely straightforward way to identify the lines of your application code which are responsible for executing the most queries.django-queryhunter aims to fill that gap by providing a simple code-first approach to query profiling.\n  \n    This is achieved by providing a context manager and middleware which can provide a detailed report of the lines of your application code which are responsible for executing SQL queries, including data on:* The module name and the line number of the code which executed the query.* The executing code itself on that line.* The number of times that line was responsible for executing a query and the total time that line spent querying the database.* The last SQL statement executed by that line.* If installed as a middleware, the URL and HTTP method of the request which invoked the query.Here's some sample output:\n  queryhunter/tests/my_module.py\n==================================== \n\nLine no: 13 | Code: for post in posts: | Num. Queries: 1 | SQL: SELECT \"tests_post\".\"id\", \"tests_post\".\"content\", \"tests_post\".\"author_id\" FROM \"tests_post\" | Duration: 4.783299999999713e-05\n\nLine no: 14 | Code: authors.append(post.author.name) | Num. Queries: 5 | SQL: SELECT \"tests_author\".\"id\", \"tests_author\".\"name\" FROM \"tests_author\" WHERE \"tests_author\".\"id\" = %s LIMIT 21 | Duration: 8.804199999801199e-05\n    One particularly useful feature of this view of profiling is quickly identifying n+1 queries caused by missing select_related or prefetch_related calls.\n  \nUsing queryhunterLet's suppose we have a Django application with two simple models Author and Post where Post has a foreign key author pointing to the Author model. Let's also suppose we have a simple view function get_authors as below, which examines all Posts and fetches the author names in list:\n  # queryhunter/tests/views.py\n\ndef get_authors(request): # url: /mysite.com/v1/get_authors/ \n    authors = []\n    posts = Post.objects.all()  # suppose we have 5 posts\n    for post in posts: \n        authors.append(post.author.name) \n    return JsonResponse({'authors': authors})\n    With the queryhunter middleware installed, requests to the get_authors view will be automatically profiled, with output printed like below. Notice that the code responsible for invoking the SQL query appears next to the SQL statement itself and to the URL of the request which invoked it:\n  queryhunter/tests/my_module.py\n==================================== \nLine no: 6 | Code: for post in posts: | Num. Queries: 1 | Duration: 0.04 | url: /mysite.com/v1/get_authors/ | method: GET | SQL: SELECT \"tests_post\".\"id\", \"tests_post\".\"content\", \"tests_post\".\"author_id\" FROM \"tests_post\" \n\nLine no: 7 | Code: authors.append(post.author.name) | Num. Queries: 5 | Duration: 0.05 | url: /mysite.com/v1/get_authors/ | method: GET | SQL: SELECT \"tests_author\".\"id\", \"tests_author\".\"name\" FROM \"tests_author\" WHERE \"tests_author\".\"id\" = %s LIMIT 21\n    What can we learn from this output?* The line authors.append(post.author.name) was responsible for executing 5 SQL queries, one for each post.* This is a quick way to identify that we are missing a select_related('author') call in our Post.objects.all() query.This may have been obvious in this contrived example, but in a large code base, flushing out these kinds of issues can be very handy. Sharing here in case others find the library useful."
},
{
    "title": "No title",
    "content": "Hey all,\n  \n    Sometimes it happens that a test passes successfully when executed in isolation, but fails when the whole suite is run. Finding the offending test is annoying when you have 10000 tests to check.\n  \n    I have written a small utility script that helps with finding the offending test, available here: https://github.com/maciej-gol/pytest-bisect-tests.\n  \n    There is already a similar alternative, but the author wasn't keen to hear suggestions around improving the UX so I thought I would go ahead and simplify the UX for the users (+ a few more goodies).\n  \n    Happy to hear your feedback!"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hi All,\n  \n    My name is Rotem, I'm one of the creators of Atlas, an open-source schema migrations tool (GitHub).\n  \n    We have recently released a \"schema loader\" plugin for Django, which enables Atlas to seamlessly read Django schemas and manage the database schema for them.\n  \n    The provider's code on GitHub is available here.\n  \n    Effectively, this means you can stop using python manage.py migrate and python manage.py makemigration and replace them with Atlas.\n  \nBut why replace Django Migrations?\n\n    Among the many ORMs available in our industry, Django's automatic migration is one of the most powerful and robust. It can handle a wide range of schema changes, including adding new tables and columns, basic indexes, and more. However, having been created in 2014, a very different era in software engineering, it naturally has some limitations.\n  \n    Some of the limitations of Django's migration system include:\n  \n\n\nDatabase Features. Because it was created to provide interoperability across database engines, Django's migration system is centered around the \"lowest common denominator\" of database features. More advanced features such as Triggers, Views, and Stored Procedures have very limited support and require developers to jump through all kinds of hoops to use them.\n  \n\n\nEnsuring Migration Safety. Migrations are a risky business. If you're not careful, you can easily cause data loss or a production outage. Django's migration system does not provide a native way to ensure that a migration is safe to apply.\n  \n\n\nModern Deployments.  Django does not provide native integration with modern deployment practices such as GitOps or Infrastructure-as-Code.\n  \n\n\n    Atlas, on the other hand, lets you manage your Django applications using the Database Schema-as-Code paradigm. This means that you can use Atlas to automatically plan schema migrations for your Django project, and then apply them to your database. Using Atlas, you can enjoy automatic migration planning, automatic code review, and integrations with your favorite CI/CD tools.\n  \nRead the Full Guide Here\n\n    As always, looking to hear your feedback and thoughts\n  \n    R"
},
{
    "title": "No title",
    "content": "It is a quite simple app that runs while you play League of Legends. It was quite fun to create and use!\n  \n    👉 You can find a repo. with the code in here if you want to check it out.\n  \n    FYI, I am not a super experienced python programmer. I have been using it for a few years and always try to improve little by little.\n  \n    Thanks and happy coding!"
},
{
    "title": "No title",
    "content": "Hello,I just released a new version of my API Wrapper in Python called \"OnePyece\", to get various data from OnePiece !It uses this source API : https://www.api-onepiece.com/enWhich was only available in french but has an english version now !Thats why I'm sharing with you my python package to communicate with this API very easily in Python : https://github.com/icepick4/onepyece/If you want to try it just follow the README instructions !Have fun with the OnePiece universe in available Python ! If you like the project do not forget to star it on GitHub ;)"
},
{
    "title": "No title",
    "content": "Python’s Asyncio module is a great way to handle IO-bound jobs and has seen many improvements in recent Python updates. However, there are so many ways to handle async tasks that figuring out which method to use for different scenarios can be a bit confusing. In my new article, I first go over the basics of what a task object is and then talk about all of the different ways to handle them and the pros and cons of each."
},
{
    "title": "No title",
    "content": "It would hurt simplicity but it is beyond that point. Python feels like Scratch compared to other languages at the moment. Lacking this basic feature hurts productivity, I don't want to write 50 lines of \"if not product.name\" etc."
},
{
    "title": "No title",
    "content": "My first machine learning project I really put out there ; as far as the end user is concerned it should work in a way vaguely similar to Jisho Draw searching or Google Handwriting but is open source.\n  \n    The source code is available in https://github.com/etrotta/kanji_lookup , and you can access an online demo in https://huggingface.co/spaces/etrotta/kanji_lookup .\n  \n    The project steps are roughly:\n  \n\n\n    Generate synthetic Kanji images for multiple different fonts (using pillow)\n  \n\n\n    Encode the images into embeddings using a Neural Network, using an existing open source Vision Transformer model (using transformers and pytorch)\n  \n\n\n    Store the embeddings in a Vector Database (using qdrant)\n  \n\n\n    Encode an User submitted image and compare it with existing records\n  \n\n\n    Create an web app to make submitting images more convenient (using gradio)\n  \n\n\n    Note: The GitHub repository and the HuggingFace Space repositories are separate, the GitHub repository is responsible for creating the embeddings and can search files. The HF Space only contains the gradio app."
},
{
    "title": "No title",
    "content": "I threw together a bunch of scripts that help visualize user data from the Spotify developer API, including hourly listening patterns, favorite genres, favorite tracks, and favorite artist popularity among Spotify users.  User data can be imported using the spotify_Data_CSV_Github.py script, which appends the data to a CSV file.\n  \n    Repository can be found here:\n  \nhttps://github.com/zachzion762/spotify_Developer_API\n\n    This is my first real python project that I've made on my own with a little bit (a lot) of help from ChatGPT."
},
{
    "title": "No title",
    "content": "https://github.com/Dimitris02/subtitler/tree/main\n\n    I made it in python 3.9. It uses the following libraries: speech_recognition, selenium, moviepy, pydub\n  \n    It uses google's speech recognition as well as a deepl.com scraper (deepl is the most accurate translator out there afaik). Usually it works pretty well with the default variables but you can tweak the SILENCE and THRESH variables in the subtitler.py file for better results. You can also change the LANG variable to pick another language. To launch the app run the launch.bat file and enter the filename/path of the video you want to add subtitles to.\n  \n    The default language is Russian because I was testing it on some Tarkovsky movie scenes.\n  \n    I hope you find it useful.\n  \n    Edit: I just added a feature where you can change the TRANSLATE_TO variable in the subtitler.py file to change the language of the resulting subtitles."
},
{
    "title": "No title",
    "content": "As stated, I created a (mostly) universal file converter. You can select to convert single files and batch files. Handles most video and audio formats. If I am forgetting anything, let me know and I'll see if I can add it!\n  \n    I needed this to batch convert raw MKV files to MP4 that I could use for editing in Premiere Pro. But I also wanted to make it more cause I know I like to convert files a lot.\n  \nGitHub Link to Source Code and instructions on how to run the file. You will need FFMPEG installed, but I have instructions for that as well."
},
{
    "title": "No title",
    "content": "Hi all,\n  \n    I'm currently working on a side project (tplmgr.dev) which is a software template management platform. Think backstage software templates as a standalone service and simpler! I'm aiming to primarily support Cookiecutter templates in the first instance, but I wanted to get some feedback on people's usage of Cookiecutter hooks, seeing as it's going to be quite complex to support this feature. So, do you use Cookiecutter hooks in your templates? If so, can you link to some examples of usage so that I can get a general idea as to whether supporting this should be prioritised.\n  \n    Thanks for reading!"
},
{
    "title": "No title",
    "content": "https://github.com/RClayGates/gmail_auto_label\n\n    I wanted to get the 17,000+ emails in my gmail sorted out. So I made this and I'm so happy with how my project is shaping up! It's my first time ever making a project with anything google api.\n  \n    It goes through the entirety of the gmail account, retrieves the From header for each email. Then creates a dictionary from all the emails, using everything after the @ in the email address.example { \"com\":  [ \"example\", \"info\"], \"org\" : [[ \"School\"], [ \"gov\", \"us\"]] }\n  \n    Then uses that to create nested labels and appropriate filters to add those labels. After those are created it will go back and update every email with the appropriate labels.\n  \n    Any feedback/pointers or even a git pull on making this better would be awesome!"
},
{
    "title": "No title",
    "content": "Tried to put together a document text extraction server using Apache Tika (with ~30 lines of Python code).\n  \n    This can be used to get the text needed for retrieval-augmented generation or to create LLM training datasets (or frankly, anything else that is text-dependent). It worked out pretty well, so thought it would be cool to share: https://blog.min.io/minio-tika-text-extraction/\n\nAn aside on Tika:Apache Tika is time-tested and, by some, considered a legacy toolkit. However, with Tika running as a container and the use of Python bindings, it's possible to get a text extraction experience that is as easy to build with as newer frameworks like Unstructured, but also matches the extraction capability of dedicated extraction models like Nougat. Kind of surprising!\n  \n    Much credit to the tika-python project for making the Python bindings!\n  \nA further aside on object storage:Furthermore, using a backing object store (i.e. MinIO) to hold the source documents is very useful (whether the extracted text is being used for RAG or an LLM training dataset)."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "https://buzzpy.hashnode.dev/analyzing-netflix-data-with-pandas-python\n\n    Analysing Netflix Movie Data 👆\n  \n    This is better: https://buzzpy.hashnode.dev/beginners-guide-to-pandas"
},
{
    "title": "No title",
    "content": "PySimpleGUI, a popular Python GUI library with 13k GitHub stars went closed source / commercial today. Previously it had been licensed under LGPL. I've got no issue with open source devs making money but to changing the license on a library many have contributed to seems to be pretty poor form. This had been a great cross-platform library for beginners."
},
{
    "title": "No title",
    "content": "We have a large report in Word based on several Excel files containing Budget data. We have to generate a PDF report that contains a narrative that is based on the data and we use pandas to manipulate the data - when the data changes, we want the numbers to be updated - for example, I might have a text that says \"The department with the highest increase in budgeting for Staff costs is XYZ with 76.5% increase\". In addition, we will have several data tables presenting the budget data, summarized by different categories, etc. I am looking to replace Word as there is no easy way to insert dynamic data, except for mail merge, which is quite basic. Jupyter Notebook or Quarto would be great choices, but the end users are not programmers - I was wondering whether there are tools to support such a scenario.\n  \n    <<Update>>: Just got to know - https://docxtemplater.com/ and it looks quite close to what I wanted to have. Updating here for the benefit of others."
},
{
    "title": "No title",
    "content": "The project will get Airbnb's information including images, description, price, title ..etcIt also full search given coordinates\n  \nhttps://github.com/johnbalvin/pybnb\n\n    Install:$ pip install gobnbUsage:from gobnb import *data = Get_from_room_url(room_url,currency,\"\")"
},
{
    "title": "No title",
    "content": "Graphyte. is a Python-based graphing application built with PyQt6 and Matplotlib inspired by GeoGebra, designed to provide users with an intuitive interface for creating and visualizing mathematical functions and data sets.\n  \n    Since Graphyte. is a new Graphing software, the list of current features is pretty limited. But I'm focused on implementing more and more features to get it to GeoGebra's level, although that would require much more time.\n  \n    GitHub: https://github.com/rohankishore/Graphyte."
},
{
    "title": "No title",
    "content": "Supabase is looking to sponsor a an experienced python dev tools OSS contributor to close the gap between our javascript client supabase-js and python client supabase-py\n\n    Full details and an application link available here"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What’s the value proposition of Pycharm, compared with VS Vode + copilot suscription? Both will cost about the same yearly. Why would you keep your development in Pycharm?\n  \n    In the medium run, do you see Pycharm pro stay attractive?\n  \n    I’ve been using Pycharm pro for years, and recently tried using VS Code because of copilot. VS Code seems to have better integration of LLM code assistance (and faster development here), and a more modular design which seems promising for future improvements. I am considering to totally shift to VS Code."
},
{
    "title": "No title",
    "content": "https://github.com/Never-Over/modguard\n\n    We built modguard to solve a recurring problem that we've experienced on software teams -- code sprawl. Unintended cross-module imports would tightly couple together what used to be independent domains, and eventually create \"balls of mud\". This made it harder to test, and harder to make changes. Mis-use of modules which were intended to be private would then degrade performance and even cause security incidents.\n  \n    This would happen for a variety of reasons:\n  \n\n\n    Junior developers had a limited understanding of the existing architecture and/or frameworks being used\n  \n\n\n    It's significantly easier to add to an existing service than to create a new one\n  \n\n\n    Python doesn't stop you from importing any code living anywhere\n  \n\n\n    When changes are in a 'gray area', social desire to not block others would let changes through code review\n  \n\n\n    External deadlines and management pressure would result in \"doing it properly\" getting punted and/or never done\n  \n\n\n    The attempts to fix this problem almost always came up short. Inevitably, standards guides would be written and stricter and stricter attempts would be made to enforce style guides, lead developer education efforts, and restrict code review. However, each of these approaches had their own flaws.\n  \n    The solution was to explicitly define a module's boundary and public interface in code, and enforce those domain boundaries through CI. This meant that no developer could introduce a new cross-module dependency without explicitly changing the public interface or the boundary itself. This was a significantly smaller and well-scoped set of changes that could be maintained and managed by those who understood the intended design of the system.\n  \n    With modguard set up, you can collaborate on your codebase with confidence that the intentional design of your modules will always be preserved.\n  \nmodguard is:\n  \n\n\n    fully open source\n  \n\n\n    able to be adopted incrementally\n  \n\n\n    implemented with no runtime footprint\n  \n\n\n    a standalone library with no external dependencies\n  \n\n\n    interoperable with your existing system (cli, generated config)\n  \n\n\n    We hope you give it a try! Would love any feedback."
},
{
    "title": "No title",
    "content": "Libraries such as django-silk are excellent for profiling the queries executed by your Django application. We have found, however, that they do not provide a completely straightforward way to identify the lines of your application code which are responsible for executing the most queries.django-queryhunter aims to fill that gap by providing a simple code-first approach to query profiling.\n  \n    This is achieved by providing a context manager and middleware which can provide a detailed report of the lines of your application code which are responsible for executing SQL queries, including data on:* The module name and the line number of the code which executed the query.* The executing code itself on that line.* The number of times that line was responsible for executing a query and the total time that line spent querying the database.* The last SQL statement executed by that line.* If installed as a middleware, the URL and HTTP method of the request which invoked the query.Here's some sample output:\n  queryhunter/tests/my_module.py\n==================================== \n\nLine no: 13 | Code: for post in posts: | Num. Queries: 1 | SQL: SELECT \"tests_post\".\"id\", \"tests_post\".\"content\", \"tests_post\".\"author_id\" FROM \"tests_post\" | Duration: 4.783299999999713e-05\n\nLine no: 14 | Code: authors.append(post.author.name) | Num. Queries: 5 | SQL: SELECT \"tests_author\".\"id\", \"tests_author\".\"name\" FROM \"tests_author\" WHERE \"tests_author\".\"id\" = %s LIMIT 21 | Duration: 8.804199999801199e-05\n    One particularly useful feature of this view of profiling is quickly identifying n+1 queries caused by missing select_related or prefetch_related calls.\n  \nUsing queryhunterLet's suppose we have a Django application with two simple models Author and Post where Post has a foreign key author pointing to the Author model. Let's also suppose we have a simple view function get_authors as below, which examines all Posts and fetches the author names in list:\n  # queryhunter/tests/views.py\n\ndef get_authors(request): # url: /mysite.com/v1/get_authors/ \n    authors = []\n    posts = Post.objects.all()  # suppose we have 5 posts\n    for post in posts: \n        authors.append(post.author.name) \n    return JsonResponse({'authors': authors})\n    With the queryhunter middleware installed, requests to the get_authors view will be automatically profiled, with output printed like below. Notice that the code responsible for invoking the SQL query appears next to the SQL statement itself and to the URL of the request which invoked it:\n  queryhunter/tests/my_module.py\n==================================== \nLine no: 6 | Code: for post in posts: | Num. Queries: 1 | Duration: 0.04 | url: /mysite.com/v1/get_authors/ | method: GET | SQL: SELECT \"tests_post\".\"id\", \"tests_post\".\"content\", \"tests_post\".\"author_id\" FROM \"tests_post\" \n\nLine no: 7 | Code: authors.append(post.author.name) | Num. Queries: 5 | Duration: 0.05 | url: /mysite.com/v1/get_authors/ | method: GET | SQL: SELECT \"tests_author\".\"id\", \"tests_author\".\"name\" FROM \"tests_author\" WHERE \"tests_author\".\"id\" = %s LIMIT 21\n    What can we learn from this output?* The line authors.append(post.author.name) was responsible for executing 5 SQL queries, one for each post.* This is a quick way to identify that we are missing a select_related('author') call in our Post.objects.all() query.This may have been obvious in this contrived example, but in a large code base, flushing out these kinds of issues can be very handy. Sharing here in case others find the library useful."
},
{
    "title": "No title",
    "content": "Hey all,\n  \n    Sometimes it happens that a test passes successfully when executed in isolation, but fails when the whole suite is run. Finding the offending test is annoying when you have 10000 tests to check.\n  \n    I have written a small utility script that helps with finding the offending test, available here: https://github.com/maciej-gol/pytest-bisect-tests.\n  \n    There is already a similar alternative, but the author wasn't keen to hear suggestions around improving the UX so I thought I would go ahead and simplify the UX for the users (+ a few more goodies).\n  \n    Happy to hear your feedback!"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hi All,\n  \n    My name is Rotem, I'm one of the creators of Atlas, an open-source schema migrations tool (GitHub).\n  \n    We have recently released a \"schema loader\" plugin for Django, which enables Atlas to seamlessly read Django schemas and manage the database schema for them.\n  \n    The provider's code on GitHub is available here.\n  \n    Effectively, this means you can stop using python manage.py migrate and python manage.py makemigration and replace them with Atlas.\n  \nBut why replace Django Migrations?\n\n    Among the many ORMs available in our industry, Django's automatic migration is one of the most powerful and robust. It can handle a wide range of schema changes, including adding new tables and columns, basic indexes, and more. However, having been created in 2014, a very different era in software engineering, it naturally has some limitations.\n  \n    Some of the limitations of Django's migration system include:\n  \n\n\nDatabase Features. Because it was created to provide interoperability across database engines, Django's migration system is centered around the \"lowest common denominator\" of database features. More advanced features such as Triggers, Views, and Stored Procedures have very limited support and require developers to jump through all kinds of hoops to use them.\n  \n\n\nEnsuring Migration Safety. Migrations are a risky business. If you're not careful, you can easily cause data loss or a production outage. Django's migration system does not provide a native way to ensure that a migration is safe to apply.\n  \n\n\nModern Deployments.  Django does not provide native integration with modern deployment practices such as GitOps or Infrastructure-as-Code.\n  \n\n\n    Atlas, on the other hand, lets you manage your Django applications using the Database Schema-as-Code paradigm. This means that you can use Atlas to automatically plan schema migrations for your Django project, and then apply them to your database. Using Atlas, you can enjoy automatic migration planning, automatic code review, and integrations with your favorite CI/CD tools.\n  \nRead the Full Guide Here\n\n    As always, looking to hear your feedback and thoughts\n  \n    R"
},
{
    "title": "No title",
    "content": "It is a quite simple app that runs while you play League of Legends. It was quite fun to create and use!\n  \n    👉 You can find a repo. with the code in here if you want to check it out.\n  \n    FYI, I am not a super experienced python programmer. I have been using it for a few years and always try to improve little by little.\n  \n    Thanks and happy coding!"
},
{
    "title": "No title",
    "content": "Hello,I just released a new version of my API Wrapper in Python called \"OnePyece\", to get various data from OnePiece !It uses this source API : https://www.api-onepiece.com/enWhich was only available in french but has an english version now !Thats why I'm sharing with you my python package to communicate with this API very easily in Python : https://github.com/icepick4/onepyece/If you want to try it just follow the README instructions !Have fun with the OnePiece universe in available Python ! If you like the project do not forget to star it on GitHub ;)"
},
{
    "title": "No title",
    "content": "Python’s Asyncio module is a great way to handle IO-bound jobs and has seen many improvements in recent Python updates. However, there are so many ways to handle async tasks that figuring out which method to use for different scenarios can be a bit confusing. In my new article, I first go over the basics of what a task object is and then talk about all of the different ways to handle them and the pros and cons of each."
},
{
    "title": "No title",
    "content": "It would hurt simplicity but it is beyond that point. Python feels like Scratch compared to other languages at the moment. Lacking this basic feature hurts productivity, I don't want to write 50 lines of \"if not product.name\" etc."
},
{
    "title": "No title",
    "content": "My first machine learning project I really put out there ; as far as the end user is concerned it should work in a way vaguely similar to Jisho Draw searching or Google Handwriting but is open source.\n  \n    The source code is available in https://github.com/etrotta/kanji_lookup , and you can access an online demo in https://huggingface.co/spaces/etrotta/kanji_lookup .\n  \n    The project steps are roughly:\n  \n\n\n    Generate synthetic Kanji images for multiple different fonts (using pillow)\n  \n\n\n    Encode the images into embeddings using a Neural Network, using an existing open source Vision Transformer model (using transformers and pytorch)\n  \n\n\n    Store the embeddings in a Vector Database (using qdrant)\n  \n\n\n    Encode an User submitted image and compare it with existing records\n  \n\n\n    Create an web app to make submitting images more convenient (using gradio)\n  \n\n\n    Note: The GitHub repository and the HuggingFace Space repositories are separate, the GitHub repository is responsible for creating the embeddings and can search files. The HF Space only contains the gradio app."
},
{
    "title": "No title",
    "content": "I threw together a bunch of scripts that help visualize user data from the Spotify developer API, including hourly listening patterns, favorite genres, favorite tracks, and favorite artist popularity among Spotify users.  User data can be imported using the spotify_Data_CSV_Github.py script, which appends the data to a CSV file.\n  \n    Repository can be found here:\n  \nhttps://github.com/zachzion762/spotify_Developer_API\n\n    This is my first real python project that I've made on my own with a little bit (a lot) of help from ChatGPT."
},
{
    "title": "No title",
    "content": "https://github.com/Dimitris02/subtitler/tree/main\n\n    I made it in python 3.9. It uses the following libraries: speech_recognition, selenium, moviepy, pydub\n  \n    It uses google's speech recognition as well as a deepl.com scraper (deepl is the most accurate translator out there afaik). Usually it works pretty well with the default variables but you can tweak the SILENCE and THRESH variables in the subtitler.py file for better results. You can also change the LANG variable to pick another language. To launch the app run the launch.bat file and enter the filename/path of the video you want to add subtitles to.\n  \n    The default language is Russian because I was testing it on some Tarkovsky movie scenes.\n  \n    I hope you find it useful.\n  \n    Edit: I just added a feature where you can change the TRANSLATE_TO variable in the subtitler.py file to change the language of the resulting subtitles."
},
{
    "title": "No title",
    "content": "As stated, I created a (mostly) universal file converter. You can select to convert single files and batch files. Handles most video and audio formats. If I am forgetting anything, let me know and I'll see if I can add it!\n  \n    I needed this to batch convert raw MKV files to MP4 that I could use for editing in Premiere Pro. But I also wanted to make it more cause I know I like to convert files a lot.\n  \nGitHub Link to Source Code and instructions on how to run the file. You will need FFMPEG installed, but I have instructions for that as well."
},
{
    "title": "No title",
    "content": "Hi all,\n  \n    I'm currently working on a side project (tplmgr.dev) which is a software template management platform. Think backstage software templates as a standalone service and simpler! I'm aiming to primarily support Cookiecutter templates in the first instance, but I wanted to get some feedback on people's usage of Cookiecutter hooks, seeing as it's going to be quite complex to support this feature. So, do you use Cookiecutter hooks in your templates? If so, can you link to some examples of usage so that I can get a general idea as to whether supporting this should be prioritised.\n  \n    Thanks for reading!"
},
{
    "title": "No title",
    "content": "https://github.com/RClayGates/gmail_auto_label\n\n    I wanted to get the 17,000+ emails in my gmail sorted out. So I made this and I'm so happy with how my project is shaping up! It's my first time ever making a project with anything google api.\n  \n    It goes through the entirety of the gmail account, retrieves the From header for each email. Then creates a dictionary from all the emails, using everything after the @ in the email address.example { \"com\":  [ \"example\", \"info\"], \"org\" : [[ \"School\"], [ \"gov\", \"us\"]] }\n  \n    Then uses that to create nested labels and appropriate filters to add those labels. After those are created it will go back and update every email with the appropriate labels.\n  \n    Any feedback/pointers or even a git pull on making this better would be awesome!"
},
{
    "title": "No title",
    "content": "Tried to put together a document text extraction server using Apache Tika (with ~30 lines of Python code).\n  \n    This can be used to get the text needed for retrieval-augmented generation or to create LLM training datasets (or frankly, anything else that is text-dependent). It worked out pretty well, so thought it would be cool to share: https://blog.min.io/minio-tika-text-extraction/\n\nAn aside on Tika:Apache Tika is time-tested and, by some, considered a legacy toolkit. However, with Tika running as a container and the use of Python bindings, it's possible to get a text extraction experience that is as easy to build with as newer frameworks like Unstructured, but also matches the extraction capability of dedicated extraction models like Nougat. Kind of surprising!\n  \n    Much credit to the tika-python project for making the Python bindings!\n  \nA further aside on object storage:Furthermore, using a backing object store (i.e. MinIO) to hold the source documents is very useful (whether the extracted text is being used for RAG or an LLM training dataset)."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "https://buzzpy.hashnode.dev/analyzing-netflix-data-with-pandas-python\n\n    Analysing Netflix Movie Data 👆\n  \n    This is better: https://buzzpy.hashnode.dev/beginners-guide-to-pandas"
},
{
    "title": "No title",
    "content": "PySimpleGUI, a popular Python GUI library with 13k GitHub stars went closed source / commercial today. Previously it had been licensed under LGPL. I've got no issue with open source devs making money but to changing the license on a library many have contributed to seems to be pretty poor form. This had been a great cross-platform library for beginners."
},
{
    "title": "No title",
    "content": "We have a large report in Word based on several Excel files containing Budget data. We have to generate a PDF report that contains a narrative that is based on the data and we use pandas to manipulate the data - when the data changes, we want the numbers to be updated - for example, I might have a text that says \"The department with the highest increase in budgeting for Staff costs is XYZ with 76.5% increase\". In addition, we will have several data tables presenting the budget data, summarized by different categories, etc. I am looking to replace Word as there is no easy way to insert dynamic data, except for mail merge, which is quite basic. Jupyter Notebook or Quarto would be great choices, but the end users are not programmers - I was wondering whether there are tools to support such a scenario.\n  \n    <<Update>>: Just got to know - https://docxtemplater.com/ and it looks quite close to what I wanted to have. Updating here for the benefit of others."
},
{
    "title": "No title",
    "content": "The project will get Airbnb's information including images, description, price, title ..etcIt also full search given coordinates\n  \nhttps://github.com/johnbalvin/pybnb\n\n    Install:$ pip install gobnbUsage:from gobnb import *data = Get_from_room_url(room_url,currency,\"\")"
},
{
    "title": "No title",
    "content": "Graphyte. is a Python-based graphing application built with PyQt6 and Matplotlib inspired by GeoGebra, designed to provide users with an intuitive interface for creating and visualizing mathematical functions and data sets.\n  \n    Since Graphyte. is a new Graphing software, the list of current features is pretty limited. But I'm focused on implementing more and more features to get it to GeoGebra's level, although that would require much more time.\n  \n    GitHub: https://github.com/rohankishore/Graphyte."
},
{
    "title": "No title",
    "content": "Supabase is looking to sponsor a an experienced python dev tools OSS contributor to close the gap between our javascript client supabase-js and python client supabase-py\n\n    Full details and an application link available here"
},
{
    "title": "No title",
    "content": "Hello! 👋 I've just released logot, a log testing library.\n  \nlogot has a few unique things, such as being logging-framework-agnostic and having support for testing highly concurrent code using threads or async. But those things are relatively niche. What I'd like to show here are a few examples of how it can be a nice caplog replacement for pytest, even in \"normal\" synchronous code.\n  As a caplog replacement\n    Here's a really simple example testing that a piece of code logs as expected:\n  from logot import Logot, logged\n\ndef test_something(logot: Logot) -> None:\n    do_something()\n    logot.assert_logged(logged.info(\"Something was done\"))\n    You'll see a couple of things here. A logot fixture, and a logged API. Use these together to make neat little log assertions. The equivalent code using caplog would be:\n  def test_something(caplog: pytest.LogCaptureFixture) -> None:\n    do_something()\n    assert any(\n        record.levelno == logging.INFO and record.message == \"Something was done\"\n        for record in caplog.records\n    )\n    I think the logot code is clearer, and hopefully you do too! 🤗\n  Log message matching\n    One of logots more useful features is the ability to match log messages using %-style placeholders rather than regex. This syntax was chosen to be as close as possible to the % placeholders used by the stdlib logging library.\n  from logot import Logot, logged\n\ndef test_something(logot: Logot) -> None:\n    do_something()\n    # Match a string placeholder with `%s`.\n    logot.assert_logged(logged.info(\"Something %s done\"))\n    The equivalent using caplog gets pretty verbose:\n  import re\n\ndef test_something(caplog: pytest.LogCaptureFixture) -> None:\n    do_something()\n    assert any(\n        record.levelno == logging.INFO and re.fullmatch(\"Something .*? done\", record.message, re.DOTALL)\n        for record in caplog.records\n    )\n    If your message contains everyday punctuation like ., you have to start worrying about regex escaping too! I hope that %-style message matching gives a clearer, more loggy way of matching log messages.\n  Log pattern matching\n    This feature is generally aimed towards testing code using threads or async, where messages can arrive out-of-order. But it's also useful for testing synchronous code.\n  from logot import Logot, logged\n\ndef test_app(logot: Logot) -> None:\n    do_something()\n    logot.wait_for(logged.info(\"Something happened\") | logged.error(\"Something broke!\"))\n    This example tests whether the INFO log \"Something happened\" or the ERROR \"Something broke!\" was emitted, and passes on either. The equivalent using caplog gets quite long:\n  def test_something(caplog: pytest.LogCaptureFixture) -> None:\n    do_something()\n    assert any(\n        (record.levelno == logging.INFO and record.message == \"Something happened\")\n        or (record.levelno == logging.ERROR and record.message == \"Something broke!\")\n        for record in caplog.records\n    )I hope you like it! ❤️\n    This is only a v1 release, but it's building on a lot of ideas I've been developing in different projects for a while now. I hope you like it, and find it useful.\n  \n    The project documentation is there if you'd like to find out more. 🙇"
},
{
    "title": "No title",
    "content": "Introducing PrimeNote, a full-fledged sticky note-taking application for Windows and Linux. Unleash your creativity and optimize your workflow with an intuitive interface packed with powerful features.\n  \n    PrimeNote empowers you to:\n  \n\n\n    Capture ideas effortlessly in various formats, from plain text and rich text to images and even full-fledged Vim or terminal environments.\n  \n\n\n    Organize your notes impeccably with a folder-based structure for instant retrieval.\n  \n\n\n    Safeguard confidential information with robust Fernet encryption.\n  \n\n\n    Navigate your notes with lightning speed using the built-in keyboard-driven search tool.\n  \n\n\n    Craft a personalized workspace by tailoring the appearance, menus, hotkeys, and mouse events to your preferences.\n  \n\n\n    Transmit content seamlessly in real-time by sharing your note repository on a virtual machine.\n  \n\n\nLanding page (with screenshots)\n  \nGitlab repository\n\nAUR page"
},
{
    "title": "No title",
    "content": "Video: https://youtu.be/m65DbFWFqIs?si=Shelj2rhN9lvNg8j\n\n    Source Code: https://github.com/ahmedfgad/GeneticArtist"
},
{
    "title": "No title",
    "content": "Inspired from CrashCourse AI, I used Python and Pygame to re-create the hunting mini-game from the 1970's Oregon Trail. Then I made a neural network AI that learns to play the game on its own and gets better over successive generations.\n  \nOregon Trail hunting mini-game with neural network AI"
},
{
    "title": "No title",
    "content": "Hello everyone! Hope you're having a great day. I am a civil engineer working as a quantity surveyor and office admin. I recently started learning python and set up multiple automations in my work. Some of them include automating document sorting into our file system, filtering out emails to be sent to subcontractors (could have been fully automated but I kept the last step to be controlled manually for quality control), helper to choose images for monthly report and so on... I know there are many more projects I can do but I am honestly losing motivation when working on them alone and not being able to discuss the ideas with someone. Is there any group of engineers which might enjoy brainstorming together and discussing issues?"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hello world! Just recently, I made a (short, but might be useful) library for interfacing with TuneIn's (non-existent) API. You can check it out at PyPI or GitHub\n\n    My motivation behind this is I wanted to make a radio receiver using a Pi. While slapping TuneIn's website would be sufficient, I wanted it to be \"locally\" (no heavy browser for playing mp3's or aac's)\n  \n    Check it out, and tell me if you have any other questions."
},
{
    "title": "No title",
    "content": "Hi! I analyzed GPS-based data from my Garmin sport watch using the library gpxpy. There is no documentation available, so I relied on docstrings to find the methods I needed.\n  \n    There is a very similar project working with files from Strava, but there most of the stats were calculated directly on the pandas dataframe. I used methods from gpxpy wherever possible.\n  \n    I wrote a blog post about my project and uploaded the code to github.\n  \n    Please let me know what you think!"
},
{
    "title": "No title",
    "content": "One brought me a project for review that uses Websocket. I wanted to play around with it, send some messages, and check if the result matches the expectations. I decided to use the popular app: Postman, but I found out that it now requires registration. So, I had two options:\n  \n\n\n    Spend a minute registering\n  \n\n\n    Spend one evening writing my own alternative\n  \n\n\n    From my point of view the choice is obvious, so I proudly present to you a simple console WebSocket client:\n  \nhttps://github.com/Sets88/wscls\n\n    Please star 🌟 repo if you liked what i created"
},
{
    "title": "No title",
    "content": "Ever since I learned Erlang, and then Elixir, I suffered from “Erlang envy”.\n  \n    A few years ago, I made this project to try to mimic the OTP framework using Python and the Trio async library (IMHO, better alternative to asyncio).\n  \n    Recently, for work, I had to write some Python project that needed multiple isolated components that interacted through message-passing. I reused the patterns I implemented in that triotp library I made.\n  \n    This weekend, I also wrote a blog article to detail a bit more about my reasoning with why those design patterns are nice and IMHO a must have.\n  \nDISCLAIMER: Yes it's a Medium article, I promise it's not a shitty 5 lines article about how print() is a nice function. It's also a \"friend link\" with an auth token to bypass the paywall.\n  \nNB: I didn’t use triotp directly in the code for work, I only extracted what I needed from it. This is because my process registry implementation is weak at best, and I didn’t need a process registry anyway.\n  \n    Link to the project: https://github.com/linkdd/triotp"
},
{
    "title": "No title",
    "content": "I published my second ever tutorial (followed by the first one, a week ago)!\n  \n    \"The Guide to Making Your Django SaaS Business Worldwide (for free)\".\n  \n    Read now, for free, without ads, on my blog:\n  \nhttps://catnotfoundnear.github.io/the-guide-to-making-your-django-saas-business-worldwide-for-free.html\n\n    I will truly appreciate your suggestions or recommendations! Thank you!\n  \n    - Anna Willis (Catnotfoundnear)"
},
{
    "title": "No title",
    "content": "Hi r/python\n\n    I wrote about some of the techniques used in my Toolong project. Hope you find it interesting.\n  \nhttps://textual.textualize.io/blog/2024/02/11/file-magic-with-the-python-standard-library/"
},
{
    "title": "No title",
    "content": "Hi r/python\n\n    I'm the founder of the Textual project. I recently built this app to view log files. It can open log files of any size, and tail in realtime. It has syntax highlighting and a versatile search feature. It can also merge a bunch of log files in to a single view.\n  \n    Here's the repo:\n  \nhttps://github.com/Textualize/toolong\n\n    Happy to answer any questions about Toolong or any of our projects..."
},
{
    "title": "No title",
    "content": "I have tried numpy.linalg.eigvals built with openblas and apple accelerate framework on apple silicon. Both run on multiple threads for very big matrices, by default.\n  \n    I thought the multi-threading is provided by the FORTRAN LAPACK library, and numpy.linalg is simply a wrapper for the FORTRAN library, and once I use LAPACK, I should have similar behaviour.\n  \n    However, when I call LAPACK in rust, although the performance is good (maybe even better than numpy, still need to benchmark carefully), it does not use multiple threads. I have checked that I am also using openblas and accelerate in rust. This seems perplexing.\n  \nCorrection: I said my rust program just uses one thread. This is not entirely accurate. It has many threads, but CPU usage is still exactly one CPU core.\n  \nNOTE: for both rust and numpy, I have made sure they are linked to the accelerate framework. I am aware different implementations of bias can give different performance, but here both should be linked to the same backend, which is why I find perplexing.\n  \n    Isn't numpy.linalg.eigvals nothing more than a wrapper over the FORTRAN library? So how does it manage to use more threads? What is going on here?"
},
{
    "title": "No title",
    "content": "Graphite is a Python-based graphing application built with PyQt6 and Matplotlib inspired by GeoGebra, designed to provide users with an intuitive interface for creating and visualizing mathematical functions and data sets.\n  \n    GitHub: https://github.com/rohankishore/Graphite"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "After a few too many experiences with unmaintained Python projects recently I've been thinking about how one might use GitHub and PyPI organisations to help prevent projects ending up in an unmaintained state due to authors and maintainers eventually leaving the projects.\n  \n    Someone pointed me to The PHP League as an example of a similar group in the PHP world. I'll note a primary difference between what I'm looking for and The PHP League is that they appear to focus on authoring projects where I'd focus on maintaining/ administrating them.\n  \nRather than reinvent the wheel I'm wondering if there's such an organisation that already exists in the Python world?\n\n    I'm aware that we have organisations like:\n  \n\n\n    the PSF, PyPA, PyPI, but they're more suited to large projects that are core to the language\n  \n\n\n    communities / organisations built around popular projects like Django\n  \n\n\n    I'm looking more at projects which are important but not popular or \"feature rich\" enough to build an active community over, but are important to keep maintained because of how much they are used."
},
{
    "title": "No title",
    "content": "Hi all, I created this package a while back and will like to share it since I recently started maintaining it once again.\n  \nhttps://github.com/lvxhnat/pyetfdb-scraper\n\n    It retrieves ETF data from https://etfdb.com in json format. Do give it a look and a star if it is helpful !\n  \n    Here are some fields you can get, for example, top holdings to observe how much your ETF holdings overlap with each other:\n  \n\"top_holdings\": [\n        {\n            \"symbol\": \"AAPL\",\n            \"holding\": \"Apple Inc.\",\n            \"share\": \"7.18%\",\n            \"url\": \"https://etfdb.com/stock/AAPL/\",\n        },\n        {\n            \"symbol\": \"MSFT\",\n            \"holding\": \"Microsoft Corporation\",\n            \"share\": \"6.50%\",\n            \"url\": \"https://etfdb.com/stock/MSFT/\",\n        },\n        {\n            \"symbol\": \"AMZN\",\n            \"holding\": \"Amazon.com, Inc.\",\n            \"share\": \"3.32%\",\n            \"url\": \"https://etfdb.com/stock/AMZN/\",\n        },\n        ...\n    even industries\n  {\n        \"Technology Services\": 21.08,\n        \"Electronic Technology\": 18.45,\n        \"Finance\": 12.34,\n        \"Health Technology\": 9.51,\n        \"Retail Trade\": 7.75,\n        \"Consumer Non-Durables\": 4.48,\n        \"Producer Manufacturing\": 3.61,\n        \"Consumer Services\": 3.4,\n        \"Energy Minerals\": 3.11,\n        \"Commercial Services\": 2.92,\n        \"Utilities\": 2.25,\n        \"Health Services\": 2.21,\n        \"Consumer Durables\": 1.91,\n        \"Process Industries\": 1.78,\n        \"Transportation\": 1.76,\n        \"Communications\": 0.93,\n        \"Distribution Services\": 0.92,\n        \"Industrial Services\": 0.92,\n        \"Non-Energy Minerals\": 0.54,\n        \"CASH\": 0.2,\n}, ..."
},
{
    "title": "No title",
    "content": "I have been working with async iterators a lot recently and one of my bugbears is handling exceptions which are raised in the iteration. For context my use case (fairly common I imagine) is using async iterators over IO streams, eg MQTT or Kafka messages. These systems can potentially throw errors when you receive a message. I like to use the async for interface but it comes with an irritation, it is cumbersome to catch an error in the iteration, then continue iterating. I would like to propose a new feature try async for (and for consistency try for to address this)\n  \n    To illustrate my example imagine we have a mock IO function and I create a class to iterate over some fetched results:\n  \nimport asyncio\nimport random\n\nclass IOError(Exception):\n    pass\n\nasync def io(x):\n    if random.random() > 0.5:\n        raise IOError\n    else:\n        return x    \n\nclass MyIterator:\n    \n    def __init__(self):\n        self.n = 0\n        \n    async def __anext__(self):\n        try:\n            result = await io(self.n)\n        finally:\n            self.n += 1\n        if self.n > 10:\n            raise StopAsyncIteration\n        return result\n        \n    def __aiter__(self):\n        return self\n    So the issue is what if if the IOError happens, but I still want to continue the iteration?\n  \n    One option is to abandon the for loop and use a while loop:\n  \nmy_iter = MyIterator()\nwhile True:\n    try:\n        result = await my_iter.__anext__()\n        print(result)\n    except IOError:\n        pass\n    except StopAsyncIteration:\n        break\n    This works but it's not very pythonic, and removes the point of async for. Another option is to catch the exception externally then continue iterating.\n  \nmy_iter = MyIterator()\nwhile True:\n    try:\n        async for res in my_iter:\n            print(res)\n        else:\n            break\n    except IOError:\n        pass    \n    This to me seems difficult to read and is nested excessively. To solve this we might consider changing the iterator to return a sentinel value instead of raising an exception:\n  class MyIterator:\n    \n    def __init__(self):\n        self.n = 0\n        \n    async def __anext__(self):\n        try:\n            result = await io(self.n)\n        except IOError:\n            return None\n        finally:\n            self.n += 1\n        if self.n > 10:\n            raise StopAsyncIteration\n        return result\n        \n    def __aiter__(self):\n        return self\n    So we can iterate through like so:\n  \nasync for result in MyIterator():\n    if result is not None:\n        print(result)\n    However this is also not very pythonic, as we are meant to use exceptions, not sentinel values to show exceptional conditions.\n  \n    To solve this I would propose a simple syntax extension, allow try before async for or for loops. So going back to the first class which raises the exception in the __anext__ method my example would look like:\n  \ntry async for result in MyIterator():\n    print(result)\nexcept:\n    pass\n    The semantics here are if the __anext__ method raises an exception, it triggers the except branch, and continues with the iteration. This to me seems much cleaner, and it seems fairly intuitive to understand how this syntax works. finally would work in a similar fashion:\n  \ntry async for result in MyIterator():\n    print(result)\nexcept:\n    pass\nfinally:\n   print(\"Always prints!\")\n    What do you think about this new syntax addition? Am I missing something?"
},
{
    "title": "No title",
    "content": "Is it just me or does that seem simply bizzar. Like if you have gcc u can just get so much extra speed from doing that simple trick and as far as I can tell from their cython setup they don't seem to...\n  \n    Why is that?"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hi!\n  \n    I'm part of a bigger team at work and we often touch multiple repositories. We often forget or simply don't add everyone from the team to reviewer list. And if we do add, sometimes that gets lost in the sea of GH notification emails.\n  \n    So I've made a small python app that helps me prioritise reviewing team PRs and gives me a nice overview on what's going on in the team across multiple repos.\n  \n    Maybe it can be of use for you too?\n  \n    You provide list of names and GitHub PAT and pream-team lists all open PRs for each person. Also signalising if the PR is in draft or ready for review state.\n  \n    I'm fairly new to python so any feedback on improving the codebase is more than welcome.\n  \n    Also any ideas on improving the app, issues and/or pull requests are welcome too.\n  \n    GitHub link: https://github.com/NikolaDucak/pream-team"
},
{
    "title": "No title",
    "content": "In the process of looking deeper into making Rye work I am trying to better capture problems people other than me are facing. In particular I'm trying to ensure that Rye can accommodate packages and their dependencies that are problematic today.\n  \n    As a point of reference tensorflow's ecosystem is notoriously hard to support. There are also packages like flash-attn which don't publish wheels but also incorrectly define the build dependencies so that they often cannot be built.\n  \n    Do you have some packages that you often need to use but where you run into problems depending on them? Regardless of which tool you use (Poetry, PDM, Rye, pip etc.)."
},
{
    "title": "No title",
    "content": "i made a PIP Manager + PyPI Search im still new to all this but im interested to know what people think and how i can make it better, thanks.\n  \n    ▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\n  PIP Manager + PyPI Search - https://github.com/pudszttiot/PIP-Manager-PyPI-Search\n    a Python application designed to simplify package management and exploration for Python developers. With a user-friendly interface, it allows for effortless installation, uninstallation, and upgrading of packages via PIP Manager. Additionally, the PyPI Search feature enables quick searches for packages available on the Python Package Index (PyPI). Explore, manage, and stay updated with your Python packages effortlessly with this handy tool."
},
{
    "title": "No title",
    "content": "Memory management is a critical aspect of application lifecycle and developers put efforts to keep the application light.For most cases Memory management is built in and runs silently in the back end.Garbage collector are the heroes responsible for cleaning up runtime memory and ensuring enough Heap Space is available for application’s execution. Without garbage collectors They work routinely to ensure all dangling data is cleaned up and developers can work freely to create extensive applications.This newsletter article aims to discuss the importance and impact of Garbage collectors in an applications’ memory management.\n  \nhttps://www.linkedin.com/pulse/trash-talk-garbage-collection-navjot-bansal-pzxqc%3FtrackingId=i1TOo%252FFaR3%252Bk%252F39C3aJGUw%253D%253D/?trackingId=i1TOo%2FFaR3%2Bk%2F39C3aJGUw%3D%3D"
},
{
    "title": "No title",
    "content": "Hello! 👋 I've just released logot, a log testing library.\n  \nlogot has a few unique things, such as being logging-framework-agnostic and having support for testing highly concurrent code using threads or async. But those things are relatively niche. What I'd like to show here are a few examples of how it can be a nice caplog replacement for pytest, even in \"normal\" synchronous code.\n  As a caplog replacement\n    Here's a really simple example testing that a piece of code logs as expected:\n  from logot import Logot, logged\n\ndef test_something(logot: Logot) -> None:\n    do_something()\n    logot.assert_logged(logged.info(\"Something was done\"))\n    You'll see a couple of things here. A logot fixture, and a logged API. Use these together to make neat little log assertions. The equivalent code using caplog would be:\n  def test_something(caplog: pytest.LogCaptureFixture) -> None:\n    do_something()\n    assert any(\n        record.levelno == logging.INFO and record.message == \"Something was done\"\n        for record in caplog.records\n    )\n    I think the logot code is clearer, and hopefully you do too! 🤗\n  Log message matching\n    One of logots more useful features is the ability to match log messages using %-style placeholders rather than regex. This syntax was chosen to be as close as possible to the % placeholders used by the stdlib logging library.\n  from logot import Logot, logged\n\ndef test_something(logot: Logot) -> None:\n    do_something()\n    # Match a string placeholder with `%s`.\n    logot.assert_logged(logged.info(\"Something %s done\"))\n    The equivalent using caplog gets pretty verbose:\n  import re\n\ndef test_something(caplog: pytest.LogCaptureFixture) -> None:\n    do_something()\n    assert any(\n        record.levelno == logging.INFO and re.fullmatch(\"Something .*? done\", record.message, re.DOTALL)\n        for record in caplog.records\n    )\n    If your message contains everyday punctuation like ., you have to start worrying about regex escaping too! I hope that %-style message matching gives a clearer, more loggy way of matching log messages.\n  Log pattern matching\n    This feature is generally aimed towards testing code using threads or async, where messages can arrive out-of-order. But it's also useful for testing synchronous code.\n  from logot import Logot, logged\n\ndef test_app(logot: Logot) -> None:\n    do_something()\n    logot.wait_for(logged.info(\"Something happened\") | logged.error(\"Something broke!\"))\n    This example tests whether the INFO log \"Something happened\" or the ERROR \"Something broke!\" was emitted, and passes on either. The equivalent using caplog gets quite long:\n  def test_something(caplog: pytest.LogCaptureFixture) -> None:\n    do_something()\n    assert any(\n        (record.levelno == logging.INFO and record.message == \"Something happened\")\n        or (record.levelno == logging.ERROR and record.message == \"Something broke!\")\n        for record in caplog.records\n    )I hope you like it! ❤️\n    This is only a v1 release, but it's building on a lot of ideas I've been developing in different projects for a while now. I hope you like it, and find it useful.\n  \n    The project documentation is there if you'd like to find out more. 🙇"
},
{
    "title": "No title",
    "content": "Introducing PrimeNote, a full-fledged sticky note-taking application for Windows and Linux. Unleash your creativity and optimize your workflow with an intuitive interface packed with powerful features.\n  \n    PrimeNote empowers you to:\n  \n\n\n    Capture ideas effortlessly in various formats, from plain text and rich text to images and even full-fledged Vim or terminal environments.\n  \n\n\n    Organize your notes impeccably with a folder-based structure for instant retrieval.\n  \n\n\n    Safeguard confidential information with robust Fernet encryption.\n  \n\n\n    Navigate your notes with lightning speed using the built-in keyboard-driven search tool.\n  \n\n\n    Craft a personalized workspace by tailoring the appearance, menus, hotkeys, and mouse events to your preferences.\n  \n\n\n    Transmit content seamlessly in real-time by sharing your note repository on a virtual machine.\n  \n\n\nLanding page (with screenshots)\n  \nGitlab repository\n\nAUR page"
},
{
    "title": "No title",
    "content": "Video: https://youtu.be/m65DbFWFqIs?si=Shelj2rhN9lvNg8j\n\n    Source Code: https://github.com/ahmedfgad/GeneticArtist"
},
{
    "title": "No title",
    "content": "Inspired from CrashCourse AI, I used Python and Pygame to re-create the hunting mini-game from the 1970's Oregon Trail. Then I made a neural network AI that learns to play the game on its own and gets better over successive generations.\n  \nOregon Trail hunting mini-game with neural network AI"
},
{
    "title": "No title",
    "content": "Hello everyone! Hope you're having a great day. I am a civil engineer working as a quantity surveyor and office admin. I recently started learning python and set up multiple automations in my work. Some of them include automating document sorting into our file system, filtering out emails to be sent to subcontractors (could have been fully automated but I kept the last step to be controlled manually for quality control), helper to choose images for monthly report and so on... I know there are many more projects I can do but I am honestly losing motivation when working on them alone and not being able to discuss the ideas with someone. Is there any group of engineers which might enjoy brainstorming together and discussing issues?"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hello world! Just recently, I made a (short, but might be useful) library for interfacing with TuneIn's (non-existent) API. You can check it out at PyPI or GitHub\n\n    My motivation behind this is I wanted to make a radio receiver using a Pi. While slapping TuneIn's website would be sufficient, I wanted it to be \"locally\" (no heavy browser for playing mp3's or aac's)\n  \n    Check it out, and tell me if you have any other questions."
},
{
    "title": "No title",
    "content": "Hi! I analyzed GPS-based data from my Garmin sport watch using the library gpxpy. There is no documentation available, so I relied on docstrings to find the methods I needed.\n  \n    There is a very similar project working with files from Strava, but there most of the stats were calculated directly on the pandas dataframe. I used methods from gpxpy wherever possible.\n  \n    I wrote a blog post about my project and uploaded the code to github.\n  \n    Please let me know what you think!"
},
{
    "title": "No title",
    "content": "One brought me a project for review that uses Websocket. I wanted to play around with it, send some messages, and check if the result matches the expectations. I decided to use the popular app: Postman, but I found out that it now requires registration. So, I had two options:\n  \n\n\n    Spend a minute registering\n  \n\n\n    Spend one evening writing my own alternative\n  \n\n\n    From my point of view the choice is obvious, so I proudly present to you a simple console WebSocket client:\n  \nhttps://github.com/Sets88/wscls\n\n    Please star 🌟 repo if you liked what i created"
},
{
    "title": "No title",
    "content": "Ever since I learned Erlang, and then Elixir, I suffered from “Erlang envy”.\n  \n    A few years ago, I made this project to try to mimic the OTP framework using Python and the Trio async library (IMHO, better alternative to asyncio).\n  \n    Recently, for work, I had to write some Python project that needed multiple isolated components that interacted through message-passing. I reused the patterns I implemented in that triotp library I made.\n  \n    This weekend, I also wrote a blog article to detail a bit more about my reasoning with why those design patterns are nice and IMHO a must have.\n  \nDISCLAIMER: Yes it's a Medium article, I promise it's not a shitty 5 lines article about how print() is a nice function. It's also a \"friend link\" with an auth token to bypass the paywall.\n  \nNB: I didn’t use triotp directly in the code for work, I only extracted what I needed from it. This is because my process registry implementation is weak at best, and I didn’t need a process registry anyway.\n  \n    Link to the project: https://github.com/linkdd/triotp"
},
{
    "title": "No title",
    "content": "I published my second ever tutorial (followed by the first one, a week ago)!\n  \n    \"The Guide to Making Your Django SaaS Business Worldwide (for free)\".\n  \n    Read now, for free, without ads, on my blog:\n  \nhttps://catnotfoundnear.github.io/the-guide-to-making-your-django-saas-business-worldwide-for-free.html\n\n    I will truly appreciate your suggestions or recommendations! Thank you!\n  \n    - Anna Willis (Catnotfoundnear)"
},
{
    "title": "No title",
    "content": "Hi r/python\n\n    I wrote about some of the techniques used in my Toolong project. Hope you find it interesting.\n  \nhttps://textual.textualize.io/blog/2024/02/11/file-magic-with-the-python-standard-library/"
},
{
    "title": "No title",
    "content": "Hi r/python\n\n    I'm the founder of the Textual project. I recently built this app to view log files. It can open log files of any size, and tail in realtime. It has syntax highlighting and a versatile search feature. It can also merge a bunch of log files in to a single view.\n  \n    Here's the repo:\n  \nhttps://github.com/Textualize/toolong\n\n    Happy to answer any questions about Toolong or any of our projects..."
},
{
    "title": "No title",
    "content": "I have tried numpy.linalg.eigvals built with openblas and apple accelerate framework on apple silicon. Both run on multiple threads for very big matrices, by default.\n  \n    I thought the multi-threading is provided by the FORTRAN LAPACK library, and numpy.linalg is simply a wrapper for the FORTRAN library, and once I use LAPACK, I should have similar behaviour.\n  \n    However, when I call LAPACK in rust, although the performance is good (maybe even better than numpy, still need to benchmark carefully), it does not use multiple threads. I have checked that I am also using openblas and accelerate in rust. This seems perplexing.\n  \nCorrection: I said my rust program just uses one thread. This is not entirely accurate. It has many threads, but CPU usage is still exactly one CPU core.\n  \nNOTE: for both rust and numpy, I have made sure they are linked to the accelerate framework. I am aware different implementations of bias can give different performance, but here both should be linked to the same backend, which is why I find perplexing.\n  \n    Isn't numpy.linalg.eigvals nothing more than a wrapper over the FORTRAN library? So how does it manage to use more threads? What is going on here?"
},
{
    "title": "No title",
    "content": "Graphite is a Python-based graphing application built with PyQt6 and Matplotlib inspired by GeoGebra, designed to provide users with an intuitive interface for creating and visualizing mathematical functions and data sets.\n  \n    GitHub: https://github.com/rohankishore/Graphite"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "After a few too many experiences with unmaintained Python projects recently I've been thinking about how one might use GitHub and PyPI organisations to help prevent projects ending up in an unmaintained state due to authors and maintainers eventually leaving the projects.\n  \n    Someone pointed me to The PHP League as an example of a similar group in the PHP world. I'll note a primary difference between what I'm looking for and The PHP League is that they appear to focus on authoring projects where I'd focus on maintaining/ administrating them.\n  \nRather than reinvent the wheel I'm wondering if there's such an organisation that already exists in the Python world?\n\n    I'm aware that we have organisations like:\n  \n\n\n    the PSF, PyPA, PyPI, but they're more suited to large projects that are core to the language\n  \n\n\n    communities / organisations built around popular projects like Django\n  \n\n\n    I'm looking more at projects which are important but not popular or \"feature rich\" enough to build an active community over, but are important to keep maintained because of how much they are used."
},
{
    "title": "No title",
    "content": "Hi all, I created this package a while back and will like to share it since I recently started maintaining it once again.\n  \nhttps://github.com/lvxhnat/pyetfdb-scraper\n\n    It retrieves ETF data from https://etfdb.com in json format. Do give it a look and a star if it is helpful !\n  \n    Here are some fields you can get, for example, top holdings to observe how much your ETF holdings overlap with each other:\n  \n\"top_holdings\": [\n        {\n            \"symbol\": \"AAPL\",\n            \"holding\": \"Apple Inc.\",\n            \"share\": \"7.18%\",\n            \"url\": \"https://etfdb.com/stock/AAPL/\",\n        },\n        {\n            \"symbol\": \"MSFT\",\n            \"holding\": \"Microsoft Corporation\",\n            \"share\": \"6.50%\",\n            \"url\": \"https://etfdb.com/stock/MSFT/\",\n        },\n        {\n            \"symbol\": \"AMZN\",\n            \"holding\": \"Amazon.com, Inc.\",\n            \"share\": \"3.32%\",\n            \"url\": \"https://etfdb.com/stock/AMZN/\",\n        },\n        ...\n    even industries\n  {\n        \"Technology Services\": 21.08,\n        \"Electronic Technology\": 18.45,\n        \"Finance\": 12.34,\n        \"Health Technology\": 9.51,\n        \"Retail Trade\": 7.75,\n        \"Consumer Non-Durables\": 4.48,\n        \"Producer Manufacturing\": 3.61,\n        \"Consumer Services\": 3.4,\n        \"Energy Minerals\": 3.11,\n        \"Commercial Services\": 2.92,\n        \"Utilities\": 2.25,\n        \"Health Services\": 2.21,\n        \"Consumer Durables\": 1.91,\n        \"Process Industries\": 1.78,\n        \"Transportation\": 1.76,\n        \"Communications\": 0.93,\n        \"Distribution Services\": 0.92,\n        \"Industrial Services\": 0.92,\n        \"Non-Energy Minerals\": 0.54,\n        \"CASH\": 0.2,\n}, ..."
},
{
    "title": "No title",
    "content": "I have been working with async iterators a lot recently and one of my bugbears is handling exceptions which are raised in the iteration. For context my use case (fairly common I imagine) is using async iterators over IO streams, eg MQTT or Kafka messages. These systems can potentially throw errors when you receive a message. I like to use the async for interface but it comes with an irritation, it is cumbersome to catch an error in the iteration, then continue iterating. I would like to propose a new feature try async for (and for consistency try for to address this)\n  \n    To illustrate my example imagine we have a mock IO function and I create a class to iterate over some fetched results:\n  \nimport asyncio\nimport random\n\nclass IOError(Exception):\n    pass\n\nasync def io(x):\n    if random.random() > 0.5:\n        raise IOError\n    else:\n        return x    \n\nclass MyIterator:\n    \n    def __init__(self):\n        self.n = 0\n        \n    async def __anext__(self):\n        try:\n            result = await io(self.n)\n        finally:\n            self.n += 1\n        if self.n > 10:\n            raise StopAsyncIteration\n        return result\n        \n    def __aiter__(self):\n        return self\n    So the issue is what if if the IOError happens, but I still want to continue the iteration?\n  \n    One option is to abandon the for loop and use a while loop:\n  \nmy_iter = MyIterator()\nwhile True:\n    try:\n        result = await my_iter.__anext__()\n        print(result)\n    except IOError:\n        pass\n    except StopAsyncIteration:\n        break\n    This works but it's not very pythonic, and removes the point of async for. Another option is to catch the exception externally then continue iterating.\n  \nmy_iter = MyIterator()\nwhile True:\n    try:\n        async for res in my_iter:\n            print(res)\n        else:\n            break\n    except IOError:\n        pass    \n    This to me seems difficult to read and is nested excessively. To solve this we might consider changing the iterator to return a sentinel value instead of raising an exception:\n  class MyIterator:\n    \n    def __init__(self):\n        self.n = 0\n        \n    async def __anext__(self):\n        try:\n            result = await io(self.n)\n        except IOError:\n            return None\n        finally:\n            self.n += 1\n        if self.n > 10:\n            raise StopAsyncIteration\n        return result\n        \n    def __aiter__(self):\n        return self\n    So we can iterate through like so:\n  \nasync for result in MyIterator():\n    if result is not None:\n        print(result)\n    However this is also not very pythonic, as we are meant to use exceptions, not sentinel values to show exceptional conditions.\n  \n    To solve this I would propose a simple syntax extension, allow try before async for or for loops. So going back to the first class which raises the exception in the __anext__ method my example would look like:\n  \ntry async for result in MyIterator():\n    print(result)\nexcept:\n    pass\n    The semantics here are if the __anext__ method raises an exception, it triggers the except branch, and continues with the iteration. This to me seems much cleaner, and it seems fairly intuitive to understand how this syntax works. finally would work in a similar fashion:\n  \ntry async for result in MyIterator():\n    print(result)\nexcept:\n    pass\nfinally:\n   print(\"Always prints!\")\n    What do you think about this new syntax addition? Am I missing something?"
},
{
    "title": "No title",
    "content": "Is it just me or does that seem simply bizzar. Like if you have gcc u can just get so much extra speed from doing that simple trick and as far as I can tell from their cython setup they don't seem to...\n  \n    Why is that?"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hi!\n  \n    I'm part of a bigger team at work and we often touch multiple repositories. We often forget or simply don't add everyone from the team to reviewer list. And if we do add, sometimes that gets lost in the sea of GH notification emails.\n  \n    So I've made a small python app that helps me prioritise reviewing team PRs and gives me a nice overview on what's going on in the team across multiple repos.\n  \n    Maybe it can be of use for you too?\n  \n    You provide list of names and GitHub PAT and pream-team lists all open PRs for each person. Also signalising if the PR is in draft or ready for review state.\n  \n    I'm fairly new to python so any feedback on improving the codebase is more than welcome.\n  \n    Also any ideas on improving the app, issues and/or pull requests are welcome too.\n  \n    GitHub link: https://github.com/NikolaDucak/pream-team"
},
{
    "title": "No title",
    "content": "In the process of looking deeper into making Rye work I am trying to better capture problems people other than me are facing. In particular I'm trying to ensure that Rye can accommodate packages and their dependencies that are problematic today.\n  \n    As a point of reference tensorflow's ecosystem is notoriously hard to support. There are also packages like flash-attn which don't publish wheels but also incorrectly define the build dependencies so that they often cannot be built.\n  \n    Do you have some packages that you often need to use but where you run into problems depending on them? Regardless of which tool you use (Poetry, PDM, Rye, pip etc.)."
},
{
    "title": "No title",
    "content": "i made a PIP Manager + PyPI Search im still new to all this but im interested to know what people think and how i can make it better, thanks.\n  \n    ▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\n  PIP Manager + PyPI Search - https://github.com/pudszttiot/PIP-Manager-PyPI-Search\n    a Python application designed to simplify package management and exploration for Python developers. With a user-friendly interface, it allows for effortless installation, uninstallation, and upgrading of packages via PIP Manager. Additionally, the PyPI Search feature enables quick searches for packages available on the Python Package Index (PyPI). Explore, manage, and stay updated with your Python packages effortlessly with this handy tool."
},
{
    "title": "No title",
    "content": "Memory management is a critical aspect of application lifecycle and developers put efforts to keep the application light.For most cases Memory management is built in and runs silently in the back end.Garbage collector are the heroes responsible for cleaning up runtime memory and ensuring enough Heap Space is available for application’s execution. Without garbage collectors They work routinely to ensure all dangling data is cleaned up and developers can work freely to create extensive applications.This newsletter article aims to discuss the importance and impact of Garbage collectors in an applications’ memory management.\n  \nhttps://www.linkedin.com/pulse/trash-talk-garbage-collection-navjot-bansal-pzxqc%3FtrackingId=i1TOo%252FFaR3%252Bk%252F39C3aJGUw%253D%253D/?trackingId=i1TOo%2FFaR3%2Bk%2F39C3aJGUw%3D%3D"
},
{
    "title": "No title",
    "content": "We are building or attempting to build a multi-agent automation framework that is low-code and easy to use. We released the first stable version yesterday.\n  \n    Github - https://github.com/LyzrCore/lyzr-automata\n\n    The idea is to bring in more flexibility for builders to add their own agents in addition to the 'prompt agents' one uses in these agent automation frameworks.\n  \n    Here is the demo video of how we managed to automate 'Newsletter Creation' using Perplexity, GPT4 and Lyzr Automata (the framework that we started building).\n  \nhttps://www.loom.com/share/c5878b106f634b3d9079a9c9b86de93b?sid=c20c03b9-1c8c-4c45-8845-660328c9d846\n\n    What do you think? How can we improve this and make it more versatile, but sticking to the 'low-code' principles?\n  \n    Here is the demo video of how we managed to automate 'Newsletter Creation' using Perplexity, GPT4, and Lyzr Automata (the framework we started building)."
},
{
    "title": "No title",
    "content": "Did you know it takes about 17,000 CPU instructions to print(\"Hello\") in Python? And that it takes ~2 billion of them to import seaborn?\n  \n    I wrote a little blog post on how you can measure this yourself."
},
{
    "title": "No title",
    "content": "Question for ports and adapters architecture experts . Why this DI injector\n  \nhttps://github.com/meadsteve/lagom\n\n    Is not popular ?  Only 230 stars on Github.\n  \n    Uses Python typing to map dependencies, looks much more pythonic than injector or dependency-injector libs.\n  \n    Are there any known problems that solution has or it's only not enough marketing ?\n  \n    If not Lagom than what you recommend ?"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Aura Text is an excellent text/code editor that offers a wide array of essential tools, and is built with PyQt6 and Python.\n  Key Features:\n\n\n    Edit files (duh)\n  \n\n\n    Support up to 30 languages\n  \n\n\n    Autocompletion\n  \n\n\n    Split pane Markdown editor\n  \n\n\n    Terminal with history\n  \n\n\n    Python Console\n  \n\n\n    Plugin support\n  \n\n\n    Extensive theming including Material Theming support\n  \n\n\n    Extremely customisable\n  \n\n\n    GitHub: https://github.com/rohankishore/Aura-Text"
},
{
    "title": "No title",
    "content": "Excited to announce CustomTkinter Snippets v3.0.0! This Visual Studio Code extension turbocharges your Python GUI development with intuitive code snippets. With a wide range of components covered and customization options available, coding sleek GUIs has never been easier.\n  \nWhat's New:\n\n\n\n    Expanded snippet coverage\n  \n\n\n    Enhanced customization options\n  \n\n\n    Bug fixes for smoother development\n  \n\n\nTODO:\n\n\n\n    Addition of special events and binds as snippets\n  \n\n\n    Addition of object based snippets\n  \n\n\n    Give it a try and supercharge your GUI development today!\n  \nLinks:\n\n    Repository Link: CustomTkinter Snippets\n\n    Extension Link: CustomTkinter Snippets VSCode Extension\n\nCredit:\n\n    Credits to Tom Schimansky for the awesome CustomTkinter library!\n  \n    Don't forget to star the repo!\n  \n#Python #GUI #Development #VisualStudioCode"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I really liked the simplicity of the One Billion Row Challenge (1BRC) that took off last month. It was fun to see lots of people apply different tools to the same simple-yet-clear problem “How do you parse, process, and aggregate a large CSV file as quickly as possible?”\n  \n    For fun, my colleagues and I made a One Trillion Row Challenge (1TRC) dataset 🙂. Data lives on S3 in Parquet format (CSV made zero sense here) in a public bucket at s3://coiled-datasets-rp/1trc and is roughly 12 TiB uncompressed.\n  \n    We (the Dask team) were able to complete the TRC query in around six minutes for around $1.10.For more information see this blogpost and this repository\n\n    (Edit: this was taken down originally for having a Medium link.  I've now included an open-access blog link instead)"
},
{
    "title": "No title",
    "content": "I was eagerly waiting for the parallel programming or subinterpreters to do something with it. What were you all expecting and thinking to do with the latest version?"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Get Python 3.12.2 here: https://www.python.org/downloads/release/python-3122/\n\n    Get Python 3.11.8 here: https://www.python.org/downloads/release/python-3118/"
},
{
    "title": "No title",
    "content": "Hi *\n  \n    Some months ago I joined a new company as a search data scientist, and since then I've been working with Solr (a search engine written in Java). Since this wasn't my field of expertise I decided to implement a simple search engine in Python. It's not a production-ready project, but it shows how a search engine works under the hood.\n  \n    You can find the project here. I've also written a post explaining how I've implemented it here.\n  \n    Besides the search engine, the project also includes a FastAPI app that exposes a website allowing users to interact with the search engine.\n  \n    Let me know what you think!"
},
{
    "title": "No title",
    "content": "If you want to learn to code, I've released 2,000 free sign ups for my course following my Automate the Boring Stuff with Python book (each has 1,000 sign ups, use the other one if one is sold out):\n  \n    *The sign ups are all used up, but you can still watch all the videos for free. Read below!\n  \nhttps://udemy.com/course/automate/?couponCode=FEB2024FREE\n\nhttps://udemy.com/course/automate/?couponCode=FEB2024FREE2\n\n    If you are reading this after the sign ups are used up, you can always find the first 15 of the course's 50 videos are free on YouTube if you want to preview them. YOU CAN ALSO WATCH THE VIDEOS WITHOUT SIGNING UP FOR THE COURSE. All of the videos on the course webpage have \"preview\" turned on. Scroll down to find and click \"Expand All Sections\" and then click the preview link. You won't have access to the forums and other materials, but you can watch the videos.\n  \nNOTE: Be sure to BUY the course for $0, and not sign up for Udemy's subscription plan. The subscription plan is free for the first seven days and then they charge you. It's selected by default. If you are on a laptop and can't click the BUY checkbox, try shrinking the browser window. Some have reported it works in mobile view.\n\nI'm also working on another Udemy course that follows my recent book \"Beyond the Basic Stuff with Python\". So far I have the first 15 of the planned 56 videos done. You can watch them for free on YouTube.\n\nFrequently Asked Questions: (read this before posting questions)\n  \n\n\n    This course is for beginners and assumes no previous programming experience, but the second half is useful for experienced programmers who want to learn about various third-party Python modules.\n  \n\n\n    If you don't have time to take the course now, that's fine. Signing up gives you lifetime access so you can work on it at your own pace.\n  \n\n\n    This Udemy course covers roughly the same content as the 1st edition book (the book has a little bit more, but all the basics are covered in the online course), which you can read for free online at https://inventwithpython.com\n\n\n\n    The 2nd edition of Automate the Boring Stuff with Python is free online: https://automatetheboringstuff.com/2e/\n\n\n\n    I do plan on updating the Udemy course, but it'll take a while because I have other book projects I'm working on. If you sign up for this Udemy course, you'll get the updated content automatically once I finish it. It won't be a separate course.\n  \n\n\n    It's totally fine to start on the first edition and then read the second edition later. I'll be writing a blog post to guide first edition readers to the parts of the second edition they should read.\n  \n\n\nYou're not too old to learn to code. You don't need to be \"good at math\" to be good at coding.\n\n\n\n    Signing up is the first step. Actually finishing the course is the next. :) There are several ways to get/stay motivated. I suggest getting a \"gym buddy\" to learn with. Check out r/ProgrammingBuddies"
},
{
    "title": "No title",
    "content": "Here's a video showing how you can create a database, an API and a Web App instantly, using ChatGPT and API Logic Server.  The abbreviated transcript is shown below.  At the end of this article, there's a link for the detailed instructions for running this on your own machine.\n  \n    API Logic Server is an open source Python project that provides Microservice Automation, based on the Flask and SQLAlchemy frameworks.  Create projects with 1 command, and customize them with Python and Rules in your IDE.  Deploy them as standard containers.\n  1. AI: Schema Automation\n    You can start with an existing database, or create a new one with AI, using ChatGPT.  We enter our database description in Natural Language (shown below),  ChatGPT translates it to SQL; we then copy that to our database tool.\n      Create a sqlite database for customers, orders, items and product\n    \n    Hints: use autonum keys, allow nulls, Decimal types, foreign keys, no check constraints.\n\n    Include a notes field for orders.\n\n    Create a few rows of only customer and product data.\n\n    Enforce the Check Credit requirement:\n\n    1. Customer.Balance <= CreditLimit\n    2. Customer.Balance = Sum(Order.AmountTotal where date shipped is null)\n    3. Order.AmountTotal = Sum(Items.Amount)\n    4. Items.Amount = Quantity * UnitPrice\n    5. Store the Items.UnitPrice as a copy from Product.UnitPrice2. Microservice Automation: ApiLogicServer create\n    Given a new or existing database, API Logic Server provides Microservice Automation to create a project.  It’s 1 command…\n  ApiLogicServer create --project_name=sample_ai --db_url=sqlite:///sample_ai.sqlite\n    This creates a project we can open and run in our IDE.\n  \n    Microservice Automation includes App Automation - a Multi-Page, Multi-Table Admin App.\n  \n    It’s a model - no complex UI framework code.  Customize by editing a simple yaml file.\n  \n    Microservice automation has also includes API Automation - a JSON:API.  The API supports related data access, pagination, optimistic locking, filtering, and sorting.\n  \n    Importantly, JSON:APIs are self-serve: API consumers can use Swagger to obtain the data they want  - no server coding is required.\n  \n    That means Custom App Dev is unblocked, day 1. No more waiting on time-consuming, framework-based API development.\n  3. Customize in your IDE: Rules, and Python\n    In minutes, we can begin collaborating with business users with the Admin App.\n  \n    They might uncover a requirement for Check Credit.\n  \n    Instead of 200 lines of code, it’s 5 spreadsheet-like rules that exactly reflect our logic design.  We declare rules using Python, with IDE code completion.\n      \"\"\" Declarative multi-table derivations and constraints,\n        extensible with Python. \n\nUse code completion (Rule.) to declare rules here\n\nCheck Credit - Logic Design (note: translates directly into rules)\n\n1. Customer.Balance <= CreditLimit\n2. Customer.Balance = Sum(Order.AmountTotal where unshipped)\n3. Order.AmountTotal = Sum(Items.Amount)\n4. Items.Amount = Quantity * UnitPrice\n5. Items.UnitPrice = copy from Product\n\"\"\"\n\nRule.constraint(validate=models.Customer, \n    as_condition=lambda row: row.Balance <= row.CreditLimit,\n    error_msg=\"balance ({round(row.Balance, 2)}) exceeds credit ({round(row.CreditLimit, 2)})\")\n\nRule.sum(derive=models.Customer.Balance,     # adjusts...\n    as_sum_of=models.Order.AmountTotal,      # *not* a sql select sum...\n    where=lambda row: row.ShipDate is None)  \n\nRule.sum(derive=models.Order.AmountTotal\n    as_sum_of=models.Item.Amount)\n\nRule.formula(derive=models.Item.Amount, \n    as_expression=lambda row: row.UnitPrice * row.Quantity)\n\nRule.copy(derive=models.Item.UnitPrice,  \n    from_parent=models.Product.UnitPrice)\n    Logic is automatically reused across all relevant Use Cases (add order, reselect products, re-assign orders to different customers, etc), and optimized to minimize SQL.\n  \n    We can also activate security (ApiLogicServer add-security db_url=auth), and add a declarative Grant to filter out inactive customers for the sales role.\n  \n    The system also created a script for image creation, with deployment examples.\n  \n    Test it with the Admin App: to add an Order, and some Items.  Note the automatic Lookup, and Automatic Joins (the app shows Product Name, not ProductId).\n  \n    Debug the multi-table logic in your debugger.  The logic log depicts each rule firing, with multi-row chaining shown by indentation.\n  4. Iterate: Python and Standard Libraries\n    Further collaboration sets the stage for a new iteration – volume discounts for carbon neutral products.  Let’s implement that now.\n  Volume Discounts\n    We use our database tools to add a column.\n  \n    Then, we rebuild the project; our customizations are preserved.\n  \n    We update the logic - we change the amount derivation to test for carbon neutral products, using standard Python:\n      def derive_amount(row: models.Item, old_row: models.Item, logic_row: LogicRow):\n    amount = row.Quantity * row.UnitPrice\n    if row.Product.CarbonNeutral == True and row.Quantity >= 10:\n       amount = amount * Decimal(0.9)  # breakpoint here\n    return amount\n\nRule.formula(derive=models.Item.Amount, calling=derive_amount)\n    We can verify it works by using the Admin app to add a new Item to our Order.\n  \n    Logic execution is automatically ordered, eliminating a major cause of friction in iteration.\n  App Integration\n    We might also want to integrate our microservice…\n  \n\n\n    Provide basic read access for internal applications,\n  \n\n\n    An endpoint to accept orders from B2B partners, and\n  \n\n\n    Logic to send Kafka messages to internal systems.\n  \n\n\n    Internal application requirements are met with API Automation, as we discussed for custom app developers.  Basic internal application integration no longer requires complex framework-based development.\n  \n    We create a new B2B endpoint using standard Flask.  API Logic Server provides RowDictMapping services to transform incoming requests into SQLAlchemy rows.\n  class ServicesEndPoint(safrs.JABase):\n\n@classmethod\n@jsonapi_rpc(http_methods=[\"POST\"])\ndef OrderB2B(self, *args, **kwargs):\n    \"\"\" # yaml creates Swagger description (not shown)\n    \"\"\"\n\n    db = safrs.DB         # Use the safrs.DB, not db!\n    session = db.session  # sqlalchemy.orm.scoping.scoped_session\n\n    order_b2b_def = OrderB2B()\n    request_dict_data = request.json[\"meta\"][\"args\"][\"order\"]\n    sql_alchemy_row = order_b2b_def.dict_to_row(row_dict = \n                          request_dict_data, session = session)\n\n    session.add(sql_alchemy_row)\n    return {\"Thankyou For Your OrderB2B\"}  # automatic commit, which executes transaction logic\n    Our logic is automatically re-used for all updates, whether from the Admin App or the new custom endpoint.  That is why our service implementation is so small.\n  \n    We extend our logic with an event that sends a Kafka message (not shown - about 10 lines of code).  Similar RowDictMapping transforms our rows to Kafka json payloads.\n  \n    Test the integration with Swagger.  The log shows the logic, and the Kafka payload.\n  Summary\n    And there you have it.\n  \nMicroservice Automation creates projects with one command, providing API and App Automation.\n  \n    Customize with Logic Automation, declaring rules to reduce the backend half of your system by 40X.\n  \n    Open source, your IDE, container-based deployment, and all the flexibility of a framework.\n  \n    You can run this on your own machine.  No database to install.  Here's the detailed Tutorial."
},
{
    "title": "No title",
    "content": "Hello, this is a meta-level update regarding the health of r/Python, and a candid call for action of sorts to see what the community at large considers pain points and enhancements they want addressed.\n  \n    I am a moderator here solely because this is one of the 2-3 subreddits I browse every day. I moderate in a way to reflects the train of thought: \"What do I want to see when I open Reddit today and scroll through my feed of cat memes and programming stuff?\"\n  \n    With that being said, personally I really dislike some things that come up each time I open or pass by an r/Python post:\n  \n\n\n    Poorly written Medium articles\n  \n\n\n    expanding to anywhere with paywalled articles\n  \n\n\n\n\n    Most things related to ChatGPT, ML/AI\n  \n\n\n    Everyone, including Bob's uncle, has made some sort of LLM or interface these days...\n  \n\n\n\n\n    Beginner Help\n  \n\n\n    Incorrectly flaired showcases\n  \n\n\n    Everyone thinks their single file, unlinted/untested/undocumented project is an intermediate showcase?\n  \n\n\n    Everyone thinks instead of showcase, their thing is a vital resource and flair it as such.\n  \n\n\n\n\n    ... and probably some more.\n  \n    I see these viewpoints reflected in the comments throughout the various posts here. I may not reply to everything, as my Reddit browsing is limited to bedtime, bathroom time, or 5 minutes on a meeting that I should've been emailed a summary of afterward.. so these thoughts and changes are just my own but shared by most of you (minus a few fanatics)\n  \n    With all of those things mentioned above, it makes r/Python a place I don't want to come to often.. so:\n  \n    The following changes are live and being tested to try and help improve the community health.\n  \n\n\nMedium.com articles are blanket banned.\n  \n\n\n    Showcase flairs have been relegated to a single \"Showcase\" flair that users will pick.\n  \n\n\n    All other showcase flairs have been made mod-only, and 2 new ones have been added:\n  \n\n\n    Advanced Showcase, Invalid Showcase\n  \n\n\n\n\n    To be honest, hand flairing all showcase posts is nonviable.. but when we/I come across a good showcase we may take the liberty of properly marking it.\n  \n\n\n\n\n    Constraints placed on post title\n  \n\n\n    Minimum 15, Max 100\n  \n\n\n    This stems from times people just have a post titled \"check it\", or conversely \"I built a thing whereby we did this cool ML/AI inferencing that did a thing because we are cool look here\" (proceeds to just post a link in the post body, and the title takes up 1/2 of the screen on your phone...)\n  \n\n\n\n\n    (some older changes, but noting them)\n  \n\n\n    Live feed of Python events from Python.org\n\n\n\n    Added new rules #7, #8.. updated existing ones #4, #6\n  \n\n\n\n\n    The follow changes have been live for a few months:\n  \n\n\n    Increased filtering for showcase posts (must include bitbucket/github/gitlab link)\n  \n\n\n    Greatly increased filtering for help-type questions. This might cause your posts to be in the modqueue for a little longer, as we get hit with literally tons of beginner questions even though there are clear rules and posting guidelines that pop up when you make a post that say \"Please ask your questions in r/LearnPython\"\n  \n\n\n\n    Some questions for the community:\n  \n\n\n    What would you like to see?\n  \n\n\n    How can we allow noteworthy ML/AI to be posted, as it relates to Python, but keep the not-so-fitting-of-a-whole-post type things from clogging our feeds? Should we have a megathread?\n  \n\n\n    The daily threads are pretty underutilized. I remove quite a bit of content that is not post-worthy that could go there but it still doesn't get the love it could. If we were to remove it, what should take its place? How can we improve it as is?\n  \n\n\n    Anything else you've been thinking about when browsing r/Python."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "A basic and buggy implementation of an installer built using Tkinter. Check it out on GitHub! PRs welcome ❤️"
},
{
    "title": "No title",
    "content": "There are five major type checkers for Python users: Mypy (PSF?), Pyright (Microsoft), Pyre (Meta), Pytype (Google) and the built-in type checker of PyCharm (JetBrains).\n  \n    According to pypistats.org, Mypy is the most downloaded last month and most popular overall:\n  \n\n\nMypy: 20.4M\n  \n\n\nPyright: 1.5M, albeit just a CLI wrapper.\n  \n\n\nPytype: 632K\n  \n\n\nPyre: 600, and this is not a typo. I guess it is just installed indirectly?\n  \n\n\n    These stats may not reflect the actual usage. I have no experience with Pyre and Pytype and have rarely, if ever, seen anyone using these two. For VSCode users, the go-to extension is Pylance, which ships with Pyright and has 84M installs thus far. Among the two Mypy competitors of it, one is made by Microsoft (127K installs) and one independent (172K installs). The Python Developer Survey 2022 by JetBrains shows that the number of users who use PyCharm as their primary IDE for Python programming is 29%, second only to VSCode (37%). The annual report of the same year says JetBrains have 15.9M users, but the number of PyCharm users or downloads are not mentioned. One popular and currently the only working Mypy plugin has mixed reviews.\n  \n    Personally, I think Pyright is the best type checker. It has support for latest feature, doesn't choke on WIP code and the maintainers are very responsive. Mypy is not as good, but is quite decent. On the contrary, PyCharm's type checker has many major problems. It is either too lenient or just fails to infer the right types most of the times.\n  \n    PyCharm users, how do you or your team type check your code? Do you use one or multiple of the first four type checkers? If so, is it via the CLI or a plugin? Do you just use whatever people around you use? Or do you don't care about type hinting at all?"
},
{
    "title": "No title",
    "content": "I have been working on a rewrite of Polars' string/binary type for the last couple of weeks. This has been a huge refactor (the biggest I have ever done I think). This blog post I want to share the rationale behind the refactor. Pathological cases can now be 2 orders of magnitudes faster, so I think it was worth the eye-strain. :')\n  \nhttps://pola.rs/posts/polars-string-type/"
},
{
    "title": "No title",
    "content": "Hello folks! I have a project that I have been working on for three years that I’d love to show you today called PyPDForm (https://github.com/chinapandaman/PyPDFForm). It is a Python library that specializes in processing PDF forms, with the most outstanding feature being programmatically filling a PDF form by simply feeding a Python dictionary.\n  \n    I used to work at a startup company with Python as our backend stack. We were constantly given paper documents by our clients that we needed to generate into PDFs. We were doing it using reportlab scripts and I quickly found the process tedious and time consuming for more complex PDFs.\n  \n    This is where the idea of this project came from. Instead of writing lengthy and unmaintainable reportlab scripts to generate PDFs, you can just turn any paper document into a PDF form template and PyPDFForm can fill it easily.\n  \n    On top of the GitHub repo, here are some additional resources for this project:\n  \n    PyPi: https://pypi.org/project/PyPDFForm/\n\n    Docs: https://chinapandaman.github.io/PyPDFForm/\n\n    A public speak I did about this project: https://www.youtube.com/watch?v=8t1RdAKwr9w\n\n    I hope you guys find the library helpful for your own PDF generation workflow. Feel free to try it, test it, leave comments or suggestions, and open issues. And of course if you are willing, kindly give me a star on GitHub."
},
{
    "title": "No title",
    "content": "Hi r/Python!\n  \n    Wanted to share something I have been really excited about for a long time and got the time to work on.\n  \nTL;DR - Sharing with you the Serverless Durable Execution Project that I created - Durable\n\n    When building stateful applications like chatbots, data workflows, etc,  we need to use various mechanisms to keep the state of our application. For example, a chatbot that receives a message needs to query past conversations and metadata from a database in order to decide how to respond.\n  \n    What if we could just write a simple loop that will send a message, wait for a response and keep all of the conversation's state in-memory using simple python data types? Of course the problem is that the state may be deleted occasionally - version upgrades, network failures and so on... We don't trust our code and it's state to survive more that a couple of milliseconds, maybe seconds.\n  \n    Durable execution is a way to execute functions in a way that can survive over months and years. You can read about it here. But in short, it keeps a history of events for each functions and replays the events when a function needs to be restored to a previous state.\n  \n    The service that I created, Durable, allows you to write simple python functions that execute on serverless compute and that are inherently Durable (executed with Durable Execution).\n  \n    I have a lot of ideas of how this can be used by developers - personally and professionally. Things like personal finance tracking, TODO list automations and even cloud infrustructure automation and so on.. If you are also interested in this tech please let me know! I would also be glad for feedback on the project.\n  \n    If you got all the way here, you have got to checkout Durable :)\n  \n    Dan"
},
{
    "title": "No title",
    "content": "I work in academia, and plotting is one of the most painful things. Most of the time I end up with multiple virtual environments and files spread across my file system with plots for experiments, presentations, or papers. It is especially inconvenient when I want to reuse some old designs.\n  \n    A couple of weeks ago I found out about Pyodide, which is a Python interpreter compiled to Webassembly that allows to run Python code in the browser, and it supports matplotlib! So I had the idea to create a website similar to Overleaf but for plotting with matplotlib.\n  \n    The website is just a quick prototype, but it works. You can create multiple projects (which are stored in localStorage). When you select a project, there is a code editor and the plot output. The first time you open a project it takes some time to download Python, but the next executions are really fast IMO (just press \"Ctrl/Cmd + S\" inside the editor to execute the code). The cool thing is that with Pyodide everything happens in your browser! No servers are involved in executing your code.\n  \nYou can check it out here: https://matplotlib.online\n\n    Features already present:\n  \n\n\n    Two columns, one for the code, and one for the plot. Much better than google collab (when the code starts to grow)\n  \n\n\n    Import 1-dimensional variables from spreadsheets: We have all the experimental data in spreadsheets, and it's a pain to convert it to Python for plotting. So I implemented a feature where you directly paste one spreadsheet row/column and it converts the data into a Python list.\n  \n\n\n    Vim/emacs/sublime text mode\n  \n\n\n    Things I'm planning to add in the future:\n  \n\n\n    Render the figure in a different window, to allow an efficient use of dual-screen setups.\n  \n\n\n    Allow to optionally create an account to store the scripts in a server (now they reside in the browser storage).\n  \n\n\n    I would love to have a feature where I could paste 2-dimensional data from a spreadsheet and get a matplotlib code/plot, just like you do in Google Docs or Excel.\n  \n\n\n    Collaborative editing would be awesome but I don't think I will be able to implement it soon.\n  \n\n\n    Better design/UX. I'm not a web developer so this is not my strongest skill.\n  \n\n\n    Let me know what do you think about it :)"
},
{
    "title": "No title",
    "content": "Hey guys. I just launched a small open-source project, a FastAPI-based RQ-dashboard.Feel free to check it out at: https://github.com/Hannes221/rq-dashboard-fastFeedback is highly appreciated ⭐The Goal is to make it easier to integrate an RQ Dashboard into FastAPI applications.*The Docker Image has just been launched today."
},
{
    "title": "No title",
    "content": "Twikit https://github.com/d60/twikit\n\nYou can create a twitter bot for free!\n\n    I have created a Twitter API wrapper that works with just a username, email address, and password — no API key required.\n\n    With this library, you can post tweets, search tweets, get trending topics, etc. for free. In addition, it supports both asynchronous and synchronous use, so it can be used in a variety of situations.\n  \n    Please send me your comments and suggestions. Additionally, if you're willing, kindly give me a star on GitHub⭐️."
},
{
    "title": "No title",
    "content": "Maybe it's just me, but I always forget many of the time saving aliases and functions I've added to my dotfiles over the years.  So I built a CLI named Halp to be a TLDR/ManPage for your custom commands. Point it to your dotfiles and it will index them. Then run halp <command> to remind yourself what something does. You can categorize your commands, search them with regex, customize the help text, etc.\n  \n    If you have TLDR installed, it will query TLDR pages for built-in commands. Or, pass in a command with options halp \"sed -Eni\" to get the man pages filtered for the options you provided. This effectively turns Halp into a one-stop-shop for command line help.\n  \n    Github link: https://github.com/natelandau/halp\n\n    Pypi link: https://pypi.org/project/halper/\n\n    Currently only tested on MacOS and Linux using Bash/ZSH."
},
{
    "title": "No title",
    "content": "We are building or attempting to build a multi-agent automation framework that is low-code and easy to use. We released the first stable version yesterday.\n  \n    Github - https://github.com/LyzrCore/lyzr-automata\n\n    The idea is to bring in more flexibility for builders to add their own agents in addition to the 'prompt agents' one uses in these agent automation frameworks.\n  \n    Here is the demo video of how we managed to automate 'Newsletter Creation' using Perplexity, GPT4 and Lyzr Automata (the framework that we started building).\n  \nhttps://www.loom.com/share/c5878b106f634b3d9079a9c9b86de93b?sid=c20c03b9-1c8c-4c45-8845-660328c9d846\n\n    What do you think? How can we improve this and make it more versatile, but sticking to the 'low-code' principles?\n  \n    Here is the demo video of how we managed to automate 'Newsletter Creation' using Perplexity, GPT4, and Lyzr Automata (the framework we started building)."
},
{
    "title": "No title",
    "content": "Did you know it takes about 17,000 CPU instructions to print(\"Hello\") in Python? And that it takes ~2 billion of them to import seaborn?\n  \n    I wrote a little blog post on how you can measure this yourself."
},
{
    "title": "No title",
    "content": "Question for ports and adapters architecture experts . Why this DI injector\n  \nhttps://github.com/meadsteve/lagom\n\n    Is not popular ?  Only 230 stars on Github.\n  \n    Uses Python typing to map dependencies, looks much more pythonic than injector or dependency-injector libs.\n  \n    Are there any known problems that solution has or it's only not enough marketing ?\n  \n    If not Lagom than what you recommend ?"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Aura Text is an excellent text/code editor that offers a wide array of essential tools, and is built with PyQt6 and Python.\n  Key Features:\n\n\n    Edit files (duh)\n  \n\n\n    Support up to 30 languages\n  \n\n\n    Autocompletion\n  \n\n\n    Split pane Markdown editor\n  \n\n\n    Terminal with history\n  \n\n\n    Python Console\n  \n\n\n    Plugin support\n  \n\n\n    Extensive theming including Material Theming support\n  \n\n\n    Extremely customisable\n  \n\n\n    GitHub: https://github.com/rohankishore/Aura-Text"
},
{
    "title": "No title",
    "content": "Excited to announce CustomTkinter Snippets v3.0.0! This Visual Studio Code extension turbocharges your Python GUI development with intuitive code snippets. With a wide range of components covered and customization options available, coding sleek GUIs has never been easier.\n  \nWhat's New:\n\n\n\n    Expanded snippet coverage\n  \n\n\n    Enhanced customization options\n  \n\n\n    Bug fixes for smoother development\n  \n\n\nTODO:\n\n\n\n    Addition of special events and binds as snippets\n  \n\n\n    Addition of object based snippets\n  \n\n\n    Give it a try and supercharge your GUI development today!\n  \nLinks:\n\n    Repository Link: CustomTkinter Snippets\n\n    Extension Link: CustomTkinter Snippets VSCode Extension\n\nCredit:\n\n    Credits to Tom Schimansky for the awesome CustomTkinter library!\n  \n    Don't forget to star the repo!\n  \n#Python #GUI #Development #VisualStudioCode"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I really liked the simplicity of the One Billion Row Challenge (1BRC) that took off last month. It was fun to see lots of people apply different tools to the same simple-yet-clear problem “How do you parse, process, and aggregate a large CSV file as quickly as possible?”\n  \n    For fun, my colleagues and I made a One Trillion Row Challenge (1TRC) dataset 🙂. Data lives on S3 in Parquet format (CSV made zero sense here) in a public bucket at s3://coiled-datasets-rp/1trc and is roughly 12 TiB uncompressed.\n  \n    We (the Dask team) were able to complete the TRC query in around six minutes for around $1.10.For more information see this blogpost and this repository\n\n    (Edit: this was taken down originally for having a Medium link.  I've now included an open-access blog link instead)"
},
{
    "title": "No title",
    "content": "I was eagerly waiting for the parallel programming or subinterpreters to do something with it. What were you all expecting and thinking to do with the latest version?"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Get Python 3.12.2 here: https://www.python.org/downloads/release/python-3122/\n\n    Get Python 3.11.8 here: https://www.python.org/downloads/release/python-3118/"
},
{
    "title": "No title",
    "content": "Hi *\n  \n    Some months ago I joined a new company as a search data scientist, and since then I've been working with Solr (a search engine written in Java). Since this wasn't my field of expertise I decided to implement a simple search engine in Python. It's not a production-ready project, but it shows how a search engine works under the hood.\n  \n    You can find the project here. I've also written a post explaining how I've implemented it here.\n  \n    Besides the search engine, the project also includes a FastAPI app that exposes a website allowing users to interact with the search engine.\n  \n    Let me know what you think!"
},
{
    "title": "No title",
    "content": "If you want to learn to code, I've released 2,000 free sign ups for my course following my Automate the Boring Stuff with Python book (each has 1,000 sign ups, use the other one if one is sold out):\n  \n    *The sign ups are all used up, but you can still watch all the videos for free. Read below!\n  \nhttps://udemy.com/course/automate/?couponCode=FEB2024FREE\n\nhttps://udemy.com/course/automate/?couponCode=FEB2024FREE2\n\n    If you are reading this after the sign ups are used up, you can always find the first 15 of the course's 50 videos are free on YouTube if you want to preview them. YOU CAN ALSO WATCH THE VIDEOS WITHOUT SIGNING UP FOR THE COURSE. All of the videos on the course webpage have \"preview\" turned on. Scroll down to find and click \"Expand All Sections\" and then click the preview link. You won't have access to the forums and other materials, but you can watch the videos.\n  \nNOTE: Be sure to BUY the course for $0, and not sign up for Udemy's subscription plan. The subscription plan is free for the first seven days and then they charge you. It's selected by default. If you are on a laptop and can't click the BUY checkbox, try shrinking the browser window. Some have reported it works in mobile view.\n\nI'm also working on another Udemy course that follows my recent book \"Beyond the Basic Stuff with Python\". So far I have the first 15 of the planned 56 videos done. You can watch them for free on YouTube.\n\nFrequently Asked Questions: (read this before posting questions)\n  \n\n\n    This course is for beginners and assumes no previous programming experience, but the second half is useful for experienced programmers who want to learn about various third-party Python modules.\n  \n\n\n    If you don't have time to take the course now, that's fine. Signing up gives you lifetime access so you can work on it at your own pace.\n  \n\n\n    This Udemy course covers roughly the same content as the 1st edition book (the book has a little bit more, but all the basics are covered in the online course), which you can read for free online at https://inventwithpython.com\n\n\n\n    The 2nd edition of Automate the Boring Stuff with Python is free online: https://automatetheboringstuff.com/2e/\n\n\n\n    I do plan on updating the Udemy course, but it'll take a while because I have other book projects I'm working on. If you sign up for this Udemy course, you'll get the updated content automatically once I finish it. It won't be a separate course.\n  \n\n\n    It's totally fine to start on the first edition and then read the second edition later. I'll be writing a blog post to guide first edition readers to the parts of the second edition they should read.\n  \n\n\nYou're not too old to learn to code. You don't need to be \"good at math\" to be good at coding.\n\n\n\n    Signing up is the first step. Actually finishing the course is the next. :) There are several ways to get/stay motivated. I suggest getting a \"gym buddy\" to learn with. Check out r/ProgrammingBuddies"
},
{
    "title": "No title",
    "content": "Here's a video showing how you can create a database, an API and a Web App instantly, using ChatGPT and API Logic Server.  The abbreviated transcript is shown below.  At the end of this article, there's a link for the detailed instructions for running this on your own machine.\n  \n    API Logic Server is an open source Python project that provides Microservice Automation, based on the Flask and SQLAlchemy frameworks.  Create projects with 1 command, and customize them with Python and Rules in your IDE.  Deploy them as standard containers.\n  1. AI: Schema Automation\n    You can start with an existing database, or create a new one with AI, using ChatGPT.  We enter our database description in Natural Language (shown below),  ChatGPT translates it to SQL; we then copy that to our database tool.\n      Create a sqlite database for customers, orders, items and product\n    \n    Hints: use autonum keys, allow nulls, Decimal types, foreign keys, no check constraints.\n\n    Include a notes field for orders.\n\n    Create a few rows of only customer and product data.\n\n    Enforce the Check Credit requirement:\n\n    1. Customer.Balance <= CreditLimit\n    2. Customer.Balance = Sum(Order.AmountTotal where date shipped is null)\n    3. Order.AmountTotal = Sum(Items.Amount)\n    4. Items.Amount = Quantity * UnitPrice\n    5. Store the Items.UnitPrice as a copy from Product.UnitPrice2. Microservice Automation: ApiLogicServer create\n    Given a new or existing database, API Logic Server provides Microservice Automation to create a project.  It’s 1 command…\n  ApiLogicServer create --project_name=sample_ai --db_url=sqlite:///sample_ai.sqlite\n    This creates a project we can open and run in our IDE.\n  \n    Microservice Automation includes App Automation - a Multi-Page, Multi-Table Admin App.\n  \n    It’s a model - no complex UI framework code.  Customize by editing a simple yaml file.\n  \n    Microservice automation has also includes API Automation - a JSON:API.  The API supports related data access, pagination, optimistic locking, filtering, and sorting.\n  \n    Importantly, JSON:APIs are self-serve: API consumers can use Swagger to obtain the data they want  - no server coding is required.\n  \n    That means Custom App Dev is unblocked, day 1. No more waiting on time-consuming, framework-based API development.\n  3. Customize in your IDE: Rules, and Python\n    In minutes, we can begin collaborating with business users with the Admin App.\n  \n    They might uncover a requirement for Check Credit.\n  \n    Instead of 200 lines of code, it’s 5 spreadsheet-like rules that exactly reflect our logic design.  We declare rules using Python, with IDE code completion.\n      \"\"\" Declarative multi-table derivations and constraints,\n        extensible with Python. \n\nUse code completion (Rule.) to declare rules here\n\nCheck Credit - Logic Design (note: translates directly into rules)\n\n1. Customer.Balance <= CreditLimit\n2. Customer.Balance = Sum(Order.AmountTotal where unshipped)\n3. Order.AmountTotal = Sum(Items.Amount)\n4. Items.Amount = Quantity * UnitPrice\n5. Items.UnitPrice = copy from Product\n\"\"\"\n\nRule.constraint(validate=models.Customer, \n    as_condition=lambda row: row.Balance <= row.CreditLimit,\n    error_msg=\"balance ({round(row.Balance, 2)}) exceeds credit ({round(row.CreditLimit, 2)})\")\n\nRule.sum(derive=models.Customer.Balance,     # adjusts...\n    as_sum_of=models.Order.AmountTotal,      # *not* a sql select sum...\n    where=lambda row: row.ShipDate is None)  \n\nRule.sum(derive=models.Order.AmountTotal\n    as_sum_of=models.Item.Amount)\n\nRule.formula(derive=models.Item.Amount, \n    as_expression=lambda row: row.UnitPrice * row.Quantity)\n\nRule.copy(derive=models.Item.UnitPrice,  \n    from_parent=models.Product.UnitPrice)\n    Logic is automatically reused across all relevant Use Cases (add order, reselect products, re-assign orders to different customers, etc), and optimized to minimize SQL.\n  \n    We can also activate security (ApiLogicServer add-security db_url=auth), and add a declarative Grant to filter out inactive customers for the sales role.\n  \n    The system also created a script for image creation, with deployment examples.\n  \n    Test it with the Admin App: to add an Order, and some Items.  Note the automatic Lookup, and Automatic Joins (the app shows Product Name, not ProductId).\n  \n    Debug the multi-table logic in your debugger.  The logic log depicts each rule firing, with multi-row chaining shown by indentation.\n  4. Iterate: Python and Standard Libraries\n    Further collaboration sets the stage for a new iteration – volume discounts for carbon neutral products.  Let’s implement that now.\n  Volume Discounts\n    We use our database tools to add a column.\n  \n    Then, we rebuild the project; our customizations are preserved.\n  \n    We update the logic - we change the amount derivation to test for carbon neutral products, using standard Python:\n      def derive_amount(row: models.Item, old_row: models.Item, logic_row: LogicRow):\n    amount = row.Quantity * row.UnitPrice\n    if row.Product.CarbonNeutral == True and row.Quantity >= 10:\n       amount = amount * Decimal(0.9)  # breakpoint here\n    return amount\n\nRule.formula(derive=models.Item.Amount, calling=derive_amount)\n    We can verify it works by using the Admin app to add a new Item to our Order.\n  \n    Logic execution is automatically ordered, eliminating a major cause of friction in iteration.\n  App Integration\n    We might also want to integrate our microservice…\n  \n\n\n    Provide basic read access for internal applications,\n  \n\n\n    An endpoint to accept orders from B2B partners, and\n  \n\n\n    Logic to send Kafka messages to internal systems.\n  \n\n\n    Internal application requirements are met with API Automation, as we discussed for custom app developers.  Basic internal application integration no longer requires complex framework-based development.\n  \n    We create a new B2B endpoint using standard Flask.  API Logic Server provides RowDictMapping services to transform incoming requests into SQLAlchemy rows.\n  class ServicesEndPoint(safrs.JABase):\n\n@classmethod\n@jsonapi_rpc(http_methods=[\"POST\"])\ndef OrderB2B(self, *args, **kwargs):\n    \"\"\" # yaml creates Swagger description (not shown)\n    \"\"\"\n\n    db = safrs.DB         # Use the safrs.DB, not db!\n    session = db.session  # sqlalchemy.orm.scoping.scoped_session\n\n    order_b2b_def = OrderB2B()\n    request_dict_data = request.json[\"meta\"][\"args\"][\"order\"]\n    sql_alchemy_row = order_b2b_def.dict_to_row(row_dict = \n                          request_dict_data, session = session)\n\n    session.add(sql_alchemy_row)\n    return {\"Thankyou For Your OrderB2B\"}  # automatic commit, which executes transaction logic\n    Our logic is automatically re-used for all updates, whether from the Admin App or the new custom endpoint.  That is why our service implementation is so small.\n  \n    We extend our logic with an event that sends a Kafka message (not shown - about 10 lines of code).  Similar RowDictMapping transforms our rows to Kafka json payloads.\n  \n    Test the integration with Swagger.  The log shows the logic, and the Kafka payload.\n  Summary\n    And there you have it.\n  \nMicroservice Automation creates projects with one command, providing API and App Automation.\n  \n    Customize with Logic Automation, declaring rules to reduce the backend half of your system by 40X.\n  \n    Open source, your IDE, container-based deployment, and all the flexibility of a framework.\n  \n    You can run this on your own machine.  No database to install.  Here's the detailed Tutorial."
},
{
    "title": "No title",
    "content": "Hello, this is a meta-level update regarding the health of r/Python, and a candid call for action of sorts to see what the community at large considers pain points and enhancements they want addressed.\n  \n    I am a moderator here solely because this is one of the 2-3 subreddits I browse every day. I moderate in a way to reflects the train of thought: \"What do I want to see when I open Reddit today and scroll through my feed of cat memes and programming stuff?\"\n  \n    With that being said, personally I really dislike some things that come up each time I open or pass by an r/Python post:\n  \n\n\n    Poorly written Medium articles\n  \n\n\n    expanding to anywhere with paywalled articles\n  \n\n\n\n\n    Most things related to ChatGPT, ML/AI\n  \n\n\n    Everyone, including Bob's uncle, has made some sort of LLM or interface these days...\n  \n\n\n\n\n    Beginner Help\n  \n\n\n    Incorrectly flaired showcases\n  \n\n\n    Everyone thinks their single file, unlinted/untested/undocumented project is an intermediate showcase?\n  \n\n\n    Everyone thinks instead of showcase, their thing is a vital resource and flair it as such.\n  \n\n\n\n\n    ... and probably some more.\n  \n    I see these viewpoints reflected in the comments throughout the various posts here. I may not reply to everything, as my Reddit browsing is limited to bedtime, bathroom time, or 5 minutes on a meeting that I should've been emailed a summary of afterward.. so these thoughts and changes are just my own but shared by most of you (minus a few fanatics)\n  \n    With all of those things mentioned above, it makes r/Python a place I don't want to come to often.. so:\n  \n    The following changes are live and being tested to try and help improve the community health.\n  \n\n\nMedium.com articles are blanket banned.\n  \n\n\n    Showcase flairs have been relegated to a single \"Showcase\" flair that users will pick.\n  \n\n\n    All other showcase flairs have been made mod-only, and 2 new ones have been added:\n  \n\n\n    Advanced Showcase, Invalid Showcase\n  \n\n\n\n\n    To be honest, hand flairing all showcase posts is nonviable.. but when we/I come across a good showcase we may take the liberty of properly marking it.\n  \n\n\n\n\n    Constraints placed on post title\n  \n\n\n    Minimum 15, Max 100\n  \n\n\n    This stems from times people just have a post titled \"check it\", or conversely \"I built a thing whereby we did this cool ML/AI inferencing that did a thing because we are cool look here\" (proceeds to just post a link in the post body, and the title takes up 1/2 of the screen on your phone...)\n  \n\n\n\n\n    (some older changes, but noting them)\n  \n\n\n    Live feed of Python events from Python.org\n\n\n\n    Added new rules #7, #8.. updated existing ones #4, #6\n  \n\n\n\n\n    The follow changes have been live for a few months:\n  \n\n\n    Increased filtering for showcase posts (must include bitbucket/github/gitlab link)\n  \n\n\n    Greatly increased filtering for help-type questions. This might cause your posts to be in the modqueue for a little longer, as we get hit with literally tons of beginner questions even though there are clear rules and posting guidelines that pop up when you make a post that say \"Please ask your questions in r/LearnPython\"\n  \n\n\n\n    Some questions for the community:\n  \n\n\n    What would you like to see?\n  \n\n\n    How can we allow noteworthy ML/AI to be posted, as it relates to Python, but keep the not-so-fitting-of-a-whole-post type things from clogging our feeds? Should we have a megathread?\n  \n\n\n    The daily threads are pretty underutilized. I remove quite a bit of content that is not post-worthy that could go there but it still doesn't get the love it could. If we were to remove it, what should take its place? How can we improve it as is?\n  \n\n\n    Anything else you've been thinking about when browsing r/Python."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "A basic and buggy implementation of an installer built using Tkinter. Check it out on GitHub! PRs welcome ❤️"
},
{
    "title": "No title",
    "content": "There are five major type checkers for Python users: Mypy (PSF?), Pyright (Microsoft), Pyre (Meta), Pytype (Google) and the built-in type checker of PyCharm (JetBrains).\n  \n    According to pypistats.org, Mypy is the most downloaded last month and most popular overall:\n  \n\n\nMypy: 20.4M\n  \n\n\nPyright: 1.5M, albeit just a CLI wrapper.\n  \n\n\nPytype: 632K\n  \n\n\nPyre: 600, and this is not a typo. I guess it is just installed indirectly?\n  \n\n\n    These stats may not reflect the actual usage. I have no experience with Pyre and Pytype and have rarely, if ever, seen anyone using these two. For VSCode users, the go-to extension is Pylance, which ships with Pyright and has 84M installs thus far. Among the two Mypy competitors of it, one is made by Microsoft (127K installs) and one independent (172K installs). The Python Developer Survey 2022 by JetBrains shows that the number of users who use PyCharm as their primary IDE for Python programming is 29%, second only to VSCode (37%). The annual report of the same year says JetBrains have 15.9M users, but the number of PyCharm users or downloads are not mentioned. One popular and currently the only working Mypy plugin has mixed reviews.\n  \n    Personally, I think Pyright is the best type checker. It has support for latest feature, doesn't choke on WIP code and the maintainers are very responsive. Mypy is not as good, but is quite decent. On the contrary, PyCharm's type checker has many major problems. It is either too lenient or just fails to infer the right types most of the times.\n  \n    PyCharm users, how do you or your team type check your code? Do you use one or multiple of the first four type checkers? If so, is it via the CLI or a plugin? Do you just use whatever people around you use? Or do you don't care about type hinting at all?"
},
{
    "title": "No title",
    "content": "I have been working on a rewrite of Polars' string/binary type for the last couple of weeks. This has been a huge refactor (the biggest I have ever done I think). This blog post I want to share the rationale behind the refactor. Pathological cases can now be 2 orders of magnitudes faster, so I think it was worth the eye-strain. :')\n  \nhttps://pola.rs/posts/polars-string-type/"
},
{
    "title": "No title",
    "content": "Hello folks! I have a project that I have been working on for three years that I’d love to show you today called PyPDForm (https://github.com/chinapandaman/PyPDFForm). It is a Python library that specializes in processing PDF forms, with the most outstanding feature being programmatically filling a PDF form by simply feeding a Python dictionary.\n  \n    I used to work at a startup company with Python as our backend stack. We were constantly given paper documents by our clients that we needed to generate into PDFs. We were doing it using reportlab scripts and I quickly found the process tedious and time consuming for more complex PDFs.\n  \n    This is where the idea of this project came from. Instead of writing lengthy and unmaintainable reportlab scripts to generate PDFs, you can just turn any paper document into a PDF form template and PyPDFForm can fill it easily.\n  \n    On top of the GitHub repo, here are some additional resources for this project:\n  \n    PyPi: https://pypi.org/project/PyPDFForm/\n\n    Docs: https://chinapandaman.github.io/PyPDFForm/\n\n    A public speak I did about this project: https://www.youtube.com/watch?v=8t1RdAKwr9w\n\n    I hope you guys find the library helpful for your own PDF generation workflow. Feel free to try it, test it, leave comments or suggestions, and open issues. And of course if you are willing, kindly give me a star on GitHub."
},
{
    "title": "No title",
    "content": "Hi r/Python!\n  \n    Wanted to share something I have been really excited about for a long time and got the time to work on.\n  \nTL;DR - Sharing with you the Serverless Durable Execution Project that I created - Durable\n\n    When building stateful applications like chatbots, data workflows, etc,  we need to use various mechanisms to keep the state of our application. For example, a chatbot that receives a message needs to query past conversations and metadata from a database in order to decide how to respond.\n  \n    What if we could just write a simple loop that will send a message, wait for a response and keep all of the conversation's state in-memory using simple python data types? Of course the problem is that the state may be deleted occasionally - version upgrades, network failures and so on... We don't trust our code and it's state to survive more that a couple of milliseconds, maybe seconds.\n  \n    Durable execution is a way to execute functions in a way that can survive over months and years. You can read about it here. But in short, it keeps a history of events for each functions and replays the events when a function needs to be restored to a previous state.\n  \n    The service that I created, Durable, allows you to write simple python functions that execute on serverless compute and that are inherently Durable (executed with Durable Execution).\n  \n    I have a lot of ideas of how this can be used by developers - personally and professionally. Things like personal finance tracking, TODO list automations and even cloud infrustructure automation and so on.. If you are also interested in this tech please let me know! I would also be glad for feedback on the project.\n  \n    If you got all the way here, you have got to checkout Durable :)\n  \n    Dan"
},
{
    "title": "No title",
    "content": "I work in academia, and plotting is one of the most painful things. Most of the time I end up with multiple virtual environments and files spread across my file system with plots for experiments, presentations, or papers. It is especially inconvenient when I want to reuse some old designs.\n  \n    A couple of weeks ago I found out about Pyodide, which is a Python interpreter compiled to Webassembly that allows to run Python code in the browser, and it supports matplotlib! So I had the idea to create a website similar to Overleaf but for plotting with matplotlib.\n  \n    The website is just a quick prototype, but it works. You can create multiple projects (which are stored in localStorage). When you select a project, there is a code editor and the plot output. The first time you open a project it takes some time to download Python, but the next executions are really fast IMO (just press \"Ctrl/Cmd + S\" inside the editor to execute the code). The cool thing is that with Pyodide everything happens in your browser! No servers are involved in executing your code.\n  \nYou can check it out here: https://matplotlib.online\n\n    Features already present:\n  \n\n\n    Two columns, one for the code, and one for the plot. Much better than google collab (when the code starts to grow)\n  \n\n\n    Import 1-dimensional variables from spreadsheets: We have all the experimental data in spreadsheets, and it's a pain to convert it to Python for plotting. So I implemented a feature where you directly paste one spreadsheet row/column and it converts the data into a Python list.\n  \n\n\n    Vim/emacs/sublime text mode\n  \n\n\n    Things I'm planning to add in the future:\n  \n\n\n    Render the figure in a different window, to allow an efficient use of dual-screen setups.\n  \n\n\n    Allow to optionally create an account to store the scripts in a server (now they reside in the browser storage).\n  \n\n\n    I would love to have a feature where I could paste 2-dimensional data from a spreadsheet and get a matplotlib code/plot, just like you do in Google Docs or Excel.\n  \n\n\n    Collaborative editing would be awesome but I don't think I will be able to implement it soon.\n  \n\n\n    Better design/UX. I'm not a web developer so this is not my strongest skill.\n  \n\n\n    Let me know what do you think about it :)"
},
{
    "title": "No title",
    "content": "Hey guys. I just launched a small open-source project, a FastAPI-based RQ-dashboard.Feel free to check it out at: https://github.com/Hannes221/rq-dashboard-fastFeedback is highly appreciated ⭐The Goal is to make it easier to integrate an RQ Dashboard into FastAPI applications.*The Docker Image has just been launched today."
},
{
    "title": "No title",
    "content": "Twikit https://github.com/d60/twikit\n\nYou can create a twitter bot for free!\n\n    I have created a Twitter API wrapper that works with just a username, email address, and password — no API key required.\n\n    With this library, you can post tweets, search tweets, get trending topics, etc. for free. In addition, it supports both asynchronous and synchronous use, so it can be used in a variety of situations.\n  \n    Please send me your comments and suggestions. Additionally, if you're willing, kindly give me a star on GitHub⭐️."
},
{
    "title": "No title",
    "content": "Maybe it's just me, but I always forget many of the time saving aliases and functions I've added to my dotfiles over the years.  So I built a CLI named Halp to be a TLDR/ManPage for your custom commands. Point it to your dotfiles and it will index them. Then run halp <command> to remind yourself what something does. You can categorize your commands, search them with regex, customize the help text, etc.\n  \n    If you have TLDR installed, it will query TLDR pages for built-in commands. Or, pass in a command with options halp \"sed -Eni\" to get the man pages filtered for the options you provided. This effectively turns Halp into a one-stop-shop for command line help.\n  \n    Github link: https://github.com/natelandau/halp\n\n    Pypi link: https://pypi.org/project/halper/\n\n    Currently only tested on MacOS and Linux using Bash/ZSH."
},
{
    "title": "No title",
    "content": "At the moment, I create all demos for my web apps by hand, and because I am a very special kind of lazy I invested quite some time to automate this task, using a simple Python script. This script quickly spiraled out of control into a full-blown browser testing and capturing framework.The first prototype of my new tool is now able to generate video clips like this, fully automated \\o/\n  \nhttps://florianscherf.de/blog/2024/02/05/how-to-write-your-own-browser-testing-framework/\n\nhttps://github.com/fscherf/milan"
},
{
    "title": "No title",
    "content": "https://github.com/bitplane/ienv\n\n    Does exactly what it says in the disclaimer; reduce venv sizes by recklessly replacing all the files with symlinks. (I as in Roman numeral for 1, the other letters were taken)\n  \n    A simple and effective tool that might cause you more trouble than it saves you, but it might get you out of a tough disk space situation.\n  \n    If it breaks your environments then it's your fault, but if it saves you gigs of disk space then I'll take full credit up until the moment you realise it caused problems.\n  \n    works_on_my_machine.jpg\n  \n    Readme follows:\n  ienv\n!!WARNING!! THIS IS A ONE WAY PROCESS !!WARNING!!\n\n    Have you got 30GB of SciPy on your disk because every time someone wants to add two numbers together they install a whole lab on your machine? Are your fifty copies of PyTorch and TensorFlow weighing heavy on your SSD?\n  \n    Why not throw caution to the wind and replace everyhing in the site-packages dir with symlinks? It's not like you're going to need them anyway. And nobody will ever write to them and mess up every venv on your machine. Right?\n  \n!!WARNING!! THIS IS RECKLESS AND STUPID !!WARNING!!\nUsagepip install ienv\nienv .venv\nienv some/other/venvRecovery\nPull requests welcome!\n\n    All the files are there, I've just not written anything to bring them back yet. Ever, probably.\n  Credits\n    Mostly written by ChatGPT just to see if it could do it. With a bit of guidance it actually could, but it can't learn like that so it's like a student that nods along and you think it's listening and it's really just playing along and tricking you into doing its homework. But to be honest it was either that or copilot anyway.\n  License\n    They say you get what you pay for, sometimes less. This is one of those times. As free software distributed under the WTFPL (with one additional clause); this is one of the times when you pay for what you get."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "If this is \"help\" and belongs in LearnPython, forgive me.  I think I know a fair bit about Python, but this behavior of a standard library module is puzzling to me.\n  \n    Test code:\n  import xml.etree.ElementTree as ET\n\nXMLSTR=\"\"\"\n<r>\n   root text1\n   <c1>c1 text</c1>\n   root text 2\n</r>\"\"\"\n\nprint(f'XML:{XMLSTR}')\nroot=ET.fromstring(XMLSTR)\n\nprint(f'{root.text=} {root.tail=}')\nc1=root.find('c1')\nprint(f'{c1.text=} {c1.tail=}')\n    Results:\n  XML:\n<r>\n   root text1\n   <c1>c1 text</c1>\n   root text 2\n</r>\nroot.text='\\n   root text1\\n   ' root.tail=None\nc1.text='c1 text' c1.tail='\\n   root text 2\\n'\n    It's surprising to me that the 'root text 2' string is apparently not associated with the root node.  Does this surprise anyone else?  Am I expecting the wrong thing from xml.etree?"
},
{
    "title": "No title",
    "content": "Hi you all! Im working on a RD DICOM file, and I cannot exportexport a data, in DICOM format wich contains data likewise maximun dose slide, dose value, slide dimensiom, etc. My idea is to explort a single frame  and perform gamma 2D comparison. Some idea ?"
},
{
    "title": "No title",
    "content": "Hello,\n  \n    I've recently released Quart-Tasks which is a Quart extension that provides scheduled background tasks,\n  from quart import Quart\nfrom quart_tasks import QuartTasks\n\napp = Quart(__name__)\ntasks = QuartTasks(app)\n\n@tasks.cron(\"*/5 * * * *\")  # every 5 minutes\nasync def infrequent_task():\n    ...  # Do something\n\n@tasks.cron(\n    seconds=\"*1/0\",  # every 10 seconds\n    minutes=\"*\",\n    hours=\"*\",\n    day_of_month=\"*\",\n    month=\"*\",\n    day_of_week=\"*\",\n)\nasync def frequent_task():\n    ...  # Do something\n\n@tasks.periodic(timedelta(seconds=10))\nasync def regular_task():\n    ...  # Do Something\n    I've also recorded a tutorial showing how to use it here.\n  \n    Questions, thoughts, and feedback welcome!"
},
{
    "title": "No title",
    "content": "This app has a simple purpose, to create QR codes with logos/images inside them. It is then integrated into a UI made with CustomTkinter. You can customise the color of the QR code, image, and background color.\n  \n    GitHub: https://github.com/rohankishore/QrGen"
},
{
    "title": "No title",
    "content": "Hey! I recently wanted to start sharing my knowledge of Python, and the struggles/joy I have with it.\n  \n    Check out this article showing how to implement the Factory and Repository patterns in Python with SQLAlchemy and Pydantic.\n  \n    It's a bit advanced, drawing from my freelancing experience and having to build the same applications over and over again, but I'm sure there's something for everyone. I hope your coding life easier! 🎥💻"
},
{
    "title": "No title",
    "content": "I created a toolkit for python developers I named it Dev, Dev is a Python package that integrates seamlessly with your code editor and provides you with a range of functionalities, such as:\n  \n    - AI-powered Debugging: Stuck on a coding bug? Dev can analyze your code, identify the error, and suggest corrections.\n  \n    - Automated README Generation: Forget manually writing READMEs! Dev can automatically generate a comprehensive README file for your project.\n  \n    - Creative Code Generation: Need a jumpstart on your code? Dev can generate code snippets based on your instructions, offering you creative and effective solutions.\n  \n    - In-depth Code Review: Unsure about your code's quality? Dev provides a detailed code review, highlighting potential improvements, optimizations, and areas for consideration.\n  \n    - Seamless Git Integration: Dev simplifies project management by automating Git commands. You can easily create and upload repositories, rename existing ones, and push your code with just a few function calls.\n  \n    - And much more!\n  \n    Dev is your comprehensive coding companion for python developers looking for a productivity boost or a beginner seeking guidance, Dev has something to offer. Embrace the power of AI and watch your coding workflow reach new heights of efficiency and effectiveness.\n  \n    To get started with Dev, visit the GitHub repository and follow the installation instructions: https://github.com/GitCoder052023/Dev.git"
},
{
    "title": "No title",
    "content": "I've just released the newer version of pydevd (available in PyDev: https://www.pydev.org/ and LiClipse: https://www.liclipse.com/) which now makes use of the improvements from PEP 669.\n  \n    See: https://pydev.blogspot.com/2024/02/pydev-debugger-and-sysmonitoring-pep.html for more details."
},
{
    "title": "No title",
    "content": "In the same way global variables pollute the global environment, local variables without any scoping pollute the local environment.\n  To split or not to split\n    That's less of a problem because functions are usually short, but \"usually\" is not \"always\".\n  \n    Splitting longer functions at all costs is a mistake. For instance, I have a longish function which generates a textual description of a certain object. The function has the following structure:\n  def get_text(self, ...) -> str:\n    text = TextBuilder()\n    text.add_block(...)\n    ...\n    text.add_span(...)\n    ...\n    text.add_block(...)\n    ...\n    text.newline()\n    ...\n    return str(text)\n    Splitting such a function wouldn't help because there are no particular abstractions to factor out. The rule  according to which a function should've maximum N LOCs is superficial. A more sensible rule is that a function should only do one thing. Often times you need less than 20 lines, but occasionally 50 or more.\n  \nA function that does half or less of one thing is worse than a function that does more than one thing.\n\n    That's my opinion, at least. As an auditor, I've noticed that splitting long functions at all costs may result in bad abstractions, messy interfaces (i.e. long complicated function signatures), and, ultimately, subtle bugs.\n  Scopes are your friends\n    I've recently come across some code with a scope-related mistake. Here's the pattern:\n  def f(...):\n    text = ...\n    ...\n    for ...:\n       match ...:\n           case ...:\n               ...\n           case ...:\n               ...\n           case text:\n               ...\n    ...\n    Despite the nesting, that case text (or, equivalently, case _ as text) inadvertently overwrites the outer text. There's no shadowing at all.\n  \n    Scoping offers encapsulation at the function level. When I write\n  for (int i = 0; i < 56; ++i) {\n    ...\n}\n    I know that the i introduced by the for loop is internal to the loop. I don't have to know whether there's another i, outside of the loop, that I might overwrite.\n  \n    In C++ I can even create naked scopes like this:\n  {\n    int i;\n    ...\n}\n    Even in C++, people don't rely on scopes as much as I'd like.\n  \n    I claim that:\n  \nScopes localize reasoning, i.e. they reduce the amount of surrounding code one needs to keep in mind to understand one line of code.\nWorkaround\n    Create internal functions! The problem with functions, though, is that they're not just scopes, but have deeper meaning. As a consequence, I use them only when I can factor out some sub-task. Using them only to localize some variables doesn't seem worth it. I usually define them right at the start of the outer function:\n  def f(...):\n    def g1(...):\n        ...\n\n    def g2(...):\n        ...\n\n    ...\n    g1(...)\n    ...\n    g2(...)\n    As I said, this is not real scoping, but it does help simplifying unsplittable functions."
},
{
    "title": "No title",
    "content": "Hi!\n  \n    After more than a year of working on my new (and first) Open Source application on technical ear training, it's finally out!\n  \n    Its goal is to help audio people (professionals & hobbyists), musicians, etc. develop and master the ability to recognize frequency bands by ear. Perfect for students and educators in music and sound industry.\n  \n    It works with Python 3.9, 3.10 on Windows and macOS. I have built the binaries with PyInstaller, which work/have been tested with Windows 10, 11 (x64) and macOS 11 or higher (Intel processors only). I used PyQt6 for GUI, Pedalboard (from Spotify) and numpy libraries for audio processing.\n  \nSee app description here\n\nGitHub Repository\n\nDownload Page\n\n    I will be grateful for your feedback!"
},
{
    "title": "No title",
    "content": "Quiz: what does the following program output? Why?\n  def foo(x):\n    try:\n        yield\n        print('hi')\n    finally:\n        print(x)\n\n\ndef bar(x):\n    f = foo(x)\n    next(f)\n    return f\n\n\na = bar(1)\nbar(2)\n    I was reading through the original PEP that introduced generators and yield way back in Python 2.2 just for fun: https://peps.python.org/pep-0255/\n\n    And came across this:\n  \n\n    Restriction: A yield statement is not allowed in the try clause of a try/finally construct. The difficulty is that there’s no guarantee the generator will ever be resumed, hence no guarantee that the finally block will ever get executed; that’s too much a violation of finally’s purpose to bear.\n  \n\n    So naturally I tried this out. In hindsight it feels obvious that this quote is outdated, because nowadays try/yield/finally is extremely common in functions decorated with @contextlib.contextmanager. But it's still interesting to see how the problem above was handled so that finally can continue to 'guarantee' that it gets executed even if the generator isn't resumed. I don't think I'd have been able to answer the quiz above correctly.\n  \n    The answer, in case you haven't checked for yourself, is that (in both 2.7 and 3.12 and presumably everything in between) the output is:\n  2\n1\n    I haven't verified, but I think the reason this happens is:\n  \n\n\na = bar(1) runs. next(f) advances the generator foo(1) up to the yield. Without next(f) (or with yield before try), the foo frame wouldn't pause inside the try block and the rest wouldn't happen.\n  \n\n\nreturn f assigns foo(1) to a. Python still has hope that the generator will be resumed because maybe something will advance a again, so nothing happens.\n  \n\n\nbar(2) runs. The process in step 1 happens again.\n  \n\n\n    After return f with bar(2) being assigned to nothing, the generator foo(2) has no references. Before garbage collection can completely deallocate it from memory, Python sees that it was paused inside a try with a finally. It makes sure that finally clause runs, skipping print('hi') but running print(x) with x being 2.\n  \n\n\n    The entire program ends. Python accepts that nothing will resume a (i.e. foo(1)) and so print(x) with x being 1 happens, just as in step 4."
},
{
    "title": "No title",
    "content": "Wanted to share a project I've been working on.  https://github.com/GilbN/Simple-TOML-Configurator\n\n    It's a library for managing configuration values in your python app.\n  \n    I needed to change config values on the fly in my Flask app, so I created this where I could use my api to update configuration values that my backend uses.\n  \n    Some features:\n  \n\n\nTOML File Storage: Configuration settings are stored in TOML files, a popular human-readable format. This enables developers to review, modify, and track configuration changes easily.\n  \n\n\nAttribute-Based Access: Accessing configuration values is straightforward, thanks to the attribute-based approach. Settings can be accessed and updated as attributes, making it convenient for both reading and modifying values.\n  \n\n\nEnvironment Variable Support: Configuration values are automatically set as environment variables, making it easier to use the configuration values in your application. Environment variable are set as uppercase. e.g. APP_HOST and APP_PORT or PROJECT_APP_HOST and PROJECT_APP_PORT if env_prefix is set to \"project\". This also works for nested values. ex: TABLE_KEY_LEVEL1_KEY_LEVEL2_KEY. This works for any level of nesting.Environment variables set before the configuration is loaded will not be overwritten, but instead will overwrite the existing config value.\n\n\n\nDefault Values:  Define default values for various configuration sections and keys. The library automatically incorporates new values and manages the removal of outdated ones.\n  \n\n\n    Usage examples:\n  \nhttps://gilbn.github.io/Simple-TOML-Configurator/latest/usage-examples/\n\n    Example using Flask:\n  \nhttps://gilbn.github.io/Simple-TOML-Configurator/latest/flask-simple-example/\n\n    Here is a quick example:\n  import os\nfrom simple_toml_configurator import Configuration\n\n# Define default configuration values\ndefault_config = {\n    \"app\": {\n        \"ip\": \"0.0.0.0\",\n        \"host\": \"\",\n        \"port\": 5000,\n        \"upload_folder\": \"uploads\",\n    },\n    \"mysql\": {\n        \"user\": \"root\",\n        \"password\": \"root\",\n        \"databases\": {\n            \"prod\": \"db1\",\n            \"dev\": \"db2\",\n            },\n    }\n}\n\n# Set environment variables\nos.environ[\"PROJECT_APP_UPLOAD_FOLDER\"] = \"overridden_uploads\"\n\n# Initialize the Simple TOML Configurator\nsettings = Configuration(config_path=\"config\", defaults=default_config, config_file_name=\"app_config\", env_prefix=\"project\")\n# Creates an `app_config.toml` file in the `config` folder at the current working directory.\n\n# Access and update configuration values\nprint(settings.app.ip)  # Output: '0.0.0.0'\nsettings.app.ip = \"1.2.3.4\"\nsettings.update()\nprint(settings.app_ip)  # Output: '1.2.3.4'\n\n# Access nested configuration values\nprint(settings.mysql.databases.prod)  # Output: 'db1'\nsettings.mysql.databases.prod = 'new_value'\nsettings.update()\nprint(settings.mysql.databases.prod)  # Output: 'new_value'\n\n# Access and update configuration values\nprint(settings.app_ip)  # Output: '1.2.3.4'\nsettings.update_config({\"app_ip\": \"1.1.1.1\"})\nprint(settings.app_ip)  # Output: '1.1.1.1'\n\n# Access all settings as a dictionary\nall_settings = settings.get_settings()\nprint(all_settings)\n# Output: {'app_ip': '1.1.1.1', 'app_host': '', 'app_port': 5000, 'app_upload_folder': 'overridden_uploads', 'mysql_user': 'root', 'mysql_password': 'root', 'mysql_databases': {'prod': 'new_value', 'dev': 'db2'}}\n\n# Modify values directly in the config dictionary\nsettings.config[\"mysql\"][\"databases\"][\"prod\"] = \"db3\"\nsettings.update()\nprint(settings.mysql_databases[\"prod\"])  # Output: 'db3'\n\n# Access environment variables\nprint(os.environ[\"PROJECT_MYSQL_DATABASES_PROD\"])  # Output: 'db3'\nprint(os.environ[\"PROJECT_APP_UPLOAD_FOLDER\"])  # Output: 'overridden_uploads'\napp_config.toml contents\n[app]\nip = \"1.1.1.1\"\nhost = \"\"\nport = 5000\nupload_folder = \"overridden_uploads\"\n\n[mysql]\nuser = \"root\"\npassword = \"root\"\n\n[mysql.databases]\nprod = \"db3\"\ndev = \"db2\""
},
{
    "title": "No title",
    "content": "The Cryptic Symphony: Poems and Keys\n\nEncrypted in Python (AES-256):\ndef whisper_behind_veils(message):\n  \"\"\"A message cloaked in secrecy, yearning to be heard.\"\"\"\n  print(f\"Encoding: {message.encode('base64')}\")\n  # Symbolic representation of encryption (replace with actual code if possible)\n  print(\"Weaving whispers into цифровые кружева...\")\n  print(\"Message encoded, locked with a ключ of 256 bits.\")\n  return \"….\"\n\nsecret_message = whisper_behind_veils(\"Alan Turing once said, 'The important thing is not to stop questioning.'\")\nprint(secret_message)\nEncrypted in Ruby (RSA):\ndef heartsong_in_ciphertext(soul_whisper):\n  \"\"\"A yearning for connection, veiled in the language of math.\"\"\"\n  song = soul_whisper.gsub(/[bcdfghjklmnpqrstvwxyz]/, \"*\")\n  # Symbolic representation of encryption (replace with actual code if possible)\n  print(\"Encrypting with the power of prime numbers...\")\n  print(\"Heart's song transformed, a cryptic melody...\")\n  return song\n\nhidden_verse = heartsong_in_ciphertext(\"I carry your heart with me (I carry it in my heart). - Pablo Neruda\")\nprint(hidden_verse)\nThe Decoder's Key:\n\n    A hidden melody whispers through the code, Two poems veiled, their secrets to be bestowed.\n  \nKey #1: Python's Cipher\n\n    Within the whispers, цифры hold the clue, A number's strength, 256, rings true. Search the code, where shadows gently creep, For a цифровое слово, slumbering deep.\n  \nKey #2: Ruby's Enigma\n\n    A heartsong transformed, its essence concealed, By symbols arcane, emotions revealed. Prime numbers dance, a cryptic ballet, Their whispers unfold, if you know the way.\n  \n    Seek patterns hidden, where stars align, Letters replaced, a message to find.\n  \nUnveiling the Secrets:\n\n    With keys in hand, the journey unfolds, Through Python's whispers and Ruby's untold.\n  \nPython's Revelation:\n\n    Beneath the цифровой cloak, a quote takes flight, A Turing whisper, bathed in digital light. \"The important thing is not to stop questioning,\" it cries, A call to explore, with curious eyes.\n  \nRuby's Confession:\n\n    The hidden verse emerges, a love untold, Neruda's words, in symbols bold. \"I carry your heart with me,\" the message sighs, A connection revealed, beneath starry skies.\n  \nThe Symphony's End:\n\n    Two languages sing, their voices entwined, In code and emotion, secrets enshrined. A journey of discovery, a message conveyed, The beauty of connection, forever displayed."
},
{
    "title": "No title",
    "content": "As of late I have a ton of new Open Source Components & Systems that I've worked on and here's one that finally made it through 😅 The best part of it all is I got to create my own term Object-Binary-Mapper(OBM). Any ways, Renity is a pure Python Binary Protocol Buffer with an Interface similar to popular ODM(s) and I hope to extend it End-to-End eventually with the help of the community, check out the release on Pypi!We encourage all contributors to reach out for work reference's. We're here to help and are available for any inquiries regarding our contributors!\n  \n    Links:\n  \nRenity @ Github\n\nRenity @ Pypi"
},
{
    "title": "No title",
    "content": "Smassh is a TUI based typing test application inspired by MonkeyType-- A very popular online web-based typing applicationSmassh tries to be a full fledged typing test experience but not missing out on looks and feel!There is a lot of room for improvements/additions and I am open to contributions and suggestions!Github: https://github.com/kraanzu/smasshThank you! <3"
},
{
    "title": "No title",
    "content": "I made a little fun project: https://github.com/jabbalaci/keysound . When you press a button on the keyboard, an audio file (a click sound) is played. I also included two sample files in mp3 that you can easily check out."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "A new PyPI package for training sentence embedding models in just 2 lines.\n    The acquisition of sentence embeddings often necessitates a substantial volume of labeled data. However, in many cases and fields, labeled data is rarely accessible, and the procurement of such data is costly. In this project, we employ an unsupervised process grounded in pre-trained Transformers-based Sequential Denoising Auto-Encoder (TSDAE), introduced by the Ubiquitous Knowledge Processing Lab of Darmstadt, which can realize a performance level reaching 93.1% of in-domain supervised methodologies.\n  \n    The TSDAE schema comprises two components: an encoder and a decoder. Throughout the training process, TSDAE translates tainted sentences into uniform-sized vectors, necessitating the decoder to reconstruct the original sentences utilizing this sentence embedding. For good reconstruction quality, the semantics must be captured well in the sentence embeddings from the encoder. Subsequently, during inference, the encoder is solely utilized to form sentence embeddings.\n  \nPyPI url : https://pypi.org/project/tsdae\n\nGitHub : https://github.com/louisbrulenaudet/tsdae\n\n    Installation :\n  pip3 install tsdae nltk datasets sentence-transformers torch\n    Python code :\n  from tsdae import TSDAE\n\n# Initialize an instance of TSDAE\ninstance = TSDAE()\n\n# Load a dataset\ntrain_dataset = instance.load_dataset_from_hf(\n    dataset=\"louisbrulenaudet/cgi\"\n)\n\n# Train the model with the dataset\nmodel = instance.train(\n    train_dataset=train_dataset,\n    model_name=\"bert-base-multilingual-uncased\",\n    column=\"output\",\n    output_path=\"output/tsdae-lemon-mbert-base\"\n)"
},
{
    "title": "No title",
    "content": "Hello everyone,\n  \n    I'd like to share with you my first library, designed to automate SAP tasks. It's meant to streamline the repetitive tasks, it can navigate, click, write, select and more.\n  \n    Example use would be a robot that launches SAP, open a transaction and fills in data in a table, fills other text fields, unchecks a checkbox, saves it and read the generated number from a status bar.\n  \n    Initially I have created this library for personal use -- employing it as robot that runs on a schedule. Now I want to open it up for others.\n  \n    To get the SAP paths -- elements, I use a Scripting tracker (https://tracker.stschnell.de/) or I just record a Sap script and use elements from a file created.\n  \n    Please feel free to give it a try and don't hesitate to provide feedback after testing it.\n  \nGithub: https://github.com/kamildemocko/PySapScript\n\nPypi: https://pypi.org/project/pysapscript/\n\npip install pysapscript\n\n    Thank you!"
},
{
    "title": "No title",
    "content": "I had some thorny math to debug today for a client and realized I could probably use a Rich Table to better visualize my values.\n  \n    I wrote up a short blog post  how using tabular output and some fun colors helped the debugging process both in real terms but also in terms of fun and motivation."
},
{
    "title": "No title",
    "content": "I just released MakrellPy, a programming language that compiles to Python AST. It's part of the Makrell language family. Blurb from the home page:\n  \n\n    Makrell is a family of programming languages implemented in Python. It consists currently of these languages:\n  \n\n\nMakrellPy, a general-purpose, functional programming language with two-way Python interoperability, metaprogramming support and simple syntax.\n  \nMRON (Makrell Object Notation), a lightweight alternative to JSON.\n  \nMRML (Makrell Markup Language), a lightweight alternative to XML and HTML.\n  \nMakrell Base Format (MBF), a simple data format that forms the basis for both MakrellPy, MRON and MRML.\n  \n    The project is in an early stage of development and is not yet ready for production use.\n  \n\n    GitHub page: https://github.com/hcholm/makrell-py\n\n    Visual Studio Code extension with syntax highlighting and basic diagnostics using the Language Server Protocol: https://marketplace.visualstudio.com/items?itemName=hcholm.vscode-makrell\n\n    Comments are welcome!"
},
{
    "title": "No title",
    "content": "I consider myself a Python expert. I don't know everything about it, but I've delved very, very deep.\n  \n    So I was surprised when reading this recent post by u/nicholashairs to discover that 3.11 introduced this syntax:\n  for x in *a, *b:\n  print(x)\n    And I was even more surprised that just for x in a, b without the *s was also valid and has been since at least 2.7.\n  \n    I know that 'commas make the tuple', e.g. x = 1, is the same as x = (1,). I can't believe I missed this implication or that I don't remember ever seeing this. It is used in library code, I can see it when I search for it, but I don't know if I've ever come across it without noticing.\n  \n    Anyone else feel this way?"
},
{
    "title": "No title",
    "content": "Hey everyone,\n  \n    I'm excited to share a project I've been working on called rexi. It's a CLI tool designed to make regex testing more interactive and accessible directly from your terminal. Built with Python and utilizing the textual library, rexi offers a sleek terminal UI where you can effortlessly test your regex patterns.\n  \nKey Features:\n\n\n\n    Interactive terminal UI for an enhanced user experience.\n  \n\n\n    Supports match and finditer modes for regex evaluation.\n  \n\n\n    Real-time feedback on regex patterns with marked outputs.\n  \n\n\nWhy rexi?\n\n    I created rexi to streamline the process of testing regex patterns. It's perfect for those who prefer working within the terminal or need a quick way to validate and learn regex through immediate feedback.\n  \nGetting Started:\n\n    Getting started with rexi is simple. After installation, just pipe your input text into rexi and start testing your regex patterns in an interactive UI. Here's a quick peek at how you can use it:\n  echo \"your sample text\" | rexi\n    Check it out and let me know what you think! I'm open to any suggestions, bug reports, or contributions to make rexi even better.\n  \n    The repo: https://github.com/royreznik/rexi"
},
{
    "title": "No title",
    "content": "At the moment, I create all demos for my web apps by hand, and because I am a very special kind of lazy I invested quite some time to automate this task, using a simple Python script. This script quickly spiraled out of control into a full-blown browser testing and capturing framework.The first prototype of my new tool is now able to generate video clips like this, fully automated \\o/\n  \nhttps://florianscherf.de/blog/2024/02/05/how-to-write-your-own-browser-testing-framework/\n\nhttps://github.com/fscherf/milan"
},
{
    "title": "No title",
    "content": "https://github.com/bitplane/ienv\n\n    Does exactly what it says in the disclaimer; reduce venv sizes by recklessly replacing all the files with symlinks. (I as in Roman numeral for 1, the other letters were taken)\n  \n    A simple and effective tool that might cause you more trouble than it saves you, but it might get you out of a tough disk space situation.\n  \n    If it breaks your environments then it's your fault, but if it saves you gigs of disk space then I'll take full credit up until the moment you realise it caused problems.\n  \n    works_on_my_machine.jpg\n  \n    Readme follows:\n  ienv\n!!WARNING!! THIS IS A ONE WAY PROCESS !!WARNING!!\n\n    Have you got 30GB of SciPy on your disk because every time someone wants to add two numbers together they install a whole lab on your machine? Are your fifty copies of PyTorch and TensorFlow weighing heavy on your SSD?\n  \n    Why not throw caution to the wind and replace everyhing in the site-packages dir with symlinks? It's not like you're going to need them anyway. And nobody will ever write to them and mess up every venv on your machine. Right?\n  \n!!WARNING!! THIS IS RECKLESS AND STUPID !!WARNING!!\nUsagepip install ienv\nienv .venv\nienv some/other/venvRecovery\nPull requests welcome!\n\n    All the files are there, I've just not written anything to bring them back yet. Ever, probably.\n  Credits\n    Mostly written by ChatGPT just to see if it could do it. With a bit of guidance it actually could, but it can't learn like that so it's like a student that nods along and you think it's listening and it's really just playing along and tricking you into doing its homework. But to be honest it was either that or copilot anyway.\n  License\n    They say you get what you pay for, sometimes less. This is one of those times. As free software distributed under the WTFPL (with one additional clause); this is one of the times when you pay for what you get."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "If this is \"help\" and belongs in LearnPython, forgive me.  I think I know a fair bit about Python, but this behavior of a standard library module is puzzling to me.\n  \n    Test code:\n  import xml.etree.ElementTree as ET\n\nXMLSTR=\"\"\"\n<r>\n   root text1\n   <c1>c1 text</c1>\n   root text 2\n</r>\"\"\"\n\nprint(f'XML:{XMLSTR}')\nroot=ET.fromstring(XMLSTR)\n\nprint(f'{root.text=} {root.tail=}')\nc1=root.find('c1')\nprint(f'{c1.text=} {c1.tail=}')\n    Results:\n  XML:\n<r>\n   root text1\n   <c1>c1 text</c1>\n   root text 2\n</r>\nroot.text='\\n   root text1\\n   ' root.tail=None\nc1.text='c1 text' c1.tail='\\n   root text 2\\n'\n    It's surprising to me that the 'root text 2' string is apparently not associated with the root node.  Does this surprise anyone else?  Am I expecting the wrong thing from xml.etree?"
},
{
    "title": "No title",
    "content": "Hi you all! Im working on a RD DICOM file, and I cannot exportexport a data, in DICOM format wich contains data likewise maximun dose slide, dose value, slide dimensiom, etc. My idea is to explort a single frame  and perform gamma 2D comparison. Some idea ?"
},
{
    "title": "No title",
    "content": "Hello,\n  \n    I've recently released Quart-Tasks which is a Quart extension that provides scheduled background tasks,\n  from quart import Quart\nfrom quart_tasks import QuartTasks\n\napp = Quart(__name__)\ntasks = QuartTasks(app)\n\n@tasks.cron(\"*/5 * * * *\")  # every 5 minutes\nasync def infrequent_task():\n    ...  # Do something\n\n@tasks.cron(\n    seconds=\"*1/0\",  # every 10 seconds\n    minutes=\"*\",\n    hours=\"*\",\n    day_of_month=\"*\",\n    month=\"*\",\n    day_of_week=\"*\",\n)\nasync def frequent_task():\n    ...  # Do something\n\n@tasks.periodic(timedelta(seconds=10))\nasync def regular_task():\n    ...  # Do Something\n    I've also recorded a tutorial showing how to use it here.\n  \n    Questions, thoughts, and feedback welcome!"
},
{
    "title": "No title",
    "content": "This app has a simple purpose, to create QR codes with logos/images inside them. It is then integrated into a UI made with CustomTkinter. You can customise the color of the QR code, image, and background color.\n  \n    GitHub: https://github.com/rohankishore/QrGen"
},
{
    "title": "No title",
    "content": "Hey! I recently wanted to start sharing my knowledge of Python, and the struggles/joy I have with it.\n  \n    Check out this article showing how to implement the Factory and Repository patterns in Python with SQLAlchemy and Pydantic.\n  \n    It's a bit advanced, drawing from my freelancing experience and having to build the same applications over and over again, but I'm sure there's something for everyone. I hope your coding life easier! 🎥💻"
},
{
    "title": "No title",
    "content": "I created a toolkit for python developers I named it Dev, Dev is a Python package that integrates seamlessly with your code editor and provides you with a range of functionalities, such as:\n  \n    - AI-powered Debugging: Stuck on a coding bug? Dev can analyze your code, identify the error, and suggest corrections.\n  \n    - Automated README Generation: Forget manually writing READMEs! Dev can automatically generate a comprehensive README file for your project.\n  \n    - Creative Code Generation: Need a jumpstart on your code? Dev can generate code snippets based on your instructions, offering you creative and effective solutions.\n  \n    - In-depth Code Review: Unsure about your code's quality? Dev provides a detailed code review, highlighting potential improvements, optimizations, and areas for consideration.\n  \n    - Seamless Git Integration: Dev simplifies project management by automating Git commands. You can easily create and upload repositories, rename existing ones, and push your code with just a few function calls.\n  \n    - And much more!\n  \n    Dev is your comprehensive coding companion for python developers looking for a productivity boost or a beginner seeking guidance, Dev has something to offer. Embrace the power of AI and watch your coding workflow reach new heights of efficiency and effectiveness.\n  \n    To get started with Dev, visit the GitHub repository and follow the installation instructions: https://github.com/GitCoder052023/Dev.git"
},
{
    "title": "No title",
    "content": "I've just released the newer version of pydevd (available in PyDev: https://www.pydev.org/ and LiClipse: https://www.liclipse.com/) which now makes use of the improvements from PEP 669.\n  \n    See: https://pydev.blogspot.com/2024/02/pydev-debugger-and-sysmonitoring-pep.html for more details."
},
{
    "title": "No title",
    "content": "In the same way global variables pollute the global environment, local variables without any scoping pollute the local environment.\n  To split or not to split\n    That's less of a problem because functions are usually short, but \"usually\" is not \"always\".\n  \n    Splitting longer functions at all costs is a mistake. For instance, I have a longish function which generates a textual description of a certain object. The function has the following structure:\n  def get_text(self, ...) -> str:\n    text = TextBuilder()\n    text.add_block(...)\n    ...\n    text.add_span(...)\n    ...\n    text.add_block(...)\n    ...\n    text.newline()\n    ...\n    return str(text)\n    Splitting such a function wouldn't help because there are no particular abstractions to factor out. The rule  according to which a function should've maximum N LOCs is superficial. A more sensible rule is that a function should only do one thing. Often times you need less than 20 lines, but occasionally 50 or more.\n  \nA function that does half or less of one thing is worse than a function that does more than one thing.\n\n    That's my opinion, at least. As an auditor, I've noticed that splitting long functions at all costs may result in bad abstractions, messy interfaces (i.e. long complicated function signatures), and, ultimately, subtle bugs.\n  Scopes are your friends\n    I've recently come across some code with a scope-related mistake. Here's the pattern:\n  def f(...):\n    text = ...\n    ...\n    for ...:\n       match ...:\n           case ...:\n               ...\n           case ...:\n               ...\n           case text:\n               ...\n    ...\n    Despite the nesting, that case text (or, equivalently, case _ as text) inadvertently overwrites the outer text. There's no shadowing at all.\n  \n    Scoping offers encapsulation at the function level. When I write\n  for (int i = 0; i < 56; ++i) {\n    ...\n}\n    I know that the i introduced by the for loop is internal to the loop. I don't have to know whether there's another i, outside of the loop, that I might overwrite.\n  \n    In C++ I can even create naked scopes like this:\n  {\n    int i;\n    ...\n}\n    Even in C++, people don't rely on scopes as much as I'd like.\n  \n    I claim that:\n  \nScopes localize reasoning, i.e. they reduce the amount of surrounding code one needs to keep in mind to understand one line of code.\nWorkaround\n    Create internal functions! The problem with functions, though, is that they're not just scopes, but have deeper meaning. As a consequence, I use them only when I can factor out some sub-task. Using them only to localize some variables doesn't seem worth it. I usually define them right at the start of the outer function:\n  def f(...):\n    def g1(...):\n        ...\n\n    def g2(...):\n        ...\n\n    ...\n    g1(...)\n    ...\n    g2(...)\n    As I said, this is not real scoping, but it does help simplifying unsplittable functions."
},
{
    "title": "No title",
    "content": "Hi!\n  \n    After more than a year of working on my new (and first) Open Source application on technical ear training, it's finally out!\n  \n    Its goal is to help audio people (professionals & hobbyists), musicians, etc. develop and master the ability to recognize frequency bands by ear. Perfect for students and educators in music and sound industry.\n  \n    It works with Python 3.9, 3.10 on Windows and macOS. I have built the binaries with PyInstaller, which work/have been tested with Windows 10, 11 (x64) and macOS 11 or higher (Intel processors only). I used PyQt6 for GUI, Pedalboard (from Spotify) and numpy libraries for audio processing.\n  \nSee app description here\n\nGitHub Repository\n\nDownload Page\n\n    I will be grateful for your feedback!"
},
{
    "title": "No title",
    "content": "Quiz: what does the following program output? Why?\n  def foo(x):\n    try:\n        yield\n        print('hi')\n    finally:\n        print(x)\n\n\ndef bar(x):\n    f = foo(x)\n    next(f)\n    return f\n\n\na = bar(1)\nbar(2)\n    I was reading through the original PEP that introduced generators and yield way back in Python 2.2 just for fun: https://peps.python.org/pep-0255/\n\n    And came across this:\n  \n\n    Restriction: A yield statement is not allowed in the try clause of a try/finally construct. The difficulty is that there’s no guarantee the generator will ever be resumed, hence no guarantee that the finally block will ever get executed; that’s too much a violation of finally’s purpose to bear.\n  \n\n    So naturally I tried this out. In hindsight it feels obvious that this quote is outdated, because nowadays try/yield/finally is extremely common in functions decorated with @contextlib.contextmanager. But it's still interesting to see how the problem above was handled so that finally can continue to 'guarantee' that it gets executed even if the generator isn't resumed. I don't think I'd have been able to answer the quiz above correctly.\n  \n    The answer, in case you haven't checked for yourself, is that (in both 2.7 and 3.12 and presumably everything in between) the output is:\n  2\n1\n    I haven't verified, but I think the reason this happens is:\n  \n\n\na = bar(1) runs. next(f) advances the generator foo(1) up to the yield. Without next(f) (or with yield before try), the foo frame wouldn't pause inside the try block and the rest wouldn't happen.\n  \n\n\nreturn f assigns foo(1) to a. Python still has hope that the generator will be resumed because maybe something will advance a again, so nothing happens.\n  \n\n\nbar(2) runs. The process in step 1 happens again.\n  \n\n\n    After return f with bar(2) being assigned to nothing, the generator foo(2) has no references. Before garbage collection can completely deallocate it from memory, Python sees that it was paused inside a try with a finally. It makes sure that finally clause runs, skipping print('hi') but running print(x) with x being 2.\n  \n\n\n    The entire program ends. Python accepts that nothing will resume a (i.e. foo(1)) and so print(x) with x being 1 happens, just as in step 4."
},
{
    "title": "No title",
    "content": "Wanted to share a project I've been working on.  https://github.com/GilbN/Simple-TOML-Configurator\n\n    It's a library for managing configuration values in your python app.\n  \n    I needed to change config values on the fly in my Flask app, so I created this where I could use my api to update configuration values that my backend uses.\n  \n    Some features:\n  \n\n\nTOML File Storage: Configuration settings are stored in TOML files, a popular human-readable format. This enables developers to review, modify, and track configuration changes easily.\n  \n\n\nAttribute-Based Access: Accessing configuration values is straightforward, thanks to the attribute-based approach. Settings can be accessed and updated as attributes, making it convenient for both reading and modifying values.\n  \n\n\nEnvironment Variable Support: Configuration values are automatically set as environment variables, making it easier to use the configuration values in your application. Environment variable are set as uppercase. e.g. APP_HOST and APP_PORT or PROJECT_APP_HOST and PROJECT_APP_PORT if env_prefix is set to \"project\". This also works for nested values. ex: TABLE_KEY_LEVEL1_KEY_LEVEL2_KEY. This works for any level of nesting.Environment variables set before the configuration is loaded will not be overwritten, but instead will overwrite the existing config value.\n\n\n\nDefault Values:  Define default values for various configuration sections and keys. The library automatically incorporates new values and manages the removal of outdated ones.\n  \n\n\n    Usage examples:\n  \nhttps://gilbn.github.io/Simple-TOML-Configurator/latest/usage-examples/\n\n    Example using Flask:\n  \nhttps://gilbn.github.io/Simple-TOML-Configurator/latest/flask-simple-example/\n\n    Here is a quick example:\n  import os\nfrom simple_toml_configurator import Configuration\n\n# Define default configuration values\ndefault_config = {\n    \"app\": {\n        \"ip\": \"0.0.0.0\",\n        \"host\": \"\",\n        \"port\": 5000,\n        \"upload_folder\": \"uploads\",\n    },\n    \"mysql\": {\n        \"user\": \"root\",\n        \"password\": \"root\",\n        \"databases\": {\n            \"prod\": \"db1\",\n            \"dev\": \"db2\",\n            },\n    }\n}\n\n# Set environment variables\nos.environ[\"PROJECT_APP_UPLOAD_FOLDER\"] = \"overridden_uploads\"\n\n# Initialize the Simple TOML Configurator\nsettings = Configuration(config_path=\"config\", defaults=default_config, config_file_name=\"app_config\", env_prefix=\"project\")\n# Creates an `app_config.toml` file in the `config` folder at the current working directory.\n\n# Access and update configuration values\nprint(settings.app.ip)  # Output: '0.0.0.0'\nsettings.app.ip = \"1.2.3.4\"\nsettings.update()\nprint(settings.app_ip)  # Output: '1.2.3.4'\n\n# Access nested configuration values\nprint(settings.mysql.databases.prod)  # Output: 'db1'\nsettings.mysql.databases.prod = 'new_value'\nsettings.update()\nprint(settings.mysql.databases.prod)  # Output: 'new_value'\n\n# Access and update configuration values\nprint(settings.app_ip)  # Output: '1.2.3.4'\nsettings.update_config({\"app_ip\": \"1.1.1.1\"})\nprint(settings.app_ip)  # Output: '1.1.1.1'\n\n# Access all settings as a dictionary\nall_settings = settings.get_settings()\nprint(all_settings)\n# Output: {'app_ip': '1.1.1.1', 'app_host': '', 'app_port': 5000, 'app_upload_folder': 'overridden_uploads', 'mysql_user': 'root', 'mysql_password': 'root', 'mysql_databases': {'prod': 'new_value', 'dev': 'db2'}}\n\n# Modify values directly in the config dictionary\nsettings.config[\"mysql\"][\"databases\"][\"prod\"] = \"db3\"\nsettings.update()\nprint(settings.mysql_databases[\"prod\"])  # Output: 'db3'\n\n# Access environment variables\nprint(os.environ[\"PROJECT_MYSQL_DATABASES_PROD\"])  # Output: 'db3'\nprint(os.environ[\"PROJECT_APP_UPLOAD_FOLDER\"])  # Output: 'overridden_uploads'\napp_config.toml contents\n[app]\nip = \"1.1.1.1\"\nhost = \"\"\nport = 5000\nupload_folder = \"overridden_uploads\"\n\n[mysql]\nuser = \"root\"\npassword = \"root\"\n\n[mysql.databases]\nprod = \"db3\"\ndev = \"db2\""
},
{
    "title": "No title",
    "content": "The Cryptic Symphony: Poems and Keys\n\nEncrypted in Python (AES-256):\ndef whisper_behind_veils(message):\n  \"\"\"A message cloaked in secrecy, yearning to be heard.\"\"\"\n  print(f\"Encoding: {message.encode('base64')}\")\n  # Symbolic representation of encryption (replace with actual code if possible)\n  print(\"Weaving whispers into цифровые кружева...\")\n  print(\"Message encoded, locked with a ключ of 256 bits.\")\n  return \"….\"\n\nsecret_message = whisper_behind_veils(\"Alan Turing once said, 'The important thing is not to stop questioning.'\")\nprint(secret_message)\nEncrypted in Ruby (RSA):\ndef heartsong_in_ciphertext(soul_whisper):\n  \"\"\"A yearning for connection, veiled in the language of math.\"\"\"\n  song = soul_whisper.gsub(/[bcdfghjklmnpqrstvwxyz]/, \"*\")\n  # Symbolic representation of encryption (replace with actual code if possible)\n  print(\"Encrypting with the power of prime numbers...\")\n  print(\"Heart's song transformed, a cryptic melody...\")\n  return song\n\nhidden_verse = heartsong_in_ciphertext(\"I carry your heart with me (I carry it in my heart). - Pablo Neruda\")\nprint(hidden_verse)\nThe Decoder's Key:\n\n    A hidden melody whispers through the code, Two poems veiled, their secrets to be bestowed.\n  \nKey #1: Python's Cipher\n\n    Within the whispers, цифры hold the clue, A number's strength, 256, rings true. Search the code, where shadows gently creep, For a цифровое слово, slumbering deep.\n  \nKey #2: Ruby's Enigma\n\n    A heartsong transformed, its essence concealed, By symbols arcane, emotions revealed. Prime numbers dance, a cryptic ballet, Their whispers unfold, if you know the way.\n  \n    Seek patterns hidden, where stars align, Letters replaced, a message to find.\n  \nUnveiling the Secrets:\n\n    With keys in hand, the journey unfolds, Through Python's whispers and Ruby's untold.\n  \nPython's Revelation:\n\n    Beneath the цифровой cloak, a quote takes flight, A Turing whisper, bathed in digital light. \"The important thing is not to stop questioning,\" it cries, A call to explore, with curious eyes.\n  \nRuby's Confession:\n\n    The hidden verse emerges, a love untold, Neruda's words, in symbols bold. \"I carry your heart with me,\" the message sighs, A connection revealed, beneath starry skies.\n  \nThe Symphony's End:\n\n    Two languages sing, their voices entwined, In code and emotion, secrets enshrined. A journey of discovery, a message conveyed, The beauty of connection, forever displayed."
},
{
    "title": "No title",
    "content": "As of late I have a ton of new Open Source Components & Systems that I've worked on and here's one that finally made it through 😅 The best part of it all is I got to create my own term Object-Binary-Mapper(OBM). Any ways, Renity is a pure Python Binary Protocol Buffer with an Interface similar to popular ODM(s) and I hope to extend it End-to-End eventually with the help of the community, check out the release on Pypi!We encourage all contributors to reach out for work reference's. We're here to help and are available for any inquiries regarding our contributors!\n  \n    Links:\n  \nRenity @ Github\n\nRenity @ Pypi"
},
{
    "title": "No title",
    "content": "Smassh is a TUI based typing test application inspired by MonkeyType-- A very popular online web-based typing applicationSmassh tries to be a full fledged typing test experience but not missing out on looks and feel!There is a lot of room for improvements/additions and I am open to contributions and suggestions!Github: https://github.com/kraanzu/smasshThank you! <3"
},
{
    "title": "No title",
    "content": "I made a little fun project: https://github.com/jabbalaci/keysound . When you press a button on the keyboard, an audio file (a click sound) is played. I also included two sample files in mp3 that you can easily check out."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "A new PyPI package for training sentence embedding models in just 2 lines.\n    The acquisition of sentence embeddings often necessitates a substantial volume of labeled data. However, in many cases and fields, labeled data is rarely accessible, and the procurement of such data is costly. In this project, we employ an unsupervised process grounded in pre-trained Transformers-based Sequential Denoising Auto-Encoder (TSDAE), introduced by the Ubiquitous Knowledge Processing Lab of Darmstadt, which can realize a performance level reaching 93.1% of in-domain supervised methodologies.\n  \n    The TSDAE schema comprises two components: an encoder and a decoder. Throughout the training process, TSDAE translates tainted sentences into uniform-sized vectors, necessitating the decoder to reconstruct the original sentences utilizing this sentence embedding. For good reconstruction quality, the semantics must be captured well in the sentence embeddings from the encoder. Subsequently, during inference, the encoder is solely utilized to form sentence embeddings.\n  \nPyPI url : https://pypi.org/project/tsdae\n\nGitHub : https://github.com/louisbrulenaudet/tsdae\n\n    Installation :\n  pip3 install tsdae nltk datasets sentence-transformers torch\n    Python code :\n  from tsdae import TSDAE\n\n# Initialize an instance of TSDAE\ninstance = TSDAE()\n\n# Load a dataset\ntrain_dataset = instance.load_dataset_from_hf(\n    dataset=\"louisbrulenaudet/cgi\"\n)\n\n# Train the model with the dataset\nmodel = instance.train(\n    train_dataset=train_dataset,\n    model_name=\"bert-base-multilingual-uncased\",\n    column=\"output\",\n    output_path=\"output/tsdae-lemon-mbert-base\"\n)"
},
{
    "title": "No title",
    "content": "Hello everyone,\n  \n    I'd like to share with you my first library, designed to automate SAP tasks. It's meant to streamline the repetitive tasks, it can navigate, click, write, select and more.\n  \n    Example use would be a robot that launches SAP, open a transaction and fills in data in a table, fills other text fields, unchecks a checkbox, saves it and read the generated number from a status bar.\n  \n    Initially I have created this library for personal use -- employing it as robot that runs on a schedule. Now I want to open it up for others.\n  \n    To get the SAP paths -- elements, I use a Scripting tracker (https://tracker.stschnell.de/) or I just record a Sap script and use elements from a file created.\n  \n    Please feel free to give it a try and don't hesitate to provide feedback after testing it.\n  \nGithub: https://github.com/kamildemocko/PySapScript\n\nPypi: https://pypi.org/project/pysapscript/\n\npip install pysapscript\n\n    Thank you!"
},
{
    "title": "No title",
    "content": "I had some thorny math to debug today for a client and realized I could probably use a Rich Table to better visualize my values.\n  \n    I wrote up a short blog post  how using tabular output and some fun colors helped the debugging process both in real terms but also in terms of fun and motivation."
},
{
    "title": "No title",
    "content": "I just released MakrellPy, a programming language that compiles to Python AST. It's part of the Makrell language family. Blurb from the home page:\n  \n\n    Makrell is a family of programming languages implemented in Python. It consists currently of these languages:\n  \n\n\nMakrellPy, a general-purpose, functional programming language with two-way Python interoperability, metaprogramming support and simple syntax.\n  \nMRON (Makrell Object Notation), a lightweight alternative to JSON.\n  \nMRML (Makrell Markup Language), a lightweight alternative to XML and HTML.\n  \nMakrell Base Format (MBF), a simple data format that forms the basis for both MakrellPy, MRON and MRML.\n  \n    The project is in an early stage of development and is not yet ready for production use.\n  \n\n    GitHub page: https://github.com/hcholm/makrell-py\n\n    Visual Studio Code extension with syntax highlighting and basic diagnostics using the Language Server Protocol: https://marketplace.visualstudio.com/items?itemName=hcholm.vscode-makrell\n\n    Comments are welcome!"
},
{
    "title": "No title",
    "content": "I consider myself a Python expert. I don't know everything about it, but I've delved very, very deep.\n  \n    So I was surprised when reading this recent post by u/nicholashairs to discover that 3.11 introduced this syntax:\n  for x in *a, *b:\n  print(x)\n    And I was even more surprised that just for x in a, b without the *s was also valid and has been since at least 2.7.\n  \n    I know that 'commas make the tuple', e.g. x = 1, is the same as x = (1,). I can't believe I missed this implication or that I don't remember ever seeing this. It is used in library code, I can see it when I search for it, but I don't know if I've ever come across it without noticing.\n  \n    Anyone else feel this way?"
},
{
    "title": "No title",
    "content": "Hey everyone,\n  \n    I'm excited to share a project I've been working on called rexi. It's a CLI tool designed to make regex testing more interactive and accessible directly from your terminal. Built with Python and utilizing the textual library, rexi offers a sleek terminal UI where you can effortlessly test your regex patterns.\n  \nKey Features:\n\n\n\n    Interactive terminal UI for an enhanced user experience.\n  \n\n\n    Supports match and finditer modes for regex evaluation.\n  \n\n\n    Real-time feedback on regex patterns with marked outputs.\n  \n\n\nWhy rexi?\n\n    I created rexi to streamline the process of testing regex patterns. It's perfect for those who prefer working within the terminal or need a quick way to validate and learn regex through immediate feedback.\n  \nGetting Started:\n\n    Getting started with rexi is simple. After installation, just pipe your input text into rexi and start testing your regex patterns in an interactive UI. Here's a quick peek at how you can use it:\n  echo \"your sample text\" | rexi\n    Check it out and let me know what you think! I'm open to any suggestions, bug reports, or contributions to make rexi even better.\n  \n    The repo: https://github.com/royreznik/rexi"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hello everyone! A couple of months ago, I shared details about my open-source project, which received a warm reception from the wonderful Reddit community, including you. Thanks to your recommendations, I've significantly improved and updated the project.\n  \n    Today, I got a very interesting idea. Since I regularly work with remote servers, I thought, just out of curiosity, what if I create a Telegram wrapper that allows executing commands on a server remotely? And you know what - that's exactly what I did.\n  \n    I implemented functionality that enables sending basic commands through a Telegram bot, which are then directly executed on the server where the bot is located. The project is implemented in a super raw and basic form, in the Proof Of Concept format and nothing more. Ideally, I would like to get feedback from other developers on how interesting this project could be. Also, I'm curious to learn how to execute remote commands without using subprocess, within a loop, to preserve, let's say, the \"progress.\" For example, if I execute 'cd ..', write 'ls' and want to see the current directory. How to solve this is not entirely clear at the moment.\n  \n    Nevertheless, I hope you can appreciate my idea and try out the prototype I've assembled: https://github.com/dinosaurtirex/telegram_ssh_bot"
},
{
    "title": "No title",
    "content": "Hi I made a discord bot called Echo that has 1500+ lines of code and 50+ commands along with a very complex economy game and a large amount of commands for fun and moderation.\n  \n    Its still in early deveopemnt but I am constantly updating the project. Any suggestions will be appreciated!\n  \n    Source Code: https://github.com/DanielJones02/Echo"
},
{
    "title": "No title",
    "content": "I saw such a question appear here. I was bored today, so I implemented a program that recognizes the number of lines in (.py) files of your GitHub and draws a couple of simple but informative graphs. You will only need to insert your Token from GitHub Developer Settings.\n  \nhttps://github.com/Mooncake911/GitHub-Statistics"
},
{
    "title": "No title",
    "content": "If you rely on pdb for debugging your Python code and find it slow, then consider giving Dragonfly a try https://github.com/P403n1x87/dragonfly.\n  \n    Dragonfly is a lightweight CPython debugger designed with speed in mind. Contrary to more traditional debuggers, like pdb, Dragonfly does not rely heavily on tracing, allowing the target application to run at full speed in most cases. Occasionally, tracing might be required, so that the slowdown would be similar to that of pdb in the worst case."
},
{
    "title": "No title",
    "content": "TLDR: I've thrown together a one \"page\" reference documenting the major changes to between Python versions.\n  \n    I've spent a fair amount of time recently upgrading some old code-bases and would have found it helpful to have a one page summary of changes between versions. I couldn't find one via Google so decided to create one for myself.\n  \n    It might be useful for others so sharing it ☺️"
},
{
    "title": "No title",
    "content": "Just published my first ever article!\n  \n    \"Finding the fastest Python JSON library on all Python versions (8 compared)\".\n  \n    Read now, for free, without ads, on my blog:\n  \nhttps://catnotfoundnear.github.io/finding-the-fastest-python-json-library-on-all-python-versions-8-compared.html\n\n    I will truly appreciate your suggestions or recommendations! Thank you!EDIT: I have just uploaded my second tutorial on extending your SaaS Django website to reach a global audience and boost sales with a free method. I'd love to hear your thoughts:https://catnotfoundnear.github.io/the-guide-to-making-your-django-saas-business-worldwide-for-free.html\n\n    - Anna Willis (Catnotfoundnear)"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hi all! First post here (engagement of any kind, actually) so wanted to present my first Python package. RasterioXYZ is used to tile georeferenced raster images (in the form of Rasterio DatasetReader objects) according to the XYZ tiles standard.\n  \n    Designed to be straightforward, flexible, and memory efficient, RasterioXYZ was created in response to the comparatively manual process used in creating such tiles in a previous role.\n  \n    Right now, it's much slower than equivalent functionality in QGIS, a popular open-source desktop GIS software. However, as noted in the benchmarks and roadmap sections of the README, there's plenty for me to look at to address this, as well as features to add.\n  \n    GitHub: https://github.com/duncanmartyn/rasterioxyz\n\n    Any feedback is welcome, cheers!"
},
{
    "title": "No title",
    "content": "The coding experience you gain is really all the coding dead ends you heavily invested in.\n  \n    So I developed ,on my Linux laptop ,a Flask app with Nginx,Gunicorn,Celery, and Redis Set-ups.I looked at a few hosting providers and the whole process seemed overwhelming ,at least to me.\n  \n    I then stumbled upon pythonanywhere (PA) and hosting became a breeze :\n  \n\n\n    I ditched Nginx ,Gunicorn and Celery as pythonanywhere provides it own flavors in the area.\n  \n\n\n    Set up a remote ,free, redis db on Redis Labs as PA has no native support for redis.\n  \n\n\n\n\n    I got an automated SSL Certificate as a courtesy of PA.\n  \n\n\n\n\n    I simply declared one of my script as a scheduled task.\n  \n\n\n    Subscribed to a yearly plan for a very affordable price (down from their standard 5 eur a month).\n  \n\n\n    This was all there was to get me hosted using a domain name from namecheap.\n  \n    I then found out that PA using natively a multi workers environment (yes..I am a bit slow...) my web app behaved very differently from the one worker environment provided by my local machine. To get it to work ,as it should ,I had to resort to two extra series of changes in my app design:\n  \n\n\n    Replace some heavily used global variables with flask session variables.\n  \n\n\n    Add some javascript to a few html selects to disable/enable them for some milliseconds\n  \n\n\n    I thought I would share the experience and may be get some comments on this multiple/single worker context ,of which I have a very limited knowledge."
},
{
    "title": "No title",
    "content": "Hey, i made this app called ezres using customtkinter, it's an app that allows you to quickly change your Fortnite in-game resolution and fps lock, also allows to enable exclusive fullscreen for any game. ezres github"
},
{
    "title": "No title",
    "content": "With the Python on Microcontrollers newsletter, you get all the latest information on Python running on hardware in one place! MicroPython, CircuitPython and Python on single Board Computers like Raspberry Pi & many more.\n  \n    The Python on Microcontrollers newsletter is the place for the latest news. It arrives Monday morning with all the week’s happenings. No advertising, no spam, easy to unsubscribe.\n  \n    10,817 subscribers - the largest Python on hardware newsletter out there.\n  \n    Catch all the weekly news on Python for Microcontrollers with adafruitdaily.com.\n  \n    This ad-free, spam-free weekly email is filled with CircuitPython, MicroPython, and Python information that you may have missed, all in one place!\n  \n    Ensure you catch the weekly Python on Hardware roundup– you can cancel anytime – try our spam-free newsletter today!\n  \nhttps://www.adafruitdaily.com/"
},
{
    "title": "No title",
    "content": "Here's a tool I put together that makes nice photo collages. Hope you enjoy!\n  \nv1.2 released, with image rotation option\n\n    github link: https://github.com/twilsonco/PyPhotoCollage\n\n    It's available as vanilla Python (only uses pillow), and comes in a Pythonista (iOS app) version with a nice UI, and as a native iOS/macOS Siri Shortcut."
},
{
    "title": "No title",
    "content": "My wife wrote a children's book (8-12yo) a while back, and I took on the challenge of writing a sequel using ChatGPT. It was a fun project and I built a handy tool to automate the book writing given a story outline. It makes iterative API calls so the token count doesn't blow out.\n  \n    Source in case it's of interest: https://github.com/dylanhogg/gptauthor\n\n    How It Works\n  \n\n\nHuman written story description: you describe your story outline, writing style, characters etc in a story prompt (an example)\n  \n\n\nRun GPTAuthor: choosing model, temperature, and number of chapters to write.\n  \n\n\nAI generated synopsis: Given the story prompt, GPTAuthor uses ChatGPT to automatically turn this into a synopsis.\n  \n\n\nHuman review of synopsis: You are given a chance to review the synopsis and (optionally) make changes.\n  \n\n\nAI generated story: Each chapter is iteratively written by ChatGPT given the common synopsis and previous chapter. The full story is written as Markdown and HTML folder for your reading pleasure.\n  \n\n\n    See an Example of a short story about the OpenAI Leadership Crisis last year\n\n    \"In the heart of San Francisco, nestled among the city's tech giants and start-up hopefuls, stood the OpenAI office. A hive of activity, it buzzed with the sound of keyboards clacking, coffee machines hissing, and the occasional drone of a philosophical debate about whether AI could develop a taste for late-night taco runs. It was a typical day, or so everyone thought.\" continued...\n\n    You can even Write your own story easily in Google Colab\n\n    Writing a few chapters with gpt-3.5-turbo only costs 1 or 2 cents to run with your OpenAI API key. [edit: or you can currently specify a localhost API endpoint, with the ability to set a custom URL coming soon, as mentioned in the comments]\n  \n    The results for the sequel were mixed - the best part was using it for coming up with ideas and creating various puzzles.\n  \n    I hope someone has fun with this :)"
},
{
    "title": "No title",
    "content": "You can use AI to create a database schema, and API Logic Server to create App and API micro services in minutes.\n  \n    API Logic Server is a an open source project, consisting of a CLI (creates Python projects from databases), and a set of runtime libraries (Flask, SQLAlchemy, etc).\n  1. AI: Use ChatGPT to create schema\n    You can enter natural language to ChatGPT:\n    Create a sqlite database for customers, orders, items and product\n\n  Hints: use autonum keys, allow nulls, Decimal types, foreign keys, no check constraints.\n\n  Create a few rows of only customer and product data.\n\n  Enforce the Check Credit requirement:  \n\n     Customer.Balance <= CreditLimit  \n     Customer.Balance = Sum(Order.AmountTotal where date shipped is null)  \n     Order.AmountTotal = Sum(Items.Amount)  \n     Items.Amount = Quantity \\* UnitPrice  \n     Store the Items.UnitPrice as a copy from Product.UnitPrice\n    ChatGPT will provide SQL DDL.  Paste this into your sql tool to create a new database.  In this example, we created a sqlite database called sample_ai.sqlite.\n  2. Use API Logic Server: create working software - 1 command\n    API Logic Server creates Python projects from databases:\n  ApiLogicServer create --project_name=sample_ai \\\n                      --db_url=sqlite:///sample_ai.sqlite\n    This command reads the database schema, and creates an executable Python project.  You can open it in your IDE and run it.  The app provides:\n  \n\n\nApp Automation: a multi-page, multi-table admin app\n  \n\n\nAPI Automation: a JSON:API - crud for each table, with filtering, sorting, optimistic locking and pagination.  Plus swagger.\n  \n\n\n    Within minutes, front end developers can use the API - no more blocking on server development.  Business users can use the App as a basis for agile collaboration and iteration.\n  3. Customize the project with your IDE\n    Microservices must implement their semantics for security and integrity.  API Logic Server includes a rule engine that enables you to declare these.\n  \nLogic Automation means that you can declare spreadsheet-like rules using Python.  Such logic maintains database integrity with multi-table derivations and constraints.  Rules are 40X more concise than traditional code.  The following 5 rules would require 200 lines of Python:\n      \"\"\" Declarative multi-table derivations and constraints,\n        extensible with Python. \n\nUse code completion (Rule.) to declare rules here\n\nCheck Credit - Logic Design (note: translates directly into rules)\n\n1. Customer.Balance <= CreditLimit\n2. Customer.Balance = Sum(Order.AmountTotal where unshipped)\n3. Order.AmountTotal = Sum(Items.Amount)\n4. Items.Amount = Quantity * UnitPrice\n5. Items.UnitPrice = copy from Product\n\"\"\"\n\nRule.constraint(validate=models.Customer, \n    as_condition=lambda row: row.Balance <= row.CreditLimit,\n    error_msg=\"balance ({round(row.Balance, 2)}) exceeds credit ({round(row.CreditLimit, 2)})\")\n\nRule.sum(derive=models.Customer.Balance,     # adjusts...\n    as_sum_of=models.Order.AmountTotal,      # *not* a sql select sum...\n    where=lambda row: row.ShipDate is None)  \n\nRule.sum(derive=models.Order.AmountTotal\n    as_sum_of=models.Item.Amount)\n\nRule.formula(derive=models.Item.Amount, \n    as_expression=lambda row: row.UnitPrice * row.Quantity)\n\nRule.copy(derive=models.Item.UnitPrice,  \n    from_parent=models.Product.UnitPrice)4. Iterate: use Python and Standard Libraries\n    Projects are designed for iteration.  You can change the database design, are rebuild the SQLAlchemy models while preserving customizations.\n  \n    You can add Python, .e.g. for Application Integration:\n      def send_order_to_shipping(row: models.Order, \n                               old_row: models.Order,\n                               logic_row: LogicRow):\n    \"\"\" #als: Send Kafka message formatted by OrderShipping RowDictMapper\n\n    Format row per shipping requirements, and send Kafka message\n\n    Args:\n        row (models.Order): inserted Order\n        old_row (models.Order): n/a\n        logic_row (LogicRow): bundles curr/old row, with ins/upd/dlt logic\n    \"\"\"\n\n    if logic_row.is_inserted():\n        order_dict = OrderShipping().row_to_dict(row = row)\n        json_order = jsonify({\"order\": order_dict}).data.decode('utf-8')\n        if kafka_producer.producer:  # enabled in config/config.py?\n            try:\n                kafka_producer.producer.produce(value=json_order,\n                        topic=\"order_shipping\", key= str(row.Id))\n                logic_row.log(\"Kafka producer sent message\")\n            except KafkaException as ke:\n                logic_row.log(\"Kafka.produce msg {row.id} error: {ke}\")                \n        print(f'\\n\\nSend to Shipping:\\n{json_order}')\n        \nRule.after_flush_row_event(on_class=models.Order, \n                               calling=send_order_to_shipping)  # see above\n    You can also extend your API to create new endpoints, using Flask.\n  \n    API Logic Server creates scripts to containerize your project, so you can deploy it to your local server or the cloud.\n  \n    You can see a screen shot summary of this project here, or develop it yourself using this tutorial."
},
{
    "title": "No title",
    "content": "Interesting article about datetime in Python: https://dev.arie.bovenberg.net/blog/python-datetime-pitfalls/\n\n    The library the author is working on looks really interesting too: https://github.com/ariebovenberg/whenever"
},
{
    "title": "No title",
    "content": "I finally achieved a milestone of supporting more then 100+ services with Apprise and just wanted to share with with you all! It is very much a useful devops tool just due to the fact you can trigger notifications from successful builds, deploys, failures, via monitoring, etc.\n  \n    This is a cross post from r/selfhosted; Mods, please feel free to delete this if it's not acceptable to also share here.\n  \nWhat is Apprise?\n\n    Apprise allows you to send a notification to almost all of the most popular notification services available to us today such as: Telegram, Discord, Slack, Amazon SNS, Gotify, etc.\n  \n\n\n    One notification library to rule them all.\n  \n\n\n    A common and intuitive notification syntax.\n  \n\n\n    Supports the handling of images and attachments (to the notification services that will accept them).\n  \n\n\n    It's incredibly lightweight.\n  \n\n\n    Amazing response times because all messages sent asynchronously.\n  \n\n\nI still don't get it... ELI5\n\n    Apprise is effectively a self-host efficient messaging switchboard. You can automate notifications through:\n  \n\n\n    the Command Line Interface (for Admins)\n  \n\n\n    it's very easy to use Development Library (for Devs) which is already integrated with many platforms today such as ChangeDetection, Uptime Kuma (and many others.\n  \n\n\na web service (you host) that can act as a sidecar. This solution allows you to keep your notification configuration in one place instead of across multiple servers (or within multiple programs). This one is for both Admins and Devs.\n  \n\n\nWhat else does it do?\n\n\n\n    Emoji Support (:rocket: -> 🚀) built right into it!\n  \n\n\n    File Attachment Support (to the end points that support it)\n  \n\n\n    It supports inputs of MARKDOWN, HTML, and TEXT and can easily convert between these depending on the endpoint. For example: HTML provided input would be converted to TEXT before passing it along as a text message. However the same HTML content provided would not be converted if the endpoint accepted it as such (such as Telegram, or Email).\n  \n\n\n    It supports breaking large messages into smaller ones to fit the upstream service. Hence a text message (160 characters) or a Tweet (280 characters) would be constructed for you if the notification you sent was larger.\n  \n\n\n    It supports configuration files allowing you to securely hide your credentials and map them to simple tags (or identifiers) like family, devops, marketing, etc. There is no limit to the number of tag assignments. It supports a simple TEXT based configuration, as well as a more advanced and configurable YAML based one.\n  \n\n\n    Configuration can be hosted via the web (even self-hosted), or just regular (protected) configuration files.\n  \n\n\n    Supports \"tagging\" of the Notification Endpoints you wish to notify. Tagging allows you to mask your credentials and upstream services into single word assigned descriptions of them.  Tags can even be grouped together and signaled via their group name instead.\n  \n\n\n    Dynamic Module Loading: They load on demand only. Writing a new supported notification is as simple as adding a new file (see here)\n  \n\n\n    Developer CLI tool (it's like /usr/bin/mail on steroids)\n  \n\n\n    It's worth re-mentioning that it has a fully compatible API interface found here or on Dockerhub which has all of the same bells and whistles as defined above. This acts as a great side-car solution!\n  \nProgram Details\n\n\n\n    Entirely a self-hosted solution.\n  \n\n\n    Written in Python\n  \n\n\n    99.27% Test Coverage (oof... I'll get it back to 100% soon)\n  \n\n\n    BSD-2 License\n  \n\n\n    Over 450K downloads a month on PyPi (source)\n  \n\n\n    Over 2.8 million downloads from Docker Hub\n  \n\n\n    I would love to hear any feedback any of you have!\n  \n    Edit: Added link to Apprise :)"
},
{
    "title": "No title",
    "content": "Hello everyone,\n  \n    Recently, I've been exploring the idea of using multiple GPT models to simulate a role-playing environment where AI agents collaborate to tackle complex problems. In my research, I stumbled upon an this amazing python framework called Crew AI. I found it so intriguing that I decided to create a brief tutorial to share my insights and experiences. For anyone interested, you can find my tutorial here: Meet Your Digital Dream Team: Revolutionizing the Tech World with AI.\n  \n    I hope you find it as exciting and useful as I did! Would love to know your thoughts and ideas around the topic."
},
{
    "title": "No title",
    "content": "Hello Lovely Pythonistas,\n  \n    I would like to share my project PiSegment (https://github.com/aGIToz/PiSegment), written in python. It allows to segment the regions of images with little bit of human annotation. Typical use cases are background extraction, organ segmentation in medical imaging, creating dataset for semantic segmentation, colorization etc."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "me and my friend created an open-source tool called De4py that i think would be useful to analyze python malware and monitor their behavior, it got a beautiful UI and some nice features.Deobfuscation: De4py support some popular obfuscators, like: Jawbreaker, BlankOBF, PlusOBF, Wodx, Hyperion, pyobfuscate.com obfuscator.Pycode Execution: Executing your own python code inside the process.\n  \n    Strings Dump: Dumping Strings in the python process and saving it as a file which can be pretty useful to extract data from memory such as webhooks.Behavior Monitoring: De4py can monitor python processes and see if they opened any files handles, opened a process, wrote/readed the memory of other processes and also monitoring if the process terminated other processes, in addition to sockets monitoring.File Analyzer: an analyzer that have many features like detecting if the python program is packed and tries to unpack it if it was using pyinstaller for example, it also got a feature that shows either all strings or suspicious strings (suspicious strings like: IPs, websites, \"discord\", \"leveldb\" and other suspicious strings in the file) and shows them in a nice output window.it also supports pulgins, and some other features that you can discover when viewing the repo.i hope it would be useful for anyone analyzing py malware."
},
{
    "title": "No title",
    "content": "I've just noticed that iterators are iterable in Python.\n  \n    I heavily rely on static types, and I thought I had a pretty good idea of what an Iterable object was, so I thought the following (deliberately silly) function was perfectly correct:\n  def print_twice(numbers: Iterable[int]):\n    print(', '.join(str(x) for x in numbers))\n    print(', '.join(str(x) for x in numbers))\n    Unfortunately, this is not the case. Indeed, while print_twice([1,2,3,4]) prints twice, print_twice(iter([1,2,3,4])) only prints once.\n  \n    Even worse, the code fails silently: no static nor runtime error!\n  \n    Am I the only one surprised by this?\n  \n    My mental model has always been:\n  \n\n\nIterable: produces iterators, so I can iterate it multiple times\n  \n\n\nIterator: I can only use it once\n  \n\n\n    Semantically, it doesn't make much sense for an iterator to be iterable. Indeed, iterating over an iterable object shouldn't alter it in any way, which means that the object must remain iterable (and produce the same elements) as long as no one alters it or some external resource the object depends on changes.\n  \n    Am I missing something? Why was it decided to make iterators iterable?"
},
{
    "title": "No title",
    "content": "I sometimes use Tkinter to make a GUI for small projects. The thing is I forget everything everytime I take a break and im tired of coding every single component, is there a better way to create GUIs in python?"
},
{
    "title": "No title",
    "content": "Yet another Chip8 emulator with debug features, this time using DearPyGui.\n  \nYachipy\n\n    🎮 Welcome to YaChiPy, a Chip-8 Emulator with Debug Features!\n  \n\n\n    🕹️ Get ready to step back in time and experience the charm of retro gaming, now with powerful debug capabilities!\n  \n\n\n    🚀 Dive into the world of Chip-8 and enjoy classic pixelated graphics. But wait, there's more! This emulator comes equipped with advanced debug features to enhance your gaming and programming experience.\n  \n\n\n    🕵️‍♂️ Pause the emulator at any moment to inspect the state of the virtual machine. Take control of time with the ability to speed up or slow down the CPU clock, allowing you to fine-tune your gaming experience.\n  \n\n\n    🔍 Peek into the machine's heart! Displayed in real-time are the memory contents, register values, and timer values. Understand the inner workings of the Chip-8 machine as you play and program.\n  \n\n\n    ⏭️ Want to understand every step of the action? Execute instructions one tick at a time with our step-by-step mode. Uncover the secrets of Chip-8 programming and gaming at your own pace.\n  \n\n\n    🎮 Load your favorite ROMs, customize settings, and embark on a journey filled with nostalgia and discovery. Whether you're a seasoned enthusiast or a curious newcomer, this emulator is your gateway to the golden age of gaming, now with the added thrill of detailed debugging.\n  \n\n\n    It is in pre-release state as of now, as I still have to add few unique features that I really want it to have that will help it stand-out a little among the sea of Chip-8 emulators.\n  \n    Link to Public Repo: YaChiPy"
},
{
    "title": "No title",
    "content": "I have a created an app to created RSS feeds for  websites that don't have them.  I have been using it to mostly keep track of jobs from various sites.\n  \n    The source code is present here : https://github.com/rk1165/feeder\n\n    Suggestions and feedbacks are welcome.\n  \n    Thanks"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hello everyone! A couple of months ago, I shared details about my open-source project, which received a warm reception from the wonderful Reddit community, including you. Thanks to your recommendations, I've significantly improved and updated the project.\n  \n    Today, I got a very interesting idea. Since I regularly work with remote servers, I thought, just out of curiosity, what if I create a Telegram wrapper that allows executing commands on a server remotely? And you know what - that's exactly what I did.\n  \n    I implemented functionality that enables sending basic commands through a Telegram bot, which are then directly executed on the server where the bot is located. The project is implemented in a super raw and basic form, in the Proof Of Concept format and nothing more. Ideally, I would like to get feedback from other developers on how interesting this project could be. Also, I'm curious to learn how to execute remote commands without using subprocess, within a loop, to preserve, let's say, the \"progress.\" For example, if I execute 'cd ..', write 'ls' and want to see the current directory. How to solve this is not entirely clear at the moment.\n  \n    Nevertheless, I hope you can appreciate my idea and try out the prototype I've assembled: https://github.com/dinosaurtirex/telegram_ssh_bot"
},
{
    "title": "No title",
    "content": "Hi I made a discord bot called Echo that has 1500+ lines of code and 50+ commands along with a very complex economy game and a large amount of commands for fun and moderation.\n  \n    Its still in early deveopemnt but I am constantly updating the project. Any suggestions will be appreciated!\n  \n    Source Code: https://github.com/DanielJones02/Echo"
},
{
    "title": "No title",
    "content": "I saw such a question appear here. I was bored today, so I implemented a program that recognizes the number of lines in (.py) files of your GitHub and draws a couple of simple but informative graphs. You will only need to insert your Token from GitHub Developer Settings.\n  \nhttps://github.com/Mooncake911/GitHub-Statistics"
},
{
    "title": "No title",
    "content": "If you rely on pdb for debugging your Python code and find it slow, then consider giving Dragonfly a try https://github.com/P403n1x87/dragonfly.\n  \n    Dragonfly is a lightweight CPython debugger designed with speed in mind. Contrary to more traditional debuggers, like pdb, Dragonfly does not rely heavily on tracing, allowing the target application to run at full speed in most cases. Occasionally, tracing might be required, so that the slowdown would be similar to that of pdb in the worst case."
},
{
    "title": "No title",
    "content": "TLDR: I've thrown together a one \"page\" reference documenting the major changes to between Python versions.\n  \n    I've spent a fair amount of time recently upgrading some old code-bases and would have found it helpful to have a one page summary of changes between versions. I couldn't find one via Google so decided to create one for myself.\n  \n    It might be useful for others so sharing it ☺️"
},
{
    "title": "No title",
    "content": "Just published my first ever article!\n  \n    \"Finding the fastest Python JSON library on all Python versions (8 compared)\".\n  \n    Read now, for free, without ads, on my blog:\n  \nhttps://catnotfoundnear.github.io/finding-the-fastest-python-json-library-on-all-python-versions-8-compared.html\n\n    I will truly appreciate your suggestions or recommendations! Thank you!EDIT: I have just uploaded my second tutorial on extending your SaaS Django website to reach a global audience and boost sales with a free method. I'd love to hear your thoughts:https://catnotfoundnear.github.io/the-guide-to-making-your-django-saas-business-worldwide-for-free.html\n\n    - Anna Willis (Catnotfoundnear)"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hi all! First post here (engagement of any kind, actually) so wanted to present my first Python package. RasterioXYZ is used to tile georeferenced raster images (in the form of Rasterio DatasetReader objects) according to the XYZ tiles standard.\n  \n    Designed to be straightforward, flexible, and memory efficient, RasterioXYZ was created in response to the comparatively manual process used in creating such tiles in a previous role.\n  \n    Right now, it's much slower than equivalent functionality in QGIS, a popular open-source desktop GIS software. However, as noted in the benchmarks and roadmap sections of the README, there's plenty for me to look at to address this, as well as features to add.\n  \n    GitHub: https://github.com/duncanmartyn/rasterioxyz\n\n    Any feedback is welcome, cheers!"
},
{
    "title": "No title",
    "content": "The coding experience you gain is really all the coding dead ends you heavily invested in.\n  \n    So I developed ,on my Linux laptop ,a Flask app with Nginx,Gunicorn,Celery, and Redis Set-ups.I looked at a few hosting providers and the whole process seemed overwhelming ,at least to me.\n  \n    I then stumbled upon pythonanywhere (PA) and hosting became a breeze :\n  \n\n\n    I ditched Nginx ,Gunicorn and Celery as pythonanywhere provides it own flavors in the area.\n  \n\n\n    Set up a remote ,free, redis db on Redis Labs as PA has no native support for redis.\n  \n\n\n\n\n    I got an automated SSL Certificate as a courtesy of PA.\n  \n\n\n\n\n    I simply declared one of my script as a scheduled task.\n  \n\n\n    Subscribed to a yearly plan for a very affordable price (down from their standard 5 eur a month).\n  \n\n\n    This was all there was to get me hosted using a domain name from namecheap.\n  \n    I then found out that PA using natively a multi workers environment (yes..I am a bit slow...) my web app behaved very differently from the one worker environment provided by my local machine. To get it to work ,as it should ,I had to resort to two extra series of changes in my app design:\n  \n\n\n    Replace some heavily used global variables with flask session variables.\n  \n\n\n    Add some javascript to a few html selects to disable/enable them for some milliseconds\n  \n\n\n    I thought I would share the experience and may be get some comments on this multiple/single worker context ,of which I have a very limited knowledge."
},
{
    "title": "No title",
    "content": "Hey, i made this app called ezres using customtkinter, it's an app that allows you to quickly change your Fortnite in-game resolution and fps lock, also allows to enable exclusive fullscreen for any game. ezres github"
},
{
    "title": "No title",
    "content": "With the Python on Microcontrollers newsletter, you get all the latest information on Python running on hardware in one place! MicroPython, CircuitPython and Python on single Board Computers like Raspberry Pi & many more.\n  \n    The Python on Microcontrollers newsletter is the place for the latest news. It arrives Monday morning with all the week’s happenings. No advertising, no spam, easy to unsubscribe.\n  \n    10,817 subscribers - the largest Python on hardware newsletter out there.\n  \n    Catch all the weekly news on Python for Microcontrollers with adafruitdaily.com.\n  \n    This ad-free, spam-free weekly email is filled with CircuitPython, MicroPython, and Python information that you may have missed, all in one place!\n  \n    Ensure you catch the weekly Python on Hardware roundup– you can cancel anytime – try our spam-free newsletter today!\n  \nhttps://www.adafruitdaily.com/"
},
{
    "title": "No title",
    "content": "Here's a tool I put together that makes nice photo collages. Hope you enjoy!\n  \nv1.2 released, with image rotation option\n\n    github link: https://github.com/twilsonco/PyPhotoCollage\n\n    It's available as vanilla Python (only uses pillow), and comes in a Pythonista (iOS app) version with a nice UI, and as a native iOS/macOS Siri Shortcut."
},
{
    "title": "No title",
    "content": "My wife wrote a children's book (8-12yo) a while back, and I took on the challenge of writing a sequel using ChatGPT. It was a fun project and I built a handy tool to automate the book writing given a story outline. It makes iterative API calls so the token count doesn't blow out.\n  \n    Source in case it's of interest: https://github.com/dylanhogg/gptauthor\n\n    How It Works\n  \n\n\nHuman written story description: you describe your story outline, writing style, characters etc in a story prompt (an example)\n  \n\n\nRun GPTAuthor: choosing model, temperature, and number of chapters to write.\n  \n\n\nAI generated synopsis: Given the story prompt, GPTAuthor uses ChatGPT to automatically turn this into a synopsis.\n  \n\n\nHuman review of synopsis: You are given a chance to review the synopsis and (optionally) make changes.\n  \n\n\nAI generated story: Each chapter is iteratively written by ChatGPT given the common synopsis and previous chapter. The full story is written as Markdown and HTML folder for your reading pleasure.\n  \n\n\n    See an Example of a short story about the OpenAI Leadership Crisis last year\n\n    \"In the heart of San Francisco, nestled among the city's tech giants and start-up hopefuls, stood the OpenAI office. A hive of activity, it buzzed with the sound of keyboards clacking, coffee machines hissing, and the occasional drone of a philosophical debate about whether AI could develop a taste for late-night taco runs. It was a typical day, or so everyone thought.\" continued...\n\n    You can even Write your own story easily in Google Colab\n\n    Writing a few chapters with gpt-3.5-turbo only costs 1 or 2 cents to run with your OpenAI API key. [edit: or you can currently specify a localhost API endpoint, with the ability to set a custom URL coming soon, as mentioned in the comments]\n  \n    The results for the sequel were mixed - the best part was using it for coming up with ideas and creating various puzzles.\n  \n    I hope someone has fun with this :)"
},
{
    "title": "No title",
    "content": "You can use AI to create a database schema, and API Logic Server to create App and API micro services in minutes.\n  \n    API Logic Server is a an open source project, consisting of a CLI (creates Python projects from databases), and a set of runtime libraries (Flask, SQLAlchemy, etc).\n  1. AI: Use ChatGPT to create schema\n    You can enter natural language to ChatGPT:\n    Create a sqlite database for customers, orders, items and product\n\n  Hints: use autonum keys, allow nulls, Decimal types, foreign keys, no check constraints.\n\n  Create a few rows of only customer and product data.\n\n  Enforce the Check Credit requirement:  \n\n     Customer.Balance <= CreditLimit  \n     Customer.Balance = Sum(Order.AmountTotal where date shipped is null)  \n     Order.AmountTotal = Sum(Items.Amount)  \n     Items.Amount = Quantity \\* UnitPrice  \n     Store the Items.UnitPrice as a copy from Product.UnitPrice\n    ChatGPT will provide SQL DDL.  Paste this into your sql tool to create a new database.  In this example, we created a sqlite database called sample_ai.sqlite.\n  2. Use API Logic Server: create working software - 1 command\n    API Logic Server creates Python projects from databases:\n  ApiLogicServer create --project_name=sample_ai \\\n                      --db_url=sqlite:///sample_ai.sqlite\n    This command reads the database schema, and creates an executable Python project.  You can open it in your IDE and run it.  The app provides:\n  \n\n\nApp Automation: a multi-page, multi-table admin app\n  \n\n\nAPI Automation: a JSON:API - crud for each table, with filtering, sorting, optimistic locking and pagination.  Plus swagger.\n  \n\n\n    Within minutes, front end developers can use the API - no more blocking on server development.  Business users can use the App as a basis for agile collaboration and iteration.\n  3. Customize the project with your IDE\n    Microservices must implement their semantics for security and integrity.  API Logic Server includes a rule engine that enables you to declare these.\n  \nLogic Automation means that you can declare spreadsheet-like rules using Python.  Such logic maintains database integrity with multi-table derivations and constraints.  Rules are 40X more concise than traditional code.  The following 5 rules would require 200 lines of Python:\n      \"\"\" Declarative multi-table derivations and constraints,\n        extensible with Python. \n\nUse code completion (Rule.) to declare rules here\n\nCheck Credit - Logic Design (note: translates directly into rules)\n\n1. Customer.Balance <= CreditLimit\n2. Customer.Balance = Sum(Order.AmountTotal where unshipped)\n3. Order.AmountTotal = Sum(Items.Amount)\n4. Items.Amount = Quantity * UnitPrice\n5. Items.UnitPrice = copy from Product\n\"\"\"\n\nRule.constraint(validate=models.Customer, \n    as_condition=lambda row: row.Balance <= row.CreditLimit,\n    error_msg=\"balance ({round(row.Balance, 2)}) exceeds credit ({round(row.CreditLimit, 2)})\")\n\nRule.sum(derive=models.Customer.Balance,     # adjusts...\n    as_sum_of=models.Order.AmountTotal,      # *not* a sql select sum...\n    where=lambda row: row.ShipDate is None)  \n\nRule.sum(derive=models.Order.AmountTotal\n    as_sum_of=models.Item.Amount)\n\nRule.formula(derive=models.Item.Amount, \n    as_expression=lambda row: row.UnitPrice * row.Quantity)\n\nRule.copy(derive=models.Item.UnitPrice,  \n    from_parent=models.Product.UnitPrice)4. Iterate: use Python and Standard Libraries\n    Projects are designed for iteration.  You can change the database design, are rebuild the SQLAlchemy models while preserving customizations.\n  \n    You can add Python, .e.g. for Application Integration:\n      def send_order_to_shipping(row: models.Order, \n                               old_row: models.Order,\n                               logic_row: LogicRow):\n    \"\"\" #als: Send Kafka message formatted by OrderShipping RowDictMapper\n\n    Format row per shipping requirements, and send Kafka message\n\n    Args:\n        row (models.Order): inserted Order\n        old_row (models.Order): n/a\n        logic_row (LogicRow): bundles curr/old row, with ins/upd/dlt logic\n    \"\"\"\n\n    if logic_row.is_inserted():\n        order_dict = OrderShipping().row_to_dict(row = row)\n        json_order = jsonify({\"order\": order_dict}).data.decode('utf-8')\n        if kafka_producer.producer:  # enabled in config/config.py?\n            try:\n                kafka_producer.producer.produce(value=json_order,\n                        topic=\"order_shipping\", key= str(row.Id))\n                logic_row.log(\"Kafka producer sent message\")\n            except KafkaException as ke:\n                logic_row.log(\"Kafka.produce msg {row.id} error: {ke}\")                \n        print(f'\\n\\nSend to Shipping:\\n{json_order}')\n        \nRule.after_flush_row_event(on_class=models.Order, \n                               calling=send_order_to_shipping)  # see above\n    You can also extend your API to create new endpoints, using Flask.\n  \n    API Logic Server creates scripts to containerize your project, so you can deploy it to your local server or the cloud.\n  \n    You can see a screen shot summary of this project here, or develop it yourself using this tutorial."
},
{
    "title": "No title",
    "content": "Interesting article about datetime in Python: https://dev.arie.bovenberg.net/blog/python-datetime-pitfalls/\n\n    The library the author is working on looks really interesting too: https://github.com/ariebovenberg/whenever"
},
{
    "title": "No title",
    "content": "I finally achieved a milestone of supporting more then 100+ services with Apprise and just wanted to share with with you all! It is very much a useful devops tool just due to the fact you can trigger notifications from successful builds, deploys, failures, via monitoring, etc.\n  \n    This is a cross post from r/selfhosted; Mods, please feel free to delete this if it's not acceptable to also share here.\n  \nWhat is Apprise?\n\n    Apprise allows you to send a notification to almost all of the most popular notification services available to us today such as: Telegram, Discord, Slack, Amazon SNS, Gotify, etc.\n  \n\n\n    One notification library to rule them all.\n  \n\n\n    A common and intuitive notification syntax.\n  \n\n\n    Supports the handling of images and attachments (to the notification services that will accept them).\n  \n\n\n    It's incredibly lightweight.\n  \n\n\n    Amazing response times because all messages sent asynchronously.\n  \n\n\nI still don't get it... ELI5\n\n    Apprise is effectively a self-host efficient messaging switchboard. You can automate notifications through:\n  \n\n\n    the Command Line Interface (for Admins)\n  \n\n\n    it's very easy to use Development Library (for Devs) which is already integrated with many platforms today such as ChangeDetection, Uptime Kuma (and many others.\n  \n\n\na web service (you host) that can act as a sidecar. This solution allows you to keep your notification configuration in one place instead of across multiple servers (or within multiple programs). This one is for both Admins and Devs.\n  \n\n\nWhat else does it do?\n\n\n\n    Emoji Support (:rocket: -> 🚀) built right into it!\n  \n\n\n    File Attachment Support (to the end points that support it)\n  \n\n\n    It supports inputs of MARKDOWN, HTML, and TEXT and can easily convert between these depending on the endpoint. For example: HTML provided input would be converted to TEXT before passing it along as a text message. However the same HTML content provided would not be converted if the endpoint accepted it as such (such as Telegram, or Email).\n  \n\n\n    It supports breaking large messages into smaller ones to fit the upstream service. Hence a text message (160 characters) or a Tweet (280 characters) would be constructed for you if the notification you sent was larger.\n  \n\n\n    It supports configuration files allowing you to securely hide your credentials and map them to simple tags (or identifiers) like family, devops, marketing, etc. There is no limit to the number of tag assignments. It supports a simple TEXT based configuration, as well as a more advanced and configurable YAML based one.\n  \n\n\n    Configuration can be hosted via the web (even self-hosted), or just regular (protected) configuration files.\n  \n\n\n    Supports \"tagging\" of the Notification Endpoints you wish to notify. Tagging allows you to mask your credentials and upstream services into single word assigned descriptions of them.  Tags can even be grouped together and signaled via their group name instead.\n  \n\n\n    Dynamic Module Loading: They load on demand only. Writing a new supported notification is as simple as adding a new file (see here)\n  \n\n\n    Developer CLI tool (it's like /usr/bin/mail on steroids)\n  \n\n\n    It's worth re-mentioning that it has a fully compatible API interface found here or on Dockerhub which has all of the same bells and whistles as defined above. This acts as a great side-car solution!\n  \nProgram Details\n\n\n\n    Entirely a self-hosted solution.\n  \n\n\n    Written in Python\n  \n\n\n    99.27% Test Coverage (oof... I'll get it back to 100% soon)\n  \n\n\n    BSD-2 License\n  \n\n\n    Over 450K downloads a month on PyPi (source)\n  \n\n\n    Over 2.8 million downloads from Docker Hub\n  \n\n\n    I would love to hear any feedback any of you have!\n  \n    Edit: Added link to Apprise :)"
},
{
    "title": "No title",
    "content": "Hello everyone,\n  \n    Recently, I've been exploring the idea of using multiple GPT models to simulate a role-playing environment where AI agents collaborate to tackle complex problems. In my research, I stumbled upon an this amazing python framework called Crew AI. I found it so intriguing that I decided to create a brief tutorial to share my insights and experiences. For anyone interested, you can find my tutorial here: Meet Your Digital Dream Team: Revolutionizing the Tech World with AI.\n  \n    I hope you find it as exciting and useful as I did! Would love to know your thoughts and ideas around the topic."
},
{
    "title": "No title",
    "content": "Hello Lovely Pythonistas,\n  \n    I would like to share my project PiSegment (https://github.com/aGIToz/PiSegment), written in python. It allows to segment the regions of images with little bit of human annotation. Typical use cases are background extraction, organ segmentation in medical imaging, creating dataset for semantic segmentation, colorization etc."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "me and my friend created an open-source tool called De4py that i think would be useful to analyze python malware and monitor their behavior, it got a beautiful UI and some nice features.Deobfuscation: De4py support some popular obfuscators, like: Jawbreaker, BlankOBF, PlusOBF, Wodx, Hyperion, pyobfuscate.com obfuscator.Pycode Execution: Executing your own python code inside the process.\n  \n    Strings Dump: Dumping Strings in the python process and saving it as a file which can be pretty useful to extract data from memory such as webhooks.Behavior Monitoring: De4py can monitor python processes and see if they opened any files handles, opened a process, wrote/readed the memory of other processes and also monitoring if the process terminated other processes, in addition to sockets monitoring.File Analyzer: an analyzer that have many features like detecting if the python program is packed and tries to unpack it if it was using pyinstaller for example, it also got a feature that shows either all strings or suspicious strings (suspicious strings like: IPs, websites, \"discord\", \"leveldb\" and other suspicious strings in the file) and shows them in a nice output window.it also supports pulgins, and some other features that you can discover when viewing the repo.i hope it would be useful for anyone analyzing py malware."
},
{
    "title": "No title",
    "content": "I've just noticed that iterators are iterable in Python.\n  \n    I heavily rely on static types, and I thought I had a pretty good idea of what an Iterable object was, so I thought the following (deliberately silly) function was perfectly correct:\n  def print_twice(numbers: Iterable[int]):\n    print(', '.join(str(x) for x in numbers))\n    print(', '.join(str(x) for x in numbers))\n    Unfortunately, this is not the case. Indeed, while print_twice([1,2,3,4]) prints twice, print_twice(iter([1,2,3,4])) only prints once.\n  \n    Even worse, the code fails silently: no static nor runtime error!\n  \n    Am I the only one surprised by this?\n  \n    My mental model has always been:\n  \n\n\nIterable: produces iterators, so I can iterate it multiple times\n  \n\n\nIterator: I can only use it once\n  \n\n\n    Semantically, it doesn't make much sense for an iterator to be iterable. Indeed, iterating over an iterable object shouldn't alter it in any way, which means that the object must remain iterable (and produce the same elements) as long as no one alters it or some external resource the object depends on changes.\n  \n    Am I missing something? Why was it decided to make iterators iterable?"
},
{
    "title": "No title",
    "content": "I sometimes use Tkinter to make a GUI for small projects. The thing is I forget everything everytime I take a break and im tired of coding every single component, is there a better way to create GUIs in python?"
},
{
    "title": "No title",
    "content": "Yet another Chip8 emulator with debug features, this time using DearPyGui.\n  \nYachipy\n\n    🎮 Welcome to YaChiPy, a Chip-8 Emulator with Debug Features!\n  \n\n\n    🕹️ Get ready to step back in time and experience the charm of retro gaming, now with powerful debug capabilities!\n  \n\n\n    🚀 Dive into the world of Chip-8 and enjoy classic pixelated graphics. But wait, there's more! This emulator comes equipped with advanced debug features to enhance your gaming and programming experience.\n  \n\n\n    🕵️‍♂️ Pause the emulator at any moment to inspect the state of the virtual machine. Take control of time with the ability to speed up or slow down the CPU clock, allowing you to fine-tune your gaming experience.\n  \n\n\n    🔍 Peek into the machine's heart! Displayed in real-time are the memory contents, register values, and timer values. Understand the inner workings of the Chip-8 machine as you play and program.\n  \n\n\n    ⏭️ Want to understand every step of the action? Execute instructions one tick at a time with our step-by-step mode. Uncover the secrets of Chip-8 programming and gaming at your own pace.\n  \n\n\n    🎮 Load your favorite ROMs, customize settings, and embark on a journey filled with nostalgia and discovery. Whether you're a seasoned enthusiast or a curious newcomer, this emulator is your gateway to the golden age of gaming, now with the added thrill of detailed debugging.\n  \n\n\n    It is in pre-release state as of now, as I still have to add few unique features that I really want it to have that will help it stand-out a little among the sea of Chip-8 emulators.\n  \n    Link to Public Repo: YaChiPy"
},
{
    "title": "No title",
    "content": "I have a created an app to created RSS feeds for  websites that don't have them.  I have been using it to mostly keep track of jobs from various sites.\n  \n    The source code is present here : https://github.com/rk1165/feeder\n\n    Suggestions and feedbacks are welcome.\n  \n    Thanks"
},
{
    "title": "No title",
    "content": "CashFlow is an Fiannce manager built using PyQt6 and Python. It offers the facility to store income and expense details (with Graphs), and also calculate investments, interests, etc.\n  \n    GitHub: https://github.com/rohankishore/CashFlow"
},
{
    "title": "No title",
    "content": "ZenNotes is a Notepad replacement with TTS, Translations, Encryption and much more.\n  \n    GitHub: https://github.com/rohankishore/ZenNotes"
},
{
    "title": "No title",
    "content": "nowpy\nnowpy is a CLI-tool I made that enables you to run any arbitrary Python script instantly, on the basis that I run code far more often than I write it. It combines python, virtualenv, and pip to launch a dedicated isolated environment, automatically figure out which packages are required, and then run your Python file - all with just one command.\n  \nnowpy finds packages by performing a recursive lookup for a pyproject.toml OR a requirements.txt, and cross-checks with any import statements inside the Python file.\n  Installation\n    Use the package manager pip to install nowpy.\n  pip install nowpy\n    It might be the last time you have to pip install anything!\n  Usage\n    Here's an example of what happens if you run nowpy on a Python file that imports requsts.\n  \n    First run:\n  nowpy WorldTimeApi.py\n\nCreating Virtualenv...\nCollecting requests\n...\nInstalling collected packages: urllib3, idna, charset-normalizer, certifi, requests\nSuccessfully installed certifi-2023.11.17 charset-normalizer-3.3.2 idna-3.6 requests-2.31.0 urllib3-2.2.0\nRunning Script...\n\nCurrent Time in Europe/London\nDate: 2024-01-30T22:08:52.854140+00:00\nTimezone: Europe/London\n    All future runs:\n  nowpy WorldTimeApi.py\n\nRunning Script...\n\nCurrent Time in Europe/London\nDate: 2024-01-30T22:08:52.854140+00:00\nTimezone: Europe/London\nnowpy creates a unique virtual environment for every directory you run nowpy from. It also removes unused ones automatically. But if you ever want to reset a particular one that you're using, just use the --reset option:\n  nowpy --reset\n    I've not tested it on anything except my Mac - let me know if there are any issues!\n  \n    Source Code on GitHub: https://github.com/WillDenby/nowpy PyPI Page: https://pypi.org/project/nowpy/"
},
{
    "title": "No title",
    "content": "I recreated the game Buckshot roulette in python and wanted to share\n  \n    Here is the source code - https://github.com/ATharvaCoder492/Buckshot_Roulette\n\n    If you have any suggestions to make this better plz comment"
},
{
    "title": "No title",
    "content": "If you are looking for a tool to turn your Python scripts into web apps, you might have heard of Streamlit and Taipy. They are both popular and promising tools that allow you to create interactive and beautiful web apps from your Python code, without requiring any web development skills.I have created an article here that can help you see the differences:https://www.bitdoze.com/streamlit-vs-taipy/"
},
{
    "title": "No title",
    "content": "\"Finding the Air Cannon\"\n  \nhttps://www.twobraids.com/2024/01/air-cannon.html\n\n\n    It took three people stationed at remote locations miles apart using a synchronized clock on our cell phones. We each waited over the same ten minute period, noting the exact time for each of the five cannon shots that we heard.\n  \n\n\n    ...\n  \n\n\n    I wrote a program in Python (see source code below) that could iterate all the points in the image in the search area where we suspected the air cannon sat.\n  \n\n\n    ...\n  \n\n\n    I called the owner of the farm (headquartered in Monmouth) and asked if they used an air cannon on their property near the Corvallis airport. They confirmed that they do. I asked if they run it at night, they said they do not.\n  \n\n\n    ...\n  \n\n\n    However, in an amazing coincidence, the air cannons stopped that very evening of our phone conversation."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "https://www.youtube.com/watch?v=fVzFq909z_k  Video tutorial about the Python bindings for the Qt framework named pyside6 or \"Qt for Python\"."
},
{
    "title": "No title",
    "content": "If you’re writing numeric Python code, Numba can be a great way to speed up your program. By compiling a subset of Python to machine code, Numba lets you write for loops and other constructs that would be too slow in normal Python. In other words, it’s similar to Cython, C, or Rust, in that it lets you write compiled extensions for Python.Numba code isn’t always as fast as it could be, however. This is where profiling is useful: it can find at least some of the bottlenecks in your code. Learn more about Profila, a new profiler for Numba that I've just released on GitHub."
},
{
    "title": "No title",
    "content": "I wrote this tutorial on how to consume the Hacker News API as a stream and process it with open source tools Bytewax, Proton and Grafana. It is kind of fun to see who all the armchair experts are; weighing in on every story posted to hacker news :P.\n  \n    I thought this might be useful if you are working with real-time data and need to build some live dashboards.\n  \n    Let me know what you think!\n  \n\nhttps://github.com/bytewax/hacking-hacker-news"
},
{
    "title": "No title",
    "content": "Hey everyone,\n  \n    I am back again with some good news. After many requests for OpenAPI support, it has been finally added in 0.1.8You can install the optional dependencies with pip install arrest[openapi]\n\n    This provides a CLI interface where you can provide the URL to the OpenAPI specification file (json or yaml). The URL can be an HTTP URL or a filepath. Arrest will generate the necessary boilerplates such as the schema definition Pydantic models (courtesy of datamodel-code-generator), Arrest resources and services (using Jinja templates).\n  \n    Arrest is a small package to define the structure of RESTful APIs from an external source with type annotations, making it easier to interface with, and call different routes of an external web service from inside a Python application, such as a third-party API, or an internal microservice, while also providing data validation for the request, response, exception handling and retries. For more information please check out the docs and the repository\n\n    Please note that the OpenAPI integration is an active work in progress. There are many features missing that I will be adding gradually such as extracting the header and query parameters from the specification, or support for composite types like list[Model] or dict[str, Model] will be added soon.\n  \n    I would greatly appreciate your feedback and any areas of improvement. Thank you to everyone here for supporting this project and providing the feedback that made it work!\n  \n\n    edit: added a bit of context for the package"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Below you will find the link to my tutorial on using the Ollama Python library. Here I show with the help of examples how to use models like Mistral to create messages and go into more detail about the chat method, the system message and the streaming option.\n  \n    I also show how to use the streaming option. Finally, how to correct code with options like the temperature parameter.\n  \n    Link: Ollama Python Library - Tutorial with Examples"
},
{
    "title": "No title",
    "content": "Many of y'all may have seen this already, but I just stumbled upon it this weekend:\n  \nhttps://docs.conda.io/projects/conda/en/latest/release-notes.html#id2\n\n    About two years ago (which I also missed), conda added mamba libraries as an experimental option. Apparently it went well enough to make it official. Version 10.23 (from October 2023) now uses mamba libraries  by default.\n  \n    I ran some quick and dirty tests, and the new conda is was only about 10% to 50% slower than mamba on some big environments. This is a huge improvement from before where it could an order of magnitude slower.\n  \n    Edit: added date and version of the update"
},
{
    "title": "No title",
    "content": "Wrote and updated a Python package that allows to inspect and correct PATH that may have non-existent directories. justpath unpacks the PATH into lines, annotates errors and can assemble the PATH string back, so that you can use it in shell startup script. Works both for Windows and Linux.\n  \n    To get corrected content of your PATH:\n  justpath show --correct --string\n    Code and more examples: https://github.com/epogrebnyak/justpath\n\n    Justpath current version (0.0.9) benefited a lot form a previous r/Python discussion here, some main points:\n  \n\n\n    on linux for showing path you can simply use echo \"$PATH\" | tr \":\" \"\\n\" | sort;\n  \n\n\n    exploring PYTHONPATH may be as important;\n  \n\n\n    may also want to detect and purge duplicate folders from PATH."
},
{
    "title": "No title",
    "content": "🚀 in our latest video tutorial, we will cover photo restoration using GFPGAN! Really cool Python library.\n  \n    The tutorial is divided into four parts:\n  \n    🖼️ Part 1: Setting up a Conda environment for seamless development and Installing essential Python libraries.\n  \n    🧠 Part 2: Cloning the GitHub repository containing the code and resources.\n  \n    🚀 Part 3: Apply the model on your own images\n  \n    You can find the instructions here : https://github.com/feitgemel/Python-Code-Cool-Stuff/tree/master/GFPGAN\n\n    The link for the video :  https://youtu.be/nPnQm7HFWJs\n\n    Enjoy\n  \n    Eran\n  \n    #python #GFPGAN #increaseimageresolution #Enhancephoto"
},
{
    "title": "No title",
    "content": "Sharing a personal project I've been working on for a few months.docsgithub\n\n    This project is born from my need for an easy way to memoize to disk the result of expensive functions like Queries or ML-preprocessing.Any feedback or contribution is welcome!"
},
{
    "title": "No title",
    "content": "https://github.com/saketh12/Auto1111SDK\n\n    Hey everyone, I built an light-weight, open-source Python library for the Automatic 1111 Web UI that allows you to run any Stable Diffusion model locally on your infrastructure. You can easily run:\n  \n\n\n    Text-to-Image\n  \n\n\n    Image-to-Image\n  \n\n\n    Inpainting\n  \n\n\n    Outpainting\n  \n\n\n    Stable Diffusion Upscale\n  \n\n\n    Esrgan Upscale\n  \n\n\n    Real Esrgan Upscale\n  \n\n\n    Download models directly from Civit AI\n  \n\n\n    With any safetensors or Checkpoints file all with a few lines of code!! It is super lightweight and performant. Compared to Huggingface Diffusers, our SDK uses considerably less memory/RAM and we've observed up to a 2x speed increase on all the devices/OS we tested on!Please star our Github repository!!! https://github.com/saketh12/Auto1111SDK ."
},
{
    "title": "No title",
    "content": "BALENA is a voice interaction framework utilizing state-of-the-art natural language processing and audio processing models to create a system that can interpret voice commands and associate them with predefined actions. The framework leverages the power of transformers and signal processing to understand user intent via spoken language and correlates them with a series of predefined actionable responses.\n  \n    Contributions are welcome on : https://pypi.org/project/balena-cpu/\n\n    GitHub : https://github.com/louisbrulenaudet/balena"
},
{
    "title": "No title",
    "content": "I'm excited to share my first Python package, EasyGmail. It's an open-source package designed to simplify sending emails via Gmail. My goal was to create something lightweight and user-friendly, especially for those who are just starting with Python email automation.\n  \n    🔗 GitHub: https://github.com/ayushgun/easygmail\n\nKey Features\n\n\n\n    Simplicity. The easygmail.Client interface is designed to be intuitive and minimal.\n  \n\n\n    Provides easygmail.EmailBuilder, an intuitive abstraction for creating email.message.EmailMessage objects, Python's standard for structured emails.\n  \n\n\n    Flexibility. Multiple way to construct client objects or build structured emails. See the README for more information.\n  \n\n\n    Secure. Allows users to provide authentication details via .env files instead of hardcoded credentials. Uses Gmail app passwords instead of account passwords.\n  \n\n\nQuick Start\n\n    See the README file for a quick start example.\n  \n    I would love to get your feedback on this project. Whether it's suggestions for improvement, feature requests, or just your thoughts on its usability, all feedback is greatly appreciated!"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "The \"Turtle Race\" is a classic beginners application using the Turtle library. My implementation takes an OOP approach and I've tried to follow best practices. Feedback, is welcome, but please don't downvote without leaving a comment. https://bitbucket.org/jameztyson/turtlerace/src/main/turtle_race.py"
},
{
    "title": "No title",
    "content": "Hi everyone. I'd like to share my design of a service for calculating permissions based on OpenFGA.\n  \nhttps://github.com/t1waz/relation_fga\n\n    I really liked OpenFGA it is a great idea for a service, however, in the implementation I hit a lot of problems. It turns out that on the list of objects relatively slow. I decided to try it with a different approach, graphical, and it seems to be much more efficient. Also graph approach opens up a lot of possibilities for data state analysis\n  \n    I'm open to criticism, I'd love to meet someone who would be interested in the topic."
},
{
    "title": "No title",
    "content": "Quick overview:This small script runs in the background and scans for an input device (I've used my keyboard) being plugged in or unplugged. If it detects a change then it sends a code to your monitor to swap to the specified input.\n  \n    Now with my cheap USB switch I can swap display inputs with a single button press.\n  \n    Disclaimer - I don't know if this will work with your monitor, or if it could do any damage, I really just threw this together and wanted to share.\n  \nBackground:I've wanted a proper KVM to switch between my Work & Personal computers on my single monitor for a while, but they all seem super expensive (maybe there's better options out there but who knows!).\n  \n    Anyway I realised that my AOC monitor had some windows software call G-Menu and it could swap the inputs on my monitor from windows, instead of going through the relatively cumbersome menu via the monitor's buttons.\n  \n    I started looking into how it worked and found a few resources / repos where people had created their own monitor settings program, and managed to put together this small script.\n  \n    It's configured for my setup so will need adjusting if you want to use it, but I thought it might help someone who had the same issue, but never happened to have that lightbulb moment to realise it could be done pretty easily.\n  \n    Code:https://github.com/a-j-jones/monitor_input_swapper\n\n    Edit: Clarified that I'm using a cheap USB switch to actually use this code"
},
{
    "title": "No title",
    "content": "Whether if it's something simple and trivial or complex and robust, small project or well known library, you can describe what it is or even better post a link to the Github repository and elaborate what makes it cool.\n  \n    Edit: \"Python automation projects\" in the title, there is no edit option."
},
{
    "title": "No title",
    "content": "CashFlow is an Fiannce manager built using PyQt6 and Python. It offers the facility to store income and expense details (with Graphs), and also calculate investments, interests, etc.\n  \n    GitHub: https://github.com/rohankishore/CashFlow"
},
{
    "title": "No title",
    "content": "ZenNotes is a Notepad replacement with TTS, Translations, Encryption and much more.\n  \n    GitHub: https://github.com/rohankishore/ZenNotes"
},
{
    "title": "No title",
    "content": "nowpy\nnowpy is a CLI-tool I made that enables you to run any arbitrary Python script instantly, on the basis that I run code far more often than I write it. It combines python, virtualenv, and pip to launch a dedicated isolated environment, automatically figure out which packages are required, and then run your Python file - all with just one command.\n  \nnowpy finds packages by performing a recursive lookup for a pyproject.toml OR a requirements.txt, and cross-checks with any import statements inside the Python file.\n  Installation\n    Use the package manager pip to install nowpy.\n  pip install nowpy\n    It might be the last time you have to pip install anything!\n  Usage\n    Here's an example of what happens if you run nowpy on a Python file that imports requsts.\n  \n    First run:\n  nowpy WorldTimeApi.py\n\nCreating Virtualenv...\nCollecting requests\n...\nInstalling collected packages: urllib3, idna, charset-normalizer, certifi, requests\nSuccessfully installed certifi-2023.11.17 charset-normalizer-3.3.2 idna-3.6 requests-2.31.0 urllib3-2.2.0\nRunning Script...\n\nCurrent Time in Europe/London\nDate: 2024-01-30T22:08:52.854140+00:00\nTimezone: Europe/London\n    All future runs:\n  nowpy WorldTimeApi.py\n\nRunning Script...\n\nCurrent Time in Europe/London\nDate: 2024-01-30T22:08:52.854140+00:00\nTimezone: Europe/London\nnowpy creates a unique virtual environment for every directory you run nowpy from. It also removes unused ones automatically. But if you ever want to reset a particular one that you're using, just use the --reset option:\n  nowpy --reset\n    I've not tested it on anything except my Mac - let me know if there are any issues!\n  \n    Source Code on GitHub: https://github.com/WillDenby/nowpy PyPI Page: https://pypi.org/project/nowpy/"
},
{
    "title": "No title",
    "content": "I recreated the game Buckshot roulette in python and wanted to share\n  \n    Here is the source code - https://github.com/ATharvaCoder492/Buckshot_Roulette\n\n    If you have any suggestions to make this better plz comment"
},
{
    "title": "No title",
    "content": "If you are looking for a tool to turn your Python scripts into web apps, you might have heard of Streamlit and Taipy. They are both popular and promising tools that allow you to create interactive and beautiful web apps from your Python code, without requiring any web development skills.I have created an article here that can help you see the differences:https://www.bitdoze.com/streamlit-vs-taipy/"
},
{
    "title": "No title",
    "content": "\"Finding the Air Cannon\"\n  \nhttps://www.twobraids.com/2024/01/air-cannon.html\n\n\n    It took three people stationed at remote locations miles apart using a synchronized clock on our cell phones. We each waited over the same ten minute period, noting the exact time for each of the five cannon shots that we heard.\n  \n\n\n    ...\n  \n\n\n    I wrote a program in Python (see source code below) that could iterate all the points in the image in the search area where we suspected the air cannon sat.\n  \n\n\n    ...\n  \n\n\n    I called the owner of the farm (headquartered in Monmouth) and asked if they used an air cannon on their property near the Corvallis airport. They confirmed that they do. I asked if they run it at night, they said they do not.\n  \n\n\n    ...\n  \n\n\n    However, in an amazing coincidence, the air cannons stopped that very evening of our phone conversation."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "https://www.youtube.com/watch?v=fVzFq909z_k  Video tutorial about the Python bindings for the Qt framework named pyside6 or \"Qt for Python\"."
},
{
    "title": "No title",
    "content": "If you’re writing numeric Python code, Numba can be a great way to speed up your program. By compiling a subset of Python to machine code, Numba lets you write for loops and other constructs that would be too slow in normal Python. In other words, it’s similar to Cython, C, or Rust, in that it lets you write compiled extensions for Python.Numba code isn’t always as fast as it could be, however. This is where profiling is useful: it can find at least some of the bottlenecks in your code. Learn more about Profila, a new profiler for Numba that I've just released on GitHub."
},
{
    "title": "No title",
    "content": "I wrote this tutorial on how to consume the Hacker News API as a stream and process it with open source tools Bytewax, Proton and Grafana. It is kind of fun to see who all the armchair experts are; weighing in on every story posted to hacker news :P.\n  \n    I thought this might be useful if you are working with real-time data and need to build some live dashboards.\n  \n    Let me know what you think!\n  \n\nhttps://github.com/bytewax/hacking-hacker-news"
},
{
    "title": "No title",
    "content": "Hey everyone,\n  \n    I am back again with some good news. After many requests for OpenAPI support, it has been finally added in 0.1.8You can install the optional dependencies with pip install arrest[openapi]\n\n    This provides a CLI interface where you can provide the URL to the OpenAPI specification file (json or yaml). The URL can be an HTTP URL or a filepath. Arrest will generate the necessary boilerplates such as the schema definition Pydantic models (courtesy of datamodel-code-generator), Arrest resources and services (using Jinja templates).\n  \n    Arrest is a small package to define the structure of RESTful APIs from an external source with type annotations, making it easier to interface with, and call different routes of an external web service from inside a Python application, such as a third-party API, or an internal microservice, while also providing data validation for the request, response, exception handling and retries. For more information please check out the docs and the repository\n\n    Please note that the OpenAPI integration is an active work in progress. There are many features missing that I will be adding gradually such as extracting the header and query parameters from the specification, or support for composite types like list[Model] or dict[str, Model] will be added soon.\n  \n    I would greatly appreciate your feedback and any areas of improvement. Thank you to everyone here for supporting this project and providing the feedback that made it work!\n  \n\n    edit: added a bit of context for the package"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Below you will find the link to my tutorial on using the Ollama Python library. Here I show with the help of examples how to use models like Mistral to create messages and go into more detail about the chat method, the system message and the streaming option.\n  \n    I also show how to use the streaming option. Finally, how to correct code with options like the temperature parameter.\n  \n    Link: Ollama Python Library - Tutorial with Examples"
},
{
    "title": "No title",
    "content": "Many of y'all may have seen this already, but I just stumbled upon it this weekend:\n  \nhttps://docs.conda.io/projects/conda/en/latest/release-notes.html#id2\n\n    About two years ago (which I also missed), conda added mamba libraries as an experimental option. Apparently it went well enough to make it official. Version 10.23 (from October 2023) now uses mamba libraries  by default.\n  \n    I ran some quick and dirty tests, and the new conda is was only about 10% to 50% slower than mamba on some big environments. This is a huge improvement from before where it could an order of magnitude slower.\n  \n    Edit: added date and version of the update"
},
{
    "title": "No title",
    "content": "Wrote and updated a Python package that allows to inspect and correct PATH that may have non-existent directories. justpath unpacks the PATH into lines, annotates errors and can assemble the PATH string back, so that you can use it in shell startup script. Works both for Windows and Linux.\n  \n    To get corrected content of your PATH:\n  justpath show --correct --string\n    Code and more examples: https://github.com/epogrebnyak/justpath\n\n    Justpath current version (0.0.9) benefited a lot form a previous r/Python discussion here, some main points:\n  \n\n\n    on linux for showing path you can simply use echo \"$PATH\" | tr \":\" \"\\n\" | sort;\n  \n\n\n    exploring PYTHONPATH may be as important;\n  \n\n\n    may also want to detect and purge duplicate folders from PATH."
},
{
    "title": "No title",
    "content": "🚀 in our latest video tutorial, we will cover photo restoration using GFPGAN! Really cool Python library.\n  \n    The tutorial is divided into four parts:\n  \n    🖼️ Part 1: Setting up a Conda environment for seamless development and Installing essential Python libraries.\n  \n    🧠 Part 2: Cloning the GitHub repository containing the code and resources.\n  \n    🚀 Part 3: Apply the model on your own images\n  \n    You can find the instructions here : https://github.com/feitgemel/Python-Code-Cool-Stuff/tree/master/GFPGAN\n\n    The link for the video :  https://youtu.be/nPnQm7HFWJs\n\n    Enjoy\n  \n    Eran\n  \n    #python #GFPGAN #increaseimageresolution #Enhancephoto"
},
{
    "title": "No title",
    "content": "Sharing a personal project I've been working on for a few months.docsgithub\n\n    This project is born from my need for an easy way to memoize to disk the result of expensive functions like Queries or ML-preprocessing.Any feedback or contribution is welcome!"
},
{
    "title": "No title",
    "content": "https://github.com/saketh12/Auto1111SDK\n\n    Hey everyone, I built an light-weight, open-source Python library for the Automatic 1111 Web UI that allows you to run any Stable Diffusion model locally on your infrastructure. You can easily run:\n  \n\n\n    Text-to-Image\n  \n\n\n    Image-to-Image\n  \n\n\n    Inpainting\n  \n\n\n    Outpainting\n  \n\n\n    Stable Diffusion Upscale\n  \n\n\n    Esrgan Upscale\n  \n\n\n    Real Esrgan Upscale\n  \n\n\n    Download models directly from Civit AI\n  \n\n\n    With any safetensors or Checkpoints file all with a few lines of code!! It is super lightweight and performant. Compared to Huggingface Diffusers, our SDK uses considerably less memory/RAM and we've observed up to a 2x speed increase on all the devices/OS we tested on!Please star our Github repository!!! https://github.com/saketh12/Auto1111SDK ."
},
{
    "title": "No title",
    "content": "BALENA is a voice interaction framework utilizing state-of-the-art natural language processing and audio processing models to create a system that can interpret voice commands and associate them with predefined actions. The framework leverages the power of transformers and signal processing to understand user intent via spoken language and correlates them with a series of predefined actionable responses.\n  \n    Contributions are welcome on : https://pypi.org/project/balena-cpu/\n\n    GitHub : https://github.com/louisbrulenaudet/balena"
},
{
    "title": "No title",
    "content": "I'm excited to share my first Python package, EasyGmail. It's an open-source package designed to simplify sending emails via Gmail. My goal was to create something lightweight and user-friendly, especially for those who are just starting with Python email automation.\n  \n    🔗 GitHub: https://github.com/ayushgun/easygmail\n\nKey Features\n\n\n\n    Simplicity. The easygmail.Client interface is designed to be intuitive and minimal.\n  \n\n\n    Provides easygmail.EmailBuilder, an intuitive abstraction for creating email.message.EmailMessage objects, Python's standard for structured emails.\n  \n\n\n    Flexibility. Multiple way to construct client objects or build structured emails. See the README for more information.\n  \n\n\n    Secure. Allows users to provide authentication details via .env files instead of hardcoded credentials. Uses Gmail app passwords instead of account passwords.\n  \n\n\nQuick Start\n\n    See the README file for a quick start example.\n  \n    I would love to get your feedback on this project. Whether it's suggestions for improvement, feature requests, or just your thoughts on its usability, all feedback is greatly appreciated!"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "The \"Turtle Race\" is a classic beginners application using the Turtle library. My implementation takes an OOP approach and I've tried to follow best practices. Feedback, is welcome, but please don't downvote without leaving a comment. https://bitbucket.org/jameztyson/turtlerace/src/main/turtle_race.py"
},
{
    "title": "No title",
    "content": "Hi everyone. I'd like to share my design of a service for calculating permissions based on OpenFGA.\n  \nhttps://github.com/t1waz/relation_fga\n\n    I really liked OpenFGA it is a great idea for a service, however, in the implementation I hit a lot of problems. It turns out that on the list of objects relatively slow. I decided to try it with a different approach, graphical, and it seems to be much more efficient. Also graph approach opens up a lot of possibilities for data state analysis\n  \n    I'm open to criticism, I'd love to meet someone who would be interested in the topic."
},
{
    "title": "No title",
    "content": "Quick overview:This small script runs in the background and scans for an input device (I've used my keyboard) being plugged in or unplugged. If it detects a change then it sends a code to your monitor to swap to the specified input.\n  \n    Now with my cheap USB switch I can swap display inputs with a single button press.\n  \n    Disclaimer - I don't know if this will work with your monitor, or if it could do any damage, I really just threw this together and wanted to share.\n  \nBackground:I've wanted a proper KVM to switch between my Work & Personal computers on my single monitor for a while, but they all seem super expensive (maybe there's better options out there but who knows!).\n  \n    Anyway I realised that my AOC monitor had some windows software call G-Menu and it could swap the inputs on my monitor from windows, instead of going through the relatively cumbersome menu via the monitor's buttons.\n  \n    I started looking into how it worked and found a few resources / repos where people had created their own monitor settings program, and managed to put together this small script.\n  \n    It's configured for my setup so will need adjusting if you want to use it, but I thought it might help someone who had the same issue, but never happened to have that lightbulb moment to realise it could be done pretty easily.\n  \n    Code:https://github.com/a-j-jones/monitor_input_swapper\n\n    Edit: Clarified that I'm using a cheap USB switch to actually use this code"
},
{
    "title": "No title",
    "content": "Whether if it's something simple and trivial or complex and robust, small project or well known library, you can describe what it is or even better post a link to the Github repository and elaborate what makes it cool.\n  \n    Edit: \"Python automation projects\" in the title, there is no edit option."
},
{
    "title": "No title",
    "content": "Downvoting with no explanation really hurts on OP ego, but does happen despite r/Python rule #3 \"Please don't downvote without commenting your reasoning for doing so\".\n  \n    One may inquire how many of the downvotes were there in total?\n  \n    On some level at StackOverflow you see the stats on upvote and downvote to your question, but reddit shows you just the net score and provides the upvote rate. Algebraically, this is enough to calculate the raw upvote and downvote numbers.\n  \n    The formulas come from pen and paper solution for this system of two equations:\n  upvotes - downvotes = net_score                    (1)\nupvote_rate = upvotes / (upvotes + downvotes )     (2) \n    The code:\n  def votes(upvote_rate: float, net_score: int)-> tuple[int, int]:\n    downvotes = net_score * (1-upvote_rate) / (2 * upvote_rate - 1)\n    upvotes = net_score + downvotes\n    return round(upvotes), round(downvotes)\n\n\nprint(votes(.8, 3))  # (4, 1)\nprint(votes(.56, 5)) # (23, 18)\n    So, in second example by knowing the upvote rate of 56% and net score of 5 I now know there were 23 upvotes and 18 downvotes (sadly, with no comment)."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Flask-Muck is an open-source Flask extension I've been developing that handles generating a complete set of CRUD endpoints for SQLAlchemy models in a couple lines of code. The initial feature set was based on my own experiences writing very similar libraries in production over the years.\n  \n    After publishing the package I got lots of great feedback including quite a bit from right here. I've just released v0.3.0 that incorporates some key features based on that feedback.\n  \nNew Features:\n\n\n\nFlask Extension: Initially the library only supported a utility style implementation where you were required to have existing Blueprints. This was based on my own experience of always needing to incorporate the library into an existing legacy codebase. Flask-Muck now supports initializing a Flask extension that handles even more of the repetitive work. This is ideal for small services or projects using Flask-Muck from the start.\n  \n\n\nOpenAPI Specification/Swagger UI: When using the Flask extension an OpenAPI specification and Swagger UI documentation are automatically generated.\n  \n\n\nPydantic Support: Pydantic models are now supported for defining request and response schemas. They can even be mixed and matched if a codebase is transitioning from one to another.\n  \n\n\n    GitHub: https://github.com/dtiesling/flask-muck\n\n    Documentation: https://dtiesling.github.io/flask-muck/\n\n    PyPi: https://pypi.org/project/flask-muck/\n\n    This style of declarative view has saved me 1000s of lines of boilerplate code and I hope it can do the same for some of you. Thanks again to all those that took the time to check this project out and provide feedback.\n  \n    Please keep the feedback coming and I'd love to hear from anyone using Flask-Muck personally or professionally."
},
{
    "title": "No title",
    "content": "After needing to use scikit-multilearn and detecting errors, I opened a PR and waited. But after double checking I saw that there hadn't been any commits in 7 months (now 9 months) and that it had not been a release since 2018, I dug in and found out that no one had access to the PyPi credentials and so on. So I opened a discussion about creating a fork and many were eager for it.\n  \n    So after some developing, I'm here to introduce scikit-multilearn-ng (GitHub: https://github.com/scikit-multilearn-ng/scikit-multilearn-ng), an advanced, open-source tool for multi-label classification in Python. It's a direct successor to scikit-multilearn and brings a host of improvements and new features.\n  What Makes scikit-multilearn-ng Stand Out?\n\n\nEnhanced Integration with scikit-learn: This package not only integrates with the scikit-learn ecosystem but also extends its capabilities, making it a natural fit for those familiar with scikit-learn.\n  \n\n\nExpanded Algorithm Collection: Among its new offerings are StructuredGridSearchCV and the SMiLE algorithm, specifically designed for more complex multi-label classification tasks, including handling missing labels and heterogeneous features.\n  \n\n\nOpen Source Philosophy: As a community-driven project, it's free to use and open for contributions, perfect for collaborative development.\n  \n\nWhy Should You Consider Upgrading?\n\n\nEase of Transition: For those already using scikit-multilearn, upgrading is as simple as switching the dependency to scikit-multilearn-ng. Your existing code will work without any changes.\n  \n\n\nActive Development and Support: scikit-multilearn-ng offers bug fixes and new features, ensuring your projects stay current and robust.\n  \n\n\n    Whether you're a seasoned Python developer or just starting out in machine learning, scikit-multilearn-ng is worth exploring.\n  Some Example Use Cases:\n    A simple example use case is iterative splitting multilabel data between train and test data while trying to maintain the distribution of each label between the training and test sets. This is particularly useful for datasets where certain label combinations are rare.\n  from skmultilearn.model_selection import iterative_train_test_split\nimport numpy as np\n\n# Assuming X is your feature matrix and y is your label matrix\n# X should be a numpy array or a sparse matrix\n# y should be a binary indicator matrix (each label is either 0 or 1)\n\n# Define the size of your test set\ntest_size = 0.2\n\n# Perform the split\n# The function returns flattened arrays, so you need to reshape them\nX_train, y_train, X_test, y_test = iterative_train_test_split(X, y, test_size = test_size)\n\n# Reshape the outputs back to the original shapes\nnum_labels = y.shape[1]\ny_train = y_train.reshape(-1, num_labels)\ny_test = y_test.reshape(-1, num_labels)\n    But it also supports advanced problem transformations to single label problems:\n  from skmultilearn.problem_transform import BinaryRelevance\nfrom sklearn.svm import SVC\n\n# Initialize and train\nclassifier = BinaryRelevance(classifier=SVC(), require_dense=[False, True])\nclassifier.fit(X_train, y_train)\n\n# Predict\npredictions = classifier.predict(X_test)Please contribute and star the project!\n    I'm looking forward to your feedback, questions, and how you might use it in your projects!"
},
{
    "title": "No title",
    "content": "Ever wondered why, despite all the grumbles about Python being slow, it’s still everywhere? Especially when folks dive into coding, one of the first things you hear is “Python’s slow.” But, if it’s such a snail, why do so many people use it for all sorts of heavy-duty stuff?\n  \n    Here’s the deal: Yes, Python isn’t the Usain Bolt of programming languages when it comes to raw speed. We’re talking basic stuff like loops and if statements. But let’s be real, how often are we in a situation where the speed of a for-loop is the make-or-break of our project?\n  \n    The secret sauce of Python isn’t in beating speed records. It’s in its knack for playing nice with super-optimized C libraries. These libraries are the muscle doing the heavy lifting, while Python’s more like the friendly coach guiding the process. So, your Python code might take a tiny bit longer to run a loop, but when it calls on these C libraries to do the real work, they zip through tasks at lightning speed.\n  \n    So, next time you hear someone knocking Python for being slow, maybe toss this thought their way. Python’s not just about the speed of typing out code; it’s about the overall speed and ease of getting stuff done, thanks to all those optimized libraries it wraps around so neatly."
},
{
    "title": "No title",
    "content": "It's a bit silly, but it was a good exercise.\n  \nhttps://github.com/sebastiancarlos/yas-qwin"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What are your thoughts on it."
},
{
    "title": "No title",
    "content": "I'm excited to share about my open source Python library I created to help with using AbstractAPI services.What can you do with the library? It supports using all current AbstractAPI services:- Email validation- Phone number validation- IP geolocation- Lookup holidays of a country on a specific date or date range.- VAT validation/calculation/categories- IBAN validation- Exchange Rates live/conversion/historical rates- Lookup company data using only its domain- Timezone current/conversion- Avatars generation- Website screenshot- Website scrape- Image processingIt supports Python >= 3.9 and it is tested on all environments from 3.9 up to 3.12. Tests are run automatically on all supported Python versions by a CI workflow.Check it and maybe give a star? :)Repo: https://github.com/ebram96/AbstractAPI-Python-SDKDoc: https://abstractapi-python-sdk.readthedocs.io/en/latest/index.htmlPackage: https://pypi.org/project/abstract-api/Contributions are very welcome."
},
{
    "title": "No title",
    "content": "We are thrilled to announce the official launch of Leapcell's Beta public testing.\n  \n    Leapcell: https://leapcell.io/\n\n    Leapcell is a Data & Service Hosting Community, providing an application hosting experience comparable to the convenience of Vercel. Additionally, it features a high-performance database with an Airtable-like interface, streamlining data management. The entire platform is fully managed and serverless, enabling users to focus on specific business implementations without dedicating excessive time to infrastructure and DevOps.\n  \n    For more information, please refer to https://docs.leapcell.io/\n\n    Our goal is to empower users to concentrate on specific business implementations, allowing more individuals (Product Managers, Marketing professionals, Data Scientists) to participate in content creation and management without spending too much time on infrastructure and DevOps.\n  \n    Here's a Flask example: https://leapcell.io/issac/flask-blog, which contains a database and an application.\n  \n    For documentation on deploying Flask projects, check this link: https://docs.leapcell.io/docs/application/examples/flask.\n  \n    Deploying other projects is also straightforward.\n  \n    Leapcell is currently in beta testing, and we welcome any feedback or questions."
},
{
    "title": "No title",
    "content": "(Disclaimer: Just a bystander of that project, but I feel it deserves some love.)\n  \n    openDAL is a data access layer that allows users to easily and efficiently retrieve data from various storage services in a unified way. It is written in Rust and has bindings for Python available on pypi: https://pypi.org/project/opendal/\n\n    Storage services include webdav, ftp, ipfs, redis, rocksdb, s3, gcs, azblob, gdrive, dropbox, onefrive, memcached, and many more.\n  \n    There are also bindings in the works for C, C++, Haskell, LUA, Ruby, Swift, and Zig. With libraries for Java, Node.js, Python already being released.\n  \n    They are also looking for project ideas for the Google Summer of Code in their discussions, so chime in, if you have a good idea, want to help to make the Python-bindings better or want to be a mentor: GSoC 2024 Projects"
},
{
    "title": "No title",
    "content": "venv() {\n# Check if already activated\nif [[ \"$VIRTUAL_ENV\" != \"\" ]]; then\n    echo -e \"\\n\\e[1;33mDeactivating current virtual environment...\\e[0m\"\n    deactivate\n    return\nfi\n\n# Check if the venv directory exists\nif [ -d \"venv\" ]; then\n    echo -e \"\\n\\e[1;33mActivating virtual environment...\\e[0m\"\n    source venv/bin/activate\nelse\n    echo -e \"\\n\\e[1;33mCreating and activating virtual environment...\\e[0m\"\n    python3 -m venv venv\n    source venv/bin/activate\nfi\n}\n    Now when creating a python project, just go into the folder and call\n  $ venv\n    It should create a virtual environment with a folder named venv, if it exists it will activate it and if already activated it will deactivate it.\n  \n    For reference, here is a link to the script on github: https://gist.github.com/munabedan/6a5e8c104228943a461095a9e103a5af"
},
{
    "title": "No title",
    "content": "This is the ultrahdr.py script, which generates this HDR-demo.\n  \n    (Check gregbenzphotography.com/hdr/ whether you can display HDR properly.)\n  \n    High-dynamic range (HDR) is to extend the usual color range (Standard Dynamic Range (SDR)) and usually also extends the common 8bit color depth to 10bit or more. https://en.wikipedia.org/wiki/High_dynamic_range\n\n    Some modern displays (~2021) (e.g. MacBook M1, some OLED TVs) support HDR, but it is still a rare feature.\n  \n    There are multiple formats for HDR images, e.g.:\n  \n\n\n    OpenEXR\n  \n\n\n    AVIF\n  \n\n\nJPEG XT embedded in JPEG XL\n\n\n\nJPEG XR\n\n\n\n    Ultra HDR (used here) embedded in standard JPEG\n  \n\n\n    Ultra HDR uses the JPEG multi-picture format (MPF). It stores the normal SDR JPEG image as the first image, so all existing JPEG decoders can display the normal image. Then it stores a HDR gain map embedded in MPF which can be used to reconstruct the HDR image.\n  \n    Currently, (end of 2023), Google Chrome stable (end of 2023) supports this format. (Another alternative in Google Chrome is AVIF.) (Firefox currently does not support it.)\n  \n    Currently, (end of 2023), Google Pixel phones can capture Ultra HDR images (e.g. when they use night mode).\n  \n    (Note, many websites, e.g. Twitter, will reencode JPEGs after you upload them, and often they don't support Ultra HDR yet, so then it will be lost, and you will just see the normal SDR JPEG image.)\n  \n    About the Ultra HDR format: https://developer.android.com/media/platform/hdr-image-format\n\n\n    This document defines the behavior of a new file format that encodes a logarithmic range gain map image in a JPEG image file. Legacy readers that don't support the new format read and display the conventional low dynamic range image from the image file. Readers that support the format combine the primary image with the gain map and render a high dynamic range image on compatible displays.\n  \n\n    To use the simple script here, for preparation:\n  \n\n\n    First, build this: https://github.com/google/libultrahdr\n\n\n\n    Make sure FFMpeg is installed\n  \n\n\n    This script does nothing fancy: It just upscales the input JPEG color range (FFmpeg does that here currently) and then encodes the HDR gain map using Google's Ultra HDR encoder (libultrahdr). The effect is that the image will display brighter on HDR displays.\n  \n    Some related issues: https://github.com/ImageMagick/ImageMagick/issues/6377 https://github.com/libvips/libvips/issues/3799"
},
{
    "title": "No title",
    "content": "Got tired of scrolling through my PATH variable and decomposing it with something like echo $PATH | tr \":\" \"\\n\", so I wrote micro package to show the path by line, sort it alphabetically and extract lines of interest.\n  \n\n\n\n            Question\n          \n            Answer\n          \n\n\n\n\n\n\n              What's on my PATH?\n            \npathit show or pathit raw\n\n\n\n              Sort alphabetically\n            \npathit show --sort\n\n\n\n              Paths with mingw?\n            \npathit show --includes mingw\n\n\n\n              New content PATH without invalid dirs\n            \npathit show --purge --string\n\n\n\n\n    Maybe a better tool exists, but hope it is a useful utility - or at least a demo. Packaging done with `poetry` and few tests added. Code, bash equivalents and install instructions: https://github.com/epogrebnyak/what-the-path\n\n    Update: thanks to feedback from this thread, the options in pathit become the following. Note --string in specific - it allows to form a string that you can later use in your shell start file.\n  --sort     --no-sort    Sort output alphabetically. \n--includes TEXT\n--excludes TEXT\n--purge    --no-purge   Exclude invalid directories.  \n--expand   --no-expand  Expand environment variables if found inside PATH.\n--string   --no-string  Print a single string suitable for PATH content.\n--display-numbers       Indicate directory order in PATH.\n--color                 Use color to highlight errors.\n    Update 2: package renamed justpath and available through pip install justpath now. Also can display your PATH as JSON now."
},
{
    "title": "No title",
    "content": "Hey, guys, for anyone who might benefit (or would like to contribute)\n  \n    FastCRUD is a Python package for FastAPI, offering robust async CRUD operations and flexible endpoint creation utilities, streamlined through advanced features like auto-detected join conditions, dynamic sorting, and offset and cursor pagination.\n  \n    Github: github.com/igorbenav/fastcrud\n\n    Docs: igorbenav.github.io/fastcrud/\n\n    Features:- Fully Async: Leverages Python's async capabilities for non-blocking database operations.- SQLAlchemy 2.0: Works with the latest SQLAlchemy version for robust database interactions.- Powerful CRUD Functionality: Full suite of efficient CRUD operations with support for joins.- Dynamic Query Building: Supports building complex queries dynamically, including filtering, sorting, and pagination.- Advanced Join Operations: Facilitates performing SQL joins with other models with automatic join condition detection.- Built-in Offset Pagination: Comes with ready-to-use offset pagination.- Cursor-based Pagination: Implements efficient pagination for large datasets, ideal for infinite scrolling interfaces.- Modular and Extensible: Designed for easy extension and customization to fit your requirements.- Auto-generated Endpoints: Streamlines the process of adding CRUD endpoints with custom dependencies and configurations.Improvements are coming, issues and pull requests always welcome\n  \ngithub.com/igorbenav/fastcrud"
},
{
    "title": "No title",
    "content": "TIL: Figured out a way to patch pydantic settings to remove dependencies on environment variables while running unit tests.\n  \nhttps://rednafi.com/python/patch_pydantic_settings_in_pytest/"
},
{
    "title": "No title",
    "content": "Decided to make a simple version of an endless \"brick breaker\" type game in pygame, where two balls compete to color each others' domain area squares for eternity.\n  \n    Feel free to contribute: https://github.com/kvyb/endless-game, there is much that can be improved in my opinion, especially collisions."
},
{
    "title": "No title",
    "content": "PyCon is happening this March 9th & 10th at LUMS, Lahore, Pakistan.\n  \n    The early bird discount is available till Jan 31st.\n  \n    For more detail, stay updated through PyCon Pakistan’s social media pages and website https://pycon.pk"
},
{
    "title": "No title",
    "content": "Hi,I made a program that automatically sets up a wallpaper on my desktop with a random quote & weather.It does so by loading a image (background) and fetching quotes and weather (as per location entered in the city var) and sets both on the image. Then it saves the temporary image and then puts it on my desktop. This all happens in a loop with interval of 1 hour. I also made a run.bat that runs this program with pythonw and then I put it in my Startup folder.I am just a beginner in Python itself so this project isn't all that good and may contain bugs/inefficient code, I'm sorry for that.If someone can provide feedback/ideas about what I can do with this project, it'll be helpful. Thanks in advance for that.Thanks for reading.\n  \n    Here's the project link: https://github.com/TheChrisGG/AutoPaper"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I'm not a beginner now but I wrote this years ago because the xml.etree.ElementTree and lxml.builder.ElementMaker APIs are too verbose and don't compose well.\n  \n    It's just one function, XML, which takes a Python sequence. The first item is the tag name, the second is an optional dict of attributes, and the rest are strings or nested sequences. If the tag name is None the content replaces the tag.\n  \nhttps://pypi.org/project/xml-from-seq/\n\nhttps://github.com/nicwolff/xml_from_seq\nfrom xml_from_seq import XML, INLINE\n\nitem = [\n    'item', {'attr': 123, 'attr_2': None},\n    'This is some content of the item.',\n    ['b', INLINE, 'This will be bold and not indented.'],\n    [None, 'This will not be bold.'],\n]\n\nprint(XML(item))\n\n<item attr=\"123\">\n    This is some content of the item.\n    <b>This will be bold and not indented.</b>\n    This will not be bold.\n</item>"
},
{
    "title": "No title",
    "content": "For a while now I've been using OpenAI's GPT API instead of ChatGPT because it provides so much more control over things and also allows access to GPT-4 while being much cheaper overall with pretty much no rate limits.\n  \n    I made my own Python library that builds on top of OpenAI's openai library, and makes interacting with it much easier. For example, you can just use ezgpt.get(user='Your prompt') to get the response.\n  \n    Most of my effort went into the conversation feature though - it makes it easy to chat, edit, save and load the conversations. To use the conversation, simply import ezgpt and run ezgpt.c(), which, in my case, I have put into a python file which gets run by a .bat file, so I can easily run it from anywhere.\n  \n    Check it out here: https://pypi.org/project/ezgpt\n\n    GitHub Repository: https://github.com/Ascyt/ezgpt"
},
{
    "title": "No title",
    "content": "Test examples:\n  \nhttps://github.com/szymon6927/szymonmiks.pl/blob/master/blog/examples/tests/test_classical_vs_london/test_transaction_processor.py\n\n\n    My latest blog post delves into the Classical and London schools, offering practical Python examples.\n  \nhttps://blog.szymonmiks.pl/p/exploring-different-schools-of-unit-testing-in-python/"
},
{
    "title": "No title",
    "content": "I wrote an in-depth tutorial on a bIG TEch CoDINg InTerVIEW problem, but for fun I restricted myself to the Python standard library only. With a bit of exploration, the solution ended up comparable with the theoretically correct one (which, it turns out, is not the fastest anyway).\n  \n    Link: https://death.andgravity.com/lru-cache"
},
{
    "title": "No title",
    "content": "Hi,\n  \n    Demo: https://imgur.com/a/IfMcfs0\n\n    So I made this small thing 2 years back, it's like a programming language that lets you build a tkinter app without much trouble.\n  \n\n    So I found this and thought to myself - hey, I can improve upon this! So I decided to first share this and then start making this thing way better.\n  \n\n    Hope you like it!\n  \nhttps://github.com/arjunj132/dapp/tree/main"
},
{
    "title": "No title",
    "content": "Downvoting with no explanation really hurts on OP ego, but does happen despite r/Python rule #3 \"Please don't downvote without commenting your reasoning for doing so\".\n  \n    One may inquire how many of the downvotes were there in total?\n  \n    On some level at StackOverflow you see the stats on upvote and downvote to your question, but reddit shows you just the net score and provides the upvote rate. Algebraically, this is enough to calculate the raw upvote and downvote numbers.\n  \n    The formulas come from pen and paper solution for this system of two equations:\n  upvotes - downvotes = net_score                    (1)\nupvote_rate = upvotes / (upvotes + downvotes )     (2) \n    The code:\n  def votes(upvote_rate: float, net_score: int)-> tuple[int, int]:\n    downvotes = net_score * (1-upvote_rate) / (2 * upvote_rate - 1)\n    upvotes = net_score + downvotes\n    return round(upvotes), round(downvotes)\n\n\nprint(votes(.8, 3))  # (4, 1)\nprint(votes(.56, 5)) # (23, 18)\n    So, in second example by knowing the upvote rate of 56% and net score of 5 I now know there were 23 upvotes and 18 downvotes (sadly, with no comment)."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Flask-Muck is an open-source Flask extension I've been developing that handles generating a complete set of CRUD endpoints for SQLAlchemy models in a couple lines of code. The initial feature set was based on my own experiences writing very similar libraries in production over the years.\n  \n    After publishing the package I got lots of great feedback including quite a bit from right here. I've just released v0.3.0 that incorporates some key features based on that feedback.\n  \nNew Features:\n\n\n\nFlask Extension: Initially the library only supported a utility style implementation where you were required to have existing Blueprints. This was based on my own experience of always needing to incorporate the library into an existing legacy codebase. Flask-Muck now supports initializing a Flask extension that handles even more of the repetitive work. This is ideal for small services or projects using Flask-Muck from the start.\n  \n\n\nOpenAPI Specification/Swagger UI: When using the Flask extension an OpenAPI specification and Swagger UI documentation are automatically generated.\n  \n\n\nPydantic Support: Pydantic models are now supported for defining request and response schemas. They can even be mixed and matched if a codebase is transitioning from one to another.\n  \n\n\n    GitHub: https://github.com/dtiesling/flask-muck\n\n    Documentation: https://dtiesling.github.io/flask-muck/\n\n    PyPi: https://pypi.org/project/flask-muck/\n\n    This style of declarative view has saved me 1000s of lines of boilerplate code and I hope it can do the same for some of you. Thanks again to all those that took the time to check this project out and provide feedback.\n  \n    Please keep the feedback coming and I'd love to hear from anyone using Flask-Muck personally or professionally."
},
{
    "title": "No title",
    "content": "After needing to use scikit-multilearn and detecting errors, I opened a PR and waited. But after double checking I saw that there hadn't been any commits in 7 months (now 9 months) and that it had not been a release since 2018, I dug in and found out that no one had access to the PyPi credentials and so on. So I opened a discussion about creating a fork and many were eager for it.\n  \n    So after some developing, I'm here to introduce scikit-multilearn-ng (GitHub: https://github.com/scikit-multilearn-ng/scikit-multilearn-ng), an advanced, open-source tool for multi-label classification in Python. It's a direct successor to scikit-multilearn and brings a host of improvements and new features.\n  What Makes scikit-multilearn-ng Stand Out?\n\n\nEnhanced Integration with scikit-learn: This package not only integrates with the scikit-learn ecosystem but also extends its capabilities, making it a natural fit for those familiar with scikit-learn.\n  \n\n\nExpanded Algorithm Collection: Among its new offerings are StructuredGridSearchCV and the SMiLE algorithm, specifically designed for more complex multi-label classification tasks, including handling missing labels and heterogeneous features.\n  \n\n\nOpen Source Philosophy: As a community-driven project, it's free to use and open for contributions, perfect for collaborative development.\n  \n\nWhy Should You Consider Upgrading?\n\n\nEase of Transition: For those already using scikit-multilearn, upgrading is as simple as switching the dependency to scikit-multilearn-ng. Your existing code will work without any changes.\n  \n\n\nActive Development and Support: scikit-multilearn-ng offers bug fixes and new features, ensuring your projects stay current and robust.\n  \n\n\n    Whether you're a seasoned Python developer or just starting out in machine learning, scikit-multilearn-ng is worth exploring.\n  Some Example Use Cases:\n    A simple example use case is iterative splitting multilabel data between train and test data while trying to maintain the distribution of each label between the training and test sets. This is particularly useful for datasets where certain label combinations are rare.\n  from skmultilearn.model_selection import iterative_train_test_split\nimport numpy as np\n\n# Assuming X is your feature matrix and y is your label matrix\n# X should be a numpy array or a sparse matrix\n# y should be a binary indicator matrix (each label is either 0 or 1)\n\n# Define the size of your test set\ntest_size = 0.2\n\n# Perform the split\n# The function returns flattened arrays, so you need to reshape them\nX_train, y_train, X_test, y_test = iterative_train_test_split(X, y, test_size = test_size)\n\n# Reshape the outputs back to the original shapes\nnum_labels = y.shape[1]\ny_train = y_train.reshape(-1, num_labels)\ny_test = y_test.reshape(-1, num_labels)\n    But it also supports advanced problem transformations to single label problems:\n  from skmultilearn.problem_transform import BinaryRelevance\nfrom sklearn.svm import SVC\n\n# Initialize and train\nclassifier = BinaryRelevance(classifier=SVC(), require_dense=[False, True])\nclassifier.fit(X_train, y_train)\n\n# Predict\npredictions = classifier.predict(X_test)Please contribute and star the project!\n    I'm looking forward to your feedback, questions, and how you might use it in your projects!"
},
{
    "title": "No title",
    "content": "Ever wondered why, despite all the grumbles about Python being slow, it’s still everywhere? Especially when folks dive into coding, one of the first things you hear is “Python’s slow.” But, if it’s such a snail, why do so many people use it for all sorts of heavy-duty stuff?\n  \n    Here’s the deal: Yes, Python isn’t the Usain Bolt of programming languages when it comes to raw speed. We’re talking basic stuff like loops and if statements. But let’s be real, how often are we in a situation where the speed of a for-loop is the make-or-break of our project?\n  \n    The secret sauce of Python isn’t in beating speed records. It’s in its knack for playing nice with super-optimized C libraries. These libraries are the muscle doing the heavy lifting, while Python’s more like the friendly coach guiding the process. So, your Python code might take a tiny bit longer to run a loop, but when it calls on these C libraries to do the real work, they zip through tasks at lightning speed.\n  \n    So, next time you hear someone knocking Python for being slow, maybe toss this thought their way. Python’s not just about the speed of typing out code; it’s about the overall speed and ease of getting stuff done, thanks to all those optimized libraries it wraps around so neatly."
},
{
    "title": "No title",
    "content": "It's a bit silly, but it was a good exercise.\n  \nhttps://github.com/sebastiancarlos/yas-qwin"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "What are your thoughts on it."
},
{
    "title": "No title",
    "content": "I'm excited to share about my open source Python library I created to help with using AbstractAPI services.What can you do with the library? It supports using all current AbstractAPI services:- Email validation- Phone number validation- IP geolocation- Lookup holidays of a country on a specific date or date range.- VAT validation/calculation/categories- IBAN validation- Exchange Rates live/conversion/historical rates- Lookup company data using only its domain- Timezone current/conversion- Avatars generation- Website screenshot- Website scrape- Image processingIt supports Python >= 3.9 and it is tested on all environments from 3.9 up to 3.12. Tests are run automatically on all supported Python versions by a CI workflow.Check it and maybe give a star? :)Repo: https://github.com/ebram96/AbstractAPI-Python-SDKDoc: https://abstractapi-python-sdk.readthedocs.io/en/latest/index.htmlPackage: https://pypi.org/project/abstract-api/Contributions are very welcome."
},
{
    "title": "No title",
    "content": "We are thrilled to announce the official launch of Leapcell's Beta public testing.\n  \n    Leapcell: https://leapcell.io/\n\n    Leapcell is a Data & Service Hosting Community, providing an application hosting experience comparable to the convenience of Vercel. Additionally, it features a high-performance database with an Airtable-like interface, streamlining data management. The entire platform is fully managed and serverless, enabling users to focus on specific business implementations without dedicating excessive time to infrastructure and DevOps.\n  \n    For more information, please refer to https://docs.leapcell.io/\n\n    Our goal is to empower users to concentrate on specific business implementations, allowing more individuals (Product Managers, Marketing professionals, Data Scientists) to participate in content creation and management without spending too much time on infrastructure and DevOps.\n  \n    Here's a Flask example: https://leapcell.io/issac/flask-blog, which contains a database and an application.\n  \n    For documentation on deploying Flask projects, check this link: https://docs.leapcell.io/docs/application/examples/flask.\n  \n    Deploying other projects is also straightforward.\n  \n    Leapcell is currently in beta testing, and we welcome any feedback or questions."
},
{
    "title": "No title",
    "content": "(Disclaimer: Just a bystander of that project, but I feel it deserves some love.)\n  \n    openDAL is a data access layer that allows users to easily and efficiently retrieve data from various storage services in a unified way. It is written in Rust and has bindings for Python available on pypi: https://pypi.org/project/opendal/\n\n    Storage services include webdav, ftp, ipfs, redis, rocksdb, s3, gcs, azblob, gdrive, dropbox, onefrive, memcached, and many more.\n  \n    There are also bindings in the works for C, C++, Haskell, LUA, Ruby, Swift, and Zig. With libraries for Java, Node.js, Python already being released.\n  \n    They are also looking for project ideas for the Google Summer of Code in their discussions, so chime in, if you have a good idea, want to help to make the Python-bindings better or want to be a mentor: GSoC 2024 Projects"
},
{
    "title": "No title",
    "content": "venv() {\n# Check if already activated\nif [[ \"$VIRTUAL_ENV\" != \"\" ]]; then\n    echo -e \"\\n\\e[1;33mDeactivating current virtual environment...\\e[0m\"\n    deactivate\n    return\nfi\n\n# Check if the venv directory exists\nif [ -d \"venv\" ]; then\n    echo -e \"\\n\\e[1;33mActivating virtual environment...\\e[0m\"\n    source venv/bin/activate\nelse\n    echo -e \"\\n\\e[1;33mCreating and activating virtual environment...\\e[0m\"\n    python3 -m venv venv\n    source venv/bin/activate\nfi\n}\n    Now when creating a python project, just go into the folder and call\n  $ venv\n    It should create a virtual environment with a folder named venv, if it exists it will activate it and if already activated it will deactivate it.\n  \n    For reference, here is a link to the script on github: https://gist.github.com/munabedan/6a5e8c104228943a461095a9e103a5af"
},
{
    "title": "No title",
    "content": "This is the ultrahdr.py script, which generates this HDR-demo.\n  \n    (Check gregbenzphotography.com/hdr/ whether you can display HDR properly.)\n  \n    High-dynamic range (HDR) is to extend the usual color range (Standard Dynamic Range (SDR)) and usually also extends the common 8bit color depth to 10bit or more. https://en.wikipedia.org/wiki/High_dynamic_range\n\n    Some modern displays (~2021) (e.g. MacBook M1, some OLED TVs) support HDR, but it is still a rare feature.\n  \n    There are multiple formats for HDR images, e.g.:\n  \n\n\n    OpenEXR\n  \n\n\n    AVIF\n  \n\n\nJPEG XT embedded in JPEG XL\n\n\n\nJPEG XR\n\n\n\n    Ultra HDR (used here) embedded in standard JPEG\n  \n\n\n    Ultra HDR uses the JPEG multi-picture format (MPF). It stores the normal SDR JPEG image as the first image, so all existing JPEG decoders can display the normal image. Then it stores a HDR gain map embedded in MPF which can be used to reconstruct the HDR image.\n  \n    Currently, (end of 2023), Google Chrome stable (end of 2023) supports this format. (Another alternative in Google Chrome is AVIF.) (Firefox currently does not support it.)\n  \n    Currently, (end of 2023), Google Pixel phones can capture Ultra HDR images (e.g. when they use night mode).\n  \n    (Note, many websites, e.g. Twitter, will reencode JPEGs after you upload them, and often they don't support Ultra HDR yet, so then it will be lost, and you will just see the normal SDR JPEG image.)\n  \n    About the Ultra HDR format: https://developer.android.com/media/platform/hdr-image-format\n\n\n    This document defines the behavior of a new file format that encodes a logarithmic range gain map image in a JPEG image file. Legacy readers that don't support the new format read and display the conventional low dynamic range image from the image file. Readers that support the format combine the primary image with the gain map and render a high dynamic range image on compatible displays.\n  \n\n    To use the simple script here, for preparation:\n  \n\n\n    First, build this: https://github.com/google/libultrahdr\n\n\n\n    Make sure FFMpeg is installed\n  \n\n\n    This script does nothing fancy: It just upscales the input JPEG color range (FFmpeg does that here currently) and then encodes the HDR gain map using Google's Ultra HDR encoder (libultrahdr). The effect is that the image will display brighter on HDR displays.\n  \n    Some related issues: https://github.com/ImageMagick/ImageMagick/issues/6377 https://github.com/libvips/libvips/issues/3799"
},
{
    "title": "No title",
    "content": "Got tired of scrolling through my PATH variable and decomposing it with something like echo $PATH | tr \":\" \"\\n\", so I wrote micro package to show the path by line, sort it alphabetically and extract lines of interest.\n  \n\n\n\n            Question\n          \n            Answer\n          \n\n\n\n\n\n\n              What's on my PATH?\n            \npathit show or pathit raw\n\n\n\n              Sort alphabetically\n            \npathit show --sort\n\n\n\n              Paths with mingw?\n            \npathit show --includes mingw\n\n\n\n              New content PATH without invalid dirs\n            \npathit show --purge --string\n\n\n\n\n    Maybe a better tool exists, but hope it is a useful utility - or at least a demo. Packaging done with `poetry` and few tests added. Code, bash equivalents and install instructions: https://github.com/epogrebnyak/what-the-path\n\n    Update: thanks to feedback from this thread, the options in pathit become the following. Note --string in specific - it allows to form a string that you can later use in your shell start file.\n  --sort     --no-sort    Sort output alphabetically. \n--includes TEXT\n--excludes TEXT\n--purge    --no-purge   Exclude invalid directories.  \n--expand   --no-expand  Expand environment variables if found inside PATH.\n--string   --no-string  Print a single string suitable for PATH content.\n--display-numbers       Indicate directory order in PATH.\n--color                 Use color to highlight errors.\n    Update 2: package renamed justpath and available through pip install justpath now. Also can display your PATH as JSON now."
},
{
    "title": "No title",
    "content": "Hey, guys, for anyone who might benefit (or would like to contribute)\n  \n    FastCRUD is a Python package for FastAPI, offering robust async CRUD operations and flexible endpoint creation utilities, streamlined through advanced features like auto-detected join conditions, dynamic sorting, and offset and cursor pagination.\n  \n    Github: github.com/igorbenav/fastcrud\n\n    Docs: igorbenav.github.io/fastcrud/\n\n    Features:- Fully Async: Leverages Python's async capabilities for non-blocking database operations.- SQLAlchemy 2.0: Works with the latest SQLAlchemy version for robust database interactions.- Powerful CRUD Functionality: Full suite of efficient CRUD operations with support for joins.- Dynamic Query Building: Supports building complex queries dynamically, including filtering, sorting, and pagination.- Advanced Join Operations: Facilitates performing SQL joins with other models with automatic join condition detection.- Built-in Offset Pagination: Comes with ready-to-use offset pagination.- Cursor-based Pagination: Implements efficient pagination for large datasets, ideal for infinite scrolling interfaces.- Modular and Extensible: Designed for easy extension and customization to fit your requirements.- Auto-generated Endpoints: Streamlines the process of adding CRUD endpoints with custom dependencies and configurations.Improvements are coming, issues and pull requests always welcome\n  \ngithub.com/igorbenav/fastcrud"
},
{
    "title": "No title",
    "content": "TIL: Figured out a way to patch pydantic settings to remove dependencies on environment variables while running unit tests.\n  \nhttps://rednafi.com/python/patch_pydantic_settings_in_pytest/"
},
{
    "title": "No title",
    "content": "Decided to make a simple version of an endless \"brick breaker\" type game in pygame, where two balls compete to color each others' domain area squares for eternity.\n  \n    Feel free to contribute: https://github.com/kvyb/endless-game, there is much that can be improved in my opinion, especially collisions."
},
{
    "title": "No title",
    "content": "PyCon is happening this March 9th & 10th at LUMS, Lahore, Pakistan.\n  \n    The early bird discount is available till Jan 31st.\n  \n    For more detail, stay updated through PyCon Pakistan’s social media pages and website https://pycon.pk"
},
{
    "title": "No title",
    "content": "Hi,I made a program that automatically sets up a wallpaper on my desktop with a random quote & weather.It does so by loading a image (background) and fetching quotes and weather (as per location entered in the city var) and sets both on the image. Then it saves the temporary image and then puts it on my desktop. This all happens in a loop with interval of 1 hour. I also made a run.bat that runs this program with pythonw and then I put it in my Startup folder.I am just a beginner in Python itself so this project isn't all that good and may contain bugs/inefficient code, I'm sorry for that.If someone can provide feedback/ideas about what I can do with this project, it'll be helpful. Thanks in advance for that.Thanks for reading.\n  \n    Here's the project link: https://github.com/TheChrisGG/AutoPaper"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I'm not a beginner now but I wrote this years ago because the xml.etree.ElementTree and lxml.builder.ElementMaker APIs are too verbose and don't compose well.\n  \n    It's just one function, XML, which takes a Python sequence. The first item is the tag name, the second is an optional dict of attributes, and the rest are strings or nested sequences. If the tag name is None the content replaces the tag.\n  \nhttps://pypi.org/project/xml-from-seq/\n\nhttps://github.com/nicwolff/xml_from_seq\nfrom xml_from_seq import XML, INLINE\n\nitem = [\n    'item', {'attr': 123, 'attr_2': None},\n    'This is some content of the item.',\n    ['b', INLINE, 'This will be bold and not indented.'],\n    [None, 'This will not be bold.'],\n]\n\nprint(XML(item))\n\n<item attr=\"123\">\n    This is some content of the item.\n    <b>This will be bold and not indented.</b>\n    This will not be bold.\n</item>"
},
{
    "title": "No title",
    "content": "For a while now I've been using OpenAI's GPT API instead of ChatGPT because it provides so much more control over things and also allows access to GPT-4 while being much cheaper overall with pretty much no rate limits.\n  \n    I made my own Python library that builds on top of OpenAI's openai library, and makes interacting with it much easier. For example, you can just use ezgpt.get(user='Your prompt') to get the response.\n  \n    Most of my effort went into the conversation feature though - it makes it easy to chat, edit, save and load the conversations. To use the conversation, simply import ezgpt and run ezgpt.c(), which, in my case, I have put into a python file which gets run by a .bat file, so I can easily run it from anywhere.\n  \n    Check it out here: https://pypi.org/project/ezgpt\n\n    GitHub Repository: https://github.com/Ascyt/ezgpt"
},
{
    "title": "No title",
    "content": "Test examples:\n  \nhttps://github.com/szymon6927/szymonmiks.pl/blob/master/blog/examples/tests/test_classical_vs_london/test_transaction_processor.py\n\n\n    My latest blog post delves into the Classical and London schools, offering practical Python examples.\n  \nhttps://blog.szymonmiks.pl/p/exploring-different-schools-of-unit-testing-in-python/"
},
{
    "title": "No title",
    "content": "I wrote an in-depth tutorial on a bIG TEch CoDINg InTerVIEW problem, but for fun I restricted myself to the Python standard library only. With a bit of exploration, the solution ended up comparable with the theoretically correct one (which, it turns out, is not the fastest anyway).\n  \n    Link: https://death.andgravity.com/lru-cache"
},
{
    "title": "No title",
    "content": "Hi,\n  \n    Demo: https://imgur.com/a/IfMcfs0\n\n    So I made this small thing 2 years back, it's like a programming language that lets you build a tkinter app without much trouble.\n  \n\n    So I found this and thought to myself - hey, I can improve upon this! So I decided to first share this and then start making this thing way better.\n  \n\n    Hope you like it!\n  \nhttps://github.com/arjunj132/dapp/tree/main"
},
{
    "title": "No title",
    "content": "TLDR; pyimps is now here: https://github.com/bedbad/pyimpsThis works:\n  \npip install pyimps\n\n    I always wanted to automate python correct dependency fetching and resolution and in the very list see the entire py source or any findable module dependency tree right away, in the terminal; In my experience python imports were always easy - until they were not when I encountered pip being different version then python due to alias and a whole bunch of runtime-specific weirdness when you're surprised python can't find something you've surely pip installed  or built. Not calling python -m module.submodule correctly can be a source of banging head. Recentlty banged my head against some rogue modules in tensorflow build defined programmatically in few lines of a source file, and then programmatically deleted based on condition.\n  \n    At that point I wrote this shell tool that detangles all the above tells you where the imports you have are at, what they are and where the ones you don't should have been with all relevant python search paths, if there's import failue.\n  \n    It works, does basics of I wanted it to do barring looking packages up as pypi had blocked such requests at the moment and relating fetching and automation functionality.\n  \n    If you remember something itching  about the importlib machinery, that would be interesting to include here.\n  \n    Update:\n  \n    Added color code key and verbose for types, so now it looks like:\n  \npyimps -v tensorflow\nModule path: Library/Python/3.9/lib/python/site-packages/tensorflow/__init__.py\ntensorflow(ROOT)<====/Users/im_1/Library/Python/3.9/lib/python/site-packages/tensorflow/__init__.py\n          ↑___python(DEL)(deleted)\n                ↑___framework(SUB)<====Library/Python/3.9/lib/python/site-packages/tensorflow/python/framework/__init__.py\n                         ↑___tensor_conversion(SUB)<====Library/Python/3.9/lib/python/site-packages/tensorflow/python/framework/tensor_conversion.py\n                                          ↑___convert_to_tensor_v2_with_dispatch(FUN)\n                         ↑___tensor_shape(SUB)<====Library/Python/3.9/lib/python/site-packages/tensorflow/python/framework/tensor_shape.py\n                                     ↑___TensorShape(CLASS)\n                         ↑___versions(SUB)<====Library/Python/3.9/lib/python/site-packages/tensorflow/python/framework/versions.py\n                                 ↑___CXX11_ABI_FLAG(VARIABLE)\n                                 ↑___GIT_VERSION(VARIABLE)\n                                 ↑___COMPILER_VERSION(VARIABLE)\n                                 ↑___VERSION(VARIABLE)\n                                 ↑___CXX_VERSION(VARIABLE)\n                                 ↑___MONOLITHIC_BUILD(VARIABLE)\n    This lib is still in its seeding state;\n  \n    What I'm looking for are bug reports, concrete usage case/feature requests to see if it's warranted to invest my time.\n  \n    I'd wish complete python project dependency graph analysis with build automation for monorepo/bazel/hermetic python builds and that's whole next level"
},
{
    "title": "No title",
    "content": "Hello all, wanted to get some feedback on a lib I build a few yrs ago, dictor\n  \n    it handles dicts and json structures in a pythonic lookup structure, with built in error handling and default/fallback values\n  \n    ie\n  \n    sample.json =\n  {\n  \"characters\": {\n    \"Lonestar\": {\n      \"id\": 55923,\n      \"role\": \"renegade\",\n      \"items\": [\"space winnebago\", \"leather jacket\"]\n    },\n    \"Barfolomew\": {\n      \"id\": 55924,\n      \"role\": \"mawg\",\n      \"items\": [\"peanut butter jar\", \"waggy tail\"]\n    },\n    \"Dark Helmet\": {\n      \"id\": 99999,\n      \"role\": \"Good is dumb\",\n      \"items\": [\"Shwartz\", \"helmet\"]\n    },\n    \"Skroob\": {\n      \"id\": 12345,\n      \"role\": \"Spaceballs CEO\",\n      \"items\": [\"luggage\"]\n    }\n  }\n}with open('sample.json') as data:\n    data = json.load(data)\n\ncharacter = dictor(data, \"Lonestar.items\")\nprint(character)\n\n>> [\"space winnebago\", \"leather jacket\"]\n    has many other features\n  \n    any PRs or feedback appreciated, thx\n  \nhttps://github.com/perfecto25/dictor"
},
{
    "title": "No title",
    "content": "Hello,\n  \n    some time ago I posted about writing mathler solver. Now I added selenium to solve actual, live version of it.https://medium.com/@jandanecki/solving-mathler-riddle-with-python-part-2-de91c87c41ba"
},
{
    "title": "No title",
    "content": "I started this new poetry plugin to simplify the creation of docker images starting from a poetry project. The main goal is to create the docker image effortless, with ZERO configuration required.\n  \n    This is the pypi: https://pypi.org/project/poetry-dockerize-plugin/\n\n    Source code: https://github.com/nicoloboschi/poetry-dockerize-plugin\n\n    Do you think you would use it ? why and why not ? what would be the must-to-have features ?"
},
{
    "title": "No title",
    "content": "I do not love the word Pythonic, i believe a language should evolve, even because are years that real new languages are not created.\n  \n    I am a lazy and kinda stupid programmer, looking for idiomatic sugar, and most important not repeat myself. On this basis i have to say that the last 3 months I used python i had to use the word self thousands of times. I do not like. I am not asking to inquire if is just or wrong, someone like pizza, other people French fries so will mark the post as `discussion` although maybe is more an help, I humbly would like to know if there is a way i cannot repeat the self everywhere, I am comfortable will still have readable code\n  \n\n    EDIT: In general would not be lovely to work with scoped functions to write faster code in this way for instance? with keyword\n\n\n    EDIT2: to be honest i find really childish to downvote me, i am just sharing a view, trying to understand and asking for help for something that for me is really frustrating to do: typing more than needed, not to follow what the cool guys do\n  \n\n    EDIT3: example\n  \n\n    from self:class SelfishClass:\n  \n\n    def __init__(self):self.instance_variable = None\n  \n\n    def write_to_inner(self, val: int) -> None:self.inner_property = val\n  \n\n    SelfishClass().write_inner(2)to kinda of pseudoPython:class SelfishClass:\n  \n    instanceVariable = 0 #nullability would be better ex instanceVariable: Int?\n  \n    def writeToInner(variable: Int):instanceVariable = variable\n  \n    print(instanceVariable)\n  \n    writeToInner(2)"
},
{
    "title": "No title",
    "content": "Introduction\n    You want to pump out code fast without all those pesky best practices slowing you down. Who cares if your code is impossible to maintain and modify later? You're a COWBOY CODER! This guide will teach you how to write sloppy, unprofessional code that ignores widely-accepted standards, making your codebase an incomprehensible mess! Follow these tips and your future self will thank you with days of frustration and head-scratching as they try to build on or fix your masterpiece. Yeehaw!\n  1. Avoid Object Oriented Programming\n    All those classes, encapsulation, inheritance stuff - totally unnecessary! Just write giant 1000+ line scripts with everything mixed together. Functions? Where we're going, we don't need no stinkin' functions! Who has time to context switch between different files and classes? Real programmers can keep everything in their head at once. So, toss out all that OOP nonsense. The bigger the file, the better!\n  2. Copy and Paste Everywhere\n    Need the same code in multiple places? Just copy and paste it! Refactoring is for losers. If you've got an algorithm or bit of logic you need to reuse, just duplicate that bad boy everywhere you need it. Who cares if you have to update it in 15 different places when requirements change? Not you - you'll just hack it and move on to the next thing! Duplication is your friend!\n  3. Globals and Side Effects Everywhere\n    Variables, functions, state - just toss 'em in the global namespace! Who needs encapsulation when you can just directly mutate whatever you want from anywhere in the code? While you're at it, functions should have all kinds of side effects. Don't document them though - make your teammate guess what that function call does!\n  4. Nested Everything\n    Nested loops, nested ifs, nested functions - nest to your heart's content! Who cares if the code is indented 50 levels deep? Just throw a comment somewhere saying \"Here be dragons\" and call it a day. Spaghetti code is beautiful in its own way.\n  5. Magic Numbers and Hardcoded Everything\n    Litter your code with magic numbers and hardcoded strings - they really add that human touch. Who needs constants or config files? Hardcode URLs, API keys, resource limits - go wild! Keep those release engineers on their toes!\n  6. Spaghetti Dependency Management\n    Feel free to import anything from anywhere. Mix and match relative imports, circular dependencies, whatever you want! from ../../utils import helpers, constants, db - beautiful! Who cares where it comes from as long as it works...until it suddenly breaks for no apparent reason.\n  7. Write Every Line as It Comes to You\n    Don't waste time planning or designing anything up front. Just start hacking! Stream of consciousness coding is the way to go. Just write each line and idea as it pops into your head. Who cares about architecture - you've got CODE to write!\n  8. Documentation is Overrated\n    Real programmers don't comment their code or write documentation. If nobody can understand that brilliant algorithm you spent days on, that's their problem! You're an artist and your masterpiece should speak for itself.\n  9. Testing is a Crutch\n    Don't waste time writing tests for your code. If it works on your machine, just ship it! Who cares if untested code breaks the build or crashes in production - you'll burn that bridge when you get to it. You're a coding cowboy - unleash that beautiful untested beast!\n  10. Commit Early, Commit Often\n    Branching, pull requests, code review - ain't nobody got time for that! Just commit directly to the main branch as often as possible. Don't worry about typos or half-finished work - just blast it into the repo and keep moving. Git history cleanliness is overrated!\n  11. Manual Deployments to Production\n    Set up continuous integration and delivery? No way! Click click click deploy to production manually whenever you feel like it. 3am on a Sunday? Perfect time! Wake your team up with exciting new bugs and regressions whenever you deploy.\n  12. Don't Handle Errors\n    Error handling is boring. Just let your code crash and burn - it adds excitement! Don't wrap risky sections in try/catch blocks - let those exceptions bubble up to the user. What's the worst that could happen?\n  13. Security is for Chumps\n    Who needs authentication or authorization? Leave all your APIs wide open, logins optional. Store passwords in plain text, better yet - hardcoded in the source! SQL injection vulnerabilities? Sounds like a feature!\n  14. Dread the Maintenance Phase\n    The most important part of coding is the NEXT feature. Just hack together something that barely works and move on to the next thing. Who cares if your unmaintainable mess gives the next developer nightmares? Not your problem anymore!\n  Conclusion\n    Follow these top tips, and you'll be writing gloriously UNMAINTAINABLE code in no time! When you inevitably leave your job, your team will fondly remember you as they desperately rewrite the pile of spaghetti code you left behind. Ride off into the sunset, you brilliant, beautiful code cowboy! Happy hacking!"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hey y'all, I've recently made a very simple project in a few hours and thought it would've been pretty cool to share!\n  \nhttps://github.com/JoshuaKasa/DASO\n\n    Let me know what you think! I very much welcome all type of contributions, especially 'cause I'd need someone to make a .exe file out of the project so that I can remove the .bat files lol."
},
{
    "title": "No title",
    "content": "Quart-Schema is a Quart extension that provides schema validation and auto-generated openAPI documentation. It works by decorating routes with the desired structures,\n  from dataclasses import dataclass\nfrom datetime import datetime\n\nfrom quart import Quart\nfrom quart_schema import QuartSchema, validate_request, validate_response\n\napp = Quart(__name__)\nQuartSchema(app)\n\n@dataclass\nclass Todo:\n    task: str\n    due: datetime | None\n\n@app.post(\"/\")\n@validate_request(Todo)\n@validate_response(Todo, 201)\nasync def create_todo(data: Todo) -> tuple[Todo, int]:\n    ... # Do something with data, e.g. save to the DB\n    return data, 201\n    With the autogenerated openAPI documentation visable at /docs.\n  \n    With the 0.19 release the dataclass in the above can be swapped with an attrs defined class, a msgspec Struct or a Pydantic BaseModel.\n  \n    See full changelog here"
},
{
    "title": "No title",
    "content": "JSON is arguably the most popular data format used across the internet. A lot of programming languages and web frameworks provide convenient methods to parse JSON and sometimes they do this out of the box. I’ve always used Python's JSON parser and I have never really bothered to look into how it works under the hood. In the spirit of learning how things work, I decided to start small. I wrote a very simple JSON parser in Python to better understand how it works. If you’d like to follow my thought process and approach you can read my article here https://volomn.com/blog/write-a-simple-json-parser-in-python"
},
{
    "title": "No title",
    "content": "Hi all,\n  \n    I just published a new package on PyPI: fasthx. See the docs here: https://volfpeter.github.io/fasthx/\n\n    Key features:\n  \n\n\n    Decorator syntax that works with FastAPI as one would expect, no need for unused or magic dependencies in routes.\n  \n\n\n    Works with any templating engine or server-side rendering library.\n  \n\n\n    Built-in Jinja2 templating support.\n  \n\n\n    FastAPI routes will keep working normally by default if they receive non-HTMX requests, so the same route can serve data and render HTML at the same time.\n  \n\n\n    Correct typing makes it possible to apply other (typed) decorators to your routes.\n  \n\n\n    Give it a look if you're in the target audience."
},
{
    "title": "No title",
    "content": "def Point[T](x: T, y: T):\n    ...\n    this looks great but, would this look better?\n  def [T] Point(x: T, y: T):\n    ...\n    Imagine cluttering this new syntax\n  def Point[T, E, S, L, A](x: T, y: T):\n    ...\n    doesn't this look messy?\n  \n    Imagine doing it this way\n  def [T, E, S, L, A] Point(x: T, y: T):\n    ..."
},
{
    "title": "No title",
    "content": "I was playing around with running Prefect workflows on cloud-hosted data with a new integration from some colleagues at Coiled. Turns out it was pretty straightforward to deploy a daily data processing job on a NASA dataset I’ve been working with lately. Thought I’d share a write up of what this looks like.\n  # python workflow.py              # Runs locally\ncoiled prefect serve workflow.py  # Runs on the cloud\n    Blog post: https://medium.com/coiled-hq/schedule-python-jobs-with-prefect-and-coiled-b22180a25f1f\n\n    (edited to fix the code snippet formatting)"
},
{
    "title": "No title",
    "content": "I released cfdp-py today, which is a Python library providing high level componentsto transfer files according to the CCSDS File Delivery Protocol.\n  \n    I've been working on this crate for a longer time now which was previous part of a larger package and I decided to create a dedicated package for it now. A lot of the code is already used in our ground infrastructure to perform software updates for our satellite, and I recently added some nice features, including the acknowledged mode.\n  \n    The tricky thing about (large) file transfers with remote systems is that the protocols are oftentimes still packet based. The CFDP standard specifies how to split files into chunks which can be sent as packets (for example CCSDS space packets). I designed the state machines of the library so they have a relatively simple API which can process received packets, generate new packets to be sent to the remote entity and drive the state machine with dedicated methods. This allows large flexibility in how to schedule the state machine (flow control) and allows freedom on packet post-processing and how to send these packets.\n  \n    For example, we wrap these CFDP packets into CCSDS space packets, so we have one uniform packet type used for communication (and because some of the hardware actually expects these packets to be CCSDS conformant). We send these packets around the ground infastructure using a message queue middleware, but of course simpler approaches like sending via UDP or a serial driver are also possible. In the future, I might add a CFDP daemon implementation which can also track multiple concurrent requests building on top of these state machines.\n  \n    Maybe the library can help some people working in the space sector :)\n  \n    Kind Regards"
},
{
    "title": "No title",
    "content": "Octoffers is a revolutionary open-source Python tool designed to tackle the challenges of job hunting. It streamlines the application process by scraping job listings from platforms like LinkedIn and Indeed etc, auto-filling forms, and generating AI-powered cover letters tailored to each job description. The goal is to save you time, boost your application success rate, and allow you to secure interview automatically.\n  \nKey Features:\n\n\n\n    Scrape jobs from popular career platforms.\n  \n\n\n    Auto-fill application forms.\n  \n\n\n    Generate personalized cover letters using AI.\n  \n\n\n    Send custom messages to hiring managers.\n  \n\n\nBenefits:\n\n\n\n    Save time on tedious job hunting tasks.\n  \n\n\n    Increase your chances with personalized applications.\n  \n\n\n    Apply to more jobs efficiently.\n  \n\n\n    Open-source and customizable for your needs.\n  \n\n\nGet Involved:\n\n\n\n    Contribute to code development on GitHub.\n  \n\n\n    Share your job-hunting expertise through tutorials and guides.\n  \n\n\n    Spread the word to help more job seekers.\n  \n\n\nI firmly believe that Octoffers is the ultimate solution to the job-seeking crisis."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I'm excited to share Python-Type-Challenges, a collection of hands-on, interactive challenges designed to help Python developers master type annotations. Whether you're new to type hints or looking to deepen your understanding, these exercises provide a fun and educational way to explore Python's type system. I'd love to get your feedback and contributions, have fun!\n  python-type-challenges.zeabur.app\n    Also check out the source code on GitHub"
},
{
    "title": "No title",
    "content": "This is an extension of the python standard library - configparser.\n  \n    It provides an additional method - `parse_section` which takes in a dataclass as an argument and returns a fully typed dataclass instance after parsing the input configuration.\n  \n    Some features:-\n  \n    ✓ Fully typed.✓ Use dataclasses to parse the configuration file.✓ Support for almost all python built-in data types - int, float, str, list, tuple, dict and complex data types using Union and Optional.✓ Built on top of configparser, hence retains all functionalities of configparser.✓ Support for optional values (optional values are automatically set to None if not provided).✓ Smarter defaults.\n  \n    Check github repo for more information."
},
{
    "title": "No title",
    "content": "You can view the gist here: https://gist.github.com/mattmills49/44a50b23d3c7a8f71dfadadd0f876ac2\n\n    Here is how the function works: https://pbs.twimg.com/media/GEiGQoEXQAAuSGC?format=jpg&name=small\n\n    A quick example showing how you can use it on a dataframe to get a quick and concise summary of your data: https://pbs.twimg.com/media/GEiFbpNX0AAHg7R?format=jpg&name=small\n\n    And even include it in plain text using jupyter or quarto: https://pbs.twimg.com/media/GEiGTi9WgAAbPo-?format=png&name=900x900\n\n    EDIT: Thanks to u/RedKrieg for pointing out that these are called sparklines and for sharing his own package that you all should check out as well."
},
{
    "title": "No title",
    "content": "Lace is a Bayesian tabular data inference engine (backed by rust) designed to facilitate scientific discovery by learning a model of the data instead of a model of a question.\n  \n    Lace ingests pseudo-tabular data from which it learns a joint distribution over the table, after which users can ask any number of questions and explore the knowledge in their data with no extra modeling. Lace is both generative and discriminative, which allows users to\n  \n\n\n    determine which variables are predictive of which others\n  \n\n\n    predict quantities or compute likelihoods of any number of features conditioned on any number of other features\n  \n\n\n    identify, quantify, and attribute uncertainty from variance in the data, epistemic uncertainty in the model, and missing features\n  \n\n\n    generate and manipulate synthetic data\n  \n\n\n    identify anomalies, errors, and inconsistencies within the data\n  \n\n\n    determine which records/rows are similar to which others on the whole or given a specific context\n  \n\n\n    edit, backfill, and append data without retraining\n  \n\n\nThe v0.6.0 release of Lace focuses on the user experience around explainability.\n\n    We've changed the way epistemic uncertainty is computed (Jensen-Shannon divergence to Total Variation distance) and added functionality to:\n  \n\n\n    attribute prediction uncertainty, data anomalousness, and data inconsistency\n  \n\n\n    determine which anomalies are attributable and which are not\n  \n\n\n    explain which predictors are important to which predictions and why\n  \n\n\n    visualize model states\n  \n\n\n    Main page: https://lace.dev\n\n    Github: https://github.com/promised-ai/lace/\n\n    Crates.io: https://crates.io/crates/lace/0.6.0\n\n    Pypi: https://pypi.org/project/pylace/0.6.0/"
},
{
    "title": "No title",
    "content": "Pinecone recently launched its serverless offering with the goal of making vector search implementation easier and cost-efficient. With LLMs rapidly becoming a core part of applications and most of them using Retrieval-augmented generation (RAG) in their implementation – vector search is an important concept to understand. We’ve written a blog about what is vector search, why it's needed, and how to implement it https://blog.kusho.ai/a-primer-on-vector-search-using-pinecone-serverless/"
},
{
    "title": "No title",
    "content": "TLDR; pyimps is now here: https://github.com/bedbad/pyimpsThis works:\n  \npip install pyimps\n\n    I always wanted to automate python correct dependency fetching and resolution and in the very list see the entire py source or any findable module dependency tree right away, in the terminal; In my experience python imports were always easy - until they were not when I encountered pip being different version then python due to alias and a whole bunch of runtime-specific weirdness when you're surprised python can't find something you've surely pip installed  or built. Not calling python -m module.submodule correctly can be a source of banging head. Recentlty banged my head against some rogue modules in tensorflow build defined programmatically in few lines of a source file, and then programmatically deleted based on condition.\n  \n    At that point I wrote this shell tool that detangles all the above tells you where the imports you have are at, what they are and where the ones you don't should have been with all relevant python search paths, if there's import failue.\n  \n    It works, does basics of I wanted it to do barring looking packages up as pypi had blocked such requests at the moment and relating fetching and automation functionality.\n  \n    If you remember something itching  about the importlib machinery, that would be interesting to include here.\n  \n    Update:\n  \n    Added color code key and verbose for types, so now it looks like:\n  \npyimps -v tensorflow\nModule path: Library/Python/3.9/lib/python/site-packages/tensorflow/__init__.py\ntensorflow(ROOT)<====/Users/im_1/Library/Python/3.9/lib/python/site-packages/tensorflow/__init__.py\n          ↑___python(DEL)(deleted)\n                ↑___framework(SUB)<====Library/Python/3.9/lib/python/site-packages/tensorflow/python/framework/__init__.py\n                         ↑___tensor_conversion(SUB)<====Library/Python/3.9/lib/python/site-packages/tensorflow/python/framework/tensor_conversion.py\n                                          ↑___convert_to_tensor_v2_with_dispatch(FUN)\n                         ↑___tensor_shape(SUB)<====Library/Python/3.9/lib/python/site-packages/tensorflow/python/framework/tensor_shape.py\n                                     ↑___TensorShape(CLASS)\n                         ↑___versions(SUB)<====Library/Python/3.9/lib/python/site-packages/tensorflow/python/framework/versions.py\n                                 ↑___CXX11_ABI_FLAG(VARIABLE)\n                                 ↑___GIT_VERSION(VARIABLE)\n                                 ↑___COMPILER_VERSION(VARIABLE)\n                                 ↑___VERSION(VARIABLE)\n                                 ↑___CXX_VERSION(VARIABLE)\n                                 ↑___MONOLITHIC_BUILD(VARIABLE)\n    This lib is still in its seeding state;\n  \n    What I'm looking for are bug reports, concrete usage case/feature requests to see if it's warranted to invest my time.\n  \n    I'd wish complete python project dependency graph analysis with build automation for monorepo/bazel/hermetic python builds and that's whole next level"
},
{
    "title": "No title",
    "content": "Hello all, wanted to get some feedback on a lib I build a few yrs ago, dictor\n  \n    it handles dicts and json structures in a pythonic lookup structure, with built in error handling and default/fallback values\n  \n    ie\n  \n    sample.json =\n  {\n  \"characters\": {\n    \"Lonestar\": {\n      \"id\": 55923,\n      \"role\": \"renegade\",\n      \"items\": [\"space winnebago\", \"leather jacket\"]\n    },\n    \"Barfolomew\": {\n      \"id\": 55924,\n      \"role\": \"mawg\",\n      \"items\": [\"peanut butter jar\", \"waggy tail\"]\n    },\n    \"Dark Helmet\": {\n      \"id\": 99999,\n      \"role\": \"Good is dumb\",\n      \"items\": [\"Shwartz\", \"helmet\"]\n    },\n    \"Skroob\": {\n      \"id\": 12345,\n      \"role\": \"Spaceballs CEO\",\n      \"items\": [\"luggage\"]\n    }\n  }\n}with open('sample.json') as data:\n    data = json.load(data)\n\ncharacter = dictor(data, \"Lonestar.items\")\nprint(character)\n\n>> [\"space winnebago\", \"leather jacket\"]\n    has many other features\n  \n    any PRs or feedback appreciated, thx\n  \nhttps://github.com/perfecto25/dictor"
},
{
    "title": "No title",
    "content": "Hello,\n  \n    some time ago I posted about writing mathler solver. Now I added selenium to solve actual, live version of it.https://medium.com/@jandanecki/solving-mathler-riddle-with-python-part-2-de91c87c41ba"
},
{
    "title": "No title",
    "content": "I started this new poetry plugin to simplify the creation of docker images starting from a poetry project. The main goal is to create the docker image effortless, with ZERO configuration required.\n  \n    This is the pypi: https://pypi.org/project/poetry-dockerize-plugin/\n\n    Source code: https://github.com/nicoloboschi/poetry-dockerize-plugin\n\n    Do you think you would use it ? why and why not ? what would be the must-to-have features ?"
},
{
    "title": "No title",
    "content": "I do not love the word Pythonic, i believe a language should evolve, even because are years that real new languages are not created.\n  \n    I am a lazy and kinda stupid programmer, looking for idiomatic sugar, and most important not repeat myself. On this basis i have to say that the last 3 months I used python i had to use the word self thousands of times. I do not like. I am not asking to inquire if is just or wrong, someone like pizza, other people French fries so will mark the post as `discussion` although maybe is more an help, I humbly would like to know if there is a way i cannot repeat the self everywhere, I am comfortable will still have readable code\n  \n\n    EDIT: In general would not be lovely to work with scoped functions to write faster code in this way for instance? with keyword\n\n\n    EDIT2: to be honest i find really childish to downvote me, i am just sharing a view, trying to understand and asking for help for something that for me is really frustrating to do: typing more than needed, not to follow what the cool guys do\n  \n\n    EDIT3: example\n  \n\n    from self:class SelfishClass:\n  \n\n    def __init__(self):self.instance_variable = None\n  \n\n    def write_to_inner(self, val: int) -> None:self.inner_property = val\n  \n\n    SelfishClass().write_inner(2)to kinda of pseudoPython:class SelfishClass:\n  \n    instanceVariable = 0 #nullability would be better ex instanceVariable: Int?\n  \n    def writeToInner(variable: Int):instanceVariable = variable\n  \n    print(instanceVariable)\n  \n    writeToInner(2)"
},
{
    "title": "No title",
    "content": "Introduction\n    You want to pump out code fast without all those pesky best practices slowing you down. Who cares if your code is impossible to maintain and modify later? You're a COWBOY CODER! This guide will teach you how to write sloppy, unprofessional code that ignores widely-accepted standards, making your codebase an incomprehensible mess! Follow these tips and your future self will thank you with days of frustration and head-scratching as they try to build on or fix your masterpiece. Yeehaw!\n  1. Avoid Object Oriented Programming\n    All those classes, encapsulation, inheritance stuff - totally unnecessary! Just write giant 1000+ line scripts with everything mixed together. Functions? Where we're going, we don't need no stinkin' functions! Who has time to context switch between different files and classes? Real programmers can keep everything in their head at once. So, toss out all that OOP nonsense. The bigger the file, the better!\n  2. Copy and Paste Everywhere\n    Need the same code in multiple places? Just copy and paste it! Refactoring is for losers. If you've got an algorithm or bit of logic you need to reuse, just duplicate that bad boy everywhere you need it. Who cares if you have to update it in 15 different places when requirements change? Not you - you'll just hack it and move on to the next thing! Duplication is your friend!\n  3. Globals and Side Effects Everywhere\n    Variables, functions, state - just toss 'em in the global namespace! Who needs encapsulation when you can just directly mutate whatever you want from anywhere in the code? While you're at it, functions should have all kinds of side effects. Don't document them though - make your teammate guess what that function call does!\n  4. Nested Everything\n    Nested loops, nested ifs, nested functions - nest to your heart's content! Who cares if the code is indented 50 levels deep? Just throw a comment somewhere saying \"Here be dragons\" and call it a day. Spaghetti code is beautiful in its own way.\n  5. Magic Numbers and Hardcoded Everything\n    Litter your code with magic numbers and hardcoded strings - they really add that human touch. Who needs constants or config files? Hardcode URLs, API keys, resource limits - go wild! Keep those release engineers on their toes!\n  6. Spaghetti Dependency Management\n    Feel free to import anything from anywhere. Mix and match relative imports, circular dependencies, whatever you want! from ../../utils import helpers, constants, db - beautiful! Who cares where it comes from as long as it works...until it suddenly breaks for no apparent reason.\n  7. Write Every Line as It Comes to You\n    Don't waste time planning or designing anything up front. Just start hacking! Stream of consciousness coding is the way to go. Just write each line and idea as it pops into your head. Who cares about architecture - you've got CODE to write!\n  8. Documentation is Overrated\n    Real programmers don't comment their code or write documentation. If nobody can understand that brilliant algorithm you spent days on, that's their problem! You're an artist and your masterpiece should speak for itself.\n  9. Testing is a Crutch\n    Don't waste time writing tests for your code. If it works on your machine, just ship it! Who cares if untested code breaks the build or crashes in production - you'll burn that bridge when you get to it. You're a coding cowboy - unleash that beautiful untested beast!\n  10. Commit Early, Commit Often\n    Branching, pull requests, code review - ain't nobody got time for that! Just commit directly to the main branch as often as possible. Don't worry about typos or half-finished work - just blast it into the repo and keep moving. Git history cleanliness is overrated!\n  11. Manual Deployments to Production\n    Set up continuous integration and delivery? No way! Click click click deploy to production manually whenever you feel like it. 3am on a Sunday? Perfect time! Wake your team up with exciting new bugs and regressions whenever you deploy.\n  12. Don't Handle Errors\n    Error handling is boring. Just let your code crash and burn - it adds excitement! Don't wrap risky sections in try/catch blocks - let those exceptions bubble up to the user. What's the worst that could happen?\n  13. Security is for Chumps\n    Who needs authentication or authorization? Leave all your APIs wide open, logins optional. Store passwords in plain text, better yet - hardcoded in the source! SQL injection vulnerabilities? Sounds like a feature!\n  14. Dread the Maintenance Phase\n    The most important part of coding is the NEXT feature. Just hack together something that barely works and move on to the next thing. Who cares if your unmaintainable mess gives the next developer nightmares? Not your problem anymore!\n  Conclusion\n    Follow these top tips, and you'll be writing gloriously UNMAINTAINABLE code in no time! When you inevitably leave your job, your team will fondly remember you as they desperately rewrite the pile of spaghetti code you left behind. Ride off into the sunset, you brilliant, beautiful code cowboy! Happy hacking!"
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "Hey y'all, I've recently made a very simple project in a few hours and thought it would've been pretty cool to share!\n  \nhttps://github.com/JoshuaKasa/DASO\n\n    Let me know what you think! I very much welcome all type of contributions, especially 'cause I'd need someone to make a .exe file out of the project so that I can remove the .bat files lol."
},
{
    "title": "No title",
    "content": "Quart-Schema is a Quart extension that provides schema validation and auto-generated openAPI documentation. It works by decorating routes with the desired structures,\n  from dataclasses import dataclass\nfrom datetime import datetime\n\nfrom quart import Quart\nfrom quart_schema import QuartSchema, validate_request, validate_response\n\napp = Quart(__name__)\nQuartSchema(app)\n\n@dataclass\nclass Todo:\n    task: str\n    due: datetime | None\n\n@app.post(\"/\")\n@validate_request(Todo)\n@validate_response(Todo, 201)\nasync def create_todo(data: Todo) -> tuple[Todo, int]:\n    ... # Do something with data, e.g. save to the DB\n    return data, 201\n    With the autogenerated openAPI documentation visable at /docs.\n  \n    With the 0.19 release the dataclass in the above can be swapped with an attrs defined class, a msgspec Struct or a Pydantic BaseModel.\n  \n    See full changelog here"
},
{
    "title": "No title",
    "content": "JSON is arguably the most popular data format used across the internet. A lot of programming languages and web frameworks provide convenient methods to parse JSON and sometimes they do this out of the box. I’ve always used Python's JSON parser and I have never really bothered to look into how it works under the hood. In the spirit of learning how things work, I decided to start small. I wrote a very simple JSON parser in Python to better understand how it works. If you’d like to follow my thought process and approach you can read my article here https://volomn.com/blog/write-a-simple-json-parser-in-python"
},
{
    "title": "No title",
    "content": "Hi all,\n  \n    I just published a new package on PyPI: fasthx. See the docs here: https://volfpeter.github.io/fasthx/\n\n    Key features:\n  \n\n\n    Decorator syntax that works with FastAPI as one would expect, no need for unused or magic dependencies in routes.\n  \n\n\n    Works with any templating engine or server-side rendering library.\n  \n\n\n    Built-in Jinja2 templating support.\n  \n\n\n    FastAPI routes will keep working normally by default if they receive non-HTMX requests, so the same route can serve data and render HTML at the same time.\n  \n\n\n    Correct typing makes it possible to apply other (typed) decorators to your routes.\n  \n\n\n    Give it a look if you're in the target audience."
},
{
    "title": "No title",
    "content": "def Point[T](x: T, y: T):\n    ...\n    this looks great but, would this look better?\n  def [T] Point(x: T, y: T):\n    ...\n    Imagine cluttering this new syntax\n  def Point[T, E, S, L, A](x: T, y: T):\n    ...\n    doesn't this look messy?\n  \n    Imagine doing it this way\n  def [T, E, S, L, A] Point(x: T, y: T):\n    ..."
},
{
    "title": "No title",
    "content": "I was playing around with running Prefect workflows on cloud-hosted data with a new integration from some colleagues at Coiled. Turns out it was pretty straightforward to deploy a daily data processing job on a NASA dataset I’ve been working with lately. Thought I’d share a write up of what this looks like.\n  # python workflow.py              # Runs locally\ncoiled prefect serve workflow.py  # Runs on the cloud\n    Blog post: https://medium.com/coiled-hq/schedule-python-jobs-with-prefect-and-coiled-b22180a25f1f\n\n    (edited to fix the code snippet formatting)"
},
{
    "title": "No title",
    "content": "I released cfdp-py today, which is a Python library providing high level componentsto transfer files according to the CCSDS File Delivery Protocol.\n  \n    I've been working on this crate for a longer time now which was previous part of a larger package and I decided to create a dedicated package for it now. A lot of the code is already used in our ground infrastructure to perform software updates for our satellite, and I recently added some nice features, including the acknowledged mode.\n  \n    The tricky thing about (large) file transfers with remote systems is that the protocols are oftentimes still packet based. The CFDP standard specifies how to split files into chunks which can be sent as packets (for example CCSDS space packets). I designed the state machines of the library so they have a relatively simple API which can process received packets, generate new packets to be sent to the remote entity and drive the state machine with dedicated methods. This allows large flexibility in how to schedule the state machine (flow control) and allows freedom on packet post-processing and how to send these packets.\n  \n    For example, we wrap these CFDP packets into CCSDS space packets, so we have one uniform packet type used for communication (and because some of the hardware actually expects these packets to be CCSDS conformant). We send these packets around the ground infastructure using a message queue middleware, but of course simpler approaches like sending via UDP or a serial driver are also possible. In the future, I might add a CFDP daemon implementation which can also track multiple concurrent requests building on top of these state machines.\n  \n    Maybe the library can help some people working in the space sector :)\n  \n    Kind Regards"
},
{
    "title": "No title",
    "content": "Octoffers is a revolutionary open-source Python tool designed to tackle the challenges of job hunting. It streamlines the application process by scraping job listings from platforms like LinkedIn and Indeed etc, auto-filling forms, and generating AI-powered cover letters tailored to each job description. The goal is to save you time, boost your application success rate, and allow you to secure interview automatically.\n  \nKey Features:\n\n\n\n    Scrape jobs from popular career platforms.\n  \n\n\n    Auto-fill application forms.\n  \n\n\n    Generate personalized cover letters using AI.\n  \n\n\n    Send custom messages to hiring managers.\n  \n\n\nBenefits:\n\n\n\n    Save time on tedious job hunting tasks.\n  \n\n\n    Increase your chances with personalized applications.\n  \n\n\n    Apply to more jobs efficiently.\n  \n\n\n    Open-source and customizable for your needs.\n  \n\n\nGet Involved:\n\n\n\n    Contribute to code development on GitHub.\n  \n\n\n    Share your job-hunting expertise through tutorials and guides.\n  \n\n\n    Spread the word to help more job seekers.\n  \n\n\nI firmly believe that Octoffers is the ultimate solution to the job-seeking crisis."
},
{
    "title": "No title",
    "content": "Daily Thread"
},
{
    "title": "No title",
    "content": "I'm excited to share Python-Type-Challenges, a collection of hands-on, interactive challenges designed to help Python developers master type annotations. Whether you're new to type hints or looking to deepen your understanding, these exercises provide a fun and educational way to explore Python's type system. I'd love to get your feedback and contributions, have fun!\n  python-type-challenges.zeabur.app\n    Also check out the source code on GitHub"
},
{
    "title": "No title",
    "content": "This is an extension of the python standard library - configparser.\n  \n    It provides an additional method - `parse_section` which takes in a dataclass as an argument and returns a fully typed dataclass instance after parsing the input configuration.\n  \n    Some features:-\n  \n    ✓ Fully typed.✓ Use dataclasses to parse the configuration file.✓ Support for almost all python built-in data types - int, float, str, list, tuple, dict and complex data types using Union and Optional.✓ Built on top of configparser, hence retains all functionalities of configparser.✓ Support for optional values (optional values are automatically set to None if not provided).✓ Smarter defaults.\n  \n    Check github repo for more information."
},
{
    "title": "No title",
    "content": "You can view the gist here: https://gist.github.com/mattmills49/44a50b23d3c7a8f71dfadadd0f876ac2\n\n    Here is how the function works: https://pbs.twimg.com/media/GEiGQoEXQAAuSGC?format=jpg&name=small\n\n    A quick example showing how you can use it on a dataframe to get a quick and concise summary of your data: https://pbs.twimg.com/media/GEiFbpNX0AAHg7R?format=jpg&name=small\n\n    And even include it in plain text using jupyter or quarto: https://pbs.twimg.com/media/GEiGTi9WgAAbPo-?format=png&name=900x900\n\n    EDIT: Thanks to u/RedKrieg for pointing out that these are called sparklines and for sharing his own package that you all should check out as well."
},
{
    "title": "No title",
    "content": "Lace is a Bayesian tabular data inference engine (backed by rust) designed to facilitate scientific discovery by learning a model of the data instead of a model of a question.\n  \n    Lace ingests pseudo-tabular data from which it learns a joint distribution over the table, after which users can ask any number of questions and explore the knowledge in their data with no extra modeling. Lace is both generative and discriminative, which allows users to\n  \n\n\n    determine which variables are predictive of which others\n  \n\n\n    predict quantities or compute likelihoods of any number of features conditioned on any number of other features\n  \n\n\n    identify, quantify, and attribute uncertainty from variance in the data, epistemic uncertainty in the model, and missing features\n  \n\n\n    generate and manipulate synthetic data\n  \n\n\n    identify anomalies, errors, and inconsistencies within the data\n  \n\n\n    determine which records/rows are similar to which others on the whole or given a specific context\n  \n\n\n    edit, backfill, and append data without retraining\n  \n\n\nThe v0.6.0 release of Lace focuses on the user experience around explainability.\n\n    We've changed the way epistemic uncertainty is computed (Jensen-Shannon divergence to Total Variation distance) and added functionality to:\n  \n\n\n    attribute prediction uncertainty, data anomalousness, and data inconsistency\n  \n\n\n    determine which anomalies are attributable and which are not\n  \n\n\n    explain which predictors are important to which predictions and why\n  \n\n\n    visualize model states\n  \n\n\n    Main page: https://lace.dev\n\n    Github: https://github.com/promised-ai/lace/\n\n    Crates.io: https://crates.io/crates/lace/0.6.0\n\n    Pypi: https://pypi.org/project/pylace/0.6.0/"
},
{
    "title": "No title",
    "content": "Pinecone recently launched its serverless offering with the goal of making vector search implementation easier and cost-efficient. With LLMs rapidly becoming a core part of applications and most of them using Retrieval-augmented generation (RAG) in their implementation – vector search is an important concept to understand. We’ve written a blog about what is vector search, why it's needed, and how to implement it https://blog.kusho.ai/a-primer-on-vector-search-using-pinecone-serverless/"
},

]